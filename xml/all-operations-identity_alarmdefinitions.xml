<?xml version="1.0"?>
<!DOCTYPE section [
 <!ENTITY % entities SYSTEM "entities.ent"> %entities;
]>
<section xmlns="http://docbook.org/ns/docbook" xmlns:xi="http://www.w3.org/2001/XInclude" xmlns:xlink="http://www.w3.org/1999/xlink" xml:id="identity_alarmdefinitions"><title>Identity Alarms</title>
    <sidebar><para>These alarms show under the Identity section of the HPE Helion Ops Console.</para>
<informaltable xml:id="table_gt2_hzp_mx" colsep="1" rowsep="1"><tgroup cols="5">
            <colspec colname="c1" colnum="1"/>
            <colspec colname="c2" colnum="2"/>
            <colspec colname="c3" colnum="3"/>
            <colspec colname="c4" colnum="4"/>
            <colspec colname="c5" colnum="5"/>
            <thead>
              <row>
                <entry>Service</entry>
                <entry>Alarm Name</entry>
                <entry>Description</entry>
                <entry>Likely Cause</entry>
                <entry>Mitigation Tasks to Perform</entry>
              </row>
            </thead>
            <tbody>
              <row>
                <entry morerows="4">identity-service</entry>
                <entry>HTTP Status</entry>
                <entry>
                  <screen>component=keystone-api
api_endpoint=public</screen>
                  <para>This check is contacting the Keystone public endpoint directly. </para>

                </entry>
                <entry>The Keystone service is down on the affected node.</entry>
                <entry>Restart the Keystone service on the affected node: <orderedlist xml:id="ol_ht2_hzp_mx">
                    <listitem><para>Log in to the lifecycle manager.</para>
</listitem>
                    <listitem><para>Use the Keystone start playbook against the affected node:
                      </para>
<screen>cd ~/scratch/ansible/next/hos/ansible
ansible-playbook -i hosts/verb_hosts keystone-start.yml --limit &lt;hostname&gt;</screen></listitem>
                  </orderedlist></entry>
              </row>
              <row>
                <entry>HTTP Status</entry>
                <entry>
                  <screen>component=keystone-api
api_endpoint=admin</screen>
                  <para>This check is contacting the Keystone admin endpoint directly.</para>

                </entry>
                <entry>The Keystone service is down on the affected node.</entry>
                <entry>Restart the Keystone service on the affected node: <orderedlist xml:id="ol_it2_hzp_mx">
                    <listitem><para>Log in to the lifecycle manager.</para>
</listitem>
                    <listitem><para>Use the Keystone start playbook against the affected node:
                      </para>
<screen>cd ~/scratch/ansible/next/hos/ansible
ansible-playbook -i hosts/verb_hosts keystone-start.yml --limit &lt;hostname&gt;</screen></listitem>
                  </orderedlist></entry>
              </row>
              <row>
                <entry>HTTP Status</entry>
                <entry>
                  <screen>component=keystone-api
monitored_host_type=vip</screen>
                  <para>This check is contacting the Keystone admin endpoint via the virtual IP address
                    (HAProxy).</para>

                </entry>
                <entry>The Keystone service is unreachable via the virtual IP address.</entry>
                <entry>If neither the <literal>api_endpoint=public</literal> or
                    <literal>api_endpoint=admin</literal> alarms are triggering at the same time then
                  there is likely a problem with haproxy. <para>You can restart the haproxy service
                    with these steps:</para>

                  <orderedlist xml:id="ol_jt2_hzp_mx">
                    <listitem><para>Log in to the lifecycle manager.</para>
</listitem>
                    <listitem><para>Use this playbook against the affected node:
                      </para>
<screen>cd ~/scratch/ansible/next/hos/ansible
ansible-playbook -i hosts/verb_hosts FND-CLU-start.yml --limit &lt;hostname&gt;</screen></listitem>
                  </orderedlist></entry>
              </row>
              <row>
                <entry>Process Check</entry>
                <entry>Separate alarms for each of these Glance services, specified by the
                    <literal>component</literal> dimension: <itemizedlist xml:id="ul_kt2_hzp_mx">
                    <listitem><para>keystone-main</para>
</listitem>
                    <listitem><para>keystone admin</para>
</listitem>
                  </itemizedlist></entry>
                <entry>Process crashed.</entry>
                <entry>
                  <para>You can restart the Keystone service with these steps:</para>

                  <orderedlist xml:id="ol_lt2_hzp_mx">
                    <listitem><para>Log in to the lifecycle manager.</para>
</listitem>
                    <listitem><para>Use this playbook against the affected node:
                      </para>
<screen>cd ~/scratch/ansible/next/hos/ansible
ansible-playbook -i hosts/verb_hosts keystone-start.yml --limit &lt;hostname&gt;</screen></listitem>
                  </orderedlist>
                  <para>Review the logs in <literal>/var/log/keystone</literal> on the affected node.</para>

                </entry>
              </row>
              <row>
                <entry>Service Log Directory Size</entry>
                <entry>Service log directory consuming more disk than its quota.</entry>
                <entry>This could be due to a service set to <literal>DEBUG</literal> instead of
                    <literal>INFO</literal> level. Another reason could be due to a repeating error
                  message filling up the log files. Finally, it could be due to log rotate not
                  configured properly so old log files are not being deleted properly.</entry>
                <entry>Find the service that is consuming too much disk space. Look at the logs. If
                    <literal>DEBUG</literal> log entries exist, set the logging level to
                    <literal>INFO</literal>. If the logs are repeatedly logging an error message, do
                  what is needed to resolve the error. If old log files exist, configure log rotate
                  to remove them. You could also choose to remove old log files by hand after
                  backing them up if needed.</entry>
              </row>
            </tbody>
          </tgroup></informaltable>
</sidebar>
  </section>
