<?xml version="1.0"?>
<?xml-stylesheet href="urn:x-suse:xslt:profiling:docbook51-profile.xsl"
 type="text/xml"
 title="Profiling step"?>
<!DOCTYPE section [
 <!ENTITY % entities SYSTEM "entities.ent"> %entities;
]>
<section xmlns="http://docbook.org/ns/docbook" xmlns:xi="http://www.w3.org/2001/XInclude" xmlns:xlink="http://www.w3.org/1999/xlink" xml:id="add_monitor_node">
 <title>Adding a Ceph Monitor Node</title>
 <para>
  During your initial installation and deployment, if you defined a dedicated
  cluster for the Ceph monitor component then these steps can be used to add
  additional Ceph monitor nodes.
 </para>
 <para>
  During your initial installation and deployment, if you defined separate
  hosts for the Ceph monitor component then these steps can be used to add
  additional Ceph monitor nodes.
 </para>
 <para>
  If you did not define separate Ceph monitor nodes in your deployment then
  these steps will not work for you.
  <!-- ref goes to a commented Ceph-related section, there is no way to fix
  it. - sknorr, 2018-04-03-->
  <!-- For details on how to define a dedicated
  Ceph monitor cluster, see <xref linkend="topic_ah5_4yx_qt"/>. -->
 </para>
 <para>
  The steps involved are:
 </para>
 <section xml:id="idg-all-operations-maintenance-ceph-add_monitor_node-xml-6">
  <title>Prerequisites</title>
  <para>
   You should have one or more baremetal servers that meet the minimum hardware
   requirements for a Ceph monitor node which are documented in the
   <xref linkend="min_hardware"/>.
  </para>
 </section>
 <section xml:id="idg-all-operations-maintenance-ceph-add_monitor_node-xml-7">
  <title>Notes</title>
  <para>
   It is recommended to maintain an odd number of Ceph monitors because the
   Paxos protocol is being used by Ceph monitors to form a consensus. Although
   clusters can be deployed with a single monitor, we strongly recommend to
   deploy three monitors on independent nodes for production usage to avoid a
   single point of failure of the monitor service.
  </para>
  <para>
   For this reason, it is recommended to increase the number of monitors two at
   a time if a valid need for more than three exists.
  </para>
 </section>
 <section xml:id="idg-all-operations-maintenance-ceph-add_monitor_node-xml-8">
  <title>Adding a Ceph Monitor Node</title>
  <para>
   These steps will show you how to add the new Ceph monitor node to your
   <literal>servers.yml</literal> file and then run the playbooks that update
   your cloud configuration. You will run these playbooks from the lifecycle
   manager.
  </para>
  <orderedlist>
   <listitem>
    <para>
     Log in to your &lcm;.
    </para>
   </listitem>
   <listitem>
    <para>
     Checkout the <literal>site</literal> branch of your local git so you can
     begin to make the necessary edits:
    </para>
<screen>cd ~/openstack/my_cloud/definition/data
git checkout site</screen>
   </listitem>
   <listitem>
    <para>
     In the same directory, edit your <literal>servers.yml</literal> file to
     add the details for your new Ceph monitor nodes. Copy and paste the
     section for an existing server having the <literal>MON-ROLE</literal> role
     and edit the values to match the new server being added.
    </para>
    <para>
     Here is an example:
    </para>
<screen>
- id: ceph-mon4
  ip-addr: 10.13.111.141
  server-group: RACK1
  role: CEP-MON-ROLE
  nic-mapping: MY-4PORT-SERVER
  mac-addr: "f0:92:1c:05:69:10"
  ilo-ip: 10.12.8.217
  ilo-password: password
  ilo-user: admin</screen>
    <important>
     <para>
      You will need to verify that the <literal>ip-addr</literal> value you
      choose for this node does not conflict with any other IP address in your
      cloud environment. You can confirm this by checking the
      <literal>~/openstack/my_cloud/info/address_info.yml</literal> file on your
      &lcm;.
     </para>
    </important>
   </listitem>
   <listitem>
    <para>
     In your <literal>control_plane.yml</literal> file you will need to check
     the values for <literal>member-count</literal>,
     <literal>min-count</literal>, and <literal>max-count</literal>. If you
     specified them, ensure that they match up with your new total node count.
     So for example, if you had previously specified <literal>member-count:
     3</literal> and are adding a fourth Ceph monitor node, you will need to
     change that value to <literal>member-count: 4</literal>.
    </para>
   </listitem>
   <listitem>
    <para>
     Commit the changes to git:
    </para>
<screen>git commit -a -m "Add new Ceph monitor node &lt;name&gt;"</screen>
   </listitem>
   <listitem>
    <para>
     Run the configuration processor:
    </para>
<screen>cd ~/openstack/ardana/ansible
ansible-playbook -i hosts/localhost config-processor-run.yml</screen>
   </listitem>
   <listitem>
    <para>
     Update your deployment directory:
    </para>
<screen>cd ~/openstack/ardana/ansible
ansible-playbook -i hosts/localhost ready-deployment.yml</screen>
   </listitem>
   <listitem>
    <para>
     Add the new node into Cobbler:
    </para>
<screen>cd ~/openstack/ardana/ansible
ansible-playbook -i hosts/localhost cobbler-deploy.yml</screen>
   </listitem>
   <listitem>
    <para>
     Then you can image the node:
    </para>
<screen>cd ~/openstack/ardana/ansible
ansible-playbook -i hosts/localhost bm-reimage.yml -e nodelist=&lt;node name&gt;</screen>
    <para>
     Before proceeding, you may want to take a look at
     <emphasis role="bold">~/openstack/my_cloud/info/server_info.yml</emphasis> to
     see if the assignment of the node you have added is what you expect. It
     may not be, as nodes will not be numbered consecutively if any have
     previously been removed. This is to prevent loss of data; the config
     processor retains data about removed nodes and keeps their ID numbers from
     being reallocated. For more information, see
     <xref linkend="persistedserverallocations"/>.
    </para>
   </listitem>
   <listitem>
    <para>
     Run the following series of playbooks complete the deployment of your
     additional Ceph monitor node(s) to your Ceph cluster:
    </para>
<screen>cd ~/scratch/ansible/next/hos/ansible
ansible-playbook -i hosts/verb_hosts site.yml --tags "generate_hosts_file"
ansible-playbook -i hosts/verb_hosts site.yml --limit &lt;hostname&gt;</screen>
    <para>
     The hostname of the newly added node can be found in the list generated
     from the output of the following command:
    </para>
<screen>grep hostname ~/openstack/my_cloud/info/server_info.yml</screen>
   </listitem>
  </orderedlist>
 </section>
 <section xml:id="idg-all-operations-maintenance-ceph-add_monitor_node-xml-9">
  <title>Verifying the New Ceph Monitor Node</title>
  <para>
   From the &lcm;, run the following command:
  </para>
<screen>ceph --cluster ceph -f json quorum_status</screen>
  <para>
   If the Ceph cluster name is different than the default (i.e.
   <emphasis role="bold">ceph</emphasis>), you need to modify the above command
   accordingly.
  </para>
  <para>
   The newly added Ceph monitor's hostname appears in the
   <literal>quorum_names</literal> section of the output of above command.
  </para>
 </section>
 <section xml:id="idg-all-operations-maintenance-ceph-add_monitor_node-xml-10">
  <title>Adding a New Ceph Monitor Node to Monitoring</title>
  <para>
   If you want to add a new Ceph monitor node to the monitoring service checks,
   there is an additional playbook that must be run to ensure this happens:
  </para>
<screen>cd ~/scratch/ansible/next/hos/ansible
ansible-playbook -i hosts/verb_hosts monasca-deploy.yml --tags active_ping_checks</screen>
 </section>
</section>
