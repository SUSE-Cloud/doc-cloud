<?xml version="1.0"?>
<!DOCTYPE section [
 <!ENTITY % entities SYSTEM "entities.ent"> %entities;
]>
<!---->
<section xmlns="http://docbook.org/ns/docbook" xmlns:xi="http://www.w3.org/2001/XInclude" xmlns:xlink="http://www.w3.org/1999/xlink" xml:id="log_management_integration">
 <title>Log Management and Integration</title>
 <section>
  <title>Overview</title>
  <para>
   &kw-hos; uses the ELK (&elasticsearch;, Logstash, Kibana) stack for log
   management across the entire cloud infrastructure. This configuration
   facilitates simple administration as well as integration with third-party
   tools. This tutorial covers how to forward your logs to a third-party tool
   or service, and how to access and search the &elasticsearch; log stores
   through API endpoints.
  </para>
 </section>
 <section xml:id="section_pvm_zkx_1x">
  <title>The ELK stack</title>
  <para>
   The ELK logging stack consists of the &elasticsearch;, Logstash, and
   Kibana elements:
  </para>
  <itemizedlist>
   <listitem>
    <formalpara>
     <title>Logstash</title>
     <!-- mwelch: Why not put this second in this list, to maintain the
     &quot;ELK&quot; order? -->
     <para>
      Logstash reads the log data from the services running on your servers,
      and then aggregates and ships that data to a storage location. By
      default,
    <!-- mwelch: The style guide says to use indexes, or does OpenStack use
    indices? -->
    <!-- jfrench: I'm not sure they have a standard. Since the two words mean
    the same thing, I'm fine with either.-->
      Logstash sends the data to the &elasticsearch; indexes, but it can also
      be configured to send data to other storage and indexing tools such as
      Splunk.
     </para>
    </formalpara>
   </listitem>
   <listitem>
    <formalpara>
     <title>&elasticsearch;</title>
     <para>
      &elasticsearch; is the storage and indexing component of the ELK stack.
      It stores and indexes the data received from Logstash. Indexing makes
      your log data searchable by tools designed for querying and analyzing
      massive sets of data. You can query the Elasticsearch datasets from the
      built-in Kibana console, a third-party data analysis tool, or through
      the &elasticsearch; API (covered later).
     </para>
    </formalpara>
   </listitem>
   <listitem>
    <formalpara>
     <title>Kibana</title>
     <para>
      Kibana provides a simple and easy-to-use method for searching,
      analyzing, and visualizing the log data stored in the &elasticsearch;
      indexes. You can customize the Kibana console to provide graphs,
      charts, and other visualizations of your log data.
     </para>
    </formalpara>
   </listitem>
  </itemizedlist>
 </section>
 <section xml:id="section_d3k_gnx_1x">
  <title>Using the &elasticsearch; API</title>
  <para>
   You can query the &elasticsearch; indexes through various language-specific
   APIs, as well as directly over the IP address and port that &elasticsearch;
   exposes on your implementation. By default, &elasticsearch; presents from
   localhost, port 9200. You can run queries directly from a terminal using
   <literal>curl</literal>. For example:
  </para>
<screen>curl -XGET 'http://localhost:9200/_search?q=tag:yourSearchTag'</screen>
  <para>
   The preceding command searches all indexes for all data with the
   "yourSearchTag" tag.
  </para>
  <para>
   You can also use the &elasticsearch; API from outside the logging node. This
   method connects over the Kibana VIP address, port 5601, using basic http
   authentication. For example, you can use the following command to perform
   the same search as the preceding search:
  </para>
<screen>curl -u kibana:&lt;password&gt; kibana_vip:5601/_search?q=tag:yourSearchTag</screen>
  <para>
   You can further refine your search to a specific index of data, in this case
   the "elasticsearch" index:
  </para>
<screen>curl -XGET 'http://localhost:9200/elasticsearch/_search?q=tag:yourSearchTag'</screen>
  <para>
   The search API is RESTful, so responses are provided in JSON format. Here's
   a sample (though empty) response:
  </para>
<screen>{
    "took":13,
    "timed_out":false,
    "_shards":{
        "total":45,
        "successful":45,
        "failed":0
    },
    "hits":{
        "total":0,
        "max_score":null,
        "hits":[]
    }
}</screen>
 </section>
 <section>
  <title>For More Information</title>
  <para>
   You can find more detailed &elasticsearch; API documentation at
   <link xlink:href="https://www.elastic.co/guide/en/elasticsearch/reference/current/search.html"/>.
  </para>
  <para>
   Review the &elasticsearch; Python API documentation at the following sources:
   <link xlink:href="http://elasticsearch-py.readthedocs.io/en/master/api.html"/>
  </para>
  <para>
   Read the &elasticsearch; Java API documentation at
   <link xlink:href="https://www.elastic.co/guide/en/elasticsearch/client/java-api/current/index.html"/>.
  </para>
 </section>
 <section xml:id="section_knd_hcf_bx">
  <title>Forwarding your logs</title>
  <para>
   You can configure Logstash to ship your logs to an outside storage and
   indexing system, such as Splunk. Setting up this configuration is as simple
   as editing a few configuration files, and then running the Ansible playbooks
   that implement the changes. Here are the steps.
  </para>
  <orderedlist xml:id="ol_mtz_mnr_bx">
   <listitem>
    <para>
     Begin by logging in to the &lcm;.
    </para>
   </listitem>
   <listitem>
    <para>
     Verify that the logging system is up and running:
    </para>
<screen>cd ~/scratch/ansible/next/ardana/ansible
ansible-playbook -i hosts/verb_hosts logging-status.yml</screen>
    <para>
     When the preceding playbook completes without error, proceed to the next
     step.
    </para>
   </listitem>
   <listitem>
    <para>
     Edit the Logstash configuration file, found at the following location:
    </para>
<screen>~/openstack/ardana/ansible/roles/logging-server/templates/logstash.conf.j2</screen>
    <para>
     Near the end of the Logstash configuration file, you will find a section for
     configuring Logstash output destinations. The following example
     demonstrates the changes necessary to forward your logs to an outside
     server (changes in bold). The configuration block sets up a TCP connection
     to the destination server's IP address over port 5514.
    </para>
<screen># Logstash outputs
    output {
      # Configure &elasticsearch; output
      # http://www.elastic.co/guide/en/logstash/current/plugins-outputs-elasticsearch.html
      elasticsearch {
        index =&gt; "&#37;{[&#64;metadata][es_index]}
        hosts =&gt; ["{{ elasticsearch_http_host }}:{{ elasticsearch_http_port }}"]
        flush_size =&gt; {{ logstash_flush_size }}
        idle_flush_time =&gt; 5
        workers =&gt; {{ logstash_threads }}
      }
      <emphasis role="bold">  # Forward Logs to Splunk on TCP port 5514 which matches the one specified in Splunk Web UI.
      tcp {
        mode =&gt; "client"
        host =&gt; "&lt;Enter Destination listener IP address&gt;"
        port =&gt; 5514
      }</emphasis>
    }
</screen>
    <para>
     Note that Logstash can forward log data to multiple sources, so there is no
     need to remove or alter the &elasticsearch; section in the preceding file.
     However, if you choose to stop forwarding your log data to &elasticsearch;,
     you can do so by removing the related section in this file, and then
     continue with the following steps.
    </para>
   </listitem>
   <listitem>
    <para>
     Commit your changes to the local git repository:
    </para>
<screen>cd ~/openstack/ardana/ansible
git add -A
git commit -m "Your commit message"</screen>
   </listitem>
   <listitem>
    <para>
     Run the configuration processor to check the status of all configuration
     files:
    </para>
<screen>ansible-playbook -i hosts/localhost config-processor-run.yml</screen>
   </listitem>
   <listitem>
    <para>
     Run the ready-deployment playbook:
    </para>
<screen>ansible-playbook -i hosts/localhost ready-deployment.yml</screen>
   </listitem>
   <listitem>
    <para>
     Implement the changes to the Logstash configuration file:
    </para>
<screen>cd ~/scratch/ansible/next/ardana/ansible
ansible-playbook -i hosts/verb_hosts logging-server-configure.yml</screen>
   </listitem>
  </orderedlist>
  <para>
   Please note that configuring the receiving service will vary from
   product to product. Consult the documentation for your particular product
   for instructions on how to set it up to receive log files from Logstash.
  </para>
 </section>
</section>
