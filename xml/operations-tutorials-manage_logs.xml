<?xml version="1.0"?>
<!DOCTYPE section [
 <!ENTITY % entities SYSTEM "entity-decl.ent"> %entities;
]>
<!---->
<section xmlns="http://docbook.org/ns/docbook" xmlns:xi="http://www.w3.org/2001/XInclude" xmlns:xlink="http://www.w3.org/1999/xlink" xml:id="log_management_integration">
 <title>Log Management and Integration</title>
 <section>
  <title>Overview</title>
  <para>
   &productname; uses the ELK (&elasticsearch;, Logstash, Kibana) stack for log
   management across the entire cloud infrastructure. This configuration
   facilitates simple administration as well as integration with third-party
   tools. This tutorial covers how to forward your logs to a third-party tool
   or service, and how to access and search the &elasticsearch; log stores
   through API endpoints.
  </para>
 </section>
 <section xml:id="section_pvm_zkx_1x">
  <title>The ELK stack</title>
  <para>
   The ELK logging stack consists of the &elasticsearch;, Logstash, and
   Kibana elements.
  </para>
  <itemizedlist>
   <listitem>
    <formalpara>
     <title>&elasticsearch;</title>
     <para>
      &elasticsearch; is the storage and indexing component of the ELK stack.
      It stores and indexes the data received from Logstash. Indexing makes
      your log data searchable by tools designed for querying and analyzing
      massive sets of data. You can query the &elasticsearch; datasets from the
      built-in Kibana console, a third-party data analysis tool, or through
      the &elasticsearch; API (covered later).
     </para>
    </formalpara>
   </listitem>
   <listitem>
    <formalpara>
     <title>Logstash</title>
     <para>
      Logstash reads the log data from the services running on your servers,
      and then aggregates and ships that data to a storage location. By
      default, Logstash sends the data to the &elasticsearch; indexes, but it
      can also be configured to send data to other storage and indexing tools
      such as Splunk.
     </para>
    </formalpara>
   </listitem>
   <listitem>
    <formalpara>
     <title>Kibana</title>
     <para>
      Kibana provides a simple and easy-to-use method for searching,
      analyzing, and visualizing the log data stored in the &elasticsearch;
      indexes. You can customize the Kibana console to provide graphs,
      charts, and other visualizations of your log data.
     </para>
    </formalpara>
   </listitem>
  </itemizedlist>
 </section>
 <section xml:id="section_d3k_gnx_1x">
  <title>Using the &elasticsearch; API</title>
  <para>
   You can query the &elasticsearch; indexes through various language-specific
   APIs, as well as directly over the IP address and port that &elasticsearch;
   exposes on your implementation. By default, &elasticsearch; presents from
   localhost, port 9200. You can run queries directly from a terminal using
   <literal>curl</literal>. For example:
  </para>
<screen>&prompt.ardana;curl -XGET 'http://localhost:9200/_search?q=tag:yourSearchTag'</screen>
  <para>
   The preceding command searches all indexes for all data with the
   "yourSearchTag" tag.
  </para>
  <para>
   You can also use the &elasticsearch; API from outside the logging node. This
   method connects over the Kibana VIP address, port 5601, using basic http
   authentication. For example, you can use the following command to perform
   the same search as the preceding search:
  </para>
<screen>curl -u kibana:&lt;password&gt; kibana_vip:5601/_search?q=tag:yourSearchTag</screen>
  <para>
   You can further refine your search to a specific index of data, in this case
   the "elasticsearch" index:
  </para>
<screen>&prompt.ardana;curl -XGET 'http://localhost:9200/elasticsearch/_search?q=tag:yourSearchTag'</screen>
  <para>
   The search API is RESTful, so responses are provided in JSON format. Here's
   a sample (though empty) response:
  </para>
<screen>{
    "took":13,
    "timed_out":false,
    "_shards":{
        "total":45,
        "successful":45,
        "failed":0
    },
    "hits":{
        "total":0,
        "max_score":null,
        "hits":[]
    }
}</screen>
 </section>
 <section>
  <title>For More Information</title>
  <para>
   You can find more detailed &elasticsearch; API documentation at
   <link xlink:href="https://www.elastic.co/guide/en/elasticsearch/reference/current/search.html"/>.
  </para>
  <para>
   Review the &elasticsearch; Python API documentation at the following sources:
   <link xlink:href="http://elasticsearch-py.readthedocs.io/en/master/api.html"/>
  </para>
  <para>
   Read the &elasticsearch; Java API documentation at
   <link xlink:href="https://www.elastic.co/guide/en/elasticsearch/client/java-api/current/index.html"/>.
  </para>
 </section>
 <section xml:id="section_knd_hcf_bx">
  <title>Forwarding your logs</title>
  <para>
   You can configure Logstash to ship your logs to an outside storage and
   indexing system, such as Splunk. Setting up this configuration is as simple
   as editing a few configuration files, and then running the Ansible playbooks
   that implement the changes. Here are the steps.
  </para>
  <orderedlist xml:id="ol_mtz_mnr_bx">
   <listitem>
    <para>
     Begin by logging in to the &clm;.
    </para>
   </listitem>
   <listitem>
    <para>
     Verify that the logging system is up and running:
    </para>
<screen>&prompt.ardana;cd ~/scratch/ansible/next/ardana/ansible
&prompt.ardana;ansible-playbook -i hosts/verb_hosts logging-status.yml</screen>
    <para>
     When the preceding playbook completes without error, proceed to the next
     step.
    </para>
   </listitem>
   <listitem>
    <para>
     Edit the Logstash configuration file, found at the following location:
    </para>
<screen>~/openstack/ardana/ansible/roles/logging-server/templates/logstash.conf.j2</screen>
    <para>
     Near the end of the Logstash configuration file, you will find a section for
     configuring Logstash output destinations. The following example
     demonstrates the changes necessary to forward your logs to an outside
     server (changes in bold). The configuration block sets up a TCP connection
     to the destination server's IP address over port 5514.
    </para>
<screen># Logstash outputs
    output {
      # Configure &elasticsearch; output
      # http://www.elastic.co/guide/en/logstash/current/plugins-outputs-elasticsearch.html
      elasticsearch {
        index =&gt; "&dollar;{[&commat;metadata][es_index]"}
        hosts =&gt; ["{{ elasticsearch_http_host }}:{{ elasticsearch_http_port }}"]
        flush_size =&gt; {{ logstash_flush_size }}
        idle_flush_time =&gt; 5
        workers =&gt; {{ logstash_threads }}
      }
      <emphasis role="bold">  # Forward Logs to Splunk on TCP port 5514 which matches the one specified in Splunk Web UI.
      tcp {
        mode =&gt; "client"
        host =&gt; "&lt;Enter Destination listener IP address&gt;"
        port =&gt; 5514
      }</emphasis>
    }
</screen>
    <para>
     Logstash can forward log data to multiple sources, so there is no need to
     remove or alter the &elasticsearch; section in the preceding file.
     However, if you choose to stop forwarding your log data to
     &elasticsearch;, you can do so by removing the related section in this
     file, and then continue with the following steps.
    </para>
   </listitem>
   <listitem>
    <para>
     Commit your changes to the local git repository:
    </para>
<screen>&prompt.ardana;cd ~/openstack/ardana/ansible
&prompt.ardana;git add -A
&prompt.ardana;git commit -m "Your commit message"</screen>
   </listitem>
   <listitem>
    <para>
     Run the configuration processor to check the status of all configuration
     files:
    </para>
<screen>&prompt.ardana;ansible-playbook -i hosts/localhost config-processor-run.yml</screen>
   </listitem>
   <listitem>
    <para>
     Run the ready-deployment playbook:
    </para>
<screen>&prompt.ardana;ansible-playbook -i hosts/localhost ready-deployment.yml</screen>
   </listitem>
   <listitem>
    <para>
     Implement the changes to the Logstash configuration file:
    </para>
<screen>&prompt.ardana;cd ~/scratch/ansible/next/ardana/ansible
&prompt.ardana;ansible-playbook -i hosts/verb_hosts kronos-server-configure.yml</screen>
   </listitem>
  </orderedlist>
  <para>
   Configuring the receiving service will vary from product to product. Consult
   the documentation for your particular product for instructions on how to set
   it up to receive log files from Logstash.
  </para>
 </section>
</section>
