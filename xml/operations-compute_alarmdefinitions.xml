<?xml version="1.0"?>
<!DOCTYPE section [
 <!ENTITY % entities SYSTEM "entity-decl.ent"> %entities;
]>
<section xmlns="http://docbook.org/ns/docbook" xmlns:xi="http://www.w3.org/2001/XInclude"
  xmlns:xlink="http://www.w3.org/1999/xlink" xml:id="compute_alarmdefinitions">
 <title>Compute Alarms</title>
 <para>
  These alarms show under the Compute section of the &productname; &opscon;.
 </para>
  <section>
   <title>SERVICE: COMPUTE</title>
   <informaltable colsep="1" rowsep="1">
    <tgroup cols="2">
     <colspec colname="c1" colnum="1" colwidth="30*"/>
     <colspec colname="c2" colnum="2" colwidth="70*"/>
     <thead>
      <row>
       <entry>Alarm Info</entry>
       <entry>Mitigation Tasks</entry>
      </row>
     </thead>
     <tbody>
      <row>
       <entry>
        <para>
         <emphasis role="bold">Name:</emphasis> HTTP Status
        </para>
        <para>
         <emphasis role="bold">Description:</emphasis> This is a <literal>nova-api</literal> health check.
        </para>
        <para>
         <emphasis role="bold">Likely cause:</emphasis> Process crashed.
        </para>
       </entry>
       <entry>Restart the <literal>nova-api</literal> process on the affected
     node. Review the <literal>nova-api.log</literal> files. Try to connect
     locally to the http port that is found in the dimension field of the alarm
     to see if the connection is accepted.</entry>
      </row>
      <row>
       <entry>
        <para>
         <emphasis role="bold">Name:</emphasis> Host Status
        </para>
        <para>
         <emphasis role="bold">Description:</emphasis>: Alarms when the specified host is down or not reachable.
        </para>
        <para>
         <emphasis role="bold">Likely cause:</emphasis> The host is down, has been rebooted, or has network
         connectivity issues.
        </para>
       </entry>
       <entry>If it is a single host, attempt to restart the system. If it is
     multiple hosts, investigate networking issues.</entry>
      </row>
      <row>
       <entry>
        <para>
         <emphasis role="bold">Name:</emphasis> Process Bound Check
        </para>
        <para>
         <emphasis role="bold">Description:</emphasis>: <literal>process_name=nova-api</literal> This alarm
         checks that the number of processes found is in a predefined range.
        </para>
        <para>
         <emphasis role="bold">Likely cause:</emphasis> Process crashed or too many processes running
        </para>
       </entry>
       <entry>Stop all the processes and restart the nova-api process on the
       affected host.  Review the system and nova-api logs.</entry>
      </row>
      <row>
       <entry>
        <para>
         <emphasis role="bold">Name:</emphasis> Process Check
        </para>
        <para>
         <emphasis role="bold">Description:</emphasis>: Separate alarms for each of these Nova services,
         specified by the <literal>component</literal> dimension:
        </para>
        <itemizedlist>
         <listitem>
          <para>
           nova-api
          </para>
         </listitem>
         <listitem>
          <para>
           nova-cert
          </para>
         </listitem>
         <listitem>
          <para>
           nova-compute
          </para>
         </listitem>
         <listitem>
          <para>
           nova-consoleauth
          </para>
         </listitem>
         <listitem>
          <para>
           nova-conductor
          </para>
         </listitem>
         <listitem>
          <para>
           nova-scheduler
          </para>
         </listitem>
         <listitem>
          <para>
           nova-novncproxy
          </para>
         </listitem>
        </itemizedlist>
        <para>
         <emphasis role="bold">Likely cause:</emphasis> Process specified by the <literal>component</literal>
         dimension has crashed on the host specified by the
         <literal>hostname</literal> dimension.
        </para>
       </entry>
       <entry>
        <para>
         Restart the process on the affected node using these steps:
        </para>
        <procedure>
         <step>
          <para>
           Log in to the &clm;.
          </para>
         </step>
         <step>
          <para>
           Use the Nova start playbook against the affected node:
          </para>
<screen>cd ~/scratch/ansible/next/ardana/ansible
ansible-playbook -i hosts/verb_hosts nova-start.yml \
--limit &lt;hostname&gt;</screen>
         </step>
        </procedure>
        <para>
         Review the associated logs. The logs will be in the format of
         <literal>&lt;service&gt;.log</literal>, such as
         <literal>nova-compute.log</literal> or
         <literal>nova-scheduler.log</literal>.
        </para>
       </entry>
      </row>
      <row>
       <entry>
        <para>
         <emphasis role="bold">Name:</emphasis> nova.heartbeat
        </para>
        <para>
         <emphasis role="bold">Description:</emphasis>: Check that all services are sending heartbeats.
        </para>
        <para>
         <emphasis role="bold">Likely cause:</emphasis> Process for service specified in the alarm has crashed
         or is hung and not reporting its status to the database. Alternatively
         it may be the service is fine but an issue with messaging or the
         database which means the status is not being updated correctly.
        </para>
       </entry>
       <entry>Restart the affected service. If the service is reporting OK the
     issue may be with RabbitMQ or MySQL. In that case, check the alarms for
     those services.</entry>
      </row>
      <row>
       <entry>
        <para>
         <emphasis role="bold">Name:</emphasis> Service Log Directory Size
        </para>
        <para>
         <emphasis role="bold">Description:</emphasis>: Service log directory consuming more disk than its quota.
        </para>
        <para>
         <emphasis role="bold">Likely cause:</emphasis> This could be due to a service set to
         <literal>DEBUG</literal> instead of <literal>INFO</literal> level.
         Another reason could be due to a repeating error message filling up
         the log files. Finally, it could be due to log rotate not configured
         properly so old log files are not being deleted properly.
        </para>
       </entry>
       <entry>Find the service that is consuming too much disk space. Look at the
     logs. If <literal>DEBUG</literal> log entries exist, set the logging level
     to <literal>INFO</literal>. If the logs are repeatedly logging an error
     message, do what is needed to resolve the error. If old log files exist,
     configure log rotate to remove them. You could also choose to remove old
     log files by hand after backing them up if needed.</entry>
      </row>
     </tbody>
    </tgroup>
   </informaltable>
  </section>
  <section>
   <title>SERVICE: IMAGE-SERVICE</title>
   <informaltable colsep="1" rowsep="1">
    <tgroup cols="2">
     <colspec colname="c1" colnum="1" colwidth="30*"/>
     <colspec colname="c2" colnum="2" colwidth="70*"/>
     <thead>
      <row>
       <entry>Alarm Info</entry>
       <entry>Mitigation Tasks</entry>
      </row>
     </thead>
     <tbody>
      <row>
       <entry>
        <para>
         <emphasis role="bold">Name:</emphasis> HTTP Status
        </para>
        <para>
         <emphasis role="bold">Description:</emphasis>: Separate alarms for
         each of these Glance services, specified by the
         <literal>component</literal> dimension:
        </para>
        <itemizedlist>
        <listitem>
         <para>
          glance-api
         </para>
        </listitem>
        <listitem>
         <para>
          glance-registry
         </para>
        </listitem>
       </itemizedlist>
        <para>
         <emphasis role="bold">Likely cause:</emphasis> API is unresponsive.
        </para>
       </entry>
       <entry>
        <para>
         Restart the process on the affected node using these steps:
        </para>
        <procedure>
        <step>
         <para>
          Log in to the &clm;.
         </para>
        </step>
        <step>
         <para>
          Use the Glance start playbook against the affected node:
         </para>
         <screen>&prompt.ardana;cd ~/scratch/ansible/next/ardana/ansible
&prompt.ardana;ansible-playbook -i hosts/verb_hosts glance-start.yml \
--limit &lt;hostname&gt;</screen>
        </step>
        </procedure>
       <para>
        Review the associated logs.
       </para></entry>
      </row>
      <row>
       <entry>
        <para>
         <emphasis role="bold">Name:</emphasis> Service Log Directory Size
        </para>
        <para>
         <emphasis role="bold">Description:</emphasis>: Service log directory consuming more disk than its quota.
        </para>
        <para>
         <emphasis role="bold">Likely cause:</emphasis> This could be due to a service set to
         <literal>DEBUG</literal> instead of <literal>INFO</literal>
         level. Another reason could be due to a repeating error message
         filling up the log files. Finally, it could be due to log rotate not
         configured properly so old log files are not being deleted properly.
        </para>
       </entry>
       <entry>Find the service that is consuming too much disk space. Look at
       the logs. If <literal>DEBUG</literal> log entries exist, set the logging
       level to <literal>INFO</literal>. If the logs are repeatedly logging an
       error message, do what is needed to resolve the error. If old log files
       exist, configure log rotate to remove them. You could also choose to
       remove old log files by hand after backing them up if needed.</entry>
      </row>
     </tbody>
    </tgroup>
   </informaltable>
  </section>
  <section>
   <title>SERVICE: BAREMETAL</title>
   <informaltable colsep="1" rowsep="1">
    <tgroup cols="2">
     <colspec colname="c1" colnum="1" colwidth="30*"/>
     <colspec colname="c2" colnum="2" colwidth="70*"/>
     <thead>
      <row>
       <entry>Alarm Info</entry>
       <entry>Mitigation Tasks</entry>
      </row>
     </thead>
     <tbody>
      <row>
       <entry>
        <para>
         <emphasis role="bold">Name:</emphasis> Process Check
        </para>
        <para>
         <emphasis role="bold">Description:</emphasis> Alarms when the
         specified process is not running: <literal>process_name = ironic-api</literal>
        </para>
        <para>
         <emphasis role="bold">Likely cause:</emphasis> The Ironic API is unresponsive.
        </para>
       </entry>
       <entry>
        <para>
        Restart the <literal>ironic-api</literal> process with these steps:
       </para>
       <procedure>
        <step>
         <para>
          Log in to the affected host via SSH.
         </para>
        </step>
        <step>
         <para>
          Restart the <literal>ironic-api</literal> process with this command:
         </para>
<screen>sudo service ironic-api restart</screen>
        </step>
       </procedure>
       </entry>
      </row>
      <row>
       <entry>
        <para>
         <emphasis role="bold">Name:</emphasis> Process Check
        </para>
        <para>
         <emphasis role="bold">Description:</emphasis> Alarms when the
         specified process is not running: <literal>process_name = ironic-conductor</literal>
        </para>
        <para>
         <emphasis role="bold">Likely cause:</emphasis> The
         <literal>ironic-conductor</literal> process has crashed.
        </para>
       </entry>
       <entry>
        <para>
        Restart the <literal>ironic-conductor</literal> process with these
        steps:
       </para>
       <procedure>
        <step>
         <para>
          Log in to the &clm;.
         </para>
        </step>
        <step>
         <para>
          Source your <literal>admin</literal> user credentials:
         </para>
<screen>source ~/service.osrc</screen>
        </step>
        <step>
         <para>
          Locate the <literal>messaging_deployer</literal> VM:
         </para>
<screen>openstack server list --all-tenants | grep mess</screen>
        </step>
        <step>
         <para>
          SSH to the <literal>messaging_deployer</literal> VM:
         </para>
<screen>sudo -u ardana ssh &lt;IP_ADDRESS&gt;</screen>
        </step>
        <step>
         <para>
          Stop the <literal>ironic-conductor</literal> process by using this
          playbook:
         </para>
<screen>cd ~/scratch/ansible/next/ardana/ansible
ansible-playbook -i hosts/verb_hosts ironic-stop.yml</screen>
        </step>
        <step>
         <para>
          Start the process back up again, effectively restarting it, by using
          this playbook:
         </para>
<screen>cd ~/scratch/ansible/next/ardana/ansible
ansible-playbook -i hosts/verb_hosts ironic-start.yml</screen>
        </step>
       </procedure>
       </entry>
      </row>
      <row>
       <entry>
        <para>
         <emphasis role="bold">Name:</emphasis> HTTP Status
        </para>
        <para>
         <emphasis role="bold">Description:</emphasis> Alarms when the
         specified HTTP endpoint is down or not reachable.
        </para>
        <para>
         <emphasis role="bold">Likely cause:</emphasis> The API is unresponsive.
        </para>
       </entry>
       <entry>
        <procedure>
        <step>
         <para>
          Log in to the &clm;.
         </para>
         </step>
        <step>
         <para>
          Source your <literal>admin</literal> user credentials:
         </para>
<screen>source ~/service.osrc</screen>
        </step>
        <step>
         <para>
          Locate the <literal>messaging_deployer</literal> VM:
         </para>
<screen>openstack server list --all-tenants | grep mess</screen>
        </step>
        <step>
         <para>
          SSH to the <literal>messaging_deployer</literal> VM:
         </para>
<screen>sudo -u ardana ssh &lt;IP_ADDRESS&gt;</screen>
        </step>
        <step>
         <para>
          Stop the <literal>ironic-api</literal> process by using this
          playbook:
         </para>
<screen>cd ~/scratch/ansible/next/ardana/ansible
ansible-playbook -i hosts/verb_hosts ironic-stop.yml</screen>
        </step>
        <step>
         <para>
          Start the process back up again, effectively restarting it, by using
          this playbook:
         </para>
<screen>cd ~/scratch/ansible/next/ardana/ansible
ansible-playbook -i hosts/verb_hosts ironic-start.yml</screen>
        </step>
        </procedure>
       </entry>
      </row>
      <row>
       <entry>
        <para>
         <emphasis role="bold">Name:</emphasis> Service Log Directory Size
        </para>
        <para>
         <emphasis role="bold">Description:</emphasis> Service log directory
         consuming more disk than its quota.
        </para>
        <para>
         <emphasis role="bold">Likely cause:</emphasis> This could be due to a
         service set to <literal>DEBUG</literal> instead of
         <literal>INFO</literal> level. Another reason could be due to a
         repeating error message filling up the log files. Finally, it could be
         due to log rotate not configured properly so old log files are not
         being deleted properly.
        </para>
       </entry>
       <entry>Find the service that is consuming too much disk space. Look at
       the logs. If <literal>DEBUG</literal> log entries exist, set the logging
       level to <literal>INFO</literal>. If the logs are repeatedly logging an
       error message, do what is needed to resolve the error. If old log files
       exist, configure log rotate to remove them. You could also choose to
       remove old log files by hand after backing them up if needed.</entry>
      </row>
     </tbody>
    </tgroup>
   </informaltable>
  </section>
 </section>
