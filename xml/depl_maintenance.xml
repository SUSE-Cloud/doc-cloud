<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE chapter
[
  <!ENTITY % entities SYSTEM "entity-decl.ent">
  %entities;
]>
<!-- Converted by suse-upgrade version 1.1 -->
<chapter xmlns="http://docbook.org/ns/docbook" xmlns:xi="http://www.w3.org/2001/XInclude" xmlns:xlink="http://www.w3.org/1999/xlink" version="5.0" xml:id="cha.depl.maintenance">
 <title>&cloud; Maintenance</title>
 <info>
<dm:docmanager xmlns:dm="urn:x-suse:ns:docmanager">
    <dm:maintainer>fs</dm:maintainer>
    <dm:status>editing</dm:status>
    <dm:deadline/>
    <dm:priority/>
    <dm:translation>no</dm:translation>
    <dm:languages/>
</dm:docmanager>
</info>
<para/>
 <sect1 xml:id="sec.depl.maintenance.updates">
  <title>Keeping the Nodes Up-to-date</title>

  <para>
   Keeping the nodes in &productname; up-to-date requires an appropriate
   setup of the update and pool repositories  and the deployment of
   either the <guimenu>Updater</guimenu> &barcl; or the &susemgr;
   &barcl;. For details, see
   <xref linkend="sec.depl.adm_conf.repos.scc"/>, <xref
   linkend="sec.depl.inst.nodes.post.updater"/>, and
   <xref linkend="sec.depl.inst.nodes.post.manager"/>.
 </para>

  <para>
   If one of those &barcl;s is deployed, patches are installed on the
   nodes. Installing patches that do not require a reboot of a node does not
   come with any service interruption. If a patch (for example, a kernel
   update) requires a reboot after the installation, services running on the
   machine that is rebooted will not be available within &cloud;.
   Therefore it is strongly recommended to install those patches during a
   maintenance window.
  </para>

  <note>
   <title>No Maintenance Mode</title>
   <para>
    As of &productname; &productnumber; it is not possible to put
    &cloud; into <quote>Maintenance Mode</quote>.
   </para>
  </note>

  <remark condition="clarity">
   2013-10-02 - fs: The following is mainly based on assumptions...
  </remark>

  <variablelist>
   <title>Consequences when Rebooting Nodes</title>
   <varlistentry>
    <term>&admserv;</term>
    <listitem>
     <para>
      While the &admserv; is offline, it is not possible to deploy new
      nodes. However, rebooting the &admserv; has no effect on starting
      &vmguest;s or on &vmguest;s already running.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>&contrnode;s</term>
    <listitem>
     <para>
      The consequences a reboot of a &contrnode; has, depends on the
      services running on that node:
     </para>
     <formalpara>
      <title>Database, &o_ident;, RabbitMQ, &o_img;, &o_comp;:</title>
      <para>
       No new &vmguest;s can be started.
      </para>
     </formalpara>
     <formalpara>
      <title>&o_objstore;:</title>
      <para>
       No object storage data is available. If &o_img; uses
       &o_objstore;, it will not be possible to start new &vmguest;s.
      </para>
     </formalpara>
     <formalpara>
      <title>&o_blockstore;, &ceph;:</title>
      <para>
       No block storage data is available.
      </para>
     </formalpara>
     <formalpara>
      <title>&o_netw;:</title>
      <para>
       No new &vmguest;s can be started. On running &vmguest;s the
       network will be unavailable.
      </para>
     </formalpara>
     <formalpara>
      <title>&o_dash;</title>
      <para>
       &o_dash; will be unavailable. Starting and managing &vmguest;s
       can be done with the command line tools.
      </para>
     </formalpara>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>&compnode;s</term>
    <listitem>
     <para>
      <remark condition="clarity">
       2013-10-02 - fs: How to ensure no new instances are started on a compute
       node while evacuating it? What about Windows compute nodes? What about
       VMware?
      </remark>
      Whenever a &compnode; is rebooted, all &vmguest;s running on
      that particular node will be shut down and must be manually restarted.
      Therefore it is recommended to <quote>evacuate</quote> the node by
      migrating &vmguest;s to another node, before rebooting it.
<!-- (see ???
      for details) -->
     </para>
    </listitem>
   </varlistentry>
  </variablelist>
 </sect1>

 <sect1 xml:id="sec.depl.maintenance.service_order">
  <title>Service Order on &cloud; Start-up or Shutdown</title>
  <para>
   In case you need to restart your complete &cloud; (after a complete shut
   down or a power outage), the nodes and services need to be started in the following
   order:
  </para>
  <orderedlist>
   <title>Service Order on Start-up</title>
   <listitem>
    <para>
     &contrnode;/Cluster on which the Database is deployed
    </para>
   </listitem>
   <listitem>
    <para>
     &contrnode;/Cluster on which RabbitMQ is deployed
    </para>
   </listitem>
   <listitem>
    <para>
     &contrnode;/Cluster on which &o_ident; is deployed
    </para>
   </listitem>
   <listitem>
    <para>For &o_objstore;:</para>
    <orderedlist>
     <listitem>
      <para>
       &stornode; on which the <literal>swift-storage</literal> role is deployed
      </para>
     </listitem>
     <listitem>
      <para>
       &stornode; on which the <literal>swift-proxy</literal> role is deployed
      </para>
     </listitem>
    </orderedlist>
   </listitem>
   <listitem>
    <para>For &ceph;:</para>
    <orderedlist>
     <listitem>
      <para>
       &stornode; on which the <literal>ceph-mon</literal> role is deployed
      </para>
     </listitem>
     <listitem>
      <para>
       &stornode; on which the <literal>ceph-osd</literal> role is deployed
      </para>
     </listitem>
     <listitem>
      <para>
       &stornode; on which the <literal>ceph-radosgw</literal> and
       <literal>ceph-mds</literal> roles are deployed (if deployed on different
       nodes: in either order)
      </para>
     </listitem>
    </orderedlist>
   </listitem>
   <listitem>
    <para>
     Any remaining &contrnode;/Cluster. The following additional rules apply:
    </para>
    <itemizedlist>
     <listitem>
      <para>
       The &contrnode;/Cluster on which the <literal>neutron-server</literal>
       role is deployed needs to be started before starting the node/cluster
       on which the <literal>neutron-l3</literal> role is deployed.
      </para>
     </listitem>
     <listitem>
      <para>
       The &contrnode;/Cluster on which the <literal>nova-controller</literal>
       role is deployed needs to be started before starting the node/cluster
       on which &o_orch; is deployed.
      </para>
     </listitem>
    </itemizedlist>
   </listitem>
   <listitem>
    <para>
     &compnode;s
    </para>
   </listitem>
  </orderedlist>
  <para>
   If multiple roles are deployed on a single &contrnode;, the services are
   automatically started in the correct order on that node. If you have more
   than one node with multiple roles, make sure they are
   started as closely as possible to the order listed above.
  </para>
  <para>
   If you need to shut down &cloud;, the nodes and services need to be terminated in
   reverse order than on start-up:
  </para>

  <orderedlist>
   <title>Service Order on Shut-down</title>
   <listitem>
    <para>
     &compnode;s
    </para>
   </listitem>
   <listitem>
    <para>
     &contrnode;/Cluster on which &o_orch; is deployed
    </para>
   </listitem>
   <listitem>
    <para>
     &contrnode;/Cluster on which the <literal>nova-controller</literal>
     role is deployed
    </para>
   </listitem>
   <listitem>
    <para>
     &contrnode;/Cluster on which the <literal>neutron-l3</literal>
     role is deployed
    </para>
   </listitem>
   <listitem>
    <para>
     All &contrnode;(s)/Cluster(s) on which neither of the following services
     is deployed: Database, RabbitMQ, and &o_ident;.
    </para>
   </listitem>
   <listitem>
    <para>For &o_objstore;:</para>
    <orderedlist>
     <listitem>
      <para>
       &stornode; on which the <literal>swift-proxy</literal> role is deployed
      </para>
     </listitem>
     <listitem>
      <para>
       &stornode; on which the <literal>swift-storage</literal> role is deployed
      </para>
     </listitem>
    </orderedlist>
   </listitem>
   <listitem>
    <para>For &ceph;:</para>
    <orderedlist>
     <listitem>
      <para>
       &stornode; on which the <literal>ceph-radosgw</literal> and
       <literal>ceph-mds</literal> roles are deployed (if deployed on different
       nodes: in either order)
      </para>
     </listitem>
     <listitem>
      <para>
       &stornode; on which the <literal>ceph-osd</literal> role is deployed
      </para>
     </listitem>
     <listitem>
      <para>
       &stornode; on which the <literal>ceph-mon</literal> role is deployed
      </para>
     </listitem>
    </orderedlist>
   </listitem>
   <listitem>
    <para>
     &contrnode;/Cluster on which &o_ident; is deployed
    </para>
   </listitem>
   <listitem>
    <para>
     &contrnode;/Cluster on which RabbitMQ is deployed
    </para>
   </listitem>
   <listitem>
    <para>
     &contrnode;/Cluster on which the Database is deployed
    </para>
   </listitem>
  </orderedlist>
 </sect1>

 <sect1 xml:id="sec.depl.maintenance.upgrade">
  <title>Upgrading from &productname; 6 to &productname; 7</title>

  <para>
   Upgrading from &productname; 6 to &productname; 7 can either be done via a
   &wi; or from the command line. Starting with &productname; 7, a
   <quote>non-disruptive</quote> update is supported, when the requirements
   listed at <xref linkend="list.depl.maintenance.upgrade.non-disruptive"/> are
   met. The non-disruptive upgrade guarantees a fully functional &cloud;
   operation during the upgrade procedure. The only feature that is not
   supported during the non-disruptive upgrade procedure is the deployment of
   additional nodes.
  </para>
  <para>
   If the requirements for a non-disruptive upgarde are not met, the
   upgrade procedure will be done in <quote>normal mode</quote>. When
   live-migration is set up, &vmguest;s will be migrated to another node,
   before the respective &compnode; will get updated to ensure continuous
   operation. However, you will not be able to access &vmguest;s during the
   upgrade of the &contrnode;s.
  </para>

  <sect2 xml:id="sec.depl.maintenance.upgrade.require">
   <title>Requirements</title>
   <para>
    When starting the upgrade process, several checks are performed to
    determine whether the &cloud; is in an upgradeable state and whether a
    non-disruptive update would be supported:
   </para>
   <itemizedlist>
    <title>General Upgrade Requirements</title>
    <listitem>
     <para>
      All nodes need to have the latest &productname; 6 updates
      <emphasis>and</emphasis> the latest &cloudos; updates installed. If this
      is not the case, refer to <xref
      linkend="sec.depl.inst.nodes.post.updater"/> for instructions on how to
      update.
     </para>
    </listitem>
    <listitem>
     <para>
      All allocated nodes need to be turned on and have to be in state
      <quote>ready</quote>.
     </para>
    </listitem>
    <listitem>
     <para>
      All &barcl; proposals need to have been successfully deployed. In case a
      proposal is in state <quote>failed</quote>, the upgrade procedure will
      refuse to start. Fix the issue or&mdash;if possible&mdash;remove the
      proposal.
     </para>
    </listitem>
    <listitem>
     <para>
      In case the pacemaker &barcl; is deployed, all clusters need to be in a
      healthy state.
     </para>
    </listitem>
    <listitem>
     <para>
      <remark condition="clarity">
       2017-02-02 - fs: Is this still true?
      </remark>
      The <literal>dns-server</literal> role must be applied to the &admserv;.
     </para>
    </listitem>
    <listitem>
     <para>
      The following repositories need to be available on a server that is
      accessible from the &admserv;. The HA repositories are only needed if you
      have an &hasetup;. It is recommended to use the same server
      that also hosts the respective repositories of the current version.
     </para>
     <remark condition="clarity">
      2017-02-02 - fs: What about SES-4 ?
     </remark>
     <simplelist>
      <member>&cloud_repo;-Pool</member>
      <member>&cloud_repo;-Update</member>
      <member>&sle_repo;-Pool</member>
      <member>&sle_repo;-Update</member>
      <member>&sleha_repo;-Pool (for &hasetup;s only)</member>
      <member>&sleha_repo;-Update (for &hasetup;s only)</member>
     </simplelist>
     <para>
      Do not add these repositories to the &cloud; repository configuration,
      yet. This needs to be done during the upgrade procedure.
     </para>
    </listitem>
    <listitem>
     <para>
      Hybrid authentication with &o_ident; is not supported during upgrade. If
      you have configured this authentication method (see <citetitle>Using
      Hybrid Authentication</citetitle> in the &productname; 6 Deplyoment
      Guide), you need to revert this setting prior to starting the upgrade. To
      do so, open the &o_ident; &barcl; <guimenu>Attribute</guimenu>
      configuration in <guimenu>Raw</guimenu> mode. Set the identity and
      assignment drivers as follows:
     </para>
     <screen>  "identity": {
    "driver": "sql"
  },
  "assignment": {
    "driver": "sql"
  },</screen>
     <para>
      After the upgrade has finished, re-enable hybrid authentication as
      described in <xref linkend="sec.depl.ostack.keystone.ldap.hybrid"/>.
     </para>
    </listitem>
    <listitem>
     <para>
      &productname; 7 comes with a new version of &o_objstore;. Replica
      handling has changed with this version&mdash;the number of replicas
      cannot be higher than the number of available disks. Check the
      <guimenu>Replica</guimenu> setting in the &o_objstore; &barcl; prior to
      starting the upgrade. If it is higher than the number of available disks,
      adjust it to match the number of disks and redeploy the &barcl;.
     </para>
    </listitem>
    <listitem>
     <para>
      Upgrading is not supported if &o_blockstore; has been deployed with the
      <literal>raw devices</literal> back-end.
     </para>
    </listitem>
   </itemizedlist>

   <itemizedlist xml:id="list.depl.maintenance.upgrade.non-disruptive">
    <title>Non-Disruptive Upgrade Requirements</title>
    <listitem>
     <para>
      All &contrnode;s need to be set up highly available.
     </para>
    </listitem>
    <listitem>
     <para>
      Live-migration support needs to be configured and enabled for the
      &compnode;s. The amount of free ressources (CPU and RAM) on the
      &compnode;s needs to be sufficient to evacuate the nodes one by one.
     </para>
    </listitem>
   </itemizedlist>
  </sect2>

  <sect2 xml:id="sec.depl.maintenance.upgrade.ui">
   <title>Upgrading from the Web Interface</title>
   <para>
    The upgrade procedure via &wi; is done via the upgrade wizard.
   </para>
   <procedure>
    <step>
     <para>
      Open a browser and point it to the upgrade wizard of the &crow; &wi;. It
      is available on the &admserv;, for example
      <literal>http://192.168.124.10/upgrade/</literal>.
     </para>
     <informalfigure>
      <mediaobject>
       <imageobject role="fo">
        <imagedata fileref="depl_upgrade_initial.png" width="100%" format="png"/>
       </imageobject>
       <imageobject role="html">
        <imagedata fileref="depl_upgrade_initial.png" width="100%" format="png"/>
       </imageobject>
      </mediaobject>
     </informalfigure>
    </step>
    <step>
     <para>
      Perform the preliminary checks to determine whether the upgrade
      requirements are met by clicking <guimenu>Check</guimenu> in
      <guimenu>Step 1: Preliminary Checks</guimenu>.
     </para>
     <para>
      The progress is shown in the &wi;. Make sure all checks are passed and
      have a green check mark. If not, make sure to fix the errors and restart
      the <command>Check</command> afterwards. Do not proceed before all checks
      are passed.
     </para>
     <informalfigure>
      <mediaobject>
       <imageobject role="fo">
        <imagedata fileref="depl_upgrade_checks_done.png" width="100%" format="png"/>
       </imageobject>
       <imageobject role="html">
        <imagedata fileref="depl_upgrade_checks_done.png" width="100%" format="png"/>
       </imageobject>
      </mediaobject>
     </informalfigure>
    </step>
    <step>
     <para>
      When all checks in the previous step have passed, <guimenu>Step 2:
      Upgrade Mode</guimenu> shows the result of the upgrade analysis. You are
      informed about whether the upgrade procedure will continue in
      non-disruptive or in normal mode.
     </para>
    </step>
    <step>
     <para>
      To start the upgrade process, click <guimenu>Begin Upgrade</guimenu> in
      <guimenu>Step 3: Begin Upgrade</guimenu>.
     </para>
    </step>
    <step>
     <para>
      After a few minutes in which the upgrade of the &admserv; is prepared,
      the display switches to the upgrade screen and asks you to
      <guimenu>Download the Backup of the &admserv;</guimenu>. Do so and store
      the backup in a safe place. In case something goes wrong during the
      upgrade procedure of the &admserv; you can restore the original state
      from this backup with the command <command>crowbarctl backup restore
      <replaceable>NAME</replaceable></command>.
     </para>
     <informalfigure>
      <mediaobject>
       <imageobject role="fo">
        <imagedata fileref="depl_upgrade_backup.png" width="100%" format="png"/>
       </imageobject>
       <imageobject role="html">
        <imagedata fileref="depl_upgrade_backup.png" width="100%" format="png"/>
       </imageobject>
      </mediaobject>
     </informalfigure>
    </step>
   </procedure>
  </sect2>

  <sect2 xml:id="sec.depl.maintenance.upgrade.cmdl">
   <title>Upgrading from the Command line</title>
   <para>
    The upgrade procedure on the command line is performed by using the program
    <command>crowbarctl</command>. For general help, run <command>crowbarctl
    help</command>. To get help on a certain subcommand, run
    <command>crowbarctl <replaceable>COMMAND</replaceable> help</command>.
   </para>
   <para>
    To review the process of the upgrade procedure, you may call
    <command>crowbarctl upgrade status</command> at any time. Steps may have
    three states: <literal>pending</literal>, <literal>running</literal>, and
    <literal>passed</literal>.
   </para>
   <procedure>
    <step>
     <para>
      To start the upgrade procedure from the command line, log in to the
      &admserv; as &rootuser;.
     </para>
    </step>
    <step>
     <para>
      Perform the preliminary checks to determine whether the upgrade
      requirements are met:
     </para>
     <screen>&prompt.root;crowbarctl upgrade prechecks</screen>
     <para>
      The command's result is shown in a table. Make sure the column
      <guimenu>Errors</guimenu> does not contain any entries. If not, make sure
      to fix the errors and restart the <command>precheck</command> command
      afterwards. Do not proceed before all checks are passed.
     </para>
<screen>&prompt.root;crowbarctl upgrade prechecks
+-------------------------------+--------+----------+--------+------+
| Check ID                      | Passed | Required | Errors | Help |
+-------------------------------+--------+----------+--------+------+
| network_checks                | true   | true     |        |      |
| cloud_healthy                 | true   | true     |        |      |
| maintenance_updates_installed | true   | true     |        |      |
| compute_status                | true   | false    |        |      |
| ha_configured                 | true   | false    |        |      |
| clusters_healthy              | true   | true     |        |      |
+-------------------------------+--------+----------+--------+------+</screen>
     <para>
      Depending on the outcome of the checks, it is automatically decided
      whether the upgrade procedure will continue in non-disruptive or in
      normal mode.
     </para>
    </step>
    <step>
     <para>
      Prepare the nodes by transitioning them into the <quote>upgrade</quote>
      state and stopping the chef daemon:
     </para>
     <screen>&prompt.root;crowbarctl upgrade prepare</screen>
     <para>
      Depending of the size of your &cloud; deployment, this step may take
      some time. Use the command <command>crowbarctl upgrade status</command>
      to monitor the status of the process named
      <literal>steps.prepare.status</literal>. It needs to be in state
      <literal>passed</literal> before you proceed:
     </para>
     <screen>&prompt.root;crowbarctl upgrade status
+--------------------------------+----------------+
| Status                         | Value          |
+--------------------------------+----------------+
| current_step                   | backup_crowbar |
| current_substep                |                |
| current_node                   |                |
| remaining_nodes                |                |
| upgraded_nodes                 |                |
| crowbar_backup                 |                |
| openstack_backup               |                |
| steps.prechecks.status         | passed         |
| steps.prepare.status           | passed         |
| steps.backup_crowbar.status    | pending        |
| steps.repocheck_crowbar.status | pending        |
| steps.admin.status             | pending        |
| steps.database.status          | pending        |
| steps.repocheck_nodes.status   | pending        |
| steps.services.status          | pending        |
| steps.backup_openstack.status  | pending        |
| steps.nodes.status             | pending        |
+--------------------------------+----------------+</screen>
    </step>
    <step>
     <para>
      Create a backup of the existing &admserv; installation. In case something
      goes wrong during the upgrade procedure of the &admserv; you can restore
      the original state from this backup with the command <command>crowbarctl
      backup restore <replaceable>NAME</replaceable></command>
     </para>
     <screen>crowbarctl upgrade backup crowbar</screen>
     <para>
      To list all existing backups including the one you have just created, run
      the following command:
     </para>
     <screen>&prompt.root;crowbarctl backup list
+----------------------------+--------------------------+--------+---------+
| Name                       | Created                  | Size   | Version |
+----------------------------+--------------------------+--------+---------+
| crowbar_upgrade_1486116507 | 2017-02-03T10:08:30.721Z | 209 KB | 3.0     |
+----------------------------+--------------------------+--------+---------+</screen>
    </step>
    <step>
     <para>
      This step prepares the upgrade of the &admserv; by checking the
      availability of the update and pool repositories for &productname;
      &productnumber; and &cloudos;. Run the following command:
     </para>
     <screen>&prompt.root;crowbarctl upgrade repocheck crowbar
+---------------------------------+--------------------------------+
| Status                          | Value                          |
+---------------------------------+--------------------------------+
| os.available                    | false                          |
| os.repos                        | SLES12-SP2-Pool                |
|                                 | SLES12-SP2-Updates             |
| os.errors.x86_64.missing        | SLES12-SP2-Pool                |
|                                 | SLES12-SP2-Updates             |
| openstack.available             | false                          |
| openstack.repos                 | SUSE-OpenStack-Cloud-7-Pool    |
|                                 | SUSE-OpenStack-Cloud-7-Updates |
| openstack.errors.x86_64.missing | SUSE-OpenStack-Cloud-7-Pool    |
|                                 | SUSE-OpenStack-Cloud-7-Updates |
+---------------------------------+--------------------------------+</screen>
     <para>
      All four required repositories are reported as missing, because they have
      not yet been added to the &crow; configuration. To add them to the
      &admserv; proceed as follows.
     </para>
     <para>
      Note that this step is for setting up the repositories for the &admserv;,
      not for the nodes in &cloud; (this will be done in a subsequent step).
     </para>
     <substeps>
      <step>
       <para>
        Start <command>yast repositories</command> and proceed with
        <guimenu>Continue</guimenu>. Replace the repositories
        <literal>SLES12-SP1-Pool</literal> and
        <literal>SLES12-SP1-Updates</literal> with the respective SP2
        repositories.
       </para>
       <para>
        If you prefer to use zypper over &yast;, you may alternatively make the
        change using <command>zypper mr</command>.
       </para>
       <!-- fs 2017-02-03: Does not seem possible to use this one
       <para>
        Note that if you have used the contents of the
        <filename>/srv/tftpboot/suse-12.1/x86_64/install</filename> directory
        as a source for the <literal>SLES12-SP1-Pool</literal>, you may also
        use the same approach now. Create the directory
        <filename>&tftp_dir;/install</filename> and mirror the contents from
        the first &cloudos; installation medium to this directory. See <xref
        linkend="sec.depl.adm_conf.repos.product"/> for instructions. Once
        done, you can use the newly created directory as a source for
        &sle_repo;.
        </para>
        -->
      </step>
      <step>
       <para>
        Next, replace the <literal>SUSE-OpenStack-Cloud-6</literal> update and
        pool repositories with the respective &productname; &productnumber;
        versions.
       </para>
       <!-- fs 2017-02-03: Does not seem possible to use this one
       <para>
        Note that if you have used the contents of the
        <filename>/srv/tftpboot/suse-12.1/x86_64/install</filename> directory
        as a source for the <literal>SLES12-SP1-Pool</literal>, you may also
        use the same approach now. Create the directory
        <filename>&tftp_dir;/repos/Cloud</filename> and mirror the contents
        from the first &productname; &productnumber; installation medium to
        this directory. See <xref linkend="sec.depl.adm_conf.repos.product"/>
        for instructions. Once done, you can use the newly created directory as
        a source for &sle_repo;.
        </para>
        -->
      </step>
     </substeps>
     <para>
      Once the repository configuration on the &admserv; has been updated, run
      the command to check the repositories again. If the configuration is
      correct, the result should look like the following:
     </para>
     <screen>&prompt.root;crowbarctl upgrade repocheck crowbar
+---------------------+--------------------------------+
| Status              | Value                          |
+---------------------+--------------------------------+
| os.available        | true                           |
| os.repos            | SLES12-SP2-Pool                |
|                     | SLES12-SP2-Updates             |
| openstack.available | true                           |
| openstack.repos     | SUSE-OpenStack-Cloud-7-Pool    |
|                     | SUSE-OpenStack-Cloud-7-Updates |
+---------------------+--------------------------------+</screen>
    </step>
    <step>
     <para>
      Now that the repositories are available, the &admserv; itself will be
      upgraded. The update will run in the background using <command>zypper
      dup</command>. Once all packages have been upgraded, the &admserv; will
      be rebooted and you will be logged out. To start the upgrade run:
     </para>
     <screen>&prompt.root;crowbarctl upgrade admin</screen>
    </step>
    <step>
     <para>
      Starting with &productname; 7, &crow; uses a &postgres; database to store
      its data. With this step, the database is created on the
      &admserv;. Alternatively a database on a remote host can be used.
     </para>
     <para>
      To create the database on the &admserv; proceed as follows:
     </para>
     <substeps>
      <step>
       <para>
        Login to the &admserv;.
       </para>
      </step>
      <step>
       <para>
        To create the database on the &admserv; with the default credentials
        (<literal>crowbar</literal>/<literal>crowbar</literal>) for the
        database, run
       </para>
       <screen>&prompt.root;crowbarctl upgrade database new </screen>
       <para>
        To use a different user name and password, run the following command
        instead:
       </para>
       <screen>&prompt.root;crowbarctl upgrade database new \
--db-username=<replaceable>USERNAME</replaceable> --db-password=<replaceable>PASSWORD</replaceable></screen>
      </step>
      <step>
       <para>
        To connect to an existing &postgres; database, use the following
        command rather than creating a new database:
       </para>
    <screen>&prompt.root;crowbarctl upgrade database connect --db-username=<replaceable>USERNAME</replaceable> \
--db-password=<replaceable>PASSWORD</replaceable> --database=<replaceable>DBNAME</replaceable> \
--host=<replaceable>IP_or_FQDN</replaceable> --port=<replaceable>PORT</replaceable></screen>
      </step>
     </substeps>
    </step>
    <step>
     <para>
      After the &admserv; has been successfully updated, the &contrnode;s and
      &compnode;s will be upgraded. At first the availability of the
      repositories used to provide packages for the &cloud; nodes is tested.
     </para>
     <para>
      Note that the configuration for these repositories differs from the one
      for the &admserv; that was already done in a previous step. In this step
      the repository locations are made available to &crow; rather than to
      libzypp on the &admserv;. To check the repository configuration run the
      following command:
     </para>
     <screen>&prompt.root;crowbarctl upgrade repocheck nodes
+---------------------------------+--------------------------------+
| Status                          | Value                          |
+---------------------------------+--------------------------------+
| ha.available                    | false                          |
| ha.repos                        | SLES12-SP2-HA-Pool             |
|                                 | SLES12-SP2-HA-Updates          |
| ha.errors.x86_64.missing        | SLES12-SP2-HA-Pool             |
|                                 | SLES12-SP2-HA- Updates         |
| os.available                    | false                          |
| os.repos                        | SLES12-SP2-Pool                |
|                                 | SLES12-SP2-Updates             |
| os.errors.x86_64.missing        | SLES12-SP2-Pool                |
|                                 | SLES12-SP2-Updates             |
| openstack.available             | false                          |
| openstack.repos                 | SUSE-OpenStack-Cloud-7-Pool    |
|                                 | SUSE-OpenStack-Cloud-7-Updates |
| openstack.errors.x86_64.missing | SUSE-OpenStack-Cloud-7-Pool    |
|                                 | SUSE-OpenStack-Cloud-7-Updates |
+---------------------------------+--------------------------------+</screen>
     <para>
      To update the locations for the listed repositories, start <command>yast
      crowbar</command> and proceed as described in <xref
      linkend="sec.depl.adm_inst.crowbar.repos"/>.
     </para>
     <para>
      Once the repository configuration for &crow; has been updated, run the
      command to check the repositories again to determine, whether the current
      configuration is correct.
     </para>
     <screen>&prompt.root;crowbarctl upgrade repocheck nodes
+---------------------+--------------------------------+
| Status              | Value                          |
+---------------------+--------------------------------+
| ha.available        | true                           |
| ha.repos            | SLE12-SP2-HA-Pool              |
|                     | SLE12-SP2-HA-Updates           |
| os.available        | true                           |
| os.repos            | SLES12-SP2-Pool                |
|                     | SLES12-SP2-Updates             |
| openstack.available | true                           |
| openstack.repos     | SUSE-OpenStack-Cloud-7-Pool    |
|                     | SUSE-OpenStack-Cloud-7-Updates |
+---------------------+--------------------------------+</screen>

     <important>
      <title>Shut Down Running &vmguest;s in Normal Mode</title>
      <para>
       If the upgrade is done in normal mode (prechecks compute_status and
       ha_configured have not been passed), you need to shut down all running
       &vmguest;s now.
      </para>
     </important>

     <important>
      <title>Product Media Repository Copies</title>
      <para>
       To PXE boot new nodes, an additional &cloudos; repository&mdash;a copy
       of the installation syste&mdash; is required. Although not required
       during the upgrade procedure, it is recommended to set up this directory
       now. Refer to <xref linkend="sec.depl.adm_conf.repos.product"/> for
       details. If you had also copied the &productname; 6 installation media
       (optional), you may also want to provide the &productname;
       &productnumber; the same way.
      </para>
      <para>
       Once the upgrade procedure has been successfully finished, you may
       delete the previous copies of the installation media in
       <filename>/srv/tftpboot/suse-12.1/x86_64/install</filename> and
       <filename>/srv/tftpboot/suse-12.1/x86_64/repos/Cloud</filename>.
      </para>
     </important>
    </step>
    <step>
     <para>
      To ensure the status of the nodes does not change during the upgrade
      process, the majority of the &ostack; services will be stopped on the
      nodes now. As a result, the &ostack; API will no longer be
      accessible. The &vmguest;s, however, will continue to run and will also
      be accessible. Run the following command:
     </para>
     <screen>&prompt.root;crowbarctl upgrade services</screen>
     <para>
      This step takes a while to finish. Monitor the process by running
      <command>crowbarctl upgrade status</command>. Do not proceed before
      <literal>steps.services.status</literal> is set to
      <literal>passed</literal>.
     </para>
    </step>
    <step>
     <para>
      The last step before upgrading the nodes is to make a backup of the
      &ostack; &postgres; database. The database dump will be stored on the
      &admserv; and can be used to restore the database in case something goes
      wrong during the upgrade.
     </para>
     <screen>&prompt.root;crowbarctl upgrade backup openstack</screen>
    </step>
    <step>
     <para>
      The final step of the upgrade procedure is upgrading the
      nodes.  To start the process, enter:
     </para>
     <screen>&prompt.root;crowbarctl upgrade nodes all</screen>
     <para>
      The upgrade process runs in the background and can be queried with
      <command>crowbarctl upgrade status</command>. Depending on the size of
      your &cloud; it may take several hours, especially when performing a
      non-disruptive update. In that case, the &compnode;s are updated
      one-by-one after &vmguest;s have been live-migrated to other nodes.
     </para>
     <para>
      Instead of upgrading <literal>all</literal> nodes you may also upgrade
      the &contrnode;s first and individual &compnode;s afterwards. Refer to
      <command>crowbarctl upgrade nodes --help</command> for details.
     </para>
    </step>
   </procedure>
  </sect2>

 </sect1>
 <sect1 xml:id="sec.depl.maintenance.hasetup">
  <title>Upgrading to an &haSetup;</title>

  <para>
   There are a few issues to pay attention to when making an existing &cloud; deployment highly available (by setting
   up HA clusters and moving roles to these clusters). To make existing services highly available, proceed
   as follows. Note that moving to an &hasetup; cannot be done without
   &cloud; service interruption, because it requires &ostack; components
   to be restarted.
  </para>

  <important>
   <title>Teaming Network Mode is Required for HA</title>
   <para>
    Teaming network mode is required for an &hasetup; of &productname;. If
    you are planning to move your cloud to an &hasetup; at a later point in
    time, make sure to deploy &cloud; with teaming network mode from the
    beginning. Otherwise a migration to an &hasetup; is not supported.
   </para>
  </important>

  <procedure>
   <step>
    <para>
     Make sure to have read the sections
     <xref linkend="sec.depl.arch.components.ha"/> and
     <xref linkend="sec.depl.req.ha"/> of this manual and taken any
     appropriate action.
    </para>
   </step>
   <step>
    <para>
     Make the HA repositories available on the &admserv; as described in
     <xref linkend="sec.depl.adm_conf.repos.scc"/>. Run the command
     <command>chef-client</command> afterward.
    </para>
   </step>
   <step>
    <para>
     Set up your cluster(s) as described in
     <xref linkend="sec.depl.ostack.pacemaker"/>.
    </para>
   </step>
   <step>
    <para>
     To move a particular role from a regular control node to a cluster, you
     need to stop the associated service(s) before re-deploying the role on
     a cluster:
    </para>
    <substeps performance="required">
     <step>
      <para>
       Log in to each node as &rootuser; on which the role is deployed and stop
       its associated service(s) (a role can have multiple services). Do so by
       running the service's start/stop script with the stop argument, for
       example:
      </para>
<screen>&prompt.root;rcopenstack-keystone stop</screen>
      <para>
       See <xref linkend="app.deploy.services"/> for a list of roles,
       services and start/stop scripts.
      </para>
     </step>
     <step>
      <para>
       The following roles need additional treatment:
      </para>
      <variablelist>
       <varlistentry>
        <term>database-server (Database &barcl;)</term>
        <listitem>
         <orderedlist spacing="normal">
          <listitem>
           <para>
            Stop the database on the node the Database &barcl; is
            deployed with the command:
           </para>
<screen>&prompt.root;rcpostgresql stop</screen>
          </listitem>
          <listitem>
           <para>
            Copy <filename>/var/lib/pgsql</filename> to a temporary location
            on the node, for example:
           </para>
<screen>&prompt.root;cp -ax /var/lib/pgsql /tmp</screen>
          </listitem>
          <listitem>
           <para>
            Redeploy the Database &barcl; to the cluster. The original
            node may also be part of this cluster.
           </para>
          </listitem>
          <listitem>
           <para>
            Log in to a cluster node and run the following command to
            determine which cluster node runs the
            <systemitem class="resource">postgresql</systemitem> service:
           </para>
<screen>&prompt.root;crm_mon -1</screen>
          </listitem>
          <listitem>
           <para>
            Log in to the cluster node running
            <systemitem class="resource">postgresql</systemitem>.
           </para>
          </listitem>
          <listitem>
           <para>
            Stop the <systemitem class="resource">postgresql</systemitem>
            service:
           </para>
<screen>&prompt.root;crm resource stop postgresql</screen>
          </listitem>
          <listitem>
           <para>
            Copy the data backed up earlier to the cluster node:
           </para>
<screen>&prompt.root;rsync -av --delete
           <replaceable>NODE_WITH_BACKUP</replaceable>:/tmp/pgsql/ /var/lib/pgsql/</screen>
          </listitem>
          <listitem>
           <para>
            Restart the <systemitem class="resource">postgresql</systemitem>
            service:
           </para>
<screen>&prompt.root;crm resource start postgresql</screen>
          </listitem>
         </orderedlist>
         <para>
          Copy the content of <filename>/var/lib/pgsql/data/</filename> from
          the original database node to the cluster node with DRBD or shared
          storage.
         </para>
        </listitem>
       </varlistentry>
       <varlistentry>
        <term>keystone-server (&o_ident; &barcl;)</term>
        <listitem>
         <para>
          If using &o_ident; with PKI tokens, the PKI keys on all nodes
          need to be re-generated. This can be achieved by removing the
          contents of <filename>/var/cache/*/keystone-signing/</filename> on
          the nodes. Use a command similar to the following on the
          &admserv; as &rootuser;:
         </para>
<screen>&prompt.root;for NODE in <replaceable>NODE1</replaceable>
         <replaceable>NODE2</replaceable> <replaceable>NODE3</replaceable>; do
  ssh $NODE rm /var/cache/*/keystone-signing/*
done</screen>
        </listitem>
       </varlistentry>
      </variablelist>
     </step>
    </substeps>
   </step>
   <step>
    <para>
     Go to the &barcl; featuring the role you want to move to the
     cluster. From the left side of the <guimenu>Deployment</guimenu> section,
     remove the node the role is currently running on. Replace it with a
     cluster from the <guimenu>Available Clusters</guimenu> section. Then
     apply the proposal and verify that application succeeded via the
     &crow; &wi;. You can also check the cluster status via &hawk;
     or the <command>crm</command> /
     <command>crm_mon</command> CLI tools.
    </para>
   </step>
   <step>
    <para>
     Repeat these steps for all roles you want to move to cluster. See
     <xref linkend="sec.depl.reg.ha.control.spof"/> for a list of services
     with HA support.
    </para>
   </step>
  </procedure>

  <important>
   <title>SSL Certificates</title>
   <para>
    Moving to an &hasetup; also requires to create SSL certificates for
    nodes in the cluster that run services using SSL. Certificates need to
    be issued for the generated names (see
    <xref linkend="ann.depl.ostack.pacemaker.prop_name"/>) and for all
    public names you have configured in the cluster.
   </para>
  </important>

  <important>
   <title>Service Management on the Cluster</title>
   <para>
    After a role has been deployed on a cluster, its services are managed by
    the HA software. You must <emphasis>never</emphasis> manually start
    or stop an HA-managed service or configure it to start on boot. Services
    may only be started or stopped by using the cluster management tools Hawk
    or the crm shell. See
    <link xlink:href="&suse-onlinedoc;/sle-ha-12/book_sleha/data/sec_ha_config_basics_resources.html"/>
    for more information.
   </para>
  </important>
 </sect1>

 <sect1 xml:id="sec.depl.maintenance.backup.admin">
  <title>Backing Up and Restoring the &admserv;</title>
  <para>
   Backing Up and Restoring the &admserv; can either be done via the &crow;
   &wi; or on the &admserv;'s command line via the <command>crowbarctl
   backup</command> command. Both tools provide the same functionality.
  </para>

  <sect2 xml:id="sec.depl.maintenance.backup.admin.ui">
   <title>Backup and Restore via the &crow; &wi;</title>
   <para>
    To use the Web interface for backing up and restoring the &admserv;, go to
    the &crow; &wi; on the &admserv;, for example
    <literal>http://192.168.124.10/</literal>. Log in as user <systemitem
    class="username">crowbar</systemitem>. The password is
    <literal>crowbar</literal> by default, if you have not changed it. Go to <menuchoice> <guimenu>Utilities</guimenu> <guimenu>Backup &amp; Restore</guimenu>
    </menuchoice>.
   </para>
   <figure>
    <title>Backup and Restore: Initial Page View</title>
    <mediaobject>
     <imageobject role="fo">
      <imagedata fileref="depl_backup_initial.png" width="75%" format="png"/>
     </imageobject>
     <imageobject role="html">
      <imagedata fileref="depl_backup_initial.png" width="100%" format="png"/>
     </imageobject>
    </mediaobject>
   </figure>
   <para>
    To create a backup, click the respective button. Provide a descriptive
    name (allowed characters are letters, numbers, dashes and underscores) and
    confirm with <guimenu>Create Backup</guimenu>. Alternatively, you can
    upload a backup, for example from a previous installation.
   </para>
   <para>
    Existing backups are listed with name and creation date. For each backup,
    three actions are available:
   </para>
   <variablelist>
    <varlistentry>
     <term><guimenu>Download</guimenu></term>
     <listitem>
      <para>
       Download a copy of the backup file. The TAR archive you receive with
       this download can be uploaded again via <guimenu>Upload Backup
       Image</guimenu>.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term><guimenu>Restore</guimenu></term>
     <listitem>
      <para>
       Restore the backup.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term><guimenu>Delete</guimenu></term>
     <listitem>
      <para>
       Delete the backup.
      </para>
     </listitem>
    </varlistentry>
   </variablelist>
   <figure>
    <title>Backup and Restore: List of Backups</title>
    <mediaobject>
     <imageobject role="fo">
      <imagedata fileref="depl_backup_list.png" width="75%" format="png"/>
     </imageobject>
     <imageobject role="html">
      <imagedata fileref="depl_backup_list.png" width="75%" format="png"/>
     </imageobject>
    </mediaobject>
   </figure>

  </sect2>

  <sect2 xml:id="sec.depl.maintenance.backup.admin.cli">
   <title>Backup and Restore via the Command Line</title>
   <para>
    Backing up and restoring the &admserv; from the command line can be done
    with the command <command>crowbarctl backup</command>. For getting general
    help, run the command <command>crowbarctl --help backup</command>, help on
    a subcommand is available by running <command>crowbarctl
    <replaceable>SUBCOMMAND</replaceable> --help</command>.
    The following commands for creating and managing backups exist:
   </para>
   <variablelist>
    <varlistentry>
     <term>
      <command>crowbarctl backup create
      <replaceable>NAME</replaceable></command>
     </term>
     <listitem>
      <para>
       Create a new backup named <replaceable>NAME</replaceable>. It will be
       stored at <filename>/var/lib/crowbar/backup</filename>.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term><command>crowbarctl backup [--yes] <replaceable>NAME</replaceable></command></term>
     <listitem>
      <para>
       Restore the backup named <replaceable>NAME</replaceable>. You will be
       asked for confirmation before any existing proposals will get
       overwritten. If using the option <option>--yes</option>, confirmations
       are tuned off and the restore is forced.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term><command>crowbarctl backup delete <replaceable>NAME</replaceable></command></term>
     <listitem>
      <para>
       Delete the backup named <replaceable>NAME</replaceable>.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term><command>crowbarctl backup download <replaceable>NAME</replaceable>
     <replaceable>[FILE]</replaceable></command></term>
     <listitem>
      <para>
       Download the backup named <replaceable>NAME</replaceable>. If you
       specify the optional <replaceable>[FILE]</replaceable>, the download is
       written to the specified file. Otherwise it is saved to the current
       working directory with an automatically generated file name. If
       specifying <literal>-</literal> for <replaceable>[FILE]</replaceable>,
       the output is written to STDOUT.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term><command>crowbarctl backup list</command></term>
     <listitem>
      <para>
       List existing backups. You can optionally specify different
       output formats and filters&mdash;refer to <command>crowbarctl backup
       list --help</command> for details.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>
      <command>crowbarctl backup upload
      <replaceable>FILE</replaceable></command>
     </term>
     <listitem>
      <para>
       Upload a backup from <replaceable>FILE</replaceable>.
      </para>
     </listitem>
    </varlistentry>
   </variablelist>
  </sect2>
 </sect1>
</chapter>
