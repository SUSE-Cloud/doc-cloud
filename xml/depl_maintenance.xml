<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE chapter 
[
  <!ENTITY % entities SYSTEM "entity-decl.ent">
  %entities;
]>
<!-- Converted by suse-upgrade version 1.1 -->
<chapter xmlns="http://docbook.org/ns/docbook" xmlns:xi="http://www.w3.org/2001/XInclude" xmlns:xlink="http://www.w3.org/1999/xlink" version="5.0" xml:id="cha.depl.maintenance">
 <title>&cloud; Maintenance</title>
 <info>
<dm:docmanager xmlns:dm="urn:x-suse:ns:docmanager">
    <dm:maintainer>fs</dm:maintainer>
    <dm:status>editing</dm:status>
    <dm:deadline/>
    <dm:priority/>
    <dm:translation>no</dm:translation>
    <dm:languages/>
</dm:docmanager>
</info>
<para/>
 <sect1 xml:id="sec.depl.maintenance.updates">
  <title>Keeping the Nodes Up-to-date</title>

  <para>
   Keeping the nodes in &productname; up-to-date requires an appropriate
   setup of the update and pool repositories (see
   <xref linkend="sec.depl.adm_conf.repos.scc"/> and the deployment of
   either the <guimenu>Updater</guimenu> &barcl; (see
   <xref linkend="sec.depl.inst.nodes.post.updater"/>) or the &susemgr;
   &barcl; (see <xref linkend="sec.depl.inst.nodes.post.manager"/>.
  </para>

  <para>
   If one of those &barcl;s is deployed, patches are installed on the
   nodes. Installing patches that do not require a reboot of a node does not
   come with any service interruption. In case a patch (for example a kernel
   update) requires a reboot after the installation, services running on the
   machine that is rebooted, will not be available within &cloud;.
   Therefore it is strongly recommended to install those patches during a
   maintenance window.
  </para>

  <note>
   <title>No Maintenance Mode</title>
   <para>
    As of &productname; &productnumber; it is not possible to put
    &cloud; into <quote>Maintenance Mode</quote>.
   </para>
  </note>

  <remark condition="clarity">
   2013-10-02 - fs: The following is mainly based on assumptions...
  </remark>

  <variablelist>
   <title>Consequences when Rebooting Nodes</title>
   <varlistentry>
    <term>&admserv;</term>
    <listitem>
     <para>
      While the &admserv; is offline, it is not possible to deploy new
      nodes. However, rebooting the &admserv; has no effect on starting
      &vmguest;s or on &vmguest;s already running.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>&contrnode;s</term>
    <listitem>
     <para>
      The consequences a reboot of a &contrnode; has, depends on the
      services running on that node:
     </para>
     <formalpara>
      <title>Database, &o_ident;, RabbitMQ, &o_img;, &o_comp;:</title>
      <para>
       No new &vmguest;s can be started.
      </para>
     </formalpara>
     <formalpara>
      <title>&o_objstore;:</title>
      <para>
       No object storage data is available. If &o_img; uses
       &o_objstore;, it will not be possible to start new &vmguest;s.
      </para>
     </formalpara>
     <formalpara>
      <title>&o_blockstore;, &ceph;:</title>
      <para>
       No block storage data is available.
      </para>
     </formalpara>
     <formalpara>
      <title>&o_netw;:</title>
      <para>
       No new &vmguest;s can be started. On running &vmguest;s the
       network will be unavailable.
      </para>
     </formalpara>
     <formalpara>
      <title>&o_dash;</title>
      <para>
       &o_dash; will be unavailable. Starting and managing &vmguest;s
       can be done with the command line tools.
      </para>
     </formalpara>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>&compnode;s</term>
    <listitem>
     <para>
      <remark condition="clarity">
       2013-10-02 - fs: How to ensure no new instances are started on a compute
       node while evacuating it? What about Windows compute nodes? What about
       VMware?
      </remark>
      Whenever a &compnode; is rebooted, all &vmguest;s running on
      that particular node will be shut down and must be manually restarted.
      Therefore it is recommended to <quote>evacuate</quote> the node by
      migrating &vmguest;s to another node, before rebooting it.
<!-- (see ???
      for details) -->
     </para>
    </listitem>
   </varlistentry>
  </variablelist>
 </sect1>
 <sect1 xml:id="sec.depl.maintenance.upgrade">
  <title>Upgrading from &productname; 4 to &productname; 5</title>

  <para>
   Upgrading from &productname; 4 to &productname; 5 is supported,
   with the following manual steps. However, note that it will turn off the
   &ostack; cloud during the upgrade and it is not possible to downgrade
   back to &productname; 4 in case of a regression or problem. It is
   highly recommended to back up data before the upgrade. In case you also
   plan to make your setup highly available, also refer to
   <xref linkend="sec.depl.maintenance.hasetup"/>.
  </para>

  <itemizedlist mark="bullet" spacing="normal">
   <title>Prerequisites</title>
   <listitem>
    <para>
     all allocated nodes are turned on
    </para>
   </listitem>
   <listitem>
    <para>
     all nodes, including the &admserv;, have the latest &productname;
     4 updates installed. If this is not the case, refer to
     <xref linkend="sec.depl.inst.nodes.post.updater"/> for instructions.
    </para>
   </listitem>
   <listitem>
    <para>
     all instances running in &cloud; must be suspended
    </para>
   </listitem>
  </itemizedlist>

  <important>
   <title>Instance Running on HyperV Nodes will not <quote>survive</quote> an Upgrade</title>
   <para>
    As of &productname; 5, HyperV Nodes need to be re-installed after the
    upgrade procedure. This re-installation will overwrite the instance's
    data and therefore they will be lost. &kvm;, VMware and &xen;
    instances are not affected.
   </para>
  </important>

  <procedure>
   <title>Upgrading &productname; 4 to 5</title>
   <step>
    <para>
     Make sure to subscribe to the SUSE Cloud 5 Pool and Update channels on
     the source that provides these channels for your &cloud;
     installation (for example an &smt; or &susemgr; server).
    </para>
    <para>
     In case the channels are provided by an &smt; server (either remote
     or on the &admserv; itself), refer to
     <xref linkend="app.deploy.smt.repos"/> for instructions on subscribing
     to the &productname; &productnumber; channels.
    </para>
   </step>
   <step>
    <para>
     Update the &productname; product repository on the &admserv;. To
     do so, insert the &productname; 5 DVD1 and run the following
     commands:
    </para>
<screen>mkdir -p /srv/tftpboot/suse-11.3/repos/Cloud
mount /dev/dvd /mnt
rsync -avP --delete /mnt/ /srv/tftpboot/suse-11.3/repos/Cloud/
umount /mnt</screen>
    <important>
     <title>New path for repositories</title>
     <para>
      As of &productname; 5, repositories have been moved from
      <filename>/srv/tftpboot/repos</filename> to
      <filename>/srv/tftpboot/suse-11.3/repos</filename> and
      <filename>/srv/tftpboot/suse-12.0/repos</filename>, to allow
      deployment of both &slsa; 11 SP3 and &slsa; 12 on nodes.
     </para>
    </important>
   </step>
   <step>
    <para>
     &productname; 5 optionally supports deploying &compnode;s with
     &slsa; 12. Deploying &ceph; on &slsa; on &slsa; 11 SP3 is
     no longer supported&mdash;ceph needs to be deployed on dedicated
     nodes running &slsa; 12. To allow deployment of SLES 12 nodes, also
     add the contents of the &slsa; 12 DVD1 and the &productname;
     &productnumber; for SLE12 Compute nodes product repository on the
     &admserv;. Skip this step if you do not plan to deploy SLE 12 nodes.
    </para>
    <substeps performance="required">
     <step>
      <para>
       Copy the contents of &slsa; 12 DVD1 by running the following
       commands on the &admserv;:
      </para>
<screen>mkdir -p /srv/tftpboot/suse-12.0/install
mount /dev/dvd /mnt
rsync -avP /mnt/ /srv/tftpboot/suse-12.0/install/
umount /mnt</screen>
     </step>
     <step>
      <para>
       Copy the contents of the &productname; 5 for SLE12 Compute nodes
       DVD by running the following commands on the &admserv;:
      </para>
<screen>mkdir -p /srv/tftpboot/suse-12.0/repos/SLE12-Cloud-Compute
mount /dev/dvd /mnt
rsync -avP --delete /mnt/ /srv/tftpboot/suse-12.0/repos/SLE12-Cloud-Compute/
umount /mnt</screen>
     </step>
    </substeps>
   </step>
   <step>
    <para>
     Update the &cloud; repository links on the &admserv;.
    </para>
    <para>
     If you are using external URLs (for example from an external &smt;
     or &susemgr; server) for the SUSE Cloud 4 Pool and Update
     repositories, you need to update the file
     <filename>/etc/crowbar/provisioner.json</filename>. Open it in an
     editor and Search for the <menuchoice> <guimenu>suse</guimenu>
     <guimenu>autoyast</guimenu> <guimenu>repos</guimenu></menuchoice>
     entry. Replace the URLs pointing to the &productname; 4 Pool and
     Update repositories with the ones pointing to version 5. Also change
     the name of these repositories to SUSE-Cloud-5-Pool and
     SUSE-Cloud-5-Updates, respectively.
    </para>
   </step>
   <step>
    <para>
     Now you need to replace the &productname; 4 Pool and Update
     repositories used by the &admserv; with the ones for version 5.
     Modify the repository configuration by running <menuchoice>
     <guimenu>&yast;</guimenu> <guimenu>Software</guimenu>
     <guimenu>Software Repositories</guimenu></menuchoice> on the
     &admserv;.
    </para>
   </step>
   <step>
    <para>
     Refresh the repositories and install the
     <systemitem class="resource">suse-cloud-upgrade</systemitem> package by
     running the following commands on the &admserv;:
    </para>
<screen>zypper refresh
zypper install suse-cloud-upgrade</screen>
   </step>
   <step>
    <para>
     Run <command>suse-cloud-upgrade</command> <option>upgrade</option> for
     the first time. It will completely shut down the &ostack; and the
     &crow; Web interface. In case the command fails, contact the
     &suse; support (see <xref linkend="sec.depl.trouble.support"/> for
     more information). At this point of the upgrade workflow it is still
     possible to go back to version 4.
    </para>
   </step>
   <step>
    <para>
     Update all packages on the &admserv; by running
     <command>zypper</command> <option>up</option>. During the update
     process you will need to accept the &productname; 5 EULA.
    </para>
   </step>
   <step>
    <para>
     Run <command>suse-cloud-upgrade</command> <option>upgrade</option> for
     the second time. Now the &crow; Web interface is available again. In
     case the command fails, contact the &suse; support (see
     <xref linkend="sec.depl.trouble.support"/> for more information). From
     this point in the installation workflow on it is no longer possible to
     go back to version 4.
    </para>
   </step>
   <step>
    <para>
     If using a &smt; Server installed on the &admserv;, double-check
     that the following repositories are available in
     <filename>/srv/tftpboot/suse-11.3/repos</filename>:
    </para>
    <itemizedlist mark="bullet" spacing="normal">
     <listitem>
      <para>
       SLES11-SP3-Pool
      </para>
     </listitem>
     <listitem>
      <para>
       SLES11-SP3-Updates
      </para>
     </listitem>
     <listitem>
      <para>
       SUSE-Cloud-5-Pool
      </para>
     </listitem>
     <listitem>
      <para>
       SUSE-Cloud-5-Updates
      </para>
     </listitem>
     <listitem>
      <para>
       SLES11-SP3-HAE-Pool (optional)
      </para>
     </listitem>
     <listitem>
      <para>
       SLES11-SP3-HAE-Updates (optional)
      </para>
     </listitem>
    </itemizedlist>
    <para>
     If they are missing, create symbolic links from the SMT mirror. For
     instance:
    </para>
<screen><?dbsuse-fo font-size="0.63em"?>ln -s /srv/www/htdocs/repo/\$RCE/SLES11-SP3-Pool/sle-11-x86_64 /srv/tftpboot/suse-11.3/repos/SLES11-SP3-Pool</screen>
    <para>
     If the &slsa; 12 repositories are also mirrored, the following
     repositories should be available in
     <filename>/srv/tftpboot/suse-12.0/repos</filename>:
    </para>
    <itemizedlist mark="bullet" spacing="normal">
     <listitem>
      <para>
       SLES12-Pool
      </para>
     </listitem>
     <listitem>
      <para>
       SLES12-Updates
      </para>
     </listitem>
     <listitem>
      <para>
       SLE-12-Cloud-Compute5-Pool
      </para>
     </listitem>
     <listitem>
      <para>
       SLE-12-Cloud-Compute5-Updates
      </para>
     </listitem>
    </itemizedlist>
    <para>
     in case &ceph; should be deployed, also add the following
     repositories:
    </para>
    <itemizedlist mark="bullet" spacing="normal">
     <listitem>
      <para>
       SUSE-Enterprise-Storage-1.0-Pool (optional)
      </para>
     </listitem>
     <listitem>
      <para>
       SUSE-Enterprise-Storage-1.0-Updates (optional)
      </para>
     </listitem>
    </itemizedlist>
   </step>
   <step>
    <para>
     Go to the &crow; Web interface and open <menuchoice>
     <guimenu>Barclamps</guimenu> <guimenu>Crowbar</guimenu> </menuchoice>.
     <guimenu>Edit</guimenu> the <guimenu>Provisioner</guimenu> proposal and
     <guimenu>Apply</guimenu> it.
    </para>
   </step>
   <step>
    <para>
     Update packages on all nodes except the &admserv; as described in
     <xref linkend="sec.depl.inst.nodes.post.updater"/>. You should use the
     following configuration:
    </para>
    <simplelist>
     <member><guimenu>Use zypper</guimenu>: update</member>
     <member><guimenu>Enable GPG checks</guimenu>: true</member>
     <member><guimenu>Automatically agree with licenses</guimenu>: true</member>
    </simplelist>
   </step>
   <step>
    <para>
     Reboot all nodes except the &admserv;. This
     <emphasis>must</emphasis> be done by running
     <command>suse-cloud-upgrade reboot-nodes</command>. Wait until all
     nodes are up and running again before proceeding.
    </para>
   </step>
   <step>
    <para>
     Go to the &crow; Web interface and open <menuchoice>
     <guimenu>Barclamps</guimenu> <guimenu>Openstack</guimenu>
     </menuchoice>. Open the &barcl; one-by-one in the given order by
     clicking <guimenu>Edit</guimenu> and re-apply each proposal with
     <guimenu>Apply</guimenu>.
    </para>
   </step>
   <step>
    <para>
     In case you have deployed Microsoft HyperV nodes, you need to
     re-install them. Go to the &crow; Web interface and open the
     <guimenu>Dashboard</guimenu>. For each HyperV node, click its name and
     then on <guimenu>Reinstall</guimenu>.
    </para>
   </step>
  </procedure>

  <para>
   After the upgrade procedure has been finished, &cloud; should be up
   and running again. You may restart all suspended instances now.
  </para>

  <tip>
   <title>Log Files of the Upgrade Procedure</title>
   <para>
    The <command>suse-cloud-upgrade</command> script writes log files on the
    &admserv; that are located at
    <filename>/var/log/crowbar/upgrade/</filename>. In case of problems with
    the upgrade, check the log files.
   </para>
  </tip>
 </sect1>
 <sect1 xml:id="sec.depl.maintenance.hasetup">
  <title>Upgrading to an &haSetup;</title>

  <para>
   When making an existing &cloud; deployment highly available by setting
   up HA clusters and moving roles to these clusters, there are a few issues
   to pay attention to. To make existing services highly available, proceed
   as follows. Note that moving to an &hasetup; cannot be done without
   &cloud; service interruption, because it requires &ostack; services
   to be restarted.
  </para>

  <important>
   <title>Teaming Network Mode is Required for HA</title>
   <para>
    Teaming network mode is required for an &hasetup; of &productname;. If
    you are planning to move your cloud to an &hasetup; at a later point in
    time, make sure to deploy &cloud; with teaming network mode from the
    beginning. Otherwise a migration to an &hasetup; is not supported.
   </para>
  </important>
  
  <procedure>
   <step>
    <para>
     Make sure to have read the sections
     <xref linkend="sec.depl.arch.components.ha"/> and
     <xref linkend="sec.depl.req.ha"/> of this manual and taken any
     appropriate action.
    </para>
   </step>
   <step>
    <para>
     Make the HA repositories available on the &admserv; as described in
     <xref linkend="cha.depl.adm_conf"/>. Run the command
     <command>chef-client</command> afterwards.
    </para>
   </step>
   <step>
    <para>
     Set up your cluster(s) as described in
     <xref linkend="sec.depl.ostack.pacemaker"/>.
    </para>
   </step>
   <step>
    <para>
     To move a particular role from a regular control node to a cluster, you
     need to stop the associated service(s) before re-deploying the role on
     a cluster:
    </para>
    <substeps performance="required">
     <step>
      <para>
       Log in to each node on which the role is deployed and stop its
       associated service(s) (a role can have multiple services). Do so by
       running the service's start/stop script with the stop argument, for
       example:
      </para>
<screen>rcopenstack-keystone stop</screen>
      <para>
       See <xref linkend="app.deploy.services"/> for a list of roles,
       services and start/stop scripts.
      </para>
     </step>
     <step>
      <para>
       The following roles need additional treatment:
      </para>
      <variablelist>
       <varlistentry>
        <term>database-server (Database &barcl;)</term>
        <listitem>
         <orderedlist spacing="normal">
          <listitem>
           <para>
            Stop the database on the node the Database &barcl; is
            deployed with the command:
           </para>
<screen>rcpostgresql stop</screen>
          </listitem>
          <listitem>
           <para>
            Copy <filename>/var/lib/pgsql</filename> to a temporary location
            on the node, for example:
           </para>
<screen>cp -ax /var/lib/pgsql /tmp</screen>
          </listitem>
          <listitem>
           <para>
            Redeploy the Database &barcl; to the cluster. The original
            node may also be part of this cluster.
           </para>
          </listitem>
          <listitem>
           <para>
            Log in to a cluster node and run the following command to
            determine which cluster node runs the
            <systemitem class="resource">postgresql</systemitem> service:
           </para>
<screen>crm_mon -1</screen>
          </listitem>
          <listitem>
           <para>
            Log in to the cluster node running
            <systemitem class="resource">postgresql</systemitem>.
           </para>
          </listitem>
          <listitem>
           <para>
            Stop the <systemitem class="resource">postgresql</systemitem>
            service:
           </para>
<screen>crm resource stop postgresql</screen>
          </listitem>
          <listitem>
           <para>
            Copy the data backed up earlier to the cluster node:
           </para>
<screen>rsync -av --delete
	   <replaceable>NODE_WITH_BACKUP</replaceable>:/tmp/pgsql/ /var/lib/pgsql/</screen>
          </listitem>
          <listitem>
           <para>
            Restart the <systemitem class="resource">postgresql</systemitem>
            service:
           </para>
<screen>crm resource start postgresql</screen>
          </listitem>
         </orderedlist>
         <para>
          Copy the content of <filename>/var/lib/pgsql/data/</filename> from
          the original database node to the cluster node with DRBD or shared
          storage.
         </para>
        </listitem>
       </varlistentry>
       <varlistentry>
        <term>keystone-server (&o_ident; &barcl;)</term>
        <listitem>
         <para>
          If using &o_ident; with PKI tokens, the PKI keys on all nodes
          need to be re-generated. This can be achieved by removing the
          contents of <filename>/var/cache/*/keystone-signing/</filename> on
          the nodes. Use a command similar to the following on the
          &admserv; as &rootuser;:
         </para>
<screen>for NODE in <replaceable>NODE1</replaceable>
	 <replaceable>NODE2</replaceable> <replaceable>NODE3</replaceable>; do
  ssh $NODE rm /var/cache/*/keystone-signing/*
done</screen>
        </listitem>
       </varlistentry>
      </variablelist>
     </step>
    </substeps>
   </step>
   <step>
    <para>
     Go to the &barcl; featuring the role you want to move to the
     cluster. Remove the node the role is currently running on from the left
     side of the <guimenu>Deployment</guimenu> section and replace it with a
     cluster from the <guimenu>Available Clusters</guimenu> section. Then
     apply the proposal and verify that application succeeded via the
     Crowbar Web UI. You can also check the cluster status via &hawk;,
     the &hbgui; (hb_gui), or the <command>crm</command> /
     <command>crm_mon</command> CLI tools.
    </para>
   </step>
   <step>
    <para>
     Repeat these steps for all roles you want to move to cluster. See
     <xref linkend="sec.depl.reg.ha.control.spof"/> for a list of services
     with HA support.
    </para>
   </step>
  </procedure>

  <important>
   <title>SSL Certificates</title>
   <para>
    Moving to an &hasetup; also requires to create SSL certificates for
    nodes in the cluster that run services using SSL. Certificates need to
    be issued for the generated names (see
    <xref linkend="ann.depl.ostack.pacemaker.prop_name"/>) and for all
    public names you have configured in the cluster.
   </para>
  </important>
  
  <important>
   <title>Service Management on the Cluster</title>
   <para>
    After a role has been deployed on a cluster, its services are managed by
    the HA software. You must <emphasis>never ever</emphasis> manually start
    or stop an HA-managed service or configure it to start on boot (for
    example by running <command>insserv</command> for the service). Services
    may only be started or stopped by using the cluster management tools Hawk,
    the Pacemaker GUI or the crm shell. See
    <link xlink:href="https://www.suse.com/documentation/sle_ha/book_sleha/data/sec_ha_configuration_basics_resources.html"/>
    for more information.
   </para>
  </important>
  
 </sect1>
 <sect1 xml:id="sec.depl.maintenance.backup.admin">
  <title>Backing Up and Restoring the &admserv;</title>

  <para>
   As an example of how to back up and restore the data on the &admserv;,
   a <filename>crowbar-backup</filename> script is shipped with
   &productname; 5.
  </para>

  <para>
   The script is part of the
   <systemitem class="resource">crowbar</systemitem> package and is
   installed to <filename>/usr/sbin/crowbar-backup</filename> on the
   &admserv;. It is tailored to back up and restore the &admserv; of
   the respective &productname; version the script is shipped with. When
   calling the <command>crowbar-backup</command> script with the
   <command>backup</command> option, it will create a TAR archive with all
   configuration data that is needed to restore the &admserv;. For a list
   of available options and a short help text, run
  </para>

<screen>./usr/sbin/crowbar-backup <option>help</option></screen>

  <variablelist>
   <title>Example Commands for Using the Script</title>
   <varlistentry>
    <term>Creating a Backup</term>
    <listitem>
<screen>/usr/sbin/crowbar-backup&nbsp;<option>backup</option>&nbsp;[<replaceable>FILENAME</replaceable>]</screen>
     <para>
      Creates a TAR archive with all configuration data that is needed to
      restore the &admserv;. If you do not specify a
      <replaceable>FILENAME</replaceable>, the data is written to
      <filename>/tmp/backup-crowbar.tar.gz</filename> by default. Any
      services that need to be stopped for creating the backup will be
      stopped automatically and will be restarted afterwards by the script
      as needed.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>Restoring a Backup From a TAR Archive</term>
    <listitem>
<screen>/usr/sbin/crowbar-backup&nbsp;<option>restore</option>&nbsp;[<replaceable>FILENAME</replaceable>]</screen>
     <para>
      Reads the configuration data for an &admserv; from the specified
      TAR archive and restores the data on the current &admserv;.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>Cleaning Up the &admserv;</term>
    <listitem>
<screen>/usr/sbin/crowbar-backup&nbsp;<option>cleanup</option></screen>
     <para>
      Stops services and deletes the configuration files on the &admserv;
      that are related to &crow;.
     </para>
     <para>
      Use only after you have created a backup of the &admserv; and only
      if you want to test the restoration process.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>Wiping the &admserv;</term>
    <listitem>
<screen>/usr/sbin/crowbar-backup&nbsp;<option>purge</option></screen>
     <para>
      Uninstalls all packages on the &admserv; that are related to
      &crow;. Additionally, deletes all configuration files that are
      related to &crow;.
     </para>
     <para>
      Use only after you have created a backup of the &admserv; and only
      if you want to test the restoration process.
     </para>
    </listitem>
   </varlistentry>
  </variablelist>
 </sect1>
</chapter>