<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE chapter
[
  <!ENTITY % entities SYSTEM "entity-decl.ent">
  %entities;
]>
<!-- Converted by suse-upgrade version 1.1 -->
<chapter xmlns="http://docbook.org/ns/docbook" xmlns:xi="http://www.w3.org/2001/XInclude" xmlns:xlink="http://www.w3.org/1999/xlink" version="5.0" xml:id="cha.depl.maintenance">
 <title>&cloud; Maintenance</title>
 <info>
<dm:docmanager xmlns:dm="urn:x-suse:ns:docmanager">
    <dm:maintainer>fs</dm:maintainer>
    <dm:status>editing</dm:status>
    <dm:deadline/>
    <dm:priority/>
    <dm:translation>no</dm:translation>
    <dm:languages/>
</dm:docmanager>
</info>
<para/>
 <sect1 xml:id="sec.depl.maintenance.updates">
  <title>Keeping the Nodes Up-To-Date</title>

  <para>
   Keeping the nodes in &productname; up-to-date requires an appropriate
   setup of the update and pool repositories and the deployment of
   either the <guimenu>Updater</guimenu> &barcl; or the &susemgr;
   &barcl;. For details, see
   <xref linkend="sec.depl.adm_conf.repos.scc"/>, <xref
   linkend="sec.depl.inst.nodes.post.updater"/>, and
   <xref linkend="sec.depl.inst.nodes.post.manager"/>.
 </para>

  <para>
   If one of those &barcl;s is deployed, patches are installed on the
   nodes. Patches that do not require a reboot will not cause a service 
   interruption. If a patch (for example, a kernel
   update) requires a reboot after the installation, services running on the
   machine that is rebooted will not be available within &cloud;.
   Therefore it is strongly recommended to install those patches during a
   maintenance window.
  </para>

  <note>
   <title>No Maintenance Mode</title>
   <para>
    As of &productname; &productnumber; it is not possible to put
    &cloud; into <quote>Maintenance Mode</quote>.
   </para>
  </note>

  <remark condition="clarity">
   2013-10-02 - fs: The following is mainly based on assumptions...
  </remark>

  <variablelist>
   <title>Consequences when Rebooting Nodes</title>
   <varlistentry>
    <term>&admserv;</term>
    <listitem>
     <para>
      While the &admserv; is offline, it is not possible to deploy new
      nodes. However, rebooting the &admserv; has no effect on starting
      &vmguest;s or on &vmguest;s already running.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>&contrnode;s</term>
    <listitem>
     <para>
      The consequences a reboot of a &contrnode; depend on the
      services running on that node:
     </para>
     <formalpara>
      <title>Database, &o_ident;, RabbitMQ, &o_img;, &o_comp;:</title>
      <para>
       No new &vmguest;s can be started.
      </para>
     </formalpara>
     <formalpara>
      <title>&o_objstore;:</title>
      <para>
       No object storage data is available. If &o_img; uses
       &o_objstore;, it will not be possible to start new &vmguest;s.
      </para>
     </formalpara>
     <formalpara>
      <title>&o_blockstore;, &ceph;:</title>
      <para>
       No block storage data is available.
      </para>
     </formalpara>
     <formalpara>
      <title>&o_netw;:</title>
      <para>
       No new &vmguest;s can be started. On running &vmguest;s the
       network will be unavailable.
      </para>
     </formalpara>
     <formalpara>
      <title>&o_dash;</title>
      <para>
       &o_dash; will be unavailable. Starting and managing &vmguest;s
       can be done with the command line tools.
      </para>
     </formalpara>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>&compnode;s</term>
    <listitem>
     <para>
      <remark condition="clarity">
       2013-10-02 - fs: How to ensure no new instances are started on a compute
       node while evacuating it? What about Windows compute nodes? What about
       &vmware;?
      </remark>
      Whenever a &compnode; is rebooted, all &vmguest;s running on
      that particular node will be shut down and must be manually restarted.
      Therefore it is recommended to <quote>evacuate</quote> the node by
      migrating &vmguest;s to another node, before rebooting it.
<!-- (see ???
      for details) -->
     </para>
    </listitem>
   </varlistentry>
  </variablelist>
 </sect1>

 <sect1 xml:id="sec.depl.maintenance.service_order">
  <title>Service Order on &cloud; Start-up or Shutdown</title>
  <para>
   In case you need to restart your complete &cloud; (after a complete shut
   down or a power outage), the nodes and services need to be started in the following
   order:
  </para>
  <orderedlist>
   <title>Service Order on Start-up</title>
   <listitem>
    <para>
     &contrnode;/Cluster on which the Database is deployed
    </para>
   </listitem>
   <listitem>
    <para>
     &contrnode;/Cluster on which RabbitMQ is deployed
    </para>
   </listitem>
   <listitem>
    <para>
     &contrnode;/Cluster on which &o_ident; is deployed
    </para>
   </listitem>
   <listitem>
    <para>For &o_objstore;:</para>
    <orderedlist>
     <listitem>
      <para>
       &stornode; on which the <literal>swift-storage</literal> role is deployed
      </para>
     </listitem>
     <listitem>
      <para>
       &stornode; on which the <literal>swift-proxy</literal> role is deployed
      </para>
     </listitem>
    </orderedlist>
   </listitem>
   <listitem>
    <para>For &ceph;:</para>
    <orderedlist>
     <listitem>
      <para>
       &stornode; on which the <literal>ceph-mon</literal> role is deployed
      </para>
     </listitem>
     <listitem>
      <para>
       &stornode; on which the <literal>ceph-osd</literal> role is deployed
      </para>
     </listitem>
     <listitem>
      <para>
       &stornode; on which the <literal>ceph-radosgw</literal> and
       <literal>ceph-mds</literal> roles are deployed (if deployed on different
       nodes: in either order)
      </para>
     </listitem>
    </orderedlist>
   </listitem>
   <listitem>
    <para>
     Any remaining &contrnode;/Cluster. The following additional rules apply:
    </para>
    <itemizedlist>
     <listitem>
      <para>
       The &contrnode;/Cluster on which the <literal>neutron-server</literal>
       role is deployed needs to be started before starting the node/cluster
       on which the <literal>neutron-l3</literal> role is deployed.
      </para>
     </listitem>
     <listitem>
      <para>
       The &contrnode;/Cluster on which the <literal>nova-controller</literal>
       role is deployed needs to be started before starting the node/cluster
       on which &o_orch; is deployed.
      </para>
     </listitem>
    </itemizedlist>
   </listitem>
   <listitem>
    <para>
     &compnode;s
    </para>
   </listitem>
  </orderedlist>
  <para>
   If multiple roles are deployed on a single &contrnode;, the services are
   automatically started in the correct order on that node. If you have more
   than one node with multiple roles, make sure they are
   started as closely as possible to the order listed above.
  </para>
  <para>
   If you need to shut down &cloud;, the nodes and services need to be terminated in
   reverse order than on start-up:
  </para>

  <orderedlist>
   <title>Service Order on Shut-down</title>
   <listitem>
    <para>
     &compnode;s
    </para>
   </listitem>
   <listitem>
    <para>
     &contrnode;/Cluster on which &o_orch; is deployed
    </para>
   </listitem>
   <listitem>
    <para>
     &contrnode;/Cluster on which the <literal>nova-controller</literal>
     role is deployed
    </para>
   </listitem>
   <listitem>
    <para>
     &contrnode;/Cluster on which the <literal>neutron-l3</literal>
     role is deployed
    </para>
   </listitem>
   <listitem>
    <para>
     All &contrnode;(s)/Cluster(s) on which neither of the following services
     is deployed: Database, RabbitMQ, and &o_ident;.
    </para>
   </listitem>
   <listitem>
    <para>For &o_objstore;:</para>
    <orderedlist>
     <listitem>
      <para>
       &stornode; on which the <literal>swift-proxy</literal> role is deployed
      </para>
     </listitem>
     <listitem>
      <para>
       &stornode; on which the <literal>swift-storage</literal> role is deployed
      </para>
     </listitem>
    </orderedlist>
   </listitem>
   <listitem>
    <para>For &ceph;:</para>
    <orderedlist>
     <listitem>
      <para>
       &stornode; on which the <literal>ceph-radosgw</literal> and
       <literal>ceph-mds</literal> roles are deployed (if deployed on different
       nodes: in either order)
      </para>
     </listitem>
     <listitem>
      <para>
       &stornode; on which the <literal>ceph-osd</literal> role is deployed
      </para>
     </listitem>
     <listitem>
      <para>
       &stornode; on which the <literal>ceph-mon</literal> role is deployed
      </para>
     </listitem>
    </orderedlist>
   </listitem>
   <listitem>
    <para>
     &contrnode;/Cluster on which &o_ident; is deployed
    </para>
   </listitem>
   <listitem>
    <para>
     &contrnode;/Cluster on which RabbitMQ is deployed
    </para>
   </listitem>
   <listitem>
    <para>
     &contrnode;/Cluster on which the Database is deployed
    </para>
   </listitem>
  </orderedlist>
 </sect1>

  <sect1 xml:id="sec.depl.maintenance.bootstrap.compute.plane">
  <title>Bootstrapping Compute Plane</title>
  <para>
   If the whole compute plane is down, it is not always obvious how to boot it
   up, because it can be subject to deadlock if evacuate attributes are set on
   every &compnode;. In this case, manual intervention is
   required. Specifically, the operator must manually choose one or more
   &compnode;s to bootstrap the compute plane, and then run the
   <command>attrd_updater -n evacuate -U no -N $node</command> command for each
   of those &compnode;s to indicate that they do not require the resurrection
   process and can have their <literal>nova-compute</literal> start up straight
   away. Once these &compnode;s are up, this breaks the deadlock allowing
   <literal>nova-evacuate</literal> to start. This way, any other nodes that
   require resurrection can be processed automatically. If no resurrection is
   desired anywhere in the cloud, then the attributes should be set to
   <literal>no</literal> for all nodes.
  </para>
  <important>
   <para>
    Keep in mind that if &compnode;s are started too long after the
    <literal>remote-*</literal> resources are started on the control plane,
    they will be liable to fencing and so this should be avoided.
   </para>
  </important>
  </sect1>
</chapter>
