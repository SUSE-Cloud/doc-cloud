<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE chapter
[
  <!ENTITY % entities SYSTEM "entity-decl.ent">
  %entities;
]>
<!-- Converted by suse-upgrade version 1.1 -->
<chapter xmlns="http://docbook.org/ns/docbook" xmlns:xi="http://www.w3.org/2001/XInclude" xmlns:xlink="http://www.w3.org/1999/xlink" version="5.0" xml:id="cha-depl-maintenance">
 <title>&cloud; Maintenance</title>
 <info>
<dm:docmanager xmlns:dm="urn:x-suse:ns:docmanager">
    <dm:maintainer>fs</dm:maintainer>
    <dm:status>editing</dm:status>
    <dm:deadline/>
    <dm:priority/>
    <dm:translation>no</dm:translation>
    <dm:languages/>
</dm:docmanager>
</info>
<para/>
 <sect1 xml:id="sec-depl-maintenance-updates">
  <title>Keeping the Nodes Up-To-Date</title>

  <para>
   Keeping the nodes in &productname; up-to-date requires an appropriate
   setup of the update and pool repositories and the deployment of
   either the <guimenu>Updater</guimenu> &barcl; or the &susemgr;
   &barcl;. For details, see
   <xref linkend="sec-depl-adm-conf-repos-scc"/>, <xref
   linkend="sec-depl-inst-nodes-post-updater"/>, and
   <xref linkend="sec-depl-inst-nodes-post-manager"/>.
 </para>

  <para>
   If one of those &barcl;s is deployed, patches are installed on the
   nodes. Patches that do not require a reboot will not cause a service
   interruption. If a patch (for example, a kernel
   update) requires a reboot after the installation, services running on the
   machine that is rebooted will not be available within &cloud;.
   Therefore it is strongly recommended to install those patches during a
   maintenance window.
  </para>

  <note>
   <title>No Maintenance Mode</title>
   <para>
    As of &productname; &productnumber; it is not possible to put
    &cloud; into <quote>Maintenance Mode</quote>.
   </para>
  </note>

  <remark condition="clarity">
   2013-10-02 - fs: The following is mainly based on assumptions...
  </remark>

  <variablelist>
   <title>Consequences when Rebooting Nodes</title>
   <varlistentry>
    <term>&admserv;</term>
    <listitem>
     <para>
      While the &admserv; is offline, it is not possible to deploy new
      nodes. However, rebooting the &admserv; has no effect on starting
      &vmguest;s or on &vmguest;s already running.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>&contrnode;s</term>
    <listitem>
     <para>
      The consequences a reboot of a &contrnode; depend on the
      services running on that node:
     </para>
     <formalpara>
      <title>Database, &o_ident;, RabbitMQ, &o_img;, &o_comp;:</title>
      <para>
       No new &vmguest;s can be started.
      </para>
     </formalpara>
     <formalpara>
      <title>&o_objstore;:</title>
      <para>
       No object storage data is available. If &o_img; uses
       &o_objstore;, it will not be possible to start new &vmguest;s.
      </para>
     </formalpara>
     <formalpara>
      <title>&o_blockstore;, &ceph;:</title>
      <para>
       No block storage data is available.
      </para>
     </formalpara>
     <formalpara>
      <title>&o_netw;:</title>
      <para>
       No new &vmguest;s can be started. On running &vmguest;s the
       network will be unavailable.
      </para>
     </formalpara>
     <formalpara>
      <title>&o_dash;</title>
      <para>
       &o_dash; will be unavailable. Starting and managing &vmguest;s
       can be done with the command line tools.
      </para>
     </formalpara>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>&compnode;s</term>
    <listitem>
     <para>
      <remark condition="clarity">
       2013-10-02 - fs: How to ensure no new instances are started on a compute
       node while evacuating it? What about Windows compute nodes? What about
       &vmware;?
      </remark>
      Whenever a &compnode; is rebooted, all &vmguest;s running on
      that particular node will be shut down and must be manually restarted.
      Therefore it is recommended to <quote>evacuate</quote> the node by
      migrating &vmguest;s to another node, before rebooting it.
<!-- (see ???
      for details) -->
     </para>
    </listitem>
   </varlistentry>
  </variablelist>
 </sect1>

 <sect1 xml:id="sec-depl-maintenance-service-order">
  <title>Service Order on &cloud; Start-up or Shutdown</title>
  <para>
   In case you need to restart your complete &cloud; (after a complete shut
   down or a power outage), the nodes and services need to be started in the following
   order:
  </para>
  <orderedlist>
   <title>Service Order on Start-up</title>
   <listitem>
    <para>
     &contrnode;/Cluster on which the Database is deployed
    </para>
   </listitem>
   <listitem>
    <para>
     &contrnode;/Cluster on which RabbitMQ is deployed
    </para>
   </listitem>
   <listitem>
    <para>
     &contrnode;/Cluster on which &o_ident; is deployed
    </para>
   </listitem>
   <listitem>
    <para>For &o_objstore;:</para>
    <orderedlist>
     <listitem>
      <para>
       &stornode; on which the <literal>swift-storage</literal> role is deployed
      </para>
     </listitem>
     <listitem>
      <para>
       &stornode; on which the <literal>swift-proxy</literal> role is deployed
      </para>
     </listitem>
    </orderedlist>
   </listitem>
   <listitem>
    <para>For &ceph;:</para>
    <orderedlist>
     <listitem>
      <para>
       &stornode; on which the <literal>ceph-mon</literal> role is deployed
      </para>
     </listitem>
     <listitem>
      <para>
       &stornode; on which the <literal>ceph-osd</literal> role is deployed
      </para>
     </listitem>
     <listitem>
      <para>
       &stornode; on which the <literal>ceph-radosgw</literal> and
       <literal>ceph-mds</literal> roles are deployed (if deployed on different
       nodes: in either order)
      </para>
     </listitem>
    </orderedlist>
   </listitem>
   <listitem>
    <para>
     Any remaining &contrnode;/Cluster. The following additional rules apply:
    </para>
    <itemizedlist>
     <listitem>
      <para>
       The &contrnode;/Cluster on which the <literal>neutron-server</literal>
       role is deployed needs to be started before starting the node/cluster
       on which the <literal>neutron-l3</literal> role is deployed.
      </para>
     </listitem>
     <listitem>
      <para>
       The &contrnode;/Cluster on which the <literal>nova-controller</literal>
       role is deployed needs to be started before starting the node/cluster
       on which &o_orch; is deployed.
      </para>
     </listitem>
    </itemizedlist>
   </listitem>
   <listitem>
    <para>
     &compnode;s
    </para>
   </listitem>
  </orderedlist>
  <para>
   If multiple roles are deployed on a single &contrnode;, the services are
   automatically started in the correct order on that node. If you have more
   than one node with multiple roles, make sure they are
   started as closely as possible to the order listed above.
  </para>
  <para>
   If you need to shut down &cloud;, the nodes and services need to be terminated in
   reverse order than on start-up:
  </para>

  <orderedlist>
   <title>Service Order on Shut-down</title>
   <listitem>
    <para>
     &compnode;s
    </para>
   </listitem>
   <listitem>
    <para>
     &contrnode;/Cluster on which &o_orch; is deployed
    </para>
   </listitem>
   <listitem>
    <para>
     &contrnode;/Cluster on which the <literal>nova-controller</literal>
     role is deployed
    </para>
   </listitem>
   <listitem>
    <para>
     &contrnode;/Cluster on which the <literal>neutron-l3</literal>
     role is deployed
    </para>
   </listitem>
   <listitem>
    <para>
     All &contrnode;(s)/Cluster(s) on which neither of the following services
     is deployed: Database, RabbitMQ, and &o_ident;.
    </para>
   </listitem>
   <listitem>
    <para>For &o_objstore;:</para>
    <orderedlist>
     <listitem>
      <para>
       &stornode; on which the <literal>swift-proxy</literal> role is deployed
      </para>
     </listitem>
     <listitem>
      <para>
       &stornode; on which the <literal>swift-storage</literal> role is deployed
      </para>
     </listitem>
    </orderedlist>
   </listitem>
   <listitem>
    <para>For &ceph;:</para>
    <orderedlist>
     <listitem>
      <para>
       &stornode; on which the <literal>ceph-radosgw</literal> and
       <literal>ceph-mds</literal> roles are deployed (if deployed on different
       nodes: in either order)
      </para>
     </listitem>
     <listitem>
      <para>
       &stornode; on which the <literal>ceph-osd</literal> role is deployed
      </para>
     </listitem>
     <listitem>
      <para>
       &stornode; on which the <literal>ceph-mon</literal> role is deployed
      </para>
     </listitem>
    </orderedlist>
   </listitem>
   <listitem>
    <para>
     &contrnode;/Cluster on which &o_ident; is deployed
    </para>
   </listitem>
   <listitem>
    <para>
     &contrnode;/Cluster on which RabbitMQ is deployed
    </para>
   </listitem>
   <listitem>
    <para>
     &contrnode;/Cluster on which the Database is deployed
    </para>
   </listitem>
  </orderedlist>
 </sect1>

 <sect1 xml:id="sec-depl-maintenance-upgrade">
  <title>Upgrading from &productname; 6 to &productname; 7</title>

  <para>
   Upgrading from &productname; 6 to &productname; 7 can be done either via a
   &wi; or from the command line. Starting with &productname; 7, a
   <quote>non-disruptive</quote> update is supported, when the requirements
   listed at <xref linkend="list-depl-maintenance-upgrade-non-disruptive"/> are
   met. The non-disruptive upgrade guarantees a fully-functional &cloud;
   operation during the upgrade procedure. The only feature that is not
   supported during the non-disruptive upgrade procedure is the deployment of
   additional nodes.
  </para>
  <para>
   If the requirements for a non-disruptive upgrade are not met, the
   upgrade procedure will be done in normal mode. When
   live-migration is set up, &vmguest;s will be migrated to another node
   before the respective &compnode; is updated to ensure continuous
   operation. You will not be able to access &vmguest;s during the
   upgrade of the &contrnode;s.
 </para>


  <important>
    <title>STONITH and &admserv;</title>
    <para>
      Make sure that the STONITH mechanism in your cloud does not rely on the state of the
      &admserv; (for example, no SBD devices located there, and IPMI is not
      using the network connection relaying on the &admserv;). Otherwise, this
      may affect the clusters when the &admserv; is rebooted during the
      upgrade procedure.
     </para>
    </important>

  <sect2 xml:id="sec-depl-maintenance-upgrade-require">
   <title>Requirements</title>
   <para>
    When starting the upgrade process, several checks are performed to
    determine whether the &cloud; is in an upgradeable state and whether a
    non-disruptive update would be supported:
   </para>
   <itemizedlist>
    <title>General Upgrade Requirements</title>
    <listitem>
     <para>
      All nodes need to have the latest &productname; 6 updates
      <emphasis>and</emphasis> the latest &slsa; 12 SP1 updates installed. If
      this is not the case, refer to <xref
      linkend="sec-depl-inst-nodes-post-updater"/> for instructions on how to
      update.
     </para>
    </listitem>
    <listitem>
     <para>
      All allocated nodes need to be turned on and have to be in state
      <quote>ready</quote>.
     </para>
    </listitem>
    <listitem>
     <para>
      All &barcl; proposals need to have been successfully deployed. If a
      proposal is in state <quote>failed</quote>, the upgrade procedure will
      refuse to start. Fix the issue or&mdash;if possible&mdash;remove the
      proposal.
     </para>
    </listitem>
    <listitem>
     <para>
      If the pacemaker &barcl; is deployed, all clusters need to be in a
      healthy state.
     </para>
    </listitem>
    <listitem>
     <para>
      The following repositories need to be available on a server that is
      accessible from the &admserv;. The HA repositories are only needed if you
      have an &hasetup;. It is recommended to use the same server
      that also hosts the respective repositories of the current version.
     </para>
     <simplelist>
      <member><literal>&cloud_repo;-Pool</literal></member>
      <member><literal>&cloud_repo;-Update</literal></member>
      <member><literal>&sle_repo;-Pool</literal></member>
      <member><literal>&sle_repo;-Update</literal></member>
      <member>
       <literal>&sleha_repo;-Pool</literal> (for &hasetup;s only)
      </member>
      <member>
       <literal>&sleha_repo;-Update</literal> (for &hasetup;s only)
      </member>
     </simplelist>
     <para>
      Do not add these repositories to the &cloud; repository configuration,
      as this needs to be done during the upgrade procedure.
     </para>
     <para>
      If you have deployed &ceph; (&ses; 2.1) you also need to make the
      &ses; 4 repositories available:
     </para>
     <simplelist>
      <member><literal>&ses_repo;-Pool</literal></member>
      <member><literal>&ses_repo;-Update</literal></member>
     </simplelist>
    </listitem>
    <listitem>
     <para>
      Hybrid authentication with &o_ident; is not supported during upgrade. If
      you have configured this authentication method (see <citetitle>Using
      Hybrid Authentication</citetitle> in the &productname; 6 Deployment
      Guide), you need to revert this setting prior to starting the upgrade. To
      do so, open the &o_ident; &barcl; <guimenu>Attribute</guimenu>
      configuration in <guimenu>Raw</guimenu> mode. Set the identity and
      assignment drivers as follows:
     </para>
     <screen>  "identity": {
    "driver": "sql"
  },
  "assignment": {
    "driver": "sql"
     },</screen>
     <!-- fs 2017-05-04: Commenting per bsc#1037531
     <para>
      After the upgrade has finished, re-enable hybrid authentication as
      described in <xref linkend="sec-depl-ostack-keystone-ldap-hybrid"/>.
     </para>
     -->
    </listitem>
    <listitem>
     <para>
      &productname; 7 comes with a new version of &o_objstore;. Replica
      handling has changed with this version&mdash;the number of replicas
      cannot be higher than the number of available disks. Check the
      <guimenu>Replica</guimenu> setting in the &o_objstore; &barcl; prior to
      starting the upgrade. If it is higher than the number of available disks,
      adjust it to match the number of disks and redeploy the &barcl;.
     </para>
    </listitem>
    <listitem>
     <para>
       Note that a non-disruptive upgrade is not supported if the
       &o_blockstore; has been deployed with the <literal>raw devices</literal>
       or <literal>local file</literal> back-end. In this case, you have to
       perform a regular upgrade, or change the &o_blockstore; back-end for a
       non-disruptive upgrade.
     </para>
    </listitem>
    <listitem>
     <para>
      If you have deployed &ceph; (&ses;) you need to upgrade all &ceph; nodes
      from version 2.1 to version 4 prior to starting the &productname;
      upgrade. For details refer to <link
      xlink:href="https://www.suse.com/documentation/ses-4/book_storage_admin/data/ceph_upgrade_general.html"/>.
     </para>
     <orderedlist>
      <listitem>
       <para>
        Log in to the &crow; &wi; for &cloud; and choose <menuchoice>
        <guimenu>Utilities</guimenu> <guimenu>Prepare Ceph Upgrade</guimenu>
        <guimenu>Prepare for the Upgrade</guimenu></menuchoice>.
       </para>
      </listitem>
      <listitem>
       <para>
        The following steps need to be performed on each &ceph; node:
       </para>
       <orderedlist numeration="loweralpha">
        <listitem>
         <para>
          Upgrade the current &slsa; SP1 to version 12 SP2. Refer to <link
          xlink:href="https://www.suse.com/documentation/sles-12/book_sle_deployment/data/cha_update_spmigration.html"/>
          for more information on supported upgrade methods. Either use the
          &yast; Online Migration tool or <command>zypper migrate</command>. Do
          <emphasis>not</emphasis> use the method using <command>zypper
          dup</command> (<citetitle>Migrating with Plain Zypper</citetitle>).
         </para>
        </listitem>
        <listitem>
         <para>
          Use <menuchoice> <guimenu>&yast;</guimenu>
          <guimenu>Software</guimenu>
          <guimenu>Software Repositories</guimenu></menuchoice> or the command
          line too <command>zypper mr</command> to replace the
          <literal>SUSE-Enterprise-Storage-2.1-Pool</literal> and
          <literal>SUSE-Enterprise-Storage-2.1-Update</literal> repositories
          with the respective repositories for version 4.
         </para>
        </listitem>
        <listitem>
         <para>
          Install the upgrade helper package:
         </para>
         <screen>&prompt.root; zypper in ses-upgrade-helper</screen>
        </listitem>
        <listitem>
         <para>
          Run the upgrade script:
         </para>
         <screen>&prompt.root; upgrade-ses.sh</screen>
         <para>
          The script does a distribution upgrade of the node. After a reboot,
          the node comes up with &cloudos; and &ses; 4 running.
         </para>
        </listitem>
       </orderedlist>
      </listitem>
      <listitem>
       <para>
        Continue with step 8 on <link
        xlink:href="https://www.suse.com/documentation/ses-4/book_storage_admin/data/ceph_upgrade_to4.html"/>.
       </para>
      </listitem>
     </orderedlist>
    </listitem>
   </itemizedlist>

   <itemizedlist xml:id="list-depl-maintenance-upgrade-non-disruptive">
    <title>Non-Disruptive Upgrade Requirements</title>
    <listitem>
     <para>
      All &contrnode;s need to be set up highly available.
     </para>
    </listitem>
    <listitem>
     <para>
      Live-migration support needs to be configured and enabled for the
      &compnode;s. The amount of free resources (CPU and RAM) on the
      &compnode;s needs to be sufficient to evacuate the nodes one by one.
     </para>
    </listitem>
    <listitem>
     <para>
       In case of a non-disruptive upgrade, &o_img; must be configured as a
       shared storage if the <guimenu>Default Storage
       Store</guimenu> value in the &o_img; is set to <literal>File</literal>.
     </para>
    </listitem>
    <listitem>
     <para>
       For a non-disruptive upgrade, only KVM-based &compnode;s with
       the <literal>nova-computer-kvm</literal> role are allowed in &productname; 6.
     </para>
    </listitem>
    <listitem>
     <para>
       If your &productname; 6 setup is using LBaaS v1, you need to
       switch to LBaaS v2 manually. To do this, switch to the
       <systemitem>Raw</systemitem> view in the &o_netw; &barcl; and set the
       <systemitem>use_lbaasv2</systemitem> parameter to
       <literal>true</literal>. You must also re-create all objects
       manually. This includes load balancers, pools, and health monitors. Note
       that switching to LBaaS v2 is not required if your &productname; 6 setup
       is configured for LBaaS v1 but is not using it actively.
     </para>
    </listitem>
    <listitem>
     <para>
       Non-disruptive upgrade is limited to the following cluster
       configurations:
     </para>
       <itemizedlist>
       <listitem>
         <para>
           Single cluster that has all supported controller roles on it
         </para>
       </listitem>
       <listitem>
         <para>
          Two clusters where one only has
          <systemitem>neutron-network</systemitem> and the other one the rest
          of the controller roles.
         </para>
       </listitem>
       <listitem>
         <para>
          Two clusters where one only has
          <systemitem>neutron-server</systemitem> plus
          <systemitem>neutron-network</systemitem> and the other one the rest
          of the controller roles.
         </para>
       </listitem>
       <listitem>
         <para>
           Two clusters, where one cluster runs the database and RabbitMQ
         </para>
       </listitem>
       <listitem>
         <para>
           Three clusters, where one cluster runs database and RabbitMQ,
           another cluster runs APIs, and the third cluster has the
           <systemitem>neutron-network</systemitem> role.
         </para>
       </listitem>
       </itemizedlist>
       <para>
       If your cluster configuration is not supported by the non-disruptive
       upgrade procedure, you can still perform a normal upgrade.
     </para>
    </listitem>
   </itemizedlist>
  </sect2>

  <sect2 xml:id="sec-depl-maintenance-upgrade-ui">
   <title>Upgrading Using the Web Interface</title>
   <para>
    The &wi; features a wizard that guides you through the upgrade
    procedure.
   </para>
   <note>
     <title>Canceling Upgrade</title>
     <para>
       You can cancel the upgrade process by clicking <guimenu>Cancel
    Upgrade</guimenu>. Note that the upgrade operation can be canceled
    only before the &admserv; upgrade is started. When the upgrade has been
    canceled, the nodes return to the ready state. However any user modifications
    must be undone manually. This includes reverting repository configuration.
      </para>
     </note>
   <procedure>
    <step>
     <para>
      To start the upgrade procedure, open the &crow; &wi; on the &admserv; and choose <menuchoice>
      <guimenu>Utilities</guimenu> <guimenu>Upgrade</guimenu>
      </menuchoice>. Alternatively, point the browser directly to the upgrade
      wizard on the &admserv;, for example
      <literal>http://192.168.124.10/upgrade/</literal>.
     </para>
     <informalfigure>
      <mediaobject>
       <imageobject role="fo">
        <imagedata fileref="depl_upgrade_initial.png" width="100%" format="PNG"/>
       </imageobject>
       <imageobject role="html">
        <imagedata fileref="depl_upgrade_initial.png" width="100%" format="PNG"/>
       </imageobject>
      </mediaobject>
     </informalfigure>
    </step>
    <step>
     <para>
      Perform the preliminary checks to determine whether the upgrade
      requirements are met by clicking <guimenu>Check</guimenu> in
      <guimenu>Step 1: Preliminary Checks</guimenu>.
     </para>
     <para>
      The &wi; displays the progress of the checks. Make sure all checks are
      passed (you should see a green marker next to each check). If errors
      occur, fix them and run
      the <command>Check</command> again. Do not proceed before all checks
      are passed.
     </para>
    </step>
    <step>
     <para>
      When all checks in the previous step have passed, <guimenu>Step 2:
      Upgrade Mode</guimenu> shows the result of the upgrade analysis. You are
      informed about whether the upgrade procedure will continue in
      non-disruptive or in normal mode.
     </para>
    </step>
    <step>
     <para>
      To start the upgrade process, click <guimenu>Begin Upgrade</guimenu> in
      <guimenu>Step 3: Begin Upgrade</guimenu>.
     </para>
    </step>
    <step>
     <para>
      While the upgrade of the &admserv; is prepared,
      the upgrade wizard prompts you to
      <guimenu>Download the Backup of the &admserv;</guimenu>. Do this, and move
      the backup to a safe place. In case something goes wrong during the
      upgrade procedure of the &admserv;, you can restore the original state
      from this backup using the <command>crowbarctl backup restore
      <replaceable>NAME</replaceable></command> command.
     </para>
     <informalfigure>
      <mediaobject>
       <imageobject role="fo">
        <imagedata fileref="depl_upgrade_backup.png" width="100%" format="PNG"/>
       </imageobject>
       <imageobject role="html">
        <imagedata fileref="depl_upgrade_backup.png" width="100%" format="PNG"/>
       </imageobject>
      </mediaobject>
     </informalfigure>
    </step>
    <step>
      <para>
        In the next step, check whether the
        repositories required for upgrading the &admserv; are available and
        updated. To do this, click
        <guimenu>Check</guimenu>. If the checks fail, add the software
        repositories as described in <xref
        linkend="cha-depl-repo-conf" /> of
        the Deployment Guide. Run the checks again, and click <guimenu>Next</guimenu>.
      </para>
      <informalfigure>
      <mediaobject>
       <imageobject role="fo">
        <imagedata fileref="depl_upgrade_repos.png" width="100%" format="PNG"/>
       </imageobject>
       <imageobject role="html">
        <imagedata fileref="depl_upgrade_repos.png" width="100%" format="PNG"/>
       </imageobject>
      </mediaobject>
     </informalfigure>
    </step>
    <step>
      <para>
        Click <guimenu>Upgrade Administration Server</guimenu> to upgrade and
        reboot the admin node. Note that this operation may take a while. When
        the &admserv; has been updated, click
        <guimenu>Next</guimenu>.
      </para>
      <informalfigure>
      <mediaobject>
       <imageobject role="fo">
        <imagedata fileref="depl_upgrade_admserv.png" width="100%" format="PNG"/>
       </imageobject>
       <imageobject role="html">
        <imagedata fileref="depl_upgrade_admserv.png" width="100%" format="PNG"/>
       </imageobject>
      </mediaobject>
     </informalfigure>
    </step>
    <step>
      <para>
        Create a new database on the &admserv; by specifying the desired
        username and password. Click then <guimenu>Create
        Database</guimenu>. If you choose to host the database on a different
        host, create the database as described in Step
        7 of <xref linkend="sec-depl-maintenance-upgrade-cmdl" />. Specify the
        required database connection information in the <guimenu>Connect
        to Database</guimenu> section of the upgrade wizard.
      </para>
      <informalfigure>
      <mediaobject>
       <imageobject role="fo">
        <imagedata fileref="depl_upgrade_db.png" width="100%" format="PNG"/>
       </imageobject>
       <imageobject role="html">
        <imagedata fileref="depl_upgrade_db.png" width="100%" format="PNG"/>
       </imageobject>
      </mediaobject>
     </informalfigure>
    </step>

    <step>
      <para>
        In the next step, check whether the
        repositories required for upgrading all nodes are available and updated.
        To do this click <guimenu>Check</guimenu>. If the checks fail, add the
        software repositories as described in <xref
        linkend="cha-depl-repo-conf" /> of
        the Deployment Guide. Run the checks again, and click
        <guimenu>Next</guimenu>.
        </para>
        <informalfigure>
      <mediaobject>
       <imageobject role="fo">
        <imagedata fileref="depl_upgrade_node_repos.png" width="100%" format="PNG"/>
       </imageobject>
       <imageobject role="html">
        <imagedata fileref="depl_upgrade_node_repos.png" width="100%" format="PNG"/>
       </imageobject>
      </mediaobject>
     </informalfigure>
    </step>

    <step>
      <para>
        The next step is to stop the &ostack; services. Before you proceed, make
        sure that no changes are made to your cloud during and after stopping
        the services. Keep in mind that &ostack; API will not be
        available until the upgrade process is completed. When you are ready,
        click <guimenu>Next</guimenu>, wait till the services are stopped, and
        click <guimenu>Next</guimenu>.
      </para>
      <informalfigure>
      <mediaobject>
       <imageobject role="fo">
        <imagedata fileref="depl_upgrade_stopservices.png" width="100%" format="PNG"/>
       </imageobject>
       <imageobject role="html">
        <imagedata fileref="depl_upgrade_stopservices.png" width="100%" format="PNG"/>
       </imageobject>
      </mediaobject>
     </informalfigure>
    </step>

    <step>
      <para>
        Before upgrading the nodes, the wizard prompts you to back up the
        OpenStack PostgreSQL database. The database backup will be stored on
        the Administration Server, and it can be used to restore the database in
        case something goes wrong during the upgrade. To back up the database,
        click <guimenu>Create Backup</guimenu>. When the backup operation is
        finished, click <guimenu>Next</guimenu>.
      </para>
      <informalfigure>
      <mediaobject>
       <imageobject role="fo">
        <imagedata fileref="depl_upgrade_backupdb.png" width="100%" format="PNG"/>
       </imageobject>
       <imageobject role="html">
        <imagedata fileref="depl_upgrade_backupdb.png" width="100%" format="PNG"/>
       </imageobject>
      </mediaobject>
     </informalfigure>
    </step>

    <step>
      <para>
        The final step is upgrading the nodes. To
        start the upgrade, click <guimenu>Upgrade Nodes</guimenu>. Depending on
        the number of nodes, the upgrade process can take some time. When the
        upgrade is completed, press <guimenu>Finish</guimenu> to return to the
        Dashboard.
      </para>
            <informalfigure>
      <mediaobject>
       <imageobject role="fo">
        <imagedata fileref="depl_upgrade_nodes.png" width="100%" format="PNG"/>
       </imageobject>
       <imageobject role="html">
        <imagedata fileref="depl_upgrade_nodes.png" width="100%" format="PNG"/>
       </imageobject>
      </mediaobject>
     </informalfigure>
    </step>
   </procedure>

   <note>
    <title>Dealing with Errors</title>
    <para>
     If an error occurs during the upgrade process, the wizard displays a
     message with a description of the error and a possible solution. After
     fixing the error, re-run the step where the error occurred.
    </para>
   </note>
  </sect2>

  <sect2 xml:id="sec-depl-maintenance-upgrade-cmdl">
   <title>Upgrading from the Command Line</title>
   <para>
    The upgrade procedure on the command line is performed by using the program
    <command>crowbarctl</command>. For general help, run <command>crowbarctl
    help</command>. To get help on a certain subcommand, run
    <command>crowbarctl <replaceable>COMMAND</replaceable> help</command>.
   </para>
   <para>
    To review the process of the upgrade procedure, you may call
    <command>crowbarctl upgrade status</command> at any time. Steps may have
    three states: <literal>pending</literal>, <literal>running</literal>, and
    <literal>passed</literal>.
   </para>
   <procedure>
    <step>
     <para>
      To start the upgrade procedure from the command line, log in to the
      &admserv; as &rootuser;.
     </para>
    </step>
    <step>
     <para>
      Perform the preliminary checks to determine whether the upgrade
      requirements are met:
     </para>
     <screen>&prompt.root;crowbarctl upgrade prechecks</screen>
     <para>
      The command's result is shown in a table. Make sure the column
      <guimenu>Errors</guimenu> does not contain any entries. If not, make sure
      to fix the errors and restart the <command>precheck</command> command
      afterwards. Do not proceed before all checks are passed.
     </para>
<screen>&prompt.root;crowbarctl upgrade prechecks
+-------------------------------+--------+----------+--------+------+
| Check ID                      | Passed | Required | Errors | Help |
+-------------------------------+--------+----------+--------+------+
| network_checks                | true   | true     |        |      |
| cloud_healthy                 | true   | true     |        |      |
| maintenance_updates_installed | true   | true     |        |      |
| compute_status                | true   | false    |        |      |
| ha_configured                 | true   | false    |        |      |
| clusters_healthy              | true   | true     |        |      |
+-------------------------------+--------+----------+--------+------+</screen>
     <para>
      Depending on the outcome of the checks, it is automatically decided
      whether the upgrade procedure will continue in non-disruptive or in
      normal mode.
     </para>

     <tip>
      <title>Forcing Normal Mode Upgrade</title>
      <para>
       The non-disruptive update will take longer than an upgrade in normal
       mode, because it performs certain tasks in parallel which are done
       sequentially during the non-disruptive upgrade. Live-migrating guests to
       other &compnode;s during the non-disruptive
       upgrade takes additional time.
      </para>
      <para>
       Therefore, if a non-disruptive upgrade is not a requirement for you, you
       may want to switch to the normal upgrade mode, even if your setup
       supports the non-disruptive method. To force the normal upgrade mode,
       run:
      </para>
      <screen>&prompt.root;crowbarctl upgrade mode normal</screen>
      <para>
       To query the current upgrade mode run:
      </para>
      <screen>&prompt.root;crowbarctl upgrade mode</screen>
      <para>
       To switch back to the non-disruptive mode run:
      </para>
      <screen>&prompt.root;crowbarctl upgrade mode non_disruptive</screen>
      <para>
       It is possible to call this command at any time during the upgrade
       process until the <literal>services</literal> step is started. After
       that point the upgrade mode can no longer be changed.
      </para>
     </tip>
    </step>
    <step>
     <para>
      Prepare the nodes by transitioning them into the <quote>upgrade</quote>
      state and stopping the chef daemon:
     </para>
     <screen>&prompt.root;crowbarctl upgrade prepare</screen>
     <para>
      Depending of the size of your &cloud; deployment, this step may take
      some time. Use the command <command>crowbarctl upgrade status</command>
      to monitor the status of the process named
      <literal>steps.prepare.status</literal>. It needs to be in state
      <literal>passed</literal> before you proceed:
     </para>
     <screen>&prompt.root;crowbarctl upgrade status
+--------------------------------+----------------+
| Status                         | Value          |
+--------------------------------+----------------+
| current_step                   | backup_crowbar |
| current_substep                |                |
| current_node                   |                |
| remaining_nodes                |                |
| upgraded_nodes                 |                |
| crowbar_backup                 |                |
| openstack_backup               |                |
| steps.prechecks.status         | passed         |
| steps.prepare.status           | passed         |
| steps.backup_crowbar.status    | pending        |
| steps.repocheck_crowbar.status | pending        |
| steps.admin.status             | pending        |
| steps.database.status          | pending        |
| steps.repocheck_nodes.status   | pending        |
| steps.services.status          | pending        |
| steps.backup_openstack.status  | pending        |
| steps.nodes.status             | pending        |
+--------------------------------+----------------+</screen>
    </step>
    <step>
     <para>
      Create a backup of the existing &admserv; installation. In case something
      goes wrong during the upgrade procedure of the &admserv; you can restore
      the original state from this backup with the command <command>crowbarctl
      backup restore <replaceable>NAME</replaceable></command>
     </para>
     <screen>&prompt.root;crowbarctl upgrade backup crowbar</screen>
     <para>
      To list all existing backups including the one you have just created, run
      the following command:
     </para>
     <screen>&prompt.root;crowbarctl backup list
+----------------------------+--------------------------+--------+---------+
| Name                       | Created                  | Size   | Version |
+----------------------------+--------------------------+--------+---------+
| crowbar_upgrade_1486116507 | 2017-02-03T10:08:30.721Z | 209 KB | 3.0     |
+----------------------------+--------------------------+--------+---------+</screen>
    </step>
    <step>
     <para>
      This step prepares the upgrade of the &admserv; by checking the
      availability of the update and pool repositories for &productname;
      &productnumber; and &cloudos;. Run the following command:
     </para>
     <screen>&prompt.root;crowbarctl upgrade repocheck crowbar
+---------------------------------+--------------------------------+
| Status                          | Value                          |
+---------------------------------+--------------------------------+
| os.available                    | false                          |
| os.repos                        | SLES12-SP2-Pool                |
|                                 | SLES12-SP2-Updates             |
| os.errors.x86_64.missing        | SLES12-SP2-Pool                |
|                                 | SLES12-SP2-Updates             |
| openstack.available             | false                          |
| openstack.repos                 | SUSE-OpenStack-Cloud-7-Pool    |
|                                 | SUSE-OpenStack-Cloud-7-Updates |
| openstack.errors.x86_64.missing | SUSE-OpenStack-Cloud-7-Pool    |
|                                 | SUSE-OpenStack-Cloud-7-Updates |
+---------------------------------+--------------------------------+</screen>
     <para>
      All four required repositories are reported as missing, because they have
      not yet been added to the &crow; configuration. To add them to the
      &admserv; proceed as follows.
     </para>
     <para>
      Note that this step is for setting up the repositories for the &admserv;,
      not for the nodes in &cloud; (this will be done in a subsequent step).
     </para>
     <substeps>
      <step>
       <para>
        Start <command>yast repositories</command> and proceed with
        <guimenu>Continue</guimenu>. Replace the repositories
        <literal>SLES12-SP1-Pool</literal> and
        <literal>SLES12-SP1-Updates</literal> with the respective SP2
        repositories.
       </para>
       <para>
        If you prefer to use zypper over &yast;, you may alternatively make the
        change using <command>zypper mr</command>.
       </para>
       <!-- fs 2017-02-03: Does not seem possible to use this one
       <para>
        Note that if you have used the contents of the
        <filename>/srv/tftpboot/suse-12.1/x86_64/install</filename> directory
        as a source for the <literal>SLES12-SP1-Pool</literal>, you may also
        use the same approach now. Create the directory
        <filename>&tftp_dir;/install</filename> and mirror the contents from
        the first &cloudos; installation medium to this directory. See <xref
        linkend="sec-depl-adm-conf-repos-product"/> for instructions. Once
        done, you can use the newly created directory as a source for
        &sle_repo;.
        </para>
        -->
      </step>
      <step>
       <para>
        Next, replace the <literal>SUSE-OpenStack-Cloud-6</literal> update and
        pool repositories with the respective &productname; &productnumber;
        versions.
       </para>
       <!-- fs 2017-02-03: Does not seem possible to use this one
       <para>
        Note that if you have used the contents of the
        <filename>/srv/tftpboot/suse-12.1/x86_64/install</filename> directory
        as a source for the <literal>SLES12-SP1-Pool</literal>, you may also
        use the same approach now. Create the directory
        <filename>&tftp_dir;/repos/Cloud</filename> and mirror the contents
        from the first &productname; &productnumber; installation medium to
        this directory. See <xref linkend="sec-depl-adm-conf-repos-product"/>
        for instructions. Once done, you can use the newly created directory as
        a source for &sle_repo;.
        </para>
        -->
      </step>
      <step>
       <para>
        Check for other (custom) repositories. All &slsa; SP1 repositories need
        to be replaced with the respective &slsa; SP2 version. In case no SP2
        version exists, disable the repository&mdash;the respective packages
        from that repository will be deleted during the upgrade.
       </para>
      </step>
     </substeps>
     <para>
      Once the repository configuration on the &admserv; has been updated, run
      the command to check the repositories again. If the configuration is
      correct, the result should look like the following:
     </para>
     <screen>&prompt.root;crowbarctl upgrade repocheck crowbar
+---------------------+--------------------------------+
| Status              | Value                          |
+---------------------+--------------------------------+
| os.available        | true                           |
| os.repos            | SLES12-SP2-Pool                |
|                     | SLES12-SP2-Updates             |
| openstack.available | true                           |
| openstack.repos     | SUSE-OpenStack-Cloud-7-Pool    |
|                     | SUSE-OpenStack-Cloud-7-Updates |
+---------------------+--------------------------------+</screen>
    </step>
    <step>
     <para>
      Now that the repositories are available, the &admserv; itself will be
      upgraded. The update will run in the background using <command>zypper
      dup</command>. Once all packages have been upgraded, the &admserv; will
      be rebooted and you will be logged out. To start the upgrade run:
     </para>
     <screen>&prompt.root;crowbarctl upgrade admin</screen>
    </step>
    <step>
     <para>
      Starting with &productname; 7, &crow; uses a &postgres; database to store
      its data. With this step, the database is created on the
      &admserv;. Alternatively, a database on a remote host can be used.
     </para>
     <para>
      To create the database on the &admserv; proceed as follows:
     </para>
     <substeps>
      <step>
       <para>
        Login to the &admserv;.
       </para>
      </step>
      <step>
       <para>
        To create the database on the &admserv; with the default credentials
        (<literal>crowbar</literal>/<literal>crowbar</literal>) for the
        database, run
       </para>
       <screen>&prompt.root;crowbarctl upgrade database new </screen>
       <para>
        To use a different user name and password, run the following command
        instead:
       </para>
       <screen>&prompt.root;crowbarctl upgrade database new \
--db-username=<replaceable>USERNAME</replaceable> --db-password=<replaceable>PASSWORD</replaceable></screen>
      </step>
      <step>
       <para>
        To connect to an existing &postgres; database, use the following
        command rather than creating a new database:
       </para>
    <screen>&prompt.root;crowbarctl upgrade database connect --db-username=<replaceable>USERNAME</replaceable> \
--db-password=<replaceable>PASSWORD</replaceable> --database=<replaceable>DBNAME</replaceable> \
--host=<replaceable>IP_or_FQDN</replaceable> --port=<replaceable>PORT</replaceable></screen>
      </step>
     </substeps>
    </step>
    <step>
     <para>
      After the &admserv; has been successfully updated, the &contrnode;s and
      &compnode;s will be upgraded. At first the availability of the
      repositories used to provide packages for the &cloud; nodes is tested.
     </para>

     <note>
       <title>Correct Metadata in the PTF Repository</title>
       <para>
         When adding new repositories to the nodes, make sure that the new PTF
         repository also contains correct metadata (even if it is empty). To do
         this, run the <command>createrepo-cloud-ptf</command> command.
        </para>
</note>

     <para>
      Note that the configuration for these repositories differs from the one
      for the &admserv; that was already done in a previous step. In this step
      the repository locations are made available to &crow; rather than to
      libzypp on the &admserv;. To check the repository configuration run the
      following command:
     </para>
     <screen>&prompt.root;crowbarctl upgrade repocheck nodes
+---------------------------------+--------------------------------+
| Status                          | Value                          |
+---------------------------------+--------------------------------+
| ha.available                    | false                          |
| ha.repos                        | SLES12-SP2-HA-Pool             |
|                                 | SLES12-SP2-HA-Updates          |
| ha.errors.x86_64.missing        | SLES12-SP2-HA-Pool             |
|                                 | SLES12-SP2-HA- Updates         |
| os.available                    | false                          |
| os.repos                        | SLES12-SP2-Pool                |
|                                 | SLES12-SP2-Updates             |
| os.errors.x86_64.missing        | SLES12-SP2-Pool                |
|                                 | SLES12-SP2-Updates             |
| openstack.available             | false                          |
| openstack.repos                 | SUSE-OpenStack-Cloud-7-Pool    |
|                                 | SUSE-OpenStack-Cloud-7-Updates |
| openstack.errors.x86_64.missing | SUSE-OpenStack-Cloud-7-Pool    |
|                                 | SUSE-OpenStack-Cloud-7-Updates |
+---------------------------------+--------------------------------+</screen>
     <para>
      To update the locations for the listed repositories, start <command>yast
      crowbar</command> and proceed as described in <xref
      linkend="sec-depl-adm-inst-crowbar-repos"/>.
     </para>
     <para>
      Once the repository configuration for &crow; has been updated, run the
      command to check the repositories again to determine, whether the current
      configuration is correct.
     </para>
     <screen>&prompt.root;crowbarctl upgrade repocheck nodes
+---------------------+--------------------------------+
| Status              | Value                          |
+---------------------+--------------------------------+
| ha.available        | true                           |
| ha.repos            | SLE12-SP2-HA-Pool              |
|                     | SLE12-SP2-HA-Updates           |
| os.available        | true                           |
| os.repos            | SLES12-SP2-Pool                |
|                     | SLES12-SP2-Updates             |
| openstack.available | true                           |
| openstack.repos     | SUSE-OpenStack-Cloud-7-Pool    |
|                     | SUSE-OpenStack-Cloud-7-Updates |
+---------------------+--------------------------------+</screen>

     <important>
      <title>Shut Down Running &vmguest;s in Normal Mode</title>
      <para>
       If the upgrade is done in normal mode (prechecks compute_status and
       ha_configured have not been passed), you need to shut down all running
       &vmguest;s now.
      </para>
     </important>

     <important>
      <title>Product Media Repository Copies</title>
      <para>
       To PXE boot new nodes, an additional &cloudos; repository&mdash;a copy
       of the installation system&mdash; is required. Although not required
       during the upgrade procedure, it is recommended to set up this directory
       now. Refer to <xref linkend="sec-depl-adm-conf-repos-product"/> for
       details. If you had also copied the &productname; 6 installation media
       (optional), you may also want to provide the &productname;
       &productnumber; the same way.
      </para>
      <para>
       Once the upgrade procedure has been successfully finished, you may
       delete the previous copies of the installation media in
       <filename>/srv/tftpboot/suse-12.1/x86_64/install</filename> and
       <filename>/srv/tftpboot/suse-12.1/x86_64/repos/Cloud</filename>.
      </para>
     </important>
    </step>
    <step>
     <para>
      To ensure the status of the nodes does not change during the upgrade
      process, the majority of the &ostack; services will be stopped on the
      nodes. As a result, the &ostack; API will no longer be
      accessible. The &vmguest;s, however, will continue to run and will also
      be accessible. Run the following command:
     </para>
     <screen>&prompt.root;crowbarctl upgrade services</screen>
     <para>
      This step takes a while to finish. Monitor the process by running
      <command>crowbarctl upgrade status</command>. Do not proceed before
      <literal>steps.services.status</literal> is set to
      <literal>passed</literal>.
     </para>
    </step>
    <step>
     <para>
      The last step before upgrading the nodes is to make a backup of the
      &ostack; &postgres; database. The database dump will be stored on the
      &admserv; and can be used to restore the database in case something goes
      wrong during the upgrade.
     </para>
     <screen>&prompt.root;crowbarctl upgrade backup openstack</screen>
    </step>
    <step>
     <para>
      The final step of the upgrade procedure is upgrading the
      nodes.  To start the process, enter:
     </para>
     <screen>&prompt.root;crowbarctl upgrade nodes all</screen>
     <para>
      The upgrade process runs in the background and can be queried with
      <command>crowbarctl upgrade status</command>. Depending on the size of
      your &cloud; it may take several hours, especially when performing a
      non-disruptive update. In that case, the &compnode;s are updated
      one-by-one after &vmguest;s have been live-migrated to other nodes.
     </para>
     <para>
      Instead of upgrading all nodes you may also upgrade
      the &contrnode;s first and individual &compnode;s afterwards. Refer to
      <command>crowbarctl upgrade nodes --help</command> for details. If you
      choose this approach, you can use the <command>crowbarctl upgrade
      status</command> command to monitor the upgrade process. The output of
      this command contains the following entries:
     </para>
      <variablelist>
        <varlistentry>
          <term>
            current_node_action
          </term>
        <listitem>
          <para>
            The current action applied to the node.
          </para>
        </listitem>
        </varlistentry>
        <varlistentry>
          <term>
            current_substep
          </term>
        <listitem>
          <para>
            Shows the current substep of the node upgrade step. For example,
            for the <command>crowbarctl upgrade nodes controllers</command>,
            the <literal>current_substep</literal> entry displays the
            <literal>controller_nodes</literal> status when upgrading controllers.
          </para>
        </listitem>
        </varlistentry>
      </variablelist>
     <para>
       After the controllers have been upgraded, the
       <literal>steps.nodes.status</literal> entry in the output displays the
       <literal>running</literal> status. Check then the status of the
       <literal>current_substep_status</literal> entry. If it displays
       <literal>finished</literal>, you can move to the next step of upgrading
       the &compnode;s.
     </para>
     <para>
       When upgrading individual &compnode;s using the <command>crowbarctl
       upgrade nodes</command> <replaceable>NODE_NAME</replaceable> command, the
       <literal>current_substep_status</literal> entry changes to
       <literal>node_finished</literal> when the upgrade of a single node is
       done. After all nodes have been upgraded, the
       <literal>current_substep_status</literal> entry displays <literal>finished</literal>.
     </para>
    </step>
   </procedure>

   <note>
    <title>Dealing with Errors</title>
    <para>
     If an error occurs during the upgrade process, the output of the
     <command>crowbarctl upgrade status</command> provides a detailed
     description of the failure. In most cases, both the output and the error
     message offer enough information for fixing the issue. When the problem has
     been solved, run the previously-issued upgrade command to resume the
     upgrade process.
    </para>
   </note>
  </sect2>

 </sect1>
 <sect1 xml:id="sec-depl-maintenance-hasetup">
  <title>Upgrading to an &haSetup;</title>

  <para>
   There are a few issues to pay attention to when making an existing &cloud;
   deployment highly available (by setting
   up HA clusters and moving roles to these clusters). To make existing services
   highly available, proceed
   as follows. Note that moving to an &hasetup; cannot be done without
   &cloud; service interruption, because it requires &ostack; components
   to be restarted.
  </para>

  <important>
   <title>Team Network Mode is Required for HA</title>
   <para>
    Team network mode is required for an &hasetup; of &productname;. If
    you are planning to move your cloud to an &hasetup; at a later point in
    time, make sure to deploy &cloud; with team network mode from the
    beginning. Otherwise a migration to an &hasetup; is not supported.
   </para>
  </important>

  <procedure>
   <step>
    <para>
     Make sure to read the sections
     <xref linkend="sec-depl-arch-components-ha"/> and
     <xref linkend="sec-depl-req-ha"/> of this manual and take any
     appropriate action.
    </para>
   </step>
   <step>
    <para>
     Make the HA repositories available on the &admserv; as described in
     <xref linkend="sec-depl-adm-conf-repos-scc"/>. Run the command
     <command>chef-client</command> afterward.
    </para>
   </step>
   <step>
    <para>
     Set up your cluster(s) as described in
     <xref linkend="sec-depl-ostack-pacemaker"/>.
    </para>
   </step>
   <step>
    <para>
     To move a particular role from a regular control node to a cluster, you
     need to stop the associated service(s) before re-deploying the role on
     a cluster:
    </para>
    <substeps performance="required">
     <step>
      <para>
       Log in to each node as &rootuser; on which the role is deployed and stop
       its associated service(s) (a role can have multiple services). Do so by
       running the service's start/stop script with the stop argument, for
       example:
      </para>
<screen>&prompt.root;rcopenstack-keystone stop</screen>
      <para>
       See <xref linkend="sec-depl-services"/> for a list of roles,
       services and start/stop scripts.
      </para>
     </step>
     <step>
      <para>
       The following roles need additional treatment:
      </para>
      <variablelist>
       <varlistentry>
        <term>database-server (Database &barcl;)</term>
        <listitem>
         <orderedlist spacing="normal">
          <listitem>
           <para>
            Stop the database on the node the Database &barcl; is
            deployed with the command:
           </para>
<screen>&prompt.root;rcpostgresql stop</screen>
          </listitem>
          <listitem>
           <para>
            Copy <filename>/var/lib/pgsql</filename> to a temporary location
            on the node, for example:
           </para>
<screen>&prompt.root;cp -ax /var/lib/pgsql /tmp</screen>
          </listitem>
          <listitem>
           <para>
            Redeploy the Database &barcl; to the cluster. The original
            node may also be part of this cluster.
           </para>
          </listitem>
          <listitem>
           <para>
            Log in to a cluster node and run the following command to
            determine which cluster node runs the
            <systemitem class="resource">postgresql</systemitem> service:
           </para>
<screen>&prompt.root;crm_mon -1</screen>
          </listitem>
          <listitem>
           <para>
            Log in to the cluster node running
            <systemitem class="resource">postgresql</systemitem>.
           </para>
          </listitem>
          <listitem>
           <para>
            Stop the <systemitem class="resource">postgresql</systemitem>
            service:
           </para>
<screen>&prompt.root;crm resource stop postgresql</screen>
          </listitem>
          <listitem>
           <para>
            Copy the data backed up earlier to the cluster node:
           </para>
<screen>&prompt.root;rsync -av --delete
           <replaceable>NODE_WITH_BACKUP</replaceable>:/tmp/pgsql/ /var/lib/pgsql/</screen>
          </listitem>
          <listitem>
           <para>
            Restart the <systemitem class="resource">postgresql</systemitem>
            service:
           </para>
<screen>&prompt.root;crm resource start postgresql</screen>
          </listitem>
         </orderedlist>
         <para>
          Copy the content of <filename>/var/lib/pgsql/data/</filename> from
          the original database node to the cluster node with DRBD or shared
          storage.
         </para>
        </listitem>
       </varlistentry>
       <varlistentry>
        <term>keystone-server (&o_ident; &barcl;)</term>
        <listitem>
         <para>
          If using &o_ident; with PKI tokens, the PKI keys on all nodes
          need to be re-generated. This can be achieved by removing the
          contents of <filename>/var/cache/*/keystone-signing/</filename> on
          the nodes. Use a command similar to the following on the
          &admserv; as &rootuser;:
         </para>
<screen>&prompt.root;for NODE in <replaceable>NODE1</replaceable>
         <replaceable>NODE2</replaceable> <replaceable>NODE3</replaceable>; do
  ssh $NODE rm /var/cache/*/keystone-signing/*
done</screen>
        </listitem>
       </varlistentry>
      </variablelist>
     </step>
    </substeps>
   </step>
   <step>
    <para>
     Go to the &barcl; featuring the role you want to move to the
     cluster. From the left side of the <guimenu>Deployment</guimenu> section,
     remove the node the role is currently running on. Replace it with a
     cluster from the <guimenu>Available Clusters</guimenu> section. Then
     apply the proposal and verify that application succeeded via the
     &crow; &wi;. You can also check the cluster status via &hawk;
     or the <command>crm</command> /
     <command>crm_mon</command> CLI tools.
    </para>
   </step>
   <step>
    <para>
     Repeat these steps for all roles you want to move to cluster. See
     <xref linkend="sec-depl-reg-ha-control-spof"/> for a list of services
     with HA support.
    </para>
   </step>
  </procedure>

  <important>
   <title>SSL Certificates</title>
   <para>
    Moving to an &hasetup; also requires creating SSL certificates for
    nodes in the cluster that run services using SSL. Certificates need to
    be issued for the generated names (see
    <xref linkend="ann-depl-ostack-pacemaker-prop-name"/>) and for all
    public names you have configured in the cluster.
   </para>
  </important>

  <important>
   <title>Service Management on the Cluster</title>
   <para>
    After a role has been deployed on a cluster, its services are managed by
    the HA software. You must <emphasis>never</emphasis> manually start
    or stop an HA-managed service or configure it to start on boot. Services
    may only be started or stopped by using the cluster management tool Hawk
    or the crm shell. See
    <link xlink:href="&suse-onlinedoc;/sle-ha-12/book_sleha/data/sec_ha_config_basics_resources.html"/>
    for more information.
   </para>
  </important>
 </sect1>

 <sect1 xml:id="sec-depl-maintenance-ha-recovery">
 <title>Recovering Clusters to a Healthy State</title>
 <!--taroth 2016-02-22: depending on the differences between recovery for
  controll node clusters (bsc#966643) vs. compute node clusters (bsc#966641), an additional
 structure level may be needed in the future -->
 <para>If one node in your cluster refuses to rejoin the cluster, it is most
  likely that the node has not been shut down cleanly. This can either happen
  because of manual intervention or because the node has been fenced (shut down)
  by the &stonith; mechanism of the cluster. Fencing is used to protect the
  integrity of data in case of a split-brain scenario.</para>
 <para>The following sections refer to problems with the &contrnode;s
  cluster and show how to recover your degraded cluster to full strength. This
  takes the following basic steps:</para>
 <procedure>
  <step>
   <!--XREF: Recovering the Pacemaker Cluster-->
   <para>
    <xref linkend="sec-deploy-ha-recovery-contr-node-add"
     xrefstyle="select:title"/>
   </para>
  </step>
  <step>
   <!--XREF: Recovering Crowbar and Chef-->
   <para>
    <xref linkend="sec-deploy-ha-recovery-contr-crow-chef"
     xrefstyle="select:title"/>
   </para>
  </step>
  <step>
   <!--XREF: Cleaning Up Resources-->
   <para>In addition, you may need to reset resource failcounts to
    allow resources to start on the node you have re-added to the cluster. See
     <xref linkend="sec-deploy-ha-recovery-cleanup"/>.</para>
  </step>
  <step>
   <!--XREF: Removing Maintenance Mode from the Node-->
   <para>In addition, you may need to manually remove the maintenance mode flag
    from a node. See <xref linkend="sec-deploy-ha-recovery-maint"/>.</para>
  </step>
 </procedure>
 <para>
   For a list of possible symptoms that help you to diagnose a
   degraded cluster, see <xref
   linkend="sec-deploy-ha-recovery-contr-symptoms"/>.</para>


 <sect2 xml:id="sec-deploy-ha-recovery-contr-symptoms">
  <title>Symptoms of a Degraded &contrnode; Cluster</title>
  <para>The following incidents may occur if a &contrnode; in your cluster
   has been shut down in an unclean state:</para>
  <itemizedlist>
   <listitem>
    <para>A VM reboots although the &cloud; administrator did not trigger
     this action.</para>
   </listitem>
   <listitem>
    <para>One of the &contrnode; in the &crow; &wi; is in status
      <literal>Problem</literal>, signified by a red dot next to the
     node.</para>
   </listitem>
   <listitem>
    <para>The &hawk; &wi; stops responding on one of the
     &contrnode;s, while it is still responding on the others.</para>
   </listitem>
   <listitem>
    <para>The SSH connection to one of the &contrnode;s freezes.</para>
   </listitem>
   <listitem>
    <para>The &ostack; components stop responding for a short while.</para>
   </listitem>
  </itemizedlist>
 </sect2>

 <sect2 xml:id="sec-deploy-ha-recovery-contr-node-add">
  <title>Re-adding the Node to the Cluster</title>
  <procedure>
   <step>
    <para>Reboot the node.</para>
   </step>
   <step>
    <para>Connect to the node via SSH from the &admserv;.</para>
   </step>
   <step>
    <para>
     If you have a 2-node cluster with the <xref
     linkend="var-depl-ostack-pacemaker-corosync-fencing"/> option set
     to <guimenu>Automatic</guimenu>, remove the block file that is created on a
     node during start of the cluster service:
    </para>
    <screen>&prompt.root;rm /var/spool/corosync/block_automatic_start</screen>
    <para>The block file avoids &stonith; deathmatches for 2-node clusters
     (where each node kills the other one, resulting in both nodes rebooting all
     the time). When &corosync; shuts down cleanly, the block file is automatically removed.
     Otherwise the block file is still present and prevents the cluster service
     from (re-)starting on that node.</para>
    <para>
     Alternatively, bypass the block file by starting the cluster service on the
     cluster node before reconnecting the node to Crowbar in the next section
     (see <xref
     linkend="sec-deploy-ha-recovery-contr-crow-chef"/>):
    </para>
    <screen>&prompt.root;<command>systemctl</command> start pacemaker</screen>
    </step>
   </procedure>
 </sect2>

 <sect2 xml:id="sec-deploy-ha-recovery-contr-crow-chef">
  <title>Recovering &crow; and &chef;</title>
  <para>Making the Pacemaker node rejoin the cluster is not enough. All nodes in
   the cloud (including the &admserv;) need to be aware that this node is
   back online. This requires the following steps for &crow; and
   &chef;:</para>
  <procedure>
   <step>
    <para>Log in to the node you have re-added to the cluster.</para>
   </step>
   <step>
    <para>Re-register the node with &crow; by executing:</para>
    <screen>&prompt.root;<command>systemctl</command> start crowbar_join</screen>
   </step>
   <step>
    <para>Log in to each of the <emphasis>other</emphasis>
     &contrnode;s.</para>
   </step>
   <step>
    <para>Trigger a &chef; run:</para>
    <screen>&prompt.root;<command>chef-client</command></screen>
   </step>
  </procedure>
 </sect2>

 <sect2 xml:id="sec-deploy-ha-recovery-cleanup">
  <title>Cleaning Up Resources</title>
  <para>
   A resource will be automatically restarted if it fails, but each
   failure increases the resource's failcount. If a
   <literal>migration-threshold</literal> has been set for the resource,
   the node will no longer run the resource when the number of failures
   reaches the migration threshold.
   <remark>aspiers 2016-03-18: todo: actually, this is
   not true until https://trello.com/c/0wmqMP52 is completed</remark>
   To allow the resource to start again on the node, reset
   the resource's failcount by cleaning up the resource manually. You can clean
   up individual resources by using the &hawk; &wi; or all in one go as
   described below:
  </para>
  <procedure>
   <step>
     <para>
      Log in to any one of the cluster nodes which is currently online
      in the cluster.  (You can check this via <command>crm_mon</command>.)
     </para>
   </step>
   <step>
    <para>Clean-up all stopped resources with the following command:</para>
    <screen>&prompt.root;crm_resource -o | \
  awk '/\tStopped |Timed Out/ { print $1 }' | \
  xargs -r -n1 crm resource cleanup</screen>
   </step>
  </procedure>
 </sect2>

 <sect2 xml:id="sec-deploy-ha-recovery-maint">
  <title>Removing the Maintenance Mode Flag from a Node</title>
  <para>During normal operation, chef-client sometimes needs to place a node
   into maintenance mode. The node is kept in maintenance mode until the
   chef-client run finishes. However, if the chef-client run fails, the node may
   be left in maintenance mode. In that case, the cluster management tools like
   crmsh or &hawk; will show all resources on that node as
    <literal>unmanaged</literal>. To remove the maintenance flag:</para>
  <procedure>
   <step>
    <para>Log in the cluster node.</para>
   </step>
   <step>
    <para>Disable the maintenance mode with:</para>
    <screen>&prompt.root;crm node ready</screen>
   </step>
  </procedure>
 </sect2>
 <sect2 xml:id="sec-deploy-ha-recovery-drbd">
  <title>
   Recovering From an Unresolvable DRBD Split Brain Situation
  </title>
  <para>
   Although policies to automatically resolve a DRBD split brain situations
   exist, there are situations which require to be resolved manually. Such a
   situation is indicated by a Kernel message like:
  </para>
  <screen>kernel: block drbd0: Split-Brain detected, dropping connection!</screen>
  <para>
   To resolve the split brain you need to choose a node which data
   modifications will be discarded. These modifications will be replaced by
   the data from the <quote>healthy</quote> node and will not be recoverable,
   so make sure to choose the right node. If in doubt, make a backup of the
   node before starting the recovery process. Proceed as follows:
  </para>
  <procedure>
   <step>
    <para>
     Put the cluster in maintenance mode:
    </para>
    <screen>&prompt.root;crm configure property maintenance-mode=true</screen>
   </step>
   <step>
    <para>
     Check if the chosen node is in primary role by running either
     <command>drbd-overview</command> or <command>drbdadm status</command> command.
</para>
   </step>
   <step>
    <para>
     If it is in primary role, stop all services using this
     resource and switch it to secondary role. If the node
     already is in secondary role, skip this step.
    </para>
    <screen>&prompt.root;drbdadm secondary <replaceable>RESOURCE</replaceable></screen>
   </step>
   <step>
    <para>
     Check if a node is in state <literal>WFConnection</literal> by looking at
     the output of <command>systemctl status drbd</command>.
    </para>
   </step>
   <step>
    <para>
     If the node is in state <literal>WFConnection</literal> disconnect
     the resource:
    </para>
    <screen>&prompt.root;drbdadm disconnect <replaceable>RESOURCE</replaceable></screen>
   </step>
   <step>
    <para>
     Discard all modifications on the chosen node. This step is
     irreversible, the modifications on the chosen node will be lost!
    </para>
    <screen>&prompt.root;drbdadm -- --discard-my-data connect <replaceable>RESOURCE</replaceable></screen>
   </step>
   <step>
    <para>
     If the other (healthy) node is in state
     <literal>WFConnection</literal>, synchronization to the chosen node
     will start automatically. If not, reconnect the healthy node to start
     the synchronization:
    </para>
    <screen>&prompt.root;drbdadm connect <replaceable>RESOURCE</replaceable></screen>
    <para>
     During the synchronization all data modifications on the chosen node
     will be overwritten with the data from the healthy node.
    </para>
   </step>
   <step>
    <para>
     When the synchronization has finished, reset the cluster to normal mode:
    </para>
    <screen>&prompt.root;crm configure property maintenance-mode=false</screen>
   </step>
  </procedure>
  </sect2>
 </sect1>
 <sect1>
  <title>Updating MariaDB with Galera</title>
  <para>
   When using Pacemaker, updating MariaDB with Galera must be done
   manually. Crowbar will not install updates automatically. In particular, this
   situation applies to upgrades to MariaDB 10.2.17 or higher from MariaDB
   10.2.16 or earlier. See <link
   xlink:href="https://mariadb.com/kb/en/library/mariadb-10222-release-notes/">MariaDB
   10.2.22 Release Notes - Notable Changes</link>.
  </para>
  <para>
   Using the Pacemaker GUI, update MariaDB with the following procedure:
  </para>
  <procedure>
   <step>
    <para>
     Put the cluster into maintenance mode. Detailed information about the
     Pacemaker GUI and its operation is available in the <link
     xlink:href="https://www.suse.com/documentation/sle_ha/singlehtml/book_sleha/book_sleha.html#cha.ha.configuration.gui">SLE
     High Availability documentation</link>.
    </para>
   </step>
   <step>
    <para>
     Perform a rolling upgrade to MariaDB following the instructions at <link
     xlink:href="https://mariadb.com/kb/en/library/upgrading-between-minor-versions-with-galera-cluster/">Upgrading
     Between Minor Versions with Galera Cluster</link>.
    </para>
     <para>
     The process involves the following steps:
    </para>
    <substeps>
     <step>
      <para>
       Stop MariaDB
      </para>
     </step>
     <step>
      <para>
       Uninstall the old versions of MariaDB and the Galera wsrep provider
      </para>
     </step>
     <step>
      <para>
       Install the new versions of MariaDB and the Galera wsrep provider
      </para>
     </step>
     <step>
      <para>
       Change configuration options if necessary
      </para>
     </step>
     <step>
      <para>
       Start MariaDB
      </para>
     </step>
     <step>
      <para>
       Run <command>mysql_upgrade</command> with the
       <literal>--skip-write-binlog</literal> option
      </para>
     </step>
    </substeps>
   </step>
   <step>
    <para>
     Each node must upgraded individually so that the cluster is always
     operational.
    </para>
   </step>
   <step>
    <para>
     Using the Pacemaker GUI, take the cluster out of maintenance mode.
    </para>
   </step>
  </procedure>
  <para>
   Using the CLI, update MariaDB with the following procedure:
  </para>
  <procedure>
   <step>
    <para>
     Mark Galera as unmanaged:
    </para>
    <screen>crm resource unmanage galera</screen>
    <para>
     Or put the whole cluster into maintenance mode:
    </para>
    <screen>crm configure property maintenance-mode=true</screen>
   </step>
   <step>
    <para>
     Pick a node other than the one currently targeted by the load balancer and
     stop MariaDB on that node:
    </para>
    <screen>crm_resource --wait --force-demote -r galera -V</screen>
   </step>
   <step>
    <para>
     Perform updates with the following steps:
    </para>
    <substeps>
     <step>
      <para>
       Uninstall the old versions of MariaDB and the Galera wsrep provider.
      </para>
     </step>
     <step>
      <para>
       Install the new versions of MariaDB and the Galera wsrep
       provider. Select the appropriate instructions at <link
       xlink:href="https://mariadb.com/kb/en/library/installing-mariadb-with-zypper/">Installing
       MariaDB with zypper</link>.
      </para>
     </step>
     <step>
      <para>
       Change configuration options if necessary.
      </para>
     </step>
    </substeps>
   </step>
     <step>
      <para>
       Start MariaDB on the node.
      </para>
      <screen>crm_resource --wait --force-promote -r galera -V</screen>
     </step>
     <step>
      <para>
       Run <command>mysql_upgrade</command> with the
       <literal>--skip-write-binlog</literal> option.
      </para>
     </step>
     <step>
      <para>
       On the other nodes, repeat the process detailed above: stop MariaDB,
       perform updates, start MariaDB, run <command>mysql_upgrade</command>.
      </para>
     </step>
     <step>
      <para>
       Mark Galera as managed:
      </para>
      <screen>crm resource manage galera</screen>
      <para>
       Or take the cluster out of maintenance mode.
      </para>
     </step>
  </procedure>
 </sect1>

 <sect1 xml:id="sec-depl-maintenance-upgrade-faq">
  <title>FAQ</title>

  <para>
    The following FAQ section offers tips and tricks related to the upgrade procedure.
  </para>
    <qandaset defaultlabel="qanda">
        <qandaentry>
          <question>
            <para>
              How can I override default timeouts for specific upgrade actions?
            </para>
          </question>
          <answer>
            <para>
              It is possible to override the default timeout values for several
              upgrade actions by creating the <systemitem>/etc/crowbar/upgrade_timeouts.yml</systemitem>
              file and specify the desired actions and their values. Below is a
              list of supported settings and their brief descriptions.
            </para>
            <variablelist>
              <varlistentry>
                <term>:prepare_repositories</term>
                <listitem>
                <para>
                  Delete the old repositories and set the new repositories required for the upgrade
                </para>
                </listitem>
              </varlistentry>
              <varlistentry>
                <term>:pre_upgrade</term>
                <listitem>
                <para>
                  Prepare node resources for the upgrade
                </para>
                </listitem>
              </varlistentry>
              <varlistentry>
                <term>:upgrade_os</term>
                <listitem>
                <para>
                  Update all system packages
                </para>
                </listitem>
              </varlistentry>
              <varlistentry>
                <term>:post_upgrade</term>
                <listitem>
                <para>
                  Restore DRBD/Pacemaker actions after the packages have been upgraded to their original state
                </para>
                </listitem>
              </varlistentry>
              <varlistentry>
                <term>:evacuate_host</term>
                <listitem>
                <para>
                  Migrate all running instances from the &compnode; that is going to be upgraded
                </para>
                </listitem>
              </varlistentry>
              <varlistentry>
                <term>:chef_upgraded</term>
                <listitem>
                <para>
                  Rejoin &crow; and run the chef-client
                </para>
                </listitem>
              </varlistentry>
              <varlistentry>
                <term>:router_migration</term>
                <listitem>
                <para>
                  Migrate all &o_netw; routers
                </para>
                </listitem>
              </varlistentry>
              <varlistentry>
                <term>:delete_pacemaker_resources</term>
                <listitem>
                <para>
                  Delete old pacemaker resources in order to generate new ones for the upgrade
                </para>
                </listitem>
              </varlistentry>
              <varlistentry>
                <term>:delete_cinder_services</term>
                <listitem>
                <para>
                  Delete &o_blockstore; services in order to generate new ones for the upgrade
                </para>
                </listitem>
              </varlistentry>
            </variablelist>
            <para>
              Below is the list of supported options with their default values
              in seconds:
            </para>
            <screen>:prepare_repositories: 120
:pre_upgrade: 300
:upgrade_os: 900
:post_upgrade: 600
:evacuate_host: 300
:chef_upgraded: 900
:router_migration: 600
:delete_pacemaker_resources: 300
:delete_cinder_services: 300</screen>
          </answer>
         </qandaentry>
      </qandaset>
 </sect1>

 <sect1 xml:id="sec-depl-maintenance-backup-admin">
  <title>Backing Up and Restoring the &admserv;</title>
  <para>
   Backing Up and Restoring the &admserv; can either be done via the &crow;
   &wi; or on the &admserv;'s command line via the <command>crowbarctl
   backup</command> command. Both tools provide the same functionality.
  </para>

  <sect2 xml:id="sec-depl-maintenance-backup-admin-ui">
   <title>Backup and Restore via the &crow; &wi;</title>
   <para>
    To use the Web interface for backing up and restoring the &admserv;, go to
    the &crow; &wi; on the &admserv;, for example
    <literal>http://192.168.124.10/</literal>. Log in as user <systemitem
    class="username">crowbar</systemitem>. The password is
    <literal>crowbar</literal> by default, if you have not changed it. Go to
    <menuchoice> <guimenu>Utilities</guimenu> <guimenu>Backup &amp; Restore</guimenu>
    </menuchoice>.
   </para>
   <figure>
    <title>Backup and Restore: Initial Page View</title>
    <mediaobject>
     <imageobject role="fo">
      <imagedata fileref="depl_backup_initial.png" width="75%" format="PNG"/>
     </imageobject>
     <imageobject role="html">
      <imagedata fileref="depl_backup_initial.png" width="100%" format="PNG"/>
     </imageobject>
    </mediaobject>
   </figure>
   <para>
       To create a backup, click the <guimenu>Create Backup Image</guimenu> button.
       Provide a descriptive name (allowed characters are letters, numbers,
       dashes and underscores) and confirm with <guimenu>Create Backup</guimenu>.
       Alternatively, you can upload a backup, for example from a previous
       installation.
   </para>
   <para>
    Existing backups are listed with name and creation date. For each backup,
    three actions are available:
   </para>
   <variablelist>
    <varlistentry>
     <term><guimenu>Download</guimenu></term>
     <listitem>
      <para>
       Download a copy of the backup file. The TAR archive you receive with
       this download can be uploaded again via <guimenu>Upload Backup
       Image</guimenu>.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term><guimenu>Restore</guimenu></term>
     <listitem>
      <para>
       Restore the backup.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term><guimenu>Delete</guimenu></term>
     <listitem>
      <para>
       Delete the backup.
      </para>
     </listitem>
    </varlistentry>
   </variablelist>
   <figure>
    <title>Backup and Restore: List of Backups</title>
    <mediaobject>
     <imageobject role="fo">
      <imagedata fileref="depl_backup_list.png" width="75%" format="PNG"/>
     </imageobject>
     <imageobject role="html">
      <imagedata fileref="depl_backup_list.png" width="75%" format="PNG"/>
     </imageobject>
    </mediaobject>
   </figure>

  </sect2>

  <sect2 xml:id="sec-depl-maintenance-backup-admin-cli">
   <title>Backup and Restore via the Command Line</title>
   <para>
    Backing up and restoring the &admserv; from the command line can be done
    with the command <command>crowbarctl backup</command>. For getting general
    help, run the command <command>crowbarctl --help backup</command>, help on
    a subcommand is available by running <command>crowbarctl
    <replaceable>SUBCOMMAND</replaceable> --help</command>.
    The following commands for creating and managing backups exist:
   </para>
   <variablelist>
    <varlistentry>
     <term>
      <command>crowbarctl backup create
      <replaceable>NAME</replaceable></command>
     </term>
     <listitem>
      <para>
       Create a new backup named <replaceable>NAME</replaceable>. It will be
       stored at <filename>/var/lib/crowbar/backup</filename>.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term><command>crowbarctl backup [--yes] <replaceable>NAME</replaceable></command></term>
     <listitem>
      <para>
       Restore the backup named <replaceable>NAME</replaceable>. You will be
       asked for confirmation before any existing proposals will get
       overwritten. If using the option <option>--yes</option>, confirmations
       are tuned off and the restore is forced.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term><command>crowbarctl backup delete <replaceable>NAME</replaceable></command></term>
     <listitem>
      <para>
       Delete the backup named <replaceable>NAME</replaceable>.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term><command>crowbarctl backup download <replaceable>NAME</replaceable>
     <replaceable>[FILE]</replaceable></command></term>
     <listitem>
      <para>
       Download the backup named <replaceable>NAME</replaceable>. If you
       specify the optional <replaceable>[FILE]</replaceable>, the download is
       written to the specified file. Otherwise it is saved to the current
       working directory with an automatically generated file name. If
       specifying <literal>-</literal> for <replaceable>[FILE]</replaceable>,
       the output is written to STDOUT.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term><command>crowbarctl backup list</command></term>
     <listitem>
      <para>
       List existing backups. You can optionally specify different
       output formats and filters&mdash;refer to <command>crowbarctl backup
       list --help</command> for details.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>
      <command>crowbarctl backup upload
      <replaceable>FILE</replaceable></command>
     </term>
     <listitem>
      <para>
       Upload a backup from <replaceable>FILE</replaceable>.
      </para>
     </listitem>
    </varlistentry>
   </variablelist>
  </sect2>
 </sect1>
</chapter>
