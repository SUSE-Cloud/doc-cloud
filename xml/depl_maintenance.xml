<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE chapter
[
  <!ENTITY % entities SYSTEM "entity-decl.ent">
  %entities;
]>
<chapter xmlns="http://docbook.org/ns/docbook" xmlns:xi="http://www.w3.org/2001/XInclude" xmlns:xlink="http://www.w3.org/1999/xlink" version="5.0" xml:id="cha.depl.maintenance">
 <title>&cloud; Maintenance</title>
 <info>
<dm:docmanager xmlns:dm="urn:x-suse:ns:docmanager">
    <dm:maintainer>fs</dm:maintainer>
    <dm:status>editing</dm:status>
    <dm:deadline/>
    <dm:priority/>
    <dm:translation>no</dm:translation>
    <dm:languages/>
</dm:docmanager>
</info>
<para/>
 <sect1 xml:id="sec.depl.maintenance.updates">
  <title>Keeping the Nodes Up-To-Date</title>
  <para>
   Keeping the nodes in &productname; up-to-date requires an appropriate
   setup of the update and pool repositories and the deployment of
   either the <guimenu>Updater</guimenu> &barcl; or the &susemgr;
   &barcl;. For details, see
   <xref linkend="sec.depl.adm_conf.repos.scc"/>, <xref
   linkend="sec.depl.inst.nodes.post.updater"/>, and
   <xref linkend="sec.depl.inst.nodes.post.manager"/>.
  </para>

  <para>
   If one of those &barcl;s is deployed, patches are installed on the
   nodes. Patches that do not require a reboot will not cause a service
   interruption. If a patch (for example, a kernel
   update) requires a reboot after the installation, services running on the
   machine that is rebooted will not be available within &cloud;.
   Therefore it is strongly recommended to install those patches during a
   maintenance window.
  </para>

  <note>
   <title>No Maintenance Mode</title>
   <para>
    As of &productname; &productnumber; it is not possible to put
    &cloud; into <quote>Maintenance Mode</quote>.
   </para>
  </note>

  <remark condition="clarity">
   2013-10-02 - fs: The following is mainly based on assumptions...
  </remark>

  <variablelist>
   <title>Consequences when Rebooting Nodes</title>
   <varlistentry>
    <term>&admserv;</term>
    <listitem>
     <para>
      While the &admserv; is offline, it is not possible to deploy new
      nodes. However, rebooting the &admserv; has no effect on starting
      &vmguest;s or on &vmguest;s already running.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>&contrnode;s</term>
    <listitem>
     <para>
      The consequences a reboot of a &contrnode; depend on the
      services running on that node:
     </para>
     <formalpara>
      <title>Database, &o_ident;, RabbitMQ, &o_img;, &o_comp;:</title>
      <para>
       No new &vmguest;s can be started.
      </para>
     </formalpara>
     <formalpara>
      <title>&o_objstore;:</title>
      <para>
       No object storage data is available. If &o_img; uses
       &o_objstore;, it will not be possible to start new &vmguest;s.
      </para>
     </formalpara>
     <formalpara>
      <title>&o_blockstore;, &ceph;:</title>
      <para>
       No block storage data is available.
      </para>
     </formalpara>
     <formalpara>
      <title>&o_netw;:</title>
      <para>
       No new &vmguest;s can be started. On running &vmguest;s the
       network will be unavailable.
      </para>
     </formalpara>
     <formalpara>
      <title>&o_dash;</title>
      <para>
       &o_dash; will be unavailable. Starting and managing &vmguest;s
       can be done with the command line tools.
      </para>
     </formalpara>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>&compnode;s</term>
    <listitem>
     <para>
      <remark condition="clarity">
       2013-10-02 - fs: How to ensure no new instances are started on a compute
       node while evacuating it? What about Windows compute nodes? What about
       &vmware;?
      </remark>
      Whenever a &compnode; is rebooted, all &vmguest;s running on
      that particular node will be shut down and must be manually restarted.
      Therefore it is recommended to <quote>evacuate</quote> the node by
      migrating &vmguest;s to another node, before rebooting it.
<!-- (see ???
      for details) -->
     </para>
    </listitem>
   </varlistentry>
  </variablelist>
 </sect1>

 <sect1 xml:id="sec.depl.maintenance.service_order">
  <title>Service Order on &cloud; Start-up or Shutdown</title>

  <para>
   In case you need to restart your complete &cloud; (after a complete shut
   down or a power outage), ensure that the external &ceph; cluster is started,
   available and healthy. Start then the nodes and services in the
   following order:
  </para>

  <orderedlist>
   <title>Service Order on Start-up</title>
   <listitem>
    <para>
     &contrnode;/Cluster on which the Database is deployed
    </para>
   </listitem>
   <listitem>
    <para>
     &contrnode;/Cluster on which RabbitMQ is deployed
    </para>
   </listitem>
   <listitem>
    <para>
     &contrnode;/Cluster on which &o_ident; is deployed
    </para>
   </listitem>
   <listitem>
    <para>
     For &o_objstore;:
    </para>
    <orderedlist>
     <listitem>
      <para>
       &stornode; on which the <literal>swift-storage</literal> role is deployed
      </para>
     </listitem>
     <listitem>
      <para>
       &stornode; on which the <literal>swift-proxy</literal> role is deployed
      </para>
     </listitem>
    </orderedlist>
   </listitem>
   <listitem>
    <para>
     Any remaining &contrnode;/Cluster. The following additional rules apply:
    </para>
    <itemizedlist>
     <listitem>
      <para>
       The &contrnode;/Cluster on which the <literal>neutron-server</literal>
       role is deployed needs to be started before starting the node/cluster
       on which the <literal>neutron-l3</literal> role is deployed.
      </para>
     </listitem>
     <listitem>
      <para>
       The &contrnode;/Cluster on which the <literal>nova-controller</literal>
       role is deployed needs to be started before starting the node/cluster
       on which &o_orch; is deployed.
      </para>
     </listitem>
    </itemizedlist>
   </listitem>
   <listitem>
    <para>
     &compnode;s
    </para>
   </listitem>
  </orderedlist>

  <para>
   If multiple roles are deployed on a single &contrnode;, the services are
   automatically started in the correct order on that node. If you have more
   than one node with multiple roles, make sure they are
   started as closely as possible to the order listed above.
  </para>

  <para>
   If you need to shut down &cloud;, the nodes and services need to be
   terminated in reverse order than on start-up:
  </para>

  <orderedlist>
   <title>Service Order on Shut-down</title>
   <listitem>
    <para>
     &compnode;s
    </para>
   </listitem>
   <listitem>
    <para>
     &contrnode;/Cluster on which &o_orch; is deployed
    </para>
   </listitem>
   <listitem>
    <para>
     &contrnode;/Cluster on which the <literal>nova-controller</literal>
     role is deployed
    </para>
   </listitem>
   <listitem>
    <para>
     &contrnode;/Cluster on which the <literal>neutron-l3</literal>
     role is deployed
    </para>
   </listitem>
   <listitem>
    <para>
     All &contrnode;(s)/Cluster(s) on which neither of the following services
     is deployed: Database, RabbitMQ, and &o_ident;.
    </para>
   </listitem>
   <listitem>
    <para>
     For &o_objstore;:
    </para>
    <orderedlist>
     <listitem>
      <para>
       &stornode; on which the <literal>swift-proxy</literal> role is
       deployed
      </para>
     </listitem>
     <listitem>
      <para>
       &stornode; on which the <literal>swift-storage</literal> role is
       deployed
      </para>
     </listitem>
    </orderedlist>
   </listitem>
   <listitem>
    <para>
     &contrnode;/Cluster on which &o_ident; is deployed
    </para>
   </listitem>
   <listitem>
    <para>
     &contrnode;/Cluster on which RabbitMQ is deployed
    </para>
   </listitem>
   <listitem>
    <para>
     &contrnode;/Cluster on which the Database is deployed
    </para>
   </listitem>
   <listitem>
    <para>
     If required, gracefully shut down an external &ceph; cluster
    </para>
   </listitem>
  </orderedlist>
 </sect1>

 <sect1 xml:id="sec.depl.maintenance.upgrade">
  <title>Upgrading from &productname; 7 to &productname; 8</title>

  <para>
   Upgrading from &productname; 7 to &productname; 8 can be done either via a
   &wi; or from the command line. A <quote>non-disruptive</quote> update is
   supported when the requirements listed at <xref
   linkend="list.depl.maintenance.upgrade.non-disruptive"/> are met. The
   non-disruptive upgrade provides a fully-functional &cloud; operation
   during most of the upgrade procedure.
  </para>
  <para>
   If the requirements for a non-disruptive upgrade are not met, the
   upgrade procedure will be done in normal mode. When
   live-migration is set up, &vmguest;s will be migrated to another node
   before the respective &compnode; is updated to ensure continuous
   operation.
 </para>

  <important>
    <title>STONITH and &admserv;</title>
    <para>
      Make sure that the STONITH mechanism in your cloud does not rely on the
      state of the &admserv; (for example, no SBD devices are located there,
      and IPMI is not using the network connection relying on the
      &admserv;). Otherwise, this may affect the clusters when the &admserv; is
      rebooted during the upgrade procedure.
     </para>
    </important>

  <sect2 xml:id="sec.depl.maintenance.upgrade.require">
   <title>Requirements</title>
   <para>
    When starting the upgrade process, several checks are performed to
    determine whether the &cloud; is in an upgradeable state and whether a
    non-disruptive update would be supported:
   </para>

   <itemizedlist>
    <title>General Upgrade Requirements</title>
    <listitem>
     <para>
      All nodes need to have the latest &productname; 7 updates <emphasis
      role="bold">and</emphasis> the latest &slsa; 12 SP2 updates installed. If
      this is not the case, refer to <xref
      linkend="sec.depl.inst.nodes.post.updater"/> for instructions on how to
      update.
     </para>
    </listitem>
    <listitem>
     <para>
      All allocated nodes need to be turned on and have to be in state
      <quote>ready</quote>.
     </para>
    </listitem>
    <listitem>
     <para>
      All &barcl; proposals need to have been successfully deployed. If a
      proposal is in state <quote>failed</quote>, the upgrade procedure will
      refuse to start. Fix the issue or&mdash;if possible&mdash;remove the
      proposal.
     </para>
    </listitem>
    <listitem>
     <para>
      If the &pacemaker; &barcl; is deployed, all clusters
      need to be in a healthy state.
     </para>
    </listitem>
    <listitem>
     <para> The upgrade will not start when &ceph; is deployed via &crow;. Only
     external &ceph; is supported. Documentation for &ses; is available at
     <link
     xlink:href="https://www.suse.com/documentation/suse-enterprise-storage-5/"/>.
     </para>
    </listitem>
    <listitem>
     <para>
      Upgrade is only possible if the <literal>SQL
      Engine</literal> in the <literal>Database</literal> &barcl; is set to
      <guimenu>&mariadb;</guimenu>. For further info, see <xref linkend="sec.depl.maintenance.postgre-mariadb.upgrade"/>
     </para>
    </listitem>
    <listitem>
     <para>
      The following repositories need to be available on a server that is
      accessible from the &admserv;. The HA repositories are only needed if you
      have an &hasetup;. It is recommended to use the same server that also
      hosts the respective repositories of the current version.
     </para>
     <simplelist>
      <member vendor="suse-crow"><literal>&cloud_repo;-Pool</literal></member>
      <member vendor="suse-crow"><literal>&cloud_repo;-Update</literal></member>
      <member><literal>&sle_repo;-Pool</literal></member>
      <member><literal>&sle_repo;-Update</literal></member>
      <member>
       <literal>&sleha_repo;-Pool</literal> (for &hasetup;s only)
      </member>
      <member>
       <literal>&sleha_repo;-Update</literal> (for &hasetup;s only)
      </member>
     </simplelist>
     <important>
      <para>
      Do not add repositories to the &cloud; repository configuration. This
      needs to be done during the upgrade procedure.
      </para>
     </important>
    </listitem>
    <listitem>
     <para>
       A non-disruptive upgrade is not supported if &o_blockstore; has been
       deployed with the <literal>raw devices</literal> or <literal>local
       file</literal> back-end. In this case, you have to perform a regular
       upgrade, or change the &o_blockstore; back-end for a non-disruptive
       upgrade.
     </para>
    </listitem>
    <listitem>
     <para>
       If &ses; is now deployed using &crow;, it should be migrated to an
       external cluster. You may want to upgrade &ses;, please refer to <link xlink:href="https://www.suse.com/documentation/suse-enterprise-storage-5/book_storage_deployment/data/ceph_upgrade_4to5crowbar.html">&ses;
       Upgrade Instructions</link>.
     </para>
    </listitem>
    <listitem>
      <para>
	Run the command <command>nova-manage db archive_deleted_rows</command> to purge deleted instances from the database table. This can significanly reduce time required for the database migration procedure.
      </para>
    </listitem>
    <listitem>
      <para>
	Run the commands <command>cinder-manage db purge</command> and <command>heat-manage purge_deleted</command> to purge database entries that are marked as deleted.
      </para>
    </listitem>
   </itemizedlist>

   <itemizedlist xml:id="list.depl.maintenance.upgrade.non-disruptive">
    <title>Non-Disruptive Upgrade Requirements</title>
    <listitem>
     <para>
      All &contrnode;s need to be set up highly available.
     </para>
    </listitem>
    <listitem>
     <para>
      A non-disruptive upgrade is not supported if the &o_blockstore;
      has been deployed with the <literal>raw devices</literal> or
      <literal>local file</literal> back-end. In this case, you have to perform
      a regular upgrade, or change the &o_blockstore; back-end for a
      non-disruptive upgrade.
     </para>
    </listitem>
    <listitem>
     <para>
      A non-disruptive upgrade is prevented if the
      <literal>cinder-volume</literal> service is placed on &compnode;. For a
      non-disruptive upgrade, <literal>cinder-volume</literal> should either be
      HA-enabled or placed on non-compute nodes.
     </para>
    </listitem>
    <listitem>
     <para>
      A non-disruptive upgrade is prevented if <literal>manila-share</literal>
      service is placed on a &compnode;. For more information, see <xref
      linkend="sec.depl.ostack.manila"/>
     </para>
    </listitem>
    <listitem>
     <para>
      Live-migration support needs to be configured and enabled for the
      &compnode;s. The amount of free resources (CPU and RAM) on the
      &compnode;s needs to be sufficient to evacuate the nodes one by one.
     </para>
    </listitem>
    <listitem>
     <para>
       In case of a non-disruptive upgrade, &o_img; must be configured as a
       shared storage if the <guimenu>Default Storage
       Store</guimenu> value in the &o_img; is set to <literal>File</literal>.
     </para>
    </listitem>
    <listitem>
     <para>
       For a non-disruptive upgrade, only KVM-based &compnode;s with
       the <literal>nova-computer-kvm</literal> role are allowed in &productname; 7.
     </para>
    </listitem>
    <listitem>
     <para>
       Non-disruptive upgrade is limited to the following cluster
       configurations:
     </para>
       <itemizedlist>
       <listitem>
         <para>
           Single cluster that has all supported controller roles on it
         </para>
       </listitem>
       <listitem>
         <para>
          Two clusters where one only has
          <systemitem>neutron-network</systemitem> and the other one has the
          rest of the controller roles.
         </para>
       </listitem>
       <listitem>
         <para>
          Two clusters where one only has
          <systemitem>neutron-server</systemitem> plus
          <systemitem>neutron-network</systemitem> and the other one has the
          rest of the controller roles.
         </para>
       </listitem>
       <listitem>
         <para>
           Two clusters, where one cluster runs the database and RabbitMQ
         </para>
       </listitem>
       <listitem>
         <para>
           Three clusters, where one cluster runs database and RabbitMQ,
           another cluster runs APIs, and the third cluster has the
           <systemitem>neutron-network</systemitem> role.
         </para>
       </listitem>
       </itemizedlist>
       <para>
       If your cluster configuration is not supported by the non-disruptive
       upgrade procedure, you can still perform a normal upgrade.
     </para>
    </listitem>
   </itemizedlist>
  </sect2>

  <sect2 xml:id="sec.depl.maintenance.postgre-mariadb.upgrade">
   <title>Preparing &postgres;-Based &productname; 7 for Upgrade</title>
   <para>
    Upgrading &productname; 7 is only possible when it uses &mariadb; deployed
    with the Database &barcl;. This means that before you can proceed with
    upgrading &productname; 7, you must migrate &postgres; to &mariadb; first. The following description covers several possible scenarios.
   </para>
   <sect3 xml:id="sec.postgre-mariadb.upgrade.scenario1">
    <title>Non-HA Setup or HA Setup with More Than 2 Nodes in the Cluster and
    &postgres; Database Backend</title>
    <para>
     Install the latest
     maintenance updates on &productname; 7. In the
     &crow; &wi;, switch to the <guimenu>Database</guimenu> &barcl;. You should
     see the new <systemitem>mysql-server</systemitem> role in the
     <guimenu>Deployment</guimenu> section. Do not change the
     <literal>sql_engine</literal> at this point. Add your &dbnode; or cluster to the
     <systemitem>mysql-server</systemitem> role and apply the
     &barcl;. &mariadb; is now deployed and running, but it is still not used
     as a back end for &ostack; services.
    </para>
    <para>
     Follow <xref linkend="postgre.mariadb.data.migration"/> to migrate the data from &postgres; to &mariadb;.
    </para>
    <procedure xml:id="postgre.mariadb.data.migration">
     <title>Data Migration</title>
     <step>
      <para>
           Run the <systemitem>/opt/dell/bin/prepare-mariadb</systemitem>
	   script on the &admnode; to prepare the &mariadb; instance by creating
	   the required users, databases, and tables.
      </para>
     </step>
     <step>
      <para>
       After the script is finished, you'll find a list of all databases and
       URLs that are ready for data migration in the
       <filename>/etc/pg2mysql/databases.yaml</filename> located on one of the &dbnode;s. The script's output
      may look as follows:</para>
      <screen>Preparing node d52-54-77-77-01-01.vo6.cloud.suse.de
Adding recipe[database::pg2mariadb_preparation] to run_list
Running chef-client on d52-54-77-77-01-01.vo6.cloud.suse.de...
Log: /var/log/crowbar/db-prepare.chef-client.log on
d52-54-77-77-01-01.vo6.cloud.suse.de Run time: 444.725193199s
Removing recipe[database::pg2mariadb_preparation] from run_list
Prepare completed for d52-54-77-77-01-01.vo6.cloud.suse.de
Summary of used databases: /etc/pg2mysql/databases.yaml on
d52-54-77-77-01-01.vo6.cloud.suse.de</screen>
      <para>The <literal>Summary of used databases:</literal> line shows the
      exact location of the <filename>/etc/pg2mysql/databases.yaml</filename> file.
      </para>
      <para>
       The <filename>/etc/pg2mysql/databases.yaml</filename> file contains a
       list of databases along with their source and target connection strings:
      </para>
<screen>keystone:
  source: postgresql://keystone:vZn3nfxXzv97@192.168.243.87/keystone
  target: mysql+pymysql://keystone:vZn3nfxXzv97@192.168.243.88/keystone?charset=utf8
glance:
  source: postgresql://glance:cOau7NhaA54N@192.168.243.87/glance
  target: mysql+pymysql://glance:cOau7NhaA54N@192.168.243.88/glance?charset=utf8
cinder:
  source: postgresql://cinder:idRll2gJPodv@192.168.243.87/cinder
  target: mysql+pymysql://cinder:idRll2gJPodv@192.168.243.88/cinder?charset=utf8</screen>
     </step>
     <step>
      <para>
       Install the <package>python-psql2mysql</package> package on the &dbnode;
       (preferably the one with the <filename>/etc/pg2mysql/databases.yaml</filename> file).
      </para>
     </step>
     <step>
      <para>
       To determine whether the &postgres; databases contain data that cannot
       be migrated to &mariadb;, run <command>psql2mysql</command> with the
      <literal>precheck</literal> option:</para>
<screen>&prompt.user;psql2mysql \
--source postgresql://keystone:vZn3nfxXzv97@192.168.243.87/keystone \
--target mysql+pymysql://keystone:vZn3nfxXzv97@192.168.243.88/keystone?charset=utf8 \
precheck</screen>
      <para>
       To run precheck operation on all databases in a single operation, use the
       <literal>--batch</literal> option and the
       <filename>/etc/pg2mysql/databases.yaml</filename> file as follows:
      </para>
<screen>&prompt.user;psql2mysql --batch /etc/pg2mysql/databases.yaml precheck</screen>
      <para>
       If the precheck indicates that there is data that cannot be imported
       into &mariadb;, modify the offending data manually to fix
       the problems. The example below shows what an output containing issues may
       look like:
      </para>
 <screen>&prompt.user;psql2mysql --source postgresql://cinder:idRll2gJPodv@192.168.243.86/cinder precheck
Table 'volumes' contains 4 Byte UTF8 characters which are incompatible with the 'utf8' encoding used by MariaDB
The following rows are affected:
+-----------------------------------------+-----------------+-------+
|               Primary Key               | Affected Column | Value |
+-----------------------------------------+-----------------+-------+
| id=5c6b0274-d18d-4153-9fda-ef3d74ab4500 |   display_name  |   💫   |
+-----------------------------------------+-----------------+-------+
Error during prechecks. 4 Byte UTF8 characters found in the source database.</screen>
     </step>
     <step>
      <para>
       Stop chef-client services on the nodes, to prevent regular runs of
       chef-client from starting database-related &ostack; services again. To
       do this from the &admnode;, you can use the <command>knife ssh roles:dns-client systemctl
       stop chef-client</command> command. Stop
       all &ostack; services that make use of the database to prevent them from
       writing new data during the migration.
      </para>
      <note>
       <title>Testing Migration Procedure</title>
       <para>
	If you want to perform a dry run of the migration procedure, you can run the
       <command>psql2mysql migrate</command> without stopping the
       database-related &ostack; services. This way, if the test migration
       fails due to errors that weren't caught by the precheck procedure, you
       can fix them with &ostack; services still running, thus minimizing the
       required downtime. When you perform the actual migration, the data in
       the target databases will be replaced with the latest one in the source databases.
       </para>
       <para>
	After the test migration and before the actual migration, it is recommended to run the
	<command>psql2mysql purge-tables</command> command to purge tables in
	the target database. While this step is optional, it speeds up the
	migration process.
       </para>
      </note>
      <para>
       On an HA setup, shut down all services that make use of the
       database. To do this, use the <command>crm</command> command for
       example:
      </para>
<screen>crm resource stop apache2 keystone cinder-api glance-api \
      neutron-server swift-proxy nova-api magnum-api sahara-api heat-api ceilometer-collector</screen>   
      <note>
       <para>
	If the <literal>Manage stateless active/active services with
	Pacemaker</literal> option in the &pacemaker; &barcl; is set to
	<literal>false</literal>, the &ostack; services must be stopped on each
	cluster node using the <command>systemctl</command> command.
       </para>
      </note>
      <para>
       <emphasis>From this point, &ostack; services
       become unavailable.</emphasis>
      </para>
     </step>
     <step>
      <para>
       You can now migrate databases using the psql2mysql tool. However, before
       performing the migration, make sure that target database nodes have
       enough free space to accommodate the migrated data. To upgrade a single
       database, use the following command format:
      </para>
<screen>&prompt.user;psql2mysql \
--source postgresql://neutron:secret@192.168.1.1/neutron \
--target mysql+pymysql://neutron:evenmoresecret@192.168.1.2/neutron?charset=utf8 \
migrate</screen>
      <para>
       To migrate all databases in one operation, use the
       <literal>--batch</literal> option and the
       <filename>/etc/pg2mysql/databases.yaml</filename> file as follows:
      </para>
<screen>&prompt.user;psql2mysql --batch /etc/pg2mysql/databases.yaml migrate</screen>
     </step>
     <step>
      <para>
       In the &crow; &wi;, switch to the <guimenu>Database</guimenu>
       &barcl;. Enable the raw view and set the value of
       <literal>sql_engine</literal> to <literal>mysql</literal>. Apply the
       &barcl;. After this step, &ostack; services should be running again and
       reconfigured to use the &mariadb; database back end.
      </para>
     </step>
     <step>
      <para>
        To prevent the &postgres;-related chef code from running, unassign the values
	from <literal>database-server</literal> role in the <guimenu>Database</guimenu>
       &barcl;, and apply the &barcl;.
      </para>
     </step>
     <step>
      <para>
       Start chef-client services on the nodes again.
      </para>
     </step>
     <step>
      <para>
       Stop &postgres; on the &dbnode;s. Uninstall &postgres; packages.
      </para>
      <substeps>
       <step>
	<para>
	 To stop the <literal>postgresql</literal> service, run the following
	 command on one cluster node:
	</para>
<screen>&prompt.user;crm resource stop postgresql
&prompt.user;crm resource stop fs-postgresql
&prompt.user;crm resource stop drbd-postgresql</screen>
        <para>
	 Run the last command only if the previous setup used DRBD.
	</para>
       </step>
       <step>
	<para>
	 Remove the packages on all cluster nodes:
	</para>
	<screen>&prompt.root;zypper rm postgresql94 postgresql94-server</screen>
       </step>
       <step>
	<para>
	 If you choose not to upgrade to &productname; 8 right away, delete unused pacemaker resource from one cluster node:
	</para>
<screen>&prompt.user;crm conf delete drbd-postgresql
&prompt.user;crm conf delete fs-postgresql
&prompt.user;crm conf delete postgresql</screen>

<note>
 <para>
  Run the <command>crm conf delete drbd-postgresql</command> command only
  if the cloud setup your are upgrading uses DRBD.
 </para>
</note>

       </step>
      </substeps>
     </step>
     <step>
      <para>
       If DRBD is not used as a backend for &rabbit;, it is possible to remove it at this point, using the following command:
      </para>
<screen>&prompt.sudo;zypper rm drbd drbd-utils</screen>
      <para>
       You can then reclaim the disk space used by &crow; for DRBD. To do this, edit the node data using <systemitem>knife</systemitem>:
      </para>
<screen>&prompt.user;knife node edit -a <replaceable>DRBD_NODE</replaceable></screen>
      <para>
        Search for <literal>claimed_disks</literal> and remove the entry with owner set to <literal>LVM_DRBD</literal>.
      </para>
      <para>
       Otherwise, skip this step until after the full upgrade is done, since the &rabbit;
       setup will be automatically switched from DRBD during the upgrade procedure.
      </para>
     </step>
    </procedure>
   </sect3>
   <sect3 xml:id="sec.postgre-mariadb.upgrade.scenario2">
    <title>HA Cluster with 2 &contrnode;s</title>
     <para>
     Before your proceed, extend the 2-node cluster with additional node that
     has no role assigned to
     it. Make sure that the new node has enough memory to serve as a &contrnode;.
     </para>
    <procedure>
    <step>
     <para>
      In &crow; &wi;, switch to the <guimenu>&pacemaker;</guimenu> &barcl;,
     enable the raw view mode, find the <literal>allow_larger_cluster</literal>
     option, and set it value to <literal>true</literal>. Note that this
     is relevant only for DRBD clusters.
     </para>
    </step>
    <step>
     <para>
      Add the <literal>pacemaker-cluster-member</literal> role to the new node
      and apply the &barcl;.
     </para>
    </step>
    <step>
     <para>
      Proceed with the migration procedure as described in
     <xref linkend="sec.postgre-mariadb.upgrade.scenario1"/>.
     </para>
    </step>
    </procedure>
   </sect3>
  </sect2>

  <sect2 xml:id="sec.depl.maintenance.upgrade.ui">
   <title>Upgrading Using the Web Interface</title>
   <para>
    The &wi; features a wizard that guides you through the upgrade
    procedure.
   </para>
   <note>
    <title>Canceling Upgrade</title>
    <para>
     You can cancel the upgrade process by clicking <guimenu>Cancel
     Upgrade</guimenu>. Note that the upgrade operation can be canceled only
     before the &admserv; upgrade is started. When the upgrade has been
     canceled, the nodes return to the ready state. However any user
     modifications must be undone manually. This includes reverting repository
     configuration.
    </para>
   </note>
   <procedure>
    <step>
     <para>
      To start the upgrade procedure, open the &crow; &wi; on the &admserv; and choose <menuchoice>
      <guimenu>Utilities</guimenu> <guimenu>Upgrade</guimenu>
      </menuchoice>. Alternatively, point the browser directly to the upgrade
      wizard on the &admserv;, for example
      <literal>http://192.168.124.10/upgrade/</literal>.
     </para>
    </step>
    <step>
     <para>
      On the first screen of the &wi; you will run preliminary checks, get
      information about the upgrade mode and start the upgrade process.
     </para>
     <informalfigure>
      <mediaobject>
       <imageobject role="fo">
        <imagedata fileref="depl_upgrade7-8_prepare.png" width="75%"/>
       </imageobject>
       <imageobject role="html">
        <imagedata fileref="depl_upgrade7-8_prepare.png"/>
       </imageobject>
      </mediaobject>
     </informalfigure>
    </step>
    <step>
     <para>
      Perform the preliminary checks to determine whether the upgrade
      requirements are met by clicking <guimenu>Check</guimenu> in
      <literal>Preliminary Checks</literal>.
     </para>
     <para>
      The &wi; displays the progress of the checks. Make sure all checks are
      passed (you should see a green marker next to each check). If errors
      occur, fix them and run the <guimenu>Check</guimenu> again. Do not
      proceed until all checks are passed.
     </para>
    </step>
    <step>
     <para>
      When all checks in the previous step have passed, <literal>Upgrade
      Mode</literal> shows the result of the upgrade analysis. It will indicate
      whether the upgrade procedure will continue in non-disruptive or in
      normal mode.
     </para>
    </step>
    <step>
     <para>
      To start the upgrade process, click <guimenu>Begin Upgrade</guimenu>.
     </para>
    </step>
    <step>
     <para>
      While the upgrade of the &admserv; is prepared, the upgrade wizard
      prompts you to <guimenu>Download the Backup of the
      &admserv;</guimenu>. When the backup is done, move it to a safe place. If
      something goes wrong during the upgrade procedure of the &admserv;, you
      can restore the original state from this backup using the
      <command>crowbarctl backup restore
      <replaceable>NAME</replaceable></command> command.
     </para>
    </step>

    <step>
     <para>
      Check that the repositories required for upgrading the &admserv; are
      available and updated. To do this, click the <guimenu>Check</guimenu>
      button. If the checks fail, add the software repositories as described in
      <xref linkend="cha.depl.repo_conf" /> of the Deployment Guide. Run the
      checks again, and click <guimenu>Next</guimenu>.
     </para>
     <informalfigure>
      <mediaobject>
       <imageobject role="fo">
        <imagedata fileref="depl_upgrade7-8_repocheck-admin.png" width="75%"/>
       </imageobject>
       <imageobject role="html">
        <imagedata fileref="depl_upgrade7-8_repocheck-admin.png"/>
       </imageobject>
      </mediaobject>
     </informalfigure>
    </step>

    <step>
      <para>
        Click <guimenu>Upgrade Administration Server</guimenu> to upgrade and
        reboot the admin node. Note that this operation may take a while. When
        the &admserv; has been updated, click <guimenu>Next</guimenu>.
      </para>
     <informalfigure>
      <mediaobject>
       <imageobject role="fo">
        <imagedata fileref="depl_upgrade7-8_admin.png" width="75%"/>
       </imageobject>
       <imageobject role="html">
        <imagedata fileref="depl_upgrade7-8_admin.png"/>
       </imageobject>
      </mediaobject>
     </informalfigure>
    </step>

    <step>
     <para>
      Check that the repositories required for upgrading all nodes are
      available and updated.  To do this click the <guimenu>Check</guimenu>
      button. If the check fails, add the software repositories as described in
      <xref linkend="cha.depl.repo_conf" /> of the Deployment Guide. Run the
      checks again, and click <guimenu>Next</guimenu>.
     </para>
     <informalfigure>
      <mediaobject>
       <imageobject role="fo">
        <imagedata fileref="depl_upgrade7-8_repocheck-nodes.png" width="75%"/>
       </imageobject>
       <imageobject role="html">
        <imagedata fileref="depl_upgrade7-8_repocheck-nodes.png"/>
       </imageobject>
      </mediaobject>
     </informalfigure>
    </step>

    <step>
     <para>
      Stop the &ostack; services. Before you proceed, be aware that no changes
      can be made to your cloud during and after stopping the services. The
      &ostack; API will not be available until the upgrade process is
      completed. When you are ready, click <guimenu>Stop
      Services</guimenu>. Wait until the services are stopped and click
      <guimenu>Next</guimenu>.
      </para>
    </step>

    <step>
      <para>
        Before upgrading the nodes, the wizard prompts you to <guimenu>Back up
        OpenStack Database</guimenu>. The &mariadb; database backup will be
        stored on the &admserv;. It can be used to restore the database in case
        something goes wrong during the upgrade. To back up the database, click
        <guimenu>Create Backup</guimenu>. When the backup operation is
        finished, click <guimenu>Next</guimenu>.
      </para>
    </step>

    <step>
      <para>
        Start the upgrade by clicking <guimenu>Upgrade Nodes</guimenu>. The
        number of nodes determines how long the upgrade process will take. When
        the upgrade is completed, press <guimenu>Finish</guimenu> to return to
        the Dashboard.
      </para>
     <informalfigure>
      <mediaobject>
       <imageobject role="fo">
        <imagedata fileref="depl_upgrade7-8_finished.png" width="75%"/>
       </imageobject>
       <imageobject role="html">
        <imagedata fileref="depl_upgrade7-8_finished.png"/>
       </imageobject>
      </mediaobject>
     </informalfigure>
    </step>
   </procedure>
   <note>
   <para>
    With this first maintenance update, only systems already using &mariadb; as
    their OpenStack database will be able to upgrade.  In a future maintenance
    update, there will be a way to migrate from &postgres; to &mariadb; so
    &postgres; users will be able to upgrade.
   </para>
   </note>
   <note>
    <title>Dealing with Errors</title>
    <para>
     If an error occurs during the upgrade process, the wizard displays a
     message with a description of the error and a possible solution. After
     fixing the error, re-run the step where the error occurred.
    </para>
   </note>
  </sect2>

  <sect2 xml:id="sec.depl.maintenance.upgrade.cmdl">
   <title>Upgrading from the Command Line</title>
   <para>
    The upgrade procedure on the command line is performed by using the program
    <command>crowbarctl</command>. For general help, run <command>crowbarctl
    help</command>. To get help on a certain subcommand, run
    <command>crowbarctl <replaceable>COMMAND</replaceable> help</command>.
   </para>
   <para>
    To review the process of the upgrade procedure, you may call
    <command>crowbarctl upgrade status</command> at any time. Steps may have
    three states: <literal>pending</literal>, <literal>running</literal>, and
    <literal>passed</literal>.
   </para>

   <procedure>
    <step>
     <para>
      To start the upgrade procedure from the command line, log in to the
      &admserv; as &rootuser;.
     </para>
    </step>
    <step>
     <para>
      Perform the preliminary checks to determine whether the upgrade
      requirements are met:
     </para>
     <screen>&prompt.root;crowbarctl upgrade prechecks</screen>
     <para>
      The command's result is shown in a table. Make sure the column
      <guimenu>Errors</guimenu> does not contain any entries. If there are
      errors, fix them and restart the <command>precheck</command> command
      afterwards. Do not proceed before all checks are passed.
     </para>
<screen>&prompt.root;crowbarctl upgrade prechecks
+-------------------------------+--------+----------+--------+------+
| Check ID                      | Passed | Required | Errors | Help |
+-------------------------------+--------+----------+--------+------+
| network_checks                | true   | true     |        |      |
| cloud_healthy                 | true   | true     |        |      |
| maintenance_updates_installed | true   | true     |        |      |
| compute_status                | true   | false    |        |      |
| ha_configured                 | true   | false    |        |      |
| clusters_healthy              | true   | true     |        |      |
+-------------------------------+--------+----------+--------+------+</screen>
     <para>
      Depending on the outcome of the checks, it is automatically decided
      whether the upgrade procedure will continue in non-disruptive or in
      normal mode.
     </para>

     <tip>
      <title>Forcing Normal Mode Upgrade</title>
      <para>
       The non-disruptive update will take longer than an upgrade in normal
       mode, because it performs certain tasks in parallel which are done
       sequentially during the non-disruptive upgrade. Live-migrating guests to
       other &compnode;s during the non-disruptive
       upgrade takes additional time.
      </para>
      <para>
       Therefore, if a non-disruptive upgrade is not a requirement for you, you
       may want to switch to the normal upgrade mode, even if your setup
       supports the non-disruptive method. To force the normal upgrade mode,
       run:
      </para>
      <screen>&prompt.root;crowbarctl upgrade mode normal</screen>
      <para>
       To query the current upgrade mode run:
      </para>
      <screen>&prompt.root;crowbarctl upgrade mode</screen>
      <para>
       To switch back to the non-disruptive mode run:
      </para>
      <screen>&prompt.root;crowbarctl upgrade mode non_disruptive</screen>
      <para>
       It is possible to call this command at any time during the upgrade
       process until the <literal>services</literal> step is started. After
       that point the upgrade mode can no longer be changed.
      </para>
     </tip>
    </step>

    <step>
     <para>
      Prepare the nodes by transitioning them into the <quote>upgrade</quote>
      state and stopping the chef daemon:
     </para>
     <screen>&prompt.root;crowbarctl upgrade prepare</screen>
     <para>
      Depending of the size of your &cloud; deployment, this step may take
      some time. Use the command <command>crowbarctl upgrade status</command>
      to monitor the status of the process named
      <literal>steps.prepare.status</literal>. It needs to be in state
      <literal>passed</literal> before you proceed:
     </para>
     <screen>&prompt.root;crowbarctl upgrade status
+--------------------------------+----------------+
| Status                         | Value          |
+--------------------------------+----------------+
| current_step                   | backup_crowbar |
| current_substep                |                |
| current_substep_status         |                |
| current_nodes                  |                |
| current_node_action            |                |
| remaining_nodes                |                |
| upgraded_nodes                 |                |
| crowbar_backup                 |                |
| openstack_backup               |                |
| suggested_upgrade_mode         | non_disruptive |
| selected_upgrade_mode          |                |
| compute_nodes_postponed        | false          |
| steps.prechecks.status         | passed         |
| steps.prepare.status           | passed         |
| steps.backup_crowbar.status    | pending        |
| steps.repocheck_crowbar.status | pending        |
| steps.admin.status             | pending        |
| steps.repocheck_nodes.status   | pending        |
| steps.services.status          | pending        |
| steps.backup_openstack.status  | pending        |
| steps.nodes.status             | pending        |
+--------------------------------+----------------+</screen>
    </step>
    <step>
     <para>
      Create a backup of the existing &admserv; installation. In case something
      goes wrong during the upgrade procedure of the &admserv; you can restore
      the original state from this backup with the command <command>crowbarctl
      backup restore <replaceable>NAME</replaceable></command>
     </para>
     <screen>&prompt.root;crowbarctl upgrade backup crowbar</screen>
     <para>
      To list all existing backups including the one you have just created, run
      the following command:
     </para>
     <screen>&prompt.root;crowbarctl backup list
+----------------------------+--------------------------+--------+---------+
| Name                       | Created                  | Size   | Version |
+----------------------------+--------------------------+--------+---------+
| crowbar_upgrade_1534864741 | 2018-08-21T15:19:03.138Z | 219 KB | 4.0     |
+----------------------------+--------------------------+--------+---------+</screen>
    </step>
    <step>
     <para>
      This step prepares the upgrade of the &admserv; by checking the
      availability of the update and pool repositories for &productname;
      &productnumber; and &cloudos;. Run the following command:
     </para>
     <screen>&prompt.root;crowbarctl upgrade repocheck crowbar
+----------------------------------------+-------------------------------------+-----------+
| Repository                             | Status                              | Type      |
+----------------------------------------+-------------------------------------+-----------+
| SLE12-SP3-HA-Pool                      | missing (x86_64), inactive (x86_64) | ha        |
| SLE12-SP3-HA-Updates                   | available                           | ha        |
| SLES12-SP3-Pool                        | available                           | os        |
| SLES12-SP3-Updates                     | available                           | os        |
| SUSE-OpenStack-Cloud-Crowbar-8-Pool    | available                           | openstack |
| SUSE-OpenStack-Cloud-Crowbar-8-Updates | available                           | openstack |
+----------------------------------------+-------------------------------------+-----------+</screen>
     <para>
      The output above indicates that the <literal>SLE12-SP3-HA-Pool</literal>
      repository is  missing, because it has
      not yet been added to the &crow; configuration. To add it to the
      &admserv; proceed as follows.
     </para>
     <para>
      Note that this step is for setting up the repositories for the &admserv;,
      not for the nodes in &cloud; (this will be done in a subsequent step).
     </para>
     <substeps>
      <step>
       <para>
        Start <command>yast repositories</command> and proceed with
        <guimenu>Continue</guimenu>. Replace the repositories
        <literal>SLES12-SP2-Pool</literal> and
        <literal>SLES12-SP2-Updates</literal> with the respective SP3
        repositories.
       </para>
       <para>
        If you prefer to use zypper over &yast;, you may alternatively make the
        change using <command>zypper mr</command>.
       </para>
      </step>
      <step>
       <para>
        Next, replace the <literal>SUSE-OpenStack-Cloud-7</literal> update and
        pool repositories with the respective &productname; &productnumber;
        versions.
       </para>
      </step>
      <step>
       <para>
        Check for other (custom) repositories. All &slsa; SP2 repositories need
        to be replaced with the respective &slsa; SP3 version. In case no SP3
        version exists, disable the repository&mdash;the respective packages
        from that repository will be deleted during the upgrade.
       </para>
      </step>
     </substeps>
     <para>
      Once the repository configuration on the &admserv; has been updated, run
      the command to check the repositories again. If the configuration is
      correct, the result should look like the following:
     </para>
     <screen>&prompt.root;crowbarctl upgrade repocheck crowbar
+---------------------+----------------------------------------+
| Status              | Value                                  |
+---------------------+----------------------------------------+
| os.available        | true                                   |
| os.repos            | SLES12-SP3-Pool                        |
|                     | SLES12-SP3-Updates                     |
| openstack.available | true                                   |
| openstack.repos     | SUSE-OpenStack-Cloud-Crowbar-8-Pool    |
|                     | SUSE-OpenStack-Cloud-Crowbar-8-Updates |
+---------------------+----------------------------------------+</screen>
    </step>
    <step>
     <para>
      Now that the repositories are available, the &admserv; itself will be
      upgraded. The update will run in the background using <command>zypper
      dup</command>. Once all packages have been upgraded, the &admserv; will
      be rebooted and you will be logged out. To start the upgrade run:
     </para>
     <screen>&prompt.root;crowbarctl upgrade admin</screen>
    </step>
    <step>
     <para>
      After the &admserv; has been successfully updated, the &contrnode;s and
      &compnode;s will be upgraded. At first the availability of the
      repositories used to provide packages for the &cloud; nodes is tested.
     </para>

     <note>
       <title>Correct Metadata in the PTF Repository</title>
       <para>
         When adding new repositories to the nodes, make sure that the new PTF
         repository also contains correct metadata (even if it is empty). To do
         this, run the <command>createrepo-cloud-ptf</command> command.
       </para>
     </note>

     <para>
      Note that the configuration for these repositories differs from the one
      for the &admserv; that was already done in a previous step. In this step
      the repository locations are made available to &crow; rather than to
      libzypp on the &admserv;. To check the repository configuration run the
      following command:
     </para>
     <screen>&prompt.root;crowbarctl upgrade repocheck nodes
+---------------------------------+----------------------------------------+
| Status                          | Value                                  |
+---------------------------------+----------------------------------------+
| ha.available                    | false                                  |
| ha.repos                        | SLES12-SP3-HA-Pool                     |
|                                 | SLES12-SP3-HA-Updates                  |
| ha.errors.x86_64.missing        | SLES12-SP3-HA-Pool                     |
|                                 | SLES12-SP3-HA- Updates                 |
| os.available                    | false                                  |
| os.repos                        | SLES12-SP3-Pool                        |
|                                 | SLES12-SP3-Updates                     |
| os.errors.x86_64.missing        | SLES12-SP3-Pool                        |
|                                 | SLES12-SP3-Updates                     |
| openstack.available             | false                                  |
| openstack.repos                 | SUSE-OpenStack-Cloud-Crowbar-8-Pool    |
|                                 | SUSE-OpenStack-Cloud-Crowbar-8-Updates |
| openstack.errors.x86_64.missing | SUSE-OpenStack-Cloud-Crowbar-8-Pool    |
|                                 | SUSE-OpenStack-Cloud-Crowbar-8-Updates |
+---------------------------------+----------------------------------------+</screen>
     <para>
      To update the locations for the listed repositories, start <command>yast
      crowbar</command> and proceed as described in <xref
      linkend="sec.depl.adm_inst.crowbar.repos"/>.
     </para>
     <para>
      Once the repository configuration for &crow; has been updated, run the
      command to check the repositories again to determine, whether the current
      configuration is correct.
     </para>
     <screen>&prompt.root;crowbarctl upgrade repocheck nodes
+---------------------+----------------------------------------+
| Status              | Value                                  |
+---------------------+----------------------------------------+
| ha.available        | true                                   |
| ha.repos            | SLE12-SP3-HA-Pool                      |
|                     | SLE12-SP3-HA-Updates                   |
| os.available        | true                                   |
| os.repos            | SLES12-SP3-Pool                        |
|                     | SLES12-SP3-Updates                     |
| openstack.available | true                                   |
| openstack.repos     | SUSE-OpenStack-Cloud-Crowbar-8-Pool    |
|                     | SUSE-OpenStack-Cloud-Crowbar-8-Updates |
+---------------------+----------------------------------------+</screen>

     <important>
      <title>Shut Down Running &vmguest;s in Normal Mode</title>
      <para>
       If the upgrade is done in normal mode (prechecks compute_status and
       ha_configured have not been passed), you need to shut down all running
       &vmguest;s now.
      </para>
     </important>

     <important>
      <title>Product Media Repository Copies</title>
      <para>
       To PXE boot new nodes, an additional &cloudos; repository&mdash;a copy
       of the installation system&mdash; is required. Although not required
       during the upgrade procedure, it is recommended to set up this directory
       now. Refer to <xref linkend="sec.depl.adm_conf.repos.product"/> for
       details. If you had also copied the &productname; 6 installation media
       (optional), you may also want to provide the &productname;
       &productnumber; the same way.
      </para>
      <para>
       Once the upgrade procedure has been successfully finished, you may
       delete the previous copies of the installation media in
       <filename>/srv/tftpboot/suse-12.2/x86_64/install</filename> and
       <filename>/srv/tftpboot/suse-12.2/x86_64/repos/Cloud</filename>.
      </para>
     </important>
    </step>

    <step>
     <para>
      To ensure the status of the nodes does not change during the upgrade
      process, the majority of the &ostack; services will be stopped on the
      nodes. As a result, the &ostack; API will no longer be
      accessible. The &vmguest;s, however, will continue to run and will also
      be accessible. Run the following command:
     </para>
     <screen>&prompt.root;crowbarctl upgrade services</screen>
     <para>
      This step takes a while to finish. Monitor the process by running
      <command>crowbarctl upgrade status</command>. Do not proceed before
      <literal>steps.services.status</literal> is set to
      <literal>passed</literal>.
     </para>
    </step>
    <step>
     <para>
      The last step before upgrading the nodes is to make a backup of the
      &ostack; &postgres; database. The database dump will be stored on the
      &admserv; and can be used to restore the database in case something goes
      wrong during the upgrade.
     </para>
     <screen>&prompt.root;crowbarctl upgrade backup openstack</screen>
    </step>
    <step>
     <para>
      The final step of the upgrade procedure is upgrading the
      nodes.  To start the process, enter:
     </para>
     <screen>&prompt.root;crowbarctl upgrade nodes all</screen>
     <para>
      The upgrade process runs in the background and can be queried with
      <command>crowbarctl upgrade status</command>. Depending on the size of
      your &cloud; it may take several hours, especially when performing a
      non-disruptive update. In that case, the &compnode;s are updated
      one-by-one after &vmguest;s have been live-migrated to other nodes.
     </para>
     <para>
      Instead of upgrading all nodes you may also upgrade
      the &contrnode;s first and individual &compnode;s afterwards. Refer to
      <command>crowbarctl upgrade nodes --help</command> for details. If you
      choose this approach, you can use the <command>crowbarctl upgrade
      status</command> command to monitor the upgrade process. The output of
      this command contains the following entries:
     </para>
      <variablelist>
        <varlistentry>
          <term>
            current_node_action
          </term>
        <listitem>
          <para>
            The current action applied to the node.
          </para>
        </listitem>
        </varlistentry>
        <varlistentry>
          <term>
            current_substep
          </term>
        <listitem>
          <para>
            Shows the current substep of the node upgrade step. For example,
            for the <command>crowbarctl upgrade nodes controllers</command>,
            the <literal>current_substep</literal> entry displays the
            <literal>controller_nodes</literal> status when upgrading controllers.
          </para>
        </listitem>
        </varlistentry>
      </variablelist>
     <para>
       After the controllers have been upgraded, the
       <literal>steps.nodes.status</literal> entry in the output displays the
       <literal>running</literal> status. Check then the status of the
       <literal>current_substep_status</literal> entry. If it displays
       <literal>finished</literal>, you can move to the next step of upgrading
       the &compnode;s.
     </para>
     <para>
      <emphasis role="bold">Postponing the Upgrade</emphasis>
     </para>
     <para>
      It is possible to stop the upgrade of compute nodes and postpone their
      upgrade with the command:
     </para>
     <screen>&prompt.root;crowbarctl upgrade nodes postpone</screen>
     <para>
      After the upgrade of compute nodes is postponed, you can go to &crow;
      &wi;, check the configuration. You can also apply some changes, provided
      they do not affect the &compnode;s. During the postponed upgrade, all
      &ostack; services should be up and running. &compnode;s are still
      running old version of services.
     </para>
     <para>
      To resume the upgrade, issue the command:
     </para>
     <screen>&prompt.root;crowbarctl upgrade nodes resume</screen>
     <para>
      And finish the upgrade with either <command>crowbarctl upgrade nodes
      all</command> or upgrade nodes one node by one with <command>crowbarctl
      upgrade nodes <replaceable>NODE_NAME</replaceable></command>.
     </para>
     <para>
       When upgrading individual &compnode;s using the <command>crowbarctl
       upgrade nodes <replaceable>NODE_NAME</replaceable></command> command, the
       <literal>current_substep_status</literal> entry changes to
       <literal>node_finished</literal> when the upgrade of a single node is
       done. After all nodes have been upgraded, the
       <literal>current_substep_status</literal> entry displays <literal>finished</literal>.
     </para>
    </step>
   </procedure>

   <note>
    <title>Dealing with Errors</title>
    <para>
     If an error occurs during the upgrade process, the output of the
     <command>crowbarctl upgrade status</command> provides a detailed
     description of the failure. In most cases, both the output and the error
     message offer enough information for fixing the issue. When the problem has
     been solved, run the previously-issued upgrade command to resume the
     upgrade process.
    </para>
   </note>
  </sect2>
  <sect2 xml:id="sec.depl.maintenance.parallel.upgrade.cmdl">
   <title>Simultaneous Upgrade of Multiple Nodes</title>
   <para>
    It is possible to select more &compnode;s for selective upgrade instead of
    just one. Upgrading multiple nodes simultaneously significantly reduces the
    time required for the upgrade.
   </para>
   <para>
    To upgrade multiple nodes simultaneously, use the following command:
   </para>
<screen>&prompt.root;crowbarctl upgrade nodes <replaceable>NODE_NAME_1</replaceable>,<replaceable>NODE_NAME_2</replaceable>,<replaceable>NODE_NAME_3</replaceable></screen>
   <para>
    Node names can be separated by comma, semicolon, or space. When using
    space as separator, put the part containing node names in quotes.
   </para>
   <para>
    Use the following command to find the names of the nodes that haven't been upgraded:
   </para>
<screen>&prompt.root;crowbarctl upgrade status nodes</screen>
   <para>
    Since the simultaneous upgrade is intended to be non-disruptive, all
    &compnode;s targeted for a simultaneous upgrade must be cleared of any
   running instances.</para>
   <note>
    <para>
     You can check what instances are running on a specific
    node using the following command:
    </para>
<screen>&prompt.user;nova list --all-tenants --host <replaceable>NODE_NAME</replaceable></screen>
    </note>
   <para>
    This means that it is not possible to pick an arbitrary number of
    &compnode;s for the simultaneous upgrade operation: you have to make sure
    that it is possible to live-migrate every instance away from the batch of
    nodes that are supposed to be upgraded in parallel. In case of high load
    on all &compnode;s, it might not be possible to upgrade more than one node
    at a time. Therefore, it is recommended to perform the following steps for
    each node targeted for the simultaneous upgrade prior to running the
    <command>crowbarctl upgrade nodes</command> command.
   </para>
   <procedure>
    <step>
     <para>
      Disable the &compnode; so it's not used as a target during
      live-evacuation of any other node:
     </para>
<screen>&prompt.user;openstack compute service set --disable <replaceable>"NODE_NAME"</replaceable> nova-compute</screen>
    </step>
    <step>
     <para>
      Evacuate all running instances from the node:
     </para>
<screen>&prompt.user;nova host-evacuate-live <replaceable>"NODE_NAME"</replaceable></screen>
    </step>
   </procedure>
   <para>
    After completing these steps, you can perform a simultaneous upgrade of
    the selected nodes.
   </para>
  </sect2>
  <sect2 xml:id="sec.depl.maintenance.upgrade.troubleshooting">
   <title>Troubleshooting Upgrade Issues</title>
    <qandaset>
     <qandaentry>
      <question>
       <para>
	Upgrade of the admin server has failed.
       </para>
      </question>
      <answer>
       <para>
	Check for empty, broken, and not signed repositories in the &admserv;
	upgrade log file <filename>/var/log/crowbar/admin-server-upgrade.log</filename>. Fix the
	repository setup. Upgrade then remaining packages manually to &cloudos;
	and &cloud; &productnumber; using the command <command>zypper dup</command>. Reboot the &admserv;.
       </para>
      </answer>
     </qandaentry>
     <qandaentry>
      <question>
       <para>
	An upgrade step repeatedly fails due to timeout.
       </para>
      </question>
      <answer>
       <para>
	Timeouts for most upgrade operations can be adjusted in the
	<filename>/etc/crowbar/upgrade_timeouts.yml</filename> file. If the
	file doesn't exist, use the following template, and modify it to your needs:
       </para>
       <screen>
	:prepare_repositories: 120
        :pre_upgrade: 300
        :upgrade_os: 1500
        :post_upgrade: 600
        :shutdown_services: 600
        :shutdown_remaining_services: 600
        :evacuate_host: 300
        :chef_upgraded: 1200
        :router_migration: 600
        :lbaas_evacuation: 600
        :set_network_agents_state: 300
        :delete_pacemaker_resources: 600
        :delete_cinder_services: 300
        :delete_nova_services: 300
        :wait_until_compute_started: 60
        :reload_nova_services: 120
        :online_migrations: 1800
       </screen>
       <para>
	The following entries may require higher values (all values are
	specified in seconds):
       </para>
	<itemizedlist>
	 <listitem>
	  <para>
	   <literal>upgrade_os</literal> Time allowed for upgrading all packages of one node.
	  </para>
	 </listitem>
	 <listitem>
	  <para>
	   <literal>chef_upgraded</literal> Time allowed for initial
	   <literal>crowbar_join</literal> and <literal>chef-client</literal>
	   run on a node that has been upgraded and rebooted.
	  </para>
	 </listitem>
	 <listitem>
	  <para>
	   <literal>evacuate_host</literal> Time allowed for live migrate all VMs from a host.
	  </para>
	 </listitem>
	</itemizedlist>
      </answer>
     </qandaentry>
     <qandaentry xml:id="live.migration.failed">
      <question>
       <para>
	Node upgrade has failed during live migration.
       </para>
      </question>
      <answer>
       <para>
	The problem may occur when it is not possible to live migrate certain
	VMs anywhere. It may be necessary to shut down or suspend other VMs to
	make room for migration. Note that the Bash shell script that starts
	the live migration for the &compnode; is executed from the
	&contrnode;. An error message generated by the <command>crowbarctl
	upgrade status</command> command contains the exact names of both
	nodes. Check the <filename>/var/log/crowbar/node-upgrade.log</filename>
	file on the &contrnode; for the information that can help you with
	troubleshooting. You might also need to check &ostack-bare; logs in
	<filename>/var/log/nova</filename> on the &compnode; as well as on the
	&contrnode;s.
       </para>
       <para>
	It is possible that live-migration of a certain VM takes too long. This
	can happen if instances are very large or network connection between
	compute hosts is slow or overloaded. If this case, try to raise the
	global timeout in
	<filename>/etc/crowbar/upgrade_timeouts.yml</filename>.  
       </para>
       <para>
	We recommend to perform the live migration manually first. After it is
	completed successfully, call the <command>crowbarctl upgrade</command>
	command again.
       </para>
       <para>
	The following commands can be helpful for analyzing issues with live migrations:
       </para>
       <screen>
	nova server-migration-list
	nova server-migration-show
        nova instance-action-list
	nova instance-action
       </screen>
       <para>
	Note that these commands require &ostack-bare; administrator privileges.
       </para>
       <para>
	The following log files may contain useful information:
       </para>
       <itemizedlist>
	<listitem>
	 <para>
	  <filename>/var/log/nova/nova-compute</filename> on the &compnode;s
	  that the migration is performed from and to.
	 </para>
	</listitem>
	<listitem>
	 <para>
	  <filename>/var/log/nova/*.log</filename> (especially log files for the
	  conductor, scheduler and placement services) on the &contrnode;s.
	 </para>
	</listitem>
       </itemizedlist>
       <para>
	It can happen that active instances and instances with heavy
	loads cannot be live migrated in a reasonable time. In that case, you
	can abort a running live-migration operation using the <command>nova
	live-migration-abort <replaceable>MIGRATION-ID</replaceable></command>
	command. You can then perform the upgrade of the specific node at a
	later time.
       </para>
       <para>
	Alternatively, it is possible to force the completion of
	the live migration by using the <command>nova
	live-migration-force-complete
	<replaceable>MIGRATION-ID</replaceable></command> command. However,
	this might pause the instances for a prolonged period of time and have
	a negative impact on the workload running inside the instance.
       </para>
      </answer>
     </qandaentry>
     <qandaentry>
      <question>
       <para>
	Node has failed during OS upgrade.
       </para>
      </question>
      <answer>
       <para>
	Possible reasons include an incorrect repository setup or package
	conflicts. Check the <filename>/var/log/crowbar/node-upgrade.log</filename> log file on the
	affected node. Check the repositories on node using the <command>zypper
	lr</command> command. Make sure the required repositories are
	available. To test the setup, install a package manually or run the
	<command>zypper dup</command> command (this command is executed by the
	upgrade script). Fix the repository setup and run the failed upgrade
	step again. If custom package versions or version locks are in place,
	make sure that they don't interfere with the <command>zypper dup</command> command.
       </para>
      </answer>
     </qandaentry>
     <qandaentry>
      <question>
       <para>
	Node does not come up after reboot.
       </para>
      </question>
      <answer>
       <para>
	In some cases, a node can take too long to reboot causing a timeout. We
	recommend to check the node manually, make sure it is online, and repeat the step.
       </para>
      </answer>
     </qandaentry>
     <qandaentry>
      <question>
       <para>
	N number of nodes were provided to compute upgrade using
	<command>crowbarctl upgrade nodes node_1,node_2,...,node_N</command>,
	but less then N were actually upgraded.
       </para>
      </question>
      <answer>
       <para>
	If the live migration cannot be performed for certain nodes due to a timeout,
	&crow; upgrades only the nodes that it was able to
	live-evacuate in the specified time. Because some nodes have been upgraded, it is possible that
	more resources will be available for live-migration when you try to run this
	step again. See also <xref linkend="live.migration.failed"/>.
       </para>
      </answer>
     </qandaentry>
     <qandaentry>
      <question>
       <para>
	Node has failed at the initial chef client run stage.
       </para>
      </question>
      <answer>
       <para>
	An unsupported entry in the configuration file may prevent a service
	from starting. This causes the node to fail at the initial
	chef client run stage. Checking the
	<filename>/var/log/crowbar/crowbar_join/chef.*</filename> log files on
	the node is a good starting point.
       </para>
      </answer>
     </qandaentry>
     <qandaentry>
      <question>
       <para>
	I need to change &ostack-bare; configuration during the upgrade but I cannot access &crow;.
       </para>
      </question>
      <answer>
       <para>
	&crow; &wi; is accessible only when an upgrade is completed or
	when it is postponed. Postponing the upgrade can be done only after
	upgrading all &contrnode;s using the <command>crowbarctl upgrade nodes
	postpone</command> command. You can then access &crow; and
	save your modifications. Before you can continue with the upgrade of
	rest of the nodes, resume the upgrade using the <command>crowbarctl
	upgrade nodes resume</command> command.
       </para>
      </answer>
     </qandaentry>
     <qandaentry>
      <question>
       <para>
	Failure occurred when evacuating routers.
       </para>
      </question>
      <answer>
       <para>
	Check the <filename>/var/log/crowbar/node-upgrade.log</filename> file on
	the node that performs the router evacuation (it should be mentioned in
	the error message). The ID of the router that failed to migrate (or the
	affected network port) is logged to
	<filename>/var/log/crowbar/node-upgrade.log</filename>. Use the
	&ostack-bare; CLI tools to check the state of the affected router and
	its ports. Fix manually, if necessary. This can be done by bringing the
	router or port up and down again. The following
	commands can be useful for solving the issue:
       </para>
       <screen>
	openstack router show <replaceable>ID</replaceable>
	openstack port list --router <replaceable>ROUTER-ID</replaceable>
	openstack port show <replaceable>PORT-ID</replaceable>
	openstack port set
       </screen>
	<para>
	 Resume the upgrade by running the failed upgrade step
	again to continue with the router migration.
       </para>
      </answer>
     </qandaentry>
     <qandaentry>
      <question>
       <para>
	Some non-controller nodes were upgraded after performing <command>crowbarctl upgrade nodes
	controllers</command>.
       </para>
      </question>
      <answer>
       <para>
	In the current upgrade implementation, &ostack-bare; nodes are divided
	into &compnode;s and other nodes. The <command>crowbarctl upgrade nodes
	controllers</command> command starts the upgrade of all the nodes that
	do not host compute services. This includes the controllers.
       </para>
      </answer>
     </qandaentry>
    </qandaset>
  </sect2>
 </sect1>

 <sect1 xml:id="sec.depl.maintenance.recover.compute.node.failure">
  <title>Recovering from &compnode; Failure</title>

  <para>
   The following procedure assumes that there is at least one &compnode;
   already running. Otherwise, see
   <xref linkend="sec.depl.maintenance.bootstrap.compute.plane"/>.
  </para>

  <procedure xml:id="pro.recover.compute.node.failure">
   <title>Procedure for Recovering from &compnode; Failure</title>
   <step xml:id="st.compnode.failed.reason">
    <para>
     If the &compnode; failed, it should have been fenced. Verify that this is
     the case. Otherwise, check <filename>/var/log/pacemaker.log</filename> on
     the &dc; to determine why the &compnode; was not fenced.
     The most likely reason is a problem with STONITH devices.
    </para>
   </step>
   <step>
    <para>
     Determine the cause of the &compnode;'s failure.
    </para>
   </step>
   <step>
    <para>
     Rectify the root cause.
    </para>
   </step>
   <step>
    <para>
     Boot the &compnode; again.
    </para>
   </step>
   <step>
    <para>
     Check whether the <systemitem>crowbar_join</systemitem> script ran
     successfully on the &compnode;. If this is not the case, check the log
     files to find out the reason. Refer to
     <xref linkend="sec.deploy.logs.crownodes"/> to find the exact
     location of the log file.
    </para>
   </step>
   <step>
    <para>
     If the <systemitem>chef-client</systemitem> agent triggered by
     <systemitem>crowbar_join</systemitem> succeeded, confirm that the
     <systemitem>pacemaker_remote</systemitem> service is up and running.
    </para>
   </step>
   <step>
    <para>
     Check whether the remote node is registered and considered healthy by the
     core cluster. If this is not the case check
     <filename>/var/log/pacemaker.log</filename> on the &dc;
     to determine the cause. There should be a remote primitive running on the
     core cluster (active/passive). This primitive is responsible for
     establishing a TCP connection to the
     <systemitem>pacemaker_remote</systemitem> service on port 3121 of the
     &compnode;. Ensure that nothing is preventing this particular TCP
     connection from being established (for example, problems with NICs,
     switches, firewalls etc.). One way to do this is to run the following
     commands:
    </para>
<screen>&prompt.user;lsof -i tcp:3121
&prompt.user;tcpdump tcp port 3121
</screen>
   </step>
   <step>
    <para>
     If &pacemaker; can communicate with the remote node, it should start the
     <systemitem>nova-compute</systemitem> service on it as part of the cloned
     group <literal>cl-g-nova-compute</literal> using the NovaCompute OCF
     resource agent. This cloned group will block startup of
     <systemitem>nova-evacuate</systemitem> until at least one clone is
     started.
    </para>
    <para>
     A necessary, related but different procedure is described in
     <xref linkend="sec.depl.maintenance.bootstrap.compute.plane"/>.
    </para>
   </step>
   <step>
    <para>
     It may happen that <systemitem>NovaCompute</systemitem> has been launched
     correctly on the &compnode; by <systemitem>lrmd</systemitem>, but the
     <systemitem>openstack-nova-compute</systemitem> service is still not
     running. This usually happens when <systemitem>nova-evacuate</systemitem>
     did not run correctly.
    </para>
    <para>
     If <systemitem>nova-evacuate</systemitem> is not
     running on one of the core cluster nodes, make sure that the service is
     marked as started (<literal>target-role="Started"</literal>). If this is
     the case, then your cloud does not have any &compnode;s already running as
     assumed by this procedure.
    </para>
    <para>
     If <systemitem>nova-evacuate</systemitem> is started but it is
     failing, check the &pacemaker; logs to determine the cause.
    </para>
    <para>
     If <systemitem>nova-evacuate</systemitem> is started and
     functioning correctly, it should call &o_comp;'s
     <literal>evacuate</literal> API to release resources used by the
     &compnode; and resurrect elsewhere any VMs that died when it failed.
    </para>
   </step>
   <step>
    <para>
     If <systemitem>openstack-nova-compute</systemitem> is running, but VMs are
     not booted on the node, check that the service is not disabled or
     forced down using the <command>nova service-list</command> command. In
     case the service is disabled, run the <command>nova service-enable
     <replaceable>SERVICE_ID</replaceable></command> command. If the service is
     forced down, run the following commands:
    </para>
<screen>&prompt.user;fence_nova_param () {
    key="$1"
    cibadmin -Q -A "//primitive[@id='fence-nova']//nvpair[@name='$key']" | \
    sed -n '/.*value="/{s///;s/".*//;p}'
}
&prompt.user;fence_compute \
    --auth-url=`fence_nova_param auth-url` \
    --endpoint-type=`fence_nova_param endpoint-type` \
    --tenant-name=`fence_nova_param tenant-name` \
    --domain=`fence_nova_param domain` \
    --username=`fence_nova_param login` \
    --password=`fence_nova_param passwd` \
    -n <replaceable>COMPUTE_HOSTNAME</replaceable> \
    --action=on
</screen>
   </step>
  </procedure>

  <para>
   The above steps should be performed automatically after the node is
   booted. If that does not happen, try the following debugging techniques.
  </para>

  <para>
   Check the <literal>evacuate</literal> attribute for the &compnode; in the
   &pacemaker; cluster's <systemitem>attrd</systemitem> service using the
   command:
  </para>
<screen>&prompt.user;attrd_updater -p -n evacuate -N <replaceable>NODE</replaceable></screen>
  <para>
   Possible results are the following:
  </para>

  <itemizedlist>
   <listitem>
    <para>
     The attribute is not set. Refer to
     <xref linkend="st.compnode.failed.reason"/> in
     <xref linkend="pro.recover.compute.node.failure"/>.
    </para>
   </listitem>
   <listitem>
    <para>
     The attribute is set to <literal>yes</literal>. This means that the
     &compnode; was fenced, but <systemitem>nova-evacuate</systemitem> never
     initiated the recovery procedure by calling &o_comp;'s evacuate API.
    </para>
   </listitem>
   <listitem>
    <para>
     The attribute contains a time stamp, in which case the recovery procedure
     was initiated at the time indicated by the time stamp, but has not
     completed yet.
    </para>
   </listitem>
   <listitem>
    <para>
     If the attribute is set to <literal>no</literal>, the recovery procedure
     recovered successfully and the cloud is ready for the &compnode; to
     rejoin.
    </para>
   </listitem>
  </itemizedlist>

  <para>
   If the attribute is stuck with the wrong value, it can be set to
   <literal>no</literal> using the command:
  </para>
<screen>&prompt.user;attrd_updater -n evacuate -U no -N <replaceable>NODE</replaceable></screen>

  <para>
   After standard fencing has been performed, fence agent
   <systemitem>fence_compute</systemitem> should activate the secondary
   fencing device (<literal>fence-nova</literal>). It does this by setting
   the attribute to <literal>yes</literal> to mark the node as needing
   recovery. The agent also calls &o_comp;'s
   <systemitem>force_down</systemitem> API to notify it that the host is down.
   You should be able to see this in
   <filename>/var/log/nova/fence_compute.log</filename> on the node in the core
   cluster that was running the <systemitem>fence-nova</systemitem> agent at
   the time of fencing. During the recovery, <literal>fence_compute</literal>
   tells &o_comp; that the host is up and running again.
  </para>
 </sect1>
 <sect1 xml:id="sec.depl.maintenance.bootstrap.compute.plane">
  <title>Bootstrapping Compute Plane</title>
  <para>
   If the whole compute plane is down, it is not always obvious how to boot it
   up, because it can be subject to deadlock if evacuate attributes are set on
   every &compnode;. In this case, manual intervention is
   required. Specifically, the operator must manually choose one or more
   &compnode;s to bootstrap the compute plane, and then run the
   <command>attrd_updater -n evacuate -U no -N <replaceable>NODE</replaceable></command>
   command for each
   of those &compnode;s to indicate that they do not require the resurrection
   process and can have their <literal>nova-compute</literal> start up straight
   away. Once these &compnode;s are up, this breaks the deadlock allowing
   <literal>nova-evacuate</literal> to start. This way, any other nodes that
   require resurrection can be processed automatically. If no resurrection is
   desired anywhere in the cloud, then the attributes should be set to
   <literal>no</literal> for all nodes.
  </para>
  <important>
   <para>
    Keep in mind that if &compnode;s are started too long after the
    <literal>remote-*</literal> resources are started on the control plane,
    they will be liable to fencing and so this should be avoided.
   </para>
  </important>
 </sect1>
</chapter>
