<?xml version="1.0"?>
<!DOCTYPE section [
 <!ENTITY % entities SYSTEM "entities.ent"> %entities;
]>
<section xml:id="nsx-configure-vxlan-transport"
 xmlns="http://docbook.org/ns/docbook" version="5.1"
 xmlns:xi="http://www.w3.org/2001/XInclude"
 xmlns:xlink="http://www.w3.org/1999/xlink">
 <title>Configure VXLAN Transport Parameters</title>
 <para>
  VXLAN is configured on a per-cluster basis, where each vSphere cluster that
  is to participate in NSX is mapped to a vSphere Distributed Virtual Switch
  (DVS). When mapping a vSphere cluster to a DVS, each ESXi host in that
  cluster is enabled for logical switches. The settings chosen in this section
  will be used in creating the VMkernel interface.
 </para>
 <para>
  Configuring transport parameters involves selecting a DVS, a VLAN ID, an MTU
  size, an IP addressing mechanism, and a NIC teaming policy. The MTU for each
  switch must be set to 1550 or higher. By default, it is set to 1600 by
  NSX. This is also the recommended setting for integration with &ostack;.
 </para>
 <para>
  To configure the VXLAN transport parameters:
 </para>
 <procedure>
  <step>
   <para>
    In the vSphere web client, navigate to
    <menuchoice><guimenu>Home</guimenu><guimenu>Networking &amp;
    Security</guimenu><guimenu>Installation</guimenu></menuchoice>.
   </para>
  </step>
  <step>
   <para>
    Select the <guimenu>Host Preparation</guimenu> tab.
   </para>
  </step>
  <step>
   <para>
    Click the <guimenu>Configure</guimenu> link in the VXLAN column.
   </para>
  </step>
  <step>
   <para>
    Enter the required information.
   </para>
  </step>
  <step>
   <para>
    If you have not already done so, create an IP pool for the VXLAN tunnel end
    points (VTEP) by clicking <guimenu>New IP Pool</guimenu>:
   </para>
  </step>
  <step>
   <para>
    Click <guimenu>OK</guimenu> to create the VXLAN network.
   </para>
  </step>
 </procedure>
 <para>
  When configuring the VXLAN transport network, consider the following:
 </para>
 <itemizedlist>
  <listitem>
   <para>
    Use a NIC teaming policy that best suits the environment being
    built. <literal>Load Balance - SRCID</literal> as the VMKNic teaming policy
    is usually the most flexible out of all the available options. This allows
    each host to have a VTEP vmkernel interface for each dvuplink on the
    selected distributed switch (two dvuplinks gives two VTEP interfaces per
    ESXi host).
   </para>
  </listitem>
  <listitem>
   <para>
    Do not mix different teaming policies for different portgroups on a VDS
    where some use Etherchannel or Link Aggregation Control Protocol (LACPv1 or
    LACPv2) and others use a different teaming policy. If uplinks are shared in
    these different teaming policies, traffic will be interrupted. If logical
    routers are present, there will be routing problems. Such a configuration
    is not supported and should be avoided.
   </para>
  </listitem>
  <listitem>
   <para>
    For larger environments it may be better to use DHCP for the VMKNic IP
    Addressing.
   </para>
  </listitem>
  <listitem>
   <para>
    For more information and further guidance, see the <link
xlink:href="https://communities.vmware.com/docs/DOC-27683">VMware NSX for
    vSphere Network Virtualization Design Guide</link>.
   </para>
  </listitem>
 </itemizedlist>
</section>
