<?xml version="1.0"?>
<!DOCTYPE section [
 <!ENTITY % entities SYSTEM "entities.ent"> %entities;
]>
<!---->
<section xmlns="http://docbook.org/ns/docbook" xmlns:xi="http://www.w3.org/2001/XInclude" xmlns:xlink="http://www.w3.org/1999/xlink" xml:id="swift_ring_rebalance"><title>Rebalancing Swift Rings</title><!---->


    <para>The Swift ring building process tries to distribute data evenly among the available disk
      drives. The data is stored in partitions. (For more information, see <xref linkend="ring-specification"/>.) If you, for example, double the
      number of disk drives in a ring, you need to move 50% of the partitions to the new drives so
      that all drives contain the same number of partitions (and hence same amount of data).
      However, it is not possible to move the partitions in a single step. It can take minutes to
      hours to move partitions from the original drives to their new drives (this process is called
      the replication process).</para>

    <para>If you move all partitions at once, there would be a period where Swift would expect to find
      partitions on the new drives, but the data has not yet replicated there so that Swift could
      not return the data to the user. Therefore, Swift will not be able to find all of the data in
      the middle of replication because some data has finished replication while other bits of data
      are still in the old locations and have not yet been moved. So it is considered best practice
      to move only one replica at a time. If the replica count is 3, you could first move 16.6% of
      the partitions and then wait until all data has replicated. Then move another 16.6% of
      partitions. Wait again and then finally move the remaining 16.6% of partitions. For any given
      object, only one of the replicas is moved at a time.</para>

    <sidebar><para><emphasis role="bold">Reasons to Move Partitions Gradually</emphasis></para>
<para>Due to the
        following factors, you must move the partitions gradually: </para>
<itemizedlist xml:id="ul_jrz_q3s_4t">
        <listitem><para>Not all devices are of the same size. &kw-hos-phrase; automatically
          assigns different weights to drives so that smaller drives store fewer partitions than
          larger drives.</para>
</listitem>
        <listitem><para>The process attempts to keep replicas of the same partition in different servers.</para>
</listitem>
        <listitem><para>Making a large change in one step (for example, doubling the number of drives in the
            ring), would result in a lot of network traffic due to the replication process and the
            system performance suffers. There are two ways to mitigate this:</para>
<itemizedlist xml:id="ul_msz_tjs_4t">
            <listitem><para>Add servers in smaller groups</para>
</listitem>
            <listitem><para>Set the weight-step attribute in the ring specification. (For more information, see
                <xref linkend="swift_weight_att"/>)</para>
</listitem>
          </itemizedlist></listitem>
      </itemizedlist></sidebar>
  </section>
