<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE chapter
[
  <!ENTITY % entities SYSTEM "entity-decl.ent">
  %entities;
]>
<!-- Converted by suse-upgrade version 1.1 -->
<chapter xmlns="http://docbook.org/ns/docbook" xmlns:xi="http://www.w3.org/2001/XInclude" xmlns:xlink="http://www.w3.org/1999/xlink" version="5.0" xml:id="cha-depl-arch">
 <title>The &cloud; Architecture</title>
 <info>
  <abstract>
   <para>
    &productname; is a managed cloud infrastructure solution that provides a
    full stack of cloud deployment and management services.
   </para>
  </abstract>
 </info>
 <para>
  &productname;  &productnumber; provides the following features:
 </para>
 <itemizedlist mark="bullet" spacing="normal">
  <listitem>
   <para>
    Open source software that is based on the &ostack; &latest_ostack;
    release.
   </para>
  </listitem>
  <listitem>
   <para>
    Centralized resource tracking providing insight into activities and
    capacity of the cloud infrastructure for optimized automated deployment
    of services.
   </para>
  </listitem>
  <listitem>
   <para>
    A self-service portal enabling end users to configure and deploy
    services as necessary, and to track resource
    consumption (&o_dash;).
   </para>
  </listitem>
  <listitem>
   <para>
    An image repository from which standardized, pre-configured virtual
    machines are published (&o_img;).
   </para>
  </listitem>
  <listitem>
   <para>
    Automated installation processes via &crow; using pre-defined scripts
    for configuring and deploying the &contrnode;(s) and Compute
    and &stornode;s.
   </para>
  </listitem>
  <listitem>
   <para>
    Multi-tenant, role-based provisioning and access control for multiple
    departments and users within your organization.
   </para>
  </listitem>
  <listitem>
   <para>
    APIs enabling the integration of third-party software, such as identity
    management and billing solutions.
   </para>
  </listitem>
  <listitem>
   <para>
    Heterogeneous hypervisor support (&xen; and &kvm;).
   </para>
  </listitem>
  <listitem>
   <para>
    An optional monitoring as a service solution, that allows to manage, track,
    and optimize the cloud infrastructure and the services provided to end
    users (&productmonitoring;, &o_monitor;).
   </para>
  </listitem>
 </itemizedlist>
 <para>
  &productname; is based on &sls;, &ostack;, &crow;, and
  &chef;. &sls; is the underlying operating system for all
  cloud infrastructure machines (also called nodes). The cloud management layer,
  &ostack;, works as the <quote>Cloud Operating
  System</quote>. &crow; and &chef; automatically deploy
  and manage the &ostack; nodes from a central &admserv;.
 </para>
 <figure>
  <title>&productname; Infrastructure</title>
  <mediaobject>
   <imageobject role="fo">
    <imagedata fileref="cloud_node_structure.png" width="100%" format="PNG"/>
   </imageobject>
   <imageobject role="html">
    <imagedata fileref="cloud_node_structure.png" width="100%" format="PNG"/>
   </imageobject>
  </mediaobject>
 </figure>
 <para>
  &productname; is deployed to four different types of machines:
 </para>
 <itemizedlist mark="bullet" spacing="normal">
  <listitem>
   <para>
    one &admserv; for node deployment and management
   </para>
  </listitem>
  <listitem>
   <para>
    one or more &contrnode;s hosting the cloud management services
   </para>
  </listitem>
  <listitem>
   <para>
    several &compnode;s on which the &vmguest;s are started
   </para>
  </listitem>
  <listitem>
   <para>
    several &monitnode; for monitoring services and servers.
   </para>
  </listitem>
 </itemizedlist>
 <sect1 xml:id="sec-depl-arch-components-admin">
  <title>The &admserv;</title>

  <para>
   The &admserv; provides all services needed to manage and deploy all
   other nodes in the cloud. Most of these services are provided by the
   &crow; tool that&mdash;together with &chef;&mdash;automates all the
   required installation and configuration tasks. Among the services
   provided by the server are DHCP, DNS, NTP, PXE, and TFTP.
  </para>

  <informalfigure>
   <mediaobject>
     <textobject>
       <phrase>Administration Server Diagram</phrase>
     </textobject>
    <imageobject role="fo">
     <imagedata fileref="cloud_admin-node-crowbar.png" width="100%" format="PNG"/>
    </imageobject>
    <imageobject role="html">
     <imagedata fileref="cloud_admin-node-crowbar.png" width="100%" format="PNG"/>
    </imageobject>
   </mediaobject>
  </informalfigure>

  <para>
   The &admserv; also hosts the software repositories for &sls; and
   &productname;, which are needed for node deployment. If no other
   sources for the software repositories are available, it can host the &smtool; (&smt;), providing up-to-date repositories
   with updates and patches for all nodes.
  </para>
 </sect1>
 <sect1 xml:id="sec-depl-arch-components-control">
  <title>The &contrnode;(s)</title>

  <para>
   The &contrnode;(s) hosts all &ostack; components needed to
   orchestrate virtual machines deployed on the &compnode;s in the
   &cloud;. &ostack; on &cloud; uses a &mariadb; database,
   which is hosted on the &contrnode;(s). The following &ostack;
   components&mdash;if deployed&mdash;run on the &contrnode;(s):
  </para>

  <itemizedlist mark="bullet" spacing="normal">
   <listitem>
    <para>
     &mariadb; database.
    </para>
   </listitem>
   <listitem>
    <para>
     &img; (&o_img;) for managing virtual images.
    </para>
   </listitem>
   <listitem>
    <para>
     &ident; (&o_ident;), providing authentication and authorization
     for all &ostack; components.
    </para>
   </listitem>
   <listitem>
    <para>
     &netw; (&o_netw;), providing <quote>networking as a
     service</quote> between interface devices managed by other &ostack;
     services.
    </para>
   </listitem>
   <listitem>
    <para>
     &blockstore; (&o_blockstore;), providing block storage.
    </para>
   </listitem>
   <listitem>
    <para>
     &ostack; &dash; (&o_dash;), providing the &dash;,
     a user Web interface for the &ostack; components.
    </para>
   </listitem>
   <listitem>
    <para>
     &comp; (&o_comp;) management (<literal>nova-controller</literal>) including API and
     scheduler.
    </para>
   </listitem>
   <listitem>
    <para>
     Message broker (RabbitMQ).
    </para>
   </listitem>
   <listitem>
    <para>
     &o_objstore; proxy server plus dispersion tools (health monitor) and
     &o_objstore; ring (index of objects, replicas, and
     devices). &o_objstore; provides object storage.
    </para>
   </listitem>
   <listitem>
    <para>
     &hawk;, a monitor for a pacemaker cluster in a High Availability (HA)
     setup.
    </para>
   </listitem>
   <listitem>
    <para>
     &o_orch;, an orchestration engine.
    </para>
   </listitem>
   <listitem>
    <para>
     &desig; provides DNS as a Service (DNSaaS)
    </para>
   </listitem>
   <listitem>
    <para>
     &o_meter; server and agents. &o_meter; collects CPU and networking data
     for billing purposes.
    </para>
   </listitem>
   <listitem>
    <para>
     ironic, the &ostack; bare metal service for provisioning physical machines.
    </para>
   </listitem>
  </itemizedlist>

  <para>
   &productname; requires a three-node cluster for any production deployment
   since it leverages a &mariadb_cluster; for high availability.
  </para>

  <para>
   We recommend deploying certain parts of &netw; (&o_netw;) on separate nodes for production clouds. See
   <xref linkend="sec-depl-ostack-neutron"/> for details.
  </para>

  <para>
   You can separate authentication and authorization services from other cloud
   services, for stronger security, by hosting &ident; (&o_ident;) on a
   separate node. Hosting &blockstore; (&o_blockstore;, particularly the
   cinder-volume role) on a separate node when using local disks for storage
   enables you to customize your storage and network hardware to best meet your
   requirements.
  </para>

  <note>
   <title>Moving Services in an Existing Setup</title>
   <para>
    If you plan to move a service from one &contrnode; to another, we strongly recommended shutting down or saving <emphasis>all</emphasis> &vmguest;s before doing so. Restart
    them after having successfully re-deployed the services. Moving
    services also requires stopping them manually on the original
    &contrnode;.
<!-- See TODO for more
    information on &cloud; maintenance. -->
<!-- TODO vuntz 2013-12-04: actually, right now, this probably doesn't work well without manual intervention (to at least stop the services on the old node -->
   </para>
  </note>
 </sect1>
 <sect1 xml:id="sec-depl-arch-components-compute">
  <title>The &compnode;s</title>

  <para>
   The &compnode;s are the pool of machines on which your &vmguest;s
   are running. These machines need to be equipped with a sufficient number
   of CPUs and enough RAM to start several &vmguest;s. They also need to
   provide sufficient hard disk space, see
   <xref linkend="sec-depl-req-storage-hardware-compute"/> for details. The
   &contrnode; distributes &vmguest;s within the pool of
   &compnode;s and provides them with the necessary network resources. The
   &ostack; component &comp; (nova) runs on the &compnode;s and
   provides the means for setting up, starting, and stopping virtual machines.
  </para>

  <para>
   &productname; supports several hypervisors, including &kvm;, &vmware;
   vSphere, and &xen;. Each image that is started with an &vmguest; is
   bound to one hypervisor. Each &compnode; can only run one hypervisor
   at a time. You will choose which hypervisor to run on each &compnode;
   when deploying the nova &barcl;.
  </para>

 </sect1>
 <sect1 xml:id="sec-depl-arch-components-storage">
  <title>The &stornode;s</title>

  <para>
   The &stornode;s are the pool of machines providing object or block
   storage. Object storage is provided by the &ostack; &swift; component,
   while block storage is provided by &o_blockstore;. The latter supports several
   back-ends, including &ceph;, that are deployed during the
   installation. Deploying &swift; and &ceph; is optional.
  </para>
 </sect1>

  <sect1 xml:id="sec-depl-arch-components-monitoring">
  <title>The Monitoring Node</title>
  <para>
The Monitoring Node is the node that has the <literal>monasca-server</literal> role assigned.
It hosts most services needed for &cloud;
Monitoring, our &o_monitor;-based monitoring and logging solution. The following
services run on this node:
</para>
<variablelist>
  <varlistentry>
    <term>Monitoring API</term>
    <listitem>
      <para>
          The &o_monitor; Web API that is used for sending metrics by &o_monitor; agents, and
  retrieving metrics with the &o_monitor; command line client and the &o_monitor; Grafana dashboard.
      </para>
    </listitem>
  </varlistentry>
  <varlistentry>
    <term>Message Queue</term>
    <listitem>
      <para>
         A Kafka instance used exclusively by &cloud; Monitoring.
      </para>
    </listitem>
  </varlistentry>
    <varlistentry>
    <term>Persister</term>
    <listitem>
      <para>
         Stores metrics and alarms in InfluxDB.
      </para>
    </listitem>
  </varlistentry>
    <varlistentry>
    <term>Notification Engine</term>
    <listitem>
      <para>
         Consumes alarms sent by the Threshold Engine and sends
notifications (e.g. via email).
      </para>
    </listitem>
  </varlistentry>
    <varlistentry>
    <term>Threshold Engine</term>
    <listitem>
      <para>
         Based on Apache Storm. Computes thresholds on metrics and handles alarming.
      </para>
    </listitem>
  </varlistentry>
  <varlistentry>
    <term>Metrics and Alarms Database</term>
    <listitem>
      <para>
         A &cassandra; database for storing metrics alarm history.
      </para>
    </listitem>
  </varlistentry>
    <varlistentry>
    <term>Config Database</term>
    <listitem>
      <para>
         A dedicated &mariadb; instance used only for monitoring related data.
      </para>
    </listitem>
  </varlistentry>
    <varlistentry>
    <term>Log API</term>
    <listitem>
      <para>
         The &o_monitor; Web API that is used for sending log entries by &o_monitor; agents, and
  retrieving log entries with the Kibana Server.
      </para>
    </listitem>
  </varlistentry>
    <varlistentry>
    <term>Log Transformer</term>
    <listitem>
      <para>
         Transforms raw log entries sent to the Log API into a format
suitable for storage.
      </para>
    </listitem>
  </varlistentry>
  <varlistentry>
    <term>Log Metrics</term>
    <listitem>
      <para>
         Sends metrics about high severity log messages to the Monitoring API.
      </para>
    </listitem>
  </varlistentry>
    <varlistentry>
    <term>Log Persister</term>
    <listitem>
      <para>
         Stores logs processed by &o_monitor; Log Transformer in the Log Database.
      </para>
    </listitem>
  </varlistentry>
    <varlistentry>
    <term>Kibana Server</term>
    <listitem>
      <para>
         A graphical web frontend for querying the Log Database.
      </para>
    </listitem>
  </varlistentry>
    <varlistentry>
    <term>Log Database</term>
    <listitem>
      <para>
         An &elasticsearch; database for storing logs.
      </para>
    </listitem>
  </varlistentry>
    <varlistentry>
    <term>Zookeeper</term>
    <listitem>
      <para>
         Cluster synchronization for Kafka and Storm.
      </para>
    </listitem>
  </varlistentry>
  </variablelist>
  <para>
Currently there can only be one Monitoring node. Clustering support is
planned for a future release. We strongly recommend using a dedicated physical
node without any other services as a &monitnode;.
</para>
</sect1>
 <sect1 xml:id="sec-depl-arch-components-ha">
  <title>&haSetup;</title>

  <para>
   A failure of components in &productname; can lead to system downtime
   and data loss. To prevent this, set up a &ha; (HA) cluster
   consisting of several nodes. You can assign certain roles to this cluster
   instead of assigning them to individual nodes. As of &productname;
   8, &contrnode;s and &compnode;s can be made highly available.
  </para>

  <para>
   For all HA-enabled roles, their respective functions are automatically
   handled by the clustering software &sle; &hasi;. The &hasi; uses
   the Pacemaker cluster stack with Pacemaker as cluster resource manager,
   and &corosync; as the messaging/infrastructure layer.
  </para>

  <para>
   View the cluster status and configuration with the cluster
   management tools &haweb; (&hawk;) or the
   <command>crm</command> shell.
  </para>

  <important>
   <title>Do Not Change the Configuration</title>
   <para>
    Use the cluster management tools only for <emphasis>viewing</emphasis>.
    All of the clustering configuration is done automatically via &crow;
    and &chef;. If you change anything via the cluster management tools
    you risk breaking the cluster. Changes done there may be reverted by the
    next run of &chef; anyway.
   </para>
  </important>

  <para>
   A failure of the &ostack; infrastructure services (running on the
   &contrnode;s) can be critical and may cause downtime within the cloud. For more information on making those services highly-available and avoiding other potential points of
   failure in your cloud setup, refer to <xref linkend="sec-depl-req-ha"/>.
  </para>
 </sect1>
</chapter>
