<?xml version="1.0"?>
<?xml-stylesheet href="urn:x-suse:xslt:profiling:docbook51-profile.xsl"
 type="text/xml"
 title="Profiling step"?>
<!DOCTYPE section [
 <!ENTITY % entities SYSTEM "entity-decl.ent"> %entities;
]>
<section xmlns="http://docbook.org/ns/docbook" xmlns:xi="http://www.w3.org/2001/XInclude" xmlns:xlink="http://www.w3.org/1999/xlink" xml:id="dpdk">
 <title>OVS-DPDK Support</title>
 <para>
  &productname; uses a version of Open vSwitch (OVS) that is built with the
  Data Plane Development Kit (DPDK) and includes a QEMU hypervisor which
  supports vhost-user.
 </para>
 <para>
  The OVS-DPDK package modifes the OVS fast path, which is normally performed
  in kernel space, and allows it to run in userspace so there is no context
  switch to the kernel for processing network packets.
 </para>
 <para>
  The EAL component of DPDK supports mapping the Network Interface Card (NIC)
  registers directly into userspace. The DPDK provides a Poll Mode Driver
  (PMD) that can access the NIC hardware from userspace and uses polling
  instead of interrupts to avoid the user to kernel transition.
 </para>
 <para>
  The PMD maps the shared address space of the VM that is provided by the
  vhost-user capability of QEMU. The vhost-user mode causes Neutron to create
  a Unix domain socket that allows communication between the PMD and QEMU. The
  PMD uses this in order to acquire the file descriptors to the pre-allocated
  VM memory. This allows the PMD to directly access the VM memory space and
  perform a fast zero-copy of network packets directly into and out of the VMs
  virtio_net vring.
 </para>
 <para>
  This yields performance improvements in the time it takes to process network
  packets.
 </para>
 <section>
  <title>Usage considerations</title>
  <para>
   The target for a DPDK Open vSwitch is VM performance and VMs only run on
   compute nodes so the following considerations are compute node specific.
  </para>
  <orderedlist>
   <listitem>
    <para>
     In order to use DPDK with VMs, <literal>hugepages</literal> must be
     enabled; please see <xref linkend="hugepages"/>. The memory to be used
     must be allocated at boot time so you must know beforehand how many VMs
     will be scheduled on a node. Also, for NUMA considerations, you want those
     hugepages on the same NUMA node as the NIC.  A VM maps its entire address
     space into a hugepage.
    </para>
   </listitem>
   <listitem>
    <para>
     For maximum performance you must reserve logical cores for DPDK poll mode
     driver (PMD) usage and for hypervisor (QEMU) usage. This keeps the Linux
     kernel from scheduling processes on those cores. The PMD threads will go
     to 100% cpu utilization since it uses polling of the hardware instead of
     interrupts. There will be at least 2 cores dedicated to PMD threads. Each
     VM will have a core dedicated to it although for less performance VMs can
     share cores.
    </para>
   </listitem>
   <listitem>
    <para>
     VMs can use the virtio_net or the virtio_pmd drivers. There is also a PMD
     for an emulated e1000.
    </para>
   </listitem>
   <listitem>
    <para>
     Only VMs that use hugepages can be sucessfully launched on a DPDK-enabled
     NIC. If there is a need to support both DPDK and non-DPDK-based VMs, an
     additional port managed by the Linux kernel must exist.
    </para>
   </listitem>
  </orderedlist>
 </section>
 <section>
  <title>For more information</title>
  <para>
   See the following topics for more information:
  </para>
 </section>
 <xi:include href="networking-hugepages.xml"/>
 <xi:include href="networking-dpdk_setup.xml"/>
 <xi:include href="networking-dpdk_configurations.xml"/>
 <xi:include href="networking-ts_dpdk.xml"/>
</section>
