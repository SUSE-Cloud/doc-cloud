<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE glossary
[
  <!ENTITY % entities SYSTEM "entity-decl.ent">
  %entities;
]>
<!-- Converted by suse-upgrade version 1.1 -->
<!-- taroth 2016-02-23: does it make sense to maintain this document for the
 future? specific for SOC are definitely all terms related to and (SLE) HA, for
 the other terms it might make more sense to include the upstream glossary that is appended to all
 upstream manuals -->
<glossary xmlns="http://docbook.org/ns/docbook" xmlns:xi="http://www.w3.org/2001/XInclude" xmlns:xlink="http://www.w3.org/1999/xlink" version="5.0" xml:id="gl.cloud">
    <title>Glossary of Terminology and Product Names</title>
     <info>
 <dm:docmanager xmlns:dm="urn:x-suse:ns:docmanager">
    <dm:maintainer>fs</dm:maintainer>
    <dm:status>editing</dm:status>
    <dm:deadline/>
    <dm:priority/>
    <dm:translation>no</dm:translation>
    <dm:languages/>
</dm:docmanager>
</info>
<glossentry xml:id="gloss.act.act"><glossterm>Active/Active</glossterm>
  <glossdef>
   <para>
    A concept of how services are running on nodes in a &ha; cluster. In
    an active/active setup, both the main and redundant systems are managed
    concurrently. If a failure of services occurs, the redundant system is
    already online, and can take over until the main system is fixed and
    brought back online.
   </para>
  </glossdef>
 </glossentry>
 <glossentry xml:id="gloss.act.pass"><glossterm>Active/Passive</glossterm>
  <glossdef>
   <para>
    A concept of how services are running on nodes in a &ha; cluster. In
    an active/passive setup, one or more services are running on an active
    cluster node, whereas the passive node stands by. If the active node fails then the services are transferred to the passive node.
   </para>
  </glossdef>
 </glossentry>
 <glossentry><glossterm>&admserv;</glossterm>
  <glossdef>
   <para>
    Also called &crow; Administration Node. Manages all other nodes. It
    assigns IP addresses to them, boots them using PXE, configures them, and
    provides them the necessary software for their roles. To provide these
    services, the &admserv; runs &crow;, &chef;, DHCP, TFTP, NTP, and other
    services.
   </para>
  </glossdef>
 </glossentry>
 <glossentry><glossterm>AMI (Amazon Machine Image)</glossterm>
  <glossdef>
   <para>
    A virtual machine that can be created and customized by a user. AMIs can
    be identified by an ID prefixed with <literal>ami-</literal>.
   </para>
  </glossdef>
 </glossentry>
 <glossentry xml:id="gloss.az"><glossterm>Availability Zone</glossterm>
  <glossdef>
   <para>
    An &ostack; method of partitioning clouds. It enables you to arrange
    &ostack; &comp; hosts into logical groups. The groups typically have
    physical isolation and redundancy from other availability zones, for
    example, by using separate power supply or network equipment for each
    zone. When users provision resources, they can specify from which
    availability zone their instance should be created. This allows cloud
    consumers to ensure that their application resources are spread across
    disparate machines to achieve high availability if the hardware fails.
    Since the Grizzly release, availability zones are implemented via host
    aggregates.
<!-- taroth 2013-06-12: found a lot of helpful details in
       http://russellbryantnet.wordpress.com/2013/05/21/availability-zones-and-host-aggregates-in-openstack-compute-nova/ -->
   </para>
  </glossdef>
 </glossentry>
 <glossentry><glossterm>AWS (Amazon Web Services)</glossterm>
  <glossdef>
   <para>
    A collection of remote computing services (including Amazon EC2, Amazon
    S3, and others) that together make up Amazon's cloud computing platform.
   </para>
  </glossdef>
 </glossentry>
 <glossentry><glossterm>&Barcl;</glossterm>
  <glossdef>
   <para>
    A set of &chef; cookbooks, templates, and other logic. Used to apply
    a particular &chef; role to individual nodes or a set of nodes.
<!--Crowbar modularization. Individual layers of the deployment infrastructure
   are packaged separately and can be integrated before, while or after installing
   Crowbar.-->
   </para>
  </glossdef>
 </glossentry>
 <glossentry><glossterm>&o_meter;</glossterm>
  <glossdef>
   <para>
    Code name for <xref linkend="gloss.ceilo"/>.
   </para>
  </glossdef>
 </glossentry>
 <glossentry><glossterm>Cell</glossterm>
  <glossdef>
   <para>
    Cells provide a new way to scale &comp; deployments. This includes the
    ability to have compute clusters (cells) in different geographic
    locations all under the same &comp; API. This allows for a single API
    server being used to control access to multiple cloud installations.
    Cells provide logical partitioning of &comp; resources in a
    child/parent relationship.
   </para>
  </glossdef>
 </glossentry>
 <glossentry><glossterm>&chef;</glossterm>
  <glossdef>
   <para>
    An automated configuration management platform for deployment of your
    entire cloud infrastructure. The &chef; server manages many of the
    software packages and allows the easy changing of nodes.
   </para>
  </glossdef>
 </glossentry>
 <glossentry><glossterm>&ceph;</glossterm>
  <glossdef>
   <para>
    A massively scalable, open source, distributed storage system. It
    consists of an object store, a block store, and a POSIX-compliant
    distributed file system.
   </para>
  </glossdef>
 </glossentry>
 <glossentry><glossterm>&o_blockstore;</glossterm>
  <glossdef>
   <para>
    Code name for <xref linkend="gloss.cinder"/>.
   </para>
  </glossdef>
 </glossentry>
 <glossentry><glossterm>Cluster</glossterm>
  <glossdef>
   <para>
    A set of connected computers that work together. In many respects (and
    from the outside) they can be viewed as a single system. Clusters can be
    further categorized depending on their purpose, for example: &ha;
    clusters, high-performance clusters, or load-balancing clusters.
   </para>
  </glossdef>
 </glossentry>
 <glossentry xml:id="gloss.partition"><glossterm>Cluster Partition</glossterm>
  <glossdef>
   <para>
    Whenever communication fails between one or more nodes and the rest of
    the cluster, a cluster partition occurs: The nodes of a cluster are
    split into partitions but still active. They can only communicate with
    nodes in the same partition and are unaware of the separated nodes. As
    the loss of the nodes on the other partition cannot be confirmed, a
    <xref linkend="gloss.splitbrain"/> scenario develops.
   </para>
  </glossdef>
 </glossentry>
 <glossentry xml:id="gloss.crm"><glossterm>Cluster Resource Manager</glossterm>
  <glossdef>
   <para>
    The main management entity in a &ha; cluster responsible for
    coordinating all non-local interactions. The
    <xref linkend="gloss.hasi"/> uses Pacemaker as CRM. Each node of the
    cluster has its own CRM instance. The instance running on the
    <xref linkend="gloss.dc"/> is the one elected to relay decisions to the
    other non-local CRMs and to process their input.
   </para>
  </glossdef>
 </glossentry>
 <glossentry><glossterm>Container</glossterm>
  <glossdef>
   <para>
    A container is a storage compartment for data. It can be thought of as a
    directory, only that it cannot be nested.
   </para>
  </glossdef>
 </glossentry>
 <glossentry><glossterm>&corosync;</glossterm>
  <glossdef>
   <para>
    The messaging/infrastructure layer used in a &ha; cluster that is set
    up with &sle; &hasi;. For example, the cluster communication
    channels are defined in
    <filename>/etc/corosync/corosync.conf</filename>.
   </para>
  </glossdef>
 </glossentry>
 <glossentry><glossterm>cloud-init</glossterm>
  <glossdef>
   <para>
    A package commonly installed in virtual machine images. It uses the SSH
    public key to initialize an instance after boot.
   </para>
  </glossdef>
 </glossentry>
 <glossentry><glossterm>&compnode;</glossterm>
  <glossdef>
   <para>
    Node within a &cloud;. A physical server running a Hypervisor. A
    &compnode; is a host for guest virtual machines that are deployed in
    the cloud. It starts virtual machines on demand using
    <literal>nova-compute</literal>. To split virtual machine load across
    more than one server, a cloud should contain multiple &compnode;s.
   </para>
  </glossdef>
 </glossentry>
 <glossentry><glossterm>&contrnode;</glossterm>
  <glossdef>
   <para>
    Node within a &cloud;. The &contrnode; is configured through the
    &admserv; and registers with the &admserv; for all required
    software. Hosts the &ostack; API endpoints and the &ostack;
    scheduler and runs the <literal>nova</literal> services&mdash;except
    for <literal>nova-compute</literal>, which is run on the &compnode;s.
    The &contrnode; coordinates everything about cloud virtual machines:
    like a central communication center it receives all requests (for
    example, if a user wants to start or stop a virtual machine). It
    communicates with the &compnode;s to coordinate fulfillment of the
    request. A cloud can contain multiple &contrnode;s.
   </para>
  </glossdef>
 </glossentry>
 <glossentry><glossterm>Cookbook</glossterm>
  <glossdef>
   <para>
    A collection of &chef; recipes which deploy a software stack or
    functionality. The unit of distribution for &chef;.
   </para>
  </glossdef>
 </glossentry>
 <glossentry><glossterm>&crow;</glossterm>
  <glossdef>
   <para>
    Bare-metal installer and an extension of &chef; server. The primary
    function of &crow; is to get new hardware into a state where it can
    be managed by &chef;. That means: Setting up BIOS and RAID, network,
    installing a basic operating system, and setting up services like DNS,
    NTP, and DHCP. The &crow; server manages all nodes, supplying
    configuration of hardware and software.
   </para>
  </glossdef>
 </glossentry>
 <glossentry xml:id="gloss.dc"><glossterm>Designated Coordinator (DC)</glossterm>
  <glossdef>
   <para>
    One <xref linkend="gloss.crm"/> in a &ha; cluster is elected as the
    Designated Coordinator (DC). The DC is the only entity in the cluster
    that can decide that a cluster-wide change needs to be performed. For
    example, fencing a node or moving resources around. After a membership change,
    the DC is elected from all nodes in the cluster.
   </para>
  </glossdef>
 </glossentry>
<!--taroth 2014-01-28: commenting because Keystone API v3 has been rejected for Cloud3
  as of today:
  <glossentry>
  <glossterm>Domain</glossterm>
  <glossdef>
   <para>In &ostack; &ident;, a domain is a collection of projects, users, and roles.
    <!-\-Users may be given a domain's administrator role. A domain administrator
     may create tenants, users, and groups within a domain and assign roles to users
     and groups.-\-></para></glossdef>
     </glossentry>-->
 <glossentry><glossterm>DRBD (Distributed Replicated Block Device)</glossterm>
  <glossdef>
   <para>
    DRBD is a block device designed for building high availability
    clusters. The whole block device is mirrored via a dedicated network and
    is seen as a network RAID-1.
   </para>
  </glossdef>
 </glossentry>
 <glossentry><glossterm>EBS (Amazon Elastic Block Store)</glossterm>
  <glossdef>
   <para>
    Block-level storage volumes for use with Amazon EC2 instances. Similar
    to &ostack; &o_blockstore;.
   </para>
  </glossdef>
 </glossentry>
 <glossentry><glossterm>EC2 (Amazon Elastic Compute Cloud)</glossterm>
  <glossdef>
   <para>
    A public cloud run by Amazon. It provides similar functionality to
    &ostack; &comp;.
   </para>
  </glossdef>
 </glossentry>
 <glossentry><glossterm>Ephemeral Disk</glossterm>
  <glossdef>
   <para>
    Ephemeral disks offer machine local disk storage linked to the life
    cycle of a virtual machine instance. When a virtual machine is
    terminated, all data on the ephemeral disk is lost. Ephemeral disks are
    not included in any snapshots.
   </para>
  </glossdef>
 </glossentry>
 <glossentry xml:id="gloss.failover"><glossterm>Failover</glossterm>
  <glossdef>
   <para>
    Occurs when a resource fails on a cluster node (or the node itself
    fails) and the affected resources are started on another node.
   </para>
  </glossdef>
 </glossentry>
 <glossentry xml:id="gloss.fencing"><glossterm>Fencing</glossterm>
  <glossdef>
   <para>
    Describes the concept of preventing access to a shared resource by
    isolated or failing cluster members. Should a cluster node fail, it will
    be shut down or reset to prevent it from causing trouble. The resources
    running on the cluster node will be moved away to another node. This
    way, resources are locked out of a node whose status is uncertain.
   </para>
  </glossdef>
 </glossentry>
 <glossentry xml:id="gloss.flavor"><glossterm>Flavor</glossterm>
  <glossdef>
   <para>
    The compute, memory, and storage capacity of <literal>nova</literal>
    computing instances (in terms of virtual CPUs, RAM, etc.). Flavors can
    be thought of as <quote>templates</quote> for the amount of cloud
    resources that are assigned to an instance.
   </para>
  </glossdef>
 </glossentry>
 <glossentry xml:id="gloss.IP.float"><glossterm>Floating IP Address</glossterm>
  <glossdef>
   <para>
    An IP address that a &comp; project can associate with a virtual
    machine. A pool of floating IP addresses is available in &ostack; &comp;,
    as configured by the cloud operator. After a floating IP address has
    been assigned to an instance, the instance can be reached from outside
    the cloud by this public IP address. Floating IP addresses can be
    dynamically disassociated and associated with other instances.
   </para>
  </glossdef>
 </glossentry>
 <glossentry xml:id="gloss.IP.fixed"><glossterm>Fixed IP Address</glossterm>
  <glossdef>
   <para>
    When an instance is launched, it is automatically assigned a fixed
    (private) IP address, which stays the same until the instance is
    explicitly terminated. Private IP addresses are used for communication
    between instances.
   </para>
  </glossdef>
 </glossentry>
 <glossentry><glossterm>&o_img;</glossterm>
  <glossdef>
   <para>
    Code name for <xref linkend="gloss.glance"/>.
   </para>
  </glossdef>
 </glossentry>
<!--taroth 2014-01-28: commenting because Keystone API v3 has been rejected for Cloud3
  as of today:
  <glossentry><glossterm>Group</glossterm>
  <glossdef>
   <para>In &ostack; &ident;, a group is a collection of users. Administrators can create
    groups to organize users and roles. <!-\-Users can be added to a group. Rather than assigning a
    role to each user individually, a role can be assigned to a group. Every group is part of a
    domain. -\-></para>
  </glossdef>
 </glossentry>-->
 <glossentry><glossterm>Guest Operating System</glossterm>
  <glossdef>
   <para>
    An instance of an operating system installed on a virtual machine.
   </para>
  </glossdef>
 </glossentry>
 <glossentry><glossterm>&o_orch;</glossterm>
  <glossdef>
   <para>
    Code name for <xref linkend="gloss.heat"/>.
   </para>
  </glossdef>
 </glossentry>
 <glossentry><glossterm>&ha; Cluster</glossterm>
  <glossdef>
   <para>
    &ha; clusters seek to minimize two things: system downtime and data
    loss. System downtime occurs when a user-facing service is unavailable
    beyond a specified maximum amount of time. System downtime and data
    loss (data is accidentally destroyed) can occur not only in case of a single
    failure. There are also cases of cascading failures,
    where a single failure deteriorates into a series of consequential
    failures.
   </para>
  </glossdef>
 </glossentry>
 <glossentry><glossterm>&o_dash;</glossterm>
  <glossdef>
   <para>
    Code name for <xref linkend="gloss.horizon"/>.
   </para>
  </glossdef>
 </glossentry>
 <glossentry><glossterm>Host</glossterm>
  <glossdef>
   <para>
    A physical computer.
   </para>
  </glossdef>
 </glossentry>
 <glossentry xml:id="gloss.host.aggr"><glossterm>Host Aggregate</glossterm>
  <glossdef>
   <para>
    An &ostack; method of grouping hosts via a common set of metadata. It
    enables you to tag groups of hosts with certain capabilities or
    characteristics. A characteristic could be related to physical location,
    allowing creation or further partitioning of availability zones. It could
    also be related to performance (for example, indicating the
    availability of SSD storage) or anything else that the cloud
    administrators deem appropriate. A host can be in more than one host
    aggregate.
   </para>
  </glossdef>
 </glossentry>
 <glossentry><glossterm>Hybrid Cloud</glossterm>
  <glossdef>
   <para>
    One of several deployment models for a cloud infrastructure. A
    composition of both public and private clouds that remain unique
    entities, but are bound together by standardized technology for enabling
    data and application portability. Integrating &susestudio; and
    &susemgr; with &cloud; delivers a platform and tools with which to
    enable enterprise hybrid clouds.
    <remark>taroth 2012-06-28: another definition of
     "hybrid cloud" is the combination of both virtual and physical machines,
     but I don't think we use the term in that sense</remark>
   </para>
  </glossdef>
 </glossentry>
 <glossentry><glossterm>Hypervisor</glossterm>
  <glossdef>
   <para>
    A piece of computer software, firmware or hardware that creates and runs
    virtual machines. It arbitrates and controls access of the virtual
    machines to the underlying hardware.
   </para>
  </glossdef>
 </glossentry>
 <glossentry><glossterm>IaaS (Infrastructure-as-a-Service)</glossterm>
  <glossdef>
   <para>
    A service model of cloud computing where processing, storage, networks,
    and other fundamental computing resources are rented over the Internet.
    It allows the customer to deploy and run arbitrary software, including
    operating systems and applications. The customer has control over
    operating systems, storage, and deployed applications but does not
    control the underlying cloud infrastructure. Housing and maintaining it
    is in the responsibility of the service provider.
   </para>
  </glossdef>
 </glossentry>
 <glossentry><glossterm>Image</glossterm>
  <glossdef>
   <para>
    A file that contains a complete Linux virtual machine.
   </para>
   <para>
    In the &productname; context, images are virtual disk images that
    represent the contents and structure of a storage medium or device
    (such as a hard disk), in a single file. Images are used as a template from which
    a virtual machine can be started. For starting a virtual machine,
    &productname; always uses a copy of the image.
   </para>
   <para>
    Images have both content and metadata; the latter are also called
    image properties.
   </para>
  </glossdef>
 </glossentry>
 <glossentry><glossterm>Instance</glossterm>
  <glossdef>
   <para>
    A virtual machine that runs inside the cloud.
   </para>
  </glossdef>
 </glossentry>
 <glossentry xml:id="gloss.instsnap"><glossterm>Instance Snapshot</glossterm>
  <glossdef>
   <para>
    A point-in-time copy of an instance. It preserves the disk state of a
    running instance and can be used to launch a new instance or to create a
    new image based upon the snapshot.
   </para>
  </glossdef>
 </glossentry>
 <glossentry><glossterm>Keypair</glossterm>
  <glossdef>
   <para>
    &ostack; &comp; injects SSH keypair credentials that are injected
    into images when they are launched.
   </para>
  </glossdef>
 </glossentry>
 <glossentry><glossterm>&o_ident;</glossterm>
  <glossdef>
   <para>
    Code name for <xref linkend="gloss.keystone"/>.
   </para>
  </glossdef>
 </glossentry>
 <glossentry><glossterm>libvirt</glossterm>
  <glossdef>
   <para>
    Virtualization API library. Used by &ostack; to interact with many of
    its supported hypervisors.
   </para>
  </glossdef>
 </glossentry>
 <glossentry><glossterm>Linux Bridge</glossterm>
  <glossdef>
   <para>
    A software allowing multiple virtual machines to share a single physical
    NIC within &ostack; &comp;. It behaves like a hub: You can connect
    multiple (physical or virtual) network interface devices to it. Any
    Ethernet frames that come in from one interface attached to the bridge
    is transmitted to all other devices.
   </para>
  </glossdef>
 </glossentry>
 <glossentry><glossterm>Logical Volume (LV)</glossterm>
  <glossdef>
   <para>
    Acts as a virtual disk partition. After creating a
    <xref linkend="gloss.vg"/>, logical volumes can be created in that
    volume group. Logical volumes can be used as raw block devices, swap
    devices, or for creating a (mountable) file system like disk partitions.
   </para>
  </glossdef>
 </glossentry>
 <glossentry><glossterm>Migration</glossterm>
  <glossdef>
   <para>
    The process of moving a virtual machine instance from one &compnode;
    to another. This process can only be executed by cloud administrators.
   </para>
  </glossdef>
 </glossentry>
 <glossentry><glossterm>Multicast</glossterm>
  <glossdef>
   <para>
    A technology used for a one-to-many communication within a network that
    can be used for cluster communication. &corosync; supports both
    multicast and unicast.
   </para>
  </glossdef>
 </glossentry>
 <glossentry><glossterm>Network</glossterm>
  <glossdef>
   <para>
    In the &ostack; &netw; API: An isolated L2 network segment
    (similar to a VLAN). It forms the basis for describing the L2 network
    topology in a given &ostack; &netw; deployment.
   </para>
  </glossdef>
 </glossentry>
 <glossentry><glossterm>&o_netw;</glossterm>
  <glossdef>
   <para>
    Code name for <xref linkend="gloss.neutron"/>.
   </para>
  </glossdef>
 </glossentry>
 <glossentry><glossterm>Node</glossterm>
  <glossdef>
   <para>
    A (physical) server that is managed by &crow;.
   </para>
  </glossdef>
 </glossentry>
 <glossentry><glossterm>&o_comp;</glossterm>
  <glossdef>
   <para>
    Code name for <xref linkend="gloss.nova"/>.
   </para>
  </glossdef>
 </glossentry>
 <glossentry><glossterm>Object</glossterm>
  <glossdef>
   <para>
    Basic storage entity in &ostack; &objstore;, representing a file
    that your store there. When you upload data to &ostack;
    &objstore;, the data is neither compressed nor encrypted, it is
    stored as-is.
   </para>
  </glossdef>
 </glossentry>
 <glossentry><glossterm>Open vBridge</glossterm>
  <glossdef>
   <para>
    A virtual networking device. It behaves like a virtual switch: network
    interface devices connect to its ports. The ports can be configured
    similar to a physical switch's port, including VLAN configurations.
   </para>
  </glossdef>
 </glossentry>
 <glossentry><glossterm>&ostack;</glossterm>
  <glossdef>
   <para>
    A collection of open source software to build and manage public and
    private clouds. Its components are designed to work together to provide
    Infrastructure as a Service and massively scalable cloud computing
    software.
   </para>
   <para>
    At the same time, &ostack; is also a community and a project.
   </para>
  </glossdef>
 </glossentry>
 <glossentry xml:id="gloss.cinder"><glossterm>&ostack; &blockstore;</glossterm>
  <glossdef>
   <para>
    One of the core &ostack; components and services (code name:
    <literal>&o_blockstore;</literal>). It provides persistent block
    level storage devices for use &ostack; compute instances. The block
    storage system manages the creation, attaching and detaching of the
    block devices to servers.
    <remark>taroth 2013-06-07: DEVs, is the following right?</remark>
    Prior to the &ostack; Grizzly release, the service was part of
    <literal>nova-volume</literal> (block service).
<!--Block storage volumes are fully integrated into &ostack; &comp;
    and the &dash; allowing for cloud users to manage their own storage
    needs.-->
   </para>
  </glossdef>
 </glossentry>
 <glossentry xml:id="gloss.nova"><glossterm>&ostack; &comp;</glossterm>
  <glossdef>
   <para>
    One of the core &ostack; components and services (code name:
    <literal>&o_comp;</literal>). It is a cloud computing fabric
    controller and as such, the main part of an IaaS system. It provides
    virtual machines on demand.
   </para>
  </glossdef>
 </glossentry>
 <glossentry xml:id="gloss.horizon"><glossterm>&ostack; &dash;</glossterm>
  <glossdef>
   <para>
    One of the core &ostack; components or services (code name:
    <literal>&o_dash;</literal>). It provides a modular Web interface for
    &ostack; services and allows end users and administrators to interact
    with each &ostack; service through the service's API.
   </para>
  </glossdef>
 </glossentry>
 <glossentry xml:id="gloss.keystone"><glossterm>&ostack; &ident;</glossterm>
  <glossdef>
   <para>
    One of the core &ostack; components or services (code name:
    <literal>&o_ident;</literal>). It provides authentication and
    authorization for all &ostack; services.
   </para>
  </glossdef>
 </glossentry>
 <glossentry xml:id="gloss.glance"><glossterm>&ostack; &img;</glossterm>
  <glossdef>
   <para>
    One of the core &ostack; components or services (code name:
    <literal>&o_img;</literal>). It provides discovery, registration, and
    delivery services for virtual disk images.
   </para>
  </glossdef>
 </glossentry>
 <glossentry xml:id="gloss.neutron"><glossterm>&ostack; &netw;</glossterm>
  <glossdef>
   <para>
    One of the core &ostack; components or services (code name:
    <literal>&o_netw;</literal>). It provides <quote>network connectivity
    as a service</quote> between interface devices (for example, vNICs)
    managed by other &ostack; services (for example, &comp;). Allows
    users to create their own networks and attach interfaces to them.
   </para>
  </glossdef>
 </glossentry>
 <glossentry xml:id="gloss.swift"><glossterm>&ostack; &objstore;</glossterm>
  <glossdef>
   <para>
    One of the core &ostack; components or services (code name:
    <literal>&o_objstore;</literal>). Allows to store and retrieve files
    while providing built-in redundancy and fail-over. Can be used for
    backing up and archiving data, streaming data to a user's Web browser,
    or developing new applications with data storage integration.
   </para>
  </glossdef>
 </glossentry>
 <glossentry xml:id="gloss.galera_sequence_number"><glossterm>Sequence number (seqno)</glossterm>
  <glossdef>
    <para>
      A term from a &mariadb_cluster; which is used for replication.
      It's a 64-bit signed integer that the node uses to denote the position of
      a given transaction in the sequence.
   </para>
  </glossdef>
 </glossentry>
 <glossentry><glossterm>&ostack; Service</glossterm>
  <glossdef>
   <para>
    A collection of Linux services (or daemons) that work together to
    provide core functionality within the &ostack; project. This can be storing
    objects, providing virtual servers, or authentication and authorization.
    All services have code names, which are also used in configuration files,
    and command line programs.
   </para>
  </glossdef>
 </glossentry>
 <glossentry xml:id="gloss.heat"><glossterm>&orch;</glossterm>
  <glossdef>
   <para>
    <remark>taroth 2013-06-07: DEVs, is the following correct?</remark>
    A module (code name: <literal>&o_orch;</literal>) to orchestrate
    multiple composite cloud applications using file-based or Web-based
    templates. It contains both a user interface and an API and describes
    your cloud deployment in a declarative language. The module is an
    integrated project of &ostack; as of the Havana release.
   </para>
  </glossdef>
 </glossentry>
 <glossentry><glossterm>PaaS (Platform-as-a-Service)</glossterm>
  <glossdef>
   <para>
    A service model of cloud computing where a computing platform and
    cloud-based application development tools are rented over the Internet.
    The customer controls software deployment and configuration settings,
    but not the underlying cloud infrastructure including network, servers,
    operating systems, or storage.
   </para>
  </glossdef>
 </glossentry>
 <glossentry><glossterm>Pacemaker</glossterm>
  <glossdef>
   <para>
    An open source cluster resource manager used in &sle; &hasi;.
   </para>
  </glossdef>
 </glossentry>
 <glossentry><glossterm>Port</glossterm>
  <glossdef>
   <para>
    In the &ostack; &netw; API: An attachment port to an L2
    &ostack; &netw; network.
<!--When a port is created on the network, it will be associated with a security group. If
    no security group is specified, it will be associated with a default security group. By default,
    the port will be allocated an available fixed IP address out of one of the designated subnets
    for each IP version. When the port is destroyed, the allocated addresses return to the pool of
    available IPs on the subnet(s). Users of the &ostack; &netw; API can either choose a specific
    IP address from the block, or let &ostack; &netw; choose the first available IP address.-->
   </para>
  </glossdef>
 </glossentry>
 <glossentry xml:id="gloss.project"><glossterm>Project</glossterm>
  <glossdef>
   <para>
    A concept in &ostack; &ident;. Used to identify a group, an
    organization, or a project (or more generically, an individual customer
    environment in the cloud). Also called <literal>tenant</literal>. The
    term <literal>tenant</literal> is primarily used in the &ostack;
    command line tools.
<!--but occasionally also appears in the &cloud; &dash;-->
   </para>
  </glossdef>
 </glossentry>
 <glossentry xml:id="gloss.proposal"><glossterm>Proposal</glossterm>
  <glossdef>
   <para>
    Special configuration for a &barcl;. It includes &barcl;-specific
    settings, and a list of nodes to which the proposal should be applied.
   </para>
  </glossdef>
 </glossentry>
 <glossentry><glossterm>Private Cloud</glossterm>
  <glossdef>
   <para>
    One of several deployment models for a cloud infrastructure. The
    infrastructure is operated exclusively for a single organization and may
    exist on or off premises. The cloud is owned and managed by the
    organization itself, by a third party or a combination of both.
   </para>
  </glossdef>
 </glossentry>
 <glossentry><glossterm>Private IP Address</glossterm>
  <glossdef>
   <para>
    See <xref linkend="gloss.IP.fixed"/>.
   </para>
  </glossdef>
 </glossentry>
 <glossentry><glossterm>Public Cloud</glossterm>
  <glossdef>
   <para>
    One of several deployment models for a cloud infrastructure. The cloud
    infrastructure is designed for use by the general public and exists on
    the premises of the cloud provider. Services like applications, storage,
    and other resources are made available to the general public for free or
    are offered on a pay-per-use model. The infrastructure is owned and
    managed by a business, academic or government organization, or some
    combination of these.
   </para>
  </glossdef>
 </glossentry>
 <glossentry><glossterm>Public IP Address</glossterm>
  <glossdef>
   <para>
    See <xref linkend="gloss.IP.float"/>.
   </para>
  </glossdef>
 </glossentry>
 <glossentry><glossterm>qcow (QEMU Copy on Write)</glossterm>
  <glossdef>
   <para>
    A disk image format supported by the QEMU virtual machine manager. A
    <literal>qcow2</literal> image helps to optimize disk space. It
    consumes disk space only when contents are written on it and grows as
    data is added.
   </para>
   <para>
    <literal>qcow2</literal> is a more recent version of the
    <literal>qcow</literal> format where a read-only base image is used, and
    all writes are stored to the <literal>qcow2</literal> image.
   </para>
  </glossdef>
 </glossentry>
 <glossentry xml:id="gloss.quorum"><glossterm>Quorum</glossterm>
  <glossdef>
   <para>
    In a cluster, a <xref linkend="gloss.partition"/> is defined to have
    quorum (is <quote>quorate</quote>) if it has the majority of nodes (or
    votes). Quorum distinguishes exactly one partition. It is part of the
    algorithm to prevent several disconnected partitions or nodes from
    proceeding and causing data and service corruption
    (<xref linkend="gloss.splitbrain"/>). Quorum is a prerequisite for
    <xref linkend="gloss.fencing"/>, which then ensures that quorum is
    indeed unique.
   </para>
  </glossdef>
 </glossentry>
 <glossentry><glossterm>Quota</glossterm>
  <glossdef>
   <para>
    Restriction of resources to prevent overconsumption within a cloud. In
    &ostack;, quotas are defined per project and contain multiple
    parameters, such as amount of RAM, number of instances, or number of
    floating IP addresses.
   </para>
  </glossdef>
 </glossentry>
 <glossentry><glossterm>RC File  (openrc.sh)</glossterm>
  <glossdef>
   <para>
    Environment file needed for the &ostack; command line tools. The RC
    file is project-specific and contains the credentials used by
    &ostack; &comp;, &img;, and &ident; services.
   </para>
  </glossdef>
 </glossentry>
 <glossentry><glossterm>Recipe</glossterm>
  <glossdef>
   <para>
    A group of &chef; scripts and templates. Recipes are used by
    &chef; to deploy a unit of functionality.
   </para>
  </glossdef>
 </glossentry>
 <glossentry><glossterm>Region</glossterm>
  <glossdef>
   <para>
    <remark>taroth 2013-06-12: DEVs, is the following correct?</remark>
    An &ostack; method of aggregating clouds. Regions are a robust way to
    share some infrastructure between &ostack; compute installations,
    while allowing for a high degree of failure tolerance. Regions have a
    separate API endpoint per installation.
   </para>
  </glossdef>
 </glossentry>
 <glossentry xml:id="gloss.rsc"><glossterm>Resource</glossterm>
  <glossdef>
   <para>
    In a &ha; context: Any type of service or application that is known
    to the cluster resource manager. Examples include an IP address, a file
    system, or a database.
   </para>
  </glossdef>
 </glossentry>
 <glossentry xml:id="gloss.ra"><glossterm>Resource Agent (RA)</glossterm>
  <glossdef>
   <para>
    A script acting as a proxy to manage a resource in a &ha; cluster.
    For example, it can start, stop or monitor a resource.
   </para>
  </glossdef>
 </glossentry>
 <glossentry><glossterm>Role</glossterm>
  <glossdef>
   <para>
    In the &crow;/&chef; context: an instance of a
    <xref linkend="gloss.proposal"/> that is active on a node.
   </para>
   <para>
    In the <xref linkend="gloss.keystone"/> context: concept of controlling
    the actions or set of operations that a user is allowed to perform. A
    role includes a set of rights and privileges. A user assuming that role
    inherits those rights and privileges.
   </para>
<!--taroth 2012-08-09: just found out that the following concept is
    obviously no longer valid in Essex,
    see https://bugs.launchpad.net/openstackbook/+bug/1000345:
    There are user-specific roles (sometimes called global roles) and
    project-specific roles. Therefore, a userâ€™s permissions in a particular
    project is defined by the intersection of his user-specific and is
    project-specific roles.-->
  </glossdef>
 </glossentry>
 <glossentry><glossterm>S3 (Amazon Simple Storage Service)</glossterm>
  <glossdef>
   <para>
    An object storage by Amazon that can be used to store and retrieve data
    on the Web. Similar in function to &ostack; &objstore;. It can act
    as a back-end store for &o_img; images.
   </para>
  </glossdef>
 </glossentry>
 <glossentry><glossterm>SaaS (Software-as-a-Service)</glossterm>
  <glossdef>
   <para>
    A service model of cloud computing where applications are hosted by a
    service provider and made available to customers remotely as a Web-based
    service.
   </para>
  </glossdef>
 </glossentry>
 <glossentry><glossterm>SBD (&stonith; Block Device)</glossterm>
  <glossdef>
   <para>
    In an environment where all nodes of a &ha; cluster have access to
    shared storage, a small partition is used for disk-based fencing.
   </para>
  </glossdef>
 </glossentry>
 <glossentry><glossterm>Security Group</glossterm>
  <glossdef>
   <para>
    Concept in &ostack; &netw;. A security group is a container for
    security group rules. Security group rules allow to specify the type of
    traffic and direction (ingress/egress) that is allowed to pass through a
    port.
   </para>
  </glossdef>
 </glossentry>
 <glossentry xml:id="gloss.spof"><glossterm>Single Point of Failure (SPOF)</glossterm>
  <glossdef>
   <para>
    An individual piece of equipment or software which will cause system
    downtime or data loss if it fails. To eliminate single points of
    failure, &ha; systems seek to provide redundancy for crucial pieces
    of equipment or software.
   </para>
  </glossdef>
 </glossentry>
  <glossentry xml:id="gloss.hammer"><glossterm>SLEShammer</glossterm>
  <glossdef>
   <para>
    When you first boot a node in &productname; via PXE, it is booted with the SLEShammer image. This performs the initial hardware discovery, and registers the node with &crow;.
    After you allocate the node, it is rebooted with a regular SLES installation image.
   </para>
  </glossdef>
 </glossentry> 
 <glossentry><glossterm>Snapshot</glossterm>
  <glossdef>
   <para>
    See <xref linkend="gloss.volsnap"/> or <xref linkend="gloss.instsnap"/>.
   </para>
  </glossdef>
 </glossentry>
 <glossentry xml:id="gloss.splitbrain"><glossterm>Split Brain</glossterm>
  <glossdef>
   <para>
    Also known as a <quote>partitioned cluster</quote> scenario. Either
    through a software or hardware failure, the cluster nodes are divided
    into two or more groups that do not know of each other.
    <xref linkend="gloss.stonith"/> prevents a split brain situation from
    badly affecting the entire cluster.
   </para>
  </glossdef>
 </glossentry>
 <glossentry xml:id="gloss.stateful"><glossterm>Stateful Service</glossterm>
  <glossdef>
   <para>
    A service where subsequent requests to the service depend on the results
    of the first request.
   </para>
  </glossdef>
 </glossentry>
 <glossentry xml:id="gloss.stateless"><glossterm>Stateless Service</glossterm>
  <glossdef>
   <para>
    A service that provides a response after your request, and then requires
    no further attention.
<!--To make a stateless service highly available, you need to provide redundant instances
    and load balance them.-->
   </para>
  </glossdef>
 </glossentry>
 <glossentry xml:id="gloss.stonith"><glossterm>&stonith;</glossterm>
  <glossdef>
   <para>
    The acronym for <quote>Shoot the other node in the head</quote>. It
    refers to the fencing mechanism that shuts down a misbehaving node to
    prevent it from causing trouble in a cluster.
   </para>
  </glossdef>
 </glossentry>
 <glossentry><glossterm>&stornode;</glossterm>
  <glossdef>
   <para>
    Node within a &cloud;. Acts as the controller for cloud-based
    storage. A cloud can contain multiple &stornode;s.
   </para>
  </glossdef>
 </glossentry>
 <glossentry><glossterm>Subnet</glossterm>
  <glossdef>
   <para>
    In the &ostack; &netw; API: A block of IP addresses and other
    network configuration (for example, a default gateway, DNS servers) that
    can be associated with an &ostack; &netw; network. Each subnet
    represents an IPv4 or IPv6 address block. Multiple subnets can be
    associated with a network, if necessary.
   </para>
  </glossdef>
 </glossentry>
 <glossentry><glossterm>&cloud; Administrator</glossterm>
  <glossdef>
   <para>
    User role in &productname;. Manages projects, users, images, flavors,
    and quotas within &productname;.
   </para>
  </glossdef>
 </glossentry>
 <glossentry><glossterm>&cloud; &dash;</glossterm>
  <glossdef>
   <para>
    The &productnamereg; &dash; is a Web interface that enables cloud
    administrators and users to manage various &ostack; services. It is based
    on &ostack; &dash; (also known under its codename
    <literal>Horizon</literal>).
   </para>
  </glossdef>
 </glossentry>
 <glossentry><glossterm>&cloud; Operator</glossterm>
  <glossdef>
   <para>
    User role in &productname;. Installs and deploys &productname;.
   </para>
  </glossdef>
 </glossentry>
 <glossentry><glossterm>&cloud; User</glossterm>
  <glossdef>
   <para>
    User role in &productname;. End user who launches and manages
    instances, can create snapshots, and use volumes for persistent storage
    within &productname;.
   </para>
  </glossdef>
 </glossentry>
 <glossentry xml:id="gloss.hasi"><glossterm>&sle; &hasi;</glossterm>
  <glossdef>
   <para>
    An integrated suite of open source clustering technologies that enables
    you to implement highly available physical and virtual Linux clusters.
   </para>
  </glossdef>
 </glossentry>
 <glossentry><glossterm>&o_objstore;</glossterm>
  <glossdef>
   <para>
    Code name for <xref linkend="gloss.swift"/>.
   </para>
  </glossdef>
 </glossentry>
 <glossentry><glossterm>TAP Device</glossterm>
  <glossdef>
   <para>
    A virtual networking device. A TAP device, such as
    <literal>vnet0</literal> is how hypervisors such as &kvm; and
    &xen; implement a virtual network interface card (vNIC). An Ethernet
    frame sent to a TAP device is received by the guest operating system.
    The tap option connects the network stack of the guest operating system
    to a TAP network device on the host.
   </para>
  </glossdef>
 </glossentry>
 <glossentry xml:id="gloss.ceilo"><glossterm>&meter;</glossterm>
  <glossdef>
   <para>
    A module (code name: <literal>&o_meter;</literal>) for metering
    &ostack;-based clouds. The project aims to provide a unique point of
    contact across all &ostack; core components for acquiring metrics.
    The metrics can then be consumed by other components such as customer billing.
    The module is an integrated project of &ostack; as of the Havana
    release.
   </para>
  </glossdef>
 </glossentry>
 <glossentry><glossterm>Tenant</glossterm>
  <glossdef>
   <para>
    See <xref linkend="gloss.project"/>.
   </para>
  </glossdef>
 </glossentry>
 <glossentry><glossterm>Unicast</glossterm>
  <glossdef>
   <para>
    A technology for sending messages to a single network destination.
    &corosync; supports both multicast and unicast. In &corosync;,
    unicast is implemented as UDP-unicast (UDPU).
   </para>
  </glossdef>
 </glossentry>
 <glossentry><glossterm>User</glossterm>
  <glossdef>
   <para>
    In the &ostack; context, a digital representation of a person,
    system, or service who uses &ostack; cloud services. Users can be
    directly assigned to a particular project and behave as if they are
    contained in that project.
   </para>
  </glossdef>
 </glossentry>
 <glossentry><glossterm>Veth Pair</glossterm>
  <glossdef>
   <para>
    A virtual networking device.
    <remark>taroth 2013-07-02: check the following!</remark>
    The acronym veth stands for virtual Ethernet interface. A veth is a pair
    of virtual network interfaces correctly
<!--taroth 2013-07-02:
    the sentence is copied from the ostack networking guide but it does not make much sense - maybe
    "correctly" should be "connected"???-->
    directly together. An Ethernet frame sent to one end of a veth pair is
    received by the other end of a veth pair. &ostack; &netw; uses
    veth pairs as virtual patch cables to make connections between virtual
    bridges.
   </para>
  </glossdef>
 </glossentry>
 <glossentry><glossterm>VLAN</glossterm>
  <glossdef>
   <para>
    A physical method for network virtualization. VLANs allow to create
    virtual networks across a distributed network. Disparate hosts
    (on independent networks) appear as if they were part of the same
    broadcast domain.
   </para>
  </glossdef>
 </glossentry>
 <glossentry><glossterm>VM (Virtual Machine)</glossterm>
  <glossdef>
   <para>
    An operating system instance that runs on top of a hypervisor. Multiple
    virtual machines can run on the same physical host at the same time.
   </para>
  </glossdef>
 </glossentry>
 <glossentry><glossterm>vNIC </glossterm>
  <glossdef>
   <para>
    Virtual network interface card.
   </para>
  </glossdef>
 </glossentry>
 <glossentry><glossterm>Volume</glossterm>
  <glossdef>
   <para>
    Detachable block storage device. Unlike a SAN, it can only be attached
    to one instance at a time.
   </para>
  </glossdef>
 </glossentry>
 <glossentry xml:id="gloss.vg"><glossterm>Volume Group (VG)</glossterm>
  <glossdef>
   <para>
    A virtual disk consisting of aggregated physical volumes. Volume groups
    can be logically partitioned into logical volumes.
   </para>
  </glossdef>
 </glossentry>
 <glossentry xml:id="gloss.volsnap"><glossterm>Volume Snapshot</glossterm>
  <glossdef>
   <para>
    A point-in-time copy of an &ostack; storage volume. Used to back up
    volumes.
   </para>
  </glossdef>
 </glossentry>
 <glossentry><glossterm>vSwitch (Virtual Switch)</glossterm>
  <glossdef>
   <para>
    <remark>taroth 2013-07-02: check!</remark>
    A software that runs on a host or node and provides the features and
    functions of a hardware-based network switch.
   </para>
  </glossdef>
 </glossentry>
 <glossentry><glossterm>Zone</glossterm>
  <glossdef>
   <para>
    <remark>taroth 2013-06-12: DEVs, is the following correct?</remark>
    A logical grouping of &comp; services and virtual machine hosts.
   </para>
  </glossdef>
 </glossentry>
</glossary>
