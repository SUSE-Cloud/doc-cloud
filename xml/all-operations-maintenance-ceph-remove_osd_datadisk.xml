<?xml version="1.0"?>
<!DOCTYPE section [
 <!ENTITY % entities SYSTEM "entities.ent"> %entities;
]>
<!---->
<section xmlns="http://docbook.org/ns/docbook" xmlns:xi="http://www.w3.org/2001/XInclude" xmlns:xlink="http://www.w3.org/1999/xlink" xml:id="remove_datadisk_osd"><title>&kw-hos-tm; &kw-hos-version-50;: Removing a Data Disk from a Ceph OSD
    Node</title><abstract><para><para>Steps for removing data disks from your OSD
      nodes.</para>If you want to remove a data disks from your existing OSD nodes, these steps
    will show you how to.</para>
</abstract>
    <!---->
    <para><para xml:id="idg-all-operations-maintenance-ceph-remove_osd_datadisk-xml-4"><phrase><phrase/></phrase></para></para>

    <sidebar xml:id="idg-all-operations-maintenance-ceph-remove_osd_datadisk-xml-6"><title>Removing a Data Disk from an Object Storage Daemon (OSD) Node</title><orderedlist>
        <listitem><para>Log in to the OSD node you want to remove the data disk from.</para>
</listitem>
        <listitem><para>Figure out the OSD number for the corresponding disk using this command:
          </para>
<screen>sudo ceph-disk list | grep &lt;disk-to-be-removed&gt; | grep osd</screen></listitem>
        <listitem><para>Using the output from the command in the previous step, locate the OSD number. </para>
<para>In the
            example below, the OSD number is <literal>4</literal>:</para>
<screen>/dev/sdn1 ceph data, active, cluster ceph, osd.4, journal /dev/sdj1</screen></listitem>
        <listitem><para>Use the following commands to take the OSD node out of the Ceph cluster and observe the
          cluster rebalance itself: </para>
<screen>ceph osd out &lt;osd_number&gt;
ceph -w</screen><para>The placement group states will change from <literal>active+clean</literal> to
              <literal>active</literal>, some <literal>degraded</literal> objects, and then finally back
            to <literal>active+clean</literal> once the migration completes. You can press CTRL+C to
            exit.</para>
</listitem>
        <listitem><para>Use the following commands to stop the OSD service, remove it from the CRUSH map, delete
          its authentication key, and remove the OSD node:
          </para>
<screen>sudo systemctl stop ceph-osd@&lt;osd_number&gt;
ceph osd crush remove osd.&lt;osd_number&gt;
ceph auth del osd.&lt;osd_number&gt;
ceph osd rm &lt;osd_number&gt;</screen></listitem>
        <listitem><para>Unmount the OSD data
          disk:</para>
<screen>sudo umount &lt;mount_path_of_datadisk&gt;</screen></listitem>
        <listitem><para>This step will remove the OSD's directories and should be performed with caution: </para>
<screen>sudo rm -rf /var/lib/ceph/osd/&lt;ceph_cluster&gt;-&lt;osd_number&gt;</screen><para>where:</para>
<informaltable xml:id="table_sgn_m5r_35" colsep="1" rowsep="1"><tgroup cols="2">
              <colspec colname="c1" colnum="1" colwidth="1.0*"/>
              <colspec colname="c2" colnum="2" colwidth="1.0*"/>
              <thead>
                <row>
                  <entry>Variable</entry>
                  <entry>Description</entry>
                </row>
              </thead>
              <tbody>
                <row>
                  <entry>&lt;ceph_cluster&gt;</entry>
                  <entry>This should be replaced with the name of your Ceph cluster.</entry>
                </row>
                <row>
                  <entry>&lt;osd_number&gt;</entry>
                  <entry>This should be replaced with the number identified in step #3
                    earlier.</entry>
                </row>
              </tbody>
            </tgroup></informaltable></listitem>

        <listitem><para>Remove the mount instructions for the same disk(s) from the <literal>/etc/fstab</literal>
          file. For example, to remove the data disk <literal>/dev/sde</literal>, find out its UUID by
          executing this command:</para>
<screen>sudo blkid | grep "ceph data"</screen><para>Example output:</para>
<screen>$ sudo blkid |grep "ceph data"
/dev/sdc1: UUID="45b7c7e8-4f2d-4226-989b-c83e7e9e2596" TYPE="xfs" PARTLABEL="ceph data" PARTUUID="6c37a311-eabf-4872-bd8e-7d426585432e"
/dev/sdf1: UUID="1cfb4de2-fe21-4e20-9a6e-ad45e90949ae" TYPE="xfs" PARTLABEL="ceph data" PARTUUID="209b9af4-8d25-4d60-a430-2934ceca6042"
/dev/sde1: UUID="51c32ca1-0ee1-4c75-bfbc-5d113a4b4402" TYPE="xfs" PARTLABEL="ceph data" PARTUUID="dfcd410d-9c56-4bb1-93c6-496bae29de5f"</screen><para>In the example above, the UUID <literal>51c32ca1-0ee1-4c75-bfbc-5d113a4b4402</literal>
            corresponds to the data disk being removed (i.e. <literal>/dev/sde</literal>. Remove the
            line in the <literal>/etc/fstab</literal> file having the entry starting with the text
              <literal>UUID="51c32ca1-0ee1-4c75-bfbc-5d113a4b4402"</literal>.</para>
</listitem>
        <listitem><para>Delete the journal partition for each OSD number removed and instruct the kernel to
          reload the table as
          follows:</para>
<screen>(echo d; echo &lt;partition-number&gt;; echo w) | sudo fdisk &lt;disk&gt;
sudo partprobe

Example:
In step #3 above, /dev/sdj1 journal partition can be removed by executing below commands: 
$ (echo d; echo 1; echo w) | sudo fdisk /dev/sdj
$ sudo partprobe</screen></listitem>
        <listitem><para>Repeat steps 1-9 for each OSD node in the Ceph cluster one at a time.</para>
</listitem>
        <listitem><para>Log in to the lifecycle manager.</para>
</listitem>
        <listitem><para>Edit the <literal>~/helion/my_cloud/definition/data/disks_osd.yml</literal> file and
          remove the details for the corresponding data disk that you removed earlier.
            </para>
<para>Example:</para>
<screen>- name: ceph-osd-data-and-journal
  devices:
     - name: /dev/sdn
  consumer:
     name: ceph
     attrs:
     usage: data
     journal_disk: /dev/sdj</screen></listitem>
        <listitem><para>Commit your configuration to the <xref linkend="using_git"/>, as follows:
          </para>
<screen>cd ~/helion/hos/ansible
git add -A
git commit -m "Removing OSD data disk"</screen></listitem>
        <listitem><para>Run the configuration processor:
          </para>
<screen>cd ~/helion/hos/ansible
ansible-playbook -i hosts/localhost config-processor-run.yml</screen></listitem>
        <listitem><para>Update your deployment directory:
          </para>
<screen>cd ~/helion/hos/ansible
ansible-playbook -i hosts/localhost ready-deployment.yml</screen></listitem>
        <listitem><para>Check the status of Ceph services with this playbook:
          </para>
<screen>cd ~/scratch/ansible/next/hos/ansible
ansible-playbook -i hosts/verb_hosts ceph-status.yml</screen></listitem>
      </orderedlist></sidebar>
  </section>
