<?xml version="1.0"?>
<?xml-stylesheet href="urn:x-suse:xslt:profiling:docbook51-profile.xsl"
 type="text/xml"
 title="Profiling step"?>
<!DOCTYPE section [
 <!ENTITY % entities SYSTEM "entity-decl.ent"> %entities;
]>
<section xmlns="http://docbook.org/ns/docbook" xmlns:xi="http://www.w3.org/2001/XInclude" xmlns:xlink="http://www.w3.org/1999/xlink" xml:id="OctaviaAdmin">
 <title>Load Balancer: Octavia Driver Administration</title>
 <para>
  This document provides the instructions on how to enable and manage various
  components of the Load Balancer Octavia driver if that driver is enabled.
 </para>
 <itemizedlist>
  <listitem>
   <para>
    <xref linkend="Alerts"/>
   </para>
  </listitem>
  <listitem>
   <para>
    <xref linkend="Tuning"/>
   </para>
   <itemizedlist>
    <listitem>
     <para>
      Homogeneous Compute Configuration
     </para>
    </listitem>
    <listitem>
     <para>
      Octavia and Floating IP's
     </para>
    </listitem>
    <listitem>
     <para>
      Configuration Files
     </para>
    </listitem>
    <listitem>
     <para>
      Spare Pools
     </para>
    </listitem>
   </itemizedlist>
  </listitem>
  <listitem>
   <para>
    <xref linkend="Amphora"/>
   </para>
   <itemizedlist>
    <listitem>
     <para>
      Updating the Cryptographic Certificates
     </para>
    </listitem>
    <listitem>
     <para>
      Accessing VM information in &o_comp;
     </para>
    </listitem>
    <listitem>
     <para>
      Initiating Failover of an Amphora VM
     </para>
    </listitem>
   </itemizedlist>
  </listitem>
 </itemizedlist>
 <section xml:id="Alerts">
  <title>Monasca Alerts</title>
  <para>
   The monasca-agent has the following Octavia-related plugins:
  </para>
  <itemizedlist>
   <listitem>
    <para>
     Process checks – checks if octavia processes are running. When it
     starts, it detects which processes are running and then monitors them.
    </para>
   </listitem>
   <listitem>
    <para>
     http_connect check – checks if it can connect to octavia api servers.
    </para>
   </listitem>
  </itemizedlist>
  <para>
   Alerts are displayed in the &opscon;.
  </para>
 </section>
 <section xml:id="Tuning">
  <title>Tuning Octavia Installation</title>
  <para>
   <emphasis role="bold">Homogeneous Compute Configuration</emphasis>
  </para>
  <para>
   Octavia works only with homogeneous compute node configurations. Currently,
   Octavia does not support multiple &o_comp; flavors. If Octavia needs to be
   supported on multiple compute nodes, then all the compute nodes should carry
   same set of physnets (which will be used for Octavia).
  </para>
  <para>
   <emphasis role="bold">Octavia and Floating IPs</emphasis>
  </para>
  <para>
   Due to a neutron limitation Octavia will only work with CVR routers. Another
   option is to use VLAN provider networks which do not require a router.
  </para>
  <para>
   You cannot currently assign a floating IP address as the VIP (user facing)
   address for a load balancer created by the Octavia driver if the underlying
   neutron network is configured to support Distributed Virtual Router (DVR).
   The Octavia driver uses a neutron function known as
   <emphasis>allowed address pairs</emphasis>
   to support load balancer fail over.
  </para>
  <para>
   There is currently a neutron bug that does not support this function in a
   DVR configuration
  </para>
  <para>
   <emphasis role="bold">Octavia Configuration Files</emphasis>
  </para>
  <para>
   The system comes pre-tuned and should not need any adjustments for most
   customers. If in rare instances manual tuning is needed, follow these steps:
  </para>
  <warning>
   <para>
    Changes might be lost during &productname; upgrades.
   </para>
  </warning>
  <para>
   Edit the Octavia configuration files in
   <literal>my_cloud/config/octavia</literal>. It is recommended that any
   changes be made in all of the Octavia configuration files.
  </para>
  <itemizedlist>
   <listitem>
    <para>
     octavia-api.conf.j2
    </para>
   </listitem>
   <listitem>
    <para>
     octavia-health-manager.conf.j2
    </para>
   </listitem>
   <listitem>
    <para>
     octavia-housekeeping.conf.j2
    </para>
   </listitem>
   <listitem>
    <para>
     octavia-worker.conf.j2
    </para>
   </listitem>
  </itemizedlist>
  <para>
   After the changes are made to the configuration files, redeploy the service.
  </para>
  <orderedlist>
   <listitem>
    <para>
     Commit changes to git.
    </para>
<screen>&prompt.ardana;cd ~/openstack
&prompt.ardana;git add -A
&prompt.ardana;git commit -m "My Octavia Config"</screen>
   </listitem>
   <listitem>
    <para>
     Run the configuration processor and ready deployment.
    </para>
<screen>&prompt.ardana;cd ~/openstack/ardana/ansible/
&prompt.ardana;ansible-playbook -i hosts/localhost config-processor-run.yml
&prompt.ardana;ansible-playbook -i hosts/localhost ready-deployment.yml</screen>
   </listitem>
   <listitem>
    <para>
     Run the Octavia reconfigure.
    </para>
<screen>&prompt.ardana;cd ~/scratch/ansible/next/ardana/ansible
&prompt.ardana;ansible-playbook -i hosts/verb_hosts octavia-reconfigure.yml</screen>
   </listitem>
  </orderedlist>
  <para>
   <emphasis role="bold">Spare Pools</emphasis>
  </para>
  <para>
   The Octavia driver provides support for creating spare pools of the HAProxy
   software installed in VMs. This means instead of creating a new load
   balancer when loads increase, create new load balancer calls will pull a
   load balancer from the spare pool. The spare pools feature consumes
   resources, therefore the load balancers in the spares pool has been set to
   0, which is the default and also disables the feature.
  </para>
  <para>
   Reasons to enable a load balancing spare pool in &productname;
  </para>
  <orderedlist>
   <listitem>
    <para>
     You expect a large number of load balancers to be provisioned all at once
     (puppet scripts, or ansible scripts) and you want them to come up quickly.
    </para>
   </listitem>
   <listitem>
    <para>
     You want to reduce the wait time a customer has while requesting a new
     load balancer.
    </para>
   </listitem>
  </orderedlist>
  <para>
   To increase the number of load balancers in your spares pool, edit the
   Octavia configuration files by uncommenting the
   <literal>spare_amphora_pool_size</literal> and adding the number of load
   balancers you would like to include in your spares pool.
  </para>
<screen># Pool size for the spare pool
# spare_amphora_pool_size = 0</screen>
  <important>
   <!-- FIXME: Removed version number 3.0 from following para. Still correct?
   - sknorr, 2018-03-27 -->
   <para>
    In &productname; the spare pool cannot be used to speed up fail
    overs. If a load balancer fails in &productname;, Octavia will always provision
    a new VM to replace that failed load balancer.
   </para>
  </important>
 </section>
 <section xml:id="Amphora">
  <title>Managing Amphora</title>
  <para>
   Octavia starts a separate VM for each load balancing function. These VMs are
   called amphora.
  </para>
  <para>
   <emphasis role="bold">Updating the Cryptographic Certificates</emphasis>
  </para>
  <para>
   Octavia uses two-way SSL encryption for communication between amphora and
   the control plane. Octavia keeps track of the certificates on the amphora
   and will automatically recycle them. The certificates on the control plane
   are valid for one year after installation of &productname;.
  </para>
  <para>
   You can check on the status of the certificate by logging into the
   controller node as root and running:
  </para>
<screen>&prompt.ardana;cd /opt/stack/service/octavia-<replaceable>SOME UUID</replaceable>/etc/certs/
openssl x509 -in client.pem  -text –noout</screen>
  <para>
   This prints the certificate out where you can check on the expiration dates.
  </para>
  <para>
   To renew the certificates, reconfigure Octavia. Reconfiguring causes Octavia
   to automatically generate new certificates and deploy them to the controller
   hosts.
  </para>
  <para>
   On the &clm; execute octavia-reconfigure:
  </para>
<screen>&prompt.ardana;cd ~/scratch/ansible/next/ardana/ansible
&prompt.ardana;ansible-playbook -i hosts/verb_hosts octavia-reconfigure.yml</screen>
  <para>
   <emphasis role="bold">Accessing VM information in &o_comp;</emphasis>
  </para>
  <para>
   You can use <literal>openstack project list</literal> as an administrative
   user to obtain information about the tenant or project-id of the Octavia
   project. In the example below, the Octavia project has a project-id of
   <literal>37fd6e4feac14741b6e75aba14aea833</literal>.
  </para>
<screen>&prompt.ardana;openstack project list
+----------------------------------+------------------+
| ID                               | Name             |
+----------------------------------+------------------+
| 055071d8f25d450ea0b981ca67f7ccee | glance-swift     |
| 37fd6e4feac14741b6e75aba14aea833 | octavia          |
| 4b431ae087ef4bd285bc887da6405b12 | swift-monitor    |
| 8ecf2bb5754646ae97989ba6cba08607 | swift-dispersion |
| b6bd581f8d9a48e18c86008301d40b26 | services         |
| bfcada17189e4bc7b22a9072d663b52d | cinderinternal   |
| c410223059354dd19964063ef7d63eca | monitor          |
| d43bc229f513494189422d88709b7b73 | admin            |
| d5a80541ba324c54aeae58ac3de95f77 | demo             |
| ea6e039d973e4a58bbe42ee08eaf6a7a | backup           |
+----------------------------------+------------------+</screen>
  <para>
   You can then use <literal>openstack server list --tenant &lt;project-id&gt;</literal> to
   list the VMs for the Octavia tenant. Take particular note of the IP address
   on the OCTAVIA-MGMT-NET; in the example below it is
   <literal>172.30.1.11</literal>. For additional nova command-line options see
   <xref linkend="idg-all-networking-octavia-admin-xml-10"/>.
  </para>
<screen>&prompt.ardana;openstack server list --tenant 37fd6e4feac14741b6e75aba14aea833
+--------------------------------------+----------------------------------------------+----------------------------------+--------+------------+-------------+------------------------------------------------+
| ID                                   | Name                                         | Tenant ID                        | Status | Task State | Power State | Networks                                       |
+--------------------------------------+----------------------------------------------+----------------------------------+--------+------------+-------------+------------------------------------------------+
| 1ed8f651-de31-4208-81c5-817363818596 | amphora-1c3a4598-5489-48ea-8b9c-60c821269e4c | 37fd6e4feac14741b6e75aba14aea833 | ACTIVE | -          | Running     | private=10.0.0.4; OCTAVIA-MGMT-NET=172.30.1.11 |
+--------------------------------------+----------------------------------------------+----------------------------------+--------+------------+-------------+------------------------------------------------+</screen>
  <important>
   <para>
    The Amphora VMs do not have SSH or any other access. In the rare case that
    there is a problem with the underlying load balancer the whole amphora will
    need to be replaced.
   </para>
  </important>
  <para>
   <emphasis role="bold">Initiating Failover of an Amphora VM</emphasis>
  </para>
  <para>
   Under normal operations Octavia will monitor the health of the amphora
   constantly and automatically fail them over if there are any issues. This
   helps to minimize any potential downtime for load balancer users. There are,
   however, a few cases a failover needs to be initiated manually:
  </para>
  <orderedlist>
   <listitem>
    <para>
     The Loadbalancer has become unresponsive and Octavia has not detected an
     error.
    </para>
   </listitem>
   <listitem>
    <para>
     A new image has become available and existing load balancers need to start
     using the new image.
    </para>
   </listitem>
   <listitem>
    <para>
     The cryptographic certificates to control and/or the HMAC password to
     verify Health information of the amphora have been compromised.<!-- FIXME
     (URL is not accessible) See
     <link xlink:href="http://octavia.io/review/master/design/version0.5/component-design.html#some-notes-on-controller-amphorae-communications">Controller
     to Amphorae communications</link> for more information. -->
    </para>
   </listitem>
  </orderedlist>
  <para>
   To minimize the impact for end users we will keep the existing load balancer
   working until shortly before the new one has been provisioned. There will be
   a short interruption for the load balancing service so keep that in mind
   when scheduling the failovers. To achieve that follow these steps (assuming
   the management ip from the previous step):
  </para>
  <orderedlist>
   <listitem>
    <para>
     Assign the IP to a SHELL variable for better readability.
    </para>
<screen>&prompt.ardana;export MGM_IP=172.30.1.11</screen>
   </listitem>
   <listitem>
    <para>
     Identify the port of the vm on the management network.
    </para>
<screen>&prompt.ardana;openstack port list | grep $MGM_IP
| 0b0301b9-4ee8-4fb6-a47c-2690594173f4 |                                                   | fa:16:3e:d7:50:92 |
{"subnet_id": "3e0de487-e255-4fc3-84b8-60e08564c5b7", "ip_address": "172.30.1.11"} |</screen>
   </listitem>
   <listitem>
    <para>
     Disable the port to initiate a failover. Note the load balancer will still
     function but cannot be controlled any longer by Octavia.
    </para>
    <note>
     <para>
      Changes after disabling the port will result in errors.
     </para>
    </note>
<screen>&prompt.ardana;openstack port set --admin-state-up False 0b0301b9-4ee8-4fb6-a47c-2690594173f4
Updated port: 0b0301b9-4ee8-4fb6-a47c-2690594173f4</screen>
   </listitem>
   <listitem>
    <para>
     You can check to see if the amphora failed over with <literal>openstack
     server list --tenant &lt;project-id&gt;</literal>. This may take some time
     and in some cases may need to be repeated several times. You can tell that
     the failover has been successful by the changed IP on the management
     network.
    </para>
<screen>&prompt.ardana;openstack server list --tenant 37fd6e4feac14741b6e75aba14aea833
+--------------------------------------+----------------------------------------------+----------------------------------+--------+------------+-------------+------------------------------------------------+
| ID                                   | Name                                         | Tenant ID                        | Status | Task State | Power State | Networks                                       |
+--------------------------------------+----------------------------------------------+----------------------------------+--------+------------+-------------+------------------------------------------------+
| 1ed8f651-de31-4208-81c5-817363818596 | amphora-1c3a4598-5489-48ea-8b9c-60c821269e4c | 37fd6e4feac14741b6e75aba14aea833 | ACTIVE | -          | Running     | private=10.0.0.4; OCTAVIA-MGMT-NET=172.30.1.12 |
+--------------------------------------+----------------------------------------------+----------------------------------+--------+------------+-------------+------------------------------------------------+</screen>
   </listitem>
  </orderedlist>
  <warning>
   <para>
    Do not issue too many failovers at once. In a big installation you might be
    tempted to initiate several failovers in parallel for instance to speed up
    an update of amphora images. This will put a strain on the &o_comp; service and
    depending on the size of your installation you might need to throttle the
    failover rate.
   </para>
  </warning>
 </section>
 <section xml:id="octavia-admin-delete">
   <title>Removing load balancers</title>
   <para>
     The following procedure demonstrates how to delete a load
     balancer that is in the <literal>ERROR</literal>,
     <literal>PENDING_CREATE</literal>, or
     <literal>PENDING_DELETE</literal> state.
   </para>
   <procedure>
     <title>Manually deleting load balancers</title>
     <step>
       <para>
         Query the Octavia service for the loadbalancer ID:
       </para>
       <screen>
&prompt.ardana;openstack loadbalancer list --column id --column name --column provisioning_status
+--------------------------------------+---------+---------------------+
| id                                   | name    | provisioning_status |
+--------------------------------------+---------+---------------------+
| d8ac085d-e077-4af2-b47a-bdec0c162928 | test-lb | ERROR               |
+--------------------------------------+---------+---------------------+
       </screen>
     </step>
     <step>
       <para>
         Query the Octavia service for the amphora IDs (in this
         example we use <literal>ACTIVE/STANDBY</literal> topology with 1 spare Amphora):
       </para>
       <screen>
&prompt.ardana;openstack loadbalancer amphora list
+--------------------------------------+--------------------------------------+-----------+--------+---------------+-------------+
| id                                   | loadbalancer_id                      | status    | role   | lb_network_ip | ha_ip       |
+--------------------------------------+--------------------------------------+-----------+--------+---------------+-------------+
| 6dc66d41-e4b6-4c33-945d-563f8b26e675 | d8ac085d-e077-4af2-b47a-bdec0c162928 | ALLOCATED | BACKUP | 172.30.1.7    | 192.168.1.8 |
| 1b195602-3b14-4352-b355-5c4a70e200cf | d8ac085d-e077-4af2-b47a-bdec0c162928 | ALLOCATED | MASTER | 172.30.1.6    | 192.168.1.8 |
| b2ee14df-8ac6-4bb0-a8d3-3f378dbc2509 | None                                 | READY     | None   | 172.30.1.20   | None        |
+--------------------------------------+--------------------------------------+-----------+--------+---------------+-------------+
       </screen>
     </step>
     <step>
       <para>
         Query the Octavia service for the loadbalancer pools:
       </para>
       <screen>
&prompt.ardana;openstack loadbalancer pool list
+--------------------------------------+-----------+----------------------------------+---------------------+----------+--------------+----------------+
| id                                   | name      | project_id                       | provisioning_status | protocol | lb_algorithm | admin_state_up |
+--------------------------------------+-----------+----------------------------------+---------------------+----------+--------------+----------------+
| 39c4c791-6e66-4dd5-9b80-14ea11152bb5 | test-pool | 86fba765e67f430b83437f2f25225b65 | ACTIVE              | TCP      | ROUND_ROBIN  | True           |
+--------------------------------------+-----------+----------------------------------+---------------------+----------+--------------+----------------+
       </screen>
     </step>
     <step>
       <para>
         Connect to the octavia database:
       </para>
       <screen>mysql&gt; use octavia</screen>
     </step>
     <step>
       <para>
         Delete any listeners, pools, health monitors, and members
         from the load balancer:
       </para>
       <screen>
mysql&gt; delete from listener where load_balancer_id = 'd8ac085d-e077-4af2-b47a-bdec0c162928';
Query OK, 1 row affected (0.01 sec)
mysql&gt; delete from health_monitor where pool_id = '39c4c791-6e66-4dd5-9b80-14ea11152bb5';
Query OK, 1 row affected (0.00 sec)
mysql&gt; delete from member where pool_id = '39c4c791-6e66-4dd5-9b80-14ea11152bb5';
Query OK, 2 rows affected (0.01 sec)
mysql&gt; delete from pool where load_balancer_id = 'd8ac085d-e077-4af2-b47a-bdec0c162928';
Query OK, 1 row affected (0.01 sec)
       </screen>
     </step>
     <step>
       <para>
         Delete the amphora entries in the database:
       </para>
       <screen>
mysql&gt; delete from amphora_health where amphora_id = '6dc66d41-e4b6-4c33-945d-563f8b26e675';
Query OK, 1 row affected (0.01 sec)
mysql&gt; update amphora set status = 'DELETED' where id = '6dc66d41-e4b6-4c33-945d-563f8b26e675';
Query OK, 1 row affected (0.00 sec)
mysql&gt; delete from amphora_health where amphora_id = '1b195602-3b14-4352-b355-5c4a70e200cf';
Query OK, 1 row affected (0.01 sec)
mysql&gt; update amphora set status = 'DELETED' where id = '1b195602-3b14-4352-b355-5c4a70e200cf';
Query OK, 1 row affected (0.00 sec)
       </screen>
     </step>
     <step>
       <para>
         Delete the load balancer instance:
       </para>
       <screen>
mysql&gt; update load_balancer set provisioning_status = 'DELETED' where id = 'd8ac085d-e077-4af2-b47a-bdec0c162928';
Query OK, 0 rows affected (0.00 sec)
       </screen>
     </step>
     <step>
       <para>
         The following script automates the above steps:
       </para>
       <screen>
#!/bin/bash

if (( $# != 1 )); then
  echo "Please specify a loadbalancer ID"
  exit 1
fi

LB_ID=$1

set -u -e -x

readarray -t AMPHORAE &lt; &lt;(openstack loadbalancer amphora list \
  --format value \
  --column id \
  --column loadbalancer_id \
  | grep ${LB_ID} \
  | cut -d ' ' -f 1)

readarray -t POOLS &lt; &lt;(openstack loadbalancer show ${LB_ID} \
  --format value \
  --column pools)

mysql octavia --execute "delete from listener where load_balancer_id = '${LB_ID}';"
for p in "${POOLS[@]}"; do
  mysql octavia --execute "delete from health_monitor where pool_id = '${p}';"
  mysql octavia --execute "delete from member where pool_id = '${p}';"
done
mysql octavia --execute "delete from pool where load_balancer_id = '${LB_ID}';"
for a in "${AMPHORAE[@]}"; do
  mysql octavia --execute "delete from amphora_health where amphora_id = '${a}';"
  mysql octavia --execute "update amphora set status = 'DELETED' where id = '${a}';"
done
mysql octavia --execute "update load_balancer set provisioning_status = 'DELETED' where id = '${LB_ID}';"
       </screen>
     </step>
   </procedure>
 </section>
 <section xml:id="idg-all-networking-octavia-admin-xml-10">
  <title>For More Information</title>
  <para>
   For more information on the OpenStackClient and Octavia terminology, see the
   <link
   xlink:href="https://docs.openstack.org/python-openstackclient/latest/">OpenStackClient</link>
   guide.
  </para>
 </section>
</section>
