<?xml version="1.0"?>
<!DOCTYPE section [
 <!ENTITY % entities SYSTEM "entities.ent"> %entities;
]>
<section xml:id="ses-integration"
 xmlns="http://docbook.org/ns/docbook" version="5.1"
 xmlns:xi="http://www.w3.org/2001/XInclude"
 xmlns:xlink="http://www.w3.org/1999/xlink">
 <title>&ses; Integration</title>
 <para>
  The current version of &productname; supports integration with &ses; (SES).
  Integrating &ses; enables &ceph; block storage as well as object and image
  storage services in &productname;.
 </para>
 <section xml:id="ses-installation">
  <title>Enabling &ses; Integration</title>
  <para>
   The &ses; integration is provided through the <package>ardana-ses</package>
   RPM package. This package is included in the
   <systemitem>patterns-cloud-ardana</systemitem> pattern, its installation is
   covered in <xref linkend="cha-depl-dep-inst"/>. The update repositories and
   the installation covered there are required to support &ses; integration.
   The latest updates should be applied before proceeding.
  </para>
 </section>
 <section xml:id="ses-config">
  <title>Configuration</title>
  <para>
   After the &ses; integration package has been installed, it must be
   configured. Files that contain relevant &ses;/&ceph; deployment information
   must be placed into a directory on the deployer node. This includes the
   configuration file that describes various aspects of the &ceph; environment
   as well as keyrings for each user and pool created in the &ceph;
   environment. In addition to that, you need to edit the
   <filename>settings.yml</filename> file to enable the &ses; integration to
   run and update all of the &productname; service configuration files.
  </para>
  <para>
   The <filename>settings.yml</filename> file resides in the
   <filename>~/openstack/my_cloud/config/ses/</filename> directory. Open the
   file for editing, uncomment the <literal>ses_config_path:</literal>
   parameter to specify the location on the deployer host containing the
   <filename>ses_config.yml</filename> file and all &ceph; keyring files. For example:
  </para>
<screen># the directory where the ses config files are.
ses_config_path: /var/lib/ardana/openstack/my_cloud/config/ses/
ses_config_file: ses_config.yml

# Allow nova libvirt images_type to be set to rbd?
# Set this to false, if you only want rbd_user and rbd_secret to be set
# in the [libvirt] section of hypervisor.conf
ses_nova_set_images_type: True

# The unique uuid for use with virsh for cinder and nova
ses_secret_id: 457eb676-33da-42ec-9a8c-9293d545c337</screen>
  <important>
   <para>
    If you are integrating with &ses; and do not want to store &o_comp; images
    in &ceph;, change the line in <filename>settings.yml</filename> from
    <literal>ses_nova_set_images_type: True</literal> to
    <literal>ses_nova_set_images_type: False</literal>
   </para>
    
   <para>
    For security reasons, you should use a unique UUID in the
    <filename>settings.yml</filename> file for
    <literal>ses_secret_id</literal>, replacing the fixed, hard-coded UUID in
    that file. You can generate a UUID that will be unique to your deployment
    using the command <command>uuidgen</command>.
   
   </para>
  </important>
  <para>
   For SES deployments that have version 5.5 and higher, there is a Salt runner
   that can create all the users, keyrings, and pools. It will also generate a yaml
   configuration that is needed to integrate with &cloud;. The integration
   runner will create the <literal>cinder</literal>, <literal>cinder-backup</literal>, and <literal>glance</literal>
   &ceph; users. Both the &o_blockstore; and &o_comp; services will have the
   same user, as &o_blockstore; needs access to create objects that &o_comp;
   uses.
  </para>
  <para>
   Login in as root to run the SES 5.5 Salt runner on the salt admin host.
  </para>
<screen>&prompt.root;salt-run --out=yaml openstack.integrate prefix=<replaceable>mycloud</replaceable></screen>
  <para>
   The prefix parameter allows pools to be created with the specified prefix.
   In this way, multiple cloud deployments can use different users and pools on
   the same SES deployment.
  </para>
  <para>
   The sample yaml output:
  </para>
<screen>ceph_conf:
 cluster_network: 10.84.56.0/21
 fsid: d5d7c7cb-5858-3218-a36f-d028df7b0673
 mon_host: 10.84.56.8, 10.84.56.9, 10.84.56.7
 mon_initial_members: ses-osd1, ses-osd2, ses-osd3
 public_network: 10.84.56.0/21
 cinder:
 key: AQCdfIRaxefEMxAAW4zp2My/5HjoST2Y8mJg8A==
 rbd_store_pool: mycloud-cinder
 rbd_store_user: cinder
 cinder-backup:
 key: AQBb8hdbrY2bNRAAqJC2ZzR5Q4yrionh7V5PkQ==
 rbd_store_pool: mycloud-backups
 rbd_store_user: cinder-backup
 glance:
 key: AQD9eYRachg1NxAAiT6Hw/xYDA1vwSWLItLpgA==
 rbd_store_pool: mycloud-glance
 rbd_store_user: glance
 nova:
 rbd_store_pool: mycloud-nova
 radosgw_urls:
     - http://10.84.56.7:80/swift/v1
     - http://10.84.56.8:80/swift/v1 </screen>
     <para>
      After you have run the <literal>openstack.integrate</literal> runner,
      copy the yaml output into the new <filename>ses_config.yml</filename>
      file, and save this file in the path specified in the
      <filename>settings.yml</filename> file on the deployer node. In this
      case, the file <filename>ses_config.yml</filename> must be saved in the
      <filename>/var/lib/ardana/openstack/my_cloud/config/ses/</filename>
      directory on the deployer node.
     </para>

     <para>
      For &ses;/&ceph; deployments that have version older than 5.5, the
      following applies. For &ceph;, it is necessary to create pools and users
      to allow the &productname; services to use the &ses;/&ceph;
      cluster. Pools and users must be created for the &o_blockstore;,
      &o_blockstore; backup, and &o_img; services. Both the &o_blockstore; and
      &o_comp; services must have the same user, as &o_blockstore; needs access
      to create objects that &o_comp; uses.  Instructions for creating and
      managing pools, users and keyrings is covered in the &ses; documentation
      under <link
      xlink:href="https://documentation.suse.com/en-us/ses/5.5/single-html/ses-admin/#storage-cephx-keymgmt">Key
      Management</link>.
     </para>
     <para>
      Example of <filename>ses_config.yml</filename>:
     </para>
<screen>ses_cluster_configuration:
    ses_cluster_name: ceph
    ses_radosgw_url: "https://192.168.56.8:8080/swift/v1"

    conf_options:
        ses_fsid: d5d7c7cb-5858-3218-a36f-d028df7b1111
        ses_mon_initial_members: ses-osd2, ses-osd3, ses-osd1
        ses_mon_host: 192.168.56.8, 192.168.56.9, 192.168.56.7
        ses_public_network: 192.168.56.0/21
        ses_cluster_network: 192.168.56.0/21

    cinder:
        rbd_store_pool: cinder
        rbd_store_pool_user: cinder
        keyring_file_name: ceph.client.cinder.keyring

    cinder-backup:
        rbd_store_pool: backups
        rbd_store_pool_user: cinder_backup
        keyring_file_name: ceph.client.cinder-backup.keyring

    # Nova uses the cinder user to access the nova pool, cinder pool
    # So all we need here is the nova pool name.
    nova:
        rbd_store_pool: nova

    glance:														
        rbd_store_pool: glance												
        rbd_store_pool_user: glance
        keyring_file_name: ceph.client.glance.keyring</screen>

	<para>
	 Example contents of the directory specified in <filename>settings.yml</filename> file:
	</para>

<screen>ardana > ~/openstack/my_cloud/config/ses> ls -al
ceph.client.cinder-backup.keyring											
ceph.client.cinder.keyring												
ceph.client.glance.keyring					
ses_config.yml</screen>

<para>
 Modify the <literal>glance_default_store</literal> option in <filename>~/openstack/my_cloud/definition/data/control_plane.yml</filename>:
</para>

<screen>.
.
            - rabbitmq
#            - glance-api
            - glance-api:
                glance_default_store: 'rbd'
            - glance-registry
            - glance-client</screen>
	<procedure>
    <step>
     <para>
      Commit your configuration to your local git repo:
     </para>
     <screen>&prompt.ardana;cd ~/openstack/ardana/ansible
&prompt.ardana;git add -A
&prompt.ardana;git commit -m "add SES integration"</screen>
   </step>
   <step>
    <para>
     Run the configuration processor.
    </para>
<screen>&prompt.ardana;cd ~/openstack/ardana/ansible
&prompt.ardana;ansible-playbook -i hosts/localhost config-processor-run.yml</screen>
   </step>
   <step>
    <para>
     Create a deployment directory.
    </para>
<screen>&prompt.ardana;cd ~/openstack/ardana/ansible
&prompt.ardana;ansible-playbook -i hosts/localhost ready-deployment.yml</screen>
   </step>
   <step>
    <para>
     Run a series of reconfiguration playbooks.
    </para>
<screen>&prompt.ardana;cd ~/scratch/ansible/next/ardana/ansible
&prompt.ardana;ansible-playbook -i hosts/verb_hosts ses-deploy.yml
&prompt.ardana;ansible-playbook -i hosts/verb_hosts cinder-reconfigure.yml
&prompt.ardana;ansible-playbook -i hosts/verb_hosts glance-reconfigure.yml
&prompt.ardana;ansible-playbook -i hosts/verb_hosts nova-reconfigure.yml</screen>
   </step>
  </procedure>
  <para>
   <emphasis role="bold">Configuring &ses; for Integration with &rados;
   Gateway</emphasis>
  </para>
  <para>
   &rados; gateway integration can be enabled (disabled) by adding (removing)
   the following line in the <filename>ses_config.yml</filename>:
  </para>
<screen>ses_radosgw_url: "https://192.168.56.8:8080/swift/v1"</screen>
  <para>
   If &rados; gateway integration is enabled, additional &ses; configuration is
   needed. &rados; gateway must be configured to use &o_ident; for
   authentication. This is done by adding the configuration statements below to
   the rados section of <filename>ceph.conf</filename> on the &rados; node.
  </para>
<screen>[client.rgw.<replaceable>HOSTNAME</replaceable>]
rgw frontends = "civetweb port=80+443s"
rgw enable usage log = true
rgw keystone url = <replaceable>KEYSTONE_ENDPOINT</replaceable> (for example:
https://192.168.24.204:5000)
rgw keystone admin user = <replaceable>KEYSTONE_ADMIN_USER</replaceable>
rgw keystone admin password = <replaceable>KEYSTONE_ADMIN_PASSWORD</replaceable>
rgw keystone admin project = <replaceable>KEYSTONE_ADMIN_PROJECT</replaceable>
rgw keystone admin domain = <replaceable>KEYSTONE_ADMIN_DOMAIN</replaceable>
rgw keystone api version = 3
rgw keystone accepted roles = admin,Member,_member_
rgw keystone accepted admin roles = admin
rgw keystone revocation interval = 0
rgw keystone verify ssl = false # If keystone is using self-signed
   certificate</screen>
  <para>
   After making these changes to <filename>ceph.conf</filename>, the &rados;
   gateway service needs to be restarted.
  </para>
  <para>
   Enabling &rados; gateway replaces the existing &objstore; endpoint with the
   &rados; gateway endpoint.
  </para>
  <para>
   <emphasis role="bold">Enabling HTTPS, Creating and Importing a
   Certificate</emphasis>
  </para>
  <para>
   &ses; integration uses the HTTPS protocol to connect to the &rados; gateway.
   However, with &ses; 5, HTTPS is not enabled by default. To enable the
   gateway role to communicate securely using SSL, you need to either have a
   CA-issued certificate or create a self-signed one. Instructions for both are
   available in the
   <link
    xlink:href="https://www.suse.com/documentation/suse-enterprise-storage-5/book_storage_admin/data/ceph_rgw_access.html">&ses;
   documentation</link>.
  </para>
  <para>
   The certificate needs to be installed on your &clm;. On the &clm;, copy the
   cert to <filename>/tmp/ardana_tls_cacerts</filename>. Then deploy it.
  </para>
<screen>&prompt.ardana;cd ~/scratch/ansible/next/ardana/ansible
&prompt.ardana;ansible-playbook -i hosts/verb_hosts tls-trust-deploy.yml
&prompt.ardana;ansible-playbook -i hosts/verb_hosts tls-reconfigure.yml</screen>
  <para>
   When creating the certificate, the <literal>subjectAltName</literal> must
   match the <literal>ses_radosgw_url</literal> entry in
   <filename>ses_config.yml</filename>. Either an IP address or FQDN can be
   used, but these values must be the same in both places.
  </para>
 </section>
 <section>
  <title>Deploying &ses; Configuration for &rados; Integration</title>
  <para>
   The following steps will deploy your configuration.
  </para>
  <procedure>
   <step>
    <para>
     Commit your configuration to your local git repo.
    </para>
<screen>&prompt.ardana;cd ~/openstack/ardana/ansible
&prompt.ardana;git add -A
&prompt.ardana;git commit -m "add SES integration"</screen>
   </step>
   <step>
    <para>
     Run the configuration processor.
    </para>
<screen>&prompt.ardana;cd ~/openstack/ardana/ansible
&prompt.ardana;ansible-playbook -i hosts/localhost config-processor-run.yml</screen>
   </step>
   <step>
    <para>
     Create a deployment directory.
    </para>
<screen>&prompt.ardana;cd ~/openstack/ardana/ansible
&prompt.ardana;ansible-playbook -i hosts/localhost ready-deployment.yml</screen>
   </step>
   <step>
    <para>
     Run a series of reconfiguration playbooks.
    </para>
<screen>&prompt.ardana;cd ~/scratch/ansible/next/ardana/ansible
&prompt.ardana;ansible-playbook -i hosts/verb_hosts ses-deploy.yml
&prompt.ardana;ansible-playbook -i hosts/verb_hosts cinder-reconfigure.yml
&prompt.ardana;ansible-playbook -i hosts/verb_hosts glance-reconfigure.yml
&prompt.ardana;ansible-playbook -i hosts/verb_hosts nova-reconfigure.yml</screen>
   </step>
   <step>
    <para>
     Reconfigure the &clm; to complete the deployment.
    </para>
<screen>&prompt.ardana;cd ~/scratch/ansible/next/ardana/ansible
&prompt.ardana;ansible-playbook -i hosts/verb_hosts ardana-reconfigure.yml</screen>
   </step>
  </procedure>
 </section>
 <section>
   <title>Enable Copy-On-Write Cloning of Images</title>
   <para>
    Due to a security issue described in <link
    xlink:href="http://docs.ceph.com/docs/master/rbd/rbd-openstack/?highlight=uuid#enable-copy-on-write-cloning-of-images"
    />, we do not recommend the copy-on-write cloning of images when
    &o_img; and &o_blockstore; are both using a &ceph; back-end.
    However, if you want to use this feature for faster operation,
    you can enable it as follows.</para>
    <procedure>
     <step>
      <para>
       Open the
       <literal>~/openstack/my_cloud/config/glance/glance-api.conf.j2</literal>
       file for editing and add <literal>show_image_direct_url = True</literal>
       under the <literal>[DEFAULT]</literal> section.
      </para>
     </step>
     <step>
      <para>
       Commit changes:</para>
<screen>git add -A
git commit -m "Enable Copy-on-Write Cloning"
</screen>
     </step>
     <step>
      <para>
       Run the required playbooks:
      </para>
<screen>
&prompt.ardana;ansible-playbook -i hosts/localhost config-processor-run.yml
&prompt.ardana;ansible-playbook -i hosts/localhost ready-deployment.yml
cd /var/lib/ardana/scratch/ansible/next/ardana/ansible
&prompt.ardana;ansible-playbook -i hosts/verb_hosts glance-reconfigure.yml
</screen>
     </step>
   </procedure>
   <warning>
    <para>
     Note that this exposes the back-end location via &o_img;'s API, so the
     end-point should not be publicly accessible when Copy-On-Write image
     cloning is enabled.
    </para>
   </warning>
  </section>
  <section>
   <title>Improve &ses; Storage Performance</title>
   <para>
    &ses; performance can be improved with Image-Volume cache. Be aware that
    Image-Volume cache and Copy-on-Write cloning cannot be used for the same
    storage back-end. For more information, see the <link
    xlink:href="https://docs.openstack.org/cinder/pike/admin/blockstorage-image-volume-cache.html">OpenStack
    documentation</link>.
   </para>
   <para>
    Enable Image-Volume cache with the following steps:
   </para>
   <procedure>
    <step>
     <para>
      Open the
      <filename>~/openstack/my_cloud/config/cinder/cinder.conf.j2</filename>
      file for editing.
     </para>
    </step>
    <step>
     <para>
      Add <literal>image_volume_cache_enabled = True</literal> option under the
      <literal>[ses_ceph]</literal> section.
     </para>
    </step>
    <step>
     <para>
      Commit changes:
     </para>
     <screen>&prompt.ardana;git add -A
&prompt.ardana;git commit -m "Enable Image-Volume cache"</screen>
    </step>
    <step>
     <para>
      Run the required playbooks:
     </para>
     <screen>&prompt.ardana;ansible-playbook -i hosts/localhost config-processor-run.yml
&prompt.ardana;ansible-playbook -i hosts/localhost ready-deployment.yml
&prompt.ardana;cd /var/lib/ardana/scratch/ansible/next/ardana/ansible
&prompt.ardana;ansible-playbook -i hosts/verb_hosts cinder-reconfigure.yml</screen>
    </step>
   </procedure>
  </section>
</section>
