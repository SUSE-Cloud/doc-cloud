<?xml version="1.0"?>
<!DOCTYPE section [
 <!ENTITY % entities SYSTEM "entity-decl.ent"> %entities;
]>
<section xml:id="ses.integration"
 xmlns="http://docbook.org/ns/docbook" version="5.1"
 xmlns:xi="http://www.w3.org/2001/XInclude"
 xmlns:xlink="http://www.w3.org/1999/xlink">
 <title>&ses; Integration</title>
 <para>
  The current version of &productname; supports integration with &ses;
  (SES). Integrating &ses; enables &ceph; to provide RADOS Block Device (RBD),
  block storage, image storage, object storage via RADOS Gateway (RGW),
  and CephFS (file storage) in &productname;. The following documentation
  outlines integration for &ses; 5 and 5.5.
 </para>
 <para>
   &ses; 5.5 uses a Salt runner that creates users and pools. Salt generates
   a yaml configuration that is needed to integrate with &cloud;.
   The integration runner creates separate users for &o_blockstore;,
   &o_blockstore; backup, and &o_img;. Both the &o_blockstore;
   and &o_comp; services have the same user, as &o_blockstore; needs access
   to create objects that &o_comp; uses.
 </para>
 <para>
   &ses; 5 uses a manual configuration that requires the creation of users and
   pools.
 </para>
 <para>
  For more information on &ses;, see
  the <link xlink:href="https://www.suse.com/documentation/suse-enterprise-storage-5/">SES 5 and 5.5 documentation</link>.
 </para>

  <section xml:id="ses.installation">
  <title>Enabling &ses; 5.5 Integration</title>
  <para>The following instructions detail integrating &ses; 5.5 with &cloud;.</para>
  <para>
   Log in in as root to run the SES 5.5 Salt runner on the salt admin host:
  </para>
  <para>If you specify no prefix (as the above command shows), by default
    pool names are prefixed with <literal>cloud-</literal> and are more
    generic.
  </para>
  <screen>&prompt.root;salt-run --out=yaml openstack.integrate</screen>
   <screen>
ceph_conf:
  cluster_network: 10.84.56.0/21
  fsid: d5d7c7cb-5858-3218-a36f-d028df7b0673
  mon_host: 10.84.56.8, 10.84.56.9, 10.84.56.7
  mon_initial_members: ses-osd1, ses-osd2, ses-osd3
  public_network: 10.84.56.0/21
cinder:
  key: AQBI5/xcAAAAABAAFP7ES4gl5tZ9qdLd611AmQ==
  rbd_store_pool: cloud-volumes
  rbd_store_user: cinder
cinder-backup:
  key: AQBI5/xcAAAAABAAVSZmfeuPl3KFvJetCygUmA==
  rbd_store_pool: cloud-backups
  rbd_store_user: cinder-backup
glance:
  key: AQBI5/xcAAAAABAALHgkBxARTZAeuoIWDsC0LA==
  rbd_store_pool: cloud-images
  rbd_store_user: glance
nova:
  rbd_store_pool: cloud-vms
  radosgw_urls:
     - http://10.84.56.7:80/swift/v1
     - http://10.84.56.8:80/swift/v1 </screen>

  <para>If you perform the command with a prefix, the prefix is applied to
   pool names and to key names. This way, multiple cloud deployments can use
   different users and pools on the same SES deployment.
  </para>
  <screen>&prompt.root;salt-run --out=yaml openstack.integrate prefix=mycloud</screen>
  <screen>
ceph_conf:
  cluster_network: 10.84.56.0/21
  fsid: d5d7c7cb-5858-3218-a36f-d028df7b0673
  mon_host: 10.84.56.8, 10.84.56.9, 10.84.56.7
  mon_initial_members: ses-osd1, ses-osd2, ses-osd3
  public_network: 10.84.56.0/21
cinder:
  key: AQAM5fxcAAAAABAAIyMeLwclr+5uegp33xdiIw==
  rbd_store_pool: mycloud-cloud-volumes
  rbd_store_user: mycloud-cinder
cinder-backup:
  key: AQAM5fxcAAAAABAAq6ZqKuMNaaJgk6OtFHMnsQ==
  rbd_store_pool: mycloud-cloud-backups
  rbd_store_user: mycloud-cinder-backup
glance:
  key: AQAM5fxcAAAAABAAvhJjxC81IePAtnkye+bLoQ==
  rbd_store_pool: mycloud-cloud-images
  rbd_store_user: mycloud-glance
nova:
  rbd_store_pool: mycloud-cloud-vms
radosgw_urls:
    - http://10.84.56.7:80/swift/v1
    - http://10.84.56.8:80/swift/v1 </screen>
  </section>

<section xml:id="ses.config">
  <title>Enabling &ses; 5 Integration</title>
 <para>The following instructions detail integrating &ses; 5 with &cloud;.</para>
 <para>
  The &ses; integration is provided through the <package>ardana-ses</package>
  RPM package. This package is included in the
   <systemitem>patterns-cloud-ardana</systemitem> pattern and the installation is
  covered in <xref linkend="cha.depl.dep_inst"/>. The update repositories and
  the installation covered there are required to support &ses;
  integration. The latest updates should be applied before proceeding.
 </para>
  <para>
   After the &ses; integration package has been installed, it must be
   configured. Files that contain relevant &ses; deployment information
   must be placed into a directory on the deployer node. This includes the
   configuration file that describes various aspects of the &ceph; environment
   as well as keyrings for each user and pool created in the &ceph;
   environment. In addition to that, you need to edit the
   <filename>settings.yml</filename> file to enable the &ses; integration to
   run and update all of the &productname; service configuration files.
  </para>
  <para>
   The <filename>settings.yml</filename> file must reside in the
   <filename>~/openstack/my_cloud/config/ses/</filename> directory. Open the
   file for editing, uncomment the <literal>ses_config_path:</literal>
   parameter, and specify the location on the deployer host containing the
   <filename>ses_config.yml</filename> and keyring files as the parameter's
   value. After you have done that, the <filename>site.yml</filename> and
   <filename>ardana-reconfigure.yml</filename> playbooks activate and configure
   the &o_blockstore;, &o_img;, and &o_comp;
   services.
  </para>
  <important>
   <para>
    For security reasons, you should use a unique UUID in the
    <filename>settings.yml</filename> file for
    <literal>ses_secret_id</literal>, replacing the fixed, hard-coded UUID in
    that file. You can generate a UUID that is unique to your deployment
    using the command <command>uuidgen</command>.
   </para>
  </important>
  <para>
   After you have run the <literal>openstack.integrate</literal> runner, copy
   the yaml into the <filename>ses_config.yml</filename> file on the deployer
   node. Then edit the <filename>settings.yml</filename> file to enable &ses;
   integration to run and update all of the &cloud; service configuration
   files. The <filename>settings.yml</filename> file resides in the
   <filename>~/openstack/my_cloud/config/ses</filename> directory. Open the
   <filename>settings.yml</filename> file for editing, uncomment the
   <literal>ses_config_path:</literal> parameter, and specify the location on
   the deployer host containing the <filename>ses_config.yml</filename> file.
  </para>
   <para>
    If you are integrating with &ses; and do not want to store &o_comp; images in
    &ceph;, the following setting is required:
   </para>
   <para>
    Edit <filename>ses_config.yml</filename> and change the line
    <literal>ses_nova_set_images_type: True</literal>
    to <literal>ses_nova_set_images_type: False</literal>
   </para>
   <procedure>
    <step>
     <para>
      Commit your configuration to your local git repo:
     </para>
     <screen>&prompt.ardana;cd ~/openstack/ardana/ansible
&prompt.ardana;git add -A
&prompt.ardana;git commit -m "add SES integration"</screen>
    </step>
    <step>
     <para>
      Run the configuration processor:
     </para>
     <screen>&prompt.ardana;cd ~/openstack/ardana/ansible
&prompt.ardana;ansible-playbook -i hosts/localhost config-processor-run.yml</screen>
    </step>
    <step>
     <para>
      Create a deployment directory:
     </para>
     <screen>&prompt.ardana;cd ~/openstack/ardana/ansible
&prompt.ardana;ansible-playbook -i hosts/localhost ready-deployment.yml</screen>
    </step>
    <step>
     <para>
      Run a series of reconfiguration playbooks:
     </para>
     <screen>&prompt.ardana;cd ~/scratch/ansible/next/ardana/ansible
&prompt.ardana;ansible-playbook -i hosts/verb_hosts ses-deploy.yml
&prompt.ardana;ansible-playbook -i hosts/verb_hosts cinder-reconfigure.yml
&prompt.ardana;ansible-playbook -i hosts/verb_hosts glance-reconfigure.yml
&prompt.ardana;ansible-playbook -i hosts/verb_hosts nova-reconfigure.yml</screen>
    </step>
    <step>
     <para>
      Reconfigure the &clm; to complete the deployment:
     </para>
     <screen>&prompt.ardana;cd ~/scratch/ansible/next/ardana/ansible
&prompt.ardana;ansible-playbook -i hosts/verb_hosts ardana-reconfigure.yml</screen>
    </step>
   </procedure>
  <para>
   In the <filename>control_plane.yml</filename> file, the &o_img;
   <literal>default_store</literal> option must be adjusted.
  </para>
  <screen>- glance-api:
            glance_default_store: 'rbd'</screen>
<important>
  <para>The following content is only relevant if you are running a
    standalone Ceph cluster (not &ses;) or a &ses; cluster that is
    before version 5.5.
  </para>
</important>
  <para>
   For &ceph;, it is necessary to create pools and users to allow the
   &productname; services to use the &ses;/&ceph; cluster. Pools and users must
   be created for &o_blockstore;, &o_blockstore; backup, &o_comp; and
   &o_img;. Instructions for creating and managing pools, users and keyrings is
   covered in the &ses; documentation under <link
   xlink:href="https://www.suse.com/documentation/suse-enterprise-storage-5/book_storage_admin/data/storage_cephx_keymgmt.html">Key
   Management</link>.
   </para>
   <para>
    After the required pools and users are set up on the &ceph;
    cluster, you have to create a <filename>ses_config.yml</filename>
    configuration file (see the example below). This file is used during
    deployment to configure all of the services. The
    <filename>ses_config.yml</filename> and the keyring files should be placed
    in a separate directory.
   </para>
   <para>
    If you are integrating with &ses; and do not want to store &o_comp; images in
    &ceph;, the following setting is required:
   </para>
   <para>
    Edit <filename>ses_config.yml</filename> and change the line
    <literal>ses_nova_set_images_type: True</literal>
    to <literal>ses_nova_set_images_type: False</literal>
   </para>
   <example>
    <title>ses_config.yml Example</title>
<screen>ses_cluster_configuration:
    ses_cluster_name: ceph
    ses_radosgw_url: "https://192.168.56.8:8080/swift/v1"

    conf_options:
        ses_fsid: d5d7c7cb-5858-3218-a36f-d028df7b1111
        ses_mon_initial_members: ses-osd2, ses-osd3, ses-osd1
        ses_mon_host: 192.168.56.8, 192.168.56.9, 192.168.56.7
        ses_public_network: 192.168.56.0/21
        ses_cluster_network: 192.168.56.0/21

    cinder:
        rbd_store_pool: cinder
        rbd_store_pool_user: cinder
        keyring_file_name: ceph.client.cinder.keyring

    cinder-backup:
        rbd_store_pool: backups
        rbd_store_pool_user: cinder_backup
        keyring_file_name: ceph.client.cinder-backup.keyring

    # Nova uses the cinder user to access the nova pool, cinder pool
    # So all we need here is the nova pool name.
    nova:
        rbd_store_pool: nova

    glance:
        rbd_store_pool: glance
        rbd_store_pool_user: glance
        keyring_file_name: ceph.client.glance.keyring</screen>
   </example>
   <para>
     The path to this directory must be specified in the
     <filename>settings.yml</filename> file, as in the example below. After
     making the changes, follow the steps to complete the configuration.
   </para>
   <screen>settings.yml
...
ses_config_path: /var/lib/ardana/ses/
ses_config_file: ses_config.yml

# The unique uuid for use with virsh for cinder and nova
 ses_secret_id: <replaceable>SES_SECRET_ID</replaceable></screen>
   <procedure>
    <step>
     <para>
      After modifying these files, commit your configuration to the local git
    repo. For more information, see <xref linkend="using_git"/>.
   </para>
   <screen>&prompt.ardana;cd ~/openstack/ardana/ansible
&prompt.ardana;git add -A
&prompt.ardana;git commit -m "configure SES 5"</screen>
    </step>
    <step>
     <para>
      Run the configuration processor:
     </para>
     <screen>&prompt.ardana;ansible-playbook -i hosts/localhost config-processor-run.yml</screen>
    </step>
    <step>
     <para>
      Create a deployment directory:
     </para>
     <screen>&prompt.ardana;ansible-playbook -i hosts/localhost ready-deployment.yml</screen>
    </step>
    <step>
     <para>
      Reconfigure Ardana:
     </para>
     <screen>&prompt.ardana;cd ~/scratch/ansible/next/ardana/ansible
&prompt.ardana;ansible-playbook -i hosts/verb_hosts ardana-reconfigure.yml</screen>
    </step>
   </procedure>
  </section>

  <section>
   <title>Configuring &ses; for Integration with &rados; Gateway</title>
   <para>
    &rados; gateway integration can be enabled (disabled) by adding (removing)
    the following line in the <filename>ses_config.yml</filename>:
   </para>
   <screen>ses_radosgw_url: "https://192.168.56.8:8080/swift/v1"</screen>
   <para>
    If &rados; gateway integration is enabled, additional &ses; configuration is
    needed. &rados; gateway must be configured to use &o_ident; for
    authentication. This is done by adding the configuration statements below
    to the rados section of <filename>ceph.conf</filename> on the &rados; node.
   </para>
   <screen>[client.rgw.<replaceable>HOSTNAME</replaceable>]
rgw frontends = "civetweb port=80+443s"
rgw enable usage log = true
rgw keystone url = <replaceable>KEYSTONE_ENDPOINT</replaceable> (for example:
https://192.168.24.204:5000)
rgw keystone admin user = <replaceable>KEYSTONE_ADMIN_USER</replaceable>
rgw keystone admin password = <replaceable>KEYSTONE_ADMIN_PASSWORD</replaceable>
rgw keystone admin project = <replaceable>KEYSTONE_ADMIN_PROJECT</replaceable>
rgw keystone admin domain = <replaceable>KEYSTONE_ADMIN_DOMAIN</replaceable>
rgw keystone api version = 3
rgw keystone accepted roles = admin,member
rgw keystone accepted admin roles = admin
rgw keystone revocation interval = 0
rgw keystone verify ssl = false # If keystone is using self-signed
   certificate</screen>
   <para>
    After making these changes to <filename>ceph.conf</filename>, the &rados;
    gateway service needs to be restarted.
   </para>
   <para>
    Enabling &rados; gateway replaces the existing &objstore; endpoint with the
    &rados; gateway endpoint.
   </para>
</section>

  <section>
   <title>Enabling HTTPS, Creating and Importing a Certificate</title>
   <para>
    &ses; integration uses the HTTPS protocol to connect to the &rados;
    gateway. However, with &ses; 5, HTTPS is not enabled by default. To enable the
    gateway role to communicate securely using SSL, you need to either have a
    CA-issued certificate or create a self-signed one. Instructions for both
    are available in the <link
    xlink:href="https://www.suse.com/documentation/suse-enterprise-storage-5/book_storage_admin/data/ceph_rgw_access.html">&ses;
    documentation</link>.
   </para>
   <para>
    The certificate needs to be installed on your &clm;. On the &clm;, copy the
    cert to <filename>/tmp/ardana_tls_cacerts</filename>. Then deploy it.
   </para>
   <screen>&prompt.ardana;cd ~/scratch/ansible/next/ardana/ansible
&prompt.ardana;ansible-playbook -i hosts/verb_hosts tls-trust-deploy.yml
&prompt.ardana;ansible-playbook -i hosts/verb_hosts tls-reconfigure.yml</screen>
   <para>
    When creating the certificate, the <literal>subjectAltName</literal> must
    match the <literal>ses_radosgw_url</literal> entry in
    <filename>ses_config.yml</filename>. Either an IP address or FQDN can be
    used, but these values must be the same in both places.
   </para>
</section>

<section>
 <title>Deploying &ses; Configuration for &rados; Integration</title>
   <para>
    The following steps deploy your configuration.
   </para>
   <procedure>
    <step>
     <para>
      Commit your configuration to your local git repo.
     </para>
     <screen>&prompt.ardana;cd ~/openstack/ardana/ansible
&prompt.ardana;git add -A
&prompt.ardana;git commit -m "add SES integration"</screen>
    </step>
    <step>
     <para>
      Run the configuration processor.
     </para>
     <screen>&prompt.ardana;cd ~/openstack/ardana/ansible
&prompt.ardana;ansible-playbook -i hosts/localhost config-processor-run.yml</screen>
    </step>
    <step>
     <para>
      Create a deployment directory.
     </para>
     <screen>&prompt.ardana;cd ~/openstack/ardana/ansible
&prompt.ardana;ansible-playbook -i hosts/localhost ready-deployment.yml</screen>
    </step>
    <step>
     <para>
      Run a series of reconfiguration playbooks.
     </para>
     <screen>&prompt.ardana;cd ~/scratch/ansible/next/ardana/ansible
&prompt.ardana;ansible-playbook -i hosts/verb_hosts ses-deploy.yml
&prompt.ardana;ansible-playbook -i hosts/verb_hosts cinder-reconfigure.yml
&prompt.ardana;ansible-playbook -i hosts/verb_hosts glance-reconfigure.yml
&prompt.ardana;ansible-playbook -i hosts/verb_hosts nova-reconfigure.yml</screen>
    </step>
    <step>
     <para>
      Reconfigure the &clm; to complete the deployment.
     </para>
     <screen>&prompt.ardana;cd ~/scratch/ansible/next/ardana/ansible
&prompt.ardana;ansible-playbook -i hosts/verb_hosts ardana-reconfigure.yml</screen>
    </step>
   </procedure>
 </section>
  </section>
    <section>
   <title>Enable Copy-On-Write Cloning of Images</title>
   <para>
    Due to a security issue described in <link
    xlink:href="http://docs.ceph.com/docs/master/rbd/rbd-openstack/?highlight=uuid#enable-copy-on-write-cloning-of-images"
    />, the copy-on-write cloning of images is not supported when
    &o_img; and &o_blockstore; are both using a &ceph; back-end.
    However, if you want to use this feature for faster operation,
    you can enable it as follows.</para>
    <procedure>
     <step>
      <para>
       Open the
       <literal>~/openstack/my_cloud/config/glance/glance-api.conf.j2</literal>
       file for editing and add <literal>show_image_direct_url = True</literal>
       under the <literal>[DEFAULT]</literal> section.
      </para>
     </step>
     <step>
      <para>
       Commit changes:</para>
<screen>git add -A
git commit -m "Enable Copy-on-Write Cloning"
</screen>
     </step>
     <step>
      <para>
       Run the required playbooks:
      </para>
<screen>
&prompt.ardana;ansible-playbook -i hosts/localhost config-processor-run.yml
&prompt.ardana;ansible-playbook -i hosts/localhost ready-deployment.yml
cd /var/lib/ardana/scratch/ansible/next/ardana/ansible
&prompt.ardana;ansible-playbook -i hosts/verb_hosts glance-reconfigure.yml
</screen>
     </step>
   </procedure>
   <warning>
    <para>
     Note that this exposes the back-end location via &o_img;'s API, so the
     end-point should not be publicly accessible when Copy-On-Write image
     cloning is enabled.
    </para>
   </warning>
  </section>
</section>
