<?xml version="1.0"?>
<!DOCTYPE section [
 <!ENTITY % entities SYSTEM "entities.ent"> %entities;
]>
<section xmlns="http://docbook.org/ns/docbook" xmlns:xi="http://www.w3.org/2001/XInclude" xmlns:xlink="http://www.w3.org/1999/xlink" xml:id="pit_database_recovery"><title>&kw-hos-tm; &kw-hos-version-50;: Point-in-Time MySQL Database Recovery</title><abstract><para><para>In this scenario, everything is still running
      (lifecycle manager, cloud controller nodes, and compute nodes) but you want to restore the
      MySQL database to a previous state.</para></para>
</abstract>
    <para>In this scenario, everything is still running (lifecycle manager, cloud controller nodes, and
      compute nodes) but you want to restore the MySQL database to a previous state.</para>

    <sidebar><title>Restore from a Swift backup</title><orderedlist>
        <listitem><para>Log in to the lifecycle manager.</para>
</listitem>
        <listitem><para>Determine which node is the first host member in the <literal>FND-MDB</literal> group,
          which will be the first node hosting the MySQL service in your cloud. You can do this by
          using these
            commands:</para>
<screen>cd ~/scratch/ansible/next/hos/ansible
grep -A1 FND-MDB--first-member hosts/verb_hosts</screen><para>Example
            output:</para>
<screen>$ grep -A1 FND-MDB--first-member hosts/verb_hosts
[FND-MDB--first-member:children]
helion-cp1-c1-m1</screen>
</listitem>
        <listitem><para>Log in to the first host member from the <literal>FND-MDB</literal> group.</para>
</listitem>
        <listitem><para>Become root: </para>
<screen>sudo su</screen></listitem>
        <listitem><para>Source the backup environment file
          </para>
<screen>source /home/stack/backup.osrc</screen></listitem>
        <listitem><para>List the jobs</para>
<screen>freezer-scheduler -c &lt;hostname&gt; job-list</screen></listitem>
        <listitem><para>Get the id corresponding to the job <literal>HLM Default: Mysql restore from
            Swift</literal>.</para>
</listitem>
        <listitem><para>Launch the restore.
          </para>
<screen>freezer-scheduler -c &lt;hostname&gt; job-start -j &lt;job-id&gt;</screen></listitem>
        <listitem><para>This will take some time. While you wait you can follow the progress in the
            <literal>/var/log/freezer-agent/freezer-scheduler.log</literal> log file.</para>
</listitem>
        <listitem><para>Log in to the lifecycle manager.</para>
</listitem>
        <listitem><para>Stop the Percona DB service:
          </para>
<screen>cd ~/scratch/ansible/next/hos/ansible
ansible-playbook -i hosts/verb_hosts percona-stop.yml</screen></listitem>
        <listitem><para>Log back in to the first node running the MySQL service. (Same node as step #3)</para>
</listitem>
        <listitem><para>Clean the MySQL directory using this command:
          </para>
<screen>sudo rm -r /var/lib/mysql/*</screen></listitem>
        <listitem><para>Copy the restored files back to the MySQL
          directory:</para>
<screen>sudo cp -pr /tmp/mysql_restore/* /var/lib/mysql</screen></listitem>
        <listitem><para>Log in to each of the other nodes in your MySQL cluster (the other nodes determined from
          step #2) and remove the <literal>grastate.dat</literal> file from each of
            them:</para>
<screen>sudo rm /var/lib/mysql/grastate.dat</screen></listitem>
        <listitem><para>Log back in to the lifecycle manager.</para>
</listitem>
        <listitem><para>Start the Percona DB service back up:
          </para>
<screen>cd ~/scratch/ansible/next/hos/ansible
ansible-playbook -i hosts/verb_hosts percona-bootstrap.yml</screen></listitem>
      </orderedlist></sidebar>
    <sidebar><title>Restore from an SSH backup</title><para>Follow the same procedure as the one for Swift but select the job <literal>HLM Default:
          Mysql restore from SSH</literal>.</para>
</sidebar>
    <sidebar><title>Restore MySQL manually</title><para>If restoring MySQL fails during the procedure outlined above, you can follow this procedure
        to manually restore MySQL:</para>
<orderedlist>
        <listitem><para>Log in to the lifecycle manager.</para>
</listitem>
        <listitem><para>Stop the MySQL cluster:
          </para>
<screen>cd ~/scratch/ansible/next/hos/ansible
ansible-playbook -i hosts/verb_hosts percona-stop.yml</screen></listitem>
        <listitem><para>On all of the nodes running the MySQL service, which should be all of your controller
          nodes, run the following command to purge the old
          database:</para>
<screen>sudo rm -r /var/lib/mysql/*</screen></listitem>
        <listitem><para>On the first node running the MySQL service restore the backup with the command below.
          If you have already restored to a temporary directory, copy the files
          again.</para>
<screen>sudo cp -pr /tmp/mysql_restore/* /var/lib/mysql</screen></listitem>
        <listitem><para>If you need to restore the files manually from SSH, follow these steps: </para>
<orderedlist>
            <listitem><para>Become root: </para>
<screen>sudo su</screen></listitem>
            <listitem><para>Create the <literal>/root/mysql_restore.ini</literal> file with the contents below. Be
              careful to substitute the <literal>{{ values }}</literal>. Note that the SSH information
              refers to the SSH server you configured for backup before installing.
              </para>
<screen>[default]
action = restore
storage = ssh
ssh_host = {{ freezer_ssh_host }}
ssh_username = {{ freezer_ssh_username }}
container = {{ freezer_ssh_base_dir }}/freezer_mysql_backup
ssh_key = /etc/freezer/ssh_key
backup_name = freezer_mysql_backup
restore_abs_path = /var/lib/mysql/
log_file = /var/log/freezer-agent/freezer-agent.log
hostname = {{ hostname of the first MySQL node }}</screen></listitem>
            <listitem><para>Execute the restore
              job:</para>
<screen>freezer-agent --config /root/mysql_restore.ini</screen></listitem>
          </orderedlist></listitem>
        <listitem><para>Also on the first node running the MySQL service, follow the next steps to start the
          cluster. </para>
<orderedlist>
            <listitem><para>Become root: </para>
<screen>sudo su</screen></listitem>
            <listitem><para>When the last step executed successfully, start the MySQL cluster:
              </para>
<screen>/etc/init.d/mysql bootstrap-pxc</screen></listitem>
            <listitem><para>Start the process with systemctl to make sure the process is monitored by upstard:
              </para>
<screen>systemctl start mysql</screen></listitem>
            <listitem><para>Make sure the mysql process started successfully by getting the status:
              </para>
<screen>systemctl status mysql</screen></listitem>
          </orderedlist></listitem>
        <listitem><para>Log back in to the lifecycle manager.</para>
</listitem>
        <listitem><para>From the lifecycle manager, execute the following playbook to start all MySQL
          instances:</para>
<screen>cd ~/scratch/ansible/next/hos/ansible
ansible-playbook -i hosts/verb_hosts percona-start.yml</screen></listitem>
        <listitem><para>MySQL cluster status can be checked using the <literal>percona-status.yml
          </literal>playbook:</para>
<screen>cd ~/scratch/ansible/next/hos/ansible
ansible-playbook -i hosts/verb_hosts percona-status.yml</screen></listitem>
        <listitem><para>On all of the nodes running the MySQL service, run the following commands as
          root:</para>
<screen>sudo su
touch /var/lib/mysql/galera.initialised
chown mysql:mysql /var/lib/mysql/galera.initialised</screen></listitem>
        <listitem><para>After approximately 10-15 minutes, the output of the <literal>percona-status.yml</literal>
          playbook should show all the MySQL nodes in sync. MySQL cluster status can be checked
          using this
            playbook:</para>
<screen>cd ~/scratch/ansible/next/hos/ansible
ansible-playbook -i hosts/verb_hosts percona-status.yml</screen><para>An
            example output is as
          follows:</para>
<screen>TASK: [FND-MDB | status | Report status of "{{ mysql_service }}"] ************* 
  ok: [helion-cp1-c1-m1-mgmt] =&gt; {
  "msg": "mysql is synced."
  }
  ok: [helion-cp1-c1-m2-mgmt] =&gt; {
  "msg": "mysql is synced."
  }
  ok: [helion-cp1-c1-m3-mgmt] =&gt; {
  "msg": "mysql is synced."
  }</screen></listitem>
      </orderedlist></sidebar>
  </section>
