<?xml version="1.0"?>
<?xml-stylesheet href="urn:x-suse:xslt:profiling:docbook51-profile.xsl"
 type="text/xml"
 title="Profiling step"?>
<!DOCTYPE chapter [
 <!ENTITY % entities SYSTEM "entity-decl.ent"> %entities;
]>

<chapter xmlns="http://docbook.org/ns/docbook" xmlns:xi="http://www.w3.org/2001/XInclude" xmlns:xlink="http://www.w3.org/1999/xlink" version="5.0" xml:id="troubleshooting">
 <title>Troubleshooting</title>
 <para>Before contacting support to help you with a problem on &cloud;, we
 strongly recommended that you gather as much information about your
 system and the problem as possible. For this purpose, &productname;
 ships with a tool called <command>supportconfig</command>. It gathers
 system information such as the current kernel version being used, the
 hardware, RPM database, partitions, and other items.
 <command>supportconfig</command> also collects the most important log
 files, making it easier for the supporters to identify and solve your
 problem.
</para>
 <section xml:id="shard-failures">
  <title>Shard Failures</title>
 <para>
 This problem may occur the first time the Log Management Window is opened. It occurs because the Elasticsearch index for the Monasca project does not yet exist. A browser reload should fix the problem in most cases.
 </para>
 <informalfigure>
  <mediaobject>
   <imageobject>
    <imagedata fileref="kibana_shard_failure-2.jpg" width="75%" format="JPG"/>
   </imageobject>
   <imageobject role="html">
    <imagedata fileref="kibana_shard_failure-2.jpg" width="100%" format="JPG"/>
   </imageobject>
   <textobject><phrase>Kibana shard failure</phrase>
   </textobject>
  </mediaobject>
 </informalfigure>
</section>
<section xml:id="something-went-wrong">
 <title>Oops! Looks like something went wrong</title>
<para>
This problem happens when you were previously logged into Horizon and the session expired, leaving a stale session state that confuses Kibana. In this case, re-login to Horizon and then reload Kibana to fix the problem.
 </para>
 <informalfigure>
  <mediaobject>
   <imageobject>
    <imagedata fileref="kibana_oops.png" width="75%" format="PNG"/>
   </imageobject>
   <imageobject role="html">
    <imagedata fileref="kibana_oops.png" width="100%" format="PNG"/>
   </imageobject>
   <textobject><phrase>Oops!</phrase>
   </textobject>
  </mediaobject>
 </informalfigure>
</section>
 <section xml:id="no-results">
  <title>No results found</title>
 <para>
This may be due to an overly narrow time window (that is, no logs were sent in the last 15 minutes, which is the default time window). Try increasing the time window with the drop-down box in the screen's top right corner. If the problem persists verify that log agents are running properly; make sure the <literal>openstack-monasca-log-agent</literal> service is running and check <filename>/var/log/monasca-log-agent/agent.log</filename> for errors on the agent nodes.
 </para>
  <informalfigure>
  <mediaobject>
   <imageobject>
    <imagedata fileref="kibana_noresults.png" width="75%" format="PNG"/>
   </imageobject>
   <imageobject role="html">
    <imagedata fileref="kibana_noresults.png" width="100%" format="PNG"/>
   </imageobject>
   <textobject><phrase>No results</phrase>
   </textobject>
  </mediaobject>
 </informalfigure>
</section>
<section xml:id="sec.depl.trouble.faq">
 <title>FAQ</title>

 <para>
  If your problem is not mentioned here, checking the log files on either
  the &admserv; or the &ostack; nodes may help. A list of log files
  is available at <xref linkend="cha.deploy.logs"/>.
 </para>

 <qandaset defaultlabel="qanda">
  <qandadiv xml:id="sec.depl.trouble.faq.admin">
   <title>Admin Node Deployment</title>
   <qandaentry>
    <question>
     <para>
      What to do if the initial &inst_crow; on the &admserv; fails?
     </para>
    </question>
    <answer>
     <para>
      Check the installation routine's log file at
      <filename>/var/log/crowbar/install.log</filename> for error messages.
     </para>
    </answer>
   </qandaentry>
   <qandaentry>
    <question>
     <para>
<!-- bnc #782337 -->
      What to do if the initial &inst_crow; on the &admserv; fails while
      deploying the IPMI/BMC network?
     </para>
    </question>
    <answer>
     <para>
      As of &productname; 8, it is assumed that each
      machine can be accessed directly via IPMI/BMC. However, this is not
      the case on certain blade hardware, where several nodes are accessed
      via a common adapter. Such a hardware setup causes an error on
      deploying the IPMI/BMC network. You need to disable the IPMI
      deployment running the following command:
     </para>
     <screen>/opt/dell/bin/json-edit -r -a "attributes.ipmi.bmc_enable" \
-v "false" /opt/dell/chef/data_bags/crowbar/bc-template-ipmi.json</screen>
     <para>
      Re-run the &inst_crow; after having disabled the IPMI deployment.
     </para>
    </answer>
   </qandaentry>
   <qandaentry>
    <question>
     <para>
      Why am I not able to reach the &admserv; from outside the admin
      network via the bastion network?
     </para>
    </question>
    <answer>
     <para>
      If <command>route</command> <option>-n</option> shows no gateway for
      the bastion network, check the value of the following entries in
      <filename>/etc/crowbar/network.json</filename>:
      <literal>"router_pref":</literal> and <literal>"router_pref":</literal>.
      Make sure the value for the bastion network's
      <literal>"router_pref":</literal> is set to a
      <emphasis>lower</emphasis> value than <literal>"router_pref":</literal>
      for the admin network.
     </para>
     <para>
      If the router preference is set correctly, <command>route</command>
      <option>-n</option> shows a gateway for the bastion network. In case
      the &admserv; is still not accessible via its admin network
      address (for example,
      <systemitem class="ipaddress">192.168.124.10</systemitem>), you need
      to disable route verification (<literal>rp_filter</literal>). Do so
      by running the following command on the &admserv;:
     </para>
<screen>echo 0 &gt; /proc/sys/net/ipv4/conf/all/rp_filter</screen>
     <para>
      If this setting solves the problem, make it permanent by editing
      <filename>/etc/sysctl.conf</filename> and setting the value for
      <literal>net.ipv4.conf.all.rp_filter</literal> to
      <literal>0</literal>.
     </para>
    </answer>
   </qandaentry>
   <qandaentry>
    <question>
     <para>
      Can I change the host name of the &admserv;?
     </para>
    </question>
    <answer>
     <para>
      No, after you have run the &inst_crow; you cannot change the host name.
      Services like &crow;, &chef;, and the RabbitMQ will fail after changing
      the host name.
     </para>
    </answer>
   </qandaentry>
   <qandaentry>
    <question>
     <para>
      What to do when browsing the &chef; Web UI gives a
      <literal>Tampered with cookie</literal> error?
     </para>
    </question>
    <answer>
     <para>
      You probably have an old cookie in your browser from a previous
      &chef; installation on the same IP address. Remove the cookie named
      <literal>_chef_server_session_id</literal> and try again.
     </para>
    </answer>
   </qandaentry>
   <qandaentry xml:id="q.depl.trouble.faq.admin.custom_repos">
    <question>
     <para>
      How to make custom software repositories from an external server (for
      example a remote SMT or SUSE Manager server) available for the nodes?
     </para>
    </question>
    <answer>
     <para>
      Custom repositories need to be added using the &yast; Crowbar
      module:
     </para>
     <procedure>
      <step>
       <para>
        Start the &yast; Crowbar module and switch to the
        <guimenu>Repositories</guimenu> tab: <menuchoice>
        <guimenu>&yast;</guimenu> <guimenu>Miscellaneous</guimenu>
        <guimenu>Crowbar</guimenu>
        <guimenu>Repositories</guimenu></menuchoice>.
       </para>
      </step>
      <step>
       <para>
        Choose <guimenu>Add Repositories</guimenu>
       </para>
      </step>
      <step>
       <para>
        Enter the following data:
       </para>
       <variablelist>
        <varlistentry>
         <term><guimenu>Name</guimenu>
         </term>
         <listitem>
          <para>
           A unique name to identify the repository.
          </para>
         </listitem>
        </varlistentry>
        <varlistentry>
         <term><guimenu>URL</guimenu>
         </term>
         <listitem>
          <para>
           Link or path to the repository.
          </para>
          </listitem>
        </varlistentry>
        <varlistentry>
         <term><guimenu>Ask On Error</guimenu>
         </term>
         <listitem>
          <para>
           Access errors to a repository are silently ignored by default.
           To ensure that you get notified of these errors, set the
           <literal>Ask On Error</literal> flag.
          </para>
         </listitem>
        </varlistentry>
        <varlistentry>
         <term><guimenu>Target Platform/Architecture</guimenu>
         </term>
         <listitem>
          <para>
           Currently only repositories for <guimenu>SLE 12 SP2</guimenu> on
           the <guimenu>x86_64</guimenu> architecture are supported. Make
           sure to select both options.
          </para>
         </listitem>
        </varlistentry>
       </variablelist>
      </step>
      <step>
       <para>
        Save your settings selecting <guimenu>OK</guimenu>.
       </para>
      </step>
     </procedure>
    </answer>
   </qandaentry>
  </qandadiv>
  <qandadiv xml:id="sec.depl.trouble.faq.ostack">
   <title>&ostack; Node Deployment</title>
   <qandaentry xml:id="var.depl.trouble.faq.ostack.login">
    <question>
     <para>
      How can I log in to a node as &rootuser;?
     </para>
    </question>
    <answer>
     <para>
      By default you cannot directly log in to a node as &rootuser;,
      because the nodes were set up without a &rootuser; password. You
      can only log in via SSH from the &admserv;. You should be able to
      log in to a node with <command>ssh&nbsp;root@</command> <replaceable>NAME</replaceable>
      where <replaceable>NAME</replaceable>
      is the name (alias) of the node.
     </para>
     <para>
      If name resolution does not work, go to the &crow; Web interface
      and open the <guimenu>Node Dashboard</guimenu>. Click the name of the
      node and look for its <guimenu>admin (eth0)</guimenu> <guimenu>IP
      Address</guimenu>. Log in to that IP address via SSH as user
      &rootuser;.
     </para>
    </answer>
   </qandaentry>
<!-- fs 2013-10-14: Needs clarification
   <qandaentry>
    <question>
     <para>
      How to change the default disk used for operating system installation?
     </para>
    </question>
    <answer>
     <para/>
    </answer>
   </qandaentry>
-->
   <qandaentry>
    <question>
     <para>
      What to do if a node refuses to boot or boots into a previous
      installation?
     </para>
    </question>
    <answer>
     <para>
      Make sure to change the boot order in the BIOS of the node, so that
      the first boot option is to boot from the network/boot using PXE.
     </para>
    </answer>
   </qandaentry>
   <qandaentry>
    <question>
     <para>
      What to do if a node hangs during hardware discovery after the very
      first boot using PXE into the <quote>SLEShammer</quote> image?
     </para>
    </question>
    <answer>
     <para>
      The &rootuser; login is enabled at a very early state in discovery
      mode, so chances are high that you can log in for debugging purposes as
      described in <xref linkend="var.depl.trouble.faq.ostack.login"/>.  If
      logging in as &rootuser; does not work, you need to set the &rootuser;
      password manually. This can either be done by setting the password via
      the Kernel command line as explained in <xref linkend="qa.depl.trouble.faq.ostack.rootpw"/>, or by creating a hook as
      explained below:
     </para>
     <procedure>
      <step>
       <para>
        Create a directory on the &admserv; named
        <filename>/updates/discovering-pre</filename>
       </para>
<screen>mkdir /updates/discovering-pre</screen>
      </step>
      <step>
       <para>
        Create a hook script <filename>setpw.hook</filename> in the
        directory created in the previous step:
       </para>
<screen>cat &gt; /updates/discovering-pre/setpw.hook &lt;&lt;EOF
#!/bin/sh
echo "root:linux" | chpasswd
EOF</screen>
      </step>
      <step>
       <para>
        Make the script executable:
       </para>
<screen>chmod a+x  /updates/discovering-pre/setpw.hook</screen>
      </step>
     </procedure>
     <para>
      If you are still cannot log in, you very likely hit a bug in the
      discovery image. Report it at
      <link xlink:href="http://bugzilla.suse.com/"/>.
     </para>
    </answer>
   </qandaentry>
   <qandaentry xml:id="qa.depl.trouble.faq.ostack.rootpw">
    <question>
     <para>
      How to provide Kernel Parameters for the SLEShammer Discovery Image?
     </para>
    </question>
    <answer>
     <para>
      Kernel Parameters for the SLEShammer Discovery Image can be provided
      via the Provisioner &barcl;. The following example shows how to set a
      &rootuser; password:
     </para>
     <procedure>
      <step>
       <para>
        Open a browser and point it to the &crow; Web interface available on
        the &admserv;, for example
        <literal>http://192.168.124.10/</literal>. Log in as user <systemitem class="username">crowbar</systemitem>. The password is
        <literal>crowbar</literal> by default, if you have not changed it.
       </para>
      </step>
      <step>
       <para>
        Open <menuchoice> <guimenu>Barclamps</guimenu>
        <guimenu>Crowbar</guimenu> </menuchoice> and click
        <guimenu>Edit</guimenu> in the <guimenu>Provisioner</guimenu> row.
       </para>
      </step>
      <step>
       <para>
        Click <guimenu>Raw</guimenu> in the <guimenu>Attributes</guimenu>
        section and add the Kernel parameter(s) to the <literal>"discovery":
        { "append": "" }</literal> line, for example;
       </para>
       <screen>  "discovery": {
   "append": "DISCOVERY_ROOT_PASSWORD=<replaceable>PASSWORD</replaceable>"
 },</screen>
      </step>
      <step>
       <para>
        <guimenu>Apply</guimenu> the proposal without changing the
        assignments in the <guimenu>Deployment</guimenu> section.
       </para>
      </step>
     </procedure>
    </answer>
   </qandaentry>
   <qandaentry>
    <question>
     <para>
      What to do when a deployed node fails to boot using PXE with the
      following error message: <quote>Could not find kernel image:
      ../suse-12.2/install/boot/x86_64/loader/linux</quote>?
     </para>
    </question>
    <answer>
     <para>
      The installation repository on the
      &admserv; at <filename>/srv/tftpboot/suse-12.3/install</filename>
      has not been set up correctly to contain the &cloudos;
      installation media. <!-- FIXME? Review the instructions at
      <xref linkend="sec.depl.adm_conf.repos.product"/>. -->
     </para>
    </answer>
   </qandaentry>
   <qandaentry>
    <question>
     <para>
      Why does my deployed node hang at <literal>Unpacking
      initramfs</literal> during boot when using PXE?
     </para>
    </question>
    <answer>
     <para>
      The node probably does not have enough RAM. You need at least 4 GB
      RAM for the deployment process to work.
     </para>
    </answer>
   </qandaentry>
   <qandaentry xml:id="q.depl.trouble.faq.ostack.problem">
    <question>
     <para>
      What to do if a node is reported to be in the state
      <literal>Problem</literal>? What to do if a node hangs at
      <quote>Executing AutoYast script:
          /usr/sbin/crowbar_join --setup</quote> after the installation is finished?
     </para>
    </question>
    <answer>
     <para>
      Be patient&mdash;the &ay; script may take a while to finish. If
      it really hangs, log in to the node as &rootuser; (see
      <xref linkend="var.depl.trouble.faq.ostack.login"/> for details).
      Check for error messages at the end of
      <filename>/var/log/crowbar/crowbar_join/chef.log</filename>. Fix the
      errors and restart the &ay; script by running the following command:
     </para>
     <screen>crowbar_join --start</screen>
     <para>
      If successful, the node will be listed in state
      <literal>Ready</literal>, when the script has finished.
     </para>
     <para>
      In cases where the initial --setup wasn't able to complete successfully,
      you can rerun that once after the previous issue is solved.
     </para>
     <para>
      If that does not help or if the log does not provide useful
      information, proceed as follows:
     </para>
     <procedure>
      <step>
       <para>
        Log in to the &admserv; and run the following command:
       </para>
       <screen>crowbar crowbar transition $<replaceable>NODE</replaceable></screen>
       <para>
        <replaceable>NODE</replaceable> needs to be replaced by the alias
        name you have given to the node when having installed it. Note that
        this name needs to be prefixed with <literal>$</literal>.
       </para>
      </step>
      <step>
       <para>
        Log in to the node and run <command>chef-client</command>.
       </para>
      </step>
      <step>
       <para>
        Check the output of the command for failures and error messages and
        try to fix the cause of these messages.
       </para>
      </step>
      <step>
       <para>
        Reboot the node.
       </para>
      </step>
     </procedure>

     <para>
      If the node is in a state where login in from the &admserv; is not
      possible, you need to create a &rootuser; password for it as
      described in <xref linkend="var.depl.inst.nodes.prep.root_login"/>.
      Now re-install the node by going to the node on the &crow; Web
      interface and clicking <guimenu>Reinstall</guimenu>. After having
      been re-installed, the node will hang again, but now you can log in
      and check the log files to find the cause.
     </para>
    </answer>
   </qandaentry>

   <qandaentry>
    <question>
     <para>
      Where to find more information when applying a &barcl; proposal
      fails?
     </para>
    </question>
    <answer>
     <para>
      Check the &chef; client log files on the &admserv; located at
      <filename>/var/log/crowbar/chef-client/d*.log</filename>. Further
      information is available from the &chef; client log files located
      on the node(s) affected by the proposal
      (<filename>/var/log/chef/client.log</filename>), and from the log
      files of the service that failed to be deployed. Additional
      information may be gained from the &crow; Web UI log files on the
      &admserv;. For a list of log file locations refer to
      <xref linkend="cha.deploy.logs"/>.
     </para>
    </answer>
   </qandaentry>
   <qandaentry>
    <question>
     <para>
      How to Prevent the &admserv; from Installing the &ostack; Nodes
      (Disable PXE and DNS Services)?
     </para>
    </question>
    <answer>
     <para>
      By default, the &ostack; nodes are installed by booting a discovery
      image from the &admserv; using PXE. They are allocated and then boot
      via PXE into an automatic installation (see <xref linkend="sec.depl.inst.nodes.install"/> for details). To
      install the &ostack; nodes manually or with a custom provisioning tool,
      you need to disable the PXE boot service and the DNS service on the
      &admserv;.
     </para>
     <para>
      As a consequence you also need to provide an external DNS server. Such
      a server needs to comply with the following requirements:
     </para>
     <itemizedlist>
      <listitem>
       <para>
         It needs to handle all domain-to-IP requests for &cloud;.
       </para>
      </listitem>
      <listitem>
       <para>
        It needs to handle all IP-to-domain requests for &cloud;.
       </para>
      </listitem>
      <listitem>
       <para>
        It needs to forward unknown requests to other DNS servers.
       </para>
      </listitem>
     </itemizedlist>
     <para>
      To disable the PXE and DNS services when setting up the &admserv;,
      proceed as follows:
     </para>
     <procedure>
      <title>Disabling PXE/DNS when Setting Up the &admserv;</title>
      <para>
       The following steps need to be performed <emphasis>before</emphasis>
       starting the &inst_crow;.
      </para>
      <step>
       <para>
        Create the file <filename>/etc/crowbar/dns.json</filename> with the
        following content:
       </para>
       <screen>{
 "attributes": {
   "dns": {
     "nameservers": [ "<replaceable>DNS_SERVER</replaceable>", "<replaceable>DNS_SERVER2</replaceable>" ],
     "auto_assign_server": false
   }
 }
}</screen>
       <para>
        Replace <replaceable>DNS_SERVER</replaceable> and
        <replaceable>DNS_SERVER2</replaceable> with the IP address(es) of the
        external DNS server(s). Specifying more than one server is optional.
       </para>
      </step>
      <step>
       <para>
        Create the file <filename>/etc/crowbar/provisioner.json</filename>
        with the following content:
       </para>
       <screen>{
 "attributes": {
   "provisioner": {
     "enable_pxe": false
   }
 }
}</screen>
      </step>
      <step>
       <para>
        If these files are present when the &inst_crow; is started, the
        &admserv; will be set up using external DNS services and no PXE boot
        server.
       </para>
      </step>
     </procedure>
     <para>
      If you already have deployed &cloud;, proceed as follows to
      disable the DNS and PXE services on the &admserv;:
     </para>
     <procedure>
      <title>Disabling PXE/DNS on an &admserv; Running &crow;</title>
      <step>
       <para>
        Open a browser and point it to the &crow; Web interface available on
        the &admserv;, for example
        <literal>http://192.168.124.10/</literal>. Log in as user <systemitem class="username">crowbar</systemitem>. The password is
        <literal>crowbar</literal> by default, if you have not changed it.
       </para>
      </step>
      <step>
       <para>
        Open <menuchoice> <guimenu>Barclamps</guimenu>
        <guimenu>Crowbar</guimenu> </menuchoice> and click
        <guimenu>Edit</guimenu> in the <guimenu>Provisioner</guimenu> row.
       </para>
      </step>
      <step>
       <para>
        Click <guimenu>Raw</guimenu> in the <guimenu>Attributes</guimenu>
        section and change the value for <guimenu>enable_pxe</guimenu> to
        <literal>false</literal>:
       </para>
       <screen>"enable_pxe": false,</screen>
      </step>
      <step>
       <para>
        <guimenu>Apply</guimenu> the proposal without changing the
        assignments in the <guimenu>Deployment</guimenu> section.
       </para>
      </step>
      <step>
       <para>
        Change to the <guimenu>DNS</guimenu> &barcl; via <menuchoice>
        <guimenu>Barclamps</guimenu> <guimenu>Crowbar</guimenu> </menuchoice>
        and click <guimenu>Edit</guimenu> in the <guimenu>DNS</guimenu> row.
       </para>
      </step>
      <step>
       <para>
        Click <guimenu>Raw</guimenu> in the <guimenu>Attributes</guimenu>
        section. Change the value for <guimenu>auto_assign_server</guimenu> to <literal>false</literal> and
        add the address(es) for the external name server(s):
       </para>
       <screen>"auto_assign_server": false,
"nameservers": [
 "<replaceable>DNS_SERVER</replaceable>",
 "<replaceable>DNS_SERVER2</replaceable>"
],</screen>
       <para>
        Replace <replaceable>DNS_SERVER</replaceable> and
        <replaceable>DNS_SERVER2</replaceable> with the IP address(es) of the
        external DNS server(s). Specifying more than one server is optional.
       </para>
      </step>
      <step>
       <para>
        <guimenu>Save</guimenu> your changes, but do not apply them, yet!
       </para>
      </step>
      <step>
       <para>
        In the <guimenu>Deployment</guimenu> section of the &barcl; remove
        all nodes from the <guimenu>dns-server</guimenu> role, but do not
        change the assignments for the <guimenu>dns-client</guimenu> role.
       </para>
      </step>
      <step>
       <para>
        <guimenu>Apply</guimenu> the &barcl;.
       </para>
      </step>
      <step>
       <para>
        When the DNS &barcl; has been successfully applied, log in to the
        &admserv; and stop the DNS service:
       </para>
       <screen>systemctl stop named</screen>
      </step>
     </procedure>
     <para>
      Now that the PXE and DNS services are disabled you can install
      &cloudos; on the &ostack; nodes. When a node is ready, add it to the
      pool of nodes as described in <xref linkend="sec.depl.inst.nodes.install.external"/>.
     </para>
    </answer>
   </qandaentry>
   <qandaentry>
    <question>
     <para>
      I have installed a new hard disk on a node that was already deployed.
      Why is it ignored by &crow;?
     </para>
    </question>
    <answer>
     <para>
      When adding a new hard disk to a node that has already been deployed,
      it can take up to 15 minutes before the new disk is detected.
     </para>
    </answer>
   </qandaentry>
   <qandaentry>
    <question>
     <para>
      How to install additional packages (for example a driver) when nodes
      are deployed?
     </para>
    </question>
    <answer>
     <para>
      &productname; offers the possibility to install additional packages that
      are not part of the default scope of packages installed on the &ostack;
      nodes. This is for example required if your hardware is only supported
      by a third party driver. It is also useful if your setup requires to
      install additional tools that would otherwise need to be installed
      manually.
     </para>
     <para>
      Prerequisite for using this feature is that the packages are available
      in a repository known on the &admserv;. Refer to <xref linkend="q.depl.trouble.faq.admin.custom_repos"/> for details, if the
      packages you want to install are not part of the repositories already
      configured.
     </para>
     <para>
      To add packages for installation on node deployment, proceed as
      follows:
     </para>
     <procedure>
      <step>
       <para>
        Open a browser and point it to the &crow; &wi; on the &admserv;, for
        example <literal>http://192.168.124.10/</literal>. Log in as user
        <systemitem class="username">crowbar</systemitem>. The password is
        <literal>crowbar</literal> by default, if you have not changed it
        during the installation.
       </para>
      </step>
      <step>
       <para>
        Go to <menuchoice> <guimenu>Barclamps</guimenu>
        <guimenu>Crowbar</guimenu> </menuchoice> and click the
        <guimenu>Edit</guimenu> button for <guimenu>Provisioner</guimenu>.
       </para>
      </step>
      <step>
       <para>
        Next click <guimenu>Raw</guimenu> in the
        <guimenu>Attributes</guimenu> page to open an editable view of the
        provisioner configuration.
       </para>
      </step>
      <step>
       <para>
        Add the following JSON code <emphasis>before</emphasis> the last
        closing curly bracket (replace the <replaceable>PACKAGE</replaceable>
        placeholders with real package names):
       </para>
       <screen>         "packages": {
   "suse-12.2": ["<replaceable>PACKAGE_1</replaceable>", "<replaceable>PACKAGE_2</replaceable>"],
 }
       </screen>
      </step>
     </procedure>
     <para>
      Note that these packages will get installed on all &ostack; nodes. If
      the change to the Provisioner &barcl; is made after nodes have already
      been deployed, the packages will be installed on the affected nodes
      with the next run of &chef; or <command>crowbar-register</command>. Package
      names will be validated against the package naming guidelines to
      prevent script-injection.
     </para>
    </answer>
   </qandaentry>
  </qandadiv>
  <qandadiv xml:id="sec.depl.trouble.faq.misc">
   <title>Miscellaneous</title>
   <qandaentry xml:id="sec.depl.trouble.faq.misc.keystone_pw">
    <question>
     <para>
      How to change the &o_ident; credentials after the &o_ident; &barcl; has
      been deployed?
     </para>
    </question>
    <answer>
     <para>
      To change the credentials for the &o_ident; administrator (<systemitem class="username">admin</systemitem>) or the regular user (<systemitem class="username">crowbar</systemitem> by default), proceed as follows:
     </para>
     <procedure>
      <step>
       <para>
        Log in to the &contrnode; on which &o_ident; is deployed as user
        &rootuser; via the &admserv;.
       </para>
      </step>
      <step>
       <para>
        In a shell, source the &ostack; RC file for the project that you want
        to upload an image to. For details, refer to <link xlink:href="http://docs.openstack.org/user-guide/common/cli_set_environment_variables_using_openstack_rc.html">Set
        environment variables using the &ostack; RC file</link> in the
        &ostack; documentation.
       </para>
      </step>
      <step>
       <para>
        Enter the following command to change the
        <replaceable>PASSWORD</replaceable> for the administrator or the
        regular user (<replaceable>USER</replaceable>):
       </para>
       <screen> keystone-manage bootstrap --bootstrap-password <replaceable>PASSWORD</replaceable> \
--bootstrap-username <replaceable>USER</replaceable></screen>
       <para>
        For a complete list of command line options, run
        <command>keystone-manage bootstrap --help</command>. Make sure to
        start the command with a <keycap function="space"/> to make sure the
        password does not appear in the command history.
       </para>
      </step>
      <step>
       <para>
        Access the &o_ident; &barcl; on the &crow; &wi; by going to
        <menuchoice> <guimenu>Barclamps</guimenu>
        <guimenu>OpenStack</guimenu></menuchoice> and click
        <guimenu>Edit</guimenu> for the &o_ident; &barcl;.
       </para>
      </step>
      <step>
       <para>
        Enter the new password for the same user you specified on the command
        line before.
       </para>
      </step>
      <step>
       <para>
        Activate the change by clicking <guimenu>Apply</guimenu>. When the
        proposal has been re-applied, the password has changed and can be
        used.
       </para>
      </step>
     </procedure>
    </answer>
   </qandaentry>
   <qandaentry xml:id="sec.depl.trouble.faq.misc.change_openstack_config">
    <question>
     <para>
      How to add or change a configuration value for an &ostack; service?
     </para>
    </question>
    <answer>
     <para>
      See <xref linkend="cha.depl.ostack.configs"/>.
     </para>
    </answer>
   </qandaentry>
  </qandadiv>
 </qandaset>

 <para>
  It is recommended to always run <command>supportconfig</command> on the
  &admserv; and on the &contrnode;(s). If a &compnode; or a
  &stornode; is part of the problem, run
  <command>supportconfig</command> on the affected node as well. For
  details on how to run <command>supportconfig</command>, see
  <link xlink:href="&suse-onlinedoc;/sles-12/book_sle_admin/data/cha_adm_support.html"/>.
 </para>

 <section xml:id="sec.depl.trouble.support.ptf">
  <title>
   Applying PTFs (Program Temporary Fixes) Provided by the &suse; L3 Support
  </title>
  <para>
   Under certain circumstances, the &suse; support may provide temporary
   fixes, the so-called PTFs, to customers with an L3 support contract.
   These PTFs are provided as RPM packages. To make them available on all
   nodes in &cloud;, proceed as follows.
  </para>
  <procedure>
   <step>
    <para>
     Download the packages from the location provided by the &suse; L3
     Support to a temporary location on the &admserv;.
    </para>
   </step>
   <step>
    <para>
     Move the packages from the temporary download location to the
     following directories on the &admserv;:
    </para>
    <variablelist>
     <varlistentry>
      <term>
       <quote>noarch</quote> packages (<filename>*.noarch.rpm</filename>):
      </term>
      <listitem>
       <simplelist type="vert">
        <member>
         <filename>&tftp_dir;/repos/PTF/rpm/noarch/</filename>
        </member>
        <member>
         <filename>&ztftp_dir;/repos/PTF/rpm/noarch/</filename>
        </member>
       </simplelist>
      </listitem>
     </varlistentry>
     <varlistentry>
      <term>
       <quote>x86_64</quote> packages (<filename>*.x86_64.rpm</filename>)
      </term>
      <listitem>
       <para>
        <filename>&tftp_dir;/repos/PTF/rpm/x86_64/</filename>
       </para>
      </listitem>
     </varlistentry>
     <varlistentry>
      <term>
       <quote>s390x</quote> packages (<filename>*.s390x.rpm</filename>)
      </term>
      <listitem>
       <para>
        <filename>&ztftp_dir;/repos/PTF/rpm/s390x/</filename>
       </para>
      </listitem>
     </varlistentry>
    </variablelist>
   </step>
   <step>
    <para>
     Create or update the repository metadata:
    </para>
<screen>createrepo-cloud-ptf</screen>
   </step>
   <step>
    <para>
     The repositories are now set up and are available for all nodes in
     &cloud; except for the &admserv;. In case the PTF also contains
     packages to be installed on the &admserv;, make the
     repository available on the &admserv; as well:
    </para>
<screen>zypper ar -f &tftp_dir;/repos/PTF PTF</screen>
   </step>
   <step>
    <para>
     To deploy the updates, proceed as described in
     <xref linkend="sec.depl.inst.nodes.post.updater"/>. Alternatively, run
     <command>zypper up</command> manually on each node.
    </para>
   </step>
  </procedure>
 </section>
</section>
</chapter>
