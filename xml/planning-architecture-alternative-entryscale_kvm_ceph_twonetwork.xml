<?xml version="1.0"?>
<!DOCTYPE section [
 <!ENTITY % entities SYSTEM "entity-decl.ent"> %entities;
]>
<section xmlns="http://docbook.org/ns/docbook" xmlns:xi="http://www.w3.org/2001/XInclude"
    xmlns:xlink="http://www.w3.org/1999/xlink" xml:id="entryscale_ceph_multinetwork" version="5.1">
    <title>Entry-scale KVM with Ceph Model with Two Networks</title>



    <para>&productname; Ceph is a unified storage system for various storage use cases for an
        OpenStack-based cloud. It is highly reliable, easy to manage, and horizontally scalable as
        demand grows.</para>

    <para>Ceph clients directly talk to OSD daemons for storage operations instead of client routing
        the request to a specific gateway as is commonly found in other storage solutions. OSD
        daemons perform data replication and participate in recovery activities. In general, a pool
        is configured with a replica count of three, causing daemons to transact three times the
        amount of client data over the cluster network. So, every 4 MB of write data is likely to
        result in 12 MB of data movement across Ceph clusters. Considering this network traffic, it
        is important to segregate Ceph data traffic, which can be primarily categorized into three
        segments:</para>

    <itemizedlist>
        <listitem>
            <para><emphasis role="bold">Management traffic</emphasis> - primarily includes all admin
                related operations such as pool creation, crush map modification, user creation,
                etc.</para>
        </listitem>
        <listitem>
            <para><emphasis role="bold">Client (data) traffic</emphasis> - primarily includes client
                requests sent to OSD daemons.</para>
        </listitem>
        <listitem>
            <para><emphasis role="bold">Cluster (replication) traffic</emphasis> - primarily
                includes replication and recovery data traffic among OSD daemons.</para>
        </listitem>
    </itemizedlist>
    <para>For a high performing cluster, the network configuration is important. Segregating the
        data traffic using multiple networks allows for this. For medium-size production
        environments we recommend to have a cluster with at least two networks: a client data
        network (front-side) and a cluster (back-side) network. For larger production environments
        we recommend that you segregate all three network traffic types by utilizing three networks.
        This particular document shows you how to setup two networks but you can use the same
        principles to create three networks.</para>

    <para>Also, segregating networks provides additional security as well because your cluster
        network does not need to be connected to the internet directly. This helps in preventing
        spoof attacks and allows the OSD daemons to keep communicating without intervention so that
        placement groups can be brought to active + clean state whenever required.</para>

    <para>This model is a variant of the Entry-scale KVM with Ceph model. It is designed with two
        VLANs: a public (front-side) network and a cluster (back-side) network. This enables more
        options in regards to scaling.</para>

    <para>This model uses the following components:</para>
    <itemizedlist xml:id="ul_ckz_3tl_s5">
        <listitem>
            <para>Three controller nodes, one KVM compute node, and three Ceph OSD nodes.</para>
        </listitem>
        <listitem>
            <para>The Ceph monitor component of the Ceph cluster is deployed on the controller nodes
                along with other OpenStack service components. This limits your cloud to three
                monitor nodes which should be suitable for most production environments.</para>
        </listitem>
        <listitem>
            <para>Allows two VLANs (that is management VLAN and OSD VLAN) which segregates Ceph
                client traffic from Ceph cluster traffic. The management network will be used to
                carry cloud management data, such as RabbitMQ, HOPS, and database traffic, Ceph
                management data, such as pool creation, as well as client data traffic, such as
                cinder-volume writing blocks to Ceph storage pools. The Ceph cluster network will be
                dedicated for OSD daemons and will be used to carry replication traffic.</para>
        </listitem>
        <listitem>
            <para>A single compute node is initially provided with this example configuration. If
                additional compute capacity is required then further compute nodes can be added to
                the configuration by adding more nodes to the compute resource plane. The same
                applies to OSD nodes as well. Three OSD nodes are initially provided with this
                example configuration. If additional OSD capacity is required then further OSD nodes
                can be added to the configuration by adding more nodes to the OSD resource
                plane.</para>
        </listitem>
    </itemizedlist>

    <para>The table below lists out the key characteristics needed per server role for this
        configuration.</para>

    <table xml:id="twonetwork">
        <title>Key Characteristics for Server Roles in Two-Network Configuration</title>
        <tgroup cols="4">
            <colspec colname="c1" colnum="1"/>
            <colspec colname="c2" colnum="2"/>
            <colspec colname="c3" colnum="3"/>
            <colspec colname="c4" colnum="4"/>
            <thead>
                <row>
                    <entry>Server role</entry>
                    <entry>Quantity</entry>
                    <entry>Compute Requirement</entry>
                    <entry>Network Requirement</entry>
                </row>
            </thead>
            <tbody>
                <row>
                    <entry>Controller</entry>
                    <entry>3</entry>
                    <entry>
                        <para>2x 10 core 2.66 GHz</para>
                        <para>96 - 128 GB RAM</para>
                    </entry>
                    <entry>2x 10Gb Dual Port NIC</entry>
                </row>
                <row>
                    <entry>Compute (KVM hypervisor)</entry>
                    <entry>1 (minimum)</entry>
                    <entry>
                        <para>2x 12 core 2.66 GHz (ES-2690v3) Intel Xeon</para>
                        <para>256 GB RAM</para>
                    </entry>
                    <entry>1x 10Gb Dual Port NIC</entry>
                </row>
                <row>
                    <entry>OSD</entry>
                    <entry>3 (minimum)</entry>
                    <entry>RAM is dependent upon the number of disks. 1 GB per TB of disk capacity
                        is recommended.</entry>
                    <entry>1x 10Gb Dual Port NIC</entry>
                </row>
            </tbody>
        </tgroup>
    </table>
    <para>This diagram below illustrates the physical networking used in this configuration.</para>

    <informalfigure>
        <mediaobject>
            <imageobject role="fo">
                <imagedata fileref="media-hos.docs-entry_scale_kvm_ceph_two_network.png" width="75%"
                    format="PNG"/>
            </imageobject>
            <imageobject role="html">
                <imagedata fileref="media-hos.docs-entry_scale_kvm_ceph_two_network.png"/>
            </imageobject>
        </mediaobject>
    </informalfigure>

    <!--<para>
        <link xlink:href="../../../media/hos.docs/entry_scale_kvm_ceph_two_network_lg.png">Download
            a high-resolution version</link>
    </para>-->

    <para>This configuration is based on the entry-scale-kvm-ceph cloud input model which is
        included with the &productname; distro. You will need to make the changes outlined below
        prior to the deployment of your Ceph cluster with two networks. Note that if you already
        have a Ceph cluster deployed with a single network these steps cannot be used to migrate to
        a dual-network setup. The recommendation in these cases will be that you make a clean
        installation which will result in the loss of your existing data unless you make
        arrangements to have it backed up beforehand.</para>

    <para><literal>Nic_mappings.yml</literal> Ensure that your baremetal server NIC interfaces are
        correctly specified in the
            <literal>~/openstack/my_cloud/definition/data/nic_mappings.yml</literal> file and that they
        meet the server requirements.</para>


    <para>Here is an example with notes in-line:</para>
    <screen>nic-mappings:

## NIC specification for controller nodes.  A bonded interface is
## used for the management network.
  - name: DL360p_4PORT
    physical-ports:
      - logical-name: hed1
        type: simple-port
        bus-address: "0000:07:00.0"

      - logical-name: hed2
        type: simple-port
        bus-address: "0000:08:00.0"

      - logical-name: hed3
        type: simple-port
        bus-address: "0000:09:00.0"

      - logical-name: hed4
        type: simple-port
        bus-address: "0000:0a:00.0"

## NIC specification for compute and OSD nodes should be
## of this type.
  - name: MY-2PORT-SERVER
    physical-ports:
      - logical-name: hed3
        type: simple-port
        bus-address: "0000:04:00.0"

      - logical-name: hed4
        type: simple-port
        bus-address: "0000:04:00.1"</screen>
    <para><literal>Net_interfaces.yml</literal> Define a new interface set for your OSD interfaces
        in the <literal>~/openstack/my_cloud/definition/data/net_interfaces.yml</literal> file.</para>
    <para>Ensure that the appropriate NIC is configured to both the Management and OSD network
        groups, indicated below:</para>
    <screen>- name: OSD-INTERFACES
 network-interfaces:
   - name: ETH3
     device:
        name: hed3
     network-groups:
        - MANAGEMENT
   - name: ETH4
     device:
        name: hed4
     network-groups:
        - OSD</screen>
    <para><literal>Network_groups.yml</literal> Define the OSD network group in the
            <literal>~/openstack/my_cloud/definition/data/network_groups.yml</literal> file:</para>
    <screen>#
# OSD
#
# This is the network group that will be used for
# internal traffic of cluster among OSDs.
#
- name: OSD
  hostname-suffix: osd

  component-endpoints:
    - ceph-osd-internal</screen>
    <para><literal>Networks.yml</literal> Define the OSD VLAN in the
            <literal>~/openstack/my_cloud/definition/data/networks.yml</literal> file:</para>
    <screen>
- name: OSD-NET
  vlanid: 112
  tagged-vlan: false
  cidr: 10.0.1.0/24
  gateway-ip: 10.0.1.1
  network-group: OSD</screen>

    <para><literal>Server_groups.yml</literal> Add the OSD network to the server groups in the
            <literal>~/openstack/my_cloud/definition/data/server_groups.yml</literal> file, indicated
        by the bold portion below:</para>
    <screen>- name: CLOUD
 server-groups:
   - AZ1
   - AZ2
   - AZ3
 networks:
   - EXTERNAL-API-NET
   - EXTERNAL-VM-NET
   - GUEST-NET
   - MANAGEMENT-NET
   <emphasis role="bold">- OSD-NET</emphasis></screen>
    <para><literal>Firewall_rules.yml</literal> Modify the firewall rules in the
            <literal>~/openstack/my_cloud/definition/data/firewall_rules.yml</literal> file to allow
        OSD nodes to be pingable via the OSD network, indicated by the bold portion below:</para>
    <screen>
- name: PING
  network-groups:
  - MANAGEMENT
  - GUEST
  - EXTERNAL-API
  <emphasis role="bold">- OSD</emphasis>
  rules:
  # open ICMP echo request (ping)
  - type: allow
    remote-ip-prefix:  0.0.0.0/0
    # icmp type
    port-range-min: 8
    # icmp code
    port-range-max: 0
    protocol: icmp</screen>
    <para>Edit the <filename>README.html</filename> and <filename>README.md</filename> files. You
        can edit the <literal>~/openstack/my_cloud/definition/README.html</literal> and
            <literal>~/openstack/my_cloud/definition/README.md</literal> files to reflect the OSD
        network group information if you wish. This change does not have any semantic implication
        and only assists with the readability of your model.</para>


</section>
