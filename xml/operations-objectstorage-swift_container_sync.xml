<?xml version="1.0"?>
<?xml-stylesheet href="urn:x-suse:xslt:profiling:docbook51-profile.xsl"
 type="text/xml"
 title="Profiling step"?>
<!DOCTYPE section [
 <!ENTITY % entities SYSTEM "entity-decl.ent"> %entities;
]>
<section xmlns="http://docbook.org/ns/docbook" xmlns:xi="http://www.w3.org/2001/XInclude" xmlns:xlink="http://www.w3.org/1999/xlink" xml:id="topic-el2-cqv-mv">
 <title>Configuring your &o_objstore; System to Allow Container Sync</title>
 <para>
  &o_objstore; has a feature where all the contents of a container can be mirrored to
  another container through background synchronization. &o_objstore; operators
  configure their system to allow/accept sync requests to/from other systems,
  and the user specifies where to sync their container to along with a secret
  synchronization key. For an overview of this feature, refer to
  <link xlink:href="http://docs.openstack.org/developer/swift/overview_container_sync.html">OpenStack
  &o_objstore; - Container to Container Synchronization</link>.
 </para>
 <section xml:id="idg-all-operations-objectstorage-swift-container-sync-xml-5">
  <title>Notes and limitations</title>
  <para>
   The container synchronization is done as a background action. When you put
   an object into the source container, it will take some time before it
   becomes visible in the destination container. Storage services will not
   necessarily copy objects in any particular order, meaning they may be
   transferred in a different order to which they were created.
  </para>
  <para>
   Container sync may not be able to keep up with a moderate upload rate to a
   container. For example, if the average object upload rate to a container is
   greater than one object per second, then container sync may not be able to
   keep the objects synced.
  </para>
  <para>
   If container sync is enabled on a container that already has a large number
   of objects then container sync may take a long time to sync the data. For
   example, a container with one million 1KB objects could take more than 11
   days to complete a sync.
  </para>
  <para>
   You may operate on the destination container just like any other container
   -- adding or deleting objects -- including the objects that are in the
   destination container because they were copied from the source container. To
   decide how to handle object creation, replacement or deletion, the system
   uses timestamps to determine what to do. In general, the latest timestamp
   "wins". That is, if you create an object, replace it, delete it and the
   re-create it, the destination container will eventually contain the most
   recently created object. However, if you also create and delete objects in
   the destination container, you get some subtle behaviours as follows:
  </para>
  <itemizedlist>
   <listitem>
    <para>
     If an object is copied to the destination container and then deleted, it
     remains deleted in the destination even though there is still a copy in
     the source container. If you modify the object (replace or change its
     metadata) in the source container, it will reappear in the destination
     again.
    </para>
   </listitem>
   <listitem>
    <para>
     The same applies to a replacement or metadata modification of an object in
     the destination container -- the object will remain as-is unless there is
     a replacement or modification in the source container.
    </para>
   </listitem>
   <listitem>
    <para>
     If you replace or modify metadata of an object in the destination
     container and then delete it in the source container, it is
     <emphasis role="bold">not</emphasis> deleted from the destination. This is
     because your modified object has a later timestamp than the object you
     deleted in the source.
    </para>
   </listitem>
   <listitem>
    <para>
     If you create an object in the source container and before the system has
     a chance to copy it to the destination, you also create an object of the
     same name in the destination, then the object in the destination is
     <emphasis role="bold">not</emphasis> overwritten by the source container's
     object.
    </para>
   </listitem>
  </itemizedlist>
  <para>
   <emphasis role="bold">Segmented objects</emphasis>
  </para>
  <para>
   Segmented objects (objects larger than 5GB) will not work seamlessly with
   container synchronization. If the manifest object is copied to the
   destination container before the object segments, when you perform a GET
   operation on the manifest object, the system may fail to find some or all of
   the object segments. If your manifest and object segments are in different
   containers, do not forget that both containers must be synchonized and that
   the container name of the object segments must be the same on both source
   and destination.
  </para>
 </section>
 <section xml:id="idg-all-operations-objectstorage-swift-container-sync-xml-6">
  <title>Prerequisites</title>
  <para>
   Container to container synchronization requires that SSL certificates are
   configured on both the source and destination systems. For more information
   on how to implement SSL, see <xref linkend="tls30"/>.
  </para>
 </section>
 <section xml:id="configure">
  <title>Configuring container sync</title>
<!-- FIXME: Two mentions of HOS 3.0 in this section. Does the procedure
  need bigger updates? - sknorr, 2018-03-27 -->
  <para>
   Container to container synchronization requires that both the source and
   destination &o_objstore; systems involved be configured to allow/accept this.  In
   the context of container to container synchronization, &o_objstore; uses the term
   <emphasis>cluster</emphasis> to denote a &o_objstore; system. &o_objstore;
   <emphasis>clusters</emphasis> correspond to <emphasis>Control
   Planes</emphasis> in &ostack; terminology.
   </para>
  <para>
   <emphasis role="bold">Gather the public API endpoints for both &o_objstore;
   systems</emphasis>
  </para>
  <para>
   Gather information about the external/public URL used by each system, as
   follows:
  </para>
  <orderedlist>
   <listitem>
    <para>
     On the &clm; of one system, get the public API endpoint of the
     system by running the following commands:
    </para>
<screen>&prompt.ardana;source ~/service.osrc
&prompt.ardana;openstack endpoint list | grep swift</screen>
    <para>
     The output of the command will look similar to this:
    </para>
<screen>&prompt.ardana;openstack endpoint list | grep swift
| 063a84b205c44887bc606c3ba84fa608 | region0 | swift           | object-store    | True    | admin     | https://10.13.111.176:8080/v1/AUTH_%(tenant_id)s |
| 3c46a9b2a5f94163bb5703a1a0d4d37b | region0 | swift           | object-store    | True    | public    | <emphasis role="bold">https://10.13.120.105:8080/v1</emphasis>/AUTH_%(tenant_id)s |
| a7b2f4ab5ad14330a7748c950962b188 | region0 | swift           | object-store    | True    | internal  | https://10.13.111.176:8080/v1/AUTH_%(tenant_id)s |</screen>
    <para>
     The portion that you want is the endpoint up to, but not including, the
     <literal>AUTH</literal> part. It is bolded in the above example,
     <literal>https://10.13.120.105:8080/v1</literal>.
    </para>
   </listitem>
   <listitem>
    <para>
     Repeat these steps on the other &o_objstore; system so you have both of the
     public API endpoints for them.
    </para>
   </listitem>
  </orderedlist>
  <para>
   <emphasis role="bold">Validate connectivity between both systems</emphasis>
  </para>
  <para>
   The &o_objstore; nodes running the <literal>swift-container</literal> service must
   be able to connect to the public API endpoints of each other for the
   container sync to work. You can validate connectivity on each system using
   these steps.
  </para>
  <para>
   For the sake of the examples, we will use the terms
   <emphasis>source</emphasis> and <emphasis>destination</emphasis> to notate
   the nodes doing the synchronization.
  </para>
  <orderedlist>
   <listitem>
    <para>
     Log in to a &o_objstore; node running the <literal>swift-container</literal>
     service on the source system. You can determine this by looking at the
     service list in your
     <literal>~/openstack/my_cloud/info/service_info.yml</literal> file for a list
     of the servers containing this service.
    </para>
   </listitem>
   <listitem>
    <para>
     Verify the SSL certificates by running this command against the
     destination &o_objstore; server:
    </para>
<screen>echo | openssl s_client -connect <replaceable>PUBLIC_API_ENDPOINT</replaceable>:8080 -CAfile /etc/ssl/certs/ca-certificates.crt</screen>
    <para>
     If the connection was successful you should see a return code of
     <literal>0 (ok)</literal> similar to this:
    </para>
<screen>...
Timeout   : 300 (sec)
Verify return code: 0 (ok)</screen>
   </listitem>
   <listitem>
    <para>
     Also verify that the source node can connect to the destination &o_objstore;
     system using this command:
    </para>
<screen>&prompt.ardana;curl -k <replaceable>DESTINATION_IP OR HOSTNAME</replaceable>:8080/healthcheck</screen>
    <para>
     If the connection was successful, you should see a response of
     <literal>OK</literal>.
    </para>
   </listitem>
   <listitem>
    <para>
     Repeat these verification steps on any system involved in your container
     synchronization setup.
    </para>
   </listitem>
  </orderedlist>
  <para>
   <emphasis role="bold">Configure container to container
   synchronization</emphasis>
  </para>
  <para>
   Both the source and destination &o_objstore; systems must be configured the same
   way, using sync realms. For more details on how sync realms work, see
   <link xlink:href="http://docs.openstack.org/developer/swift/overview_container_sync.html#configuring-container-sync">OpenStack
   &o_objstore; - Configuring Container Sync</link>.
  </para>
  <para>
   To configure one of the systems, follow these steps:
  </para>
  <orderedlist>
   <listitem>
    <para>
     Log in to the &clm;.
    </para>
   </listitem>
   <listitem>
    <para>
     Edit the
     <literal>~/openstack/my_cloud/config/swift/container-sync-realms.conf.j2</literal>
     file and uncomment the sync realm section.
    </para>
    <para>
     Here is a sample showing this section in the file:
    </para>
<screen>#Add sync realms here, for example:
# [realm1]
# key = realm1key
# key2 = realm1key2
# cluster_name1 = https://host1/v1/
# cluster_name2 = https://host2/v1/</screen>
   </listitem>
   <listitem>
    <para>
     Add in the details for your source and destination systems. Each realm you
     define is a set of clusters that have agreed to allow container syncing
     between them. These values are case sensitive.
    </para>
    <para>
     Only one <literal>key</literal> is required. The second key is optional
     and can be provided to allow an operator to rotate keys if desired. The
     values for the clusters must contain the prefix
     <literal>cluster_</literal> and will be populated with the public API
     endpoints for the systems.
    </para>
   </listitem>
   <listitem>
    <para>
     Commit the changes to git:
    </para>
<screen>&prompt.ardana;git add -A
&prompt.ardana;git commit -a -m "Add node &lt;name&gt;"</screen>
   </listitem>
   <listitem>
    <para>
     Run the configuration processor:
    </para>
<screen>&prompt.ardana;cd ~/openstack/ardana/ansible
&prompt.ardana;ansible-playbook -i hosts/localhost config-processor-run.yml</screen>
   </listitem>
   <listitem>
    <para>
     Update the deployment directory:
    </para>
<screen>&prompt.ardana;cd ~/openstack/ardana/ansible
&prompt.ardana;ansible-playbook -i hosts/localhost ready-deployment.yml</screen>
   </listitem>
   <listitem>
    <para>
     Run the &o_objstore; reconfigure playbook:
    </para>
<screen>&prompt.ardana;cd ~/scratch/ansible/next/ardana/ansible/
&prompt.ardana;ansible-playbook -i hosts/verb_hosts swift-reconfigure.yml</screen>
   </listitem>
   <listitem>
    <para>
     Run this command to validate that your container synchronization is
     configured:
    </para>
<screen>&prompt.ardana;source ~/service.osrc
&prompt.ardana;swift capabilities</screen>
    <para>
     Here is a snippet of the output showing the container sync information.
     This should be populated with your cluster names:
    </para>
<screen>...
Additional middleware: container_sync
 Options:
  realms: {u'INTRACLUSTER': {u'clusters': {u'THISCLUSTER': {}}}}</screen>
   </listitem>
   <listitem>
    <para>
     Repeat these steps on any other &o_objstore; systems that will be involved in
     your sync realms.
    </para>
   </listitem>
  </orderedlist>
 </section>
 <section xml:id="intra-cluster-sync">
  <title>Configuring Intra Cluster Container Sync</title>
  <para>
   It is possible to use the &o_objstore; container sync functionality to sync objects
   between containers within the same &o_objstore; system. &o_objstore; is automatically
   configured to allow intra cluster container sync. Each &o_objstore; PAC server will
   have an intracluster container sync realm defined in
   <literal>/etc/swift/container-sync-realms.conf</literal>.
  </para>
  <para>
   For example:
  </para>
<screen># The intracluster realm facilitates syncing containers on this system
[intracluster]
key = lQ8JjuZfO
# key2 =
cluster_thiscluster = http://<replaceable>SWIFT-PROXY-VIP</replaceable>:8080/v1/</screen>
  <para>
   The keys defined in <literal>/etc/swift/container-sync-realms.conf</literal>
   are used by the container-sync daemon to determine trust. On top of this
   the containers that will be in sync will need a seperate shared key they
   both define in container metadata to establish their trust between each other.
  </para>
  <orderedlist>
   <listitem>
    <para>
     Create two containers, for example container-src and container-dst. In
     this example we will sync one way from container-src to container-dst.
    </para>
<screen>&prompt.ardana;openstack container create container-src
&prompt.ardana;openstack container create container-dst</screen>
   </listitem>
   <listitem>
    <para>
     Determine your &o_objstore; account. In the following example it is AUTH_1234
    </para>
<screen>&prompt.ardana;openstack container show
                                 Account: AUTH_1234
                              Containers: 3
                                 Objects: 42
                                   Bytes: 21692421
Containers in policy "erasure-code-ring": 3
   Objects in policy "erasure-code-ring": 42
     Bytes in policy "erasure-code-ring": 21692421
                            Content-Type: text/plain; charset=utf-8
             X-Account-Project-Domain-Id: default
                             X-Timestamp: 1472651418.17025
                              X-Trans-Id: tx81122c56032548aeae8cd-0057cee40c
                           Accept-Ranges: bytes</screen>
   </listitem>
   <listitem>
    <para>
     Configure container-src to sync to container-dst using a key specified
     by both containers. Replace <replaceable>KEY</replaceable> with your key.
    </para>
<screen>&prompt.ardana;openstack container set -t '//intracluster/thiscluster/AUTH_1234/container-dst' -k '<replaceable>KEY</replaceable>' container-src</screen>
   </listitem>
   <listitem>
    <para>
     Configure container-dst to accept synced objects with this key
    </para>
<screen>&prompt.ardana;openstack container set -k '<replaceable>KEY</replaceable>' container-dst</screen>
   </listitem>
   <listitem>
    <para>
     Upload objects to container-src. Within a number of minutes the objects
     should be automatically synced to container-dst.
    </para>
   </listitem>
  </orderedlist>
  <para>
   <emphasis role="bold">Changing the intracluster realm key</emphasis>
  </para>
  <para>
   The intracluster realm key used by container sync to sync objects between
   containers in the same &o_objstore; system is automatically generated. The process
   for changing passwords is described in
   <xref linkend="servicePasswords"/>.
  </para>
  <para>
   The steps to change the intracluster realm key are as follows.
  </para>
  <orderedlist>
   <listitem>
    <para>
     On the &clm; create a file called
     <literal>~/openstack/change_credentials/swift_data_metadata.yml</literal>
     with the contents included below. The <literal>consuming-cp</literal> and
     <literal>cp</literal> are the control plane name specified in
     <literal>~/openstack/my_cloud/definition/data/control_plane.yml</literal>
     where the swift-container service is running.
    </para>
<screen>swift_intracluster_sync_key:
 metadata:
 - clusters:
   - swpac
   component: swift-container
   consuming-cp: control-plane-1
   cp: control-plane-1
 version: '2.0'</screen>
   </listitem>
   <listitem>
    <para>
     Run the following commands
    </para>
<screen>&prompt.ardana;cd ~/openstack/ardana/ansible
&prompt.ardana;ansible-playbook -i hosts/localhost config-processor-run.yml
&prompt.ardana;ansible-playbook -i hosts/localhost ready-deployment.yml</screen>
   </listitem>
   <listitem>
    <para>
     Reconfigure the &o_objstore; credentials
    </para>
<screen>&prompt.ardana;cd ~/scratch/ansible/next/ardana/ansible/
&prompt.ardana;ansible-playbook -i hosts/verb_hosts swift-reconfigure-credentials-change.yml</screen>
   </listitem>
   <listitem>
    <para>
     Delete
     <literal>~/openstack/change_credentials/swift_data_metadata.yml</literal>
    </para>
<screen>&prompt.ardana;rm ~/openstack/change_credentials/swift_data_metadata.yml</screen>
   </listitem>
   <listitem>
    <para>
     On a &o_objstore; PAC server check that the intracluster realm key has been
     updated in <literal>/etc/swift/container-sync-realms.conf</literal>
    </para>
<screen># The intracluster realm facilitates syncing containers on this system
[intracluster]
key = aNlDn3kWK</screen>
   </listitem>
   <listitem>
    <para>
     Update any containers using the intracluster container sync to use the new
     intracluster realm key
    </para>
<screen>&prompt.ardana;openstack container set -k 'aNlDn3kWK' container-src
&prompt.ardana;openstack container set -k 'aNlDn3kWK' container-dst</screen>
   </listitem>
  </orderedlist>
 </section>
</section>
