<?xml version="1.0"?>
<?xml-stylesheet href="urn:x-suse:xslt:profiling:docbook51-profile.xsl"
 type="text/xml"
 title="Profiling step"?>
<!DOCTYPE chapter [
 <!ENTITY % entities SYSTEM "entity-decl.ent"> %entities;
]>
<section xmlns="http://docbook.org/ns/docbook" xmlns:xi="http://www.w3.org/2001/XInclude" xmlns:xlink="http://www.w3.org/1999/xlink" xml:id="upgradeto50">
 <title>Upgrade to &product; 5.0</title>
 <section xml:id="idg-all-upgrade-upgradeto50-xml-2">
  <title>Performing the Upgrade from &product; 4.0 to &product; 5.0</title>
  <warning>
   <para>
    The process for upgrading from &product; 4.0 to &product; 5.0
    requires a reboot. It also requires that you <emphasis role="bold">do not
    make any configuration changes before the upgrade</emphasis>. Config
    changes must only be made after the upgrade process is complete.
   </para>
  </warning>
  <para>
   When upgrading from &product; 4.0 to &product; 5.0 you will use
   the same configuration files in your
   <literal>~/openstack/my_cloud/definition</literal> directory as you used when
   originally installing the software.
  </para>
  <para>
   The configuration you specified will remain unchanged through the upgrade
   process; <emphasis role="bold">however</emphasis>, if you made any
   configuration changes by passing parameters to any Ansible playbooks using
   the <literal>-e</literal> option, you should find the affected files and
   make the required changes directly in the files in
   <literal>~/openstack/my_cloud</literal> and commit them to the git repository
   so that those changes are tracked and applied during the upgrade process.
   Otherwise they will not be applied after the upgrade.
  </para>
  <para>
   After the upgrade process has completed, the Cinder Volume and Backup
   services will by default run on the first node in the Cinder Volume host
   group. To migrate these services to a different node follow the instructions
   in <xref linkend="sec.operation.manage-block-storage"/>.
  </para>
  <para>
   You may also install &product; 5.0 as a full install. Those
   instructions are found in the installation guide. See
   <xref linkend="install_overview"/> for details.
   If you choose to do a fresh install it is important that you completely
   wipe your disks prior to doing the installation, otherwise errors may
   occur.
  </para>
 </section>
 <section xml:id="idg-all-upgrade-upgradeto50-xml-3">
  <title>Prerequisites</title>
  <para>
   These steps assume you have an installed and working &product; 4.0
   cloud.
  </para>
 </section>
 <section xml:id="mainSteps">
  <title>Upgrade Overview</title>
  <para>
   To perform the upgrade, the workflow will look something like this:
  </para>
  <orderedlist>
   <listitem>
    <para>
     Run the upgrade to apply updates to all nodes in the cloud (including the
     node hosting the &lcm; itself), as per instructions below.
    </para>
   </listitem>
   <listitem>
    <para>
     Reboot all of the nodes in your cloud in order to take in the new kernel
     changes introduced in &product; 5.0. Instructions on rebooting your
     cloud nodes are provided below.
    </para>
   </listitem>
  </orderedlist>
 </section>
 <section xml:id="upgradesteps">
  <title>Upgrade Instructions</title>
  <orderedlist>
   <listitem>
    <para>
     Get status of all your services, if desired
    </para>
<screen>cd ~/scratch/ansible/next/ardana/ansible
ansible-playbook -i hosts/verb_hosts hlm-status.yml</screen>
    <para>
     If status comes back bad for any service, run the service reconfigure
     playbook, with naming convention
     <literal>&lt;service&gt;-reconfigure.yml</literal>, for that service, then
     run the status playbook for that service and see if the issue is resolved.
    </para>
    <para>
     The status playbooks have this naming convention:
    </para>
<screen>&lt;service&gt;-status.yml</screen>
    <para>
     A full list is included at the end of this document.
    </para>
   </listitem>
   <listitem>
    <para>
     Sign in and download the &product; 5.0 product and signature files at
     the links below:
    </para>
    <orderedlist>
     <listitem>
      <para>
       <link xlink:href="http://www.hpe.com/software/entitlements">Software
       Entitlement Portal</link>
      </para>
     </listitem>
    </orderedlist>
   </listitem>
   <listitem>
    <para>
     You can verify the download was complete via the signature verification
     process outlined
     <link xlink:href="https://h20392.www2.hpe.com/portal/swdepot/displayProductInfo.do?productNumber=HPLinuxCodeSigning">here</link>.
    </para>
   </listitem>
   <listitem>
    <para>
     To begin the upgrade process, log in to the &lcm; as the user
     you created during the &product; 4.0 deployment, and mount the
     install media at <literal>/media/cdrom</literal>; for example:
    </para>
<screen>sudo mount HelionOpenStack-5.0.iso /media/cdrom</screen>
   </listitem>
   <!-- NEED THE NAME OF THE TAR FILE USED TO INSTALL HOS 5.0 -->
   <listitem>
    <para>
     Unpack the following tarball:
    </para>
<screen>cd ~
tar faxv /media/cdrom/hos/hos-5.0.0-20161014T020249Z.tar</screen>
   </listitem>
   <listitem>
    <para>
     Run the included initialization script to update the deployer:
    </para>
<screen>~/hos-5.0.0/hos-init.bash</screen>
    <important>
     <para>
      If you encounter a code merge error, please see
      <xref linkend="upgrade_git_merge"/>. After git merge
      conflicts have been resolved, please repeat step #6 to re-run
      hos-init.bash
     </para>
    </important>
   </listitem>

    <!-- Commented in DITA original -->
    <!--
    <li>If you have made any changes to your configuration files, ensure that you commit them to
      your local git and if any merge conflicts are reported, you must resolve them before
      proceeding (See the article on how to <xref linkend="upgrade_git_merge"/> (Resolve any Merge
        Conflicts)):
      <codeblock>cd ~/helion
git add –A
git commit –m "My changes"</codeblock>
    </li>-->

    <!--<li>Note, if you want to install the Rados Gateway service, you must do so AFTER the upgrade
      completes. At that point you can follow the instructions in <xref
        href="../operations/maintenance/ceph/add_ceph_rados_node.dita#add_monitor_node"/></li>-->
   <listitem xml:id="idg-all-upgrade-upgradeto50-xml-6">
    <para>
     Run the configuration processor:
    </para>
<screen>cd ~/openstack/ardana/ansible
ansible-playbook -i hosts/localhost config-processor-run.yml</screen>
   </listitem>
   <listitem>
    <para>
     Update your deployment directory:
    </para>
<screen>cd ~/openstack/ardana/ansible
ansible-playbook -i hosts/localhost ready-deployment.yml</screen>
   </listitem>
   <listitem>
    <para>
     Run the upgrade playbook (required for all models/deployments):
    </para>
<screen>cd ~/scratch/ansible/next/ardana/ansible
ansible-playbook -i hosts/verb_hosts hlm-upgrade.yml</screen>
   </listitem>
    <!-- Commented in DITA original. -->
    <!--<li><b>ESX - additional step:</b> Once the upgrade is complete, skip to the <xref
        href="upgrade21to30.dita#upgradeto30/esx">Further ESX Model Upgrade Instructions</xref>
      section.</li>-->
  </orderedlist>
 </section>
 <section xml:id="rebooting_general">
  <title>Rebooting Your Nodes</title>
  <para>
   To complete the upgrade and for &slsa; updates to take effect,
   you must reboot all your nodes. The instructions for rebooting controller,
   compute, and storage nodes are found below. Note that you should follow the
   order prescribed for each type of deployment, for example, Swift, VSA, or
   Ceph.
  </para>
 </section>
 <section xml:id="idg-all-upgrade-upgradeto50-xml-9">
  <title>Recommended Node Reboot Order</title>
  <para>
   To ensure that rebooted nodes reintegrate into the cluster, the key is
   having enough time between controller reboots.
  </para>
  <para>
   The recommended way to achieve this is as follows:
  </para>
  <orderedlist>
   <listitem>
    <para>
     Reboot controller nodes one-by-one with a suitable interval in between. If
     you alternate between controllers and compute nodes you will gain more
     time between the controller reboots.
    </para>
   </listitem>
   <listitem>
    <para>
     Reboot of compute nodes (if present in your cloud).
    </para>
   </listitem>
   <listitem>
    <para>
     Reboot of Swift nodes (if present in your cloud).
    </para>
   </listitem>
   <listitem>
    <para>
     Reboot of VSA nodes (if present in your cloud)
    </para>
   </listitem>
   <listitem>
    <para>
     Reboot of Ceph nodes (if present in your cloud).
    </para>
   </listitem>
   <listitem>
    <para>
     Reboot of ESX nodes (if present in your cloud).
    </para>
   </listitem>
  </orderedlist>
 </section>
 <section xml:id="idg-all-upgrade-upgradeto50-xml-10">
  <title>Rebooting Controller Nodes</title>
  <para>
   <emphasis role="bold">Migrate Singleton Services First</emphasis>
  </para>
  <para>
   The first consideration before rebooting any controller nodes is that there
   are a few services that run as singletons (non-HA), thus they will be
   unavailable while the controller they run on is down. Typically this is a
   very small window, but if you want to retain the service during the reboot
   of that server you should take special action to maintain service, such as
   migrating the service.
  </para>
  <para>
   For these steps, if your singleton services are running on controller1 and
   you move them to controller2, then ensure you move them back to controller1
   before proceeding to reboot controller2.
  </para>
  <para>
   <emphasis role="bold">For the <literal>cinder-volume</literal> singleton
   service:</emphasis>
  </para>
  <para>
   Execute the following command on each controller node to determine which
   node is hosting the cinder-volume singleton. It should be running on only
   one node:
  </para>
<screen>ps auxww | grep cinder-volume | grep -v grep</screen>
  <para>
   Run the <literal>cinder-migrate-volume.yml</literal> playbook - details
   about the Cinder volume and backup migration instructions can be found in
   <xref linkend="sec.operation.manage-block-storage"/>.
  </para>
  <para>
   <emphasis role="bold">For the <literal>nova-consoleauth</literal> singleton
   service:</emphasis>
  </para>
  <para>
   The <literal>nova-consoleauth</literal> component runs by default on the
   first controller node, that is, the host with
   <literal>consoleauth_host_index=0</literal>. To move it to another
   controller node before rebooting controller 0, run the ansible playbook
   <literal>nova-start.yml</literal> and pass it the index of the next
   controller node. For example, to move it to controller 2 (index of 1), run:
  </para>
<screen>cd ~/scratch/ansible/next/ardana/ansible
ansible-playbook -i hosts/verb_hosts nova-start.yml --extra-vars "consoleauth_host_index=1"</screen>
  <para>
   <emphasis role="bold">Reboot the Controllers</emphasis>
  </para>
  <para>
   In order to reboot the controller nodes, you must first retrieve a list of
   nodes in your cloud running control plane services.
  </para>
<screen>for i in $(grep -w cluster-prefix ~/openstack/my_cloud/definition/data/control_plane.yml | awk '{print $2}'); do grep $i ~/scratch/ansible/next/ardana/ansible/hosts/verb_hosts | grep ansible_ssh_host | awk '{print $1}'; done</screen>
  <para>
   Then perform the following steps from your &lcm; for each of
   your controller nodes:
  </para>
  <orderedlist>
   <listitem>
    <para>
     If any singleton services are active on this node, they will be
     unavailable while the node is down. If you want to retain the service
     during the reboot, you should take special action to maintain the service,
     such as migrating the service as appropriate as noted above.
    </para>
   </listitem>
   <listitem>
    <para>
     Stop all services on the controller node that you are rebooting first:
    </para>
<screen>cd ~/scratch/ansible/next/ardana/ansible
ansible-playbook -i hosts/verb_hosts hlm-stop.yml --limit <replaceable>CONTROLLER_NODE</replaceable></screen>
   </listitem>
   <listitem>
    <para>
     Reboot the controller node, for example, run the following command on the
     controller itself:
    </para>
<screen>sudo reboot</screen>
    <para>
     Note that the current node being rebooted could be hosting the &lcm;.
    </para>
   </listitem>
   <listitem>
    <para>
     Wait for the controller node to become ssh-able and allow an additional
     minimum of five minutes for the controller node to settle. Start all
     services on the controller node:
    </para>
<screen>cd ~/scratch/ansible/next/ardana/ansible
ansible-playbook -i hosts/verb_hosts hlm-start.yml --limit <replaceable>CONTROLLER_NODE</replaceable></screen>
   </listitem>
   <listitem>
    <para>
     Verify that the status of all services on that is OK on the controller
     node:
    </para>
<screen>cd ~/scratch/ansible/next/ardana/ansible
ansible-playbook -i hosts/verb_hosts hlm-status.yml --limit <replaceable>CONTROLLER_NODE</replaceable></screen>
   </listitem>
   <listitem>
    <para>
     When above start operation has completed successfully, you may proceed to
     the next controller node. Ensure that you migrate your singleton services
     off the node first.
    </para>
   </listitem>
  </orderedlist>
  <note>
   <para>
    It is important that you not begin the reboot procedure for a new controller
    node until the reboot of the previous controller node has been completed
    successfully (i.e. the hlm-status playbook has completed without error).
   </para>
  </note>
  <note>
   <para>
    If your system includes VSA nodes, rebooting a controller node will not
    re-start CMC as it is Java application. You will need to restart it.
   </para>
  </note>
 </section>
 <section xml:id="idg-all-upgrade-upgradeto50-xml-11">
  <title>Rebooting Compute Nodes</title>
  <para>
   To reboot a compute node the following operations will need to be
   performed:
  </para>
  <itemizedlist>
   <listitem>
    <para>
     Disable provisioning of the node to take the node offline to prevent
     further instances being scheduled to the node during the reboot.
    </para>
   </listitem>
   <listitem>
    <para>
     Identify instances that exist on the compute node, and then either:
    </para>
    <itemizedlist>
     <listitem>
      <para>
       Live migrate the instances off the node before actioning the reboot. OR
      </para>
     </listitem>
     <listitem>
      <para>
       Stop the instances
      </para>
     </listitem>
    </itemizedlist>
   </listitem>
   <listitem>
    <para>
     Reboot the node
    </para>
   </listitem>
   <listitem>
    <para>
     Restart the Nova services
    </para>
   </listitem>
   <listitem>
    <para>
     Restart any instances stopped above
    </para>
   </listitem>
  </itemizedlist>
  <orderedlist>
   <listitem>
    <para>
     Disable provisioning:
    </para>
<screen>nova service-disable --reason "<replaceable>DESCRIBE_REASON</replaceable>" <replaceable>NODE_NAME</replaceable> nova-compute</screen>
    <para>
     If the node has existing instances running on it these instances will need
     to be migrated or stopped prior to re-booting the node.
    </para>
   </listitem>
   <listitem>
    <para>
     Live migrate existing instances. Identify the instances on the compute
     node. Note: The following command must be run with nova admin credentials.
    </para>
<screen>nova list --host <replaceable>HOSTNAME</replaceable> --all-tenants</screen>
   </listitem>
   <listitem>
    <para>
     Migrate or Stop the instances on the compute node.
    </para>
    <para>
     If your instance is booted from a volume and has any number of Cinder
     volume attached, use the <literal>nova live-migration</literal> command:
    </para>
<screen>nova live-migration <replaceable>INSTANCE_UUID</replaceable> [<replaceable>TARGET_COMPUTE_HOST</replaceable>]</screen>
    <para>
     If your instance has local (ephemeral) disk(s) only, you can use the
     <option>--block-migrate</option> option:
    </para>
<screen>nova live-migration --block-migrate <replaceable>INSTANCE-UUID</replaceable> [<replaceable>TARGET_COMPUTE_HOST</replaceable>]</screen>
    <para>
     <replaceable>TARGET_COMPUTE_HOST</replaceable> is optional. If you do not
     specify a target host then the nova scheduler will choose a node for you.
    </para>
    <para>
     If your instances have a mix of ephemeral and block storage then live
     migration will not work and you can use the <literal>nova stop</literal>
     command:
    </para>
    <para>
     Stop the instances on the node by running the following command for each
     of the instances:
    </para>
<screen>nova stop <replaceable>INSTANCE-UUID</replaceable></screen>
    <para>
     For more details about live migration, see
     <xref linkend="liveInstMigration"/>.
    </para>
   </listitem>
   <listitem>
    <para>
     Stop all services on the Compute node:
    </para>
<screen>cd ~/scratch/ansible/next/ardana/ansible
ansible-playbook -i hosts/verb_hosts hlm-stop.yml --limit <replaceable>COMPUTE_NODE</replaceable></screen>
   </listitem>
   <listitem>
    <para>
     SSH to your Compute nodes and reboot them:
    </para>
<screen>sudo reboot</screen>
    <para>
     The operating system cleanly shuts down services and then automatically
     reboots. If you want to be very thorough, run your backup jobs just before
     you reboot.
    </para>
   </listitem>
   <listitem>
    <para>
     Run the hlm-start.yml playbook from the &lcm;. If needed, use
     the bm-power-up.yml playbook to restart the node. Specify just the node(s)
     you want to start in the <literal>nodelist</literal> parameter arguments,
     that is,
     <literal>nodelist=<replaceable>NODE1</replaceable>[,<replaceable>NODE2</replaceable>][,<replaceable>NODE3</replaceable>]</literal>.
    </para>
<screen>cd ~/openstack/ardana/ansible
ansible-playbook -i hosts/localhost bm-power-up.yml -e nodelist=&lt;compute node&gt;</screen>
   </listitem>
   <listitem>
    <para>
     Execute the <filename>hlm-start.yml</filename> playbook.
     Specifying the node(s) you want to start in the <literal>limit</literal>
     parameter
     arguments. This parameter accepts wildcard arguments and also
     <literal>@<replaceable>FILENAME</replaceable></literal> to process all
     hosts listed in the file.
    </para>
<screen>cd ~/scratch/ansible/next/ardana/ansible
ansible-playbook -i hosts/verb_hosts hlm-start.yml --limit <replaceable>COMPUTE_NODE</replaceable></screen>
   </listitem>
   <listitem>
    <para>
     Re-enable provisioning on the node:
    </para>
<screen>nova service-enable <replaceable>NODE-NAME</replaceable> nova-compute</screen>
   </listitem>
   <listitem>
    <para>
     Restart any instances you stopped.
    </para>
<screen>nova start <replaceable>INSTANCE-UUID</replaceable></screen>
   </listitem>
  </orderedlist>
 </section>
 <section xml:id="idg-all-upgrade-upgradeto50-xml-12">
  <title>Rebooting Swift Nodes</title>
  <para>
   If your Swift services are on controller node, please follow the controller
   node reboot instructions above.
  </para>
  <para>
   For a dedicated Swift PAC cluster or Swift Object resource node:
  </para>
  <para>
   For each Swift host
  </para>
  <orderedlist>
   <listitem>
    <para>
     Stop all services on the Swift node:
    </para>
<screen>cd ~/scratch/ansible/next/ardana/ansible
ansible-playbook -i hosts/verb_hosts hlm-stop.yml --limit &lt;swift node&gt;</screen>
   </listitem>
   <listitem>
    <para>
     Reboot the Swift node by running the following command on the Swift node
     itself:
    </para>
<screen>sudo reboot</screen>
   </listitem>
   <listitem>
    <para>
     Wait for the node to become ssh-able and then start all services on the
     Swift node:
    </para>
<screen>cd ~/scratch/ansible/next/ardana/ansible
ansible-playbook -i hosts/verb_hosts hlm-start.yml --limit &lt;swift node&gt; </screen>
   </listitem>
  </orderedlist>
 </section>
 <section xml:id="idg-all-upgrade-upgradeto50-xml-14">
  <title>Rebooting VSA Nodes</title>
  <para>
   If your cloud is deployed with VSA, then the reboot sequence will be as
   follows:
  </para>
  <orderedlist>
   <listitem>
    <para>
     Identify all VSA nodes of a given cluster
    </para>
   </listitem>
   <listitem>
    <para>
     Pick one of the VSA nodes of the cluster and reboot it manually
    </para>
   </listitem>
   <listitem>
    <para>
     Check VSA VM's status :
    </para>
<screen>ansible-playbook -i hosts/verb_hosts vsa-status.yml --limit &lt;vsa_node name&gt;</screen>
   </listitem>
   <listitem>
    <para>
     Wait for VSA to come up, Check cluster status using CMC ( status should be
     "normal")
    </para>
   </listitem>
   <listitem>
    <para>
     Run the following command from your first controller node which will open
     the HP StoreVirtual Centralized Management Console (CMC) GUI
    </para>
<screen>/opt/HP/StoreVirtual/UI/jre/bin/java -jar /opt/HP/StoreVirtual/UI/UI.jar </screen>
    <para>
     If you are unable to access the CMC utility, you should verify the port
     you access it with this command:
    </para>
<screen>netstat -anp | grep vnc</screen>
    <para>
     If that does not resolve the issue, you may need to restart the VNC server
     with this command:
    </para>
<screen>vnc4server</screen>
   </listitem>
   <listitem>
    <para>
     To view the cluster status , go to: management group -&gt; click on the
     respective cluster -&gt; check for status
    </para>
   </listitem>
   <listitem>
    <para>
     Repeat steps 2-4 for the other remaining nodes of the same VSA cluster.
    </para>
   </listitem>
   <listitem>
    <para>
     Go to step 1 and repeat all steps for the other clusters.
    </para>
   </listitem>
  </orderedlist>
 </section>
 <section xml:id="idg-all-upgrade-upgradeto50-xml-15">
  <title>Rebooting Ceph</title>
  <para>
   There are two categories of ceph nodes:
  </para>
  <itemizedlist>
   <listitem>
    <para>
     monitor nodes
    </para>
   </listitem>
   <listitem>
    <para>
     OSD nodes
    </para>
   </listitem>
  </itemizedlist>
  <para>
   Ceph nodes should be rebooted in their position in the overall sequence
   described in the reboot order section above.
  </para>
  <para>
   You should reboot both categories of ceph nodes in the order: monitor
   nodes, then OSD nodes. All ceph nodes should reboot one-by-one, in series.
  </para>
  <para>
   If a monitor is deployed on controller node, then the rebooting sequence of
   the controller caters for the monitor nodes, so you need to additionally
   reboot only the OSD nodes.
  </para>
  <para>
   If the monitor is deployed as a standalone; i.e., as separate set of
   resource nodes, you should reboot the monitor nodes before rebooting the
   OSD nodes.
  </para>
  <para>
   For example, if there are three monitor nodes and three OSD nodes say, MON1,
   MON2 and MON3, with OSD1, OSD2, OSD3, then you should reboot MON1, then MON2
   and then MON3 followed by OSD1, then OSD2 and so on In both cases, please
   ensure that you are serializing the reboot of a given family (monitor or OSD
   or controller); that is, do not reboot them all at the same time. This will
   ensure that each service is up and running and there is no impact on the
   client side. To reboot Ceph nodes, follow the sequence below to safely
   reboot the entire Ceph cluster:
  </para>
  <orderedlist>
   <listitem>
    <para>
     Reboot a Ceph monitor node in the cluster.
    </para>
   </listitem>
   <listitem>
    <para>
     Once the monitor node comes up, wait for a minute and then execute the
     following command:
    </para>
<screen>ansible-playbook -i hosts/verb_hosts ceph-status.yml --limit &lt;monitor-hostname&gt;</screen>
   </listitem>
   <listitem>
    <para>
     If the above playbook executes successfully, repeat from step 1 to reboot
     the next Ceph monitor node in the cluster.
    </para>
   </listitem>
   <listitem>
    <para>
     Once all the monitor nodes have been rebooted, execute
    </para>
<screen>ceph quorum_status </screen>
    <para>
     on a monitor node and ensure that all the monitors have joined the quorum
     by observing that in the output, the "quorum_names" section lists all the
     monitors in the Ceph cluster.
    </para>
   </listitem>
   <listitem>
    <para>
     Login to the OSD node to be rebooted and execute the following command to
     avoid the cluster rebalancing itself:
    </para>
<screen>ceph osd set noout</screen>
   </listitem>
   <listitem>
    <para>
     Stop all the OSD services on the current node by executing below command:
    </para>
<screen>sudo systemctl stop  ceph-osd@*</screen>
   </listitem>
   <listitem>
    <para>
     Now reboot the OSD node.
    </para>
   </listitem>
   <listitem>
    <para>
     Once the OSD node is up, log in and execute the following command and
     observe that a line similar to "osd.0 192.17.11.7:6801/3824 boot" appears
     for all the OSDs on the node:
    </para>
<screen>ceph -w</screen>
    <para>
     To quit, this command press Ctrl + C
    </para>
   </listitem>
   <listitem>
    <para>
     Verify that all the OSDs on this node are 'up' by executing following
     command (and observing that all the OSDs under the current host are
     reported as 'up')
    </para>
<screen>ceph osd tree</screen>
    <para>
     If any of the OSDs are reported as 'down', please check the logs for the
     corresponding OSD (in the
     <literal>/var/log/ceph/&lt;cluster-name&gt;-osd.&lt;osd_number&gt;.log</literal>
     file) for resolving the issue.
    </para>
   </listitem>
   <listitem>
    <para>
     Now execute the following command from the re-booted OSD node, to unset
     the cluster from noout :
    </para>
<screen>ceph osd unset noout</screen>
   </listitem>
   <listitem>
    <para>
     Execute the following playbook from the &lcm; node to check
     the status of the OSDs that have just come up:
    </para>
<screen>ansible-playbook -i hosts/verb_hosts ceph-status.yml --limit &lt;OSD-hostname&gt;</screen>
   </listitem>
   <listitem>
    <para>
     If the above playbook does not report any errors, repeat from step 5 for
     another Ceph OSD node in the cluster.
    </para>
   </listitem>
   <listitem>
    <para>
     Once the entire cluster is rebooted, execute this command
    </para>
<screen>ceph health detail</screen>
    <para>
     to ensure that you see HEALTH_OK.
    </para>
   </listitem>
  </orderedlist>
 </section>
 <section>
  <title>Rebooting OVSvApp</title>
  <para>
   After upgrade, execute the following commands to restart the OVSvApp VM.
  </para>
  <orderedlist xml:id="ol_ibp_zh3_px">
   <listitem>
    <para>
     Stop all services on the OVSvApp node.
    </para>
<screen>cd ~/scratch/ansible/next/ardana/ansible
ansible-playbook -i hosts/verb_hosts hlm-stop.yml --limit &lt;ovsvapp nodes&gt;</screen>
   </listitem>
   <listitem>
    <para>
     SSH to your OVSvApp nodes and reboot them.
    </para>
<screen>sudo reboot</screen>
   </listitem>
   <listitem>
    <para>
     Execute the <literal>hlm-start.yml</literal> playbook. Specify the node(s)
     you want to start in the 'limit' parameter arguments.
    </para>
<screen>cd ~/scratch/ansible/next/ardana/ansible
ansible-playbook -i hosts/verb_hosts hlm-start.yml --limit &lt;ovsvapp nodes&gt;</screen>
   </listitem>
  </orderedlist>
 </section>
 <section xml:id="checkVersion">
  <title>Checking Version Updates</title>
  <para>
   To check the &slsa; version run from the &lcm;
   node to any other node via its IP address:
  </para>
<screen>ssh &lt;IP_address&gt; uname -a</screen>
  <para>
   The output should be:
  </para>
<screen>Linux ardana-cp1-c1-m2-mgmt 4.4.7-1-amd64-hpelinux #hpelinux1 SMP Thu Apr 14 13:21:49 UTC 2016 x86_64 GNU/Linux</screen>
  <para>
   To check to see if a node has been successfully updated on a control plane
   node, run from the &lcm; node to any other node via its IP
   address: :
  </para>
<screen>ssh &lt;IP_address&gt; cat /etc/HP_Helion_version | grep Helion</screen>
  <para>
   The output should look like this:
  </para>
<screen>&productname; hos-3.0.0 (build 01-1017)</screen>
 </section>
 <section xml:id="idg-all-upgrade-upgradeto50-xml-18">
  <title>Further ESX Model Upgrade Instructions</title>
  <para>
   Once you have completed the initial upgrade instructions, you must follow
   these additional steps for ESX deployments:
  </para>
  <para>
   Check exsiting ESX Compute:
  </para>
  <orderedlist>
   <listitem>
    <para>
     Verify existing ESX computes can be seen by EON and are activated.
    </para>
<screen>eon resource-list
-----------------------------------------------------------------------------------------------------------------------------------------+
ID	Name	Moid	Resource Manager ID	IP Address	Port	Type	State
-----------------------------------------------------------------------------------------------------------------------------------------+
fdf19467-e7a1-496f-81e5-bcb07bc6e564	Compute-02	domain-c9949	70be00c8-691b-43af-b757-e67e4b11425c	UNSET	UNSET	esxcluster	activated
-----------------------------------------------------------------------------------------------------------------------------------------+</screen>
   </listitem>
  </orderedlist>
  <para>
   Activate New ESX Compute Cluster
  </para>
  <orderedlist>
   <listitem>
    <para>
     After upgrade, download the new <literal>activationtemplate.json</literal>
     file for a new cluster activation.
    </para>
<screen>eon get-activation-template [--filename &lt;ACTIVATION_JSON_NAME&gt;] --type &lt;RESOURCE_TYPE&gt;</screen>
   </listitem>
   <listitem>
    <para>
     Fill the appropriate details in the
     <literal>activationtemplate.json</literal> file.
    </para>
   </listitem>
   <listitem>
    <para>
     Activate the new cluster on an already-activated Data center (DC) or a new
     Data center can be performed with the following command:
    </para>
<screen>eon resource-activate &lt;RESOURCE_ID&gt; --config-json &lt;activate-template&gt;</screen>
    <para>
     <emphasis role="bold">Example:</emphasis>
    </para>
<screen>eon resource-activate 9ab2196d-37d2-4006-a2c7-08946b5194ea --config-json activate-template.json</screen>
   </listitem>
  </orderedlist>
 </section>
 <section xml:id="magnum_upgrade">
  <title>Adding Magnum After Upgrade</title>
  <para>
   Adding Magnum after the upgrade of &product; 4.0 to &product;
   5.0 can be found in the document, <xref linkend="sec.magnum.exist"/>.
  </para>
 </section>
 <section xml:id="idg-all-upgrade-upgradeto50-xml-20">
  <title>Get List of Status Playbooks</title>
  <para>
   Running the following command will yield a list of status playbooks:
  </para>
<screen>cd ~/scratch/ansible/next/ardana/ansible
ls *status*</screen>
  <para>
   Here is the list:
  </para>
<screen>stack@helion-control-plane-cluster1-m1-mgmt:~/scratch/ansible/next/ardana/ansible$ ls *status*
barbican-status.yml       heat-status.yml              monasca-status.yml
bind-status.yml           ardana-pre-upgrade-status.yml   neutron-status.yml
bm-power-status.yml       ardana-status.yml               nova-status.yml
ceilometer-status.yml     ardana-ux-services-status.yml   octavia-status.yml
_ceph_cluster_status.yml  horizon-status.yml           ops-console-status.yml
ceph-status.yml           hos-extra-status.yml         percona-status.yml
cinder-status.yml         ironic-status.yml            powerdns-status.yml
cmc-status.yml            keystone-status.yml          rabbitmq-status.yml
designate-status.yml      kronos-api-status.yml        sherpa-status.yml
eon-status.yml            logging-producer-status.yml  swift-status.yml
FND-AP2-status.yml        logging-server-status.yml    vsa-status.yml
FND-CLU-status.yml        logging-status.yml           zookeeper-status.yml
freezer-status.yml        memcached-status.yml
glance-status.yml         monasca-agent-status.yml
</screen>
 </section>
</section>
