<?xml version="1.0"?>
<!DOCTYPE section [
 <!ENTITY % entities SYSTEM "entities.ent"> %entities;
]>
<section xmlns="http://docbook.org/ns/docbook" xmlns:xi="http://www.w3.org/2001/XInclude" xmlns:xlink="http://www.w3.org/1999/xlink" xml:id="release-notes-501"><title>&kw-hos-tm;
    &kw-hos-version-501;: Release Notes</title><abstract><para><para>An overview of the features contained within &kw-hos-version-501;, including known issues and workarounds for this
      release.</para></para>
</abstract>
    
    <!---->

    
    <sidebar><title>Fixed in this release</title><para><emphasis role="bold">&kw-hos-phrase-501; supports SLES12 SP3 for Compute Nodes</emphasis></para>
<para>SLES12 SP2 is supported for compute nodes in &kw-hos-phrase-50;. &kw-hos-phrase-501; includes support for the more recently released SLES12 SP3.
          </para>
<important><para>&kw-hos-phrase-501; does not support SLES12 SP2
          compute nodes. If you intend to upgrade to &kw-hos-phrase-501; you will
          be required to upgrade your compute nodes to SP3. See steps below for upgrading SLES
          compute nodes from SP2 to SP3. </para>
</important>
<para><emphasis role="bold">Using the Lifecycle Manager to Deploy SLES Compute Nodes</emphasis></para>
<para>The method used for deploying SLES compute nodes using Cobbler on the lifecycle manager
        uses legacy BIOS. 
        
      </para>
<para>Deploying legacy BIOS SLES compute nodes</para>
<para>The installation process for SLES nodes is almost
        identical to that of HPE Linux nodes as described in the topic for Installation for 
        <link xlink:href="https://docs.hpcloud.com/hos-5.x/helion/installation/installing_kvm.html">
          &kw-hos; Entry-scale KVM Cloud</link>. 
        The key differences are: 
      </para>
<itemizedlist>
          <listitem><para>The standard SLES ISO (SLE-12-SP3-Server-DVD-x86_64-GM-DVD1.iso) must be accessible
            via <literal>/home/stack/sles12sp3.iso</literal>. Rename the ISO or create a symbolic
            link.
            </para>
<screen >mv SLE-12-SP3-Server-DVD-x86_64-GM-DVD1.iso /home/stack/sles12sp3.iso</screen></listitem>
          <listitem><para>The contents of the SLES SDK ISO (SLE-12-SP3-SDK-DVD-x86_64-GM-DVD1.iso) must be
            mounted or copied to <literal>/opt/hlm_packager/hlm/sles12/zypper/SDK/</literal>. If you
            choose to mount the ISO, we recommend creating an <literal>/etc/fstab</literal> entry to
            ensure the ISO is mounted after a reboot. 
            </para>
<itemizedlist>
              <listitem><para>If mounting the .iso on loopback
                </para>
<screen >sudo mount -o loop /home/stack/sles12.iso /opt/hlm_packager/hlm/sles12/zypper/SDK/</screen></listitem>
              <listitem><para>Entry for mounting the .iso on loopback in /etc/fstab
                </para>
<screen >/home/stack/sles12.iso    /opt/hlm_packager/hlm/sles12/zypper/SDK/    iso9660    loop    0    0</screen></listitem>
            </itemizedlist></listitem>
        <listitem><para>You must identify the node(s) on which you want to install SLES, by adding the key/value
            pair distro-id: <literal>sles12sp3-x86_64</literal> to server details in
              <literal>servers.yml</literal>. You will also need to update
              <literal>net_interfaces.yml</literal>, <literal>server_roles.yml</literal>,
              <literal>disk_compute.yml</literal> and <literal>control_plane.yml</literal>. For more
            information on configuration of the Input Model for SLES, see SLES Compute Model. </para>
</listitem>
          <listitem><para>&kw-hos; playbooks currently do not take care of SDK, so it needs to
            be added manually. The following command needs to be run on every SLES compute node:
            </para>
<screen >deployer_ip=192.168.10.254
zypper addrepo --no-gpg-checks --refresh http://$deployer_ip:79/hlm/sles12/zypper/SDK SLES-SDK</screen></listitem>
        </itemizedlist>
<para><emphasis role="bold">Upgrading &kw-hos-phrase; with SLES Compute SP2 to SP3</emphasis></para>
<para>You can upgrade your SLES compute nodes from SP2 to SP3 by following these steps. 
        </para>
<orderedlist>
          <listitem><para>You must be running &kw-hos-phrase; with SLES12 SP2 Compute</para>
</listitem>
          <listitem><para>Upgrade the Compute OS from SP2 to SP3 by following these steps. 
            </para>
<itemizedlist>
              <listitem><para>On the deployer, unmount the sp2 content, example:
                </para>
<screen >sudo umount /opt/hlm_packager/hlm/sles12/zypper/OS/
sudo umount /opt/hlm_packager/hlm/sles12/zypper/SDK/</screen></listitem>
              <listitem><para>Copy or mount the contents of SLES SDK ISO, example:
                </para>
<screen >sudo mount -o loop SLE-12-SP3-SDK-DVD-x86_64-GM-DVD1.iso /opt/hlm_packager/hlm/sles12/zypper/SDK/
sudo mount -o loop SLE-12-SP3-Server-DVD-x86_64-GM-DVD1.iso /opt/hlm_packager/hlm/sles12/zypper/OS/ </screen></listitem>
              <listitem><para>On sles12sp2 compute, remove old repositories if applicable
                </para>
<screen >sudo zypper removerepo SLES-SDK
sudo zypper removerepo SLES12-SP2-12.2-0</screen></listitem>
              <listitem><para>On sles12sp2 compute, clean up old repositories if applicable
                </para>
<screen >sudo zypper clean</screen></listitem>
              <listitem><para>Add updated repositories (Update the value of <literal>deployer_ip</literal> as
                necessary)
                </para>
<screen >deployer_ip=192.168.10.254
zypper addrepo --no-gpg-checks --refresh http://$deployer_ip:79/hlm/sles12/zypper/OS SLES-OS
zypper addrepo --no-gpg-checks --refresh http://$deployer_ip:79/hlm/sles12/zypper/SDK SLES-SDK</screen></listitem>
              <listitem><para>Run zypper to refresh
                </para>
<screen >sudo <literal>zypper</literal> refresh</screen></listitem>
              <listitem><para>Run <literal>zypper</literal> to upgrade: </para>
<screen >sudo zypper up</screen></listitem>
            </itemizedlist></listitem>
        
          <listitem><para>Mount the 5.0.1 ISO image
            </para>
<screen >sudo mount -o loop HelionOpenStack-5.0.iso /media/cdrom</screen></listitem>
          <listitem><para>Unpack the following tarball:
            </para>
<screen >cd ~
tar fxv /media/cdrom/hos/hos-5.0.0-20161014T020249Z.tar</screen></listitem>
          <listitem><para>Run the included initialization script to update the deployer:
            </para>
<screen >~/hos-5.0.1/hos-init.bash</screen></listitem>
          <listitem><para>Run the configuration processor:
            </para>
<screen >cd ~/helion/hos/ansible
ansible-playbook -i hosts/localhost config-processor-run.yml</screen></listitem>
          <listitem><para>Update the deployment directory:
            </para>
<screen >cd ~/helion/hos/ansible
ansible-playbook -i hosts/localhost ready-deployment.yml</screen></listitem>
          <listitem><para>Run site.yml
            </para>
<screen >cd ~/scratch/ansible/next/hos/ansible
ansible-playbook -i hosts/verb_hosts site.yml</screen></listitem>
          <listitem><para>If Ceph is configured, run hlm-cloud-configure.yml
            </para>
<screen >cd ~/scratch/ansible/next/hos/ansible
ansible-playbook -i hosts/verb_hosts hlm-cloud-configure.yml</screen></listitem>
          <listitem><para>Reboot all</para>
</listitem>
        </orderedlist>
<para><emphasis role="bold">HPE Linux Changes</emphasis></para>
<para>HPE Helion OpenStack 5.0.1 includes a new version of the hLinux operating system which has fixes for CVEs known to us and deemed to be issues not mitigated by architecture at the time of ISO creation.</para>
<para><emphasis role="bold">MTU Bug for LBaaS</emphasis></para>
<para>Contains fix for OpenStack bug <link xlink:href="https://review.openstack.org/#/c/407617/">https://review.openstack.org/#/c/407617/</link></para>
<para>LBaaSv2 sets up a VIF without specifying its MTU. Therefore, the VIF always gets the
        default MTU of 1500. When attaching the load balancer to a VXLAN-backed project (tenant)
        network, which by default has a MTU of 1450, this leads to packet dropping.</para>
<para>Removed hardcoded value which prevented LBaaSv2 from using MTU specified by the network object.</para>
<para><emphasis role="bold">Magnum Fedora Atomic for &kw-hos-phrase; must use version fedora-25</emphasis></para>
<para>You can find more information in the document 
      </para>
<para><emphasis role="bold">3PAR Fix</emphasis></para>
<para>Fixed an issue with the 3PAR driver to ensure that CHAP credentials are used correctly after compute host is rebooted.</para>
<para><emphasis role="bold">Adding Compute Nodes with Ceph already configured</emphasis></para>
<para>An additional step is needed to add compute nodes to a cloud where Ceph has already been configured.
        </para>
<itemizedlist>
          <listitem><para>Complete the compute host deployment using site.yml as noted in  in the &kw-hos-phrase; documentation.</para>
</listitem>
          <listitem><para>Then run the following command.
            </para>
<screen >cd ~/scratch/ansible/next/hos/ansible/ansible-playbook -i hosts/verb_hosts ceph-client-prepare.yml</screen></listitem>
        </itemizedlist>
<para>
      This applies to hLinux, SLES, RHEL and RHEV computes.
      </para>
</sidebar>
    
    <!---->
  </section>
