<?xml version="1.0"?>
<?xml-stylesheet href="urn:x-suse:xslt:profiling:docbook51-profile.xsl"
    type="text/xml"
    title="Profiling step"?>
<!DOCTYPE section [
<!ENTITY % entities SYSTEM "entity-decl.ent"> %entities;
]>
<section xmlns="http://docbook.org/ns/docbook" xmlns:xi="http://www.w3.org/2001/XInclude" xmlns:xlink="http://www.w3.org/1999/xlink" xml:id="upgrade-soc">
 <title>Upgrading &clm; 8 to &clm; 9</title>
 <para>
   Before undertaking the upgrade from &cloud; (or HOS) 8 &clm; to
   &productname; &productnumber; &clm;, you need to ensure that your
   existing &cloud; 8 &clm; installation is up to date by following the
   <link xlink:href="https://documentation.suse.com/hpe-helion/8/html/hpe-helion-openstack-clm-all/system-maintenance.html#maintenance-update"/>.
 </para>
 <para>
   Ensure you review the following resources:
 </para>
 <itemizedlist>
   <listitem>
     <para>
       <link xlink:href="https://documentation.suse.com/hpe-helion/8/html/hpe-helion-openstack-clm-all/system-maintenance.html#maintenance-update/"/>
     </para>
   </listitem>
   <listitem>
     <para>
       <link xlink:href="https://documentation.suse.com/soc/8/html/suse-openstack-cloud-clm-all/system-maintenance.html#maintenance-update"/>
     </para>
   </listitem>
 </itemizedlist>
 <para>
   To confirm that all nodes have been successfully updated with no pending
   actions, run the <filename>ardana-update-status.yml</filename> playbook on
   the &clm; deployer node as follows:
 </para>
 <screen>&prompt.ardana;cd scratch/ansible/next/ardana/ansible/
&prompt.ardana;ansible-playbook -i hosts/verb_hosts ardana-update-status.yml
 </screen>
 <note>
   <para>
     Ensure that all nodes have been updated and that there are no pending
     update actions remaining to be completed. In particular, ensure that
     any nodes that need to be rebooted have been, using the documented reboot procedure.
   </para>
 </note>
 <procedure>
   <title>Running the Pre-Upgrade Validation Checks to Ensure that your Cloud is Ready for Upgrade</title>
   <step>
     <para>
       Once all nodes have been successfully updated, and there are no pending
       update actions remaining, you should be able to run the
       <filename>ardana-pre-upgrade-validations.sh</filename> script, as follows:
     </para>
     <screen>&prompt.ardana;cd scratch/ansible/next/ardana/ansible/
&prompt.ardana;./ardana-pre-upgrade-validations.sh
~/scratch/ansible/next/ardana/ansible ~/scratch/ansible/next/ardana/ansible

PLAY [Initialize an empty list of msgs] ***************************************

TASK: [set_fact ] *************************************************************
ok: [localhost]
...

PLAY RECAP ********************************************************************
...
localhost                  : ok=8    changed=5    unreachable=0    failed=0

msg: Please refer to /var/log/ardana-pre-upgrade-validations.log for the results of this run. Ensure that any messages in the file that have the words FAIL or WARN are resolved.
</screen>
     <para>
       The last line of output from the <filename>ardana-pre-upgrade-validations.sh</filename>
       script will tell you the name of its log file â€“ in this case,
       <filename>/var/log/ardana-pre-upgrade-validations.log</filename>. If you
       look at the log file, you will see content similar to the following:
     </para>
     <screen>&prompt.ardana;sudo cat /var/log/ardana-pre-upgrade-validations.log
ardana-cp-dbmqsw-m1*************************************************************
NOTE: pre-upgrade-swift-checks: Swift XFS inode size of 256 for /srv/node/disk0 is smaller than SLE 12 SP4 recommended 512. Some recommended XFS data integrity features may not be available after upgrade.
NOTE: pre-upgrade-swift-checks: Swift XFS inode size of 256 for /srv/node/disk1 is smaller than SLE 12 SP4 recommended 512. Some recommended XFS data integrity features may not be available after upgrade.
NOTE: pre-upgrade-swift-checks: Swift XFS inode size of 256 for /srv/node/disk2 is smaller than SLE 12 SP4 recommended 512. Some recommended XFS data integrity features may not be available after upgrade.

ardana-cp-dbmqsw-m2*************************************************************
NOTE: pre-upgrade-swift-checks: Swift XFS inode size of 256 for /srv/node/disk0 is smaller than SLE 12 SP4 recommended 512. Some recommended XFS data integrity features may not be available after upgrade.
NOTE: pre-upgrade-swift-checks: Swift XFS inode size of 256 for /srv/node/disk1 is smaller than SLE 12 SP4 recommended 512. Some recommended XFS data integrity features may not be available after upgrade.
NOTE: pre-upgrade-swift-checks: Swift XFS inode size of 256 for /srv/node/disk2 is smaller than SLE 12 SP4 recommended 512. Some recommended XFS data integrity features may not be available after upgrade.

ardana-cp-dbmqsw-m3*************************************************************
NOTE: pre-upgrade-swift-checks: Swift XFS inode size of 256 for /srv/node/disk0 is smaller than SLE 12 SP4 recommended 512. Some recommended XFS data integrity features may not be available after upgrade.
NOTE: pre-upgrade-swift-checks: Swift XFS inode size of 256 for /srv/node/disk1 is smaller than SLE 12 SP4 recommended 512. Some recommended XFS data integrity features may not be available after upgrade.
NOTE: pre-upgrade-swift-checks: Swift XFS inode size of 256 for /srv/node/disk2 is smaller than SLE 12 SP4 recommended 512. Some recommended XFS data integrity features may not be available after upgrade.

ardana-cp-mml-m1****************************************************************
SUCCESS: Keystone V2 ==> V3 API config changes detected.
ardana-cp-mml-m2****************************************************************
SUCCESS: Keystone V2 ==> V3 API config changes detected.
ardana-cp-mml-m3****************************************************************
SUCCESS: Keystone V2 ==> V3 API config changes detected.
localhost***********************************************************************
     </screen>
     <para>
       The report states the following:
     </para>
     <variablelist>
       <varlistentry>
         <term>SUCCESS: Keystone V2 ==> V3 API config changes detected.</term>
         <listitem>
           <para>
             This check confirms that your cloud has been updated with the
             necessary changes such that all services will be using Keystone V3 API.
             This means that there should be minimal interruption of service during
             the upgrade. This is important because the Keystone V2 API has been
             removed in &productname; &productnumber;.
           </para>
         </listitem>
       </varlistentry>
       <varlistentry>
         <term>NOTE: pre-upgrade-swift-checks: Swift XFS inode size of 256 for
           /srv/node/disk0 is smaller than SLE 12 SP4 recommended 512. Some
           recommended XFS data integrity features may not be available after upgrade.</term>
         <listitem>
           <para>
             This check will only report something if you have local &o_objstore;
             configured and it is formatted with the SLE 12 SP3 default XFS
             inode size of 256. In SLE 12 SP4, the default XFS inode size for a
             newly formatted XFS file system has been increased to 512, to
             allow room for enabling some additional XFS data-integrity features by default.
           </para>
         </listitem>
       </varlistentry>
     </variablelist>
   </step>
 </procedure>
 <note>
   <para>
     There will be no loss of functionality as regards the &o_objstore;
     solution after the upgrade. The difference is that some additional
     XFS features will not be available on file systems which were formatted
     under SLE 12 SP3 or earlier. These XFS features aid in the detection of,
     and recovery from, data corruption. They are enabled by default for XFS
     file systems formatted under SLE 12 SP 4.
   </para>
 </note>
 <procedure>
   <title>Additional Pre-Upgrade Checks That Should Be Performed</title>
   <para>
     In addition to the automated upgrade checks above, there are some checks
     that should be performed manually.
   </para>
   <step>
     <para>
       For each network interface device specified in the input model under
       <filename>~ardana/openstack/my_cloud/definition</filename>, ensure that
       there is only one untagged VLAN. The &productname; &productnumber; &clm;
       configuration processor will fail with an error if it detects this problem
       during the upgrade, so address this problem before starting the upgrade process.
     </para>
   </step>
   <step>
     <para>
       If the deployer node is not a standalone system, but
       is instead co-located with the DB services, this can lead to
       potentially longer service disruptions during the upgrade process. To
       determine if this is the case, check if the deployer node (OPS-LM--first-member)
       is a member of the database nodes (FND-MDB). You can do this with the
       following command:
     </para>
     <screen>&prompt.ardana;cd scratch/ansible/next/ardana/ansible/
&prompt.ardana;ansible -i hosts/verb_hosts 'FND-MDB:&amp;OPS-LM--first-member' --list-hosts
     </screen>
     <para>
       If the output is:
     </para>
     <screen>
       No hosts matched
     </screen>
     <para>
       Then the deployer node is not co-located with the database nodes.
       Otherwise, if the command reports a hostname, then there may be
       additional interruptions to the database services during the upgrade.
     </para>
   </step>
   <step>
     <para>
       Similarly, if the deployer is co-located with the database services,
       and you are also trying to run a local SMT service on the deployer
       node, you will run into issues trying to configure the SMT to enable and
       mirror the SLE 12 SP4 and &productname; &productnumber; repositories.
     </para>
     <para>
       In such cases, it is recommended that you run the SMT services on a
       different node, and NFS-import the <filename>/srv/www/htdocs/repo</filename> onto the deployer
       node, instead of trying to run the SMT services locally.
     </para>
   </step>
 </procedure>
 <note>
   <title>Backup the &clm; Configuration Settings</title>
     <para>
       The integrated backup solution in &productname; 8 &clm;, freezer, is no
       longer available in &productname; &productnumber; &clm;. Therefore, we
       recommend doing a manual backup to a server that is not a member of the
       cloud, as per <xref linkend="bura-overview"/>.
     </para>
   </note>

 <section xml:id="upgrade-overivew">
   <title>Migrating the Deployer Node Packages</title>
   <para>
     The upgrade process first migrates the &productname; 8 &clm; deployer node
     to SLE 12 SP4 + SOC 9 CLM packages.
   </para>
   <important>
     <para>
       If the deployer node is not a dedicated node, but is instead a member
       of one of the cloud control planes, then some services may restart with
       the SLE 12 SP4 and SOC 9 CLM versions of the software during the migration.
       This may mean that:
     </para>
     <itemizedlist>
       <listitem>
         <para>
           Some services fail to restart. This will be resolved when the
           appropriate &productname; &productnumber; config changes are applied
           by running the <filename>ardana-upgrade.yml</filename> playbook, later
           during the upgrade process.
         </para>
       </listitem>
       <listitem>
         <para>
           Other services may log excessive warnings about connectivity issues
           and backwards-compatibility warnings. This will be resolved
           when the relevant services are upgraded during the <filename>ardana-upgrade.yml</filename>
           playbook run.
         </para>
       </listitem>
     </itemizedlist>
   </important>

   <procedure>
     <title>Migrating an SCC/SMT Registered Deployer Node</title>
     <para>
       In order to upgrade the deployer node to be based on &productname; &productnumber;
       &clm;, you first need to migrate the system to SLE 12 SP4 with the &productname; &productnumber;
       &clm; product installed.
     </para>
     <para>
       The process for migrating the deployer node differs somewhat, depending
       on whether your deployer node is registered with the SUSE Customer Centre
       (or an SMT mirror), versus using locally-maintained repositories available
       at the relevant locations.
     </para>
     <para>
       If your deployer node is registered with the SCC or an SMT, the migration
       process requires the <literal>zypper-migration-plugin</literal> package to be installed.
     </para>
     <step>
       <para>
         If you are using an SMT server to mirror the relevant repositories,
         then you need to enable mirroring of the relevant repositories. See
         <xref linkend="app-deploy-smt-repos"/> for more information.
       </para>
       <para>
         Ensure that the mirroring process has completed before proceeding.
       </para>
     </step>
     <step>
       <para>
         Ensure that the <literal>zypper-migration-plugin</literal> package is
         installed; if not, install it:
       </para>
       <screen>&prompt.ardana;sudo zypper install zypper-migration-plugin
Refreshing service 'SMT-http_smt_example_com'.
Loading repository data...
Reading installed packages...
'zypper-migration-plugin' is already installed.
No update candidate for 'zypper-migration-plugin-0.10-12.4.noarch'. The highest available version is already installed.
Resolving package dependencies...

Nothing to do.
       </screen>
     </step>
     <step>
       <para>
         De-register the SUSE Linux Enterprise Server LTSS 12 SP3 x86_64
         extension (if enabled):
       </para>
       <screen>&prompt.ardana;sudo SUSEConnect --status-text
Installed Products:
------------------------------------------

  SUSE Linux Enterprise Server 12 SP3 LTSS
  (SLES-LTSS/12.3/x86_64)

  Registered

------------------------------------------

  SUSE Linux Enterprise Server 12 SP3
  (SLES/12.3/x86_64)

  Registered

------------------------------------------

  SUSE OpenStack Cloud 8
  (suse-openstack-cloud/8/x86_64)

  Registered

------------------------------------------


ardana > sudo SUSEConnect -d -p SLES-LTSS/12.3/x86_64
Deregistering system from registration proxy https://smt.example.com/

Deactivating SLES-LTSS 12.3 x86_64 ...
-> Refreshing service ...
-> Removing release package ...
ardana > sudo SUSEConnect --status-text
Installed Products:
------------------------------------------

  SUSE Linux Enterprise Server 12 SP3
  (SLES/12.3/x86_64)

  Registered

------------------------------------------

  SUSE OpenStack Cloud 8
  (suse-openstack-cloud/8/x86_64)

  Registered

------------------------------------------
       </screen>
     </step>
     <step>
       <para>
         Disable any other SLE 12 SP3 or SOC (or HOS) 8 CLM related repositories.
         The zypper migration process should detect and disable most of these
         automatically, but in some cases it may not catch all of them, which
         can lead to a minor disruption later during the upgrade procedure. For
         example, to disable any repositories served from the
         <filename>/srv/www/suse-12.3</filename> directory or the SUSE-12-4 alias
         under http://localjost:79/, you could use the following commands:
       </para>
       <screen>&prompt.ardana;zypper repos --show-enabled-only --uri | grep -e dir:///srv/www/suse-12.3 -e http://localhost:79/SUSE-12-4/ | cut -d '|' -f 2
PTF
SLES12-SP3-LTSS-Updates
SLES12-SP3-Pool
SLES12-SP3-Updates
SUSE-OpenStack-Cloud-8-Pool
SUSE-OpenStack-Cloud-8-Updates
ardana > for repo in $(zypper repos --show-enabled-only --uri | grep -e dir:///srv/www/suse-12.3 -e http://localhost:79/SUSE-12-4/ | cut -d '|' -f 2); do sudo zypper modifyrepo --disable "${repo}"; done
Repository 'PTF' has been successfully disabled.
Repository 'SLES12-SP3-LTSS-Updates' has been successfully disabled.
Repository 'SLES12-SP3-Pool' has been successfully disabled.
Repository 'SLES12-SP3-Updates' has been successfully disabled.
Repository 'SUSE-OpenStack-Cloud-8-Pool' has been successfully disabled.
Repository 'SUSE-OpenStack-Cloud-8-Updates' has been successfully disabled.
       </screen>
     </step>
     <step>
       <para>
         Remove the PTF repository, which is based on SLE 12 SP3 (a new one,
         based on SLE 12 SP4, will be created during the upgrade process):
       </para>
       <screen>&prompt.ardana;zypper repos | grep PTF
 2 | PTF                                                               | PTF                                      | No      | (r ) Yes  | Yes
ardana > sudo zypper removerepo PTF
Removing repository 'PTF' ..............................................................................................[done]
Repository 'PTF' has been removed.
       </screen>
     </step>
     <step>
       <para>
         Remove the Cloud media repository (if defined):
       </para>
       <screen>&prompt.ardana;zypper repos | grep '[|] Cloud '
 1 | Cloud                          | SUSE OpenStack Cloud 8 DVD #1  | Yes     | (r ) Yes  | No
ardana > sudo zypper removerepo Cloud
Removing repository 'SUSE OpenStack Cloud 8 DVD #1' ....................................................................[done]
Repository 'SUSE OpenStack Cloud 8 DVD #1' has been removed.
       </screen>
     </step>
     <step>
       <para>
         Run the zypper migration command, which should offer a single choice:
         namely, to upgrade to SLE 12 SP4 and SOC 9 CLM. You need to accept the
         offered choice, then answer <option>yes</option> to any prompts to disable obsoleted repositories.
         At that point, the zypper migration command will run <command>zypper dist-upgrade</command>,
         which will prompt you to agree with the proposed package changes. Finally,
         you will to agree with any new licenses. After this, the package upgrade
         of the deployer node will proceed. The output of the running <command>zypper migration</command>
         should look something like the following:
       </para>
       <screen>&prompt.ardana;sudo zypper migration

Executing 'zypper  refresh'

Repository 'SLES12-SP3-Pool' is up to date.
Repository 'SLES12-SP3-Updates' is up to date.
Repository 'SLES12-SP3-Pool' is up to date.
Repository 'SLES12-SP3-Updates' is up to date.
Repository 'SUSE-OpenStack-Cloud-8-Pool' is up to date.
Repository 'SUSE-OpenStack-Cloud-8-Updates' is up to date.
Repository 'OpenStack-Cloud-8-Pool' is up to date.
Repository 'OpenStack-Cloud-8-Updates' is up to date.
All repositories have been refreshed.

Executing 'zypper  --no-refresh patch-check --updatestack-only'

Loading repository data...
Reading installed packages...

0 patches needed (0 security patches)

Available migrations:

    1 | SUSE Linux Enterprise Server 12 SP4 x86_64
        SUSE OpenStack Cloud 9 x86_64


[num/q]: 1

Executing 'snapper create --type pre --cleanup-algorithm=number --print-number --userdata important=yes --description 'before online migration''

The config 'root' does not exist. Likely snapper is not configured.
See 'man snapper' for further instructions.
Upgrading product SUSE Linux Enterprise Server 12 SP4 x86_64.
Found obsolete repository SLES12-SP3-Updates
Disable obsolete repository SLES12-SP3-Updates [y/n] (y): y
... disabling.
Found obsolete repository SLES12-SP3-Pool
Disable obsolete repository SLES12-SP3-Pool [y/n] (y): y
... disabling.
Upgrading product SUSE OpenStack Cloud 9 x86_64.
Found obsolete repository OpenStack-Cloud-8-Pool
Disable obsolete repository OpenStack-Cloud-8-Pool [y/n] (y): y
... disabling.

Executing 'zypper --releasever 12.4 ref -f'

Warning: Enforced setting: $releasever=12.4
Forcing raw metadata refresh
Retrieving repository 'SLES12-SP4-Pool' metadata .......................................................................[done]
Forcing building of repository cache
Building repository 'SLES12-SP4-Pool' cache ............................................................................[done]
Forcing raw metadata refresh
Retrieving repository 'SLES12-SP4-Updates' metadata ....................................................................[done]
Forcing building of repository cache
Building repository 'SLES12-SP4-Updates' cache .........................................................................[done]
Forcing raw metadata refresh
Retrieving repository 'SUSE-OpenStack-Cloud-9-Pool' metadata ...........................................................[done]
Forcing building of repository cache
Building repository 'SUSE-OpenStack-Cloud-9-Pool' cache ................................................................[done]
Forcing raw metadata refresh
Retrieving repository 'SUSE-OpenStack-Cloud-9-Updates' metadata ........................................................[done]
Forcing building of repository cache
Building repository 'SUSE-OpenStack-Cloud-9-Updates' cache .............................................................[done]
Forcing raw metadata refresh
Retrieving repository 'OpenStack-Cloud-8-Updates' metadata .............................................................[done]
Forcing building of repository cache
Building repository 'OpenStack-Cloud-8-Updates' cache ..................................................................[done]
All repositories have been refreshed.

Executing 'zypper --releasever 12.4  --no-refresh  dist-upgrade --no-allow-vendor-change '

Warning: Enforced setting: $releasever=12.4
Warning: You are about to do a distribution upgrade with all enabled repositories. Make sure these repositories are compatible before you continue. See 'man zypper' for more information about this command.
Loading repository data...
Reading installed packages...
Computing distribution upgrade...

...

525 packages to upgrade, 14 to downgrade, 62 new, 5 to remove, 1 to change vendor, 1 to change arch.
Overall download size: 1.24 GiB. Already cached: 0 B. After the operation, additional 780.8 MiB will be used.
Continue? [y/n/...? shows all options] (y): y
...
    dracut: *** Generating early-microcode cpio image ***
    dracut: *** Constructing GenuineIntel.bin ****
    dracut: *** Store current command line parameters ***
    dracut: Stored kernel commandline:
    dracut:  rd.lvm.lv=ardana-vg/root
    dracut:  root=/dev/mapper/ardana--vg-root rootfstype=ext4 rootflags=rw,relatime,data=ordered
    dracut: *** Creating image file '/boot/initrd-4.4.180-94.127-default' ***
    dracut: *** Creating initramfs image file '/boot/initrd-4.4.180-94.127-default' done ***

Output of btrfsmaintenance-0.2-18.1.noarch.rpm %posttrans script:
    Refresh script btrfs-scrub.sh for monthly
    Refresh script btrfs-defrag.sh for none
    Refresh script btrfs-balance.sh for weekly
    Refresh script btrfs-trim.sh for none

There are some running programs that might use files deleted by recent upgrade. You may wish to check and restart some of them. Run 'zypper ps -s' to list these programs.
       </screen>
     </step>
   </procedure>

   <procedure>
     <title>Migrating a Deployer Node with Locally-Managed Repositories</title>
     <para>
       In this configuration, you need to manually migrate the system using
       <command>zypper dist-upgrade</command>, according to the following steps:
     </para>
     <step>
       <para>
         Disable any SLE 12 SP3 or &productname; 8 CLM related repositories.
         Leaving the SLE 12 SP3 and/or SOC (or HOS) 8 CLM related repositories
         enabled can lead to a minor disruption later during the upgrade procedure.
         For example, to disable any repositories served from the
         <filename>/srv/www/suse-12.3</filename> directory, or the SUSE-12-4
         alias under http://localjost:79/,  use the following commands:
       </para>
       <screen>&prompt.ardana;zypper repos --show-enabled-only --uri | grep -e dir:///srv/www/suse-12.3 -e http://localhost:79/SUSE-12-4/ | cut -d '|' -f 2
PTF
SLES12-SP3-LTSS-Updates
SLES12-SP3-Pool
SLES12-SP3-Updates
SUSE-OpenStack-Cloud-8-Pool
SUSE-OpenStack-Cloud-8-Updates
ardana > for repo in $(zypper repos --show-enabled-only --uri | grep -e dir:///srv/www/suse-12.3 -e http://localhost:79/SUSE-12-4/ | cut -d '|' -f 2); do sudo zypper modifyrepo --disable "${repo}"; done
Repository 'PTF' has been successfully disabled.
Repository 'SLES12-SP3-LTSS-Updates' has been successfully disabled.
Repository 'SLES12-SP3-Pool' has been successfully disabled.
Repository 'SLES12-SP3-Updates' has been successfully disabled.
Repository 'SUSE-OpenStack-Cloud-8-Pool' has been successfully disabled.
Repository 'SUSE-OpenStack-Cloud-8-Updates' has been successfully disabled.
       </screen>
       <note>
         <para>
           The SLES12-SP3-LTSS-Updates repository should only be present if
           you have purchased the optional  SLE 12 SP3 LTSS support. Whether
           or not it is configured will not impact the upgrade process.
         </para>
       </note>
     </step>
     <step>
       <para>
         Remove the PTF repository, which is based on SLE 12 SP3. A new one
         based on SLE 12 SP4 will be created during the upgrade process.
       </para>
       <screen>&prompt.ardana;zypper repos | grep PTF
 2 | PTF                                                               | PTF                                      | Yes     | (r ) Yes  | Yes
ardana > sudo zypper removerepo PTF
Removing repository 'PTF' ..............................................................................................[done]
Repository 'PTF' has been removed.
       </screen>
     </step>
     <step>
       <para>
         Remove the Cloud media repository if defined.
       </para>
       <screen>&prompt.ardana;zypper repos | grep '[|] Cloud '
 1 | Cloud                          | SUSE OpenStack Cloud 8 DVD #1  | Yes     | (r ) Yes  | No
ardana > sudo zypper removerepo Cloud
Removing repository 'SUSE OpenStack Cloud 8 DVD #1' ....................................................................[done]
Repository 'SUSE OpenStack Cloud 8 DVD #1' has been removed.
       </screen>
     </step>
     <step>
       <para>
         Ensure the deployer node has access to the SLE 12 SP4 and SOC 9 CLM
         repositories as documented in <xref linkend="cha-depl-repo-conf-lcm"/>
         paying attention to the non-SMT based repository setup. When you run
         <command>zypper repos --show-enabled-only</command>, the output should look similar to the following:
       </para>
       <screen>&prompt.ardana;zypper repos --show-enabled-only
#  | Alias                          | Name                           | Enabled | GPG Check | Refresh
---+--------------------------------+--------------------------------+---------+-----------+--------
 1 | Cloud                          | SUSE OpenStack Cloud 9 DVD #1  | Yes     | (r ) Yes  | No
 7 | SLES12-SP4-Pool                | SLES12-SP4-Pool                | Yes     | (r ) Yes  | No
 8 | SLES12-SP4-Updates             | SLES12-SP4-Updates             | Yes     | (r ) Yes  | Yes
 9 | SUSE-OpenStack-Cloud-9-Pool    | SUSE-OpenStack-Cloud-9-Pool    | Yes     | (r ) Yes  | No
10 | SUSE-OpenStack-Cloud-9-Updates | SUSE-OpenStack-Cloud-9-Updates | Yes     | (r ) Yes  | Yes
       </screen>
       <note>
         <para>
           The Cloud repository above is optional. Its content is equivalent to
           the SUSE-Openstack-Cloud-9-Pool repository.
         </para>
       </note>
     </step>
     <step>
       <para>
         Run the <command>zypper dist-upgrade</command> command to upgrade the
         deployer node:
       </para>
       <screen>&prompt.ardana;sudo zypper dist-upgrade

Warning: You are about to do a distribution upgrade with all enabled repositories. Make sure these repositories are compatible before you continue. See 'man zypper' for more information about this command.
Loading repository data...
Reading installed packages...
Computing distribution upgrade...

...

525 packages to upgrade, 14 to downgrade, 62 new, 5 to remove, 1 to change vendor, 1 to change arch.
Overall download size: 1.24 GiB. Already cached: 0 B. After the operation, additional 780.8 MiB will be used.
Continue? [y/n/...? shows all options] (y): y
...
    dracut: *** Generating early-microcode cpio image ***
    dracut: *** Constructing GenuineIntel.bin ****
    dracut: *** Store current command line parameters ***
    dracut: Stored kernel commandline:
    dracut:  rd.lvm.lv=ardana-vg/root
    dracut:  root=/dev/mapper/ardana--vg-root rootfstype=ext4 rootflags=rw,relatime,data=ordered
    dracut: *** Creating image file '/boot/initrd-4.4.180-94.127-default' ***
    dracut: *** Creating initramfs image file '/boot/initrd-4.4.180-94.127-default' done ***

Output of btrfsmaintenance-0.2-18.1.noarch.rpm %posttrans script:
    Refresh script btrfs-scrub.sh for monthly
    Refresh script btrfs-defrag.sh for none
    Refresh script btrfs-balance.sh for weekly
    Refresh script btrfs-trim.sh for none

There are some running programs that might use files deleted by recent upgrade. You may wish to check and restart some of them. Run 'zypper ps -s' to list these programs.
       </screen>
       <note>
         <para>
           You may need to run the <command>zypper dist-upgrade</command> command more than
           once if it determines it needs to update the zypper command
           infrastructure on your system to be able to successfully
           dist-upgrade the node; the command will tell you if you need to run it again.
         </para>
       </note>
     </step>
   </procedure>
 </section>


  <section xml:id="upgrade-deployer-node-config">
    <title>Upgrading the Deployer Node Configuration Settings</title>
    <para>
      Now that the deployer node packages have been migrated to SLE 12 SP4 and
      &productname; &productnumber; &clm;, we need to update the configuration
      settings to be &productname; &productnumber; &clm; based.
    </para>
    <para>
      The first step is to run the <command>ardana-init</command> command. This will:
    </para>
    <itemizedlist>
      <listitem>
        <para>
          Add the PTF repository, creating it if needed.
        </para>
      </listitem>
      <listitem>
        <para>
          Optionally add appropriate local repository references for any
          SMT-provided SLE 12 SP4 and SOC 9 repositories.
        </para>
      </listitem>
      <listitem>
        <para>
          Upgrade the deployer account <literal>~/openstack</literal> area to be
          based upon &productname; &productnumber; &clm; Ansible sources.
        </para>
        <itemizedlist>
          <listitem>
            <para>
              This will import the new &productname; &productnumber; &clm; Ansible code into the Git
              repository on the ardana branch and the rebase the customer
              site branch on top of the updated ardana branch.
            </para>
          </listitem>
          <listitem>
            <para>
              Follow the directions to resolve any Git merge conflicts that may
              arise due to local changes that may have been made on the site branch:
            </para>
            <screen>&prompt.ardana;ardana-init
 To continue installation copy your cloud layout to:
     /var/lib/ardana//openstack/my_cloud/definition

 Then execute the installation playbooks:
     cd /var/lib/ardana//openstack/ardana/ansible
     git add -A
     git commit -m 'My config'
     ansible-playbook -i hosts/localhost cobbler-deploy.yml
     ansible-playbook -i hosts/localhost bm-reimage.yml
     ansible-playbook -i hosts/localhost config-processor-run.yml
     ansible-playbook -i hosts/localhost ready-deployment.yml
     cd /var/lib/ardana//scratch/ansible/next/ardana/ansible
     ansible-playbook -i hosts/verb_hosts site.yml

 If you prefer to use the UI to install the product, you can
 do either of the following:
     - If you are running a browser on this machine, you can point
       your browser to http://localhost:9085 to start the install
       via the UI.
     - If you are running the browser on a remote machine, you will
       need to create an ssh tunnel to access the UI.  Please refer
       to the Ardana installation documentation for further details.
            </screen>
          </listitem>
        </itemizedlist>
      </listitem>
    </itemizedlist>
    <note>
      <para>
        As we are upgrading to &productname; &productnumber; &clm;, we do not need
        to run the suggested <filename>bm-reimage.yml</filename> playbook.
      </para>
    </note>

    <procedure>
      <title>Updating the Bare-Metal Provisioning Configuration</title>
      <para>
        If you were previously using the cobbler-based integrated provisioning
        solution, then you will need to perform the following steps to import
        the SLE 12 SP4 ISO and update the default provisioning distribution:
      </para>
      <step>
        <para>
          Ensure there is a copy of the <filename>SLE-12-SP4-Server-DVD-x86_64-GM-DVD1.iso</filename>,
          named <filename>sles12sp4.iso</filename>, available in the <filename>/var/lib/ardana</filename> directory.
        </para>
      </step>
      <step>
        <para>
          Ensure that any distro entries in <filename>servers.yml</filename>
          (or whichever file holds the server node definitions) under
          <filename>~ardana/openstack/my_cloud/definition</filename> are updated to
          reference sles12sp4 if they are currently sles12sp3.
        </para>
        <note>
          <para>
            The default distribution will now be sles12sp4, so if there are no
            specific distro entries specified for the servers, then no change
            will be required.
          </para>
        </note>
        <para>
          If you have made any changes to the <filename>~ardana/openstack/my_cloud/definition</filename>
          files, you will need to commit those changes, as follows:
        </para>
        <screen>&prompt.ardana;cd ~/openstack/my_cloud/definition
 &prompt.ardana;git add -A
 &prompt.ardana;git commit -m "Update sles12sp3 distro entries to sles12sp4"
        </screen>
      </step>
      <step>
        <para>
          Run the <filename>cobbler-deploy.yml</filename> playbook to import
          the SLE 12 SP4 distribution as the new default distribution:
        </para>
        <screen>&prompt.ardana;cd ~/openstack/ardana/ansible
 &prompt.ardana;ansible-playbook -i hosts/localhost cobbler-deploy.yml
 Enter the password that will be used to access provisioned nodes:
 confirm Enter the password that will be used to access provisioned nodes:

 PLAY [localhost] **************************************************************

 GATHERING FACTS ***************************************************************
 ok: [localhost]

 TASK: [pbstart.yml pb_start_playbook] *****************************************
 ok: [localhost] => {
     "msg": "Playbook started - cobbler-deploy.yml"
 }

 msg: Playbook started - cobbler-deploy.yml

 ...

 PLAY [localhost] **************************************************************

 TASK: [pbfinish.yml pb_finish_playbook] ***************************************
 ok: [localhost] => {
     "msg": "Playbook finished - cobbler-deploy.yml"
 }

 msg: Playbook finished - cobbler-deploy.yml

 PLAY RECAP ********************************************************************
 localhost                  : ok=92   changed=45   unreachable=0    failed=0
        </screen>
      </step>
    </procedure>
    <para>
      You are now ready to upgrade the input model to be compatible.
    </para>

    <procedure>
      <title>Upgrading the Cloud Input Model</title>
      <para>
        At this point, there are some mandatory changes that will need to be made
        to the existing input model to permit the upgrade proceed. These mandatory
        changes represent:
      </para>
      <itemizedlist>
        <listitem>
          <para>
            the removal of previously-deprecated service components
          </para>
        </listitem>
        <listitem>
          <para>
            the dropping of service components that are no longer supported
          </para>
        </listitem>
        <listitem>
          <para>
            there can be only one untagged VLAN per network interface
          </para>
        </listitem>
        <listitem>
          <para>
            there must be a <literal>MANAGEMENT</literal> network group
          </para>
        </listitem>
      </itemizedlist>
      <para>
        There are also some service components that have been made redundant
        and have no effect. These should be removed to quieten the associated
        <filename>config-processor-run.yml</filename> warnings.
      </para>
      <para>
        For example, if you run the <filename>configuration-processor-run.yml</filename>
        playbook from the <filename>~ardana/openstack/ardana/ansible</filename>
        directory before you made the necessary input model changes, you should
        see it fail with errors similar to those shown below, unless your input
        model doesn't deploy the problematic service component:
      </para>
      <screen>&prompt.ardana;cd ~/openstack/ardana/ansible
 &prompt.ardana;ansible-playbook -i hosts/localhost config-processor-run.yml
 Enter encryption key (press return for none):
 confirm Enter encryption key (press return for none):
 To change encryption key enter new key (press return for none):
 confirm To change encryption key enter new key (press return for none):

 PLAY [localhost] **************************************************************

 GATHERING FACTS ***************************************************************
 ok: [localhost]

 ...

             "################################################################################",
             "# The configuration processor failed.  ",
             "#   control-planes-2.0        WRN: cp:openstack-core: 'designate-pool-manager' has been deprecated and will be replaced by 'designate-worker'. The replacement component will be automatically deployed in a future release. You will then need to update the input model to remove this warning.",
             "",
             "#   control-planes-2.0        WRN: cp:openstack-core: 'manila-share' service component is deprecated. The 'manila-share' service component can be removed as manila share service will be deployed where manila-api is specified. This is not a deprecation for openstack-manila-share but just an entry deprecation in input model.",
             "",
             "#   control-planes-2.0        WRN: cp:openstack-core: 'designate-zone-manager' has been deprecated and will be replaced by 'designate-producer'. The replacement component will be automatically deployed in a future release. You will then need to update the input model to remove this warning.",
             "",
             "#   control-planes-2.0        WRN: cp:openstack-core: 'glance-registry' has been deprectated and is no longer deployed. Please update you input model to remove any 'glance-registry' service component specifications to remove this warning.",
             "",
             "#   control-planes-2.0        WRN: cp:mml: 'ceilometer-api' is no longer used by Ardana and will not be deployed. Please update your input model to remove this warning.",
             "",
             "#   control-planes-2.0        WRN: cp:sles-compute: 'neutron-lbaasv2-agent' has been deprecated and replaced by 'octavia' and will not be deployed in a future release. Please update your input model to remove this warning.",
             "",
             "#   control-planes-2.0        ERR: cp:common-service-components: Undefined component 'freezer-agent'",
             "#   control-planes-2.0        ERR: cp:openstack-core: Undefined component 'nova-console-auth'",
             "#   control-planes-2.0        ERR: cp:openstack-core: Undefined component 'heat-api-cloudwatch'",
             "#   control-planes-2.0        ERR: cp:mml: Undefined component 'freezer-api'",
             "################################################################################"
         ]
     }
 }

 TASK: [debug var=config_processor_result.stderr] ******************************
 ok: [localhost] => {
     "var": {
         "config_processor_result.stderr": "/usr/lib/python2.7/site-packages/ardana_configurationprocessor/cp/model/YamlConfigFile.py:95: YAMLLoadWarning: calling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.\n  self._contents = yaml.load(''.join(lines))"
     }
 }

 TASK: [fail msg="Configuration processor run failed, see log output above for details"] ***
 failed: [localhost] => {"failed": true}
 msg: Configuration processor run failed, see log output above for details

 msg: Configuration processor run failed, see log output above for details

 FATAL: all hosts have already failed -- aborting

 PLAY RECAP ********************************************************************
            to retry, use: --limit @/var/lib/ardana/config-processor-run.retry

 localhost                  : ok=8    changed=5    unreachable=0    failed=1
      </screen>
      <para>
        To resolve any errors and warnings like those shown above, you will
        need to perform the following actions:
      </para>
      <step>
        <para>
          Remove any service component entries that are no longer valid from the
          <filename>control_plane.yml</filename> (or whichever file hold the
          control plane definitions) under <filename>~ardana/openstack/my_cloud/definition</filename>.
          This means that you have to comment out (or delete) any lines for the
          following service components, which are no longer available:
        </para>
        <itemizedlist>
          <listitem>
            <para>
              freezer-agent
            </para>
          </listitem>
          <listitem>
            <para>
              freezer-api
            </para>
          </listitem>
          <listitem>
            <para>
              heat-api-cloudwatch
            </para>
          </listitem>
          <listitem>
            <para>
              nova-console-auth
            </para>
          </listitem>
        </itemizedlist>
        <note>
          <para>
            This should resolve the errors that cause the <filename>config-processor-run.yml</filename>
            playbook to fail.
          </para>
        </note>
      </step>
      <step>
        <para>
          Similarly, remove any service components that are redundant and no
          longer required. This means that you should comment out (or delete)
          any lines for the following service components:
        </para>
        <itemizedlist>
          <listitem>
            <para>
              ceilometer-api
            </para>
          </listitem>
          <listitem>
            <para>
              glance-registry
            </para>
          </listitem>
          <listitem>
            <para>
              manila-share
            </para>
          </listitem>
          <listitem>
            <para>
              neuton-lbaasv2-agent
            </para>
          </listitem>
        </itemizedlist>
        <note>
          <para>
            This should resolve most of the warnings reported by the
            <filename>config-processor-run.yml</filename> playbook.
          </para>
        </note>
        <para>
          If you have deployed the designate service components
          (designate-pool-manager and designate-zone-manager) in your cloud,
          you will see warnings like those shown above, indicating that these
          service components have been deprecated.
        </para>
        <para>
          You can switch to using the newer designate-worker and designate-producer
          service components, which will quieten these deprecation warnings
          produced by the <filename>config-processor-run.yml</filename> playbook run.
        </para>
     </step>
     <step>
      <para>
        Once you have made the necessary changes to your input model, if
        you run <command>git diff</command> under the <filename>~ardana/openstack/my_cloud/definition</filename>
        you should see output similar to the following:
      </para>
      <screen>&prompt.ardana;cd ~/openstack/my_cloud/definition
 &prompt.ardana;git diff
 diff --git a/my_cloud/definition/data/control_plane.yml b/my_cloud/definition/data/control_plane.yml
 index f7cfd84..2c1a73c 100644
 --- a/my_cloud/definition/data/control_plane.yml
 +++ b/my_cloud/definition/data/control_plane.yml
 @@ -32,7 +32,6 @@
          - NEUTRON-CONFIG-CP1
        common-service-components:
          - lifecycle-manager-target
 -        - freezer-agent
          - stunnel
          - monasca-agent
          - logging-rotate
 @@ -118,12 +117,10 @@
              - cinder-volume
              - cinder-backup
              - glance-api
 -            - glance-registry
              - nova-api
              - nova-placement-api
              - nova-scheduler
              - nova-conductor
 -            - nova-console-auth
              - nova-novncproxy
              - neutron-server
              - neutron-ml2-plugin
 @@ -137,7 +134,6 @@
              - horizon
              - heat-api
              - heat-api-cfn
 -            - heat-api-cloudwatch
              - heat-engine
              - ops-console-web
              - barbican-api
 @@ -151,7 +147,6 @@
              - magnum-api
              - magnum-conductor
              - manila-api
 -            - manila-share

          - name: mml
            cluster-prefix: mml
 @@ -164,9 +159,7 @@

              # freezer-api shares elastic-search with logging-server
              # so must be co-located with it
 -            - freezer-api

 -            - ceilometer-api
              - ceilometer-polling
              - ceilometer-agent-notification
              - ceilometer-common
 @@ -194,4 +187,3 @@
              - neutron-l3-agent
              - neutron-metadata-agent
              - neutron-openvswitch-agent
 -            - neutron-lbaasv2-agent
      </screen>
    </step>
    <step>
      <para>
        If you are happy with these changes, commit them into
        the Git repository as follows:
      </para>
      <screen>&prompt.ardana;cd ~/openstack/my_cloud/definition
 &prompt.ardana;git add -A
 &prompt.ardana;git commit -m "SOC 9 CLM Upgrade input model migration"
      </screen>
    </step>
    <step>
      <para>
        Now you ready to run the <filename>config-processor-run.yml</filename> playbook. If the
        necessary input model changes have been made, it will complete
        sucessfully:
      </para>
      <screen>&prompt.ardana;cd ~/openstack/ardana/ansible
 &prompt.ardana;ansible-playbook -i hosts/localhost config-processor-run.yml
 Enter encryption key (press return for none):
 confirm Enter encryption key (press return for none):
 To change encryption key enter new key (press return for none):
 confirm To change encryption key enter new key (press return for none):

 PLAY [localhost] **************************************************************

 GATHERING FACTS ***************************************************************
 ok: [localhost]

 ...
 PLAY RECAP ********************************************************************
 localhost                  : ok=24   changed=20   unreachable=0    failed=0
     </screen>
   </step>
 </procedure>
 </section>

 <section xml:id="upgrade-cloud-services">
   <title>Upgrading Cloud Services</title>
   <para>
     The deployer node is now ready to be used to upgrade the remaining
     cloud nodes and running services.
   </para>
   <note>
     <para>
       The <filename>ardana-upgrade.yml</filename> playbook runs the upgrade process
       against all nodes in parallel, though some of the steps are serialised
       to run on only one node at a time to avoid triggering potentially
       problematic race conditions. As such, the playbook can take a long time to run.
     </para>
   </note>

   <procedure>
     <title>Generate the Based Scratch Area</title>
     <step>
       <para>
        Generate the updated scratch area using the SOC 9 CLM Ansible sources:
       </para>
       <screen>&prompt.ardana;cd ~/openstack/ardana/ansible
&prompt.ardana;ansible-playbook -i hosts/localhost ready-deployment.yml

PLAY [localhost] **************************************************************

GATHERING FACTS ***************************************************************
ok: [localhost]

...

PLAY RECAP ********************************************************************
localhost                  : ok=31   changed=16   unreachable=0    failed=0
       </screen>
     </step>
     <step>
       <para>
         Confirm that there are no pending updates for the deployer node.
         This could happen if you are using an SMT to manage the repositories,
         and updates have been released through the official channels since
         the deployer node was mirgated. To check for any pending &clm; package
         updates, you can run the <filename>ardana-update-pkgs.yml</filename> playbook as follows:
       </para>
       <screen>&prompt.ardana;ansible-playbook -i hosts/verb_hosts ardana-update-pkgs.yml --limit OPS-LM--first-member

PLAY [all] ********************************************************************

TASK: [setup] *****************************************************************
ok: [ardana-cp-dplyr-m1]

PLAY [localhost] **************************************************************

TASK: [pbstart.yml pb_start_playbook] *****************************************
ok: [localhost] => {
    "msg": "Playbook started - ardana-update-pkgs.yml"
}

...

TASK: [_ardana-update-status | Report update status] **************************
ok: [ardana-cp-dplyr-m1] => {
    "msg": "=====================================================================\nUpdate status for node ardana-cp-dplyr-m1:\n=====================================================================\nNo pending update actions on the ardana-cp-dplyr-m1 host\nwere collected or reset during this update run or persisted during\nprevious unsuccessful or incomplete update runs.\n\n====================================================================="
}

msg: =====================================================================
Update status for node ardana-cp-dplyr-m1:
=====================================================================
No pending update actions on the ardana-cp-dplyr-m1 host
were collected or reset during this update run or persisted during
previous unsuccessful or incomplete update runs.

=====================================================================

PLAY [localhost] **************************************************************

TASK: [pbfinish.yml pb_finish_playbook] ***************************************
ok: [localhost] => {
    "msg": "Playbook finished - ardana-update-pkgs.yml"
}

msg: Playbook finished - ardana-update-pkgs.yml

PLAY RECAP ********************************************************************
ardana-cp-dplyr-m1         : ok=98   changed=12   unreachable=0    failed=0
localhost                  : ok=6    changed=2    unreachable=0    failed=0
       </screen>
       <note>
         <para>
           If running the <filename>ardana-update-pkgs.yml</filename> playbook identifies that there
           were updates that needed to be installed on your deployer, then you
           need to go back to running the <command>ardana-init</command> command, followed by the
           <filename>cobbler-deploy.yml</filename> playbook, then the <filename>config-processor-run.yml</filename>
           playbook, and finally the <filename>ready-deployment.yml</filename> playbook, addressing
           any additional input model changes that may be needed if any. Then,
           repeat this step to check for any pending updates before
           continuing with the upgrade.
         </para>
       </note>
     </step>
     <step>
       <para>
         Double-check that there are no pending actions needed for the deployer node
         by running the <filename>ardana-update-status.yml</filename> playbook, as follows:
       </para>
       <screen>&prompt.ardana;ansible-playbook -i hosts/verb_hosts ardana-update-status.yml --limit OPS-LM--first-member

PLAY [resources] **************************************************************

...

TASK: [_ardana-update-status | Report update status] **************************
ok: [ardana-cp-dplyr-m1] => {
    "msg": "=====================================================================\nUpdate status for node ardana-cp-dplyr-m1:\n=====================================================================\nNo pending update actions on the ardana-cp-dplyr-m1 host\nwere collected or reset during this update run or persisted during\nprevious unsuccessful or incomplete update runs.\n\n====================================================================="
}

msg: =====================================================================
Update status for node ardana-cp-dplyr-m1:
=====================================================================
No pending update actions on the ardana-cp-dplyr-m1 host
were collected or reset during this update run or persisted during
previous unsuccessful or incomplete update runs.

=====================================================================

PLAY RECAP ********************************************************************
ardana-cp-dplyr-m1         : ok=12   changed=0    unreachable=0    failed=0
       </screen>
     </step>
     <step>
       <para>
         Having verified that there are no pending actions detected, it is
         safe to proceed with running the <filename>ardana-upgrade.yml</filename>
         playbook to upgrade the entire cloud:
       </para>
       <screen>&prompt.ardana;cd ~/scratch/ansible/next/ardana/ansible
&prompt.ardana;ansible-playbook -i hosts/verb_hosts ardana-upgrade.yml
PLAY [all] ********************************************************************

...

TASK: [pbstart.yml pb_start_playbook] *****************************************
ok: [localhost] => {
    "msg": "Playbook started - ardana-upgrade.yml"
}

msg: Playbook started - ardana-upgrade.yml

...
...
...
...
...

TASK: [pbfinish.yml pb_finish_playbook] ***************************************
ok: [localhost] => {
    "msg": "Playbook finished - ardana-upgrade.yml"
}

msg: Playbook finished - ardana-upgrade.yml
       </screen>
     </step>
     </procedure>
     <para>
       The <filename>ardana-upgrade.yml</filename> playbook run will take a long time. The
       <command>zypper dist-upgrade</command> phase is seialised across all of
       the nodes and usually takes between 5 and 10 minutes for each node. This
       is followed by the cloud service upgrade phase, which will take
       approximately the same amount of time as a full cloud deploy. During
       this time, the cloud should remain basically functional, though there
       may be brief interruptions to some services. However, it is recommended
       that any workload management tasks are avoided during this period.
     </para>
     <note>
       <para>
         Until the <filename>ardana-upgrade.yml</filename> playbook run has
         ompleted successfully, other playbooks such as the <filename>ardana-status.yml</filename>,
         may report status problems. This is because some services that are
         expected to be running, may not be installed, enabled, or migrated yet.
       </para>
     </note>
     <para>
       The <filename>ardana-upgrade.yml</filename> playbook run may sometimes
       fail during the whole cloud upgrade phase, if a service (for example,
       the monasca-thresh service) is slow to restart. In such cases, it is
       safe to run the <filename>ardana-upgrade.yml</filename> playbook again,
       and in most cases it should continue past the stage that failed
       previously. However, if the same problem persists across multiple runs,
       contact your support team for assistance.
     </para>
     <important>
       <para>
         It is important to disable all SLE 12 SP3 &productname; 8 &clm;
         repositories before migrating the deployer to SLE 12 SP4 &productname;
         &productnumber; &clm;. If you did not do this, then the first time you
         run the <filename>ardana-upgrade.yml</filename> playbook, it may
         complain that there are pending updates for the deployer node.
         This will require you to repeat the earlier steps to upgrade th
          deployer node, starting with running the <command>ardana-init</command>
          command. If this happens, repeat the steps as requested. Note that this
          does not represent a serious problem.
       </para>
     </important>

   <para>
     In &productname; &productnumber; &clm; the LBaaS V2 legacy driver has been
     deprecated and removed. As part of the <filename>ardana-upgrade.yml</filename> playbook run,
     all existing LBaaS V2 loadbalancers will be automatically migrated to being
     based on the Octavia Amphora provider. To enable creation of any new Octavia
     based loadbalancer instances, you need to ensure that an appropriate Amphora
     image is registered for use when creating instances, by following
     <xref linkend="OctaviaInstall"/>.
   </para>
   <note>
     <para>
       While running the ardana-upgrade.yml playbook, a point will be
       reached when the Neutron services are upgraded. As part of this upgrade,
       any existing LBaaS V2 load balancer definitions will be migrated to
       Octavia Amphora-based load balancer definitions.
     </para>
     <para>
       After this migration of load balancer definitions has completed,
       if a load balancer failover is triggered, then the replacement load
       balancer may fail to start, as an appropriate Octavia Amphora image
       for SOC 9 CLM won't be available yet.
     </para>
     <para>
       However, once the Octavia Amphora image has been uploaded using the
       above instructions, then it will be possible to recover any failed
       load balancers by re-triggering the failover: follow the instructions at
       <link xlink:href="https://docs.openstack.org/python-octaviaclient/latest/cli/index.html#loadbalancer-failover"/>.
     </para>
   </note>
 </section>

 <section xml:id="upgrade-reboot-nodes-kernel">
   <title>Rebooting the Nodes into the SLE 12 SP4 Kernel</title>
   <para>
     At this point, all of the cloud services have been upgraded, but
     the nodes are still running the SLE 12 SP3 kernel. The final step in
     the upgrade workflow is to reboot all of the nodes in the
     cloud in a controlled fashion, to ensure that active services failover
     appropriately.
   </para>
   <para>
     The recommended order for rebooting nodes is to start with the deployer.
     This requires special handling, since the Ansible-based automation cannot
     fully manage the reboot of the node that it is running on.
   </para>
   <para>
     After that, we recommend rebooting the rest of the nodes in the control
     planes in a rolling-reboot fashion, ensuring that high-availability services
     remain available.
   </para>
   <para>
     Finally, the compute nodes can be rebooted, either individually or
     in groups, as is appropriate to avoid interruptions to running workloads.
   </para>
   <warning>
     <para>
       Do not reboot all your control plane nodes at the same time.
     </para>
   </warning>

   <procedure>
     <title>Rebooting the Deployer Node</title>
     <para>
       The reboot of the deployer node requires additional steps, as the
       Ansible-based automation framework cannot fully automate the reboot of
       the node that runs the ansible-playbook commands.
     </para>
     <step>
       <para>
         Run the <filename>ardana-reboot.yml</filename> playbook limited to the
         deployer node, either by name, or using the logical node identified
         <literal>OPS-LM--first-member</literal>, as follows:
       </para>
       <screen>&prompt.ardana;cd ~/scratch/ansible/next/ardana/ansible
&prompt.ardana;ansible-playbook -i hosts/verb_hosts ardana-reboot.yml --limit OPS-LM--first-member

PLAY [all] ********************************************************************

TASK: [setup] *****************************************************************
ok: [ardana-cp-dplyr-m1]

PLAY [localhost] **************************************************************

TASK: [pbstart.yml pb_start_playbook] *****************************************
ok: [localhost] => {
    "msg": "Playbook started - ardana-reboot.yml"
}

msg: Playbook started - ardana-reboot.yml

...

TASK: [ardana-reboot | Deployer node has to be rebooted manually] *************
failed: [ardana-cp-dplyr-m1] => {"failed": true}
msg: The deployer node needs to be rebooted manually. After reboot, resume by running the post-reboot playbook:
cd ~/scratch/ansible/next/ardana/ansible ansible-playbook -i hosts/verb_hosts _ardana-post-reboot.yml --limit ardana-cp-dplyr-m1

msg: The deployer node needs to be rebooted manually. After reboot, resume by running the post-reboot playbook:
cd ~/scratch/ansible/next/ardana/ansible ansible-playbook -i hosts/verb_hosts _ardana-post-reboot.yml --limit ardana-cp-dplyr-m1

FATAL: all hosts have already failed -- aborting

PLAY RECAP ********************************************************************
           to retry, use: --limit @/var/lib/ardana/ardana-reboot.retry

ardana-cp-dplyr-m1         : ok=8    changed=3    unreachable=0    failed=1
localhost                  : ok=7    changed=0    unreachable=0    failed=0
       </screen>
       <para>
         The <filename>ardana-reboot.yml</filename> playbook will fail when run
         on a deployer node; this is expected. The reported failure message
         tells you what you need to do to complete the remaining steps of the
         reboot manually: namely, rebooting the node, then logging back in again
         to run the <filename>_ardana-post-reboot.yml</filename> playbook, to start any services
         that need to be running on the node.
       </para>
     </step>
     <step>
       <para>
         Manually reboot the deployer node, for example with <command>shutdown -r now</command>.
       </para>
     </step>
     <step>
       <para>
         Once the deployer node has rebooted, you need to log in again and run
         the <filename>_ardana-post-reboot.yml</filename> playbook to complete
         the startup of any services that should be running on the deployer node, as follows:
       </para>
       <screen>&prompt.ardana;cd ~/scratch/ansible/next/ardana/ansible
&prompt.ardana;ansible-playbook _ardana-post-reboot.yml --limit OPS-LM--first-member

PLAY [resources] **************************************************************

TASK: [Set pending_clm_update] ************************************************
skipping: [ardana-cp-dplyr-m1]

...

TASK: [pbfinish.yml pb_finish_playbook] ***************************************
ok: [localhost] => {
    "msg": "Playbook finished - ardana-status.yml"
}

msg: Playbook finished - ardana-status.yml

PLAY RECAP ********************************************************************
ardana-cp-dplyr-m1         : ok=26   changed=0    unreachable=0    failed=0
localhost                  : ok=19   changed=1    unreachable=0    failed=0
</screen>
     </step>
   </procedure>

   <procedure>
     <title>Rebooting the Remaining Control Plane Nodes</title>
     <para>
       For the remaining nodes, you can use <filename>ardana-reboot.yml</filename>
       to fully automate the reboot process. However, it is recommended that you
       reboot the nodes in a rolling-reboot fashion, such that high-availability
       services continue to run without interruption. Similarly, to avoid
       interruption of service for any singleton services (such as the cinder-volume
       and cinder-backup services), they should be migrated off the intended
       node before it is rebooted, and then migrated back again afterwards.
     </para>
     <step>
       <para>
         Use ansible's command's <option>--list-hosts</option> option to
         list the remaining nodes in the cloud that are neither the deployer
         nor a compute node:
       </para>
       <screen>&prompt.ardana;cd ~/scratch/ansible/next/ardana/ansible
&prompt.ardana;ansible -i hosts/verb_hosts --list-hosts 'resources:!OPS-LM--first-member:!NOV-CMP'
    ardana-cp-dbmqsw-m1
    ardana-cp-dbmqsw-m2
    ardana-cp-dbmqsw-m3
    ardana-cp-osc-m1
    ardana-cp-osc-m2
    ardana-cp-mml-m1
    ardana-cp-mml-m2
    ardana-cp-mml-m3
  </screen>
     </step>
     <step>
       <para>
         Use the following command to generate the set of ansible-playbook
         commands that need to be run to reboot all the nodes sequentially:
       </para>
       <screen>&prompt.ardana;cd ~/scratch/ansible/next/ardana/ansible
&prompt.ardana;for node in $(ansible -i hosts/verb_hosts --list-hosts 'resources:!OPS-LM--first-member:!NOV-CMP'); do echo ansible-playbook -i hosts/verb_hosts ardana-reboot.yml --limit ${node} || break; done
ansible-playbook -i hosts/verb_hosts ardana-reboot.yml --limit ardana-cp-dbmqsw-m1
ansible-playbook -i hosts/verb_hosts ardana-reboot.yml --limit ardana-cp-dbmqsw-m2
ansible-playbook -i hosts/verb_hosts ardana-reboot.yml --limit ardana-cp-dbmqsw-m3
ansible-playbook -i hosts/verb_hosts ardana-reboot.yml --limit ardana-cp-osc-m1
ansible-playbook -i hosts/verb_hosts ardana-reboot.yml --limit ardana-cp-osc-m2
ansible-playbook -i hosts/verb_hosts ardana-reboot.yml --limit ardana-cp-mml-m1
ansible-playbook -i hosts/verb_hosts ardana-reboot.yml --limit ardana-cp-mml-m2
ansible-playbook -i hosts/verb_hosts ardana-reboot.yml --limit ardana-cp-mml-m3
</screen>
      <warning>
        <para>
          Do not reboot all your control plane nodes at the same time.
        </para>
      </warning>
     </step>
     <step>
       <para>
         To reboot a specific control plane node, you can use the above
         ansible-playbook commands as follows:
       </para>
       <screen>&prompt.ardana;cd ~/scratch/ansible/next/ardana/ansible
&prompt.ardana;ansible-playbook -i hosts/verb_hosts ardana-reboot.yml --limit ardana-cp-mml-m3

PLAY [all] ********************************************************************

TASK: [setup] *****************************************************************
ok: [ardana-cp-mml-m3]

PLAY [localhost] **************************************************************

TASK: [pbstart.yml pb_start_playbook] *****************************************
ok: [localhost] => {
    "msg": "Playbook started - ardana-reboot.yml"
}

msg: Playbook started - ardana-reboot.yml



...

PLAY [localhost] **************************************************************

TASK: [pbfinish.yml pb_finish_playbook] ***************************************
ok: [localhost] => {
    "msg": "Playbook finished - ardana-status.yml"
}

msg: Playbook finished - ardana-status.yml

PLAY [localhost] **************************************************************

TASK: [pbfinish.yml pb_finish_playbook] ***************************************
ok: [localhost] => {
    "msg": "Playbook finished - ardana-reboot.yml"
}

msg: Playbook finished - ardana-reboot.yml

PLAY RECAP ********************************************************************
ardana-cp-mml-m3           : ok=389  changed=105  unreachable=0    failed=0
localhost                  : ok=27   changed=1    unreachable=0    failed=0
</screen>
     </step>
   </procedure>
   <note>
     <para>
       You can reboot more than one control plane node at a time, but only if
       they are members of different control plane clusters. For example, you
       could reboot one node out of each of the Openstack controller, database,
       Swift, monitoring or logging clusters, so long as doing do only reboots
       one node out of each cluster at the same time.
     </para>
   </note>

   <para>
     When rebooting the first member of the control plane cluster where
     monitoring services run, the <literal>monasca-thresh</literal> service can sometimes fail
     to start up in a timely fashion when the node is coming back up after
     being rebooted. This can cause <filename>ardana-reboot.yml</filename> to fail.
     See below for suggestions on how to handle this problem.
   </para>

   <procedure>
     <title>Getting <literal>monasca-thresh</literal> Running After an <filename>ardana-reboot.yml</filename> Failure</title>
       <para>
         If the <filename>ardana-reboot.yml</filename> playbook failed because
         <literal>monasca-thresh</literal> didn't start up in a timely fashion
         after a reboot, you can retry starting the services on the node using
         the <filename>_ardana-post-reboot.yml</filename> playbook for the node.
         This is similar to the manual handling of the deployer reboot, since
         the node has already successfully rebooted onto the new kernel, and
         you just need to get the required services running again on the node.
       </para>
       <para>
         It can sometimes take up to 15 minutes for the <literal>monasca-thresh</literal>
         service to successfully start in such cases.
       </para>
     <step>
       <para>
         However if the service still fails to start after than time, then
         you may need to force a restart of the <literal>storm-nimbus</literal> and
         <literal>storm-supervisor</literal> services on all nodes in the
         <literal>MON-THR</literal> node group, as follows:
       </para>
       <screen>&prompt.ardana;cd ~/scratch/ansible/next/ardana/ansible
&prompt.ardana;ansible MON-THR -b -m shell -a "systemctl restart storm-nimbus"
ardana-cp-mml-m1 | success | rc=0 >>


ardana-cp-mml-m2 | success | rc=0 >>


ardana-cp-mml-m3 | success | rc=0 >>


ardana > ansible MON-THR -b -m shell -a "systemctl restart storm-supervisor"
ardana-cp-mml-m1 | success | rc=0 >>


ardana-cp-mml-m2 | success | rc=0 >>


ardana-cp-mml-m3 | success | rc=0 >>


ardana > ansible-playbook -i hosts/verb_hosts _ardana-post-reboot.yml --limit ardana-cp-mml-m1
</screen>
     </step>
   </procedure>
   <para>
     If the <literal>monasca-thresh</literal> service still fails to start up,
     contact your support team.
   </para>

   <para>
     To check which control plane nodes have not yet been rebooted onto
     the new kernel, you can use an ansible command to run the command <command>uname -r</command>
     on the target nodes, as follows:
   </para>
   <screen>&prompt.ardana;cd ~/scratch/ansible/next/ardana/ansible
&prompt.ardana;ansible -i hosts/verb_hosts 'resources:!OPS-LM--first-member:!NOV-CMP' -m command -a 'uname -r'
ardana-cp-dbmqsw-m1 | success | rc=0 >>
4.12.14-95.57-default

ardana-cp-dbmqsw-m3 | success | rc=0 >>
4.12.14-95.57-default

ardana-cp-osc-m1 | success | rc=0 >>
4.12.14-95.57-default

ardana-cp-dbmqsw-m2 | success | rc=0 >>
4.12.14-95.57-default

ardana-cp-mml-m2 | success | rc=0 >>
4.12.14-95.57-default

ardana-cp-osc-m2 | success | rc=0 >>
4.12.14-95.57-default

ardana-cp-mml-m1 | success | rc=0 >>
4.12.14-95.57-default

ardana-cp-mml-m3 | success | rc=0 >>
4.12.14-95.57-default

ardana > uname -r
4.12.14-95.57-default
   </screen>
   <para>
     If any node's <command>uname -r</command> value does not match the kernel
     that the deployer is running, you probably have not yet rebooted that node.
   </para>

   <procedure>
     <title>Rebooting the Compute Nodes</title>
     <para>
       Finally, you need to reboot the compute nodes. Rebooting multiple
       compute nodes at the same time is possible, so long as doing so does
       not compromise the integrity of running workloads. We recommended
       that you evacuate groups of compute nodes in a controlled fashion,
       enabling them to be rebooted together.
     </para>
     <warning>
       <para>
         Do not reboot all of your compute nodes at the same time.
       </para>
     </warning>
     <step>
       <para>
         To see all the compute nodes that are available to be rebooted, you
         can run the following command:
       </para>
       <screen>&prompt.ardana;cd ~/scratch/ansible/next/ardana/ansible
    &prompt.ardana;ansible -i hosts/verb_hosts --list-hosts NOV-CMP
    ardana-cp-slcomp0001
    ardana-cp-slcomp0002
...
    ardana-cp-slcomp0080
  </screen>
     </step>
     <step>
       <para>
         Reboot the compute nodes, individually or in groups, using the
         <filename>ardana-reboot.yml</filename> playbook as follows:
       </para>
       <screen>&prompt.ardana;cd ~/scratch/ansible/next/ardana/ansible
    &prompt.ardana;ansible-playbook -i hosts/verb_hosts ardana-reboot.yml --limit ardana-cp-slcomp0001,ardana-cp-slcomp0002

PLAY [all] ********************************************************************

TASK: [setup] *****************************************************************
ok: [ardana-cp-slcomp0001]
ok: [ardana-cp-slcomp0002]

PLAY [localhost] **************************************************************

TASK: [pbstart.yml pb_start_playbook] *****************************************
ok: [localhost] => {
    "msg": "Playbook started - ardana-reboot.yml"
}

msg: Playbook started - ardana-reboot.yml

...

PLAY [localhost] **************************************************************

TASK: [pbfinish.yml pb_finish_playbook] ***************************************
ok: [localhost] => {
    "msg": "Playbook finished - ardana-status.yml"
}

msg: Playbook finished - ardana-status.yml

PLAY [localhost] **************************************************************

TASK: [pbfinish.yml pb_finish_playbook] ***************************************
ok: [localhost] => {
    "msg": "Playbook finished - ardana-reboot.yml"
}

msg: Playbook finished - ardana-reboot.yml

PLAY RECAP ********************************************************************
ardana-cp-slcomp0001       : ok=120  changed=11   unreachable=0    failed=0
ardana-cp-slcomp0002       : ok=120  changed=11   unreachable=0    failed=0
localhost                  : ok=27   changed=1    unreachable=0    failed=0
  </screen>
     </step>
   </procedure>
   <important>
     <para>
       You must ensure that there is sufficient unused workload capacity to
       host any migrated workload or Amphora instances that may be running on
       the targeted compute nodes.
     </para>
     <para>
       When rebooting multiple compute nodes at the same time, consider
       manually migrating any running workloads and Amphora instances off the
       target nodes in advance, to avoid any potential risk of workload or
       service interruption.
     </para>
   </important>
 </section>

 <section xml:id="post-upgrade-tasks">
   <title>Post-Upgrade Tasks</title>
   <para>
     After the cloud has been upgraded to &productname; &productnumber; &clm;,
     if designate was previously configured, then the deprecated service
     components, <literal>designate-zone-manager</literal> and
     <literal>designate-pool-manager</literal>, were being used.
   </para>
   <para>
     They will continue to operate correctly under &productname; &productnumber; &clm;,
     but we recommend that you migrate to using the newer <literal>designate-worker</literal>
     <literal>designate-producer</literal> service components instead by
     following the procedure documented in <xref linkend="DNS-MIGRATE"/>.
   </para>

   <procedure>
     <title>Cleanup Orphaned Packages</title>
     <step>
       <para>
         After migrating the deployer node, there are a small number of
         packages that were installed that are no longer requiredâ€”such as
         the ceilometer and freezer virtualenv (venv) packages.
       </para>
       <para>
         Safely removed these packages with the following command:
       </para>
       <screen>&prompt.ardana;zypper packages --orphaned
Loading repository data...
Reading installed packages...
S | Repository | Name                             | Version                           | Arch
--+------------+----------------------------------+-----------------------------------+-------
i | @System    | python-flup                      | 1.0.3.dev_20110405-2.10.52        | noarch
i | @System    | python-happybase                 | 0.9-1.64                          | noarch
i | @System    | venv-openstack-ceilometer-x86_64 | 9.0.8~dev7-12.24.2                | noarch
i | @System    | venv-openstack-freezer-x86_64    | 5.0.0.0~xrc2~dev2-10.22.1         | noarch
ardana> sudo zypper remove venv-openstack-ceilometer-x86_64 venv-openstack-freezer-x86_64
Loading repository data...
Reading installed packages...
Resolving package dependencies...

The following 2 packages are going to be REMOVED:
  venv-openstack-ceilometer-x86_64 venv-openstack-freezer-x86_64

2 packages to remove.
After the operation, 79.0 MiB will be freed.
Continue? [y/n/...? shows all options] (y): y
(1/2) Removing venv-openstack-ceilometer-x86_64-9.0.8~dev7-12.24.2.noarch ..................................................................[done]
Additional rpm output:
/usr/lib/python2.7/site-packages/ardana_packager/indexer.py:148: YAMLLoadWarning: calling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.
  return yaml.load(f)


(2/2) Removing venv-openstack-freezer-x86_64-5.0.0.0~xrc2~dev2-10.22.1.noarch ..............................................................[done]
Additional rpm output:
/usr/lib/python2.7/site-packages/ardana_packager/indexer.py:148: YAMLLoadWarning: calling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.
  return yaml.load(f)
       </screen>
     </step>
   </procedure>

   <procedure>
     <title>Delete freezer Containers from the Swift Object Store</title>
     <para>
       The freezer service has been deprecated and removed from &productname; &productnumber; &clm;,
       but the backups that the freezer service created before you upgraded
       will still be consuming space in your Swift Object store.
     </para>
     <para>
       Therefore, once you have completed the upgrade successfully, you can
       safely delete the containers that freezer used to hold the database and
       ring backups, freeing up that space.
     </para>
     <step>
       <para>
         Using the credentials in the <filename>backup.osrc</filename> file,
         found on the deployer node in the ardana account's home directory,
         run the following commands:
       </para>
       <screen>&prompt.ardana;. ~/backup.osrc
&prompt.ardana;swift list
freezer_database_backups
freezer_rings_backups
ardana> swift delete --all
freezer_database_backups/data/tar/ardana-cp-dbmqsw-m2-host_freezer_mysql_backup/1598505404/1_1598548599/segments/000000021
freezer_database_backups/data/tar/ardana-cp-dbmqsw-m2-host_freezer_mysql_backup/1598505404/2_1598605266/data1
...
freezer_database_backups/data/tar/ardana-cp-dbmqsw-m2-host_freezer_mysql_backup/1598505404/0_1598505404/segments/000000001
freezer_database_backups
freezer_rings_backups/metadata/tar/ardana-cp-dbmqsw-m1-host_freezer_swift_builder_dir_backup/1598548636/0_1598548636/metadata
...
freezer_rings_backups/data/tar/ardana-cp-dbmqsw-m1-host_freezer_swift_builder_dir_backup/1598548636/0_1598548636/data
freezer_rings_backups
</screen>
     </step>
   </procedure>
 </section>

</section>
