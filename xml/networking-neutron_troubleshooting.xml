<?xml version="1.0"?>
<!DOCTYPE section [
 <!ENTITY % entities SYSTEM "entity-decl.ent"> %entities;
]>
<section xmlns="http://docbook.org/ns/docbook" xmlns:xi="http://www.w3.org/2001/XInclude"
        xmlns:xlink="http://www.w3.org/1999/xlink" xml:id="neutron_troubleshooting">
 <title>Network Service Troubleshooting</title>
 <para>
  Troubleshooting scenarios with resolutions for the Networking service.
 </para>
 <section xml:id="network.fail.troublshoot">
  <title>Troubleshooting Network failures</title>
  <para>
   <emphasis role="bold">CVR HA - Split-brain result of failover of L3 agent
   when master comes back up</emphasis> This situation is specific to when L3
   HA is configured and a network failure occurs to the node hosting the
   currently active l3 agent. L3 HA is intended to provide HA in situations
   where the l3-agent crashes or the node hosting an l3-agent crashes/restarts.
   In the case of a physical networking issue which isolates the active l3
   agent, the stand-by l3-agent takes over but when the physical networking
   issue is resolved, traffic to the VMs is disrupted due to a "split-brain"
   situation in which traffic is split over the two L3 agents. The solution is
   to restart the L3-agent that was originally the master.
  </para>
  <para>
   <emphasis role="bold">OVSvApp loses connectivity with vCenter</emphasis> If
   the OVSvApp loses connectivity with the vCenter cluster, you will receive
   the following errors:
  </para>
  <procedure>
   <step>
    <para>
     The OVSvApp VM will go into ERROR state
    </para>
   </step>
   <step>
    <para>
     The OVSvApp VM will not get IP address
    </para>
   </step>
  </procedure>
  <para>
   When you see these symptoms:
  </para>
  <procedure>
   <step>
    <para>
     Restart the OVSvApp agent on the OVSvApp VM.
    </para>
   </step>
   <step>
    <para>
     Execute the following command to restart the Network (&o_netw;) service:
    </para>
<screen>sudo service neutron-ovsvapp-agent restart</screen>
   </step>
  </procedure>
<!---->
  <para>
   <emphasis role="bold">Fail over a plain CVR router because the node became
   unavailable:</emphasis>
  </para>
  <procedure>
   <step>
    <para>
     Get a list of l3 agent UUIDs which can be used in the commands that follow
    </para>
<screen> openstack network agent list | grep l3 </screen>
   </step>
   <step>
    <para>
     Determine the current host
    </para>
<screen> openstack network agent list –routers &lt;router uuid&gt; </screen>
   </step>
   <step>
    <para>
     Remove the router from the current host
    </para>
<screen>openstack network agent remove router –agent-type l3 &lt;current l3 agent uuid&gt; &lt;router uuid&gt;</screen>
   </step>
   <step>
    <para>
     Add the router to a new host
    </para>
<screen>openstack network agent add router –agent-type l3 &lt;new l3 agent uuid&gt; &lt;router uuid&gt; </screen>
   </step>
  </procedure>
  <para>
   <emphasis role="bold">Trouble setting maximum transmission units
   (MTU)</emphasis> <xref linkend="configureMTU"/>
  </para>
  <para>
   <emphasis role="bold">Floating IP on allowed_address_pair port with
   DVR-routed networks</emphasis> <literal>allowed_address_pair</literal>
  </para>
  <para>
   <emphasis role="bold">You may notice this issue:</emphasis> If you have an
   <literal>allowed_address_pair</literal> associated with multiple virtual
   machine (VM) ports, and if all the VM ports are ACTIVE, then the
   <literal>allowed_address_pair</literal> port binding will have the last
   ACTIVE VM's binding host as its bound host.
  </para>
  <para>
   <emphasis role="bold">In addition, you may notice</emphasis> that if the
   floating IP is assigned to the <literal>allowed_address_pair</literal> that
   is bound to multiple VMs that are ACTIVE, then the floating IP will not work
   with DVR routers. This is different from the centralized router behavior
   where it can handle unbound <literal>allowed_address_pair</literal> ports
   that are associated with floating IPs.
  </para>
  <para>
   Currently we support <literal>allowed_address_pair</literal> ports with DVR
   only if they have floating IPs enabled, and have just one ACTIVE port.
  </para>
  <para>
   Using the CLI, you can follow these steps:
  </para>
  <procedure>
   <step>
    <para>
     Create a network to add the host to:
    </para>
<screen>$ openstack network create vrrp-net</screen>
   </step>
   <step>
    <para>
     Attach a subnet to that network with a specified allocation-pool range:
    </para>
<screen>$ openstack subnet create  --name vrrp-subnet --allocation-pool start=10.0.0.2,end=10.0.0.200 vrrp-net 10.0.0.0/24</screen>
   </step>
   <step>
    <para>
     Create a router, uplink the vrrp-subnet to it, and attach the router to an
     upstream network called public:
    </para>
<screen>$ openstack router create router1
$ openstack router add subnet router1 vrrp-subnet
$ openstack router set router1 public</screen>
    <para>
     Create a security group called vrrp-sec-group and add ingress rules to
     allow ICMP and TCP port 80 and 22:
    </para>
<screen>$ openstack security group create vrrp-sec-group
$ openstack security group rule create  --protocol icmp vrrp-sec-group
$ openstack security group rule create  --protocol tcp  --port-range-min80 --port-range-max80 vrrp-sec-group
$ openstack security group rule create  --protocol tcp  --port-range-min22 --port-range-max22 vrrp-sec-group</screen>
   </step>
   <step>
    <para>
     Next, boot two instances:
    </para>
<screen>$ nova boot --num-instances 2 --image ubuntu-12.04 --flavor 1 --nic net-id=24e92ee1-8ae4-4c23-90af-accb3919f4d1 vrrp-node --security_groups vrrp-sec-group</screen>
   </step>
   <step>
    <para>
     When you create two instances, make sure that both the instances are not
     in ACTIVE state before you associate the
     <literal>allowed_address_pair</literal>. The instances:
    </para>
<screen>$ nova list
+--------------------------------------+-------------------------------------------------+--------+------------+-------------+--------------------------------------------------------+
| ID                                   | Name                                            | Status | Task State | Power State | Networks                                               |
+--------------------------------------+-------------------------------------------------+--------+------------+-------------+--------------------------------------------------------+
| 15b70af7-2628-4906-a877-39753082f84f | vrrp-node-15b70af7-2628-4906-a877-39753082f84f | ACTIVE  | -          | Running     | vrrp-net=10.0.0.3                                      |
| e683e9d1-7eea-48dd-9d3a-a54cf9d9b7d6 | vrrp-node-e683e9d1-7eea-48dd-9d3a-a54cf9d9b7d6 | DOWN    | -          | Running     | vrrp-net=10.0.0.4                                      |
+--------------------------------------+-------------------------------------------------+--------+------------+-------------+--------------------------------------------------------+    </screen>
   </step>
   <step>
    <para>
     Create a port in the VRRP IP range that was left out of the ip-allocation
     range:
    </para>
<screen>$ openstack port create --fixed-ip ip_address=10.0.0.201 --security-group vrrp-sec-group vrrp-net
Created a new port:
+-----------------------+-----------------------------------------------------------------------------------+
| Field                 | Value                                                                             |
+-----------------------+-----------------------------------------------------------------------------------+
| admin_state_up        | True                                                                              |
| allowed_address_pairs |                                                                                   |
| device_id             |                                                                                   |
| device_owner          |                                                                                   |
| fixed_ips             | {"subnet_id": "94a0c371-d37c-4796-821e-57c2a8ec65ae", "ip_address": "10.0.0.201"} |
| id                    | 6239f501-e902-4b02-8d5c-69062896a2dd                                              |
| mac_address           | fa:16:3e:20:67:9f                                                                 |
| name                  |                                                                                   |
| network_id            | 24e92ee1-8ae4-4c23-90af-accb3919f4d1                                              |
| port_security_enabled | True                                                                              |
| security_groups       | 36c8131f-d504-4bcc-b708-f330c9f6b67a                                              |
| status                | DOWN                                                                              |
| tenant_id             | d4e4332d5f8c4a8eab9fcb1345406cb0                                                  |
+-----------------------+-----------------------------------------------------------------------------------+</screen>
   </step>
   <step>
    <para>
     Another thing to cross check after you associate the allowed_address_pair
     port to the VM port, is whether the
     <literal>allowed_address_pair</literal> port has inherited the VM's host
     binding:
    </para>
<screen>$ neutron --os-username admin --os-password ZIy9xitH55 --os-tenant-name admin port-show f5a252b2-701f-40e9-a314-59ef9b5ed7de
+-----------------------+--------------------------------------------------------------------------------------------------------+
| Field                 | Value                                                                                                  |
+-----------------------+--------------------------------------------------------------------------------------------------------+
| admin_state_up        | True                                                                                                   |
| allowed_address_pairs |                                                                                                        |
| {color:red}binding:host_id{color} | ...-cp1-comp0001-mgmt                                                                      |
| binding:profile       | {}                                                                                                     |
| binding:vif_details   | {"port_filter": true, "ovs_hybrid_plug": true}                                                         |
| binding:vif_type      | ovs                                                                                                    |
| binding:vnic_type     | normal                                                                                                 |
| device_id             |                                                                                                        |
| device_owner          | compute:None                                                                                           |
| dns_assignment        | {"hostname": "host-10-0-0-201", "ip_address": "10.0.0.201", "fqdn": "host-10-0-0-201.openstacklocal."} |
| dns_name              |                                                                                                        |
| extra_dhcp_opts       |                                                                                                        |
| fixed_ips             | {"subnet_id": "94a0c371-d37c-4796-821e-57c2a8ec65ae", "ip_address": "10.0.0.201"}                      |
| id                    | 6239f501-e902-4b02-8d5c-69062896a2dd                                                                   |
| mac_address           | fa:16:3e:20:67:9f                                                                                      |
| name                  |                                                                                                        |
| network_id            | 24e92ee1-8ae4-4c23-90af-accb3919f4d1                                                                   |
| port_security_enabled | True                                                                                                   |
| security_groups       | 36c8131f-d504-4bcc-b708-f330c9f6b67a                                                                   |
| status                | DOWN                                                                                                   |
| tenant_id             | d4e4332d5f8c4a8eab9fcb1345406cb0                                                                       |
+-----------------------+--------------------------------------------------------------------------------------------------------+</screen>
   </step>
   <step>
    <para>
     Note that you were allocated a port with the IP address 10.0.0.201 as
     requested. Next, associate a floating IP to this port to be able to access
     it publicly:
    </para>
<screen>$ openstack floating ip create --port-id=6239f501-e902-4b02-8d5c-69062896a2dd public
Created a new floatingip:
+---------------------+--------------------------------------+
| Field               | Value                                |
+---------------------+--------------------------------------+
| fixed_ip_address    | 10.0.0.201                           |
| floating_ip_address | 10.36.12.139                         |
| floating_network_id | 3696c581-9474-4c57-aaa0-b6c70f2529b0 |
| id                  | a26931de-bc94-4fd8-a8b9-c5d4031667e9 |
| port_id             | 6239f501-e902-4b02-8d5c-69062896a2dd |
| router_id           | 178fde65-e9e7-4d84-a218-b1cc7c7b09c7 |
| tenant_id           | d4e4332d5f8c4a8eab9fcb1345406cb0     |
+---------------------+--------------------------------------+</screen>
   </step>
   <step>
    <para>
     Now update the ports attached to your VRRP instances to include this IP
     address as an allowed-address-pair so they will be able to send traffic
     out using this address. First find the ports attached to these instances:
    </para>
<screen>$ openstack port list -- --network_id=24e92ee1-8ae4-4c23-90af-accb3919f4d1
+--------------------------------------+------+-------------------+-----------------------------------------------------------------------------------+
| id                                   | name | mac_address       | fixed_ips                                                                         |
+--------------------------------------+------+-------------------+-----------------------------------------------------------------------------------+
| 12bf9ea4-4845-4e2c-b511-3b8b1ad7291d |      | fa:16:3e:7a:7b:18 | {"subnet_id": "94a0c371-d37c-4796-821e-57c2a8ec65ae", "ip_address": "10.0.0.4"}   |
| 14f57a85-35af-4edb-8bec-6f81beb9db88 |      | fa:16:3e:2f:7e:ee | {"subnet_id": "94a0c371-d37c-4796-821e-57c2a8ec65ae", "ip_address": "10.0.0.2"}   |
| 6239f501-e902-4b02-8d5c-69062896a2dd |      | fa:16:3e:20:67:9f | {"subnet_id": "94a0c371-d37c-4796-821e-57c2a8ec65ae", "ip_address": "10.0.0.201"} |
| 87094048-3832-472e-a100-7f9b45829da5 |      | fa:16:3e:b3:38:30 | {"subnet_id": "94a0c371-d37c-4796-821e-57c2a8ec65ae", "ip_address": "10.0.0.1"}   |
| c080dbeb-491e-46e2-ab7e-192e7627d050 |      | fa:16:3e:88:2e:e2 | {"subnet_id": "94a0c371-d37c-4796-821e-57c2a8ec65ae", "ip_address": "10.0.0.3"}   |
+--------------------------------------+------+-------------------+-----------------------------------------------------------------------------------+</screen>
   </step>
   <step>
    <para>
     Add this address to the ports c080dbeb-491e-46e2-ab7e-192e7627d050 and
     12bf9ea4-4845-4e2c-b511-3b8b1ad7291d which are 10.0.0.3 and 10.0.0.4 (your
     vrrp-node instances):
    </para>
<screen>$ openstack port set  c080dbeb-491e-46e2-ab7e-192e7627d050 --allowed_address_pairs list=truetype=dict ip_address=10.0.0.201
$ openstack port set  12bf9ea4-4845-4e2c-b511-3b8b1ad7291d --allowed_address_pairs list=truetype=dict ip_address=10.0.0.201</screen>
   </step>
   <step>
    <para>
     The allowed-address-pair 10.0.0.201 now shows up on the port:
    </para>
<screen>$ openstack port show 12bf9ea4-4845-4e2c-b511-3b8b1ad7291d
+-----------------------+---------------------------------------------------------------------------------+
| Field                 | Value                                                                           |
+-----------------------+---------------------------------------------------------------------------------+
| admin_state_up        | True                                                                            |
| allowed_address_pairs | {"ip_address": "10.0.0.201", "mac_address": "fa:16:3e:7a:7b:18"}                |
| device_id             | e683e9d1-7eea-48dd-9d3a-a54cf9d9b7d6                                            |
| device_owner          | compute:None                                                                    |
| fixed_ips             | {"subnet_id": "94a0c371-d37c-4796-821e-57c2a8ec65ae", "ip_address": "10.0.0.4"} |
| id                    | 12bf9ea4-4845-4e2c-b511-3b8b1ad7291d                                            |
| mac_address           | fa:16:3e:7a:7b:18                                                               |
| name                  |                                                                                 |
| network_id            | 24e92ee1-8ae4-4c23-90af-accb3919f4d1                                            |
| port_security_enabled | True                                                                            |
| security_groups       | 36c8131f-d504-4bcc-b708-f330c9f6b67a                                            |
| status                | ACTIVE                                                                          |
| tenant_id             | d4e4332d5f8c4a8eab9fcb1345406cb0                                                | </screen>
   </step>
  </procedure>
<!---->
  <para>
   <emphasis role="bold">OpenStack traffic that must traverse VXLAN tunnel
   dropped when using HPE 5930 switch</emphasis> Cause: UDP destination port
   4789 is conflicting with OpenStack VXLAN traffic.
  </para>
  <para>
   There is a configuration setting you can use in the switch to configure the
   port number the HPN kit will use for its own VXLAN tunnels. Setting this to
   a port number other than the one &o_netw; will use by default (4789) will
   keep the HPN kit from absconding with &o_netw;'s VXLAN traffic. Specifically:
  </para>
  <para>
   <emphasis role="bold">Parameters: </emphasis>
  </para>
  <para>
   port-number: Specifies a UDP port number in the range of 1 to 65535. As a
   best practice, specify a port number in the range of 1024 to 65535 to avoid
   conflict with well-known ports.
  </para>
  <para>
   <emphasis role="bold">Usage guidelines:</emphasis>
  </para>
  <para>
   You must configure the same destination UDP port number on all VTEPs in a
   VXLAN.
  </para>
  <para>
   Examples
  </para>
<screen># Set the destination UDP port number to 6666 for VXLAN packets.
&lt;Sysname&gt; system-view
[Sysname] vxlan udp-port 6666</screen>
  <para>
   Use vxlan udp-port to configure the destination UDP port number of VXLAN
   packets.   Mandatory for all VXLAN packets to specify a UDP port Default
   The destination UDP port number is 4789 for VXLAN packets.
  </para>
  <para>
   OVS can be configured to use a different port number itself:
  </para>
<screen># (IntOpt) The port number to utilize if tunnel_types includes 'vxlan'. By
# default, this will make use of the Open vSwitch default value of '4789' if
# not specified.
#
# vxlan_udp_port =
# Example: vxlan_udp_port = 8472
# </screen>
  <section xml:id="DOCS-3584">
   <title>Issue: PCI-PT virtual machine gets stuck at boot</title>
   <para>
    If you are using a machine that uses Intel NICs, if the PCI-PT virtual
    machine gets stuck at boot, the boot agent should be disabled.
   </para>
   <para>
    When Intel cards are used for PCI-PT, sometimes the tenant virtual machine
    gets stuck at boot. If this happens, you should download Intel bootutils
    and use it to disable the bootagent.
   </para>
   <para>
    Use the following steps:
   </para>
   <procedure>
    <step>
     <para>
      Download <literal>preebot.tar.gz</literal> from the
      <link
                                                  xlink:href="https://downloadcenter.intel.com/download/19186/Intel-Ethernet-Connections-Boot-Utility-Preboot-Images-and-EFI-Drivers"
                                                  >Intel
      website</link>.
     </para>
    </step>
    <step>
     <para>
      Untar the <literal>preboot.tar.gz</literal> file on the compute host
      where the PCI-PT virtual machine is to be hosted.
     </para>
    </step>
    <step>
     <para>
      Go to path <literal>~/APPS/BootUtil/Linux_x64</literal> and then run
      following command:
     </para>
<screen>./bootutil64e -BOOTENABLE disable -all</screen>
    </step>
    <step>
     <para>
      Now boot the PCI-PT virtual machine and it should boot without getting
      stuck.
     </para>
    </step>
   </procedure>
  </section>
 </section>
</section>
