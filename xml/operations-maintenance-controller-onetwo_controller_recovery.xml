<?xml version="1.0"?>
<?xml-stylesheet href="urn:x-suse:xslt:profiling:docbook51-profile.xsl"
 type="text/xml"
 title="Profiling step"?>
<!DOCTYPE section [
 <!ENTITY % entities SYSTEM "entities.ent"> %entities;
]>
<section xmlns="http://docbook.org/ns/docbook" xmlns:xi="http://www.w3.org/2001/XInclude" xmlns:xlink="http://www.w3.org/1999/xlink" xml:id="onetwo_controller_recovery">
 <title>One or Two Controller Node Disaster Recovery</title>
 <para>
  This scenario makes the following assumptions:
 </para>
 <itemizedlist>
  <listitem>
   <para>
    Your &clm; is still intact and working.
   </para>
  </listitem>
  <listitem>
   <para>
    One or two of your controller nodes went down, but not the entire cluster.
   </para>
  </listitem>
  <listitem>
   <para>
    The node needs to be rebuilt from scratch, not simply rebooted.
   </para>
  </listitem>
 </itemizedlist>
 <section>
  <title>Steps to recovering one or two controller nodes</title>
  <orderedlist>
   <listitem>
    <para>
     Ensure that your node has power and all of the hardware is functioning.
    </para>
   </listitem>
   <listitem>
    <para>
     Log in to the &clm;.
    </para>
   </listitem>
   <listitem>
    <para>
     Verify that all of the information in your
     <literal>~/openstack/my_cloud/definition/data/servers.yml</literal> file is
     correct for your controller node. You may need to replace the existing
     information if you had to either replacement your entire controller node
     or just pieces of it.
    </para>
   </listitem>
   <listitem>
    <para>
     If you made changes to your <literal>servers.yml</literal> file then
     commit those changes to your local git:
    </para>
<screen>&prompt.ardana;git add -A
&prompt.ardana;git commit -a -m "editing controller information"</screen>
   </listitem>
   <listitem>
    <para>
     Run the configuration processor:
    </para>
<screen>&prompt.ardana;cd ~/openstack/ardana/ansible
&prompt.ardana;ansible-playbook -i hosts/localhost config-processor-run.yml</screen>
   </listitem>
   <listitem>
    <para>
     Update your deployment directory:
    </para>
<screen>&prompt.ardana;cd ~/openstack/ardana/ansible
&prompt.ardana;ansible-playbook -i hosts/localhost ready-deployment.yml</screen>
   </listitem>
   <listitem>
    <para>
     Ensure that Cobbler has the correct system information:
    </para>
    <orderedlist>
     <listitem>
      <para>
       If you replaced your controller node with a completely new machine, you
       need to verify that Cobbler has the correct list of controller nodes:
      </para>
<screen>&prompt.ardana;sudo cobbler system list</screen>
     </listitem>
     <listitem>
      <para>
       Remove any controller nodes from Cobbler that no longer exist:
      </para>
<screen>&prompt.ardana;sudo cobbler system remove --name=&lt;node&gt;</screen>
     </listitem>
     <listitem>
      <para>
       Add the new node into Cobbler:
      </para>
<screen>&prompt.ardana;cd ~/openstack/ardana/ansible
&prompt.ardana;ansible-playbook -i hosts/localhost cobbler-deploy.yml</screen>
     </listitem>
    </orderedlist>
   </listitem>
   <listitem>
    <para>
     Then you can image the node:
    </para>
<screen>&prompt.ardana;cd ~/openstack/ardana/ansible
&prompt.ardana;ansible-playbook -i hosts/localhost bm-reimage.yml -e nodelist=&lt;node_name&gt;</screen>
    <note>
     <para>
      If you do not know the <literal>&lt;node name&gt;</literal> already,
      you can get it by using <command>sudo cobbler system list</command>.
     </para>
    </note>
    <para>
     Before proceeding, you may want to take a look at
     <emphasis role="bold">info/server_info.yml</emphasis> to see if the
     assignment of the node you have added is what you expect. It may not be,
     as nodes will not be numbered consecutively if any have previously been
     removed. This is to prevent loss of data; the config processor retains
     data about removed nodes and keeps their ID numbers from being
     reallocated. See the Persisted Server Allocations section in for
     information on how this works.
    </para>
   </listitem>
   <listitem>
    <para>
     [OPTIONAL] - Run the <literal>wipe_disks.yml</literal> playbook to ensure
     all of your partitions on your nodes are completely wiped prior to
     continuing with the installation. The <filename>wipe_disks.yml</filename>
     playbook is only meant to be run on systems immediately after running
     <filename>bm-reimage.yml</filename>. If used for any other case, it may
     not wipe all of the expected partitions.
    </para>
<screen>&prompt.ardana;cd ~/scratch/ansible/next/ardana/ansible/
&prompt.ardana;ansible-playbook -i hosts/verb_hosts wipe_disks.yml --limit &lt;controller_node_hostname&gt;</screen>
   </listitem>
   <listitem>
    <para>
     Complete the rebuilding of your controller node with the two playbooks
     below:
    </para>
<screen>&prompt.ardana;cd ~/scratch/ansible/next/ardana/ansible
&prompt.ardana;ansible-playbook -i hosts/verb_hosts osconfig-run.yml -e rebuild=True --limit=&lt;controller_node_hostname&gt;
&prompt.ardana;ansible-playbook -i hosts/verb_hosts ardana-deploy.yml -e rebuild=True --limit=&lt;controller_node_hostname&gt;</screen>
   </listitem>
  </orderedlist>
 </section>
</section>
