<?xml version="1.0"?>
<?xml-stylesheet href="urn:x-suse:xslt:profiling:docbook51-profile.xsl"
 type="text/xml"
 title="Profiling step"?>
<!DOCTYPE section [
 <!ENTITY % entities SYSTEM "entity-decl.ent"> %entities;
]>
<section xmlns="http://docbook.org/ns/docbook" xmlns:xi="http://www.w3.org/2001/XInclude" xmlns:xlink="http://www.w3.org/1999/xlink" xml:id="onetwo_controller_recovery">
 <title>One or Two Controller Node Disaster Recovery</title>
 <para>
  This scenario makes the following assumptions:
 </para>
 <itemizedlist>
  <listitem>
   <para>
    Your &clm; is still intact and working.
   </para>
  </listitem>
  <listitem>
   <para>
    One or two of your controller nodes went down, but not the entire cluster.
   </para>
  </listitem>
  <listitem>
   <para>
    The node needs to be rebuilt from scratch, not simply rebooted.
   </para>
  </listitem>
 </itemizedlist>
 <section>
  <title>Steps to recovering one or two controller nodes</title>
  <orderedlist>
   <listitem>
    <para>
     Ensure that your node has power and all of the hardware is functioning.
    </para>
   </listitem>
   <listitem>
    <para>
     Log in to the &clm;.
    </para>
   </listitem>
   <listitem>
    <para>
     Verify that all of the information in your
     <literal>~/openstack/my_cloud/definition/data/servers.yml</literal> file is
     correct for your controller node. You may need to replace the existing
     information if you had to either replacement your entire controller node
     or just pieces of it.
    </para>
   </listitem>
   <listitem>
    <para>
     If you made changes to your <literal>servers.yml</literal> file then
     commit those changes to your local git:
    </para>
<screen>&prompt.ardana;git add -A
&prompt.ardana;git commit -a -m "editing controller information"</screen>
   </listitem>
   <listitem>
    <para>
     Run the configuration processor:
    </para>
<screen>&prompt.ardana;cd ~/openstack/ardana/ansible
&prompt.ardana;ansible-playbook -i hosts/localhost config-processor-run.yml</screen>
   </listitem>
   <listitem>
    <para>
     Update your deployment directory:
    </para>
<screen>&prompt.ardana;cd ~/openstack/ardana/ansible
&prompt.ardana;ansible-playbook -i hosts/localhost ready-deployment.yml</screen>
   </listitem>
   <listitem>
    <para>
     Ensure that Cobbler has the correct system information:
    </para>
    <orderedlist>
     <listitem>
      <para>
       If you replaced your controller node with a completely new machine, you
       need to verify that Cobbler has the correct list of controller nodes:
      </para>
<screen>&prompt.ardana;sudo cobbler system list</screen>
     </listitem>
     <listitem>
      <para>
       Remove any controller nodes from Cobbler that no longer exist:
      </para>
<screen>&prompt.ardana;sudo cobbler system remove --name=&lt;node&gt;</screen>
     </listitem>
     <listitem>
      <para>
       Add the new node into Cobbler:
      </para>
<screen>&prompt.ardana;cd ~/openstack/ardana/ansible
&prompt.ardana;ansible-playbook -i hosts/localhost cobbler-deploy.yml</screen>
     </listitem>
    </orderedlist>
   </listitem>
   <listitem>
    <para>
     Then you can image the node:
    </para>
<screen>&prompt.ardana;cd ~/openstack/ardana/ansible
&prompt.ardana;ansible-playbook -i hosts/localhost bm-reimage.yml -e nodelist=&lt;node_name&gt;</screen>
    <note>
     <para>
      If you do not know the <literal>&lt;node name&gt;</literal> already,
      you can get it by using <command>sudo cobbler system list</command>.
     </para>
    </note>
    <para>
     Before proceeding, look at <emphasis
     role="bold">info/server_info.yml</emphasis> to see if the assignment of
     the node you have added is what you expect. It may not be, as nodes will
     not be numbered consecutively if any have previously been removed. This is
     to prevent loss of data; the configuration processor retains data about
     removed nodes and keeps their ID numbers from being reallocated. For more
     information about how this works, see <xref
     linkend="persistedserverallocations"/>.
    </para>
   </listitem>
   <listitem>
    <para>
     Run the <filename>wipe_disks.yml</filename> playbook to ensure all of your
     partitions on your nodes are completely wiped prior to continuing with the
     installation.
    </para>
    <important>
     <para>
      The <filename>wipe_disks.yml</filename> playbook is only meant to be run
      on systems immediately after running
      <filename>bm-reimage.yml</filename>. If used for any other situation, it
      may not wipe all of the expected partitions.
     </para>
    </important>
<screen>&prompt.ardana;cd ~/scratch/ansible/next/ardana/ansible/
&prompt.ardana;ansible-playbook -i hosts/verb_hosts wipe_disks.yml --limit &lt;controller_node_hostname&gt;</screen>
   </listitem>
   <listitem>
    <para>
     Complete the rebuilding of your controller node with the two playbooks
     below:
    </para>
<screen>&prompt.ardana;cd ~/scratch/ansible/next/ardana/ansible
&prompt.ardana;ansible-playbook -i hosts/verb_hosts osconfig-run.yml -e rebuild=True --limit=&lt;controller_node_hostname&gt;
&prompt.ardana;ansible-playbook -i hosts/verb_hosts ardana-deploy.yml -e rebuild=True --limit=&lt;controller_node_hostname&gt;</screen>
   </listitem>
  </orderedlist>
 </section>
</section>
