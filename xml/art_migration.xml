<?xml version="1.0" encoding="UTF-8"?>
<?xml-stylesheet href="urn:x-suse:xslt:profiling:docbook50-profile.xsl"
 type="text/xml"
 title="Profiling step"?>
<!DOCTYPE article
[
  <!ENTITY % entities SYSTEM "entity-decl.ent">
    %entities;
]>
<article version="5.1" xml:lang="en" xml:id="art.migration"
  xmlns="http://docbook.org/ns/docbook"
  xmlns:xi="http://www.w3.org/2001/XInclude"
  xmlns:xlink="http://www.w3.org/1999/xlink">
 <info>
  <title>Migration Guide</title>
  <date><?dbtimestamp format="B d, Y"?></date>
  <!-- <xi:include href="common_copyright_gfdl.xml"/> -->
  <xi:include href="authors.xml"/>
 </info>
<sect1>
  <title>Migration from HOS 5.x to HOS 5.SCP</title>
  <para>
    The migration procedure is based on the following assumptions.
  </para>
  <itemizedlist>
    <listitem>
      <para>
	Customer is responsible for backing up all the control plane and deployer nodes to an external data store before starting migration
      </para>
      <itemizedlist>
	<listitem>
	  <para>
	    We do not support rollback of migration process once started
	   </para>
	  </listitem>
      </itemizedlist>
    </listitem>
    <listitem>
      <para>
	MML data will not be migrated (HOS 5.SCP includes a new MML backend
	which will be installed fresh)
    </para>
    </listitem>
    <listitem>
      <para>
	Any data not kept on a separate disk from the primary root partition will be lost during reimaging:
    </para>
    <itemizedlist>
      <listitem>
	<para>
	  Swift objects
	 </para>
	</listitem>
	<listitem>
	  <para>
	    Audit logs
	   </para>
	  </listitem>
    </itemizedlist>
    </listitem>
    <listitem>
      <para>
	All computes are running supported Linux distribution:
    </para>
    <itemizedlist>
      <listitem>
	<para>
	  SLES12 SP3 for SLES computes
	 </para>
	</listitem>
	<listitem>
	  <para>
	    RHEL 7.3 for RHEL computes
	   </para>
	  </listitem>
    </itemizedlist>
    </listitem>
    <listitem>
      <para>
	Only supported input model services
    </para>
    <itemizedlist>
      <listitem>
	<para>
	  No embedded HOS Ceph or VSA
	 </para>
	</listitem>
    </itemizedlist>
    </listitem>
    <listitem>
      <para>
	Only supported third-party integrations/extensions
    </para>
    <itemizedlist>
      <listitem>
	<para>
	  No unsupported customized virtual environments
	 </para>
	</listitem>
    </itemizedlist>
    </listitem>
  </itemizedlist>
  <procedure>
    <title>Preparation Phase</title>
<step>
  <para>
    Run validation playbook to ensure that current cloud load and configuration
    is compatible with migration process:
  </para>
  <screen>    (old-deployer)$ git clone -b hp/prerelease/newton https://gerrit.suse.provo.cloud/hp/hlm-migration-tools
(old-deployer)$ cd hlm-migration-tools/ansible
(old-deployer)$ ansible-playbook -i ~/scratch/ansible/next/hos/ansible/hosts/verb_hosts pre-migration-validations.yml
  </screen>
  <para>
    The validation playbook performs the following checks:
  </para>
  <itemizedlist>
    <listitem>
      <para>
       Proper HOS 5.0.1/5.0.2 installed version number
      </para>
    </listitem>
    <listitem>
      <para>
       C8-compatible Linux distributions on compute nodes
    </para>
    </listitem>
    <listitem>
     <para>
      System clock synchronization across cloud nodes
     </para>
    </listitem>
    <listitem>
     <para>
      HA capacity of controller nodes (L3/DHCP agent)
     </para>
    </listitem>
    <listitem>
     <para>
      HA capacity of controller nodes (Swift)
     </para>
    </listitem>
    <listitem>
     <para>
      Data model and Cinder back-end compatibility (no VSA or Ceph)
     </para>
    </listitem>
  </itemizedlist>
</step>
<step>
  <para>
   Identify a candidate "seed" controller node in the Primary control plane
   meeting the following requirements:
  </para>
  <itemizedlist>
   <listitem>
    <para>
     Networking connectivity is equivalent to that of existing deployer
    </para>
   </listitem>
   <listitem>
    <para>
     Member of the database cluster (running MySQL service)
    </para>
   </listitem>
   <listitem>
    <para>
     Not actively hosting any singleton Openstack services (cinder-volume or nova-consoleauth), test for empty output using:
    </para>
    <screen>(each controller)$ ps -ef | egrep 'cinder-volume|nova-consoleauth'</screen>
   </listitem>
  </itemizedlist>
</step>
<step>
  <para>
   Prepare cloud for migration
  </para>
  <itemizedlist>
  <listitem>
   <para>
    Ensure that the data model enables deployer behavior by the new seed
    node.
   </para>
   <itemizedlist>
    <listitem>
     <para>
          Edit the <filename>disks_*.yml</filename> file that is used by your
    server role that includes monitoring services, and replace the existing
    vertica partition definition with two Cassandra partitions:
   </para>
   <screen>- name: cassandra
            size: (whatever size was used originally by vertica partition, less 1%)
            mount: /var/cassandra/data
            fstype: ext4
            mkfs-opts: -O large_file
            consumer:
              name: vertica
          - name: cassandra_log
            size: 1%
            mount: /var/cassandra/commitlog
            fstype: ext4
            mkfs-opts: -O large_file
   </screen>
    </listitem>
    <listitem>
     <para>
      Edit <filename>~/helion/my_cloud/definition/data/control_plane.yml</filename> and identify the cluster definition that already includes an entry for <filename>mysql</filename> in its service-components
     </para>
    </listitem>
    <listitem>
     <para>
      Ensure that <systemitem>lifecycle-manager</systemitem> is included alongside it in the same
      cluster's service-components list. If it was necessary to add this as a
      new entry, it will also need to be removed from its original position in
      the deployer
     </para>
    </listitem>
    <listitem>
     <para>
      Add a new entry for <systemitem>nova-placement-api</systemitem> alongside whichever cluster already includes <systemitem>nova-api</systemitem>
     </para>
    </listitem>
    <listitem>
     <para>
      Commit your changes to the git model:
     </para>
     <screen>(deployer)$ git commit -a -m "Prepare for HOS 5.1 installation"</screen>
    </listitem>
   </itemizedlist>
  </listitem>
  <listitem>
   <para>
    Consider the effect of control plane downtime on existing DHCP leases
   </para>
   <itemizedlist>
    <listitem>
     <para>
      Default DHCP lease time is 48 hours as defined in <systemitem>dhcp_lease_duration</systemitem> in <filename>~/helion/hos/ansible/roles/neutron-common/templates/neutron.conf.j2</filename>
     </para>
    </listitem>
    <listitem>
     <para>
      DHCP leases are administered by Neutron and dnsmasq, and this functionality will be unavailable during the control plane downtime
     </para>
    </listitem>
    <listitem>
     <para>
      DHCP clients typically renew their leases 50% of the way into the lease time, or upon reboot
     </para>
    </listitem>
    <listitem>
     <para>
      Under typical circumstances, this leaves a 24-hour window for control
      plane downtime at any point before workload's DHCP leases run out and
      cannot be renewed, which is enough for performing a migration
     </para>
    </listitem>
    <listitem>
     <para>
      To observe the upcoming timeouts of DHCP leases, run the following command from a controller node:
     </para>
     <screen>for i in $(cat /var/run/neutron/dhcp/*/leases | awk '{print $1}'); do date -d @$i; done</screen>
    </listitem>
    <listitem>
     <para>
      If you find it necessary to increase the DHCP lease time before migrating, edit <systemitem>dhcp_lease_duration</systemitem> in the file described above, commit the change, and run the <filename>neutron-configure.yml</filename> playbook
     </para>
    </listitem>
   </itemizedlist>
  </listitem>
  </itemizedlist>
</step>
<step>
 <para>
  Decommission seed node from existing cloud (if the intended seed node hosts control plane services beyond the database)
 </para>
 <itemizedlist>
  <listitem>
   <para>
    For non-DVR clouds, evacuate any Neutron L3 routers existing on the seed node
   </para>
   <screen>(old-deployer)$ cd ~/scratch/ansible/next/hos/ansible
(old-deployer)$ cp -r ~/hlm-migration-tools/ansible/* ./
(old-deployer)$ ansible-playbook -i hosts/verb_hosts
   neutron-router-evacuate.yml
   --limit=<replaceable>SEED_HOST</replaceable></screen>
   <para>
     <replaceable>SEED_HOST</replaceable> could be for example <systemitem>helion-cp1-c1-m2</systemitem>
   </para>
  </listitem>
  <listitem>
   <para>
    For proper behavior, you must use the <systemitem>--limit</systemitem>
    argument to select the target seed host. This will output a success or
    error message at the end of its run.
   </para>
  </listitem>
 </itemizedlist>
</step>
 <step>
  <para>
   Re-image the seed controller as SLES-based
  </para>
  <itemizedlist>
   <listitem>
    <para>
     Display the current network connections of the seed controller (before re-imaging) for reference, and save it locally
    </para>
    <screen>(seed)$ ip a</screen>
   </listitem>
   <listitem>
    <para>
     Reinstall the OS on the seed node's primary disk using an ISO, cobbler (from the existing HOS 5.0.1+ deployer), or other provisioner
    </para>
   </listitem>
   <listitem>
    <para>
         If using cobbler, update the <filename>servers.yml</filename> entry for all nodes in the control plane to specify <systemitem>distro-id: sles12sp3-x86_64</systemitem> in each one, and commit the change to git:
    </para>
    <screen>(old-deployer)$ cd ~/helion/hos/ansible
(old-deployer)$ sudo cobbler system remove --name <replaceable>SEED_NODE_ID</replaceable>
(old-deployer)$ ansible-playbook -i hosts/localhost cobbler-deploy.yml
(old-deployer)$ ansible-playbook -i hosts/localhost bm-reimage.yml -e nodelist=<replaceable>SEED_NODE_ID</replaceable></screen>
    <para>
     Replace <replaceable>SEED_NODE_ID</replaceable> with the id of seed node from <filename>servers.yml</filename>.
    </para>
   </listitem>
   <listitem>
    <para>
         Otherwise reformat disk using partitioning to match the current configuration
    </para>
   </listitem>
   <listitem>
    <para>
   Copy entire <filename>~stack</filename> directory from original deployer
  </para>
  <screen>(old-deployer)$ rsync -avP ~
  <replaceable>SEED_IP</replaceable>:/home</screen>
   </listitem>
   <listitem>
    <para>
     Ensure that <filename>~/sles12sp3sdk.iso</filename> was either transferred from the original deployer or is downloaded and put in place (with that file name exactly)
    </para>
   </listitem>
   <listitem>
    <para>
     If cobbler will be used from the new seed for provisioning the remaining cloud controller nodes:
    </para>
    <itemizedlist>
     <listitem>
      <para>
       Ensure that the new deployer is able to reach ILO interfaces referenced in <filename>servers.yml</filename>
      </para>
      <screen>(seed)$ ping <replaceable>ILO_IP</replaceable></screen>
     </listitem>
     <listitem>
      <para>
       If network accessibility to the ILOs requires special network configuration and you would like to use HLM for that process based on your model:
      </para>
      <screen>(old-deployer)$ ansible-playbook -i hosts/verb_hosts hlm-refresh-facts.yml
(old-deployer)$ ansible-playbook -i hosts/verb_hosts -l <replaceable>LOCAL_NODE_ID</replaceable> osconfig-run.yml</screen>
     <para>
      Replace <replaceable>LOCAL_NODE_ID</replaceable> with the id of local node from <filename>verb_hosts</filename>.
     </para>
     </listitem>
     <listitem>
      <para>
       Kill any <systemitem>dhcpd</systemitem> service running on the old deployer:
      </para>
      <screen>(old-deployer)$ sudo pkill dhcpd</screen>
     </listitem>
    </itemizedlist>
   </listitem>
  </itemizedlist>
 </step>
 <step>
  <para>
   Mount the SLES12SP3 ISO so that it's available to the HOS installer
  </para>
  <itemizedlist>
   <listitem>
    <para>
     Ensure that <filename>~/sles12sp3.iso</filename> was either transferred from the original deployer or is downloaded and put in place (with that file name exactly):
    </para>
    <screen>(seed)$ sudo mkdir -p /media/cdrom
(seed)$ sudo mount ~/sles12sp3.iso /media/cdrom
(seed)$ sudo vi /etc/zypp/repos.d/SLES12-SP3-12.3-0.repo</screen>
   <itemizedlist>
    <listitem>
     <para>
      Update the baseurl line to read <literal>baseurl=file:///media/cdrom</literal>
     </para>
    </listitem>
   </itemizedlist>
   </listitem>
  </itemizedlist>
 </step>
<step>
  <para>
   Install HOS 5.SCP on the new deployer by downloading and unpacking HOS 5.SCP installation content:
  </para>
  <screen>(seed)$ cd ~; tar -xf hos-5.1.0-*.tar
(seed)$ cd ~/hos-5.1.0; ./hos-init.bash</screen>
 </step>
 <step>
  <para>
   Prepare for running playbooks:
  </para>
  <screen>(seed)$ cd ~/helion/hos/ansible
(seed)$ ansible-playbook -i hosts/localhost config-processor-run.yml
(seed)$ ansible-playbook -i hosts/localhost ready-deployment.yml</screen>
 </step>
 <step>
  <para>
   Initialize ansible host facts and ensure that existing cloud is fully
   addressable by new deployer:
  </para>
  <screen>(seed)$ cd ~/scratch/ansible/next/hos/ansible
(seed)$ ansible-playbook -i hosts/verb_hosts hlm-refresh-facts.yml</screen>
 </step>
 <step>
  <para>
   Prune the primary database to remove data that is no longer relevant:
  </para>
<screen>(seed)$ cd ~/scratch/ansible/next/hos/ansible
(seed)$ ansible-playbook -i hosts/verb_hosts percona-prune.yml</screen>
 </step>
</procedure>
<procedure>
<title>Migration Phase</title>

<warning>
 <title>Downtime</title>
 <para>
  Downtime will be in effect until all the following steps are completed
 </para>
</warning>

 <step>
  <para>
   Stop or pause OpenStack services on the control plane (excluding the seed have been freshly imaged)
  </para>
  <screen>(seed)$ cd ~/scratch/ansible/next/hos/ansible
(seed)$ ./hlm-quiesce.sh --limit '!<replaceable>SEED_HOST</replaceable>'</screen>
  <para>
   Note that the <replaceable>SEED_HOST</replaceable> name should match exactly the hostname in the <filename>hosts/verb_hosts</filename> inventory, such as <literal>helion-cp1-c1-m1</literal>
  </para>
 </step>
 <step>
  <para>
   Create a database backup by retrieving MySQL database contents from another node (not the seed) in the database cluster:
  </para>
  <screen>(seed)$ cd ~/scratch/ansible/next/hos/ansible
(seed)$ ansible-playbook -i hosts/verb_hosts percona-vertica-removal-cleanup.yml
(seed)$ ansible-playbook -i hosts/verb_hosts percona-export.yml -e dbcontent=~/mysql_dump.sql</screen>
 </step>
 <step>
  <para>
   Reimage remaining servers in control plane
  </para>
  <itemizedlist>
   <listitem>
    <para>
     Reinstall the OS's on control plane primary disks using an ISO, cobbler,
     or other provisioner. If using cobbler:
    </para>
    <screen>(seed)$ cd ~/helion/hos/ansible
(seed)$ ansible-playbook -i hosts/localhost cobbler-deploy.yml
(seed)$ ansible-playbook -i hosts/localhost prepare-sles-grub2.yml -e nodelist=<replaceable>NODE_IDs</replaceable>
(seed)$ ansible-playbook -i hosts/localhost bm-reimage.yml -e nodelist=<replaceable>NODE_IDs</replaceable></screen>
  <para>
   Where <replaceable>NODE_IDs</replaceable> are comma-separated IDs of all non-seed control plane and deployer nodes from <filename>servers.yml</filename>
  </para> 
  </listitem>
  <listitem>
   <para>
        Otherwise reformat disk using partitioning to match the current configuration
   </para>
  </listitem>
  </itemizedlist>
 </step>
 <step>
  <para>
   Perform HOS 5.SCP installation using flags to enable database import:
  </para>
  <screen>(seed)$ cd ~/scratch/ansible/next/hos/ansible
(seed)$ ansible-playbook -i hosts/verb_hosts site.yml -e dbcontent=~/mysql_dump.sql</screen>
 </step>
 <step>
  <para>
   Optionally, if 3Par backend was configured previously, please follow the procedures to enable it again.
  </para>
 </step>
</procedure>
<procedure>
 <title>Cleanup Phase</title>

 <note>
  <title>No Downtime</title>
  <para>
   These steps do not incur service downtime and can take place any time while the cloud is up and running.
  </para>
 </note>
 <step>
  <para>
   Delete database backup from seed node:
  </para>
  <screen>(seed)$ rm ~/mysql_dump.sql</screen>
 </step>
 <step>
  <para>
   If desired, transfer deployer role in the HOS 5.SCP cloud to a different node, such as one that originally served as a dedicated deployer
  </para>
  <itemizedlist>
   <listitem>
    <para>
     If the deployer will be moving to an entirely different cluster (for
     example, Database cluster to Controller cluster or standalone Deployer cluster), additional software packages will need to be installed
    </para>
    <itemizedlist>
     <listitem>
      <para>
       Edit
       <filename>~/helion/my_cloud/definition/data/control_plane.yml</filename>
       and move the <systemitem>lifecycle-manager</systemitem> from its current
       position into the new desired cluster service-components list. Commit the change to the git model:
      </para>
      <screen>(seed)$ git commit -a -m "Enable lifecycle-manager for targeted seed node"</screen>
     </listitem>
     <listitem>
      <para>
       Kill the DHCP services running on the current seed:
      </para>
      <screen>(seed)$ sudo pkill dhcpd</screen>
     </listitem>
     <listitem>
      <para>
       Install the lifecycle-manager services on the new deployer:
      </para>
      <screen>(seed)$ ansible-playbook -i hosts/verb_hosts site.yml -l <replaceable>NEW_DEPLOYER_HOST</replaceable></screen>
     <para>
      <replaceable>NEW_DEPLOYER_HOST</replaceable> could be for example <systemitem>helion-cp1-c0-m1</systemitem>
     </para>
     </listitem>
    </itemizedlist>
   </listitem>
   <listitem>
    <para>
     Copy the home folder from the seed to the new deployer:
    </para>
    <screen>(seed)$ rsync -avP ~ <replaceable>NEW_DEPLOYER_IP</replaceable>:/home</screen>
   </listitem>
   <listitem>
    <para>
     Untar or copy HOS 5.SCP contents to new deployer home directory
    </para>
   </listitem>
   <listitem>
    <para>
     Initialize the deployer node:
    </para>
    <screen>(new deployer)$ cd ~/hos-5.1.0; ./hos-init.bash</screen>
   </listitem>
  </itemizedlist>
 </step>
 
</procedure>
</sect1>
</article>
