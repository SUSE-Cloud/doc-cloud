<?xml version="1.0" encoding="UTF-8"?>
<?xml-stylesheet href="urn:x-suse:xslt:profiling:docbook50-profile.xsl"
 type="text/xml"
 title="Profiling step"?>
<!DOCTYPE article
[
  <!ENTITY % entities SYSTEM "entity-decl.ent">
    %entities;
]>
<article version="5.1" xml:lang="en" xml:id="art.hos.migration"
  xmlns="http://docbook.org/ns/docbook"
  xmlns:xi="http://www.w3.org/2001/XInclude"
  xmlns:xlink="http://www.w3.org/1999/xlink">
 <info>
  <title>Migration Guide</title><date>
<?dbtimestamp format="B d, Y"?></date>
<!-- <xi:include href="common_copyright_gfdl.xml"/> -->
<!--<xi:include href="authors.xml"/>-->
 </info>
 <important>
   <title>Use Latest Documentation</title>
   <para>
     Always use the latest online version of the documentation for the
     migration process.
   </para>
 </important>
 <sect1 xml:id="hos.scp.migration">
  <title>Migration from &hpecloud; 5.0.x to &hpecloud; 5.SCP</title>

  <para>
   The migration procedure is based on the following assumptions:
  </para>

  <itemizedlist>
   <listitem>
    <para>
     The customer is responsible for backing up all the control plane and
     deployer nodes to an external data store before starting the migration.
    </para>
    <important>
     <title>No Migration Rollback</title>
     <para>
      We do not support rollback of migration after the process has been
      started.
     </para>
    </important>
   </listitem>
   <listitem>
    <para>
     Metering, monitoring, and logging (MML) data will not be migrated (a clean
     installation will be done using a new MML back-end included with
     &hpecloud; 5.SCP).
    </para>
   </listitem>
   <listitem>
    <para>
     Upgrading will require re-imaging the operating system on the physical disks
     containing the root partition of your controller nodes. Existing data on these
     disks, which may include audit logs and &swift; objects, may be lost as a
     result. (Note that the primary OpenStack MySQL database will undergo a
     backup/restore as part of the upgrade process to ensure that its contents are
     preserved.)
    </para>
   </listitem>
   <listitem>
    <para>
     Review your existing partitioning model to understand how your data will be
     affected by re-imaging. Any data that is not on a disk separate from the
     primary root partition will be lost during re-imaging.
     &swift; objects and audit logs.
    </para>
   </listitem>
   <listitem>
    <para>
     All &compnode;s are running supported Linux distributions:
    </para>
    <itemizedlist>
     <listitem>
      <para>
       &sls; 12 SP3 for &slsa; &compnode;s
      </para>
     </listitem>
     <listitem>
      <para>
       &rhla; 7.3 for &rhla; &compnode;s
      </para>
     </listitem>
    </itemizedlist>
   </listitem>
   <listitem>
    <para>
     The cloud data model does not contain services which are not included in
     &productname; &productnumber;, such as Ceph and VSA.
    </para>
   </listitem>
   <listitem>
    <para>
     Any customizations to virtual environments used by &ostack; components will
     be lost during the upgrade process. These may include third party extensions,
     regardless of origin, that have not been included in
     &productname; &productnumber;.
    </para>
   </listitem>
   <listitem>
    <para>
     No significant issues or alarms are actively being reported by &o_monitor; and
     &opscon;.
    </para>
   </listitem>
  </itemizedlist>
  <para>
   Preparation Phase
  </para>
  <procedure>
   <step>
    <para>
     Because all Freezer content will be obsolete upon migrating to the new platform,
     it is recommended to stop Freezer jobs and eliminate historic backups.
    </para>
<screen><prompt>(old-deployer)> </prompt>source ~/backup.osrc
<prompt>(old-deployer)> </prompt>for id in $(freezer job-list -f value | awk '{print $1}')
  do freezer job-stop $id
done
<prompt>(old-deployer)> </prompt>for container in $(openstack container list -f value)
  for backup in $(openstack object list $container -f value)
    do openstack object delete $container $backup
  done
done</screen>
   </step>
   <step>
    <para>
     Obtain the &hpecloud; 5.SCP installation ISO and use the
     <command>mount</command>
     command to make its contents available on the existing deployer:
    </para>
<screen><prompt>(old-deployer)> </prompt>mkdir hos-5.scp
<prompt>(old-deployer)> </prompt>sudo mount <replaceable>HOS5SCP_ISO_FILE</replaceable> hos-5.scp
<prompt>(old-deployer)> </prompt>tar -xf hos-5.scp/hos/hlm-migration-tools.tar.gz</screen>
   </step>
   <step>
    <para>
     Run the validation playbook to ensure that the current cloud load and
     configuration is compatible with the migration process:
    </para>
<screen><prompt>(old-deployer)> </prompt>cd hlm-migration-tools/ansible
<prompt>(old-deployer)> </prompt>ansible-playbook \
  -i ~/scratch/ansible/next/hos/ansible/hosts/verb_hosts \
  pre-migration-validations.yml</screen>
    <para>
     The validation playbook checks that:
    </para>
    <itemizedlist>
     <listitem>
      <para>
       The &hpecloud; version number is either 5.0.1 or 5.0.3
      </para>
     </listitem>
     <listitem>
      <para>
       &productname; &productnumber;-compatible Linux distributions on
       &compnode;s
      </para>
     </listitem>
     <listitem>
      <para>
       System clocks are synchronized across cloud nodes
      </para>
     </listitem>
     <listitem>
      <para>
       HA capacity of &contrnode;s (L3/DHCP agent)
      </para>
     </listitem>
     <listitem>
      <para>
       Configuration and capacity of &contrnode;s (&swift;, &o_blockstore;)
      </para>
     </listitem>
     <listitem>
      <para>
       Data model compatibility with &productname; &productnumber; (no vsa, ceph,
       eon, or cmc-service)
      </para>
     </listitem>
    </itemizedlist>
    <warning>
     <title>Check Validation Results</title>
     <para>
      Review the contents of /var/log/pre-migration-validations.log immediately
      to discover if any errors were identified while running the validation playbook.
      Any identified issues must be resolved before proceeding with the migration.
     </para>
    </warning>
   </step>
   <step>
    <para>
     Identify a candidate seed &contrnode; in the primary control plane. This
     node must meet the following requirements:
    </para>
    <itemizedlist>
     <listitem>
      <para>
       Networking connectivity is equivalent to that of the existing deployer.
      </para>
     </listitem>
     <listitem>
      <para>
       It is a member of the database cluster (running the MySQL service).
      </para>
     </listitem>
     <listitem>
      <para>
       It is not actively hosting any singleton &ostack; services
       (<systemitem>cinder-volume</systemitem> or
       <systemitem>nova-consoleauth</systemitem>). Test for empty output with
       the following command:
      </para>
<screen><prompt>(each controller)> </prompt>ps -ef | egrep 'cinder-volume|nova-consoleauth'</screen>
     </listitem>
    </itemizedlist>
   </step>
   <step>
    <para>
     Consider the effect of control plane downtime on existing DHCP leases:
    </para>
    <itemizedlist>
     <listitem>
      <para>
       Default DHCP lease time is 48 hours as defined in the
       <filename>~/helion/hos/ansible/roles/neutron-common/templates/neutron.conf.j2</filename>
       file by the <systemitem>dhcp_lease_duration</systemitem> parameter.
      </para>
     </listitem>
     <listitem>
      <para>
       DHCP leases are administered by &o_netw; and dnsmasq. This
       functionality will not be available during the control plane downtime.
      </para>
     </listitem>
     <listitem>
      <para>
       DHCP clients typically renew their leases either after 50% of the
       lease time or upon reboot.
      </para>
     </listitem>
     <listitem>
      <para>
       Normally, this leaves a 24-hour window for control plane downtime at
       any point before workload's DHCP leases run out and cannot be renewed.
       While this is typically enough for performing a migration, increasing
       the DHCP lease time to 144 hours is recommended to ensure that lease
       expiration will not be a concern that affects your migration, even if
       unexpected delays in the process are encountered.
      </para>
     </listitem>
     <listitem>
      <para>
       To view the upcoming timeouts of DHCP leases, run the following
       command from each &contrnode;:
      </para>
<screen>for i in $(cat /var/run/neutron/dhcp/*/leases | awk '{print $1}'); do date -d @$i; done</screen>
     </listitem>
     <listitem>
      <para>
       To increase the DHCP lease time before migrating, open the
       <filename>~/helion/hos/ansible/roles/neutron-common/templates/neutron.conf.j2</filename>
       file and edit the <systemitem>dhcp_lease_duration</systemitem> parameter.
       Commit the change, and run the <filename>neutron-configure.yml</filename> playbook.
       Then, continue to monitor the upcoming DHCP lease expirations as described
       above until all existing leases have been refreshed to a lengthier period.
      </para>
     </listitem>
    </itemizedlist>
   </step>
   <step>
    <para>
     Prepare the cloud data model for migration:
    </para>
    <itemizedlist>
     <listitem>
      <para>
       In the <filename>~/helion/my_cloud/definition/data/disks_*.yml</filename>
       files used by your server role that includes monitoring services, replace
       the existing <literal>Vertica</literal> partition definition with two
       <literal>Cassandra</literal> partitions:
      </para>
<screen>- name: cassandra_db
  size: (whatever size was used originally by vertica partition, less 1%)
  mount: /var/cassandra/data
  fstype: ext4
  mkfs-opts: -O large_file
  consumer:
    name: vertica
- name: cassandra_log
  size: 1%
  mount: /var/cassandra/commitlog
  fstype: ext4
  mkfs-opts: -O large_file</screen>
     </listitem>
     <listitem>
      <para>
       Edit
       <filename>~/helion/my_cloud/definition/data/control_plane.yml</filename>
       and identify the cluster definition that already includes an entry for
       <filename>mysql</filename> in its service-components.
      </para>
     </listitem>
     <listitem>
      <para>
       Ensure that <systemitem>lifecycle-manager</systemitem> is included in
       the same cluster's service-components list. If it was necessary to add
       this as a new entry, it will also need to be removed from its original
       position in the deployer.
      </para>
     </listitem>
     <listitem>
      <para>
       Add a new entry for <systemitem>nova-placement-api</systemitem>
       alongside whichever cluster already includes
       <systemitem>nova-api</systemitem>.
      </para>
     </listitem>
     <listitem>
      <para>
       Update the <filename>servers.yml</filename> entry for
       all nodes in the control plane to specify <systemitem>distro-id:
       sles12sp3-x86_64</systemitem> in each one.
      </para>
     </listitem>
     <listitem>
      <para>
       Commit your changes to the Git model:
      </para>
<screen><prompt>(old-deployer)> </prompt>git commit -a -m "Prepare for HOS 5.1 installation"</screen>
     </listitem>
    </itemizedlist>
   </step>
   <step>
    <para>
     If the intended seed node also currently hosts &o_netw; L3 routing services,
     evacuate the routers from the node so that it can be re-imaged without
     interruption to tenant networks.
    </para>
    <itemizedlist>
     <listitem>
      <para>
       For non-DVR clouds, evacuate any existing &o_netw; L3 routers on the
       seed node:
      </para>
<screen><prompt>(old-deployer)> </prompt>cd ~/scratch/ansible/next/hos/ansible
<prompt>(old-deployer)> </prompt>cp -r ~/hlm-migration-tools/ansible/* ./
<prompt>(old-deployer)> </prompt>ansible-playbook -i hosts/verb_hosts \
  neutron-router-evacuate.yml --limit=<replaceable>SEED_HOST</replaceable></screen>
      <para>
       <replaceable>SEED_HOST</replaceable> represents the name of the seed node,
       for example, <systemitem>ardana-cp1-c1-m2</systemitem>.
      </para>
      <important>
       <title>Host Selection</title>
       <para>
        For correct behavior, you must use the <command>--limit</command>
        argument to select the target seed host. This will output a success or
        error message at the end of its run.
       </para>
      </important>
     </listitem>
    </itemizedlist>
   </step>
   <step>
    <para>
     Re-image the seed controller as a &slsa;-based system:
    </para>
    <itemizedlist>
     <listitem>
      <para>
       Display the current network connections of the seed controller (before
       re-imaging) for reference and save it locally.
      </para>
<screen><prompt>(seed)> </prompt>ip a</screen>
     </listitem>
     <listitem>
      <para>
       Reinstall the operating system on the seed node's primary disk using an
       ISO, cobbler (from the existing deployer), or other provisioner.
      </para>
     </listitem>
     <listitem>
      <para>
       If using cobbler, begin the seed node re-imaging by running:
      </para>
<screen><prompt>(old-deployer)> </prompt>cd ~/helion/hos/ansible
<prompt>(old-deployer)> </prompt>sudo cobbler system remove --name <replaceable>SEED_NODE_ID</replaceable>
<prompt>(old-deployer)> </prompt>ansible-playbook -i hosts/localhost cobbler-deploy.yml
<prompt>(old-deployer)> </prompt>ansible-playbook -i hosts/localhost bm-reimage.yml -e nodelist=<replaceable>SEED_NODE_ID</replaceable></screen>
      <para>
       Replace <replaceable>SEED_NODE_ID</replaceable> with the ID of the seed
       node from the <filename>servers.yml</filename> file.
      </para>
      <para>
       Cobbler will initialize the seed node's networking to use the IP address
       of your original deployer as the default gateway. This will negatively
       impact the network's operation later on in the migration, and should be
       updated after the system is up and running. On the new seed node, edit the
       file <filename>/etc/sysconfig/network/routes</filename> and change the address
       in the line starting with "default" to point to your environment's actual
       default gateway. (You may be able to copy this from the value specified in
       your existing deployer.) Then, restart networking with:
      </para>
<screen><prompt>(seed)> </prompt>sudo systemctl restart network</screen>
     </listitem>
     <listitem>
      <para>
       Otherwise, if installing manually or using a proprietary tool for
       provisioning, reformat the primary partition (preserving the overall
       partitioning scheme based on the data model) and install the &sls; 12 SP3
       operating system. Follow the instructions as described in
       <xref linkend="cha.depl.dep_inst"/>, but when creating the user account
       and group, use the name <systemitem class="username">stack</systemitem>
       instead of <systemitem class="username">ardana</systemitem>.
      </para>
     </listitem>
    </itemizedlist>

    <important>
     <title>Manual Installation</title>
     <para>
      If the server's boot mode is set to Legacy BIOS boot, then it is possible
      to use cobbler. However, if the boot mode of the server is set to UEFI
      boot, the operating system must be installed manually.
     </para>
     <para>
      You should also perform installation on the root disk and avoid
      removing any disks and partitions during the install. This way, if the
      seed node also originally contained &o_objstore; data, it will be
      left intact.
     </para>
    </important>

   </step>
   <step>
    <para>
     Transfer configuration and files from your existing cloud to the new seed node
    </para>
    <itemizedlist>
     <listitem>
      <para>
       Copy the entire home directory from the original deployer:
      </para>
<screen><prompt>(old-deployer)> </prompt>rsync -avP ~ <replaceable>SEED_IP</replaceable>:/home</screen>
     </listitem>
     <listitem>
      <para>
       If cobbler will be used from the new seed for provisioning the remaining
       cloud &contrnode;s:
      </para>
      <itemizedlist>
       <listitem>
        <para>
         Ensure that the seed node is able to reach IPMI interfaces
         referenced in the <filename>servers.yml</filename> file:
        </para>
<screen><prompt>(seed)> </prompt>ping <replaceable>IPMI_IP</replaceable></screen>
       </listitem>
       <listitem>
        <para>
         If you are not able to ping IPMI IP addresses because network accessibility
         to the IPMIs requires special network configuration and you would like to
         use &lcm; for that process based on your existing data model:
        </para>
<screen><prompt>(old-deployer)> </prompt>ansible-playbook -i hosts/verb_hosts \
  hlm-refresh-facts.yml
<prompt>(old-deployer)> </prompt>ansible-playbook -i hosts/verb_hosts -l <replaceable>SEED_NODE_ID</replaceable> \
  osconfig-run.yml</screen>
        <para>
         Replace <replaceable>SEED_NODE_ID</replaceable> with the ID of the
         seed node as listed in the <filename>hosts/verb_hosts</filename> file.
        </para>
       </listitem>
       <listitem>
        <para>
         Kill the DHCP services running on the old deployer:
        </para>
<screen><prompt>(old-deployer)> </prompt>sudo pkill dhcpd</screen>
       </listitem>
      </itemizedlist>
     </listitem>
    </itemizedlist>
   </step>
   <step>
    <para>
     Confirm the existence of package repository ISOs and mount the operating system
     image to make it available to the &hpecloud; installer.
    </para>
    <itemizedlist>
     <listitem>
      <important>
       <title>ISO filenames</title>
       <para>
        For correct behavior, the filenames of the images described below must match
        exactly their names in the user's home directory.
       </para>
      </important>
      <para>
       If your cloud includes &rhla; &compnode;s, ensure that
       <filename>~/rhel7.iso</filename> was either transferred from the original
       deployer or is downloaded to the user's home directory.
      </para>
      <para>
       Ensure that <filename>~/sles12sp3sdk.iso</filename> was either
       transferred from the original deployer or is downloaded to the user's home
       directory.
      </para>
      <para>
       Ensure that <filename>~/sles12sp3.iso</filename> was either transferred
       from the original deployer or is downloaded to the home directory on the
       seed node under the <filename>sles12sp3.iso</filename>. Then, mount it:
      </para>
<screen><prompt>(seed)> </prompt>sudo mkdir -p /media/cdrom
<prompt>(seed)> </prompt>echo /home/stack/sles12sp3.iso /media/cdrom iso9660 loop 0 0 \
  | sudo tee -a /etc/fstab
<prompt>(seed)> </prompt>sudo mount -a</screen>
      <para>
       If you installed the seed node using cobbler from the original deployer,
       edit the primary package repository on the new seed node to point it to
       the ISO media instead of a remote network location.
      </para>
<screen><prompt>(seed)> </prompt>sudo zypper rr SLES12-SP3-12.3-0
<prompt>(seed)> </prompt>sudo zypper ar file:/media/cdrom SLES12-SP3-12.3-0</screen>
     </listitem>
    </itemizedlist>
   </step>
   <step>
    <para>
     Install &hpecloud; 5.SCP on the seed node by downloading and unpacking
     &hpecloud; 5.SCP installation content:
    </para>
<screen><prompt>(seed)> </prompt>cd ~; tar -xf hos-5.scp/hos/hos-5.1.0-*.tar
<prompt>(seed)> </prompt>sudo mkdir -p /var/log/apache2
<prompt>(seed)> </prompt>cd ~/hos-5.1.0; ./hos-init.bash</screen>
   </step>
   <step>
    <para>
     Update repository references of &compnode; as appropriate to point to
     the seed node.
    </para>
    <itemizedlist>
     <listitem>
      <para>
       On each &slsa; compute node, remove outdated repositories that point
       to the old deployer. To do this, run <command>zypper lr -u</command> and
       remove listings referring to the original deployer's IP address
       using <command>zypper rr <replaceable>REPO_NAME</replaceable></command>.
       Note that the repositories <systemitem>SLES-OS</systemitem> and
       <systemitem>SLES-SDK</systemitem> should never be removed, as they are
       system entries that will automatically be updated to point to the new
       seed node during installation.
      </para>
     </listitem>
     <listitem>
      <para>
       On each &rhla; compute node, edit the repositories located in either
       <filename>/etc/yum.repos.d</filename> or <filename>/etc/yum/repos.d</filename>
       to redirect any IP addresses from the old deployer to point to the new
       seed node. (These references are likely to be found in the files
       <filename>cobbler-config.repo</filename> or <filename>rhel73.repo</filename>.)
      </para>
      <para>
       Then, refresh the yum cache by running:
      </para>
<screen><prompt>(rhel-compute)> </prompt>sudo yum clean all</screen>
     </listitem>
    </itemizedlist>
   </step>
   <step>
    <para>
     Prepare for running the migration playbook:
    </para>
<screen><prompt>(seed)> </prompt>cd ~/helion/hos/ansible
<prompt>(seed)> </prompt>ansible-playbook -i hosts/localhost config-processor-run.yml
<prompt>(seed)> </prompt>ansible-playbook -i hosts/localhost ready-deployment.yml</screen>
   </step>
   <step>
    <para>
     Initialize ansible host facts and ensure that the existing cloud is fully
     addressable by the new deployer:
    </para>
<screen><prompt>(seed)> </prompt>cd ~/scratch/ansible/next/hos/ansible
<prompt>(seed)> </prompt>ansible-playbook -i hosts/verb_hosts hlm-refresh-facts.yml</screen>
   </step>
   <step>
    <note>
     <title>&ostack; Clouds Only</title>
     <para>
      The following step is only necessary on &ostack; clouds and should not be
      used in standalone &swift; deployments.
     </para>
    </note>
    <para>
     Flush the &o_ident; token cache by running the following command on a node
     hosting the Keystone API service. (This may be the seed node in an entry-scale
     environment, or a dedicated controller node in a mid-scale environment.)
    </para>
<screen><prompt>(controller)> </prompt>sudo /opt/stack/service/keystone/venv/bin/keystone-manage token_flush</screen>
    <para>
     Then, prune the primary database to remove data that is no longer relevant:
    </para>
<screen><prompt>(seed)> </prompt>cd ~/scratch/ansible/next/hos/ansible
<prompt>(seed)> </prompt>ansible-playbook -i hosts/verb_hosts percona-prune.yml</screen>
   </step>
  </procedure>

  <procedure>
   <title>Migration Phase</title>
   <warning>
    <title>Downtime</title>
    <para>
     The cloud will not be available until all of the following steps are
     completed.
    </para>
   </warning>
   <step>
    <para>
     Stop or pause &ostack; services on the control plane (excluding the seed
     that has been freshly imaged):
    </para>
<screen><prompt>(seed)> </prompt>cd ~/scratch/ansible/next/hos/ansible
<prompt>(seed)> </prompt>./hlm-quiesce.sh --limit '!<replaceable>SEED_HOST</replaceable>'</screen>
    <para>
     The <replaceable>SEED_HOST</replaceable> name must exactly match the host
     name that is referenced in <filename>hosts/verb_hosts</filename>, such as
     <literal>ardana-cp1-c1-m1</literal>.
    </para>
   </step>
   <step>
    <para>
     Create a database backup by retrieving MySQL database contents from
     another node (not the seed) in the database cluster:
    </para>
<screen><prompt>(seed)> </prompt>cd ~/scratch/ansible/next/hos/ansible
<prompt>(seed)> </prompt>ansible-playbook -i hosts/verb_hosts percona-vertica-removal-cleanup.yml
<prompt>(seed)> </prompt>ansible-playbook -i hosts/verb_hosts percona-export.yml \
  -e dbcontent=~/mysql_dump.sql</screen>
   </step>
   <step>
    <para>
     Re-image the remaining servers in the control plane.
    </para>
    <itemizedlist>
     <listitem>
      <para>
       Reinstall the operating systems on the control plane primary disks using
       an ISO, cobbler, or other provisioner. If using cobbler:
      </para>
<screen><prompt>(seed)> </prompt>cd ~/openstack/ardana/ansible
<prompt>(seed)> </prompt>ansible-playbook -i hosts/localhost cobbler-deploy.yml
<prompt>(seed)> </prompt>ansible-playbook -i hosts/localhost prepare-sles-grub2.yml \
  -e nodelist=<replaceable>NODE_IDs</replaceable>
<prompt>(seed)> </prompt>ansible-playbook -i hosts/localhost bm-reimage.yml \
  -e nodelist=<replaceable>NODE_IDs</replaceable></screen>
      <para>
       In a mid-scale cloud topology, the original database content may have been
       stored on the non-primary drive in the system. If this was the case,
       the <filename>wipe_disk.yml</filename> playbook should be used after the
       database nodes have been re-imaged to purge the old data. For example:
      </para>
<screen>ansible-playbook -i hosts/verb_hosts wipe_disks.yml \
  --limit dbmq-m1,dbmq-m2,dbmq-m3</screen>
      <para>
       (Be sure to specify your own list of database nodes matching their
       identifiers from the <filename>servers.yml</filename> file.)
      </para>
      <para>
       Do not run the <filename>wipe_disk.yml</filename> playbook on any nodes
       containing &o_objstore; object storage.
      </para>
     </listitem>
     <listitem>
      <para>
       Otherwise, if installing manually or using a proprietary tool for
       provisioning, reformat the primary partition (preserving the overall
       partitioning scheme based on the data model) and install the &sls; 12 SP3
       operating system. Follow the instructions as described in
       <xref linkend="cha.depl.dep_inst"/>, but when creating the user account
       and group, use the name <systemitem class="username">stack</systemitem>
       instead of <systemitem class="username">ardana</systemitem>.
      </para>
     </listitem>
    </itemizedlist>
   </step>
   <step>
    <para>
     Perform the &hpecloud; 5.SCP installation using flags to enable database
     import from the snapshot that was captured from the cloud:
    </para>
<screen><prompt>(seed)> </prompt>cd ~/scratch/ansible/next/hos/ansible
<prompt>(seed)> </prompt>ansible-playbook -i hosts/verb_hosts site.yml -e dbcontent=~/mysql_dump.sql</screen>
   </step>
   <step>
    <para>
     At this time, the cloud should again be operational, with the exception
     of monitoring components that will remain offline until the upgrade to
     &productname; &productnumber; is fully complete.
    </para>
   </step>
  </procedure>

  <procedure>
   <title>Cleaning Up</title>
   <note>
    <title>No Downtime</title>
    <para>
     These steps do not cause service downtime and can be performed while the
     cloud is up and running.
    </para>
   </note>
   <step>
    <para>
     Delete the database backup from the seed node:
    </para>
<screen><prompt>(seed)> </prompt>rm ~/mysql_dump.sql</screen>
   </step>
   <step>
    <para>
     If desired, transfer the deployer role in the &hpecloud; 5.SCP cloud to a
     different node, such as one that originally served as a dedicated
     deployer.
    </para>
    <itemizedlist>
     <listitem>
      <para>
       If the deployer will be moving to an entirely different cluster (for
       example, Database cluster to Controller cluster or standalone Deployer
       cluster), additional software packages need to be installed.
      </para>
      <itemizedlist>
       <listitem>
        <para>
         Edit
         <filename>~/helion/my_cloud/definition/data/control_plane.yml</filename>
         and move the <systemitem>lifecycle-manager</systemitem> from its
         current location into the new desired cluster's service-components list.
         Commit the change:
        </para>
<screen><prompt>(seed)> </prompt>git commit -a -m "Enable lifecycle-manager for new deployer"</screen>
       </listitem>
       <listitem>
        <para>
         Execute these steps to prepare for running playbooks:
        </para>
<screen>
<prompt>(seed)> </prompt>cd ~/helion/hos/ansible
<prompt>(seed)> </prompt>ansible-playbook -i hosts/localhost config-processor-run.yml
<prompt>(seed)> </prompt>ansible-playbook -i hosts/localhost ready-deployment.yml</screen>
       </listitem>
       <listitem>
        <para>
         Install the <systemitem>lifecycle-manager</systemitem> services on the
         new deployer and remove cloud-wide references to the seed:
        </para>
<screen><prompt>(seed)> </prompt> cd ~/scratch/ansible/next/hos/ansible
<prompt>(seed)> </prompt>ansible-playbook -i hosts/verb_hosts site.yml -l <replaceable>NEW_DEPLOYER_HOST</replaceable>
<prompt>(seed)> </prompt>ansible resources -i hosts/verb_hosts -a "zypper rr SLES12-SP3-12.3-0" --become</screen>
        <para>
         <replaceable>NEW_DEPLOYER_HOST</replaceable> must match the node's identity in the
         file <filename>hosts/verb_hosts</filename> (for example,
         <systemitem>ardana-cp1-c0-m1</systemitem>)
        </para>
       </listitem>
      </itemizedlist>
     </listitem>
     <listitem>
      <para>
       Stop the deployer-related services and repositories running on the current seed:
      </para>
<screen><prompt>(seed)> </prompt>sudo systemctl stop dhcpd
<prompt>(seed)> </prompt>sudo systemctl disable dhcpd
<prompt>(seed)> </prompt>sudo systemctl stop cobblerd
<prompt>(seed)> </prompt>sudo systemctl disable cobblerd
<prompt>(seed)> </prompt>sudo sed -i /sles12sp3.iso/d /etc/fstab
<prompt>(seed)> </prompt>sudo umount /media/cdrom</screen>
     </listitem>
     <listitem>
      <para>
       Copy the user's home directory from the seed to the new deployer:
      </para>
<screen><prompt>(seed)> </prompt>rsync -avP ~/ <replaceable>NEW_DEPLOYER_IP</replaceable>:/var/lib/ardana/</screen>
     </listitem>
     <listitem>
      <para>
       Connect to the new deployer and initialize it for use:
      </para>
<screen><prompt>(new-deployer)> </prompt>cd ~
<prompt>(new-deployer)> </prompt>sudo mkdir -p /media/cdrom
<prompt>(new-deployer)> </prompt>echo /var/lib/ardana/sles12sp3.iso /media/cdrom iso9660 loop 0 0 \
  | sudo tee -a /etc/fstab
<prompt>(new-deployer)> </prompt>sudo mount -a
<prompt>(new deployer)> </prompt>tar -xf hos-5.scp/hos/hos-5.1.0-*.tar
<prompt>(new deployer)> </prompt>cd ~/hos-5.1.0
<prompt>(new deployer)> </prompt>./hos-init.bash
<prompt>(new deployer)> </prompt>cd ~/helion/hos/ansible
<prompt>(new deployer)> </prompt>ansible-playbook -i hosts/localhost cobbler-deploy.yml
<prompt>(new-deployer)> </prompt>ansible-playbook -i hosts/localhost config-processor-run.yml
<prompt>(new-deployer)> </prompt>ansible-playbook -i hosts/localhost ready-deployment.yml</screen>
     </listitem>
     <listitem>
      <para>
       On each &slsa; compute node, remove all repositories that point to the
       seed node. To do this, run <command>zypper lr -u</command> and
       remove any listings referring to the original deployer's IP address
       using <command>zypper rr <replaceable>REPO_NAME</replaceable></command>.
      </para>
     </listitem>
     <listitem>
      <para>
       On each &rhla; compute node, edit the repositories located in either
       <filename>/etc/yum.repos.d</filename> or <filename>/etc/yum/repos.d</filename>
       to redirect any IP addresses from the seed node to point to the new
       deployer. (These references are likely to be found in the files
       <filename>cobbler-config.repo</filename> or <filename>rhel73.repo</filename>.)
      </para>
      <para>
       Then, refresh the yum cache by running:
      </para>
<screen><prompt>(rhel-compute)> </prompt>sudo yum clean all</screen>
     </listitem>
     <listitem>
      <para>
       Use the new deployer to inform all other cloud nodes of its new role.
      </para>
<screen><prompt>(new-deployer)> </prompt>cd ~/scratch/ansible/next/hos/ansible
<prompt>(new-deployer)> </prompt>ansible-playbook -i hosts/verb_hosts osconfig-run.yml</screen>
     </listitem>
    </itemizedlist>
   </step>
  </procedure>
 </sect1>
 <sect1 xml:id="ardana.newton.migration">
  <title>Migration from &hpecloud; 5.SCP to Ardana.Newton</title>

  <para>
   The following procedure describes the process of migrating from &hpecloud;
   5.SCP to Ardana.Newton.
  </para>
  <para>
   From this point forward, all commands should be executed on the new deployer
   node (which will be the same as the earlier seed node unless this role was
   explicitly transferred using the instructions at the end of the prior section).
  </para>
  <procedure>
   <title>Preparation</title>
   <step xml:id="st.migrate.git-convert">
    <para>
     Convert the <filename>helion</filename> Git repository to an
     <filename>openstack</filename> Git repository for use by Ardana:
    </para>
<screen>cd ~
cp -r helion openstack
cd openstack
git checkout site</screen>
    <para>
     If your environment included a SES storage backend, remove all references
     to it from your data model. This may be done by using <command>git revert
     <replaceable>COMMIT-ID</replaceable></command> to revert a specific commit
     including the SES backend definition, or <command>git checkout hos --
     <replaceable>FILENAME</replaceable></command> to restore an entire file to
     its original version (followed by <command>git diff --staged</command> to
     inspect the difference between files and ensure that you ar satisfied with
     the final condition of the configuration file.) Committing these edits will
     be necessary for the following files in the <filename>~/openstack</filename>
     git repository:
    </para>
    <itemizedlist>
     <listitem>
      <para>
       <filename>hos/ansible/roles/_CND-CMN/templates/cinder.conf.j2</filename>
      </para>
     </listitem>
     <listitem>
      <para>
       <filename>hos/ansible/roles/NOV-CMP-KVM/templates/hypervisor.conf.j2</filename>
      </para>
     </listitem>
     <listitem>
      <para>
       <filename>hos/ansible/roles/GLA-API/templates/glance-api.conf.j2</filename>
       (if the SES backend was additionally configured for use with &o_img;)
      </para>
     </listitem>
    </itemizedlist>
    <para>
     Save a record of all other manual changes that have been previously committed
     to the <filename>hos/ansible</filename> directory, as you will need to
     reapply them later:
    </para>
<screen>git diff hos -- hos/ansible > /tmp/hos.diff</screen>
    <para>
     Forcibly undo all the changes identified:
    </para>
<screen>rm -rf hos/ansible
git checkout hos -- hos/ansible</screen>
    <para>
     Reset the codebase to make it ready for Ardana, then transform the rest of
     the repository:
    </para>
<screen>
git commit -a -m "Resetting codebase in prep for Ardana"
git branch -m hos ardana
git checkout ardana
sed -i 's/hos/ardana/' .gitignore
git commit -a -m "Prep for Ardana"
git mv hos ardana
git mv hos_extensions ardana_extensions
for i in $(find -L my_cloud/config -xtype l)
  do ln -sfn $(readlink $i | sed 's/\/hos\//\/ardana\//') $i
done
git commit -a -m "Conversion to Ardana"
git checkout cp-persistent
sed -i 's/component: helion-ca/component: ardana-ca/' \
my_cloud/persistent_state/private_data_control-plane-1.yml
sed -i 's/helion_internal_ca/ardana_internal_ca/' \
my_cloud/persistent_state/private_data_control-plane-1.yml
git commit -a -m "Conversion to Ardana"
git checkout site
git merge ardana -m "Conversion to Ardana"
</screen>
    <para>
     Reinstate the changes that were originally made to the contents of the
     <filename>hos/ansible</filename> directory
    </para>
<screen>sed 's/ \([ab]\?[\/]\?\)hos\/ansible/ \1ardana\/ansible/g' /tmp/hos.diff | git apply</screen>
    <para>
     If merge conflicts are reported, some files may need to be edited manually
     to reconcile the difference between the HOS and Ardana playbooks with
     respect to manual changes previously made to the repository. Once this is
     completed, continue to commit all updates:
    </para>
<screen>git add -A
git commit -a -m "Reinstated hos playbook customizations (batch)"
</screen>
   </step>
   <step>
    <para>
     Remove and archive the &hpecloud; 5.SCP-related content:
    </para>
<screen>mv ~/scratch ~/scratch.scp
sudo rm /etc/apache2/sites-enabled/deployer_venv_server.conf</screen>
   </step>
   <step>
    <para>
     Prepare the <filename>ardana_packager</filename> working directory to
     include existing &suse; repositories:
    </para>
<screen>sudo mkdir -p /opt/ardana_packager/ardana/sles12/zypper
sudo ln -s /opt/hlm_packager/hlm /opt/ardana_packager/hlm
sudo ln -s /opt/hlm_packager/hlm/sles12/zypper/OS \
  /opt/ardana_packager/ardana/sles12/zypper/OS</screen>
   </step>
   <step>
    <para>
     Install Ardana.Newton content onto the deployer by mounting the installation
     media and running the following commands:
    </para>
<screen>cd ~
sudo mkdir -p /media/ardana-0.35.0
sudo mount ~/ardana-0.35.0*.iso /media/ardana-0.35.0
tar -xf /media/ardana-0.35.0/ardana/ardana-0.35.0*.tar
cd ardana-0.35.0
./ardana-init.bash</screen>
   </step>
   <step>
    <para>
     Install Ardana.Newton:
    </para>
<screen>cd ~/openstack/ardana/ansible
ansible-playbook -i hosts/localhost config-processor-run.yml
ansible-playbook -i hosts/localhost ready-deployment.yml

cd ~/scratch.scp/ansible/next/hos/ansible/
ansible-playbook -i hosts/verb_hosts osconfig-iptables-rename.yml
ansible -i hosts/verb_hosts resources --become \
  -a "sed -i 's/helion/ardana/' /etc/iproute2/rt_tables"</screen>
    <para>
     Because the OpenStack service versions are comparable between &hpecloud;
     5.0.x and Ardana.Newton, it is not necessary to run the entire
     deploy/upgrade playbook for Ardana.Newton at this time.
    </para>
   </step>
  </procedure>
 </sect1>
 <sect1 xml:id="ardana.pike.migration">
  <title>Migration from Ardana.Newton to Ardana.Pike</title>
  <procedure>
   <step>
    <para>
     Create the <systemitem>ardana</systemitem> user throughout all nodes in the
     cloud:
    </para>
<screen>cd ~/scratch/ansible/next/ardana/ansible
ansible-playbook -i hosts/verb_hosts ardana-user-create.yml</screen>
   </step>
   <step>
    <para>
     Mount the &productname; &productnumber; installation media on the host
     (after copying the ISO file into the user's home directory)
    </para>
<screen>sudo mkdir -p /media/cloud
echo $(ls ~/HPE-HELION-*.iso) /media/cloud iso9660 loop 0 0 \
    | sudo tee -a /etc/fstab
sudo mount -a</screen>
   </step>
   <step>
    <para>
     Add the contents of the ISO as a Zypper repository:
    </para>
<screen>sudo zypper ar -G -f file:/media/cloud Cloud
sudo zypper refresh</screen>
   </step>
   <step>
    <para>
     Begin the installation:
    </para>
<screen>sudo zypper -n in patterns-cloud-ardana ardana-ses
sudo su - ardana
ARDANA_INIT_AUTO=1 /usr/bin/ardana-init</screen>
   </step>
   <step>
    <para>
     This procedure creates the directory
     <filename>/srv/www/suse-12.3/x86_64/repos/PTF</filename> and adds it as
     a software repository. To enable a successful migration, you must add
     required Program Temporary Fix (PTF) RPM packages to this repository
     manually. These packages are provided to you by &suse; via an active
     L3 support contract.
    </para>
   </step>
   <step>
    <para>
     Apply the PTF packages obtained in the previous step and confirm
     that they install correctly:
    </para>
<screen>&prompt.ardana;cd ~/openstack/ardana/ansible
&prompt.ardana;ansible-playbook -i hosts/localhost deployer-init.yml
&prompt.sudo;zypper refresh PTF
&prompt.sudo;zypper dup --from PTF
&prompt.ardana;rpm -qa | grep PTF</screen>
    <para>
     Ensure that all migration RPMs available to you via your &suse; support
     contract are present in the resulting list. If any are missing, repeat
     the above process or contact technical support before continuing.
    </para>
   </step>
   <step>
    <para>
     Apply the updates received via PTF packages to your active cloud:
    </para>
<screen>/usr/bin/ardana-init</screen>
   </step>
   <step>
    <para>
     Adjust data model to install <systemitem>cassandra</systemitem> instead
     of <systemitem>vertica</systemitem> (continuing as
     <systemitem>ardana</systemitem> user):
    </para>
<screen>cd ~/openstack/ardana/ansible</screen>
    <para>
     Manually replace <literal>vertica</literal> with <literal>cassandra</literal>
     in the service-components list, and remove any entry for
     <literal>ops-console-monitor</literal>:
    </para>
<screen>vi ../../my_cloud/definition/data/control_plane.yml</screen>
    <para>
     In files where a <systemitem>cassandra_db</systemitem> partition
     exists, manually update its consumer name from <literal>vertica</literal> to
     <literal>cassandra</literal> and add an identical
     <literal>consumer</literal> block to the
     <systemitem>cassandra_log</systemitem> partition definition:
    </para>
<screen>vi ../../my_cloud/definition/data/disks_*</screen>
    <para>
     If your environment includes a SES backend, add configuration files to enable
     it based on the <xref linkend="ses.integration"/> instructions. (However,
     the <filename>site.yml</filename> and
     <filename>ardana-reconfigure.yml</filename> playbooks should not be used at
     this time.)
    </para>
    <para>
     Save the updates to the data model:
    </para>
<screen>git commit -a -m 'Updated data model'</screen>
   </step>
   <step>
    <para>
     Build the <filename>~/scratch</filename> directory for the upgrade:
    </para>
<screen>ansible-playbook -i hosts/localhost config-processor-run.yml
ansible-playbook -i hosts/localhost ready-deployment.yml</screen>
   </step>
   <step>
    <para>
     Prepare the servers for Pike infrastructure:
    </para>
<screen>cd ~/scratch/ansible/next/ardana/ansible
ansible-playbook -i hosts/verb_hosts FND-AP2-vhosts-transition.yml</screen>
   </step>
   <step>
    <para>
     Set up package repositories for installation:
    </para>
<screen>sudo ln -s /media/cloud /srv/www/suse-12.3/x86_64/repos/Cloud
sudo ln -s /opt/hlm_packager/hlm/sles12/zypper/extras/sles12sp3-ceph-updates \
  /srv/www/suse-12.3/x86_64/repos/sles12sp3-ceph-updates
ansible-playbook -i hosts/verb_hosts ardana-replace-legacy-repos.yml</screen>
   </step>
   <step>
   <para>
    Run the &productname; &productnumber; upgrade playbook:
   </para>
<screen>ansible-playbook -i hosts/verb_hosts -vvv ardana-upgrade-from-legacy.yml</screen>
    <para>
     Note that the <filename>ardana-upgrade-from-legacy.yml</filename> playbook is
     not guaranteed to be idempotent, and re-running it upon a failure may produce
     new errors. If this playbook fails, please contact customer support and be
     prepared to share details of the failure from the contents of
     <filename>~/.ansible/ansible.log</filename> to determine appropriate next
     steps.
    </para>
   </step>
   <step>
    <para>
     After the base installation has completed, the PTF and other temporary packages
     can be removed:
    </para>
<screen>cd ~/scratch/ansible/next/ardana/ansible
ansible -i hosts/verb_hosts resources --become -m shell \
  -a "if [ -x /usr/bin/zypper ]; then zypper rr sles12sp3-ceph-updates; fi"
sudo rm /srv/www/suse-12.3/x86_64/repos/sles12sp3-ceph-updates
sudo rm /srv/www/suse-12.3/x86_64/repos/PTF/*
cd ~/openstack/ardana/ansible
ansible-playbook -i hosts/localhost deployer-init.yml
sudo zypper refresh PTF
sudo zypper dup --from Cloud</screen>
   </step>
   <step>
    <para>
     Now that the upgrade has completed, it will be necessary to reboot the nodes
     in your cloud for changes to fully take effect and re-enable normal behavior.
     Reboot your cloud per the instructions in <xref linkend="stop_restart"/>.
    </para>
   </step>
   <step>
    <para>
     Determine how to handle modified policy files which restrict the admin
     user's privileges more tightly than in previous releases:
    </para>
    <para>
     With this upgrade, the policy.json files have been aligned with
     &ostack-current;. When running &ostack; commands an operator may see
     messages saying <literal>Policy doesn't allow ...</literal> such as:
    </para>
    <screen>Policy doesn't allow volume_extension:services:index to be
    performed</screen>
    <para>
     In this case the operator can be given permission by adding a role to the
     user using the <command>openstack role add</command> command, for example:
    </para>
    <screen>&prompt.ardana;openstack role add -/-user
    <replaceable>USER_NAME</replaceable> -/-project
    <replaceable>PROJECT_NAME</replaceable> cinder_admin</screen>
    <para>
     Role assignments should be carefully considered. In an environment which
     requires separation of duties, each operator should be assigned only the
     roles they require to perform their duties and should use their own user
     account to perform their administrative tasks. For more information see
     the <xref linkend="roleSegregation"/>.
    </para>
   </step>
  </procedure>
 </sect1>
 <sect1>
  <title>Applying Product Updates</title>
  <procedure>
   <step>
    <para>
     Before applying updates to your cloud, you should replace the repositories
     configured on each node during the migration with links to an &smt; server
     that mirrors the latest versions of your software.
    </para>
    <para>
     The easiest way to provide the required repositories on the &lcm; Server is
     to set up an &smt; server as described in
     <xref linkend="app.deploy.smt_lcm"/>. Alternatives to setting up an
     &smt; server are described in <xref linkend="cha.depl.repo_conf_lcm"/>.
    </para>
    <para>
     Before continuing, ensure that the "Cloud" repository has been replaced
     by the following repositories on all nodes:
    </para>
    <itemizedlist>
     <listitem>
      <para>
       SLES12-SP3-Pool
      </para>
     </listitem>
     <listitem>
      <para>
       SLES12-SP3-Updates
      </para>
     </listitem>
     <listitem>
      <para>
       SUSE-OpenStack-Cloud-8-Pool
      </para>
     </listitem>
     <listitem>
      <para>
       SUSE-OpenStack-Cloud-8-Updates
      </para>
     </listitem>
    </itemizedlist>
   </step>
   <step>
    <para>
     Apply the latest versions of software packages using the process described in
     <xref linkend="maintenance_update"/>.
    </para>
   </step>
  </procedure>
 </sect1>
</article>
