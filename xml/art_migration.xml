<?xml version="1.0" encoding="UTF-8"?>
<?xml-stylesheet href="urn:x-suse:xslt:profiling:docbook50-profile.xsl"
 type="text/xml"
 title="Profiling step"?>
<!DOCTYPE article
[
  <!ENTITY % entities SYSTEM "entity-decl.ent">
    %entities;
]>
<article version="5.1" xml:lang="en" xml:id="art.hos.migration"
  xmlns="http://docbook.org/ns/docbook"
  xmlns:xi="http://www.w3.org/2001/XInclude"
  xmlns:xlink="http://www.w3.org/1999/xlink">
 <info>
  <title>Migration Guide</title><date>
<?dbtimestamp format="B d, Y"?></date>
<!-- <xi:include href="common_copyright_gfdl.xml"/> -->
<!--<xi:include href="authors.xml"/>-->
 </info>
 <important>
   <title>Use Latest Documentation</title>
   <para>
     Always use the latest online version of the documentation for the
     migration process.
   </para>
 </important>
 <sect1 xml:id="hos.scp.migration">
  <title>Migration from &hpecloud; 5.0.x to &hpecloud; 5.SCP</title>

  <para>
   The migration procedure is based on the following assumptions:
  </para>

  <itemizedlist>
   <listitem>
    <para>
     The customer is responsible for backing up all the control plane and
     deployer nodes to an external data store before starting the migration.
    </para>
    <important>
     <title>No Migration Rollback</title>
     <para>
      We do not support rollback of migration after the process has been
      started.
     </para>
    </important>
   </listitem>
   <listitem>
    <para>
     Metering, monitoring, and logging (MML) data will not be migrated (a clean
     installation will be done using a new MML back-end included with
     &hpecloud; 5.SCP).
    </para>
   </listitem>
   <listitem>
    <para>
     Upgrading will require re-imaging the operating system on the physical disks
     containing the root partition of your controller nodes. Existing data on these
     disks, which may include audit logs and &swift; objects, may be lost as a
     result. (Note that the primary OpenStack MySQL database will undergo a
     backup/restore as part of the upgrade process to ensure that its contents are
     preserved.)
    </para>
   </listitem>
   <listitem>
    <para>
     Review your existing partitioning model to understand how your data will be
     affected by re-imaging. Any data that is not on a disk separate from the
     primary root partition will be lost during re-imaging.
     &swift; objects and audit logs.
    </para>
   </listitem>
   <listitem>
    <para>
     All &compnode;s are running supported Linux distributions:
    </para>
    <itemizedlist>
     <listitem>
      <para>
       &sls; 12 SP3 for &slsa; &compnode;s
      </para>
     </listitem>
     <listitem>
      <para>
       &rhla; 7.3 for &rhla; &compnode;s
      </para>
     </listitem>
    </itemizedlist>
   </listitem>
   <listitem>
    <para>
     The cloud data model does not contain services which are not included in
     &productname; &productnumber;, such as Ceph and VSA.
    </para>
   </listitem>
   <listitem>
    <para>
     Any customizations to virtual environments used by OpenStack components will
     be lost during the upgrade process. These may include third party extensions,
     regardless of origin, that have not been included in
     &productname; &productnumber;
    </para>
   </listitem>
   <listitem>
    <para>
     No significant issues or alarms are actively being reported by Monasca and
     OpsConsole
    </para>
   </listitem>
  </itemizedlist>

  <procedure>
   <title>Preparation Phase</title>
   <step>
    <para>
     Obtain the &hpecloud; 5.SCP installation ISO and use the
     <command>mount</command>
     command to make its contents available on the existing deployer:
    </para>
    <screen><prompt>(old-deployer)> </prompt>mkdir hos-5.scp
<prompt>(old-deployer)> </prompt>sudo mount <replaceable>HOS5SCP_ISO_FILE</replaceable> hos-5.scp
<prompt>(old-deployer)> </prompt>tar -xvf hos-5.scp/hos/hlm-migration-tools.tar.gz ./
    </screen>
   </step>
   <step>
    <para>
     Run the validation playbook to ensure that the current cloud load and
     configuration is compatible with the migration process:
    </para>
<screen><prompt>(old-deployer)> </prompt>cd hlm-migration-tools/ansible
<prompt>(old-deployer)> </prompt>ansible-playbook \
  -i ~/scratch/ansible/next/hos/ansible/hosts/verb_hosts \
  pre-migration-validations.yml
  </screen>
    <para>
     The validation playbook checks that:
    </para>
    <itemizedlist>
     <listitem>
      <para>
       The &hpecloud; version number is either 5.0.1 or 5.0.3
      </para>
     </listitem>
     <listitem>
      <para>
       &productname; &productnumber;-compatible Linux distributions on
       &compnode;s
      </para>
     </listitem>
     <listitem>
      <para>
       System clocks are synchronized across cloud nodes
      </para>
     </listitem>
     <listitem>
      <para>
       HA capacity of &contrnode;s (L3/DHCP agent)
      </para>
     </listitem>
     <listitem>
      <para>
       Configuration and capacity of &contrnode;s (&swift;, &o_blockstore;)
      </para>
     </listitem>
     <listitem>
      <para>
       Data model compatibility with &productname; &productnumber; (no vsa, ceph,
       eon, or cmc-service)
      </para>
     </listitem>
    </itemizedlist>
   </step>
   <step>
    <para>
     Identify a candidate seed &contrnode; in the primary control plane. This
     node must meet the following requirements:
    </para>
    <itemizedlist>
     <listitem>
      <para>
       Networking connectivity is equivalent to that of the existing deployer.
      </para>
     </listitem>
     <listitem>
      <para>
       It is a member of the database cluster (running the MySQL service).
      </para>
     </listitem>
     <listitem>
      <para>
       It is not actively hosting any singleton &ostack; services
       (<systemitem>cinder-volume</systemitem> or
       <systemitem>nova-consoleauth</systemitem>). Test for empty output with
       the following command:
      </para>
<screen><prompt>(each controller)> </prompt>ps -ef | egrep 'cinder-volume|nova-consoleauth'</screen>
     </listitem>
    </itemizedlist>
   </step>
   <step>
    <para>
     Prepare the cloud data model for migration:
    </para>
    <itemizedlist>
     <listitem>
      <para>
       In the <filename>~/openstack/my_cloud/definition/data/disks_*.yml</filename>
       files used by your server role that includes monitoring services, replace
       the existing <literal>Vertica</literal> partition definition with two
       <literal>Cassandra</literal> partitions:
      </para>
<screen>- name: cassandra_db
  size: (whatever size was used originally by vertica partition, less 1%)
  mount: /var/cassandra/data
  fstype: ext4
  mkfs-opts: -O large_file
  consumer:
    name: vertica
- name: cassandra_log
  size: 1%
  mount: /var/cassandra/commitlog
  fstype: ext4
  mkfs-opts: -O large_file
   </screen>
     </listitem>
     <listitem>
      <para>
       Edit
       <filename>~/openstack/my_cloud/definition/data/control_plane.yml</filename>
       and identify the cluster definition that already includes an entry for
       <filename>mysql</filename> in its service-components.
      </para>
     </listitem>
     <listitem>
      <para>
       Ensure that <systemitem>lifecycle-manager</systemitem> is included in
       the same cluster's service-components list. If it was necessary to add
       this as a new entry, it will also need to be removed from its original
       position in the deployer.
      </para>
     </listitem>
     <listitem>
      <para>
       Add a new entry for <systemitem>nova-placement-api</systemitem>
       alongside whichever cluster already includes
       <systemitem>nova-api</systemitem>.
      </para>
     </listitem>
     <listitem>
      <para>
       Commit your changes to the Git model:
      </para>
<screen><prompt>(deployer)> </prompt>git commit -a -m "Prepare for HOS 5.1 installation"</screen>
     </listitem>
     <listitem>
      <para>
       Consider the effect of control plane downtime on existing DHCP leases:
      </para>
      <itemizedlist>
       <listitem>
        <para>
         Default DHCP lease time is 48 hours as defined in the
         <filename>~/openstack/ardana/ansible/roles/neutron-common/templates/neutron.conf.j2</filename>
         file by the <systemitem>dhcp_lease_duration</systemitem> parameter.
        </para>
       </listitem>
       <listitem>
        <para>
         DHCP leases are administered by &o_netw; and dnsmasq. This
         functionality will not be available during the control plane downtime.
        </para>
       </listitem>
       <listitem>
        <para>
         DHCP clients typically renew their leases either after 50% of the
         lease time or upon reboot.
        </para>
       </listitem>
       <listitem>
        <para>
         Normally, this leaves a 24-hour window for control plane downtime at
         any point before workload's DHCP leases run out and cannot be renewed.
         This is enough for performing a migration.
        </para>
       </listitem>
       <listitem>
        <para>
         To view the upcoming timeouts of DHCP leases, run the following
         command from each &contrnode;:
        </para>
<screen>for i in $(cat /var/run/neutron/dhcp/*/leases | awk '{print $1}'); do date -d @$i; done</screen>
       </listitem>
       <listitem>
        <para>
         If it is necessary to increase the DHCP lease time before migrating,
         open the
         <filename>~/openstack/ardana/ansible/roles/neutron-common/templates/neutron.conf.j2</filename>
         and edit the <systemitem>dhcp_lease_duration</systemitem> parameter.
         Commit the change, and run the
         <filename>neutron-configure.yml</filename> playbook.
        </para>
       </listitem>
      </itemizedlist>
     </listitem>
    </itemizedlist>
   </step>
   <step>
    <para>
     If the intended seed node also currently hosts &o_netw; L3 routing services,
     evacuate the routers from the node so that it can be re-imaged without
     interruption to tenant networks.
    </para>
    <itemizedlist>
     <listitem>
      <para>
       For non-DVR clouds, evacuate any existing &o_netw; L3 routers on the
       seed node:
      </para>
<screen><prompt>(old-deployer)> </prompt>cd ~/scratch/ansible/next/hos/ansible
<prompt>(old-deployer)> </prompt>cp -r ~/hlm-migration-tools/ansible/* ./
<prompt>(old-deployer)> </prompt>ansible-playbook -i hosts/verb_hosts \
  neutron-router-evacuate.yml --limit=<replaceable>SEED_HOST</replaceable></screen>
      <para>
       <replaceable>SEED_HOST</replaceable> represents the name of the seed node,
       for example, <systemitem>ardana-cp1-c1-m2</systemitem>.
      </para>
      <important>
       <title>No Migration Rollback</title>
       <para>
        For correct behavior, you must use the <command>--limit</command>
        argument to select the target seed host. This will output a success or
        error message at the end of its run.
       </para>
      </important>
     </listitem>
    </itemizedlist>
   </step>
   <step>
    <para>
     Re-image the seed controller as a &slsa;-based system:
    </para>
    <itemizedlist>
     <listitem>
      <para>
       Display the current network connections of the seed controller (before
       re-imaging) for reference and save it locally.
      </para>
<screen><prompt>(seed)> </prompt>ip a</screen>
     </listitem>
     <listitem>
      <para>
       Reinstall the operating system on the seed node's primary disk using an
       ISO, cobbler (from the existing deployer), or other provisioner.
      </para>
     </listitem>
     <listitem>
      <para>
       If using cobbler, update the <filename>servers.yml</filename> entry for
       all nodes in the control plane to specify <systemitem>distro-id:
       sles12sp3-x86_64</systemitem> in each one, and commit the change to Git.
       Then, run:
      </para>
<screen><prompt>(old-deployer)> </prompt>cd ~/openstack/ardana/ansible
<prompt>(old-deployer)> </prompt>sudo cobbler system remove --name <replaceable>SEED_NODE_ID</replaceable>
<prompt>(old-deployer)> </prompt>ansible-playbook -i hosts/localhost cobbler-deploy.yml
<prompt>(old-deployer)> </prompt>ansible-playbook -i hosts/localhost bm-reimage.yml -e nodelist=<replaceable>SEED_NODE_ID</replaceable></screen>
      <para>
       Replace <replaceable>SEED_NODE_ID</replaceable> with the ID of the seed
       node from the <filename>servers.yml</filename> file.
      </para>
     </listitem>
     <listitem>
      <para>
       Otherwise, if installing manually or using a proprietary tool for
       provisioning, reformat the primary partition (preserving the overall
       partitioning scheme based on the data model) and install the &sls; 12 SP3
       operating system.
      </para>
     </listitem>
     <listitem>
      <para>
       Update repository references of &compnode; as appropriate to point to
       the seed node.
      </para>
      <itemizedlist>
       <listitem>
        <para>
         On each &slsa; compute node, if the file
         <filename>/etc/zypp/repos.d/SLES12-SP3-12.3-0.repo</filename> exists
         (pointing to the cobbler deployment repo on the original deployer),
         delete it:
        </para>
<screen><prompt>(sles-compute)> </prompt>sudo rm /etc/zypp/repos.d/SLES12-SP3-12.3-0.repo
<prompt>(sles-compute)> </prompt>sudo zypper refresh</screen>
        <para>
         On each &rhla; compute node, edit the file
         <filename>/etc/yum/repos.d/cobbler-config.repo</filename> (if
         &rhla; 7.3 was installed from scratch) or
         <filename>/etc/yum.repos.d/rhel73.repo</filename> (if
         &rhla; was upgraded to 7.3) and update any IP addresses previously
         pointing to the original deployer so that they are directed to the
         new seed node.
        </para>
        <para>
         Then, refresh the yum cache by running:
        </para>
<screen><prompt>(rhel-compute)> </prompt>sudo yum clean</screen>
       </listitem>
      </itemizedlist>
     </listitem>
    </itemizedlist>
   </step>
   <step>
    <para>
     Transfer configuration and files from your existing cloud to the new seed node
    </para>
    <itemizedlist>
     <listitem>
      <para>
       Copy the entire home directory from the original deployer:
      </para>
<screen><prompt>(old-deployer)> </prompt>rsync -avP ~ <replaceable>SEED_IP</replaceable>:/home</screen>
     </listitem>
     <listitem>
      <para>
       If your cloud includes &rhla; &compnode;s, ensure that
       <filename>~/rhel7.iso</filename> was either transferred from the original
       deployer or is downloaded to the user's home directory.
      </para>
      <para>
       Ensure that <filename>~/sles12sp3sdk.iso</filename> was either
       transferred from the original deployer or is downloaded to the user's home
       directory.
      </para>
     </listitem>
     <listitem>
      <para>
       If cobbler will be used from the new seed for provisioning the remaining
       cloud &contrnode;s:
      </para>
      <itemizedlist>
       <listitem>
        <para>
         Ensure that the seed node is able to reach iLO interfaces
         referenced in the <filename>servers.yml</filename> file:
        </para>
<screen><prompt>(seed)> </prompt>ping <replaceable>ILO_IP</replaceable></screen>
       </listitem>
       <listitem>
        <para>
         If you are not able to ping iLO IP addresses because network accessibility
         to the iLOs requires special network configuration and you would like to
         use &lcm; for that process based on your existing data model:
        </para>
<screen><prompt>(old-deployer)> </prompt>ansible-playbook -i hosts/verb_hosts \
  hlm-refresh-facts.yml
<prompt>(old-deployer)> </prompt>ansible-playbook -i hosts/verb_hosts -l <replaceable>SEED_NODE_ID</replaceable> \
  osconfig-run.yml</screen>
        <para>
         Replace <replaceable>SEED_NODE_ID</replaceable> with the ID of the
         seed node as listed in the <filename>hosts/verb_hosts</filename> file.
        </para>
       </listitem>
       <listitem>
        <para>
         Kill the DHCP services running on the old deployer:
        </para>
<screen><prompt>(old-deployer)> </prompt>sudo pkill dhcpd</screen>
       </listitem>
      </itemizedlist>
     </listitem>
    </itemizedlist>
   </step>
   <step>
    <para>
     Mount the <filename>~/sles12sp3.iso</filename> image to make it available
     to the &hpecloud; installer.
    </para>
    <itemizedlist>
     <listitem>
      <para>
       Ensure that <filename>~/sles12sp3.iso</filename> was either transferred
       from the original deployer or is downloaded to the home directory on the
       seed node under the <filename>sles12sp3.iso</filename>:
      </para>
<screen><prompt>(seed)> </prompt>sudo mkdir -p /media/cdrom
<prompt>(seed)> </prompt>sudo mount ~/sles12sp3.iso /media/cdrom</screen>
      <para>
       Edit the primary package repository on the new node so that it points
       to the ISO media instead of a remote network location. If you installed
       the seed node using cobbler from the original deployer as describe above,
       the repository will be configured in
      <filename>/etc/zypp/repos.d/SLES12-SP3-12.3-0.repo</filename>.
      </para>
<screen><prompt>(seed)> </prompt>sudo vi /etc/zypp/repos.d/SLES12-SP3-12.3-0.repo</screen>
      <itemizedlist>
       <listitem>
        <para>
         Update the <literal>baseurl</literal> line to read
         <literal>baseurl=file:/media/cdrom</literal>.
        </para>
       </listitem>
      </itemizedlist>
     </listitem>
    </itemizedlist>
   </step>
   <step>
    <para>
     Install &hpecloud; 5.SCP on the new deployer by downloading and unpacking
     &hpecloud; 5.SCP installation content:
    </para>
<screen><prompt>(seed)> </prompt>cd ~; tar -xf hos-5.1.0-*.tar
<prompt>(seed)> </prompt>cd ~/hos-5.1.0; ./hos-init.bash</screen>
   </step>
   <step>
    <para>
     Prepare for running playbooks:
    </para>
<screen><prompt>(seed)> </prompt>cd ~/openstack/ardana/ansible
<prompt>(seed)> </prompt>ansible-playbook -i hosts/localhost config-processor-run.yml
<prompt>(seed)> </prompt>ansible-playbook -i hosts/localhost ready-deployment.yml</screen>
   </step>
   <step>
    <para>
     Initialize ansible host facts and ensure that the existing cloud is fully
     addressable by the new deployer:
    </para>
<screen><prompt>(seed)> </prompt>cd ~/scratch/ansible/next/hos/ansible
<prompt>(seed)> </prompt>ansible-playbook -i hosts/verb_hosts hlm-refresh-facts.yml</screen>
   </step>
   <step>
    <para>
     Prune the primary database to remove data that is no longer relevant:
    </para>
<screen><prompt>(seed)> </prompt>cd ~/scratch/ansible/next/hos/ansible
<prompt>(seed)> </prompt>ansible-playbook -i hosts/verb_hosts percona-prune.yml</screen>
   </step>
  </procedure>

  <procedure>
   <title>Migration Phase</title>
   <warning>
    <title>Downtime</title>
    <para>
     The cloud will not be available until all of the following steps are
     completed.
    </para>
   </warning>
   <step>
    <para>
     Stop or pause &ostack; services on the control plane (excluding the seed
     that has been freshly imaged):
    </para>
<screen><prompt>(seed)> </prompt>cd ~/scratch/ansible/next/hos/ansible
<prompt>(seed)> </prompt>./hlm-quiesce.sh --limit '!<replaceable>SEED_HOST</replaceable>'</screen>
    <para>
     The <replaceable>SEED_HOST</replaceable> name must exactly match the host
     name that is referenced in <filename>hosts/verb_hosts</filename>, such as
     <literal>ardana-cp1-c1-m1</literal>.
    </para>
   </step>
   <step>
    <para>
     Create a database backup by retrieving MySQL database contents from
     another node (not the seed) in the database cluster:
    </para>
<screen><prompt>(seed)> </prompt>cd ~/scratch/ansible/next/hos/ansible
<prompt>(seed)> </prompt>ansible-playbook -i hosts/verb_hosts percona-vertica-removal-cleanup.yml
<prompt>(seed)> </prompt>ansible-playbook -i hosts/verb_hosts percona-export.yml \
  -e dbcontent=~/mysql_dump.sql</screen>
   </step>
   <step>
    <para>
     Re-image the remaining servers in the control plane.
    </para>
    <itemizedlist>
     <listitem>
      <para>
       Reinstall the operating systems on the control plane primary disks using
       an ISO, cobbler, or other provisioner. If using cobbler:
      </para>
<screen><prompt>(seed)> </prompt>cd ~/openstack/ardana/ansible
<prompt>(seed)> </prompt>ansible-playbook -i hosts/localhost cobbler-deploy.yml
<prompt>(seed)> </prompt>ansible-playbook -i hosts/localhost prepare-sles-grub2.yml \
  -e nodelist=<replaceable>NODE_IDs</replaceable>
<prompt>(seed)> </prompt>ansible-playbook -i hosts/localhost bm-reimage.yml \
  -e nodelist=<replaceable>NODE_IDs</replaceable></screen>
      <para>
       Replace <replaceable>NODE_IDs</replaceable> by a comma-separated list of
       IDs of all non-seed control plane and deployer nodes in the
       <filename>servers.yml</filename> file.
      </para>
     </listitem>
     <listitem>
      <para>
       Otherwise reformat the disk using partitioning to match the current
       configuration.
      </para>
     </listitem>
    </itemizedlist>
   </step>
   <step>
    <para>
     Perform the &hpecloud; 5.SCP installation using flags to enable database
     import:
    </para>
<screen><prompt>(seed)> </prompt>cd ~/scratch/ansible/next/hos/ansible
<prompt>(seed)> </prompt>ansible-playbook -i hosts/verb_hosts site.yml -e dbcontent=~/mysql_dump.sql</screen>
   </step>
   <step>
    <para>
     Optionally, if the <literal>3Par</literal> back-end was configured
     previously, follow the procedures to enable it again according to the
     &hpecloud; 5.0.x documentation.
    </para>
   </step>
   <step>
    <para>
     At this time, the cloud should again be operational, with the exception
     of monitoring components that will remain offline until the upgrade to
     &productname; &productnumber; is fully complete.
    </para>
   </step>
  </procedure>

  <procedure>
   <title>Cleaning Up</title>
   <note>
    <title>No Downtime</title>
    <para>
     These steps do not cause service downtime and can be performed while the
     cloud is up and running.
    </para>
   </note>
   <step>
    <para>
     Delete the database backup from the seed node:
    </para>
<screen><prompt>(seed)> </prompt>rm ~/mysql_dump.sql</screen>
   </step>
   <step>
    <para>
     If desired, transfer the deployer role in the &hpecloud; 5.SCP cloud to a
     different node, such as one that originally served as a dedicated
     deployer.
    </para>
    <itemizedlist>
     <listitem>
      <para>
       If the deployer will be moving to an entirely different cluster (for
       example, Database cluster to Controller cluster or standalone Deployer
       cluster), additional software packages need to be installed.
      </para>
      <itemizedlist>
       <listitem>
        <para>
         Edit
         <filename>~/openstack/my_cloud/definition/data/control_plane.yml</filename>
         and move the <systemitem>lifecycle-manager</systemitem> from its
         current location into the new desired cluster service-components list.
         Commit the change to Git:
        </para>
<screen><prompt>(seed)> </prompt>git commit -a -m "Enable lifecycle-manager for targeted seed node"</screen>
       </listitem>
       <listitem>
        <para>
         Kill the DHCP services running on the current seed:
        </para>
<screen><prompt>(seed)> </prompt>sudo pkill dhcpd</screen>
       </listitem>
       <listitem>
        <para>
         Install the <systemitem>lifecycle-manager</systemitem> services on the
         new deployer:
        </para>
<screen><prompt>(seed)> </prompt>ansible-playbook -i hosts/verb_hosts site.yml -l <replaceable>NEW_DEPLOYER_HOST</replaceable></screen>
        <para>
         <replaceable>NEW_DEPLOYER_HOST</replaceable> could be for example
         <systemitem>ardana-cp1-c0-m1</systemitem>
        </para>
       </listitem>
      </itemizedlist>
     </listitem>
     <listitem>
      <para>
       Copy the <filename>home</filename> directory from the seed to the new
       deployer:
      </para>
<screen><prompt>(seed)> </prompt>rsync -avP ~ <replaceable>NEW_DEPLOYER_IP</replaceable>:/home</screen>
     </listitem>
     <listitem>
      <para>
       Initialize the deployer node:
      </para>
<screen><prompt>(new deployer)> </prompt>cd ~
<prompt>(new deployer)> </prompt>tar -xf hos-5.1.0-*.tar
<prompt>(new deployer)> </prompt>cd ~/hos-5.1.0
<prompt>(new deployer)> </prompt>./hos-init.bash</screen>
     </listitem>
    </itemizedlist>
   </step>
  </procedure>
 </sect1>
 <sect1 xml:id="ardana.newton.migration">
  <title>Migration from &hpecloud; 5.SCP to Ardana.Newton</title>

  <para>
   The following procedure describes the process of migrating from &hpecloud;
   5.SCP to Ardana.Newton.
  </para>
  <para>
   From this point forward, all commands should be executed on the new deployer
   node (which will be the same as the earlier seed node unless this role was
   explicitly transferred using the instructions at the end of the prior section).
  </para>
  <procedure>
   <title>Preparation</title>
   <step xml:id="st.migrate.git-convert">
    <para>
     Convert the <filename>helion</filename> Git repository to an
     <filename>openstack</filename> Git repository for use by Ardana:
    </para>
<screen>cd ~
cp -r helion openstack
cd openstack
git checkout site</screen>
    <para>
     Save a record of all manual changes that have been previously committed
     to the <filename>hos/ansible</filename> directory, as you will need to
     reapply them later:
    </para>
<screen>git diff hos -- hos/ansible > /tmp/hos.diff</screen>
    <para>
     Undo all manual changes:
    </para>
<screen>rm -rf hos/ansible
git checkout hos -- hos/ansible</screen>
    <para>
     Reset the codebase to make it ready for Ardana, then refactor the rest of
     the repository:
    </para>
<screen>
git commit -a -m "Resetting codebase in prep for Ardana"
git branch -m hos ardana
git checkout ardana
sed -i 's/hos/ardana/' .gitignore
git commit -a -m "Prep for Ardana"
git mv hos ardana
git mv hos_extensions ardana_extensions
for i in $(find -L my_cloud/config -xtype l)
  do ln -sfn $(readlink $i | sed 's/\/hos\//\/ardana\//') $i
done
git commit -a -m "Conversion to Ardana"
git checkout cp-persistent
sed -i 's/component: helion-ca/component: ardana-ca/' \
my_cloud/persistent_state/private_data_control-plane-1.yml
sed -i 's/helion_internal_ca/ardana_internal_ca/' \
my_cloud/persistent_state/private_data_control-plane-1.yml
git commit -a -m "Conversion to Ardana"
git checkout site
git merge ardana -m "Conversion to Ardana"
</screen>
    <para>
     Reinstate the changes that were originally made to the contents of the
     <filename>hos/ansible</filename> directory
    </para>
<screen>sed 's/ \([ab]\?[\/]\?\)hos\/ansible/ \1ardana\/ansible/g' /tmp/hos.diff | git apply
git add -A
git commit -a -m "Reinstated hos playbook customizations (batch)"
</screen>
   </step>
   <step>
    <para>
     Remove and archive the &hpecloud; 5.SCP-related content:
    </para>
<screen>mv ~/scratch ~/scratch.scp
sudo rm /etc/apache2/sites-enabled/deployer_venv_server.conf</screen>
   </step>
   <step>
    <para>
     Prepare the <filename>ardana_packager</filename> working directory to
     include existing &suse; repositories:
    </para>
<screen>sudo mkdir /opt/ardana_packager
sudo ln -s /opt/hlm_packager/hlm /opt/ardana_packager/hlm</screen>
   </step>
   <step>
    <para>
     Install Ardana.Newton content onto deployer by mounting the installation
     media and running the following commands:
    </para>
<screen>sudo cd ~
mkdir -p /media/ardana-0.35.0
sudo mount ~/ardana-0.35.0*.iso /media/ardana-0.35.0
tar -xzf /media/ardana-0.35.0/ardana/ardana-0.35.0*.tar
cd ardana-0.35.0
./ardana-init.bash</screen>
   </step>
   <step>
    <para>
     Install Ardana.Newton:
    </para>
<screen>cd ~/openstack/ardana/ansible
ansible-playbook -i hosts/localhost config-processor-run.yml
ansible-playbook -i hosts/localhost ready-deployment.yml
cd ~/scratch/ansible/next/ardana/ansible/
ansible-playbook -i hosts/verb_hosts osconfig-iptables-rename.yml
ansible-playbook -i hosts/verb_hosts ardana-upgrade.yml</screen>
   </step>
  </procedure>
 </sect1>
 <sect1 xml:id="ardana.pike.migration">
  <title>Migration from Ardana.Newton to Ardana.Pike</title>
  <procedure>
   <step>
    <para>
     Create the <systemitem>ardana</systemitem> user throughout all nodes in the
     cloud:
    </para>
<screen>cd ~/scratch/ansible/next/ardana/ansible
ansible-playbook -i hosts/verb_hosts ardana-user-create.yml</screen>
   </step>
   <step>
    <para>
     Mount the &productname; &productnumber; installation media on the host
     (after copying the ISO file into the user's home directory)
    </para>
<screen>sudo mkdir -p /media/cloud
sudo mount ~/HPE-HELION-OPENSTACK-*.iso /media/cloud</screen>
   </step>
   <step>
    <para>
     Add the contents of the ISO as a Zypper repository and begin installation:
    </para>
<screen>sudo zypper ar -G -f file:/media/cloud Cloud
sudo zypper refresh</screen>
   </step>
   <step>
    <para>
     Begin the Ardana.Pike installation:
    </para>
<screen>sudo zypper -n in patterns-cloud-ardana
sudo su - ardana
ARDANA_INIT_AUTO=1 /usr/bin/ardana-init</screen>
   </step>
   <step>
    <para>
     Adjust data model to install <systemitem>cassandra</systemitem> instead
     of <systemitem>vertica</systemitem> (continuing as
     <systemitem>ardana</systemitem> user):
    </para>
<screen>cd /var/lib/ardana/openstack/ardana/ansible</screen>
    <para>
     Manually replace <literal>vertica</literal> with <literal>cassandra</literal>
     in the service-components list, and remove any entry for
     <literal>ops-console-monitor</literal>:
    </para>
<screen>vi ../../my_cloud/definition/data/control_plane.yml</screen>
    <para>
     In files where a <systemitem>cassandra_db</systemitem> partition
     exists, manually update its consumer name from <literal>vertica</literal> to
     <literal>cassandra</literal> and add an identical
     <literal>consumer</literal> block to the
     <systemitem>cassandra_log</systemitem> partition definition:
    </para>
<screen>vi ../../my_cloud/definition/data/disks_*</screen>
    <para>
     Save the updates to the data model:
    </para>
<screen>git commit -a -m 'Updated data model'</screen>
   </step>
   <step>
    <para>
     Build the <filename>~/scratch</filename> directory for the upgrade:
    </para>
<screen>ansible-playbook -i hosts/localhost config-processor-run.yml
ansible-playbook -i hosts/localhost ready-deployment.yml</screen>
   </step>
   <step>
    <para>
     Prepare the servers for Pike infrastructure:
    </para>
<screen>cd ~/scratch/ansible/next/ardana/ansible
ansible-playbook -i hosts/verb_hosts FND-AP2-vhosts-transition.yml
ansible-playbook -i hosts/verb_hosts tls-deploy.yml</screen>
   </step>
   <step>
    <para>
     Set up package repositories for installation:
    </para>
<screen>sudo ln -s /media/cloud /srv/www/suse-12.3/x86_64/repos/Cloud
ansible-playbook -i hosts/verb_hosts ardana-replace-legacy-repos.yml</screen>
   </step>
   <step>
    <para>
     Apply manual updates to playbooks to enable proper upgrade behavior
    </para>
<screen>sed -i '61d' roles/powerdns-post-configure/tasks/db_configure.yml
sed -i '64i\    - domains\' roles/powerdns-post-configure/tasks/db_configure.yml
sed -i '60d' _ardana-upgrade-base.yml
sed -i "60i\    shell: zypper repos | grep \"|\" | grep -v \"^#\" | \
  cut -d'|' -f2\\" _ardana-upgrade-base.yml</screen>
   </step>
   <step>
   <para>
    Run the &productname; &productnumber; upgrade playbook:
   </para>
<screen>ansible-playbook -i hosts/verb_hosts -vvv ardana-upgrade-from-legacy.yml</screen>
   </step>
   <step>
    <para>
     Note that the <filename>ardana-upgrade-from-legacy.yml</filename> playbook is
     not guaranteed to be idempotent, and re-running it upon a failure may produce
     new errors. If this playbook fails, please contact customer support and be
     prepared to share details of the failure from the contents of
     <filename>~/.ansible/ansible.log</filename> to determine appropriate next
     steps.
    </para>
   </step>
   <step>
    <para>
     Restart all services:
    </para>
<screen>cd ~/scratch/ansible/next/ardana/ansible
ansible-playbook -i hosts/verb_hosts ardana-stop.yml
ansible-playbook -i hosts/verb_hosts ardana-start.yml</screen>
   </step>
   <step>
    <para>
     Determine how to handle modified policy files which restrict the admin
     user's privileges more tightly than in previous releases:
    </para>
    <para>
     With this upgrade, the <filename>policy.json</filename> files have been
     aligned with &ostack; Pike. This means that the admin user is assigned fewer
     roles than in previous releases. If you have scripts or runbook
     instructions which use the admin user may need to modify them to ensure that
     the necessary privileges are available for their actions, or modify the
     policies established in their environment.
    </para>
    <para>
     As an example, sourcing the <filename>service.osrc</filename> file now
     provides fewer roles than in previous releases. With this release, loading
     the <filename>service.osrc</filename> and attempting to run <command>cinder
     service-list</command> will result in the error "Policy doesn't allow
     volume_extension:services:index to be performed" because the admin user of
     the admin project no longer has the cinder_admin role by default, nor does it
     have the nova_admin, glance_admin, or swiftoperator roles.
    </para>
    <para>
     We recommended determining which roles, if any, need to be
     applied to particular users, and add only those roles. This will yield the
     greatest security benefits and keep user privileges more distinct.
    </para>
    <para>
     In an environment without security concerns or with few administrative users,
     you may prefer to add in all of the removed roles to make the system
     fully backwards-compatible. To do this for the admin user of the admin
     project, the following commands can be used:
    </para>
<screen>openstack role add --user admin --project admin swiftoperator
openstack role add --user admin --project admin glance_admin
openstack role add --user admin --project admin cinder_admin
openstack role add --user admin --project admin nova_admin</screen>
    <para>
     Executing the above commands restores the roles to administer &o_comp;,
     &o_blockstore;, &o_img;, and &o_objstore; for the admin user of the
     admin project. If you are using the admin user in other projects, or other
     users in the admin project, may wish to add some or all of these roles for
     the other users or projects; this needs to be determined on a case-by-case
     basis for each user and project.
    </para>
    <para>
     In an environment which requires separation of duties, these roles should not
     all be assigned to one user; instead, each administrator should be assigned
     only the roles they require to perform their duties and should use their own
     user to perform their administrative tasks.
    </para>
   </step>
  </procedure>
  </sect1>
</article>
