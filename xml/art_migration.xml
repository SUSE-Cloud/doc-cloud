<?xml version="1.0" encoding="UTF-8"?>
<?xml-stylesheet href="urn:x-suse:xslt:profiling:docbook50-profile.xsl"
 type="text/xml"
 title="Profiling step"?>
<!DOCTYPE article
[
  <!ENTITY % entities SYSTEM "entity-decl.ent">
    %entities;
]>
<article version="5.1" xml:lang="en" xml:id="art.hos.migration"
  xmlns="http://docbook.org/ns/docbook"
  xmlns:xi="http://www.w3.org/2001/XInclude"
  xmlns:xlink="http://www.w3.org/1999/xlink">
 <info>
  <title>Migration Guide</title><date>
<?dbtimestamp format="B d, Y"?></date>
<!-- <xi:include href="common_copyright_gfdl.xml"/> -->
<!--<xi:include href="authors.xml"/>-->
 </info>
 <para>
  <remark role="clarity">taroth 2018-02-27: who is the target audience for this document?
  cloud operators? cloud administrators?</remark>
  <remark>taroth 2018-02-27: in terms of terminology, I guess that 'seed', 'deployer'
  should be changed to 'seed nodes', 'deployer nodes' etc.</remark>
 </para>
 <sect1 xml:id="hos.scp.migration">
  <title>Migration from &hpecloud; 5.x to &hpecloud; 5.SCP</title>

  <para>
   The migration procedure is based on the following assumptions:
  </para>

  <itemizedlist>
   <listitem>
    <para>
     The customer is responsible for backing up all the control plane and
     deployer nodes to an external data store before starting the migration.
    </para>
    <important>
     <title>No Migration Rollback</title>
     <para>
      We do not support rollback of migration after the process has been
      started.
     </para>
    </important>
   </listitem>
   <listitem>
    <para>
     Metering, monitoring, and logging (MML) data will not be migrated (a clean
     installation will be done using a new MML back-end included with
     &hpecloud; 5.SCP).
    </para>
   </listitem>
   <listitem>
    <para>
     Any data that is not on a disk separate from the primary root partition
     will be lost during re-imaging. This includes &swift; objects and audit
     logs.
    </para>
   </listitem>
   <listitem>
    <para>
     All &compnode;s are running supported Linux distributions:
    </para>
    <itemizedlist>
     <listitem>
      <para>
       &sls; 12 SP3 for &slsa; &compnode;s
      </para>
     </listitem>
     <listitem>
      <para>
       &rhla; 7.3 for &rhla; &compnode;s
      </para>
     </listitem>
    </itemizedlist>
   </listitem>
   <listitem>
    <para>
     There are only supported input model services.
    </para>
    <itemizedlist>
     <listitem>
      <para>
       Ceph and VSA are not embedded in &productname; &productnumber;.
      </para>
     </listitem>
    </itemizedlist>
   </listitem>
   <listitem>
    <para>
     Only supported third-party integrations and extensions are installed.
    </para>
    <itemizedlist>
     <listitem>
      <para>
       There are no unsupported customized virtual environments.
      </para>
     </listitem>
    </itemizedlist>
   </listitem>
  </itemizedlist>

  <procedure>
   <title>Preparation Phase</title>
   <step>
    <para>
     Run the validation playbook to ensure that the current cloud load and
     configuration is compatible with the migration process:
     <remark role="clarity">taroth 2018-02-27: what is 'validation playbook'? the name of a script?
    then it should be tagged with &lt;literal/&gt;</remark>
    </para>
<screen>(old-deployer)> git clone -b hp/prerelease/newton https://gerrit.suse.provo.cloud/hp/hlm-migration-tools
(old-deployer)> cd hlm-migration-tools/ansible
(old-deployer)> ansible-playbook -i ~/scratch/ansible/next/hos/ansible/hosts/verb_hosts pre-migration-validations.yml
  </screen>
    <para>
     The validation playbook checks the following:
    </para>
    <itemizedlist>
     <listitem>
      <para>
       The &hpecloud; version number: either 5.0.1 or 5.0.2
       <remark role="clarity">taroth 2018-02-27: not sure what this should mean:
       does the version number HOS version number need to be either 5.0.1 or 5.0.2?
       then rephrase to: 'Is the HOS version number either 5.0.1 or 5.0.2?'</remark>
      </para>
     </listitem>
     <listitem>
      <para>
       &productname; &productnumber;-compatible Linux distributions on
       &compnode;s
      </para>
     </listitem>
     <listitem>
      <para>
       System clock synchronization across cloud nodes
      </para>
     </listitem>
     <listitem>
      <para>
       HA capacity of &contrnode;s (L3/DHCP agent)
      </para>
     </listitem>
     <listitem>
      <para>
       HA capacity of &contrnode;s (&swift;)
      </para>
     </listitem>
     <listitem>
      <para>
       Data model and &o_blockstore; back-end compatibility (no VSA or Ceph)
      </para>
     </listitem>
    </itemizedlist>
   </step>
   <step>
    <para>
     Identify a candidate seed &contrnode; in the primary control plane. This
     node must meet the following requirements:
    </para>
    <itemizedlist>
     <listitem>
      <para>
       Networking connectivity is equivalent to that of the existing deployer.
      </para>
     </listitem>
     <listitem>
      <para>
       It is a member of the database cluster (running the MySQL service).
      </para>
     </listitem>
     <listitem>
      <para>
       It is not actively hosting any singleton &ostack; services
       (<systemitem>cinder-volume</systemitem> or
       <systemitem>nova-consoleauth</systemitem>). Test for empty output with
       the following command:
      </para>
<screen>(each controller)> ps -ef | egrep 'cinder-volume|nova-consoleauth'</screen>
     </listitem>
    </itemizedlist>
   </step>
   <step>
    <para>
     Prepare the cloud for migration:
    </para>
    <itemizedlist>
     <listitem>
      <para>
       Ensure that the data model enables deployer behavior by the new seed
       node.
       <remark>taroth 2018_02-27: what is this supposed to mean?</remark>
      </para>
      <itemizedlist>
       <listitem>
        <para>
         In the <filename>disks_*.yml</filename> file used by your server role
         that includes monitoring services, replace the existing
         <literal>Vertica</literal> partition definition with two
         <literal>Cassandra</literal> partitions:
        </para>
<screen>- name: cassandra
            size: (whatever size was used originally by vertica partition, less 1%)
            mount: /var/cassandra/data
            fstype: ext4
            mkfs-opts: -O large_file
            consumer:
              name: vertica
          - name: cassandra_log
            size: 1%
            mount: /var/cassandra/commitlog
            fstype: ext4
            mkfs-opts: -O large_file
   </screen>
       </listitem>
       <listitem>
        <para>
         Edit
         <filename>~/helion/my_cloud/definition/data/control_plane.yml</filename>
         and identify the cluster definition that already includes an entry for
         <filename>mysql</filename> in its service-components.
        </para>
       </listitem>
       <listitem>
        <para>
         Ensure that <systemitem>lifecycle-manager</systemitem> is included in
         the same cluster's service-components list. If it was necessary to add
         this as a new entry, it will also need to be removed from its original
         position in the deployer.
        </para>
       </listitem>
       <listitem>
        <para>
         Add a new entry for <systemitem>nova-placement-api</systemitem>
         alongside whichever cluster already includes
         <systemitem>nova-api</systemitem>.
        </para>
       </listitem>
       <listitem>
        <para>
         Commit your changes to the git model:
        </para>
<screen>(deployer)> git commit -a -m "Prepare for HOS 5.1 installation"</screen>
       </listitem>
      </itemizedlist>
     </listitem>
     <listitem>
      <para>
       Consider the effect of control plane downtime on existing DHCP leases:
      </para>
      <itemizedlist>
       <listitem>
        <para>
         Default DHCP lease time is 48 hours as defined in the
         <filename>~/helion/hos/ansible/roles/neutron-common/templates/neutron.conf.j2</filename>
         file by the <systemitem>dhcp_lease_duration</systemitem> parameter.
        </para>
       </listitem>
       <listitem>
        <para>
         DHCP leases are administered by &o_netw; and dnsmasq. This
         functionality will not be available during the control plane downtime.
        </para>
       </listitem>
       <listitem>
        <para>
         DHCP clients typically renew their leases either after 50% of the
         lease time or upon reboot.
        </para>
       </listitem>
       <listitem>
        <para>
         Normally, this leaves a 24-hour window for control plane downtime at
         any point before workload's DHCP leases run out and cannot be renewed.
         This is enough for performing a migration.
        </para>
       </listitem>
       <listitem>
        <para>
         To view the upcoming timeouts of DHCP leases, run the following
         command from a &contrnode;:
        </para>
<screen>for i in $(cat /var/run/neutron/dhcp/*/leases | awk '{print $1}'); do date -d @$i; done</screen>
       </listitem>
       <listitem>
        <para>
         If it is necessary to increase the DHCP lease time before migrating,
         open the
         <filename>~/helion/hos/ansible/roles/neutron-common/templates/neutron.conf.j2</filename>
         and edit the <systemitem>dhcp_lease_duration</systemitem> parameter.
         Commit the change, and run the
         <filename>neutron-configure.yml</filename> playbook.
        </para>
       </listitem>
      </itemizedlist>
     </listitem>
    </itemizedlist>
   </step>
   <step>
    <para>
     Decommission the seed node from the existing cloud (if the intended seed
     node hosts control plane services beyond the database):
     <remark role="clarity">taroth 2018-02-27: is 'if the intended seed node hosts control plane
  services beyond the database' the condition which must be fulfilled to execute
  the following two actions (in the itemizedlist)? if yes, rephrase the sentence to: 
  'If the intended seed node hosts control plane services beyond the database, decommission the seed
   node from the existing cloud.' If the following two actions explain how to decommission, end the
   introductory sentence with a colon: 'If the intended seed node hosts control plane services beyond
   the database, decommission the seed node from the existing cloud:'</remark>
    </para>
    <itemizedlist>
     <listitem>
      <para>
       For non-DVR clouds, evacuate any existing &o_netw; L3 routers on the
       seed node:
      </para>
<screen>(old-deployer)> cd ~/scratch/ansible/next/hos/ansible
(old-deployer)> cp -r ~/hlm-migration-tools/ansible/* ./
(old-deployer)> ansible-playbook -i hosts/verb_hosts
   neutron-router-evacuate.yml
   --limit=<replaceable>SEED_HOST</replaceable></screen>
      <para>
       <replaceable>SEED_HOST</replaceable> could be for example
       <systemitem>helion-cp1-c1-m2</systemitem>.
      </para>
     </listitem>
     <listitem>
      <para>
       For correct behavior, you must use the <systemitem>--limit</systemitem>
       argument to select the target seed host. This will output a success or
       error message at the end of its run.
      </para>
     </listitem>
    </itemizedlist>
   </step>
   <step>
    <para>
     Re-image the seed controller as a &slsa;-based system:
    </para>
    <itemizedlist>
     <listitem>
      <para>
       Display the current network connections of the seed controller (before
       re-imaging) for reference and save it locally.
      </para>
<screen>(seed)> ip a</screen>
     </listitem>
     <listitem>
      <para>
       Reinstall the operating system on the seed node's primary disk using an
       ISO, cobbler (from the existing HOS 5.0.1+ deployer), or other
       provisioner.
      </para>
     </listitem>
     <listitem>
      <para>
       If using cobbler, update the <filename>servers.yml</filename> entry for
       all nodes in the control plane to specify <systemitem>distro-id:
       sles12sp3-x86_64</systemitem> in each one, and commit the change to Git:
      </para>
<screen>(old-deployer)> cd ~/helion/hos/ansible
(old-deployer)> sudo cobbler system remove --name <replaceable>SEED_NODE_ID</replaceable>
(old-deployer)> ansible-playbook -i hosts/localhost cobbler-deploy.yml
(old-deployer)> ansible-playbook -i hosts/localhost bm-reimage.yml -e nodelist=<replaceable>SEED_NODE_ID</replaceable></screen>
      <para>
       Replace <replaceable>SEED_NODE_ID</replaceable> with the ID of the seed
       node from the <filename>servers.yml</filename> file.
      </para>
     </listitem>
     <listitem>
      <para>
       Otherwise reformat the disk using partitioning to match the current
       configuration.
      </para>
     </listitem>
     <listitem>
      <para>
       Copy the entire <filename>~stack</filename> directory from the original
       deployer:
      </para>
<screen>(old-deployer)> rsync -avP ~<replaceable>SEED_IP</replaceable>:/home</screen>
     </listitem>
     <listitem>
      <para>
       Ensure that <filename>~/sles12sp3sdk.iso</filename> was either
       transferred from the original deployer or is downloaded to the correct
       location under the <filename>sles12sp3sdk.iso</filename> name.
      </para>
     </listitem>
     <listitem>
      <para>
       If cobbler will be used from the new seed for provisioning the remaining
       cloud &contrnode;s:
      </para>
      <itemizedlist>
       <listitem>
        <para>
         Ensure that the new deployer is able to reach ILO interfaces
         referenced in the <filename>servers.yml</filename> file:
        </para>
<screen>(seed)> ping <replaceable>ILO_IP</replaceable></screen>
       </listitem>
       <listitem>
        <para>
         If network accessibility to the ILOs requires special network
         configuration and you would like to use HLM for that process based on
         your model:
        </para>
<screen>(old-deployer)> ansible-playbook -i hosts/verb_hosts hlm-refresh-facts.yml
(old-deployer)> ansible-playbook -i hosts/verb_hosts -l <replaceable>LOCAL_NODE_ID</replaceable> osconfig-run.yml</screen>
        <para>
         Replace <replaceable>LOCAL_NODE_ID</replaceable> with the ID of the
         local node from the <filename>verb_hosts</filename> file.
        </para>
       </listitem>
       <listitem>
        <para>
         Kill the DHCP services running on the old deployer:
        </para>
<screen>(old-deployer)> sudo pkill dhcpd</screen>
       </listitem>
      </itemizedlist>
     </listitem>
    </itemizedlist>
   </step>
   <step>
    <para>
     Mount the <filename>~/sles12sp3.iso</filename> image to make it available
     to the HOS installer.
    </para>
    <itemizedlist>
     <listitem>
      <para>
       Ensure that <filename>~/sles12sp3.iso</filename> was either transferred
       from the original deployer or is downloaded to the correct location
       under the <filename>sles12sp3.iso</filename>:
      </para>
<screen>(seed)> sudo mkdir -p /media/cdrom
(seed)> sudo mount ~/sles12sp3.iso /media/cdrom
(seed)> sudo vi /etc/zypp/repos.d/SLES12-SP3-12.3-0.repo</screen>
      <itemizedlist>
       <listitem>
        <para>
         Update the <literal>baseurl</literal> line to read
         <literal>baseurl=file:///media/cdrom</literal>.
        </para>
       </listitem>
      </itemizedlist>
     </listitem>
    </itemizedlist>
   </step>
   <step>
    <para>
     Install HOS 5.SCP on the new deployer by downloading and unpacking HOS
     5.SCP installation content:
    </para>
<screen>(seed)> cd ~; tar -xf hos-5.1.0-*.tar
(seed)> cd ~/hos-5.1.0; ./hos-init.bash</screen>
   </step>
   <step>
    <para>
     Prepare for running playbooks:
    </para>
<screen>(seed)> cd ~/helion/hos/ansible
(seed)> ansible-playbook -i hosts/localhost config-processor-run.yml
(seed)> ansible-playbook -i hosts/localhost ready-deployment.yml</screen>
   </step>
   <step>
    <para>
     Initialize ansible host facts and ensure that the existing cloud is fully
     addressable by the new deployer:
    </para>
<screen>(seed)> cd ~/scratch/ansible/next/hos/ansible
(seed)> ansible-playbook -i hosts/verb_hosts hlm-refresh-facts.yml</screen>
   </step>
   <step>
    <para>
     Prune the primary database to remove data that is no longer relevant:
    </para>
<screen>(seed)> cd ~/scratch/ansible/next/hos/ansible
(seed)> ansible-playbook -i hosts/verb_hosts percona-prune.yml</screen>
   </step>
  </procedure>

  <procedure>
   <title>Migration Phase</title>
   <warning>
    <title>Downtime</title>
    <para>
     The cloud will not be available until all of the following steps are
     completed.
    </para>
   </warning>
   <step>
    <para>
     Stop or pause &ostack; services on the control plane (excluding the seeds
     that have been freshly imaged):
    </para>
<screen>(seed)> cd ~/scratch/ansible/next/hos/ansible
(seed)> ./hlm-quiesce.sh --limit '!<replaceable>SEED_HOST</replaceable>'</screen>
    <para>
     The <replaceable>SEED_HOST</replaceable> name must exactly match the host
     name that is referenced in <filename>hosts/verb_hosts</filename>, such as
     <literal>helion-cp1-c1-m1</literal>.
    </para>
   </step>
   <step>
    <para>
     Create a database backup by retrieving MySQL database contents from
     another node (not the seed) in the database cluster:
    </para>
<screen>(seed)> cd ~/scratch/ansible/next/hos/ansible
(seed)> ansible-playbook -i hosts/verb_hosts percona-vertica-removal-cleanup.yml
(seed)> ansible-playbook -i hosts/verb_hosts percona-export.yml -e dbcontent=~/mysql_dump.sql</screen>
   </step>
   <step>
    <para>
     Re-image the remaining servers in the control plane.
    </para>
    <itemizedlist>
     <listitem>
      <para>
       Reinstall the operating systems on the control plane primary disks using
       an ISO, cobbler, or other provisioner. If using cobbler:
      </para>
<screen>(seed)> cd ~/helion/hos/ansible
(seed)> ansible-playbook -i hosts/localhost cobbler-deploy.yml
(seed)> ansible-playbook -i hosts/localhost prepare-sles-grub2.yml -e nodelist=<replaceable>NODE_IDs</replaceable>
(seed)> ansible-playbook -i hosts/localhost bm-reimage.yml -e nodelist=<replaceable>NODE_IDs</replaceable></screen>
      <para>
       Replace <replaceable>NODE_IDs</replaceable> by a comma-separated list of
       IDs of all non-seed control plane and deployer nodes in the
       <filename>servers.yml</filename> file.
      </para>
     </listitem>
     <listitem>
      <para>
       Otherwise reformat the disk using partitioning to match the current
       configuration.
      </para>
     </listitem>
    </itemizedlist>
   </step>
   <step>
    <para>
     Perform the HOS 5.SCP installation using flags to enable database import:
    </para>
<screen>(seed)> cd ~/scratch/ansible/next/hos/ansible
(seed)> ansible-playbook -i hosts/verb_hosts site.yml -e dbcontent=~/mysql_dump.sql</screen>
   </step>
   <step>
    <para>
     Optionally, if the <literal>3Par</literal> back-end was configured
     previously, follow the procedures to enable it again.
    </para>
   </step>
  </procedure>

  <procedure>
   <title>Cleaning Up</title>
   <note>
    <title>No Downtime</title>
    <para>
     These steps do not cause service downtime and can be performed at any time
     while the cloud is up and running.
    </para>
   </note>
   <step>
    <para>
     Delete the database backup from the seed node:
    </para>
<screen>(seed)> rm ~/mysql_dump.sql</screen>
   </step>
   <step>
    <para>
     If desired, transfer the deployer role in the HOS 5.SCP cloud to a
     different node, such as one that originally served as a dedicated
     deployer.
    </para>
    <itemizedlist>
     <listitem>
      <para>
       If the deployer will be moving to an entirely different cluster (for
       example, Database cluster to Controller cluster or standalone Deployer
       cluster), additional software packages need to be installed.
      </para>
      <itemizedlist>
       <listitem>
        <para>
         Edit
         <filename>~/helion/my_cloud/definition/data/control_plane.yml</filename>
         and move the <systemitem>lifecycle-manager</systemitem> from its
         current location into the new desired cluster service-components list.
         Commit the change to Git:
        </para>
<screen>(seed)> git commit -a -m "Enable lifecycle-manager for targeted seed node"</screen>
       </listitem>
       <listitem>
        <para>
         Kill the DHCP services running on the current seed:
        </para>
<screen>(seed)> sudo pkill dhcpd</screen>
       </listitem>
       <listitem>
        <para>
         Install the <systemitem>lifecycle-manager</systemitem> services on the
         new deployer:
        </para>
<screen>(seed)> ansible-playbook -i hosts/verb_hosts site.yml -l <replaceable>NEW_DEPLOYER_HOST</replaceable></screen>
        <para>
         <replaceable>NEW_DEPLOYER_HOST</replaceable> could be for example
         <systemitem>helion-cp1-c0-m1</systemitem>
        </para>
       </listitem>
      </itemizedlist>
     </listitem>
     <listitem>
      <para>
       Copy the <filename>home</filename> directory from the seed to the new
       deployer:
      </para>
<screen>(seed)> rsync -avP ~ <replaceable>NEW_DEPLOYER_IP</replaceable>:/home</screen>
     </listitem>
     <listitem>
      <para>
       Unpack or copy HOS 5.SCP contents to the <filename>home</filename>
       directory of the new deployer.
      </para>
     </listitem>
     <listitem>
      <para>
       Initialize the deployer node:
      </para>
<screen>(new deployer)> cd ~/hos-5.1.0; ./hos-init.bash</screen>
     </listitem>
    </itemizedlist>
   </step>
  </procedure>
 </sect1>
 <sect1 xml:id="ardana.newton.migration">
  <title>Migration from HOS 5.SCP to Ardana.Newton</title>

  <para>
   The following procedure describes the process of migrating from HOS 5.SCP to
   Ardana.Newton.
  </para>

  <procedure>
   <title>Preparation Phase</title>
   <step>
    <para>
     Convert the <filename>helion</filename> Git repo to
     <filename>openstack</filename> Git repo for use by Ardana:
    </para>
<screen>>cd ~
cp -r helion openstack
cd openstack
git checkout site</screen>
    <para>
     Undo any manual changes that have been previously committed to the
     hos/ansible directory
    </para>
<screen>git checkout hos -- hos/ansible</screen>
    <para>
     If any files were modified, review their changes and note that the
     customizations will need to be reapplied in the ardana playbooks:
    </para>
<screen>> git diff
git commit -a -m "Resetting codebase in prep for Ardana"
# then, continue to refactor the rest of the repo
git branch -m hos ardana
git checkout ardana
sed -i 's/hos/ardana/' .gitignore
git commit -a -m "Prep for Ardana"
git mv hos ardana
git mv hos_extensions ardana_extensions
git commit -a -m "Conversion to Ardana"
git checkout cp-persistent
sed -i 's/component: helion-ca/component: ardana-ca/'\
  my_cloud/persistent_state/private_data_control-plane-1.yml
sed -i 's/helion_internal_ca/ardana_internal_ca/' \
  my_cloud/persistent_state/private_data_control-plane-1.yml
git commit -a -m "Conversion to Ardana"
git checkout site</screen>
    <para>
     Pull forward the changes made earlier on the ardana branch:
    </para>
<screen>>git merge ardana -m "Conversion to Ardana"</screen>
   </step>
   <step>
    <para>
     Rename the old <filename>scratch</filename> directory to archive any
     remaining &hpecloud; 5.SCP files:
    </para>
<screen>mv ~/scratch ~/scratch.scp</screen>
   </step>
   <step>
    <para>
     Prepare the <filename>ardana_packager</filename> working directory to
     include existing &suse; repositories:
    </para>
<screen>> sudo mkdir /opt/ardana_packager
> sudo ln -s /opt/hlm_packager/hlm /opt/ardana_packager/hlm</screen>
   </step>
   <step>
    <para>
     Install Ardana.Newton content onto deployer by unpacking the ardana
     installer into ~/ardana-0.35.0 and then running the following commands:
    </para>
<screen>> cd ~/ardana-0.35.0
> ./ardana-init.bash</screen>
    <para>
     If the following error occurs, proceed as follows:
    </para>
<screen>TASK: [package-consumer | configure | Download the manifest file] *************
failed: [localhost] => {"exception": "", "extra_mode_bits": 0, "failed": true, "group": "root", "name": null, "service": null, "version": null}
msg: Installation failed</screen>
    <para>
     Run the command <command>sudo service apache2 restart</command> and run
     the <filename>ardana-init.bash</filename> script again.
    </para>
   </step>
   <step>
    <para>
     Install Ardana.Newton:
    </para>
<screen>> cd ~/openstack/ardana/ansible</screen>
    <para>
     Reapply any changes to playbooks that were undone in the hos code tree in
     step 1
    </para>
<screen>> ansible-playbook -i hosts/localhost cobbler-deploy.yml
> ansible-playbook -i hosts/localhost config-processor-run.yml
> ansible-playbook -i hosts/localhost ready-deployment.yml
> cd ~/scratch/ansible/next/ardana/ansible/
> ansible-playbook -i hosts/verb_hosts osconfig-iptables-rename.yml
> ansible-playbook -i hosts/verb_hosts ardana-upgrade.yml</screen>
   </step>
  </procedure>
 </sect1>
</article>
