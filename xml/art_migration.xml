<?xml version="1.0" encoding="UTF-8"?>
<?xml-stylesheet href="urn:x-suse:xslt:profiling:docbook50-profile.xsl"
 type="text/xml"
 title="Profiling step"?>
<!DOCTYPE article
[
  <!ENTITY % entities SYSTEM "entity-decl.ent">
    %entities;
]>
<article version="5.1" xml:lang="en" xml:id="art.hos.migration"
  xmlns="http://docbook.org/ns/docbook"
  xmlns:xi="http://www.w3.org/2001/XInclude"
  xmlns:xlink="http://www.w3.org/1999/xlink">
 <info>
  <title>Migration Guide</title><date>
<?dbtimestamp format="B d, Y"?></date>
<!-- <xi:include href="common_copyright_gfdl.xml"/> -->
<!--<xi:include href="authors.xml"/>-->
 </info>
 <para>
  <remark role="clarity">taroth 2018-02-27: who is the target audience for this document?
  cloud operators? cloud administrators?</remark>
 </para>
 <sect1 xml:id="hos.scp.migration">
  <title>Migration from &hpecloud; 5.x to &hpecloud; 5.SCP</title>

  <para>
   The migration procedure is based on the following assumptions:
  </para>

  <itemizedlist>
   <listitem>
    <para>
     The customer is responsible for backing up all the control plane and
     deployer nodes to an external data store before starting the migration.
    </para>
    <important>
     <title>No Migration Rollback</title>
     <para>
      We do not support rollback of migration after the process has been
      started.
     </para>
    </important>
   </listitem>
   <listitem>
    <para>
     Metering, monitoring, and logging (MML) data will not be migrated (a clean
     installation will be done using a new MML back-end included with
     &hpecloud; 5.SCP).
    </para>
   </listitem>
   <listitem>
    <para>
     Any data that is not on a disk separate from the primary root partition
     will be lost during re-imaging. This includes &swift; objects and audit
     logs.
    </para>
   </listitem>
   <listitem>
    <para>
     All &compnode;s are running supported Linux distributions:
    </para>
    <itemizedlist>
     <listitem>
      <para>
       &sls; 12 SP3 for &slsa; &compnode;s
      </para>
     </listitem>
     <listitem>
      <para>
       &rhla; 7.3 for &rhla; &compnode;s
      </para>
     </listitem>
    </itemizedlist>
   </listitem>
   <listitem>
    <para>
     There are only supported input model services.
    </para>
    <itemizedlist>
     <listitem>
      <para>
       Ceph and VSA are not embedded in &productname; &productnumber;.
      </para>
     </listitem>
    </itemizedlist>
   </listitem>
   <listitem>
    <para>
     Only supported third-party integrations and extensions are installed.
    </para>
    <itemizedlist>
     <listitem>
      <para>
       There are no unsupported customized virtual environments.
      </para>
     </listitem>
    </itemizedlist>
   </listitem>
   <listitem>
    <para>
     No significant issues or alarms being reported by Monasca and OpsConsole
    </para>
   </listitem>
  </itemizedlist>

  <procedure>
   <title>Preparation Phase</title>
   <step>
    <para>
     Obtain the &hpecloud; 5.SCP installation ISO and use the
     <command>mount</command>
     command to make its contents available on the deployer:
    </para>
    <screen>(old-deployer)$ mkdir hos-5.scp
(old-deployer)$ sudo mount <replaceable>HOS5SCP_ISO_FILE</replaceable> hos-5.scp
(old-deployer)$ tar -xzvf hos-5.scp/hos/hlm-migration-tools.tar.gz ./
    </screen>
   </step>
   <step>
    <para>
     Run the validation playbook to ensure that the current cloud load and
     configuration is compatible with the migration process:
    </para>
<screen>(old-deployer)> git clone -b hp/prerelease/newton https://gerrit.suse.provo.cloud/hp/hlm-migration-tools
(old-deployer)> cd hlm-migration-tools/ansible
(old-deployer)> ansible-playbook -i ~/scratch/ansible/next/hos/ansible/hosts/verb_hosts pre-migration-validations.yml
  </screen>
    <para>
     The validation playbook checks the following:
    </para>
    <itemizedlist>
     <listitem>
      <para>
       That the &hpecloud; version number is either 5.0.1 or 5.0.2
      </para>
     </listitem>
     <listitem>
      <para>
       &productname; &productnumber;-compatible Linux distributions on
       &compnode;s
      </para>
     </listitem>
     <listitem>
      <para>
       System clock synchronization across cloud nodes
      </para>
     </listitem>
     <listitem>
      <para>
       HA capacity of &contrnode;s (L3/DHCP agent)
      </para>
     </listitem>
     <listitem>
      <para>
       HA capacity of &contrnode;s (&swift;)
      </para>
     </listitem>
     <listitem>
      <para>
       Data model and &o_blockstore; back-end compatibility (no VSA, Ceph, or cmc-service)
      </para>
     </listitem>
    </itemizedlist>
   </step>
   <step>
    <para>
     Identify a candidate seed &contrnode; in the primary control plane. This
     node must meet the following requirements:
    </para>
    <itemizedlist>
     <listitem>
      <para>
       Networking connectivity is equivalent to that of the existing deployer.
      </para>
     </listitem>
     <listitem>
      <para>
       It is a member of the database cluster (running the MySQL service).
      </para>
     </listitem>
     <listitem>
      <para>
       It is not actively hosting any singleton &ostack; services
       (<systemitem>cinder-volume</systemitem> or
       <systemitem>nova-consoleauth</systemitem>). Test for empty output with
       the following command:
      </para>
<screen>(each controller)> ps -ef | egrep 'cinder-volume|nova-consoleauth'</screen>
     </listitem>
    </itemizedlist>
   </step>
   <step>
    <para>
     Prepare the cloud for migration:
    </para>
    <itemizedlist>
     <listitem>
      <para>
       Ensure that the data model enables deployer behavior by the new seed
       node.
       <remark>taroth 2018_02-27: what is this supposed to mean?</remark>
      </para>
      <itemizedlist>
       <listitem>
        <para>
         In the <filename>disks_*.yml</filename> file used by your server role
         that includes monitoring services, replace the existing
         <literal>Vertica</literal> partition definition with two
         <literal>Cassandra</literal> partitions:
        </para>
<screen>- name: cassandra_db
            size: (whatever size was used originally by vertica partition, less 1%)
            mount: /var/cassandra/data
            fstype: ext4
            mkfs-opts: -O large_file
            consumer:
              name: vertica
          - name: cassandra_log
            size: 1%
            mount: /var/cassandra/commitlog
            fstype: ext4
            mkfs-opts: -O large_file
   </screen>
       </listitem>
       <listitem>
        <para>
         Edit
         <filename>~/openstack/my_cloud/definition/data/control_plane.yml</filename>
         and identify the cluster definition that already includes an entry for
         <filename>mysql</filename> in its service-components.
        </para>
       </listitem>
       <listitem>
        <para>
         Ensure that <systemitem>lifecycle-manager</systemitem> is included in
         the same cluster's service-components list. If it was necessary to add
         this as a new entry, it will also need to be removed from its original
         position in the deployer.
        </para>
       </listitem>
       <listitem>
        <para>
         Add a new entry for <systemitem>nova-placement-api</systemitem>
         alongside whichever cluster already includes
         <systemitem>nova-api</systemitem>.
        </para>
       </listitem>
       <listitem>
        <para>
         Commit your changes to the Git model:
        </para>
<screen>(deployer)> git commit -a -m "Prepare for HOS 5.1 installation"</screen>
       </listitem>
      </itemizedlist>
     </listitem>
     <listitem>
      <para>
       Consider the effect of control plane downtime on existing DHCP leases:
      </para>
      <itemizedlist>
       <listitem>
        <para>
         Default DHCP lease time is 48 hours as defined in the
         <filename>~/helion/hos/ansible/roles/neutron-common/templates/neutron.conf.j2</filename>
         file by the <systemitem>dhcp_lease_duration</systemitem> parameter.
        </para>
       </listitem>
       <listitem>
        <para>
         DHCP leases are administered by &o_netw; and dnsmasq. This
         functionality will not be available during the control plane downtime.
        </para>
       </listitem>
       <listitem>
        <para>
         DHCP clients typically renew their leases either after 50% of the
         lease time or upon reboot.
        </para>
       </listitem>
       <listitem>
        <para>
         Normally, this leaves a 24-hour window for control plane downtime at
         any point before workload's DHCP leases run out and cannot be renewed.
         This is enough for performing a migration.
        </para>
       </listitem>
       <listitem>
        <para>
         To view the upcoming timeouts of DHCP leases, run the following
         command from a &contrnode;:
        </para>
<screen>for i in $(cat /var/run/neutron/dhcp/*/leases | awk '{print $1}'); do date -d @$i; done</screen>
       </listitem>
       <listitem>
        <para>
         If it is necessary to increase the DHCP lease time before migrating,
         open the
         <filename>~/helion/hos/ansible/roles/neutron-common/templates/neutron.conf.j2</filename>
         and edit the <systemitem>dhcp_lease_duration</systemitem> parameter.
         Commit the change, and run the
         <filename>neutron-configure.yml</filename> playbook.
        </para>
       </listitem>
      </itemizedlist>
     </listitem>
    </itemizedlist>
   </step>
   <step>
    <para>
     Decommission the seed node from the existing cloud (if the intended seed
     node hosts control plane services beyond the database):
     <remark role="clarity">taroth 2018-02-27: is 'if the intended seed node hosts control plane
  services beyond the database' the condition which must be fulfilled to execute
  the following two actions (in the itemizedlist)? if yes, rephrase the sentence to: 
  'If the intended seed node hosts control plane services beyond the database, decommission the seed
   node from the existing cloud.' If the following two actions explain how to decommission, end the
   introductory sentence with a colon: 'If the intended seed node hosts control plane services beyond
   the database, decommission the seed node from the existing cloud:'</remark>
    </para>
    <itemizedlist>
     <listitem>
      <para>
       For non-DVR clouds, evacuate any existing &o_netw; L3 routers on the
       seed node:
      </para>
<screen>(old-deployer)> cd ~/scratch/ansible/next/hos/ansible
(old-deployer)> cp -r ~/hlm-migration-tools/ansible/* ./
(old-deployer)> ansible-playbook -i hosts/verb_hosts
   neutron-router-evacuate.yml
   --limit=<replaceable>SEED_HOST</replaceable></screen>
      <para>
       <replaceable>SEED_HOST</replaceable> could be for example
       <systemitem>helion-cp1-c1-m2</systemitem>.
      </para>
     </listitem>
     <listitem>
      <para>
       For correct behavior, you must use the <command>--limit</command>
       argument to select the target seed host. This will output a success or
       error message at the end of its run.
      </para>
     </listitem>
    </itemizedlist>
   </step>
   <step>
    <para>
     Re-image the seed controller as a &slsa;-based system:
    </para>
    <itemizedlist>
     <listitem>
      <para>
       Display the current network connections of the seed controller (before
       re-imaging) for reference and save it locally.
      </para>
<screen>(seed)> ip a</screen>
     </listitem>
     <listitem>
      <para>
       Reinstall the operating system on the seed node's primary disk using an
       ISO, cobbler (from the existing &hpecloud; 5.0.1+ deployer), or other
       provisioner.
      </para>
     </listitem>
     <listitem>
      <para>
       If using cobbler, update the <filename>servers.yml</filename> entry for
       all nodes in the control plane to specify <systemitem>distro-id:
       sles12sp3-x86_64</systemitem> in each one, and commit the change to Git:
      </para>
<screen>(old-deployer)> cd ~/helion/hos/ansible
(old-deployer)> sudo cobbler system remove --name <replaceable>SEED_NODE_ID</replaceable>
(old-deployer)> ansible-playbook -i hosts/localhost cobbler-deploy.yml
(old-deployer)> ansible-playbook -i hosts/localhost bm-reimage.yml -e nodelist=<replaceable>SEED_NODE_ID</replaceable></screen>
      <para>
       Replace <replaceable>SEED_NODE_ID</replaceable> with the ID of the seed
       node from the <filename>servers.yml</filename> file.
      </para>
     </listitem>
     <listitem>
      <para>
       Otherwise reformat the disk using partitioning to match the current
       configuration.
      </para>
     </listitem>
     <listitem>
      <para>
       Copy the entire <filename>~stack</filename> directory from the original
       deployer:
      </para>
<screen>(old-deployer)> rsync -avP ~<replaceable>SEED_IP</replaceable>:/home</screen>
     </listitem>
     <listitem>
      <para>
       Ensure that <filename>~/sles12sp3sdk.iso</filename> was either
       transferred from the original deployer or is downloaded to the correct
       location under the <filename>sles12sp3sdk.iso</filename> name.
      </para>
     </listitem>
     <listitem>
      <para>
       If cobbler will be used from the new seed for provisioning the remaining
       cloud &contrnode;s:
      </para>
      <itemizedlist>
       <listitem>
        <para>
         Ensure that the new deployer is able to reach ILO interfaces
         referenced in the <filename>servers.yml</filename> file:
        </para>
<screen>(seed)> ping <replaceable>ILO_IP</replaceable></screen>
       </listitem>
       <listitem>
        <para>
         If network accessibility to the ILOs requires special network
         configuration and you would like to use HLM for that process based on
         your model:
        </para>
<screen>(old-deployer)> ansible-playbook -i hosts/verb_hosts hlm-refresh-facts.yml
(old-deployer)> ansible-playbook -i hosts/verb_hosts -l <replaceable>LOCAL_NODE_ID</replaceable> osconfig-run.yml</screen>
        <para>
         Replace <replaceable>LOCAL_NODE_ID</replaceable> with the ID of the
         local node from the <filename>verb_hosts</filename> file.
        </para>
       </listitem>
       <listitem>
        <para>
         Kill the DHCP services running on the old deployer:
        </para>
<screen>(old-deployer)> sudo pkill dhcpd</screen>
       </listitem>
      </itemizedlist>
     </listitem>
    </itemizedlist>
   </step>
   <step>
    <para>
     Mount the <filename>~/sles12sp3.iso</filename> image to make it available
     to the &hpecloud; installer.
    </para>
    <itemizedlist>
     <listitem>
      <para>
       Ensure that <filename>~/sles12sp3.iso</filename> was either transferred
       from the original deployer or is downloaded to the correct location
       under the <filename>sles12sp3.iso</filename>:
      </para>
<screen>(seed)> sudo mkdir -p /media/cdrom
(seed)> sudo mount ~/sles12sp3.iso /media/cdrom
(seed)> sudo vi /etc/zypp/repos.d/SLES12-SP3-12.3-0.repo</screen>
      <itemizedlist>
       <listitem>
        <para>
         Update the <literal>baseurl</literal> line to read
         <literal>baseurl=file:///media/cdrom</literal>.
        </para>
       </listitem>
      </itemizedlist>
     </listitem>
    </itemizedlist>
   </step>
   <step>
    <para>
     Install &hpecloud; 5.SCP on the new deployer by downloading and unpacking
     &hpecloud;Â 5.SCP installation content:
    </para>
<screen>(seed)> cd ~; tar -xf hos-5.1.0-*.tar
(seed)> cd ~/hos-5.1.0; ./hos-init.bash</screen>
   </step>
   <step>
    <para>
     Prepare for running playbooks:
    </para>
<screen>(seed)> cd ~/helion/hos/ansible
(seed)> ansible-playbook -i hosts/localhost config-processor-run.yml
(seed)> ansible-playbook -i hosts/localhost ready-deployment.yml</screen>
   </step>
   <step>
    <para>
     Initialize ansible host facts and ensure that the existing cloud is fully
     addressable by the new deployer:
    </para>
<screen>(seed)> cd ~/scratch/ansible/next/hos/ansible
(seed)> ansible-playbook -i hosts/verb_hosts hlm-refresh-facts.yml</screen>
   </step>
   <step>
    <para>
     Prune the primary database to remove data that is no longer relevant:
    </para>
<screen>(seed)> cd ~/scratch/ansible/next/hos/ansible
(seed)> ansible-playbook -i hosts/verb_hosts percona-prune.yml</screen>
   </step>
  </procedure>

  <procedure>
   <title>Migration Phase</title>
   <warning>
    <title>Downtime</title>
    <para>
     The cloud will not be available until all of the following steps are
     completed.
    </para>
   </warning>
   <step>
    <para>
     Stop or pause &ostack; services on the control plane (excluding the seeds
     that have been freshly imaged):
    </para>
<screen>(seed)> cd ~/scratch/ansible/next/hos/ansible
(seed)> ./hlm-quiesce.sh --limit '!<replaceable>SEED_HOST</replaceable>'</screen>
    <para>
     The <replaceable>SEED_HOST</replaceable> name must exactly match the host
     name that is referenced in <filename>hosts/verb_hosts</filename>, such as
     <literal>helion-cp1-c1-m1</literal>.
    </para>
   </step>
   <step>
    <para>
     Create a database backup by retrieving MySQL database contents from
     another node (not the seed) in the database cluster:
    </para>
<screen>(seed)> cd ~/scratch/ansible/next/hos/ansible
(seed)> ansible-playbook -i hosts/verb_hosts percona-vertica-removal-cleanup.yml
(seed)> ansible-playbook -i hosts/verb_hosts percona-export.yml -e dbcontent=~/mysql_dump.sql</screen>
   </step>
   <step>
    <para>
     Re-image the remaining servers in the control plane.
    </para>
    <itemizedlist>
     <listitem>
      <para>
       Reinstall the operating systems on the control plane primary disks using
       an ISO, cobbler, or other provisioner. If using cobbler:
      </para>
<screen>(seed)> cd ~/helion/hos/ansible
(seed)> ansible-playbook -i hosts/localhost cobbler-deploy.yml
(seed)> ansible-playbook -i hosts/localhost prepare-sles-grub2.yml -e nodelist=<replaceable>NODE_IDs</replaceable>
(seed)> ansible-playbook -i hosts/localhost bm-reimage.yml -e nodelist=<replaceable>NODE_IDs</replaceable></screen>
      <para>
       Replace <replaceable>NODE_IDs</replaceable> by a comma-separated list of
       IDs of all non-seed control plane and deployer nodes in the
       <filename>servers.yml</filename> file.
      </para>
     </listitem>
     <listitem>
      <para>
       Otherwise reformat the disk using partitioning to match the current
       configuration.
      </para>
     </listitem>
    </itemizedlist>
   </step>
   <step>
    <para>
     Perform the &hpecloud; 5.SCP installation using flags to enable database
     import:
    </para>
<screen>(seed)> cd ~/scratch/ansible/next/hos/ansible
(seed)> ansible-playbook -i hosts/verb_hosts site.yml -e dbcontent=~/mysql_dump.sql</screen>
   </step>
   <step>
    <para>
     Optionally, if the <literal>3Par</literal> back-end was configured
     previously, follow the procedures to enable it again.
    </para>
   </step>
  </procedure>

  <procedure>
   <title>Cleaning Up</title>
   <note>
    <title>No Downtime</title>
    <para>
     These steps do not cause service downtime and can be performed while the
     cloud is up and running.
    </para>
   </note>
   <step>
    <para>
     Delete the database backup from the seed node:
    </para>
<screen>(seed)> rm ~/mysql_dump.sql</screen>
   </step>
   <step>
    <para>
     If desired, transfer the deployer role in the &hpecloud; 5.SCP cloud to a
     different node, such as one that originally served as a dedicated
     deployer.
    </para>
    <itemizedlist>
     <listitem>
      <para>
       If the deployer will be moving to an entirely different cluster (for
       example, Database cluster to Controller cluster or standalone Deployer
       cluster), additional software packages need to be installed.
      </para>
      <itemizedlist>
       <listitem>
        <para>
         Edit
         <filename>~/openstack/my_cloud/definition/data/control_plane.yml</filename>
         and move the <systemitem>lifecycle-manager</systemitem> from its
         current location into the new desired cluster service-components list.
         Commit the change to Git:
        </para>
<screen>(seed)> git commit -a -m "Enable lifecycle-manager for targeted seed node"</screen>
       </listitem>
       <listitem>
        <para>
         Kill the DHCP services running on the current seed:
        </para>
<screen>(seed)> sudo pkill dhcpd</screen>
       </listitem>
       <listitem>
        <para>
         Install the <systemitem>lifecycle-manager</systemitem> services on the
         new deployer:
        </para>
<screen>(seed)> ansible-playbook -i hosts/verb_hosts site.yml -l <replaceable>NEW_DEPLOYER_HOST</replaceable></screen>
        <para>
         <replaceable>NEW_DEPLOYER_HOST</replaceable> could be for example
         <systemitem>helion-cp1-c0-m1</systemitem>
        </para>
       </listitem>
      </itemizedlist>
     </listitem>
     <listitem>
      <para>
       Copy the <filename>home</filename> directory from the seed to the new
       deployer:
      </para>
<screen>(seed)> rsync -avP ~ <replaceable>NEW_DEPLOYER_IP</replaceable>:/home</screen>
     </listitem>
     <listitem>
      <para>
       Unpack or copy &hpecloud; 5.SCP contents to the <filename>home</filename>
       directory of the new deployer.
      </para>
     </listitem>
     <listitem>
      <para>
       Initialize the deployer node:
      </para>
<screen>(new deployer)> cd ~/hos-5.1.0; ./hos-init.bash</screen>
     </listitem>
    </itemizedlist>
   </step>
  </procedure>
 </sect1>
 <sect1 xml:id="ardana.newton.migration">
  <title>Migration from &hpecloud; 5.SCP to Ardana.Newton</title>

  <para>
   The following procedure describes the process of migrating from &hpecloud;
   5.SCP to Ardana.Newton.
  </para>

  <procedure>
   <title>Preparation</title>
   <step xml:id="st.migrate.git-convert">
    <para>
     Convert the <filename>helion</filename> Git repository to an
     <filename>openstack</filename> Git repository for use by Ardana:
    </para>
<screen>cd ~
cp -r helion openstack
cd openstack
git checkout site</screen>
    <para>
     Save a record of all manual changes that have been previously committed
     to the <filename>hos/ansible</filename> directory, as you will need to
     reapply them later:
    </para>
<screen>git diff hos -- hos/ansible > /tmp/hos.diff</screen>
    <para>
     Undo all manual changes:
    </para>
<screen>git checkout hos -- hos/ansible</screen>
    <para>
     Reset the codebase to make it ready for Ardana, then refactor the rest of
     the repository:
    </para>
<screen>
git commit -a -m "Resetting codebase in prep for Ardana"
git branch -m hos ardana
git checkout ardana
sed -i 's/hos/ardana/' .gitignore
git commit -a -m "Prep for Ardana"
git mv hos ardana
git mv hos_extensions ardana_extensions
for i in $(find -L my_cloud/config -xtype l); \
do ln -sfn $(readlink $i | sed 's/\/hos\//\/ardana\//') $i; done
git commit -a -m "Conversion to Ardana"
git checkout cp-persistent
sed -i 's/component: helion-ca/component: ardana-ca/' \
my_cloud/persistent_state/private_data_control-plane-1.yml
sed -i 's/helion_internal_ca/ardana_internal_ca/' \
my_cloud/persistent_state/private_data_control-plane-1.yml
git commit -a -m "Conversion to Ardana"
git checkout site
git merge ardana -m "Conversion to Ardana"
# reinstate the changes that were originally made to the contents of hos/ansible
sed 's/ \([ab]\?[\/]\?\)hos\/ansible/ \1ardana\/ansible/g' /tmp/hos.diff | git apply
git commit -a -m "Reinstated hos playbook customizations (batch)"
</screen>
   </step>
   <step>
    <para>
     Remove and archive the &hpecloud; 5.SCP-related content:
    </para>
<screen>mv ~/scratch ~/scratch.scp
sudo rm /etc/apache2/sites-enabled/deployer_venv_server.conf</screen>
   </step>
   <step>
    <para>
     Prepare the <filename>ardana_packager</filename> working directory to
     include existing &suse; repositories:
    </para>
<screen>sudo mkdir /opt/ardana_packager
sudo ln -s /opt/hlm_packager/hlm /opt/ardana_packager/hlm</screen>
   </step>
   <step>
    <para>
     Install Ardana.Newton content onto deployer by unpacking the ardana
     installer into <filename>~/ardana-0.35.0</filename> and then running the following commands:
    </para>
<screen>cd ~/ardana-0.35.0
./ardana-init.bash</screen>
   </step>
   <step>
    <para>
     Install Ardana.Newton:
    </para>
<screen>cd ~/openstack/ardana/ansible</screen>
    <para>
     Reapply any changes to playbooks that were undone in the hos code tree in
     <xref linkend="st.migrate.git-convert"/>:
    </para>
<screen>ansible-playbook -i hosts/localhost cobbler-deploy.yml
ansible-playbook -i hosts/localhost config-processor-run.yml
ansible-playbook -i hosts/localhost ready-deployment.yml
cd ~/scratch/ansible/next/ardana/ansible/
ansible-playbook -i hosts/verb_hosts osconfig-iptables-rename.yml
ansible-playbook -i hosts/verb_hosts ardana-upgrade.yml</screen>
   </step>
  </procedure>
 </sect1>
 <sect1 xml:id="ardana.pike.migration">
  <title>Migration from Ardana.Newton to Ardana.Pike</title>
  <para>
   The following instructions are suitable for use with RC2 build #0053.
  </para>
  <procedure>
   <step>
    <para>
     Create the <systemitem>ardana</systemitem> user throughout the cloud nodes:
    </para>
<screen>cd ~/scratch/ansible/next/ardana/ansible</screen>
   </step>
   <step>
    <para>
     Mount the &productname; &productnumber; installation media on the host
     (after copying the ISO file into the user's home directory)
    </para>
<screen>sudo mkdir -p /opt/ardana_packager/SUSE-12-3/x86_64/repos/Cloud
sudo mount ~/SUSE-OPENSTACK-CLOUD-8-x86_64-Build0334-Media1.iso \
/opt/ardana_packager/SUSE-12-3/x86_64/repos/Cloud</screen>
   </step>
   <step>
    <para>
     Add the contents of the ISO as a Zypper repository and begin installation:
    </para>
<screen>sudo zypper ar -G -f file:///opt/ardana_packager/SUSE-12-3/x86_64/repos/Cloud Cloud
sudo zypper refresh</screen>
   </step>
   <step>
    <para>
     Begin the Ardana.Pike installation:
    </para>
<screen>sudo zypper -n in patterns-cloud-ardana
sudo su - ardana
ARDANA_INIT_AUTO=1 /usr/bin/ardana-init</screen>
   </step>
   <step>
    <para>
     Adjust data model to install <systemitem>cassandra</systemitem> instead
     of <systemitem>vertica</systemitem> (continuing as
     <systemitem>ardana</systemitem> user):
    </para>
<screen>$ cd /var/lib/ardana/openstack/ardana/ansible</screen>
    <para>
     Replace <literal>vertica</literal> with <literal>cassandra</literal> in
     the service-components list:
    </para>
<screen>vi ../../my_cloud/definition/data/control_plane.yml</screen>
    <para>
         In files where a <systemitem>cassandra_db</systemitem> partition
	 exists, update its consumer name from <literal>vertica</literal> to
	 <literal>cassandra</literal>  and add an identical
	 <literal>consumer</literal> block to the
	 <systemitem>cassandra_log</systemitem> partition definition:
    </para>
<screen>vi ../../my_cloud/definition/data/disks_*</screen>
    <para>
     Update the data model:
    </para>
<screen>git commit -a -m 'Update data model for cassandra'</screen>
   </step>
   <step>
    <para>
     Prepare the repository for Pike installation:
    </para>
<screen>
ansible-playbook -i hosts/localhost config-processor-run.yml
ansible-playbook -i hosts/localhost ready-deployment.yml</screen>
   </step>
   <step>
    <para>
     Prepare the servers for Pike infrastructure:
    </para>
<screen>cd ~/scratch/ansible/next/ardana/ansible
ansible-playbook -i hosts/verb_hosts FND-AP2-vhosts-transition.yml</screen>
   </step>
   <step>
    <para>
     Set up &scc; repositories or link to existing &smt; repositories. Replace
     the &hpecloud; &rhla; repositories on &rhel; &compnode;s with &productname;
     &productnumber; sources. Follow the public documentation for populating
     &suse; repositories and then proceed with the following command:
    </para>
<screen>ansible-playbook -i hosts/verb_hosts
ardana-replace-legacy-repos.yml</screen>
   </step>
   <step>
   <para>
    Run the Pike upgrade playbook:
   </para>
<screen>ansible-playbook -i hosts/verb_hosts -vvv \
ardana-upgrade-from-legacy.yml</screen>
   </step>
   <step>
    <para>
     If you had to run <filename>ardana-upgrade-from-legacy.yml</filename>
     multiple times in order to complete the upgrade, restart all services using
     <filename>ardana-stop.yml</filename> and
     <filename>ardana-start.yml</filename>.
    </para>
   </step>
  </procedure>
  </sect1>
</article>
