<?xml version="1.0" encoding="UTF-8"?>
<?xml-stylesheet href="urn:x-suse:xslt:profiling:docbook50-profile.xsl"
 type="text/xml"
 title="Profiling step"?>
<!DOCTYPE article
[
  <!ENTITY % entities SYSTEM "entity-decl.ent">
    %entities;
]>
<article version="5.1" xml:lang="en" xml:id="art.hos.migration"
  xmlns="http://docbook.org/ns/docbook"
  xmlns:xi="http://www.w3.org/2001/XInclude"
  xmlns:xlink="http://www.w3.org/1999/xlink">
 <info>
  <title>Migration Guide</title><date>
<?dbtimestamp format="B d, Y"?></date>
<!-- <xi:include href="common_copyright_gfdl.xml"/> -->
<!--<xi:include href="authors.xml"/>-->
 </info>
 <important>
   <title>Use Latest Documentation</title>
   <para>
     Always use the latest online version of the documentation for the
     migration process.
   </para>
 </important>
 <sect1 xml:id="hos.scp.migration">
  <title>Migration from &hpecloud; 5.0.x to &hpecloud; 5.SCP</title>

  <para>
   The migration procedure is based on the following assumptions:
  </para>

  <itemizedlist>
   <listitem>
    <para>
     Skilled customer staff should be readily available to assist in the
     migration, with expertise in the areas of cloud administration,
     network administration, storage administration, and knowledge of
     data models and configuration settings used by the &clm;.
    </para>
    <para>
     Ideally, systems involved in the upgrade should be physically accessible
     in case any operations become necessary that cannot be administered
     via their remote network interfaces.
    </para>
    <para>
     The customer is responsible for backing up all the control plane and
     deployer nodes to an external data store before starting the migration.
    </para>
    <important>
     <title>No Migration Rollback</title>
     <para>
      We do not support rollback of migration after the process has been
      started.
     </para>
    </important>
   </listitem>
   <listitem>
    <para>
     Metering, monitoring, and logging (MML) data will not be migrated (a clean
     installation will be done using a new MML back-end included with
     &hpecloud; 5.SCP).
    </para>
   </listitem>
   <listitem>
    <para>
     Upgrading will require re-imaging the operating system on the physical disks
     containing the root partition of your controller nodes. Existing data on these
     disks, which may include audit logs and &swift; objects, may be lost as a
     result. Ensure that your model files and any other important files on the
     deployer are copied to a safe location. (Note that the primary OpenStack
     MySQL database will undergo a backup/restore as part of the upgrade process
     to ensure that its contents are preserved.)
    </para>
   </listitem>
   <listitem>
    <para>
     Review your existing partitioning model to understand how your data will be
     affected by re-imaging. Any data that is not on a disk separate from the
     primary root partition (i.e. &swift; objects or audit logs) will be lost
     during re-imaging. Note that the control plane nodes must be booted from
     physical drives and must not have any attached SAN disks.
    </para>
   </listitem>
   <listitem>
    <para>
     All &compnode;s are running supported Linux distributions, and any prior
     hLinux &compnode;s have been removed from the cloud. (For details about
     migrating instances from hLinux to &slsa; or &rhla; &compnode;s, please
     review the following HOS 5 documentation: Release Notes, Installation
     Guide, and Operations Guide.)
    </para>
    <itemizedlist>
     <listitem>
      <para>
       &sls; 12 SP3 for &slsa; &compnode;s
      </para>
     </listitem>
     <listitem>
      <para>
       &rhla; 7.3 for &rhla; &compnode;s
      </para>
     </listitem>
    </itemizedlist>
   </listitem>
   <listitem>
    <para>
     There are some limitations to migrating instances. For details about
     migrating instances from hLinux to &slsa; or &rhla; &compnode;s, please
     review the following HOS 5 documentation: Release Notes, Installation
     Guide and Operations Guide.
    </para>
   </listitem>
   <listitem>
    <para>
     The cloud data model does not contain services which are not included in
     &productname; &productnumber;, such as Ceph and VSA.
    </para>
   </listitem>
   <listitem>
    <para>
     In order to migrate ESX compute VMs as part of this process, existing
     ESX hosts must be capable of migrating VMs between them, as rebalancing
     workloads will be necessary while ESX hosts are individually removed
     and restored as members of the clouds.
    </para>
   </listitem>
   <listitem>
    <para>
     Any customizations to virtual environments used by &ostack; components will
     be lost during the upgrade process. These may include third party extensions,
     regardless of origin, that have not been included in
     &productname; &productnumber;.
    </para>
   </listitem>
   <listitem>
    <para>
     No significant issues or alarms are actively being reported by &o_monitor; and
     &opscon;.
    </para>
   </listitem>
  </itemizedlist>
  <para>
   Preparation Phase
  </para>
  <procedure>
   <step>
    <para>
     Because all Freezer content will be obsolete upon migrating to the new platform,
     it is recommended to stop Freezer jobs and eliminate historic backups.
    </para>
<screen><prompt>(old-deployer)> </prompt>source ~/backup.osrc
<prompt>(old-deployer)> </prompt>for id in $(freezer job-list -f value | awk '{print $1}')
  do freezer job-stop $id
done
<prompt>(old-deployer)> </prompt>for container in $(openstack container list -f value); do
  for backup in $(openstack object list $container -f value); do
    openstack object delete $container $backup
    echo "Deleting $backup"
  done
done</screen>
   </step>
   <step>
    <para>
     If your cloud includes VMware ESX compute clusters, save the EON service
     activation template for future reference, as the EON service will no
     longer exist once migration is complete.
    </para>
<screen><prompt>(old-deployer)> </prompt>source ~/service.osrc
<prompt>(old-deployer)> </prompt>eon get-activation-template \
  --filename ~/activationtemplate.json --type esxcluster</screen>
   </step>
   <step>
    <para>
     Obtain the &hpecloud; 5.SCP installation ISO and use the
     <command>mount</command>
     command to make its contents available on the existing deployer:
    </para>
<screen><prompt>(old-deployer)> </prompt>mkdir hos-5.scp
<prompt>(old-deployer)> </prompt>sudo mount <replaceable>HELION5SCP_ISO_FILE</replaceable> hos-5.scp
<prompt>(old-deployer)> </prompt>tar -xzvf hos-5.scp/hos/hlm-migration-tools.tar.gz
<prompt>(old-deployer)> </prompt>sudo umount hos-5.scp</screen>
   </step>
   <step>
    <para>
     Run the validation playbook to ensure that the current cloud load and
     configuration is compatible with the migration process:
    </para>
<screen><prompt>(old-deployer)> </prompt>cd hlm-migration-tools/ansible
<prompt>(old-deployer)> </prompt>ansible-playbook \
  -i ~/scratch/ansible/next/hos/ansible/hosts/verb_hosts \
  pre-migration-validations.yml</screen>
    <para>
     The validation playbook checks that:
    </para>
    <itemizedlist>
     <listitem>
      <para>
       The &hpecloud; version number is either 5.0.1 or 5.0.3
      </para>
     </listitem>
     <listitem>
      <para>
       &productname; &productnumber;-compatible Linux distributions on
       &compnode;s
      </para>
     </listitem>
     <listitem>
      <para>
       System clocks are synchronized across cloud nodes
      </para>
     </listitem>
     <listitem>
      <para>
       HA capacity of &contrnode;s (L3/DHCP agent)
      </para>
     </listitem>
     <listitem>
      <para>
       Configuration and capacity of &contrnode;s (&swift;, &o_blockstore;)
      </para>
     </listitem>
     <listitem>
      <para>
       Data model compatibility with &productname; &productnumber; (no vsa, ceph,
       eon-*, or cmc-service)
      </para>
     </listitem>
    </itemizedlist>
    <warning>
     <title>Check Validation Results</title>
     <para>
      Review the contents of /var/log/pre-migration-validations.log immediately
      to discover if any errors were identified while running the validation playbook.
      Any identified issues must be resolved before proceeding with the migration.
     </para>
    </warning>
   </step>
   <step>
    <para>
     Set up SMT repository hosting on a &slsa; server external to your cloud so that
     operating system and software updates can be applied during migration without
     waiting for additional download steps.
    </para>
   </step>
   <step>
    <para>
     Identify a candidate seed &contrnode; in the primary control plane. This
     node must meet the following requirements:
    </para>
    <itemizedlist>
     <listitem>
      <para>
       Networking connectivity is equivalent to that of the existing deployer.
      </para>
     </listitem>
     <listitem>
      <para>
       It is a member of the database cluster (running the MySQL service).
      </para>
     </listitem>
     <listitem>
      <para>
       It is not actively hosting any singleton &ostack; services
       (<systemitem>cinder-volume</systemitem> or
       <systemitem>nova-consoleauth</systemitem>). Test for empty output with
       the following command:
      </para>
<screen><prompt>(each controller)> </prompt>ps -ef | egrep 'cinder-volume|nova-consoleauth'</screen>
     </listitem>
    </itemizedlist>
   </step>
   <step>
    <para>
     Consider the effect of control plane downtime on existing DHCP leases:
    </para>
    <itemizedlist>
     <listitem>
      <para>
       Default DHCP lease time is 48 hours as defined in the
       <filename>~/helion/hos/ansible/roles/neutron-common/templates/neutron.conf.j2</filename>
       file by the <systemitem>dhcp_lease_duration</systemitem> parameter.
      </para>
     </listitem>
     <listitem>
      <para>
       DHCP leases are administered by &o_netw; and dnsmasq. This
       functionality will not be available during the control plane downtime.
      </para>
     </listitem>
     <listitem>
      <para>
       DHCP clients typically renew their leases either after 50% of the
       lease time or upon reboot.
      </para>
     </listitem>
     <listitem>
      <para>
       Normally, this leaves a 24-hour window for control plane downtime at
       any point before workload's DHCP leases run out and cannot be renewed.
       While this is typically enough for performing a migration, increasing
       the DHCP lease time to 144 hours (518400 seconds) is recommended to
       ensure that lease expiration will not be a concern that affects your
       migration, even if unexpected delays in the process are encountered.
      </para>
     </listitem>
     <listitem>
      <para>
       To view the upcoming timeouts of DHCP leases:
      </para>
      <procedure>
       <step>
        <para>
         Determine which servers are running the DHCP agents
        </para>
        <screen><prompt>(old-deployer)> </prompt>neutron agent-list  | awk '/DHCP/{print $7}'
mig-cp1-neut-m2-mgmt
mig-cp1-neut-m1-mgmt</screen>
       </step>
       <step>
        <para>
         From the old deployer, SSH to each host listed above, and display
         the lease times for active VMs.
        </para>
        <screen><prompt>(old-deployer)> </prompt>ssh mig-cp1-neut-m2-mgmt
        <prompt>(mig-cp1-neut-m2)> </prompt>for i in $(cat /var/run/neutron/dhcp/*/leases | awk '{print $1}'); do
                     date -d @$i
                   done
<prompt>(mig-cp1-neut-m2)> </prompt>exit
...</screen>
       </step>
      </procedure>
     </listitem>
     <listitem>
      <para>
       To increase the DHCP lease time before migrating, open the
       <filename>roles/neutron-common/templates/neutron.conf.j2</filename>
       file from the <filename>~/helion/hos/ansible</filename> directory, and
       edit the <systemitem>dhcp_lease_duration</systemitem> parameter. Then,
       apply the change with:
      </para>
      <screen><prompt>(old-deployer)> </prompt>git commit -a -m "DHCP lease adjustment"
<prompt>(old-deployer)> </prompt>ansible-playbook -i hosts/localhost config-processor-run.yml
<prompt>(old-deployer)> </prompt>ansible-playbook -i hosts/localhost ready-deployment.yml
<prompt>(old-deployer)> </prompt>cd ~/scratch/ansible/next/hos/ansible
<prompt>(old-deployer)> </prompt>ansible-playbook -i hosts/verb_hosts neutron-reconfigure.yml </screen>
      <para>
       This will reset the duration for new leases, but the existing IP
       assignments to VMs will wait until their original DHCP lease expires
       before being refreshed with the updated lease duration. Thus, these
       steps should be performed at least 2/3rds of the lease time prior to
       the planned start of migration.
      </para>
      <para>
       Continue to monitor the upcoming DHCP lease expirations as described
       above until all existing leases have been refreshed to a lengthier period,
       and then continue with the migration.
      </para>
     </listitem>
    </itemizedlist>
   </step>
   <step>
    <para>
     Prepare the cloud data model for migration:
    </para>
    <itemizedlist>
     <listitem>
      <para>
       In the <filename>~/helion/my_cloud/definition/data/disks_*.yml</filename>
       files used by your server role that includes monitoring services, replace
       the existing <literal>Vertica</literal> partition definition with two
       <literal>Cassandra</literal> partitions:
      </para>
<screen>- name: cassandra_db
  size: (original size used by vertica partition, minus 1%)
  mount: /var/cassandra/data
  fstype: ext4
  mkfs-opts: -O large_file
  consumer:
    name: vertica
- name: cassandra_log
  size: 1%
  mount: /var/cassandra/commitlog
  fstype: ext4
  mkfs-opts: -O large_file</screen>
     </listitem>
     <listitem>
      <para>
       Edit
       <filename>~/helion/my_cloud/definition/data/control_plane.yml</filename>
       and identify the cluster definition that already includes an entry for
       <filename>mysql</filename> in its service-components.
      </para>
     </listitem>
     <listitem>
      <para>
       Ensure that <systemitem>lifecycle-manager</systemitem> is included in
       the same cluster's service-components list. If it was necessary to add
       this as a new entry, it will also need to be removed from its original
       position in the deployer.
      </para>
     </listitem>
     <listitem>
      <para>
       Update the <filename>servers.yml</filename> entry for
       all nodes in the control plane to specify <systemitem>distro-id:
       sles12sp3-x86_64</systemitem> in each one. (Otherwise, they will be
       re-imaged with hLinux.)
      </para>
      <note>
       <title>VMware ESX Nodes</title>
       <para>
        All ESX proxy and OVSvApp nodes should be included in the control
        plane updates to the new <systemitem>distro-id</systemitem> as
        described above.
       </para>
      </note>
     </listitem>
     <listitem>
      <para>
       If your environment includes a 3PAR storage backend, edit the file
       <filename>~/helion/my_cloud/config/cinder/cinder.conf.j2</filename>
       and update your driver to its new name (from <systemitem>HP3PARFCDriver</systemitem>
       to <systemitem>HPE3PARFCDriver</systemitem>, or from
       <systemitem>HP3PARISCSIDriver</systemitem> to <systemitem>HPE3PARISCSIDriver</systemitem>.)
      </para>
     </listitem>
     <listitem>
      <para>
       Commit your changes to the Git model:
      </para>
<screen><prompt>(old-deployer)> </prompt>git commit -a -m "Prepare for HOS 5.1 installation"</screen>
     </listitem>
    </itemizedlist>
   </step>
   <step>
    <para>
     If the intended seed node also currently hosts &o_netw; L3 routing services,
     evacuate the routers from the node so that it can be re-imaged without
     interruption to tenant networks.
    </para>
    <itemizedlist>
     <listitem>
      <para>
       In Distributed Virtual Router (DVR) enabled clouds, responsibility for
       network routing is shared across the compute nodes, and downtime of the
       seed node as a controller will have no adverse effect on workload network
       traffic. To see if DVR is enabled in your cloud, run the following command
       and check the entries in the "Distributed" column.
      </para>
<screen><prompt>(old-deployer)> </prompt>source ~/service.osrc
<prompt>(old-deployer)> </prompt>neutron router-list</screen>
      <para>
       If you observe non-distributed routers, then the next step will ensure
       that any responsibility for those routers' function is migrated away from
       the seed node to other nodes in the controller cluster, so that routing
       remains available even while the seed node is taken offline. (This step
       is not necessary if all your routers are distributed.)
      </para>
<screen><prompt>(old-deployer)> </prompt>cd ~/scratch/ansible/next/hos/ansible
<prompt>(old-deployer)> </prompt>cp -r ~/hlm-migration-tools/ansible/* ./
<prompt>(old-deployer)> </prompt>ansible-playbook -i hosts/verb_hosts \
  neutron-router-evacuate.yml --limit=<replaceable>SEED_HOST</replaceable></screen>
      <para>
       <replaceable>SEED_HOST</replaceable> represents the name of the seed node,
       for example, <systemitem>hos-cp1-dbmq-m3</systemitem>.
      </para>
      <important>
       <title>Host Selection</title>
       <para>
        For correct behavior, you must use the <command>--limit</command>
        argument to select the target seed host. This will output a success or
        error message at the end of its run.
       </para>
      </important>
     </listitem>
    </itemizedlist>
   </step>
   <step>
    <para>
     Re-image the seed controller as a &slsa;-based system:
    </para>
    <itemizedlist>
     <listitem>
      <para>
       Display the current network connections of the seed controller (before
       re-imaging) for reference and save it locally.
      </para>
<screen><prompt>(seed)> </prompt>ip a</screen>
     </listitem>
     <listitem>
      <para>
       Reinstall the operating system on the seed node's primary disk using an
       ISO, cobbler (from the existing deployer), or other provisioner.
       Note that support for provisioning with cobbler will be available only
       if the seed node is configured to use Legacy Boot Mode; a setting of
       UEFI Boot Mode will require manual installation.
      </para>
     </listitem>
     <listitem>
      <para>
       If using cobbler, begin the seed node re-imaging by running:
      </para>
<screen><prompt>(old-deployer)> </prompt>cd ~/helion/hos/ansible
<prompt>(old-deployer)> </prompt>sudo cobbler system remove --name <replaceable>SEED_NODE_ID</replaceable>
<prompt>(old-deployer)> </prompt>ansible-playbook -i hosts/localhost cobbler-deploy.yml
<prompt>(old-deployer)> </prompt>ansible-playbook -i hosts/localhost bm-reimage.yml -e nodelist=<replaceable>SEED_NODE_ID</replaceable></screen>
      <para>
       Replace <replaceable>SEED_NODE_ID</replaceable> with the ID of the seed
       node from the <filename>servers.yml</filename> file.
      </para>
      <para>
       Cobbler will initialize the seed node's networking to use the IP address
       of your original deployer as the default gateway. This will negatively
       impact the network's operation later on in the migration, and should be
       updated after the system is up and running. On the new seed node, edit
       or create the file <filename>/etc/sysconfig/network/routes</filename>
       and change the address in the line starting with "default" to point to
       your environment's actual default gateway. It has the format:
      </para>
      <screen>default 192.168.1.254 - -</screen>
      <para>
       You may be able to copy this from the value specified in that file on
       your existing deployer. Then, restart networking with:
      </para>
<screen><prompt>(seed)> </prompt>sudo systemctl restart network</screen>
     </listitem>
     <listitem>
      <para>
       Otherwise, if installing manually or using a proprietary tool for
       provisioning, reformat the primary partition (preserving the overall
       partitioning scheme based on the data model) and install the &sls; 12 SP3
       operating system. Follow the instructions as described in
       <xref linkend="cha.depl.dep_inst"/>, but when creating the user account
       and group, use the name <systemitem class="username">stack</systemitem>
       instead of <systemitem class="username">ardana</systemitem>.
      </para>
      <para>
       It is very important to avoid changing partitioning layouts or
       reformatting any volumes other than the primary partition
       during the installation.  This is necessary to preserve any
       &o_objstore; object data that may be present on the server.
      </para>
     </listitem>
    </itemizedlist>

   </step>
   <step>
    <para>
     Transfer configuration and files from your existing cloud to the new seed node
    </para>
    <itemizedlist>
     <listitem>
      <para>
       The disk volume on the new seed may be smaller than the corresponding volume
       on the old deployer. It may be necessary to firstremove extraneous files
       from your deployer (such as old qcow2 or ISO images), then transfer
       configuration and files from the old deployer to the new seed node:
      </para>
<screen><prompt>(old-deployer)> </prompt>rsync -avP ~ <replaceable>SEED_IP</replaceable>:/home
<prompt>(old-deployer)> </prompt>scp -r /tmp/hel* <replaceable>SEED_IP</replaceable>:/tmp</screen>
     </listitem>
     <listitem>
      <para>
       If cobbler will be used from the new seed for provisioning the remaining
       cloud &contrnode;s:
      </para>
      <itemizedlist>
       <listitem>
        <para>
         Ensure that the seed node is able to reach IPMI interfaces
         referenced in the <filename>servers.yml</filename> file:
        </para>
<screen><prompt>(seed)> </prompt>for s in `grep "ilo-ip" ~/helion/my_cloud/definition/data/servers.yml | grep -v "#" | cut -d ":" -f 2 | tr -d " " `
  do ping -q -c 2 $s
done</screen>
       </listitem>
       <listitem>
        <para>
         If additional routes are required for the seed to reach the IPMI
         interfaces, add these network routes to the seed. For example:
        </para>
        <screen><prompt>(seed)> </prompt>sudo ip route add <replaceable>IPMI_NETWORK</replaceable> via <replaceable>IPMI_GATEWAY_FROM_SEED</replaceable></screen>
       </listitem>
       <listitem>
        <para>
         Kill the DHCP services running on the old deployer:
        </para>
<screen><prompt>(old-deployer)> </prompt>sudo pkill dhcpd</screen>
       </listitem>
      </itemizedlist>
     </listitem>
    </itemizedlist>
   </step>
   <step>
    <para>
     On the new seed node, remove the package repository that was configured when
     its operating system was first installed. Then, mount the operating system
     image to make it available to the &hpecloud; installer.
    </para>
    <itemizedlist>
     <listitem>
      <important>
       <title>ISO filenames</title>
       <para>
        For correct behavior, the filenames of the images described below must match
        exactly their names in the user's home directory.
       </para>
      </important>
      <para>
       If your cloud includes &rhla; &compnode;s, ensure that
       <filename>~/rhel7.iso</filename> was either transferred from the original
       deployer or is downloaded to the user's home directory.
      </para>
      <para>
       Ensure that <filename>~/sles12sp3sdk.iso</filename> was either
       transferred from the original deployer or is downloaded to the user's home
       directory.
      </para>
      <para>
       Ensure that <filename>~/sles12sp3.iso</filename> was either transferred
       from the original deployer or is downloaded to the home directory on the
       seed node and named <filename>sles12sp3.iso</filename>. Then, mount it:
      </para>
<screen><prompt>(seed)> </prompt>sudo mkdir -p /media/cdrom
<prompt>(seed)> </prompt>sudo mount /home/stack/sles12sp3.iso /media/cdrom</screen>
      <para>
       If you installed the seed node using cobbler from the original deployer,
       replace the primary package repository on the new seed node with a link
       to the content delivered on the ISO media instead of a remote network
       location.
      </para>
<screen><prompt>(seed)> </prompt>sudo zypper rr SLES12-SP3-12.3-0
<prompt>(seed)> </prompt>sudo zypper ar file:/media/cdrom SLES12-SP3-12.3-0
<prompt>(seed)> </prompt>sudo zypper ref</screen>
     </listitem>
    </itemizedlist>
   </step>
   <step>
    <para>
     Install &hpecloud; 5.SCP on the seed node by downloading and unpacking
     &hpecloud; 5.SCP installation content:
    </para>
<screen><prompt>(seed)> </prompt>cd ~
<prompt>(seed)> </prompt>mkdir -p hos-5.scp
<prompt>(seed)> </prompt>sudo mount <replaceable>HELION5SCP_ISO_FILE</replaceable> hos-5.scp
<prompt>(seed)> </prompt>tar -xf hos-5.scp/hos/hos-5.1.0-*.tar
<prompt>(seed)> </prompt>sudo mkdir -p /var/log/apache2
<prompt>(seed)> </prompt>cd ~/hos-5.1.0
<prompt>(seed)> </prompt>./hos-init.bash</screen>
   </step>
   <step>
    <para>
     Update repository references of &compnode; as appropriate to point to
     the seed node.
    </para>
    <itemizedlist>
     <listitem>
      <para>
       On each SLES compute node, remove all the repositories except for
       <systemitem>SLES-OS</systemitem> and <systemitem>SLES-SDK</systemitem>.
       Use <filename>zypper lr</filename>, to list them and
       <filename>zypper rr &lt;REPO_NAME&gt;</filename> to remove them.
       If your compute nodes were kept up to date using the SUSE Updates
       repository via an SMT server, they will get future updates via the
       SMT server running on or accessible by the future deployer.
      </para>
     </listitem>
     <listitem>
      <para>
       On each &rhla; compute node, edit the repositories located in either
       <filename>/etc/yum.repos.d</filename> or <filename>/etc/yum/repos.d</filename>
       to redirect any IP addresses from the old deployer to point to the new
       seed node. (These references are likely to be found in the files
       <filename>cobbler-config.repo</filename> or <filename>rhel73.repo</filename>.)
      </para>
      <para>
       Then, refresh the yum cache by running:
      </para>
<screen><prompt>(rhel-compute)> </prompt>sudo yum clean all</screen>
     </listitem>
    </itemizedlist>
   </step>
   <step>
    <para>
     If VMware ESX compute hosts are present in the cloud, encrypt your ESX login
     credentials for embedding in the data model. This will require use of
     <systemitem>the HOS_USER_PASSWORD_ENCRYPT_KEY</systemitem> environment
     variable and the <filename>hosencrypt.py</filename> script, which
     is also used for encrypting the following other data model elements:
    </para>
    <itemizedlist>
     <listitem><para>Ironic OneView password</para></listitem>
     <listitem><para>Cinder back-end passwords (optional)</para></listitem>
     <listitem>
      <para>
       Barbican master key for <systemitem>simple_crypto_plugin</systemitem>
       backend (optional)
      </para>
     </listitem>
     <listitem><para>HPE DCN Nuage VSD password</para></listitem>
    </itemizedlist>
    <para>
     The use of <filename>hosencrypt.py</filename> for encryption is
     required for VMware ESX credentials (unlike some of the items above,
     which may optionally be stored in plaintext.)
    </para>
    <para>
     If you have used <filename>hosencrypt.py</filename> previously for
     any of the items above, then you have already selected an encryption key
     for use with your cloud. This encryption key must be exported into the
     environment variable <systemitem>HOS_USER_PASSWORD_ENCRYPT_KEY</systemitem>
     when the password is encrypted, and again before running playbooks such
     as <filename>site.yml</filename>. If this is your first time using such
     encryption, choose a new, unique encryption key and proceed:
    </para>
    <screen><prompt>(seed)> </prompt>export HOS_USER_PASSWORD_ENCRYPT_KEY=<replaceable>ENCRYPTIONKEY</replaceable>
<prompt>(seed)> </prompt>./hosencrypt.py
unencrypted value? <replaceable>&lt;VCENTERPASSWORD&gt;</replaceable>
@hos@U2FsdGVkX180/6dJIPPt49epntxAOFP0MtpjWsOaYzc=</screen>
    <para>
     Capture the resulting encrypted value and paste it into the
     <replaceable>password</replaceable> field of
     <filename>~/helion/my_cloud/definition/data/pass_through.yml</filename>,
     replacing the value previously set there by EON. If you have questions
     about the structure or content in pass_through.yml, comments regarding
     its use are provided with the &hpecloud; 5.SCP example data models such as in
     <filename>~/helion/examples/entry-scale-esx-kvm-vsa/data/pass_through.yml</filename>.
    </para>
   </step>
   <step>
    <para>
     In your cloud's data model, edit the control plane definition and
     add a new data model entry for <systemitem>nova-placement-api</systemitem>
     into the cluster which already includes <systemitem>nova-api</systemitem>.
    </para>
<screen><prompt>(seed)> </prompt>cd ~/helion/my_cloud/definition/data
<prompt>(seed)> </prompt>vi control_plane.yml
<prompt>(seed)> </prompt>git commit -a -m "Adding nova-placement-api"</screen>
   </step>
   <step>
    <para>
     Prepare for running the migration playbook:
    </para>
<screen><prompt>(seed)> </prompt>cd ~/helion/hos/ansible
<prompt>(seed)> </prompt>ansible-playbook -i hosts/localhost config-processor-run.yml
<prompt>(seed)> </prompt>ansible-playbook -i hosts/localhost ready-deployment.yml</screen>
   </step>
   <step>
    <para>
     Initialize ansible host facts and ensure that the existing cloud is fully
     addressable by the new deployer:
    </para>
<screen><prompt>(seed)> </prompt>cd ~/scratch/ansible/next/hos/ansible
<prompt>(seed)> </prompt>ansible-playbook -i hosts/verb_hosts hlm-refresh-facts.yml</screen>
   </step>
   <step>
    <note>
     <title>&ostack; Clouds Only</title>
     <para>
      The following step is only necessary on &ostack; clouds and should not be
      used in standalone &swift; deployments.
     </para>
    </note>
    <para>
     Flush the &o_ident; token cache by running the following command on any node
     hosting the Keystone API service. (This may be the seed node in an entry-scale
     environment, or a dedicated controller node in a mid-scale environment.)
    </para>
<screen><prompt>(controller)> </prompt>sudo /opt/stack/service/keystone/venv/bin/keystone-manage token_flush</screen>
    <para>
     Then, prune the primary database to remove data that is no longer relevant:
    </para>
<screen><prompt>(seed)> </prompt>cd ~/scratch/ansible/next/hos/ansible
<prompt>(seed)> </prompt>ansible-playbook -i hosts/verb_hosts percona-prune.yml</screen>
   </step>
  </procedure>

  <procedure>
   <title>Migration Phase</title>
   <warning>
    <title>Downtime</title>
    <para>
     The cloud will not be available until all of the following steps are
     completed.
    </para>
   </warning>
   <step>
    <para>
     If your cloud's data model is password-protected, use the following method
     to save the password in a local file before proceeding:
    </para>
<screen><prompt>(seed)> </prompt>touch ~/.vault_pass.txt
<prompt>(seed)> </prompt> chmod 600 ~/.vault_pass.txt
<prompt>(seed)> </prompt> vi ~/.vault_pass.txt</screen>
    <para>
     Ensure that the contents of the file are set to include only the
     plaintext vault password. Then, after saving it, run
     <command>export ANSIBLE_VAULT_PASSWORD_FILE=~/.vault_pass.txt</command>
     in preparation for upcoming steps.
    </para>
   </step>
   <step>
    <para>
     Stop or pause &ostack; services on the control plane (excluding the seed
     that has been freshly imaged):
    </para>
<screen><prompt>(seed)> </prompt>cd ~/scratch/ansible/next/hos/ansible
<prompt>(seed)> </prompt>./hlm-quiesce.sh --limit '!<replaceable>SEED_HOST</replaceable>'</screen>
    <para>
     The <replaceable>SEED_HOST</replaceable> name must exactly match the host
     name that is referenced in <filename>hosts/verb_hosts</filename>, such as
     <literal>ardana-cp1-c1-m1</literal>.
    </para>
   </step>
   <step>
    <para>
     Create a database backup by retrieving MySQL database contents from
     a running database node in the database cluster.
    </para>
<screen><prompt>(seed)> </prompt>cd ~/scratch/ansible/next/hos/ansible
<prompt>(seed)> </prompt>ansible-playbook -i hosts/verb_hosts percona-vertica-removal-cleanup.yml
<prompt>(seed)> </prompt>ansible-playbook -i hosts/verb_hosts percona-export.yml \
  -e dbcontent=~/mysql_dump.sql</screen>
   </step>
   <step>
    <para>
     Re-image the remaining servers in the control plane.
    </para>
    <itemizedlist>
     <listitem>
      <para>
       Re-image the remaining servers in the control plane. If the original deployer
       is a standalone deployer, then it can be preserved until later, to assist with
       recovery (if needed). If the deployer function is combined with a controller,
       then ensure that the previous section's steps to transfer configuration and
       files using rsync has completed so that no important data is lost.
      </para>
      <para>
       Reinstall the operating systems on the control plane primary disks using
       an ISO, cobbler, or other provisioner. If using cobbler, you will need to
       create the grub config files for all the SLES nodes using the following
       commands:
      </para>
<screen><prompt>(seed)> </prompt>cd ~/helion/hos/ansible
<prompt>(seed)> </prompt>ansible-playbook -i hosts/localhost cobbler-deploy.yml
<prompt>(seed)> </prompt>ansible-playbook -i hosts/localhost prepare-sles-grub2.yml \
  -e nodelist=<replaceable>NODE_IDs</replaceable>
<prompt>(seed)> </prompt>ansible-playbook -i hosts/localhost bm-reimage.yml \
-e nodelist=<replaceable>NODE_IDs</replaceable></screen>
      <note>
       <para>
        Do not reimage the &compnode;s, an original standalone deployer, or seed.
        Identify the set of remaining control plane nodes from the full list of
        systems addressable by cobbler, as shown by
        <command>cobbler system list</command>, and provide them as a comma-separated
        list in the <systemitem>nodelist</systemitem> parameter.
       </para>
      </note>
     </listitem>
     <listitem>
      <para>
       If installing the operating system manually (which can be done by booting
       from the &hpecloud; 5.SCP installation ISO) or using a proprietary tool for
       provisioning, reformat the primary partition (preserving the overall
       partitioning scheme based on the data model) and install the &sls; 12 SP3
       operating system. Follow the instructions as described in
       <xref linkend="cha.depl.dep_inst"/>, but when creating the user account
       and group, use the name <systemitem class="username">stack</systemitem>
       instead of <systemitem class="username">ardana</systemitem>.
      </para>
     </listitem>
     <listitem>
      <para>
       ESX (nova-proxy) nodes must also be rebuilt at this time, and must be done
       using the above-mentioned manual process (not using cobbler). For these
       virtual machines, reinstall the operating system using the &sls; 12 SP3
       installation ISO. Note that, when installing, you must select a
       partitioning method using logical volumes and establish a primary
       logical volume which is reasonably small in size (i.e. 50GB). It will
       be automatically expanded according to your data model during upcoming
       steps in the migration process. Again, the proper system user account name
       and group for these VMs is <systemitem class="username">stack</systemitem>.
      </para>
     </listitem>
     <listitem>
      <para>
       In a mid-scale cloud topology, some data files may have been
       stored on the non-primary drive(s) in the system. If this was the case,
       the <filename>wipe_disk.yml</filename> playbook should be used (after the
       nodes have been rebuilt) to purge the old data.
      </para>
      <para>
       Make a note of the output of the following command:
      </para>
<screen><prompt>(seed)> </prompt>cd ~/scratch/ansible/next/hos/ansible
<prompt>(seed)> </prompt>ansible-playbook -i hosts/verb_hosts hlm-refresh-facts.yml</screen>
      <para>
       Specify your own list of nodes matching the identifiers shown in the results
       of the previous command. However, do not run the <filename>wipe_disk.yml</filename>
       playbook on any &compnode;s or nodes containing &o_objstore; object storage.
      </para>
<screen><prompt>(seed)> </prompt>ansible-playbook -i hosts/verb_hosts wipe_disks.yml \
  --limit dbmq-m1,dbmq-m2,dbmq-m3,mtrmon-m1,mtrmon-m2,mtrmon-m3</screen>
     </listitem>
    </itemizedlist>
   </step>
   <step>
    <para>
     The repos should now be set up on all servers. Verify that this is the
     case with the following:
    </para>
<screen><prompt>(seed)> </prompt>cd ~/scratch/ansible/next/hos/ansible
<prompt>(seed)> </prompt>ansible resources -a "zypper ref" --become</screen>
    <para>
     All issues with the repositories must be resolved before continuing.
     All servers except &rhla; &compnode;s should complete successfully.
     All &slsa; servers should reference the repo on port 79 on the new seed.
    </para>
   </step>
   <step>
    <para>
     Create the log directory for the configuration processor, and then run the
     &hpecloud; 5.SCP installation. Enable database import from the snapshot that
     was captured from your cloud by using the <systemitem>dbcontent</systemitem>
     flag:
    </para>
<screen><prompt>(seed)> </prompt>mkdir -p /var/log/configuration_processor
<prompt>(seed)> </prompt>cd ~/scratch/ansible/next/hos/ansible
<prompt>(seed)> </prompt>ansible-playbook -i hosts/verb_hosts -vvv site.yml -e dbcontent=~/mysql_dump.sql</screen>
   </step>
   <step>
    <para>
     This will be a time-consuming installation, and should be allowed to
     complete without interruption. Once it is done, the cloud should again
     be operational, with the exception of monitoring components that will
     remain offline until the upgrade to &productname; &productnumber; is
     fully complete.
    </para>
   </step>
   <step>
    <para>
     If OVSvApp nodes are present in the environment, they should now be
     re-imaged with an operating system installed from the &sls; 12 SP3 ISO.
     Follow the directions above regarding ESX (nova-proxy) nodes exactly,
     doing the same for OVSvApp nodes.
    </para>
    <note>
     <para>
      Note that, while a VMware ESX host's OVSvApp node is offline, other running
      VMs on the same host will experience degraded network connectivity. To ensure
      maximum uptime, migrate all other VMs away from a host before reinstalling
      its OVSvApp node. Once the node is fully online again, the affected VMs
      can be returned to their original host.
     </para>
    </note>
    <para>
     After re-imaging an OVSvApp node, install &hpecloud; 5.SCP components on it
     from your seed using:
    </para>
<screen><prompt>(seed)> </prompt>ansible-playbook -i hosts/verb_hosts site.yml \
  -l '<replaceable>OVSVAPP_HOSTS</replaceable>'</screen>
    <para>
     More than one OVSvApp host can be installed at once by separating their names
     using commas. Note that each host should be identified by its Ansible-friendly
     name, matching the way it is referred to in the
     <filename>hosts/verb_hosts</filename> file.
    </para>
   </step>
  </procedure>

  <procedure>
   <title>Cleaning Up</title>
   <note>
    <title>No Downtime</title>
    <para>
     These steps do not cause service downtime and can be performed while the
     cloud is up and running.
    </para>
   </note>
   <step>
    <para>
     Deployer nodes offer limited disk space, and it is appropriate at this
     time to transfer the <filename>~/mysql_dump.sql</filename> file to an
     external location outside the cloud (where it can be available in case of
     any recovery needs) and remove it from the seed node:
    </para>
<screen><prompt>(seed)> </prompt>rm ~/mysql_dump.sql</screen>
   </step>
   <step>
    <para>
     If desired, transfer the deployer role in the &hpecloud; 5.SCP cloud to a
     different node, such as one that originally served as a dedicated
     deployer.
    </para>
    <itemizedlist>
     <listitem>
      <para>
       If the deployer will be moving to an entirely different cluster (for
       example, Database cluster to Controller cluster or standalone Deployer
       cluster), additional software packages need to be installed.
      </para>
      <itemizedlist>
       <listitem>
        <para>
         Edit
         <filename>~/helion/my_cloud/definition/data/control_plane.yml</filename>
         and move the <systemitem>lifecycle-manager</systemitem> from its
         current location into the new desired cluster's service-components list.
         Commit the change:
        </para>
<screen><prompt>(seed)> </prompt>git commit -a -m "Enable lifecycle-manager for new deployer"</screen>
       </listitem>
       <listitem>
        <para>
         Execute these steps to prepare for running playbooks:
        </para>
<screen>
<prompt>(seed)> </prompt>cd ~/helion/hos/ansible
<prompt>(seed)> </prompt>ansible-playbook -i hosts/localhost config-processor-run.yml
<prompt>(seed)> </prompt>ansible-playbook -i hosts/localhost ready-deployment.yml</screen>
       </listitem>
       <listitem>
        <para>
         At this point, it is appropriate to re-image a standalone deployer that
         would have been left intact through the upgrade steps to this point,
         to ensure that the &slsa; operating system is loaded.
        </para>
<screen><prompt>(seed)> </prompt> cd ~/helion/hos/ansible
<prompt>(seed)> </prompt>ansible-playbook -i hosts/localhost bm-reimage.yml \
  -e nodelist=<replaceable>ORIGINAL_DEPLOYER</replaceable></screen>
       </listitem>
       <listitem>
        <para>
         Install the <systemitem>lifecycle-manager</systemitem> services on the
         new deployer and remove cloud-wide references to the seed:
        </para>
<screen><prompt>(seed)> </prompt> cd ~/scratch/ansible/next/hos/ansible
<prompt>(seed)> </prompt>ansible-playbook -i hosts/verb_hosts site.yml -l <replaceable>NEW_DEPLOYER_HOST</replaceable>
<prompt>(seed)> </prompt>ansible resources -i hosts/verb_hosts -m shell \
  -a "zypper rr SLES12-SP3-12.3-0 || true" --become</screen>
        <para>
         <replaceable>NEW_DEPLOYER_HOST</replaceable> must match the node's identity in the
         file <filename>hosts/verb_hosts</filename> (for example,
         <systemitem>ardana-cp1-c0-m1</systemitem>)
        </para>
       </listitem>
      </itemizedlist>
     </listitem>
     <listitem>
      <para>
       Stop the deployer-related services and repositories running on the current seed:
      </para>
<screen><prompt>(seed)> </prompt>sudo systemctl stop dhcpd
<prompt>(seed)> </prompt>sudo systemctl disable dhcpd
<prompt>(seed)> </prompt>sudo systemctl stop cobblerd
<prompt>(seed)> </prompt>sudo systemctl disable cobblerd
<prompt>(seed)> </prompt>sudo sed -i /sles12sp3.iso/d /etc/fstab
<prompt>(seed)> </prompt>sudo umount /media/cdrom</screen>
     </listitem>
     <listitem>
      <para>
       If you used the cobbler mechanism from your seed node earlier to re-image
       the node which will now be serving as your new deployer, its zypper
       repos should be changed to use local sources instead of retrieving content
       from the seed node via the network:
      </para>
      <screen><prompt>(new-deployer)> </prompt>sudo zypper rr SLES-OS
<prompt>(new-deployer)> </prompt>sudo zypper ar file:/media/cdrom SLES12-SP3-12.3-0</screen>
      <para>
       The old repos on other &slsa; systems will automatically be reset to the
       new deployer later when the <filename>osconfig-run.yml</filename> playbook is run.
      </para>
     </listitem>
     <listitem>
      <para>
       Unmount any ISOs that are no longer necessary on the seed node:
      </para>
<screen><prompt>(seed)> </prompt>sudo umount /home/stack/hos5.scp</screen>
     </listitem>
     <listitem>
      <para>
       Copy the user's home directory and certificates from the seed to the new deployer:
      </para>
<screen><prompt>(seed)> </prompt>rsync -avP ~/ <replaceable>NEW_DEPLOYER_IP</replaceable>:/home/stack
<prompt>(seed)> </prompt>scp -r /tmp/hel* <replaceable>NEW_DEPLOYER_IP</replaceable>:/tmp</screen>
     </listitem>
     <listitem>
      <para>
       Connect to the new deployer and initialize it for use:
      </para>
<screen><prompt>(new-deployer)> </prompt>cd ~
<prompt>(new-deployer)> </prompt>rm -rf .ansible_fact_cache
<prompt>(new-deployer)> </prompt>sudo mkdir -p /media/cdrom
<prompt>(new-deployer)> </prompt>sudo mount /home/stack/sles12sp3.iso /media/cdrom
<prompt>(new-deployer)> </prompt>sudo mount <replaceable>HELION5SCP_ISO_FILE</replaceable> hos-5.scp
<prompt>(new deployer)> </prompt>tar -xf hos-5.scp/hos/hos-5.1.0-*.tar
<prompt>(new deployer)> </prompt>cd ~/hos-5.1.0
<prompt>(new deployer)> </prompt>./hos-init.bash
<prompt>(new deployer)> </prompt>cd ~/helion/hos/ansible
<prompt>(new deployer)> </prompt>ansible-playbook -i hosts/localhost cobbler-deploy.yml
<prompt>(new-deployer)> </prompt>ansible-playbook -i hosts/localhost config-processor-run.yml
<prompt>(new-deployer)> </prompt>ansible-playbook -i hosts/localhost ready-deployment.yml</screen>
     </listitem>
     <listitem>
      <para>
       On each &rhla; &compnode;, edit the repositories located in either
       <filename>/etc/yum.repos.d</filename> or <filename>/etc/yum/repos.d</filename>
       to redirect any IP addresses from the seed node to point to the new
       deployer. (These references are likely to be found in the files
       <filename>cobbler-config.repo</filename> or <filename>rhel73.repo</filename>.)
      </para>
      <para>
       Then, refresh the yum cache by running:
      </para>
<screen><prompt>(rhel-compute)> </prompt>sudo yum clean all</screen>
     </listitem>
     <listitem>
      <para>
       Use the new deployer to inform all other cloud nodes of its new role.
      </para>
<screen><prompt>(new-deployer)> </prompt>cd ~/scratch/ansible/next/hos/ansible
<prompt>(new-deployer)> </prompt>ansible-playbook -i hosts/verb_hosts osconfig-run.yml</screen>
     </listitem>
    </itemizedlist>
   </step>
  </procedure>
 </sect1>
 <sect1 xml:id="ardana.newton.migration">
  <title>Migration from &hpecloud; 5.SCP to Ardana.Newton</title>

  <para>
   The following procedure describes the process of migrating from &hpecloud;
   5.SCP to Ardana.Newton.
  </para>
  <para>
   From this point forward, all commands should be executed on the new deployer
   node (which will be the same as the earlier seed node unless this role was
   explicitly transferred using the instructions at the end of the prior section).
  </para>
  <procedure>
   <title>Preparation</title>
   <step xml:id="st.migrate.git-convert">
    <para>
     Convert the <filename>helion</filename> Git repository to an
     <filename>openstack</filename> Git repository for use by Ardana:
    </para>
<screen><prompt>(stack)> </prompt>cd ~
<prompt>(stack)> </prompt>cp -r helion openstack
<prompt>(stack)> </prompt>cd openstack
<prompt>(stack)> </prompt>git checkout site</screen>
   </step>
   <step>
    <para>
     If your environment included a SES storage backend, remove all references
     to it from your data model. This may be done by using <command>git revert
     <replaceable>COMMIT-ID</replaceable></command> to revert a specific commit
     including the SES backend definition, or <command>git checkout hos --
     <replaceable>FILENAME</replaceable></command> to restore an entire file to
     its original version (followed by <command>git diff --staged</command> to
     inspect the difference between files and ensure that you ar satisfied with
     the final condition of the configuration file.) Committing these edits will
     be necessary for the following files in the <filename>~/openstack</filename>
     git repository:
    </para>
    <itemizedlist>
     <listitem>
      <para>
       <filename>hos/ansible/roles/_CND-CMN/templates/cinder.conf.j2</filename>
      </para>
     </listitem>
     <listitem>
      <para>
       <filename>hos/ansible/roles/NOV-CMP-KVM/templates/hypervisor.conf.j2</filename>
      </para>
     </listitem>
     <listitem>
      <para>
       <filename>hos/ansible/roles/GLA-API/templates/glance-api.conf.j2</filename>
       (if the SES backend was additionally configured for use with &o_img;)
      </para>
     </listitem>
    </itemizedlist>
   </step>
   <step>
    <para>
     Save a record of all other manual changes that have been previously committed
     to the <filename>hos/ansible</filename> directory, as you will need to
     reapply them later:
    </para>
<screen><prompt>(stack)> </prompt>git diff hos -- hos/ansible > /tmp/hos.diff</screen>
    <para>
     Forcibly undo all the changes identified:
    </para>
<screen><prompt>(stack)> </prompt>rm -rf hos/ansible
<prompt>(stack)> </prompt>git checkout hos -- hos/ansible</screen>
   </step>
   <step>
    <para>
     Reset the codebase to make it ready for Ardana, then transform the rest of
     the repository:
    </para>
<screen><prompt>(stack)> </prompt>git commit -a -m "Resetting codebase in prep for Ardana"
<prompt>(stack)> </prompt>git branch -m hos ardana
<prompt>(stack)> </prompt>git checkout ardana
<prompt>(stack)> </prompt>sed -i 's/hos/ardana/' .gitignore
<prompt>(stack)> </prompt>git commit -a -m "Prep for Ardana"
<prompt>(stack)> </prompt>git mv hos ardana
<prompt>(stack)> </prompt>git mv hos_extensions ardana_extensions
<prompt>(stack)> </prompt>for i in $(find -L my_cloud/config -xtype l)
  do ln -sfn $(readlink $i | sed 's/\/hos\//\/ardana\//') $i
done
<prompt>(stack)> </prompt>git commit -a -m "Conversion to Ardana"
<prompt>(stack)> </prompt>git checkout cp-persistent
<prompt>(stack)> </prompt>sed -i 's/component: helion-ca/component: ardana-ca/' \
  my_cloud/persistent_state/private_data_control-plane-1.yml
<prompt>(stack)> </prompt>sed -i 's/helion_internal_ca/ardana_internal_ca/' \
  my_cloud/persistent_state/private_data_control-plane-1.yml
<prompt>(stack)> </prompt>git commit -a -m "Conversion to Ardana"
<prompt>(stack)> </prompt>git checkout site
<prompt>(stack)> </prompt>git merge ardana -m "Conversion to Ardana"
</screen>
    <para>
     Any merge conflicts that arise through this procedure can be addressed
     in the manner described in <xref linkend="git_merge"/>.
    </para>
   </step>
   <step>
    <para>
     If the file /tmp/hos.diff contains changes that were saved prior to the
     repo conversion above, reinstate them in the
     <filename>ardana/ansible</filename> directory.
    </para>
<screen><prompt>(stack)> </prompt>if [ -s /tmp/hos.diff ]; then
  sed 's/ \([ab]\?[\/]\?\)hos\/ansible/ \1ardana\/ansible/g' /tmp/hos.diff | git apply
fi</screen>
    <para>
     If merge conflicts are reported, some files may need to be edited manually
     to reconcile the updates to file paths and directory structure that make
     the codebase suitable for upgrading to Ardana.Newton. Further information
     about merge operations can be found at <xref linkend="git_merge"/>.
    </para>
<screen><prompt>(stack)> </prompt>git add -A
<prompt>(stack)> </prompt>git commit -a -m "Reinstated hos playbook customizations (batch)"
</screen>
   </step>
   <step>
    <para>
     Remove and archive the &hpecloud; 5.SCP-related content:
    </para>
<screen><prompt>(stack)> </prompt>mv ~/scratch ~/scratch.scp
<prompt>(stack)> </prompt>sudo rm /etc/apache2/sites-enabled/deployer_venv_server.conf</screen>
   </step>
   <step>
    <para>
     Prepare the <filename>ardana_packager</filename> working directory to
     include existing &suse; repositories:
    </para>
<screen><prompt>(stack)> </prompt>sudo mkdir -p /opt/ardana_packager/ardana/sles12/zypper
<prompt>(stack)> </prompt>sudo ln -s /opt/hlm_packager/hlm /opt/ardana_packager/hlm
<prompt>(stack)> </prompt>sudo ln -s /opt/hlm_packager/hlm/sles12/zypper/OS \
  /opt/ardana_packager/ardana/sles12/zypper/OS</screen>
   </step>
   <step>
    <para>
     Install Ardana.Newton content onto the deployer by mounting the installation
     media and running the following commands:
    </para>
<screen><prompt>(stack)> </prompt>cd ~
<prompt>(stack)> </prompt>sudo mkdir -p /media/ardana-0.35.0
<prompt>(stack)> </prompt>sudo mount ~/<replaceable>HELION5CLM_ISO_FILE</replaceable> /media/ardana-0.35.0
<prompt>(stack)> </prompt>tar -xf /media/ardana-0.35.0/ardana/ardana-0.35.0*.tar
<prompt>(stack)> </prompt>cd ardana-0.35.0
<prompt>(stack)> </prompt>./ardana-init.bash</screen>
    <para>
     If merge conflicts are reported, some files may need to be edited manually
     to reconcile updates being introduced to the playbooks by the Ardana.Newton
     codebase. Use caution when evaluating these merge conflicts, and ensure
     that the only changes given priority over the incoming playbooks are those
     which are specifically relevant to your cloud's configuration and should be
     retained into the future. Further information about merge operations can be
     found at <xref linkend="git_merge"/>.
    </para>
   </step>
   <step>
    <para>
     Install Ardana.Newton:
    </para>
<screen><prompt>(stack)> </prompt>cd ~/openstack/ardana/ansible
<prompt>(stack)> </prompt>ansible-playbook -i hosts/localhost config-processor-run.yml
<prompt>(stack)> </prompt>ansible-playbook -i hosts/localhost ready-deployment.yml

<prompt>(stack)> </prompt>cd ~/scratch.scp/ansible/next/hos/ansible/
<prompt>(stack)> </prompt>ansible-playbook -i hosts/verb_hosts osconfig-iptables-rename.yml
<prompt>(stack)> </prompt>ansible -i hosts/verb_hosts resources --become \
  -a "sed -i 's/helion/ardana/' /etc/iproute2/rt_tables"</screen>
    <para>
     Because the OpenStack service versions are comparable between &hpecloud;
     5.0.x and Ardana.Newton, it is not necessary to run the entire
     deploy/upgrade playbook for Ardana.Newton at this time.
    </para>
   </step>
  </procedure>
 </sect1>
 <sect1 xml:id="ardana.pike.migration">
  <title>Migration from Ardana.Newton to Ardana.Pike</title>
  <procedure>
   <step>
    <para>
     Create the <systemitem>ardana</systemitem> user throughout all nodes in the
     cloud:
    </para>
<screen><prompt>(stack)> </prompt>cd ~/scratch/ansible/next/ardana/ansible
<prompt>(stack)> </prompt>ansible-playbook -i hosts/verb_hosts ardana-user-create.yml</screen>
   </step>
   <step>
    <para>
     Mount the &productname; &productnumber; installation media on the host
     (after copying the ISO file into the user's home directory)
    </para>
<screen><prompt>(stack)> </prompt>sudo mkdir -p /media/cloud
<prompt>(stack)> </prompt>echo /home/stack/HPE-HELION-OPENSTACK-8-x86_64-GM-DVD1.iso \
  /media/cloud iso9660 loop 0 0 | sudo tee -a /etc/fstab
<prompt>(stack)> </prompt>sudo mount -a</screen>
   </step>
   <step>
    <para>
     Add the contents of the ISO as a Zypper repository:
    </para>
<screen><prompt>(stack)> </prompt>sudo zypper ar -G -f file:/media/cloud Cloud
<prompt>(stack)> </prompt>sudo zypper refresh</screen>
   </step>
   <step>
    <para>
     Begin the installation:
    </para>
<screen><prompt>(stack)> </prompt>sudo zypper -n in patterns-cloud-ardana</screen>
   </step>
   <step>
    <para>
     Before continuing the installation, add a password of your choosing
     to the new <systemitem>ardana</systemitem> user:
    </para>
<screen><prompt>(stack)> </prompt>sudo passwd ardana</screen>
   </step>
   <step>
    <para>
     Then, <command>exit</command> your current session on the deployer
     and open a new session as the <systemitem>ardana</systemitem> user,
     logging in with the password that you just established. From this
     point on, the <systemitem>ardana</systemitem> user will be used
     for all operations, and the <systemitem>stack</systemitem> user should
     be considered deprecated.
    </para>
    <para>
     To ensure that the <systemitem>stack</systemitem> user does not continue
     to be used, locking its account with the following command
     is highly recommended:
    </para>
<screen>&prompt.ardana;sudo usermod -L stack
&prompt.ardana;sudo rm -rf ~stack/.ssh/authorized_keys
&prompt.ardana;sudo mv ~stack/scratch ~stack/scratch.newton
&prompt.ardana;sudo mv ~stack/helion ~stack/helion.newton</screen>
   </step>
   <step>
    <para>
     Continue the installation as the <systemitem>ardana</systemitem> user:
    </para>
<screen>&prompt.ardana;ARDANA_INIT_AUTO=1 /usr/bin/ardana-init</screen>
    <para>
     If merge conflicts are reported, some files may need to be edited manually
     to reconcile the difference between playbook versions with
     respect to manual changes previously made for your cloud. Use caution
     when evaluating these merge conflicts, and ensure that the only changes
     given priority over the incoming playbooks are those which are
     specifically relevant to your cloud's configuration and should be
     retained into the future. Further information about merge operations
     can be found at <xref linkend="git_merge"/>
    </para>
   </step>
   <step>
    <para>
     The installation process creates the directory
     <filename>/srv/www/suse-12.3/x86_64/repos/PTF</filename> and adds it as
     a software repository. To enable a successful migration, you must add
     required Program Temporary Fix (PTF) RPM packages to this repository
     manually. These packages are provided to you by &suse; via an active
     L3 support contract.
    </para>
    <para>
     To add the PTF RPMs manually, the RPMs should be copied into the
     <filename>/srv/www/suse-12.3/x86_64/repos/PTF</filename> directory. For
     example, if the downloaded RPMs are located in <filename>/tmp/PTF</filename>:
    </para>
    <screen>&prompt.ardana;sudo cp /tmp/PTF/*.rpm /srv/www/suse-12.3/x86_64/repos/PTF</screen>
   </step>
   <step>
    <para>
     Apply the PTF packages obtained in the previous step and confirm
     that they install correctly:
    </para>
<screen>&prompt.ardana;cd ~/openstack/ardana/ansible
&prompt.ardana;ansible-playbook -i hosts/localhost deployer-init.yml
&prompt.ardana;sudo zypper refresh PTF
&prompt.ardana;sudo zypper dup --from PTF
&prompt.ardana;sudo zypper in ardana-ses
&prompt.ardana;rpm -qa | grep PTF</screen>
    <para>
     Ensure that all migration RPMs starting with the prefix "ardana-" available
     via your &suse; support contract are present in the resulting list. If
     any are missing, repeat the above process or contact technical support
     before continuing.
    </para>
   </step>
   <step>
    <para>
     Apply the updates received via PTF packages to your active cloud:
    </para>
<screen>&prompt.ardana;/usr/bin/ardana-init</screen>
   </step>
   <step>
    <para>
     Set the default &o_img; store to <systemitem>hlm-default-store</systemitem>
     so that your existing image content remains accessible following
     the upgrade.
    </para>
<screen>&prompt.ardana;cd ~/openstack/ardana/ansible
&prompt.ardana;sed -i 's/ardana-default-store/hlm-default-store/' \
  roles/GLA-API/defaults/main.yml</screen>
    <para>
     Update certificate references to use the new "ardana" name:
    </para>
<screen>&prompt.ardana;sed -i 's/cert-file: helion/cert-file: ardana/' \
  ../../my_cloud/definition/data/network_groups.yml \
  ../../my_cloud/definition/data/control_plane.yml</screen>
    <para>
     Adjust data model to install <literal>cassandra</literal> instead of
     <literal>vertica</literal>. Manually replace vertica with cassandra in the
     service-components list, and remove any entry for ops-console-monitor:
    </para>
<screen>&prompt.ardana;vi ../../my_cloud/definition/data/control_plane.yml</screen>
    <para>
     In files where a <systemitem>cassandra_db</systemitem> partition
     exists, manually update its consumer name from <literal>vertica</literal> to
     <literal>cassandra</literal> and add an identical
     <literal>consumer</literal> block to the
     <systemitem>cassandra_log</systemitem> partition definition with:
    </para>
<screen>&prompt.ardana;vi ../../my_cloud/definition/data/disks_*</screen>
    <para>
     For example:
    </para>
    <screen>- name: cassandra_db
  size: 20%
  mount: /var/cassandra/data
  fstype: ext4
  mkfs-opts: -O large_file
  consumer:
    name: cassandra

- name: cassandra_log
  size: 1%
  mount: /var/cassandra/commitlog
  fstype: ext4
  mkfs-opts: -O large_file
  consumer:
    name: cassandra</screen>
    <para>
     If your environment includes a SES backend, add configuration files to enable
     it based on the <xref linkend="ses.integration"/> instructions. (However,
     the <filename>site.yml</filename> and
     <filename>ardana-reconfigure.yml</filename> playbooks should not be used at
     this time.)
    </para>
    <para>
     Save the updates to the data model:
    </para>
<screen>&prompt.ardana;git commit -a -m 'Updated data model'</screen>
   </step>
   <step>
    <para>
     Build the <filename>~/scratch</filename> directory for the upgrade:
    </para>
<screen>&prompt.ardana;ansible-playbook -i hosts/localhost config-processor-run.yml
&prompt.ardana;ansible-playbook -i hosts/localhost ready-deployment.yml</screen>
   </step>
   <step>
    <para>
     Prepare the servers for Pike infrastructure:
    </para>
<screen>&prompt.ardana;cd ~/scratch/ansible/next/ardana/ansible
&prompt.ardana;ansible-playbook -i hosts/verb_hosts FND-AP2-vhosts-transition.yml
&prompt.ardana;sudo rename helion ardana /tmp/helion_*
&prompt.ardana;sudo rename helion ardana /tmp/ardana_*/helion*</screen>
   </step>
   <step>
    <para>
     Set up and verify package repositories for installation:
    </para>
<screen>&prompt.ardana;sudo ln -s /media/cloud /srv/www/suse-12.3/x86_64/repos/Cloud
&prompt.ardana;sudo ln -s /opt/hlm_packager/hlm/sles12/zypper/extras/sles12sp3-ceph-updates \
  /srv/www/suse-12.3/x86_64/repos/sles12sp3-ceph-updates
&prompt.ardana;ansible-playbook -i hosts/verb_hosts ardana-replace-legacy-repos.yml</screen>
    <para>
     Check that the repos have been properly set up, and that refreshing them
     succeeds for all &slsa; cloud nodes:
    </para>
<screen>&prompt.ardana;ansible resources -m shell -a "zypper ref" --become</screen>
   </step>
   <step>
    <para>
     If your environment includes &rhla; &compnode;s, prepare your deployer for
     provisioning &ostack; services on them by following the steps described in
     <xref linkend="install_rhel_compute_node"/>.
    </para>
    <note>
     <para>
      If you opt to install an SMT server on your new deployer for mirroring
      the <literal>CentOS</literal> repo, you must first run the command
      <command>sudo ln -s /var/lib/mysql/mysql.sock /var/run/mysql/mysql.sock</command>
      to enable SMT access to the database. (This access is enabled permanently
      upon completion of the upgrade, but must be manually set up if needed
      at this time.)
     </para>
     <para>
      If the CA Management procedure is used to set up a root CA certificate in
      support of the deployer SMT server, you must immediately follow it by running
      <command>sudo ln -s /etc/ssl/ca-bundle.pem /etc/ssl/certs/ca-certificates.crt</command>
      before proceeding with the software upgrade.
     </para>
    </note>
   </step>
   <step>
   <para>
    Run the &productname; &productnumber; upgrade playbook:
   </para>
<screen>&prompt.ardana;ansible-playbook -i hosts/verb_hosts -vvv ardana-upgrade-from-legacy.yml</screen>
    <para>
     Note that the <filename>ardana-upgrade-from-legacy.yml</filename> playbook is
     not guaranteed to be idempotent, and re-running it upon a failure may produce
     new errors. If this playbook fails, please contact customer support and be
     prepared to share details of the failure from the contents of
     <filename>~/.ansible/ansible.log</filename> to determine appropriate next
     steps.
    </para>
   </step>
   <step>
    <para>
     Now that the upgrade has completed, it will be necessary to reboot the
     control plane nodes in your cloud for changes to take effect. At this time,
     reboot only the control plane nodes (not compute nodes) in your cloud per
     the instructions in <xref linkend="stop_restart"/>.
    </para>
   </step>
   <step>
    <para>
     If your cloud includes OpenStack-provisioned load balancers, the following
     step is necessary to update the Amphora load balancer image used by the
     Octavia service:
    </para>
<screen>&prompt.ardana;ansible-playbook -i hosts/verb_hosts -vvv service-guest-image \
  -e service_package=$(ls /srv/www/suse-12.3/x86_64/repos/PTF/openstack-octavia-amphora-image*.rpm)</screen>
   </step>
   <step>
    <para>
     After updating the Amphora image with the above playbook, the following
     steps are recommended to confirm that load balancer functionality works
     as expected. (This will ensure that your workloads remain available through
     the upcoming reboots of compute nodes.)
    </para>
    <itemizedlist>
     <listitem>
      <para>
       Confirm using <command>neutron lbaas-loadbalancer-list</command> that any
       existing load balances are online and reporting with state
       <systemitem>ACTIVE</systemitem>.
      </para>
     </listitem>
     <listitem>
      <para>
       Create a new load balancer with <command>neutron lbaas-loadbalancer-create
       migration_test <replaceable>OCTAVIA-MGMT-NET-ID</replaceable></command>.
       (The ID of OCTAVIA-MGMT-NET is available from the list reported by
       <command>openstack network list</command>.)
      </para>
     </listitem>
     <listitem>
      <para>
       Run <command>neutron lbaas-loadbalancer-list</command> to ensure that the 
       new load balancer appears and becomes <systemitem>ACTIVE</systemitem>,
       then try using <command>nova delete</command> to remove its associated VM
       and ensure that it is recreated by the failover mechanism.
      </para>
     </listitem>
    </itemizedlist>
   </step>
   <step>
    <para>
     You may now proceed to reboot the compute nodes in your cloud for all
     changes to fully take effect and re-enable normal behavior. Continue to refer
     to the steps described in <xref linkend="stop_restart"/>.
    </para>
   </step>
   <step>
    <para>
     At this time, the PTF and other temporary packages can be removed:
    </para>
<screen>&prompt.ardana;cd ~/scratch/ansible/next/ardana/ansible
&prompt.ardana;ansible -i hosts/verb_hosts resources --become -m shell \
  -a "zypper rr sles12sp3-ceph-updates || true"
&prompt.ardana;sudo rm /srv/www/suse-12.3/x86_64/repos/sles12sp3-ceph-updates
&prompt.ardana;sudo rm -rf /srv/www/suse-12.3/x86_64/repos/PTF/*.rpm
&prompt.ardana;cd ~/openstack/ardana/ansible
&prompt.ardana;ansible-playbook -i hosts/localhost deployer-init.yml
&prompt.ardana;sudo zypper refresh PTF
&prompt.ardana;sudo zypper dup --from Cloud</screen>
    <note>
     <para>
      When running <command>sudo zypper dup --from Cloud</command>, warnings will
      appear about packages being downgraded. This is expected and should be
      acknowledged as you proceed.
     </para>
    </note>
   </step>
   <step>
    <para>
     Determine how to handle modified policy files which restrict the admin
     user's privileges more tightly than in previous releases:
    </para>
    <para>
     With this upgrade, the policy.json files have been aligned with
     &ostack-current;. When running &ostack; commands an operator may see
     messages saying <literal>Policy doesn't allow ...</literal> such as:
    </para>
    <screen>Policy doesn't allow volume_extension:services:index to be performed</screen>
    <para>
     In this case the operator can be given permission by adding a role to the
     user using the <command>openstack role add</command> command, for example:
    </para>
<screen>&prompt.ardana;openstack role add --user <replaceable>USER_NAME</replaceable> \
  --project <replaceable>PROJECT_NAME</replaceable> cinder_admin</screen>
    <para>
     Role assignments should be carefully considered. In an environment which
     requires separation of duties, each operator should be assigned only the
     roles they require to perform their duties and should use their own user
     account to perform their administrative tasks. For more information see
     the <xref linkend="roleSegregation"/>.
    </para>
   </step>
  </procedure>
 </sect1>
 <sect1>
  <title>Applying Product Updates</title>
  <procedure>
   <step>
    <para>
     Before applying updates to your cloud, you should replace the repositories
     configured on each node during the migration with links to an &smt; server
     that mirrors the latest versions of your software. To begin this process,
     register your deployer node and install the &hpecloud; extension as described
     in <xref linkend="sec.depl.adm_inst.add_on"/>.
    </para>
    <para>
     The easiest way to provide the required repositories on the &clm; Server is
     to configure it as an &smt; server as described in
     <xref linkend="app.deploy.smt_lcm"/>. Alternatives to setting up an
     &smt; server are described in <xref linkend="cha.depl.repo_conf_lcm"/>.
    </para>
   </step>
   <step>
    <para>
     Next, use the following commands to replace the original ISO repositories
     with the latest package sources provided by &suse;:
    </para>
<screen>&prompt.ardana;cd ~/scratch/ansible/next/ardana/ansible
&prompt.ardana;sudo rm /opt/hlm_packager/hlm/sles12/zypper/OS /srv/www/suse-12.3/x86_64/repos/Cloud
&prompt.ardana;ansible resources -i hosts/verb_hosts -m shell \
  -a "zypper rr Cloud; zypper rr SLES-OS || true" --become
&prompt.ardana;ansible-playbook -i hosts/verb_hosts _osconfig-setup-repos.yml</screen>
   </step>
   <step>
    <para>
     Ensure that the "Cloud" repository has been replaced
     by the following repositories on all &slsa; nodes:
    </para>
    <itemizedlist>
     <listitem>
      <para>
       SLES12-SP3-Pool
      </para>
     </listitem>
     <listitem>
      <para>
       SLES12-SP3-Updates
      </para>
     </listitem>
     <listitem>
      <para>
       HPE-Helion-OpenStack-8-Pool
      </para>
     </listitem>
     <listitem>
      <para>
       HPE-Helion-OpenStack-8-Update
      </para>
     </listitem>
    </itemizedlist>
   </step>
   <step>
    <para>
     Finally, apply the latest versions of software packages using the process described in
     <xref linkend="maintenance_update"/>.
    </para>
   </step>
  </procedure>
 </sect1>
</article>
