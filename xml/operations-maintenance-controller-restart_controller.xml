<?xml version="1.0"?>
<?xml-stylesheet href="urn:x-suse:xslt:profiling:docbook51-profile.xsl"
 type="text/xml"
 title="Profiling step"?>
<!DOCTYPE section [
 <!ENTITY % entities SYSTEM "entities.ent"> %entities;
]>
<section xmlns="http://docbook.org/ns/docbook" xmlns:xi="http://www.w3.org/2001/XInclude" xmlns:xlink="http://www.w3.org/1999/xlink" xml:id="recover_downed_cluster">
 <title>Restarting Controller Nodes After a Reboot</title>
 <para>
  Steps to follow if one or more of your controller nodes lose network
  connectivity or power, which includes if the node is either rebooted or needs
  hardware maintenance.
 </para>
<!-- Comment from DITA original: -->
<!-- WHEN, WHY? -->
 <para>
  When a controller node is rebooted, needs hardware maintenance, loses
  network connectivity or loses power, these steps will help you recover the
  node.
 </para>
 <para>
  These steps may also be used if the Host Status (ping) alarm is triggered
  for one or more of your controller nodes.
 </para>
<!-- Comment from DITA original: -->
<!-- HOW, WHO, WHERE -->
 <section xml:id="idg-all-operations-maintenance-controller-restart_controller-xml-7">
  <title>Prerequisites</title>
  <para>
   The following conditions must be true in order to perform these steps
   successfully:
  </para>
  <itemizedlist>
   <listitem>
    <para>
     Each of your controller nodes should be powered on.
    </para>
   </listitem>
   <listitem>
    <para>
     Each of your controller nodes should have network connectivity, verified
     by SSH connectivity from the &lcm; to them.
    </para>
   </listitem>
   <listitem>
    <para>
     The operator who performs these steps will need access to the lifecycle
     manager.
    </para>
   </listitem>
  </itemizedlist>
 </section>
 <section xml:id="mysql">
  <title>Recovering the &mariadb; Database</title>
  <para>
   The recovery process for your &mariadb; database cluster will depend on how
   many of your controller nodes need to be recovered. We will cover two
   scenarios:
  </para>
  <para>
   <emphasis role="bold">Scenario 1: Recovering one or two of your controller
   nodes but not the entire cluster</emphasis>
  </para>
  <para>
   Follow these steps to recover one or two of your controller nodes but not the
   entire cluster, then use these steps:
  </para>
  <procedure>
   <step>
    <para>
     Ensure the controller nodes have power and are booted to the command
     prompt.
    </para>
   </step>
   <step>
    <para>
     If the &mariadb; service is not started, start it with this command:
    </para>
<screen>sudo service mysql start</screen>
   </step>
   <step>
    <para>
     If &mariadb; fails to start, proceed to the next section which covers the
     bootstrap process.
    </para>
   </step>
  </procedure>
  <para>
   <emphasis role="bold">Scenario 2: Recovering the entire controller cluster
   with the bootstrap playbook</emphasis>
  </para>
  <para>
   If the scenario above failed or if you need to recover your entire control
   plane cluster, use the process below to recover the &mariadb; database.
  </para>
  <procedure>
   <step>
    <para>
     Make sure no <literal>mysqld</literal> daemon is running on any node in
     the cluster before you continue with the steps in this procedure. If there
     is a <literal>mysqld</literal> daemon running, then use the command below
     to shut down the daemon.
    </para>
    <screen>sudo systemctl stop mysql</screen>
    <para>
     If the mysqld daemon does not go down following the service stop, then
     kill the daemon using <literal>kill -9</literal> before continuing.
    </para>
   </step>
   <step>
    <para>
     On the deployer node, execute the
     <filename>galera-bootstrap.yml</filename> playbook which will
     automatically determine the log sequence number, bootstrap the main node,
     and start the database cluster.
    </para>
<screen>cd ~/scratch/ansible/next/ardana/ansible
ansible-playbook -i hosts/verb_hosts galera-bootstrap.yml</screen>
   </step>
  </procedure>
 </section>
 <section xml:id="hlm">
  <title>Restarting Services on the Controller Nodes</title>
  <para>
   From the &lcm; you should execute the
   <literal>ardana-start.yml</literal> playbook for each node that was brought
   down so the services can be started back up.
  </para>
  <para>
   If you have a dedicated (separate) &lcm; node you can use this
   syntax:
  </para>
<screen>cd ~/scratch/ansible/next/ardana/ansible
ansible-playbook -i hosts/verb_hosts ardana-start.yml --limit=&lt;hostname_of_node&gt;</screen>
  <para>
   If you have a shared &lcm;/controller setup and need to restart
   services on this shared node, you can use <literal>localhost</literal> to
   indicate the shared node, like this:
  </para>
<screen>cd ~/scratch/ansible/next/ardana/ansible
ansible-playbook -i hosts/verb_hosts ardana-start.yml --limit=&lt;hostname_of_node&gt;,localhost</screen>
  <note>
   <para>
    If you leave off the <literal>--limit</literal> switch, the playbook will
    be run against all nodes.
   </para>
  </note>
 </section>
 <section xml:id="monasca">
  <title>Restart the Monitoring Agents</title>
  <para>
   As part of the recovery process, you should also restart the
   <literal>monasca-agent</literal> and these steps will show you how:
  </para>
  <orderedlist>
   <listitem>
    <para>
     Log in to the &lcm;.
    </para>
   </listitem>
   <listitem>
    <para>
     Stop the <literal>monasca-agent</literal>:
    </para>
<screen>cd ~/scratch/ansible/next/ardana/ansible
ansible-playbook -i hosts/verb_hosts monasca-agent-stop.yml</screen>
   </listitem>
   <listitem>
    <para>
     Restart the <literal>monasca-agent</literal>:
    </para>
<screen>cd ~/scratch/ansible/next/ardana/ansible
ansible-playbook -i hosts/verb_hosts monasca-agent-start.yml</screen>
   </listitem>
   <listitem>
    <para>
     You can then confirm the status of the <literal>monasca-agent</literal>
     with this playbook:
    </para>
<screen>cd ~/scratch/ansible/next/ardana/ansible
ansible-playbook -i hosts/verb_hosts monasca-agent-status.yml</screen>
   </listitem>
  </orderedlist>
 </section>
</section>
