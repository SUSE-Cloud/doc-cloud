<?xml version="1.0"?>
<?xml-stylesheet href="urn:x-suse:xslt:profiling:docbook51-profile.xsl"
 type="text/xml"
 title="Profiling step"?>
<!DOCTYPE section [
 <!ENTITY % entities SYSTEM "entity-decl.ent"> %entities;
]>
<section xmlns="http://docbook.org/ns/docbook" xmlns:xi="http://www.w3.org/2001/XInclude" xmlns:xlink="http://www.w3.org/1999/xlink" xml:id="full_recovery_test">
 <title>Full Disaster Recovery Test</title>
 <para>
  Full Disaster Recovery Test
 </para>
 <section>
  <title>Prerequisites</title>
  <itemizedlist>
   <listitem>
    <para>
     &productname; platform
    </para>
   </listitem>
   <listitem>
    <para>
     An external server to store backups with SSH
    </para>
   </listitem>
  </itemizedlist>
 </section>
 <section>
   <title>Goals</title>
   <para>A high level view for testing disaster recovery of the platform.</para>
   <procedure>
     <step>
       <para>Back up the control plane using the manual backup procedure and
       <command>scp</command> to a remote host</para>
     </step>
     <step>
       <para>Backup the &cassandra; Database</para>
     </step>
     <step>
       <para>Re-install Controller 1 with the &productname; ISO</para>
     </step>
     <step>
       <para>Use manual restore steps to recover deployment data (and model)</para>
     </step>
     <step>
       <para>Re-install &productname; on Controllers 1, 2, 3</para>
     </step>
     <step>
       <para>Recover the backup of the &mariadb; database</para>
     </step>
     <step>
       <para>Recover the &cassandra; Database</para>
     </step>
   </procedure>
 </section>
 <section>
   <title>Description of the testing environment</title>
   <para>The testing environment is similar to the Entry Scale model.</para>
   <para>It uses five servers: three &contrnode;s and two &compnode;s.</para>
   <para>The controller node has three disks. The first is reserved for the
   system; the others are used for &swift;.</para>
   <note>
    <para>
     During this Disaster Recovery test, we have saved the data on disks 2 and
     3 of the &swift; controllers, which allows for &swift; objects to be
     restored the recovery. If these disks were also wiped, &swift; data would
     be lost, but the procedure would not change. The only difference is that
     &o_img; images would be lost and would have to be uploaded again.
    </para>
   </note>
 </section>
 <section>
   <title>Disaster recovery test note</title>
   <para>If it is not specified otherwise, all the commands should be executed on controller 1, which is also the deployer node.</para>
 </section>
 <section>
   <title>Pre-Disaster testing</title>
   <para>
    In order to validate the procedure after recovery, we need to create some
    workloads.
   </para>
   <procedure>
     <step>
       <para>
         Source the service credential file
       </para>
<screen>&prompt.ardana;source ~/service.osrc</screen>
     </step>
     <step>
       <para>
         Copy an image to the platform and create a Glance image with it.
         In this example, Cirros is used
       </para>
<screen>&prompt.ardana;openstack image create --disk-format raw --container-format \
bare --public --file ~/cirros-0.3.5-x86_64-disk.img cirros</screen>
     </step>
     <step>
       <para>Create a network</para>
       <screen>&prompt.ardana;openstack network create test_net</screen>
     </step>
     <step>
       <para>Create a subnet</para>
<screen>&prompt.ardana;neutron subnet-create 07c35d11-13f9-41d4-8289-fa92147b1d44 192.168.42.0/24 --name test_subnet</screen>
     </step>
     <step>
       <para>Create some instances</para>
       <screen>&prompt.ardana;openstack server create server_1 --image 411a0363-7f4b-4bbc-889c-b9614e2da52e --flavor m1.small --nic net-id=07c35d11-13f9-41d4-8289-fa92147b1d44
&prompt.ardana;openstack server create server_2 --image 411a03...e2da52e --flavor m1.small --nic net-id=07c35d...147b1d44
&prompt.ardana;openstack server create server_3 --image 411a03...e2da52e --flavor m1.small --nic net-id=07c35d...147b1d44
&prompt.ardana;openstack server create server_4 --image 411a03...e2da52e --flavor m1.small --nic net-id=07c35d...147b1d44
&prompt.ardana;openstack server create server_5 --image 411a03...e2da52e --flavor m1.small --nic net-id=07c35d...147b1d44
&prompt.ardana;openstack server list</screen>
     </step>
     <step>
       <para>Create containers and objects</para>
<screen>&prompt.ardana;swift upload container_1 ~/service.osrc
var/lib/ardana/service.osrc

&prompt.ardana;swift upload container_1 ~/backup.osrc
swift upload container_1 ~/backup.osrc

&prompt.ardana;swift list container_1
var/lib/ardana/backup.osrc
var/lib/ardana/service.osrc</screen>
     </step>
   </procedure>
 </section>
 <section>
   <title>Preparation of the test backup server</title>
   <section>
     <title>Preparation to store backups</title>
     <para>
      In this example, backups are stored on the server
      <literal>192.168.69.132</literal>
     </para>
     <procedure>
      <step>
       <para>
        Connect to the backup server
       </para>
      </step>
      <step>
       <para>
        Create the user
       </para>
       <screen>&prompt.root;useradd <replaceable>BACKUPUSER</replaceable> --create-home --home-dir /mnt/backups/</screen>
      </step>
      <step>
       <para>
        Switch to that user
       </para>
       <screen>&prompt.root;su <replaceable>BACKUPUSER</replaceable></screen>
      </step>
      <step>
       <para>
        Create the SSH keypair
       </para>
       <screen><prompt>backupuser &gt; </prompt>ssh-keygen -t rsa
> # Leave the default for the first question and do not set any passphrase
> Generating public/private rsa key pair.
> Enter file in which to save the key (/mnt/backups//.ssh/id_rsa):
> Created directory '/mnt/backups//.ssh'.
> Enter passphrase (empty for no passphrase):
> Enter same passphrase again:
> Your identification has been saved in /mnt/backups//.ssh/id_rsa
> Your public key has been saved in /mnt/backups//.ssh/id_rsa.pub
> The key fingerprint is:
> a9:08:ae:ee:3c:57:62:31:d2:52:77:a7:4e:37:d1:28 backupuser@padawan-ccp-c0-m1-mgmt
> The key's randomart image is:
> +---[RSA 2048]----+
> |          o      |
> |   . . E + .     |
> |  o . . + .      |
> | o +   o +       |
> |  + o o S .      |
> | . + o o         |
> |  o + .          |
> |.o .             |
> |++o              |
> +-----------------+
</screen>
      </step>
      <step>
       <para>
        Add the public key to the list of the keys authorized to connect to
        that user on this server
       </para>
       <screen><prompt>backupuser &gt; </prompt>cat /mnt/backups/.ssh/id_rsa.pub >> /mnt/backups/.ssh/authorized_keys</screen>
      </step>
      <step>
       <para>
        Print the private key. This will be used for the backup configuration
        (ssh_credentials.yml file)
       </para>
       <screen><prompt>backupuser &gt; </prompt>cat /mnt/backups/.ssh/id_rsa

> -----BEGIN RSA PRIVATE KEY-----
> MIIEogIBAAKCAQEAvjwKu6f940IVGHpUj3ffl3eKXACgVr3L5s9UJnb15+zV3K5L
> BZuor8MLvwtskSkgdXNrpPZhNCsWSkryJff5I335Jhr/e5o03Yy+RqIMrJAIa0X5
> ...
> ...
> ...
> iBKVKGPhOnn4ve3dDqy3q7fS5sivTqCrpaYtByJmPrcJNjb2K7VMLNvgLamK/AbL
> qpSTZjicKZCCl+J2+8lrKAaDWqWtIjSUs29kCL78QmaPOgEvfsw=
> -----END RSA PRIVATE KEY-----</screen>
      </step>
     </procedure>
   </section>
   <section>
     <title>Preparation to store &cassandra; backups</title>
     <para>In this example, backups will be stored on the server
     <literal>192.168.69.132</literal>, in the
     <filename>/mnt/backups/cassandra_backups/</filename> directory.</para>
     <procedure>
      <step>
       <para>
        Create a directory on the backup server to store &cassandra; backups.
       </para>
       <screen><prompt>backupuser &gt; </prompt>mkdir /mnt/backups/cassandra_backups</screen>
      </step>
      <step>
       <para>
        Copy the private SSH key from the backup server to all controller
        nodes.
       </para>
       <screen><prompt>backupuser &gt; </prompt>scp /mnt/backups/.ssh/id_rsa ardana@<replaceable>CONTROLLER</replaceable>:~/.ssh/id_rsa_backup
Password:
id_rsa     100% 1675     1.6KB/s   00:00</screen>
       <para>
        Replace <replaceable>CONTROLLER</replaceable> with each control node
        e.g. doc-cp1-c1-m1-mgmt, doc-cp1-c1-m2-mgmt etc
       </para>
      </step>
      <step>
       <para>
        Log in to each controller node and copy the private SSH key to
        <filename>.ssh</filename> directory of the root user.
       </para>
       <screen>&prompt.ardana; sudo cp /var/lib/ardana/.ssh/id_rsa_backup /root/.ssh/</screen>
      </step>
      <step>
       <para>
        Verify that you can SSH to the backup server as backupuser using the
        private key.
       </para>
       <screen>&prompt.root;ssh -i ~/.ssh/id_rsa_backup backupuser@doc-cp1-comp0001-mgmt</screen>
      </step>
     </procedure>
   </section>
 </section>
 <section>
   <title>Perform Backups for disaster recovery test</title>
   <section>
    <title>Execute backup of &cassandra;</title>
    <para>
     Create a <filename>cassandra-backup-extserver.sh</filename> script on all
     controller nodes.
    </para>
    <screen>&prompt.root;cat &gt; ~/cassandra-backup-extserver.sh &lt;&lt; EOF
#!/bin/sh

# backup user
BACKUP_USER=backupuser
# backup server
BACKUP_SERVER=192.168.69.132
# backup directory
BACKUP_DIR=/mnt/backups/cassandra_backups/

# Setup variables
DATA_DIR=/var/cassandra/data/data
NODETOOL=/usr/bin/nodetool

# example: cassandra-snp-2018-06-26-1003
SNAPSHOT_NAME=cassandra-snp-\$(date +%F-%H%M)
HOST_NAME=\$(/bin/hostname)_

# Take a snapshot of &cassandra; database
\$NODETOOL snapshot -t \$SNAPSHOT_NAME monasca

# Collect a list of directories that make up the snapshot
SNAPSHOT_DIR_LIST=\$(find \$DATA_DIR -type d -name \$SNAPSHOT_NAME)
for d in \$SNAPSHOT_DIR_LIST
  do
    # copy snapshot directories to external server
    rsync -avR -e "ssh -i /root/.ssh/id_rsa_backup" \$d \$BACKUP_USER@\$BACKUP_SERVER:\$BACKUP_DIR/\$HOST_NAME\$SNAPSHOT_NAME
  done

\$NODETOOL clearsnapshot monasca
EOF</screen>
<screen>&prompt.root;chmod +x ~/cassandra-backup-extserver.sh</screen>
    <para>
     Execute following steps on all the controller nodes
    </para>
    <note>
     <para>
      The <filename>/usr/local/sbin/cassandra-backup-extserver.sh</filename>
      script should be executed on all three controller nodes at the same
      time (within seconds of each other) for a successful backup.
     </para>
    </note>
    <procedure>
     <step>
      <para>
       Edit the
       <filename>/usr/local/sbin/cassandra-backup-extserver.sh</filename>
       script
      </para>
      <para>
       Set <replaceable>BACKUP_USER</replaceable> and
       <replaceable>BACKUP_SERVER</replaceable> to the desired backup user (for
       example, <systemitem class="username">backupuser</systemitem>) and
       desired backup server (for example, <literal>192.168.68.132</literal>),
       respectively.
      </para>
      <screen>
BACKUP_USER=backupuser
BACKUP_SERVER=192.168.69.132
BACKUP_DIR=/mnt/backups/cassandra_backups/</screen>
     </step>
     <step>
      <para>
       Execute <filename>~/cassandra-backup-extserver.sh</filename> on on all
       controller nodes which are also &cassandra; nodes.
      </para>
<screen>&prompt.root;~/cassandra-backup-extserver.sh

Requested creating snapshot(s) for [monasca] with snapshot name [cassandra-snp-2018-06-28-0251] and options {skipFlush=false}
Snapshot directory: cassandra-snp-2018-06-28-0251
sending incremental file list
created directory /mnt/backups/cassandra_backups//doc-cp1-c1-m1-mgmt_cassandra-snp-2018-06-28-0251
/var/
/var/cassandra/
/var/cassandra/data/
/var/cassandra/data/data/
/var/cassandra/data/data/monasca/

...
...
...

/var/cassandra/data/data/monasca/measurements-e29033d0488d11e8bdabc32666406af1/snapshots/cassandra-snp-2018-06-28-0306/mc-72-big-Summary.db
/var/cassandra/data/data/monasca/measurements-e29033d0488d11e8bdabc32666406af1/snapshots/cassandra-snp-2018-06-28-0306/mc-72-big-TOC.txt
/var/cassandra/data/data/monasca/measurements-e29033d0488d11e8bdabc32666406af1/snapshots/cassandra-snp-2018-06-28-0306/schema.cql
sent 173,691 bytes  received 531 bytes  116,148.00 bytes/sec
total size is 171,378  speedup is 0.98
Requested clearing snapshot(s) for [monasca]</screen>
     </step>
     <step>
      <para>
       Verify the &cassandra; backup directory on the backup server.
      </para>
      <screen><prompt>backupuser &gt; </prompt>ls -alt /mnt/backups/cassandra_backups
total 16
drwxr-xr-x 4 backupuser users 4096 Jun 28 03:06 .
drwxr-xr-x 3 backupuser users 4096 Jun 28 03:06 doc-cp1-c1-m2-mgmt_cassandra-snp-2018-06-28-0306
drwxr-xr-x 3 backupuser users 4096 Jun 28 02:51 doc-cp1-c1-m1-mgmt_cassandra-snp-2018-06-28-0251
drwxr-xr-x 8 backupuser users 4096 Jun 27 20:56 ..

$backupuser@backupserver> du -shx /mnt/backups/cassandra_backups/*
6.2G    /mnt/backups/cassandra_backups/doc-cp1-c1-m1-mgmt_cassandra-snp-2018-06-28-0251
6.3G    /mnt/backups/cassandra_backups/doc-cp1-c1-m2-mgmt_cassandra-snp-2018-06-28-0306</screen>
     </step>
    </procedure>
   </section>
   <section>
    <title>Execute backup of &productname;</title>
    <procedure>
     <step>
      <para>
       Back up the &clm; using the procedure at <xref
       linkend="clm_data_backup"/>
      </para>
     </step>
     <step>
      <para>
       Back up the &mariadb; database using the procedure at <xref
       linkend="mariadb_database_backup"/>
      </para>
     </step>
     <step>
      <para>
       Back up &swift; rings using the procedure at <xref
       linkend="swift_ring_backup"/>
      </para>
     </step>
     <step>
      <para>
       Wait until all the SSH backup jobs have finished running. This can take
       many hours.
      </para>
     </step>
     <step>
      <para>
       Save the backup TAR archives on a remote server with
       <command>scp</command>.
      </para>
     </step>
    </procedure>
    <section>
     <title>Restore the first controller</title>
     <procedure>
      <step>
       <para>
        Execute the restore helper playbook. When prompted, enter the host name
        of the first controller. In the following example, that is
        <literal>doc-cp1-c1-m1-mgmt</literal>
       </para>
<screen>&prompt.ardana;cd ~/openstack/ardana/ansible
&prompt.ardana;ansible-playbook -i hosts/localhost _deployer_restore_helper.yml</screen>
      </step>
      <step>
       <para>
        Execute the restore. When prompted, leave the first value empty (none)
        and validate the restore by typing <literal>yes</literal>.
       </para>
       <screen>&prompt.ardana;sudo su
&prompt.ardana;cd /root/deployer_restore_helper/
&prompt.ardana;./deployer_restore_script.sh</screen>
      </step>
     </procedure>
   </section>
   <section>
     <title>Re-deployment of controllers 1, 2 and 3</title>
     <procedure>
      <step>
       <para>
        Change back to the default ardana user.
       </para>
      </step>
      <step>
       <para>
        Run the <filename>cobbler-deploy.yml</filename> playbook.
       </para>
       <screen>&prompt.ardana;cd ~/openstack/ardana/ansible
&prompt.ardana;ansible-playbook -i hosts/localhost cobbler-deploy.xml</screen>
      </step>
      <step>
       <para>
        Run the <filename>bm-reimage.yml</filename> playbook limited to the
        second and third controllers.
       </para>
       <screen>&prompt.ardana;ansible-playbook -i hosts/localhost bm-reimage.yml -e nodelist=controller2,controller3</screen>
       <para>
       The names of controller2 and controller3. Use the
       <filename>bm-power-status.yml</filename> playbook to check the cobbler
       names of these nodes.
       </para>
      </step>
      <step>
       <para>
        Run the <filename>site.yml</filename> playbook limited to the three
        controllers and localhost&mdash;in this example,
        <literal>doc-cp1-c1-m1-mgmt</literal>,
        <literal>doc-cp1-c1-m2-mgmt</literal>,
        <literal>doc-cp1-c1-m3-mgmt</literal>, and <literal>localhost</literal>
       </para>
       <screen>&prompt.ardana;cd ~/scratch/ansible/next/ardana/ansible
&prompt.ardana;ansible-playbook -i hosts/verb_hosts site.yml --limit \
doc-cp1-c1-m1-mgmt,doc-cp1-c1-m2-mgmt,doc-cp1-c1-m3-mgmt,localhost</screen>
      </step>
     </procedure>
   </section>
   <section>
    <title>Restore Databases</title>
    <section>
     <title>Restore &mariadb; database</title>
     <procedure>
      <step>
       <para> Retrieve the &mariadb; backup that was created with <xref
       linkend="mariadb_database_backup"/>.
       </para>
      </step>
      <step>
       <para>
        Create a temporary directory and extract the TAR archive (for example,
       <filename>mydb.tar.gz</filename>).
       </para>
       <screen>&prompt.ardana;mkdir /tmp/mysql_restore; sudo tar -z --incremental \
--extract --ignore-zeros --warning=none --overwrite --directory /tmp/mysql_restore/ \
-f mydb.tar.gz</screen>
      </step>
      <step>
       <para>
        Verify that the files have been restored on the controller.
       </para>
       <screen>&prompt.ardana;sudo du -shx /tmp/mysql_restore/*
16K     /tmp/mysql_restore/aria_log.00000001
4.0K    /tmp/mysql_restore/aria_log_control
3.4M    /tmp/mysql_restore/barbican
8.0K    /tmp/mysql_restore/ceilometer
4.2M    /tmp/mysql_restore/cinder
2.9M    /tmp/mysql_restore/designate
129M    /tmp/mysql_restore/galera.cache
2.1M    /tmp/mysql_restore/glance
4.0K    /tmp/mysql_restore/grastate.dat
4.0K    /tmp/mysql_restore/gvwstate.dat
2.6M    /tmp/mysql_restore/heat
752K    /tmp/mysql_restore/horizon
4.0K    /tmp/mysql_restore/ib_buffer_pool
76M     /tmp/mysql_restore/ibdata1
128M    /tmp/mysql_restore/ib_logfile0
128M    /tmp/mysql_restore/ib_logfile1
12M     /tmp/mysql_restore/ibtmp1
16K     /tmp/mysql_restore/innobackup.backup.log
313M    /tmp/mysql_restore/keystone
716K    /tmp/mysql_restore/magnum
12M     /tmp/mysql_restore/mon
8.3M    /tmp/mysql_restore/monasca_transform
0       /tmp/mysql_restore/multi-master.info
11M     /tmp/mysql_restore/mysql
4.0K    /tmp/mysql_restore/mysql_upgrade_info
14M     /tmp/mysql_restore/nova
4.4M    /tmp/mysql_restore/nova_api
14M     /tmp/mysql_restore/nova_cell0
3.6M    /tmp/mysql_restore/octavia
208K    /tmp/mysql_restore/opsconsole
38M     /tmp/mysql_restore/ovs_neutron
8.0K    /tmp/mysql_restore/performance_schema
24K     /tmp/mysql_restore/tc.log
4.0K    /tmp/mysql_restore/test
8.0K    /tmp/mysql_restore/winchester
4.0K    /tmp/mysql_restore/xtrabackup_galera_info</screen>
      </step>
      <step>
       <para> Stop &productname; services on the three controllers as in the
       following example (using the hostnames of the controllers in your
       configuration)
       </para>
       <screen>&prompt.ardana;cd ~/scratch/ansible/next/ardana/ansible
       &prompt.ardana;ansible-playbook -i hosts/verb_hosts ardana-stop.yml --limit \
doc-cp1-c1-m1-mgmt,doc-cp1-c1-m2-mgmt,doc-cp1-c1-m3-mgmt,localhost</screen>
      </step>
      <step>
       <para>
        Delete the files in the <filename>mysql</filename> directory and copy
        the restored backup to that directory.
       </para>
       <screen>&prompt.root;cd /var/lib/mysql/
&prompt.root;rm -rf ./*
&prompt.root;cp -pr /tmp/mysql_restore/* ./</screen>
      </step>
      <step>
       <para>
        Switch back to the ardana user when the copy is finished.
       </para>
      </step>
     </procedure>
    </section>
    <section>
     <title>&cassandra; database restore</title>
     <para>
      Create a script called
      <filename>cassandra-restore-extserver.sh</filename> on all controller
      nodes
     </para>
     <screen>&prompt.root;cat &gt; ~/cassandra-restore-extserver.sh &lt;&lt; EOF
#!/bin/sh

# backup user
BACKUP_USER=backupuser
# backup server
BACKUP_SERVER=192.168.69.132
# backup directory
BACKUP_DIR=/mnt/backups/cassandra_backups/

# Setup variables
DATA_DIR=/var/cassandra/data/data
NODETOOL=/usr/bin/nodetool

HOST_NAME=\$(/bin/hostname)_

#Get snapshot name from command line.
if [ -z "\$*"  ]
then
  echo "usage \$0 &lt;snapshot to restore&gt;"
  exit 1
fi
SNAPSHOT_NAME=\$1

# restore
rsync -av -e "ssh -i /root/.ssh/id_rsa_backup" \$BACKUP_USER@\$BACKUP_SERVER:\$BACKUP_DIR/\$HOST_NAME\$SNAPSHOT_NAME/ /

# set ownership of newley restored files
chown -R cassandra:cassandra \$DATA_DIR/monasca/*

# Get a list of snapshot directories that have files to be restored.
RESTORE_LIST=\$(find \$DATA_DIR -type d -name \$SNAPSHOT_NAME)

# use RESTORE_LIST to move snapshot files back into place of database.
for d in \$RESTORE_LIST
do
  cd \$d
  mv * ../..
  KEYSPACE=\$(pwd | rev | cut -d '/' -f4 | rev)
  TABLE_NAME=\$(pwd | rev | cut -d '/' -f3 |rev | cut -d '-' -f1)
  \$NODETOOL refresh \$KEYSPACE \$TABLE_NAME
done
cd
# Cleanup snapshot directories
\$NODETOOL clearsnapshot \$KEYSPACE
EOF</screen>
     <screen>&prompt.root;chmod +x ~/cassandra-restore-extserver.sh</screen>
     <para>
      Execute following steps on all the controller nodes.
     </para>
     <procedure>
      <step>
       <para>
        Edit the <filename>~/cassandra-restore-extserver.sh</filename> script.
       </para>
       <para>
        Set
        <replaceable>BACKUP_USER</replaceable>,<replaceable>BACKUP_SERVER</replaceable>
        to the desired backup user (for example, <systemitem
        class="username">backupuser</systemitem>) and the desired backup server
        (for example, <literal>192.168.68.132</literal>), respectively.
       </para>
       <screen>BACKUP_USER=backupuser
BACKUP_SERVER=192.168.69.132
BACKUP_DIR=/mnt/backups/cassandra_backups/</screen>
      </step>
      <step>
       <para>
        Execute <filename>~/cassandra-restore-extserver.sh</filename>
        <replaceable>SNAPSHOT_NAME</replaceable>
       </para>
       <para>
        Find <replaceable>SNAPSHOT_NAME</replaceable> from listing of
       /mnt/backups/cassandra_backups. All the directories have the format
       <replaceable>HOST</replaceable>_<replaceable>SNAPSHOT_NAME</replaceable>.
       </para>
       <screen>&prompt.ardana;ls -alt /mnt/backups/cassandra_backups
total 16
drwxr-xr-x 4 backupuser users 4096 Jun 28 03:06 .
drwxr-xr-x 3 backupuser users 4096 Jun 28 03:06 doc-cp1-c1-m2-mgmt_cassandra-snp-2018-06-28-0306</screen>
<screen>&prompt.root;~/cassandra-restore-extserver.sh cassandra-snp-2018-06-28-0306

receiving incremental file list
./
var/
var/cassandra/
var/cassandra/data/
var/cassandra/data/data/
var/cassandra/data/data/monasca/
var/cassandra/data/data/monasca/alarm_state_history-e6bbdc20488d11e8bdabc32666406af1/
var/cassandra/data/data/monasca/alarm_state_history-e6bbdc20488d11e8bdabc32666406af1/snapshots/
var/cassandra/data/data/monasca/alarm_state_history-e6bbdc20488d11e8bdabc32666406af1/snapshots/cassandra-snp-2018-06-28-0306/
var/cassandra/data/data/monasca/alarm_state_history-e6bbdc20488d11e8bdabc32666406af1/snapshots/cassandra-snp-2018-06-28-0306/manifest.json
var/cassandra/data/data/monasca/alarm_state_history-e6bbdc20488d11e8bdabc32666406af1/snapshots/cassandra-snp-2018-06-28-0306/mc-37-big-CompressionInfo.db
var/cassandra/data/data/monasca/alarm_state_history-e6bbdc20488d11e8bdabc32666406af1/snapshots/cassandra-snp-2018-06-28-0306/mc-37-big-Data.db
...
...
...
/usr/bin/nodetool clearsnapshot monasca</screen>
      </step>
     </procedure>
    </section>
    <section>
     <title>Restart &productname; services</title>
     <procedure>
      <step>
       <para>
        Restart the &mariadb; database
       </para>
       <screen>&prompt.ardana;cd ~/scratch/ansible/next/ardana/ansible
&prompt.ardana;ansible-playbook -i hosts/verb_hosts galera-bootstrap.yml</screen>
       <para>
        On the deployer node, execute the
        <filename>galera-bootstrap.yml</filename> playbook which will determine
        the log sequence number, bootstrap the main node, and start the
        database cluster.
       </para>
       <para>
        If this process fails to recover the database cluster, refer to <xref
        linkend="mysql"/>.
       </para>
      </step>
      <step>
       <para>
        Restart &productname; services limited to the three controllers as in
        the following example.
       </para>
       <screen>&prompt.ardana;ansible-playbook -i hosts/verb_hosts ardana-start.yml \
--limit doc-cp1-c1-m1-mgmt,doc-cp1-c1-m2-mgmt,doc-cp1-c1-m3-mgmt,localhost</screen>
      </step>
      <step>
       <para>
        Reconfigure &productname;
       </para>
       <screen>&prompt.ardana;ansible-playbook -i hosts/verb_hosts ardana-reconfigure.yml</screen>
      </step>
     </procedure>
    </section>
   </section>
   <section>
    <title>Post restore testing</title>
    <procedure>
     <step>
      <para>
       Source the service credential file
      </para>
      <screen>&prompt.ardana;source ~/service.osrc</screen>
     </step>
     <step>
      <para>
       &swift;
      </para>
      <screen>&prompt.ardana;swift list
container_1
volumebackups

&prompt.ardana;swift list container_1
var/lib/ardana/backup.osrc
var/lib/ardana/service.osrc

&prompt.ardana;swift download container_1 /tmp/backup.osrc</screen>
     </step>
     <step>
      <para>
       &o_netw;
      </para>
      <screen>&prompt.ardana;openstack network list
+--------------------------------------+---------------------+--------------------------------------+
| ID                                   | Name                | Subnets                              |
+--------------------------------------+---------------------+--------------------------------------+
| 07c35d11-13f9-41d4-8289-fa92147b1d44 | test-net             | 02d5ca3b-1133-4a74-a9ab-1f1dc2853ec8|
+--------------------------------------+---------------------+--------------------------------------+</screen>
     </step>
     <step>
      <para>
       &o_img;
      </para>
      <screen>&prompt.ardana;openstack image list
+--------------------------------------+----------------------+--------+
| ID                                   | Name                 | Status |
+--------------------------------------+----------------------+--------+
| 411a0363-7f4b-4bbc-889c-b9614e2da52e | cirros-0.4.0-x86_64  | active |
+--------------------------------------+----------------------+--------+
&prompt.ardana;openstack image save --file /tmp/cirros f751c39b-f1e3-4f02-8332-3886826889ba
&prompt.ardana;ls -lah /tmp/cirros
-rw-r--r-- 1 ardana ardana 12716032 Jul  2 20:52 /tmp/cirros</screen>
     </step>
     <step>
      <para>
       &o_comp;
      </para>
      <screen>&prompt.ardana;openstack server list

&prompt.ardana;openstack server create server_6 --image 411a0363-7f4b-4bbc-889c-b9614e2da52e  --flavor m1.small --nic net-id=07c35d11-13f9-41d4-8289-fa92147b1d44
+-------------------------------------+------------------------------------------------------------+
| Field                               | Value                                                      |
+-------------------------------------+------------------------------------------------------------+
| OS-DCF:diskConfig                   | MANUAL                                                     |
| OS-EXT-AZ:availability_zone         |                                                            |
| OS-EXT-SRV-ATTR:host                | None                                                       |
| OS-EXT-SRV-ATTR:hypervisor_hostname | None                                                       |
| OS-EXT-SRV-ATTR:instance_name       |                                                            |
| OS-EXT-STS:power_state              | NOSTATE                                                    |
| OS-EXT-STS:task_state               | scheduling                                                 |
| OS-EXT-STS:vm_state                 | building                                                   |
| OS-SRV-USG:launched_at              | None                                                       |
| OS-SRV-USG:terminated_at            | None                                                       |
| accessIPv4                          |                                                            |
| accessIPv6                          |                                                            |
| addresses                           |                                                            |
| adminPass                           | iJBoBaj53oUd                                               |
| config_drive                        |                                                            |
| created                             | 2018-07-02T21:02:01Z                                       |
| flavor                              | m1.small (2)                                               |
| hostId                              |                                                            |
| id                                  | ce7689ff-23bf-4fe9-b2a9-922d4aa9412c                       |
| image                               | cirros-0.4.0-x86_64 (f751c39b-f1e3-4f02-8332-3886826889ba) |
| key_name                            | None                                                       |
| name                                | server_6                                                   |
| progress                            | 0                                                          |
| project_id                          | cca416004124432592b2949a5c5d9949                           |
| properties                          |                                                            |
| security_groups                     | name='default'                                             |
| status                              | BUILD                                                      |
| updated                             | 2018-07-02T21:02:01Z                                       |
| user_id                             | 8cb1168776d24390b44c3aaa0720b532                           |
| volumes_attached                    |                                                            |
+-------------------------------------+------------------------------------------------------------+

&prompt.ardana;openstack server list
+--------------------------------------+----------+--------+---------------------------------+---------------------+-----------+
| ID                                   | Name     | Status | Networks                        | Image               | Flavor    |
+--------------------------------------+----------+--------+---------------------------------+---------------------+-----------+
| ce7689ff-23bf-4fe9-b2a9-922d4aa9412c | server_6 | ACTIVE | n1=1.1.1.8                      | cirros-0.4.0-x86_64 | m1.small  |

&prompt.ardana;openstack server delete ce7689ff-23bf-4fe9-b2a9-922d4aa9412c</screen>
     </step>
    </procedure>
   </section>
  </section>
 </section>
</section>
