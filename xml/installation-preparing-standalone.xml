<?xml version="1.0"?>
<!DOCTYPE chapter [
 <!ENTITY % entities SYSTEM "entities.ent"> %entities;
]>
<chapter xml:id="preparing-standalone"
 xmlns="http://docbook.org/ns/docbook" version="5.1"
 xmlns:xi="http://www.w3.org/2001/XInclude"
 xmlns:xlink="http://www.w3.org/1999/xlink">
 <title>Preparing for Stand-Alone Deployment</title>
 <section>
 <title>&clm; Installation Alternatives</title>
 <para>
  The &clm; can be installed on a Control Plane or on a stand-alone
  server.
 </para>
 <itemizedlist>
  <listitem>
   <para>
    Installing the &clm; on a Control Plane is done during the process of
    deploying your Cloud. Your Cloud and the &clm; are deployed together.
   </para>
  </listitem>
  <listitem>
   <para>
    With a standalone &clm;, you install the deployer first and then deploy
    your Cloud in a separate process. Either the &install_ui; or command line
    can be used to deploy a stand-alone &clm;.
   </para>
  </listitem>
 </itemizedlist>
 <para>
  Each method is suited for particular needs. The best choice depends on your
  situation.
 </para>
 <para>
  <emphasis role="bold">Stand-alone Deployer</emphasis>
 </para>
 <itemizedlist>
  <listitem>
   <para>
    + Compared to a Control Plane deployer, a stand-alone deployer is easier to
    backup and redeploy in case of disaster
   </para>
  </listitem>
  <listitem>
   <para>
    + Separates cloud management from components being managed
   </para>
  </listitem>
  <listitem>
   <para>
    + Does not use Control Plane resources
   </para>
  </listitem>
  <listitem>
   <para>
    - Another server is required (less of a disadvantage if using a VM)
   </para>
  </listitem>
  <listitem>
   <para>
    - Installation may be more complex than a Control Plane &clm;
   </para>
  </listitem>
 </itemizedlist>
 <para>
  <emphasis role="bold">Control Plane Deployer</emphasis>
 </para>
 <itemizedlist>
  <listitem>
   <para>
    + Installation is usually simpler than installing a stand-alone deployer
   </para>
  </listitem>
  <listitem>
   <para>
    + Requires fewer servers or VMs
   </para>
  </listitem>
  <listitem>
   <para>
    - Could contend with workloads for resources
   </para>
  </listitem>
  <listitem>
   <para>
    - Harder to redeploy in case of failure compared to stand-alone deployer
   </para>
  </listitem>
  <listitem>
   <para>
    - There is a risk to the &clm; when updating or modifying controllers
   </para>
  </listitem>
  <listitem>
   <para>
    - Runs on one of the servers that is deploying or managing your Cloud
   </para>
  </listitem>
 </itemizedlist>
 <para>
  <emphasis role="bold">Summary</emphasis>
 </para>
 <itemizedlist>
  <listitem>
   <para>
    A Control Plane &clm; is best for small, simple Cloud deployments.
   </para>
  </listitem>
  <listitem>
   <para>
    With a larger, more complex cloud, a stand-alone deployer provides better
    recoverability and the separation of manager from managed components.
   </para>
  </listitem>
 </itemizedlist>
 </section>
 <section>
  <title>Installing a Stand-Alone Deployer</title>
  <para>
   If you do not intend to install a stand-alone deployer, proceed to
   installing the &clm; on a Control Plane.
  </para>
  <itemizedlist>
   <listitem>
    <para>
     Instructions for GUI installation are in <xref linkend="install-gui"/>.
    </para>
   </listitem>
   <listitem>
    <para>
     Instructions for installing via the command line are in <xref
     linkend="install-kvm"/>.
    </para>
   </listitem>
  </itemizedlist>
  <xi:include xpointer="element(/1/3)" href="installation-kvm_xpointer.xml"/>
  <section>
   <title>Configuring for a Stand-Alone Deployer</title>
  <para>
   The following steps are necessary to set up a stand-alone deployer whether
   you will be using the &install_ui; or command line.
  </para>
  <procedure>
   <step>
    <para>
     Copy the &productname; Entry-scale KVM example input model to a
     stand-alone input model. This new input model will be edited so that it
     can be used as a stand-alone &clm; installation.
    </para>
<screen>&prompt.user;cp -r ~/openstack/examples/entry-scale-kvm/* \
~/openstack/examples/entry-scale-kvm-stand-alone-deployer</screen>
   </step>
   <step>
    <para>
     Change to the new directory
    </para>
<screen>&prompt.user;cd ~/openstack/examples/entry-scale-kvm-stand-alone-deployer</screen>
   </step>
   <step>
    <para>
     Edit the cloudConfig.yml file to change the name of the input model. This
     will make the model available both to the &install_ui; and to the command
     line installation process.
    </para>
    <para>
     Change <literal>name: entry-scale-kvm</literal> to <literal>name:
     entry-scale-kvm-stand-alone-deployer</literal>
    </para>
   </step>
   <step>
    <para>
     Change to the <filename>data</filename> directory.
    </para>
   </step>
   <step>
    <para>
     Make the following edits to your configuration files.
    </para>
    <important>
     <para>
      The indentation of each of the input files is important and will cause
      errors if not done correctly. Use the existing content in each of these
      files as a reference when adding additional content for your &clm;.
     </para>
    </important>
    <itemizedlist>
     <listitem>
      <para>
       Update <filename>control_plane.yml</filename> to add the &clm;.
      </para>
     </listitem>
     <listitem>
      <para>
       Update <filename>server_roles.yml</filename> to add the &clm; role.
      </para>
     </listitem>
     <listitem>
      <para>
       Update <filename>net_interfaces.yml</filename> to add the interface
       definition for the &clm;.
      </para>
     </listitem>
     <listitem>
      <para>
       Create a <filename>disks_lifecycle_manager.yml</filename> file to define
       the disk layout for the &clm;.
      </para>
     </listitem>
     <listitem>
      <para>
       Update <filename>servers.yml</filename> to add the dedicated &clm; node.
      </para>
     </listitem>
    </itemizedlist>
    <para>
     <filename>Control_plane.yml</filename>: The snippet below shows the
     addition of a single node cluster into the control plane to host the &clm;
     service.
    </para>
    <important>
     <para>
      In addition to adding the new cluster, you also have to remove the &clm;
      component from the <literal>cluster1</literal> in the examples.
     </para>
    </important>
    <screen>  clusters:
<emphasis role="bold">     - name: cluster0
       cluster-prefix: c0
       server-role: LIFECYCLE-MANAGER-ROLE
       member-count: 1
       allocation-policy: strict
       service-components:
         - lifecycle-manager</emphasis>
         - ntp-client
     - name: cluster1
       cluster-prefix: c1
       server-role: CONTROLLER-ROLE
       member-count: 3
       allocation-policy: strict
       service-components:
         - ntp-server</screen>
    <para>
     This specifies a single node of role
     <literal>LIFECYCLE-MANAGER-ROLE</literal> hosting the &clm;.
    </para>
    <para>
     <filename>Server_roles.yml</filename>: The snippet below shows the
     insertion of the new server roles definition:
    </para>
<screen>   server-roles:

<emphasis role="bold">      - name: LIFECYCLE-MANAGER-ROLE
        interface-model: LIFECYCLE-MANAGER-INTERFACES
        disk-model: LIFECYCLE-MANAGER-DISKS</emphasis>

      - name: CONTROLLER-ROLE</screen>
    <para>
     This defines a new server role which references a new interface-model and
     disk-model to be used when configuring the server.
    </para>
    <para>
     <filename>net-interfaces.yml</filename>: The snippet below shows the
     insertion of the network-interface info:
    </para>
<screen><emphasis role="bold">    - name: LIFECYCLE-MANAGER-INTERFACES
      network-interfaces:
        - name: BOND0
          device:
             name: bond0
          bond-data:
             options:
                 mode: active-backup
                 miimon: 200
                 primary: hed3
             provider: linux
             devices:
                 - name: hed3
                 - name: hed4
          network-groups:
             - MANAGEMENT</emphasis></screen>
    <para>
     This assumes that the server uses the same physical networking layout as
     the other servers in the example.
    </para>
    <para>
     <filename>disks_lifecycle_manager.yml</filename>: In the examples,
     disk-models are provided as separate files (this is just a convention, not
     a limitation) so the following should be added as a new file named
     <filename>disks_lifecycle_manager.yml</filename>:
    </para>
<screen>---
   product:
      version: 2

   disk-models:
<emphasis role="bold">   - name: LIFECYCLE-MANAGER-DISKS
     # Disk model to be used for &clm;s nodes
     # /dev/sda_root is used as a volume group for /, /var/log and /var/crash
     # sda_root is a templated value to align with whatever partition is really used
     # This value is checked in os config and replaced by the partition actually used
     # on sda e.g. sda1 or sda5

     volume-groups:
       - name: ardana-vg
         physical-volumes:
           - /dev/sda_root

       logical-volumes:
       # The policy is not to consume 100% of the space of each volume group.
       # 5% should be left free for snapshots and to allow for some flexibility.
          - name: root
            size: 80%
            fstype: ext4
            mount: /
          - name: crash
            size: 15%
            mount: /var/crash
            fstype: ext4
            mkfs-opts: -O large_file
        consumer:
              name: os</emphasis></screen>
    <para>
     <filename>Servers.yml</filename>: The snippet below shows the insertion of
     an additional server used for hosting the &clm;. Provide the address
     information here for the server you are running on, that is, the node
     where you have installed the &kw-hos; ISO.
    </para>
<screen>  servers:
     # NOTE: Addresses of servers need to be changed to match your environment.
     #
     #       Add additional servers as required

<emphasis role="bold">     #Lifecycle-manager
     - id: lifecycle-manager
       ip-addr: <replaceable>YOUR IP ADDRESS HERE</replaceable>
       role: LIFECYCLE-MANAGER-ROLE
       server-group: RACK1
       nic-mapping: HP-SL230-4PORT
       mac-addr: 8c:dc:d4:b5:c9:e0
       # ipmi information is not needed </emphasis>

     # Controllers
     - id: controller1
       ip-addr: 192.168.10.3
       role: CONTROLLER-ROLE</screen>
   </step>
  </procedure>
  <para>
   With the stand-alone input model complete, you are ready to proceed to
   installing the stand-alone deployer with either the &install_ui; or the
   command line.
  </para>
 </section>
 </section>
</chapter>
