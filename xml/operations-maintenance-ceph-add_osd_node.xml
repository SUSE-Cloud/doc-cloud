<?xml version="1.0"?>
<?xml-stylesheet href="urn:x-suse:xslt:profiling:docbook51-profile.xsl"
 type="text/xml"
 title="Profiling step"?>
<!DOCTYPE section [
 <!ENTITY % entities SYSTEM "entities.ent"> %entities;
]>
<section xmlns="http://docbook.org/ns/docbook" xmlns:xi="http://www.w3.org/2001/XInclude" xmlns:xlink="http://www.w3.org/1999/xlink" xml:id="add_osdnode">
 <title>Adding an Object Storage Daemon (OSD) Node</title>
 <para>
  Adding additional OSD nodes allows you to increase storage capacity.
 </para>
 <para>
  You may find that you have the need to increase your Ceph storage capacity,
  or would simply like to increase the number of Ceph OSD nodes in a cluster.
  In either case, adding a Ceph object storage node (OSD node) is a simple and
  straightforward process.
 </para> 
 <section xml:id="idg-all-operations-maintenance-ceph-add_osd_node-xml-6">
  <title>Prerequisites</title>
  <para>
   To begin, make sure you have one or more baremetal servers that meet the
   minimum hardware requirements for an object storage daemon (OSD) node, as
   documented in the <xref linkend="min_hardware"/>.
  </para>
  <para>
   Ensure that disks designated to be used for the OSD data and journal storage
   meet the conditions below. If they do not, Ceph configuration will fail:
  </para>
  <itemizedlist>
   <listitem>
    <para>
     Any existing partitions must be deleted
    </para>
   </listitem>
   <listitem>
    <para>
     The disk should not be mounted
    </para>
   </listitem>
   <listitem>
    <para>
     The disk should not be in use or held by some other device
    </para>
   </listitem>
  </itemizedlist>
 </section>
 <section xml:id="idg-all-operations-maintenance-ceph-add_osd_node-xml-8">
  <title>Adding an Object Storage Daemon (OSD) Node</title>
  <para>
   The following steps will show you how to add a new OSD node to your
   <literal>servers.yml</literal> file and then run the playbooks that update
   your cloud configuration. You will run these playbooks from the lifecycle
   manager.
  </para>
  <orderedlist>
   <listitem>
    <para>
     Log in to your &lcm;.
    </para>
   </listitem>
   <listitem>
    <para>
     Checkout the <literal>site</literal> branch of your local git so you can
     begin to make the necessary edits:
    </para>
<screen>cd ~/openstack/my_cloud/definition/data
git checkout site</screen>
   </listitem>
   <listitem>
    <para>
     In the same directory, edit your <literal>servers.yml</literal> file to
     add the details about your new OSD node(s). You can simply copy and paste
     the section for an existing server with the <literal>OSD-ROLE</literal>
     role and edit the values to match the new server being added.
    </para>
    <para>
     For example, if you already had a cluster of three OSD nodes and needed to
     add a fourth one, you would add the details for your new OSD node to the
     bottom of the file in this format:
    </para>
<screen># Ceph OSD Nodes
- id: osd4
  ip-addr: 192.168.10.9
  role: OSD-ROLE
  server-group: RACK1
  nic-mapping: MY-2PORT-SERVER
  mac-addr: 8b:f6:9e:ca:3b:78
  ilo-ip: 192.168.9.9
  ilo-password: password
  ilo-user: admin</screen>
    <para>
     You can find detailed descriptions of these fields in
     <xref linkend="configobj_servers"/>.
    </para>
    <important>
     <para>
      Verify that the <literal>ip-addr</literal> value you
      choose for this new node does not conflict with any other IP address in
      your cloud environment. You can confirm this by checking the
      <literal>~/openstack/my_cloud/info/address_info.yml</literal> file on your
      &lcm;.
     </para>
    </important>
   </listitem>
   <listitem>
    <para>
     In your <literal>control_plane.yml</literal> file, in the section that
     details the OSD server-role, check the values for
     <literal>member-count</literal>, <literal>min-count</literal>, and
     <literal>max-count</literal>, if you specified them, to ensure that they
     match up with your new total node count. For example, if you had
     previously specified <literal>member-count: 3</literal> and are adding a
     fourth OSD node, need to change that value to
     <literal>member-count: 4</literal>. Note that these values are optional
     and may or may not be specified in your
     <literal>control_plane.yml</literal> file. If the values are not
     specified, proceed to Step 5. <!-- FIXME: <xref/> for step 5. -->
    </para>
    <para>
     For more information, see <xref linkend="configobj_controlplane"/>.
    </para>
   </listitem>
   <listitem>
    <para>
     Commit the changes to git:
    </para>
<screen>git add -A
git commit -a -m "Add node &lt;name&gt;"</screen>
   </listitem>
   <listitem>
    <para>
     Run the configuration processor:
    </para>
<screen>cd ~/openstack/ardana/ansible
ansible-playbook -i hosts/localhost config-processor-run.yml</screen>
   </listitem>
   <listitem>
    <para>
     Update your deployment directory:
    </para>
<screen>cd ~/openstack/ardana/ansible
ansible-playbook -i hosts/localhost ready-deployment.yml</screen>
   </listitem>
   <listitem>
    <para>
     Add the new node into Cobbler:
    </para>
<screen>cd ~/openstack/ardana/ansible
ansible-playbook -i hosts/localhost cobbler-deploy.yml</screen>
   </listitem>
   <listitem>
    <para>
     Then you can image the node:
    </para>
<screen>cd ~/openstack/ardana/ansible
ansible-playbook -i hosts/localhost bm-reimage.yml -e nodelist=&lt;node name&gt;</screen>
    <para>
     Before proceeding, you may want to take a look at
     <emphasis role="bold">~/openstack/my_cloud/info/server_info.yml</emphasis> to
     see if the assignment of the node you have added is what you expect. It
     may not be, as nodes will not be numbered consecutively if any have
     previously been removed. This behavior is by design, and is intended to
     prevent loss of data; the configuration processor retains data about
     removed nodes and keeps their ID numbers from being reallocated.
     For more information, see <xref linkend="persistedserverallocations"/>.
    </para>
   </listitem>
   <listitem>
    <para>
     Run the following series of playbooks to complete the deployment of your
     additional OSD node(s) to your Ceph cluster:
    </para>
<screen>cd ~/scratch/ansible/next/hos/ansible
ansible-playbook -i hosts/verb_hosts site.yml --tags "generate_hosts_file"
ansible-playbook -i hosts/verb_hosts site.yml --limit &lt;hostname&gt;</screen>
<!-- Comment from DITA original: -->
<!--The last above command is erroring on "Run Monasca Agent detection
plugin..." "failed detection of plugin MonAgent"-->
    <para>
     The hostname of the newly added node can be found in the list generated
     from the output of the following command:
    </para>
<screen>grep hostname ~/openstack/my_cloud/info/server_info.yml</screen>
   </listitem>
  </orderedlist>
 </section>
 <section xml:id="idg-all-operations-maintenance-ceph-add_osd_node-xml-9">
  <title>Verifying the New OSD Node</title>
  <para>
   From the &lcm;, run the following command:
  </para>
<screen>ceph --cluster ceph osd tree</screen>
  <para>
   If the Ceph cluster name is different than the default (i.e.
   <emphasis role="bold">ceph</emphasis>), you need to modify the above command
   accordingly.
  </para>
  <para>
   When you run the above command, if the OSD node has been successfully added
   to the existing Ceph cluster, the hostname of the newly added OSD should
   appear in the output.
  </para>
 </section>
 <section xml:id="idg-all-operations-maintenance-ceph-add_osd_node-xml-10">
  <title>Adding a New OSD Node to Monitoring</title>
  <para>
   If you want to add your new OSD node to the monitoring service checks, run
   the following additional playbook:
  </para>
<screen>cd ~/scratch/ansible/next/hos/ansible
ansible-playbook -i hosts/verb_hosts monasca-deploy.yml --tags active_ping_checks</screen>
 </section>
</section>
