<?xml version="1.0"?>
<!DOCTYPE section [
<!ENTITY % entities SYSTEM "entities.ent"> %entities;
]>
<section xmlns="http://docbook.org/ns/docbook" xmlns:xi="http://www.w3.org/2001/XInclude"
         xmlns:xlink="http://www.w3.org/1999/xlink" xml:id="storage_alarmdefinitions">
 <title>存储（Storage）警报</title>
 <para>
  以下警报在 &productname; &opscon; 的存储（Storage）分类中出现。
 </para>
 <informaltable colsep="1" rowsep="1">
  <tgroup cols="5">
   <colspec colname="c1" colnum="1"/>
   <colspec colname="c2" colnum="2"/>
   <colspec colname="c3" colnum="3"/>
   <colspec colname="c4" colnum="4"/>
   <colspec colname="c5" colnum="5"/>
   <thead>
    <row>
     <entry>服务</entry>
     <entry>警报名称</entry>
     <entry>描述</entry>
     <entry>可能的原因</entry>
     <entry>故障排除步骤</entry>
    </row>
   </thead>
   <tbody>
    <row>
     <entry morerows="25">object-storage</entry>
     <entry>swiftlm-scan monitor</entry>
     <entry><literal>swiftlm-scan</literal> 不能执行监视任务时发出的警报。</entry>
     <entry><literal>swiftlm-scan</literal> 程序被用于监视和测量一些指标。如果它不能监视或测量某些指标，它会发出此警报</entry>
     <entry>
      <para>
       点击警报来查看 <literal>Details</literal> 项中的 <literal>msg</literal> 项，这里的信息可能会解释错误问题。要想查看/检查问题，您也可以登录 <literal>hostname</literal> 项指定的主机并运行以下命令： 
      </para>
      <screen>sudo swiftlm-scan | python -mjson.tool</screen>
      <para>
       <literal>msg</literal> 项在 
       <literal>value_meta</literal> 中。
      </para>
     </entry>
    </row>
    <row>
     <entry>Swift account replicator last completed in 12 hours</entry>
     <entry>某个 <literal>account-replicator</literal> 进程没在 12 小时以内完成复制循环时发出的警报</entry>
     <entry>这可能说明 <literal>account-replication</literal> 进程已经卡住。</entry>
     <entry>
      <para>
       SSH 连接受影响的主机并用以下指令重启进程：
      </para>
      <screen>sudo systemctl restart swift-account-replicator</screen>
      <para>
       造成这个问题的另一个可能原因是有可能有损坏的文件系统。请在受影响节点上的以下日志中寻找支持这个可能的信号。
      </para>
      <screen>/var/log/swift/swift.log
       /var/log/kern.log</screen>
      <para>
       文件系统可能需要被擦除，如果您需要相关的建议，请联系 &serviceteam;。在这之后，您可以用以下的步骤重新格式化文件系统：
      </para>
      <orderedlist xml:id="ol_jbr_2zp_mx">
       <listitem>
        <para>
         登录 &lcm;。
        </para>
       </listitem>
       <listitem>
        <para>
         对受影响的节点使用 Swift 部署脚本，这会格式化已被擦除的文件系统：
        </para>
<screen>cd ~/scratch/ansible/next/ardana/ansible/
         ansible-playbook -i hosts/verb_hosts swift-deploy.yml --limit &lt;hostname&gt;</screen>
       </listitem>
      </orderedlist>
     </entry>
    </row>
    <row>
     <entry>Swift container replicator last completed in 12 hours</entry>
     <entry>某个 <literal>container-replicator</literal> 进程没在 12 小时以内完成复制循环时发出的警报</entry>
     <entry>这可能说明 <literal>container-replication</literal> 进程已经卡住。</entry>
     <entry>
      <para>
       SSH 连接受影响的主机并用以下指令重启进程：
      </para>
      <screen>sudo systemctl restart swift-container-replicator</screen>
      <para>
       造成这个问题的另一个可能原因是有可能有损坏的文件系统。请在受影响节点上的以下日志中寻找支持这个可能的信号。
      </para>
      <screen>/var/log/swift/swift.log
       /var/log/kern.log</screen>
      <para>
       文件系统可能需要被擦除，如果您需要相关的建议，请联系 &serviceteam;。在这之后，您可以用以下的步骤重新格式化文件系统：
      </para>
      <orderedlist xml:id="ol_kbr_2zp_mx">
       <listitem>
        <para>
         登录 &lcm;。
        </para>
       </listitem>
       <listitem>
        <para>
         对受影响的节点使用 Swift 部署脚本，这会格式化已被擦除的文件系统：
        </para>
<screen>cd ~/scratch/ansible/next/ardana/ansible/
         ansible-playbook -i hosts/verb_hosts swift-deploy.yml --limit &lt;hostname&gt;</screen>
       </listitem>
      </orderedlist>
     </entry>
    </row>
    <row>
     <entry>Swift object replicator last completed in 24 hours</entry>
     <entry>某个 <literal>object-replicator</literal> 进程没在 24 小时以内完成复制循环时发出的警报</entry>
     <entry>这可能说明 <literal>object-replication</literal> 进程已经卡住。</entry>
     <entry>
      <para>
       SSH 连接受影响的主机并用以下指令重启进程：
      </para>
      <screen>sudo systemctl restart swift-account-replicator</screen>
      <para>
       造成这个问题的另一个可能原因是有可能有损坏的文件系统。请在受影响节点上的以下日志中寻找支持这个可能的信号。
      </para>
      <screen>/var/log/swift/swift.log
       /var/log/kern.log</screen>
      <para>
       文件系统可能需要被擦除，如果您需要相关的建议，请联系 &serviceteam;。在这之后，您可以用以下的步骤重新格式化文件系统：
      </para>
      <orderedlist xml:id="ol_lbr_2zp_mx">
       <listitem>
        <para>
         登录 &lcm;。
        </para>
       </listitem>
       <listitem>
        <para>
         对受影响的节点使用 Swift 部署脚本，这会格式化已被擦除的文件系统：
        </para>
<screen>cd ~/scratch/ansible/next/ardana/ansible/
         ansible-playbook -i hosts/verb_hosts swift-deploy.yml --limit &lt;hostname&gt;</screen>
       </listitem>
      </orderedlist>
     </entry>
    </row>
    <row>
     <entry>Swift configuration file ownership</entry>
     <entry><literal>/etc/swift</literal> 中的文件或文件夹不被 Swift 拥有时发出的警报。</entry>
     <entry>有人手动在 <literal>/etc/swift</literal> 中编辑或添加了文件。</entry>
     <entry>
      <para>
       使用以下指令更改 <literal>/etc/swift</literal> 中文件的拥有者：
      </para>
      <screen>sudo chown swift.swift /etc/swift/, /etc/swift/*</screen>
     </entry>
    </row>
    <row>
     <entry>Swift data filesystem ownership</entry>
     <entry><literal>/srv/node</literal> 中的文件或文件夹不被 Swift 拥有时发出的警报。</entry>
     <entry>对于 <literal>/srv/node/*</literal> 中的目录，可能会发生根分区重新映像或重新安装，并且分配给 Swift 用户的 UID 发生更改。这样目录和文件就不由分配给 Swift 用户的 UID 所有。</entry>
     <entry>对于 <literal>/srv/node/*</literal> 中的目录和文件，比较此系统和其他系统上 swift 用户的 UID 以及 <literal>/srv/node/*</literal> 所有者的 UID。如果可能，使 Swift 用户的 UID 与目录/文件匹配。否则，使用与上面类似的命令更改 <literal>/srv/node</literal> 路径下所有文件和目录的所有权。</entry>
    </row>
    <row>
     <entry>Drive URE errors detected</entry>
     <entry><literal>swift-drive-audit</literal> 报告 Swift 服务使用的硬盘上存在不可恢复的读取错误时的警报。</entry>
     <entry>Swift 尝试访问目录时发生了不可恢复的读取错误。</entry>
     <entry>
      <para>
       报告的 URE 仅适用于文件系统元数据（即目录结构）。对于目标文件中的 URE，Swift 系统会自动删除该文件，并从其他副本中复制一个新副本。
      </para>
      <para>
       URE 是大型硬盘的常规功能。这并不意味着硬盘发生故障。但是，如果您在特定硬盘上获得常规 URE，则可能表示硬盘确实已失败并应更换。
      </para>
      <para>
       您可以使用标准 XFS 修复操作来更正文件系统中的 URE。
      </para>
      <para>
       如果 XFS 修复失败，您应该按如下方式擦除 GPT 表（其中 &lt;drive_name&gt; 替换为实际的硬盘名称）：
      </para>
      <screen>sudo dd if=/dev/zero of=/dev/sd&lt;drive_name&gt; bs=$((1024*1024)) count=1</screen>
      <para>
       然后，按照以下步骤重新格式化硬盘，重新挂载，并在受影响的节点上重新启动 Swift 服务。
      </para>
      <orderedlist xml:id="ol_mbr_2zp_mx">
       <listitem>
        <para>
         登录 &lcm;。
        </para>
       </listitem>
       <listitem>
        <para>
         运行 Swift 重配置脚本，指定受影响的节点：
        </para>
<screen>cd ~/scratch/ansible/next/ardana/ansible/
         ansible-playbook -i hosts/verb_hosts _swift-configure.yml --limit &lt;hostname&gt;</screen>
       </listitem>
      </orderedlist>
      <para>
       重新格式化包含 Swift 数据的硬盘是安全的，因为 Swift 维护其他数据副本（通常，Swift 配置为包含所有数据的三个副本）。
      </para>
     </entry>
    </row>
    <row>
     <entry>Swift service</entry>
     <entry><literal>component</literal> 字段指定的 Swift 进程未运行时发出警报。</entry>
     <entry>由 <literal>hostname</literal> 维度指定的主机上的 <literal>component</literal> 维度指定的守护程序已停止运行。</entry>
     <entry>
      <para>
       检查 <literal>/var/log/swift/swift.log</literal> 文件，查找与 Swift 进程相关的可能错误消息。有问题的过程列在 <literal>component</literal> 维度的报警维度中。
      </para>
      <para>
       通过运行 <literal>swift-start.yml</literal> 脚本重新启动 Swift 进程，步骤如下：
      </para>
      <orderedlist xml:id="ol_nbr_2zp_mx">
       <listitem>
        <para>
         登录 &lcm;。
        </para>
       </listitem>
       <listitem>
        <para>
         对受影响的主机运行 Swift 启动脚本：
        </para>
<screen>cd ~/scratch/ansible/next/ardana/ansible/
         ansible-playbook -i hosts/verb_hosts swift-start.yml --limit &lt;hostname&gt;</screen>
       </listitem>
      </orderedlist>
     </entry>
    </row>
    <row>
     <entry>Swift filesystem mount point status</entry>
     <entry>如果未正确挂载 Swift 使用的文件系统/硬盘时发出的警报。</entry>
     <entry>
      <para>
       <literal>device</literal> 维度指定的设备未正确挂载在 <literal>mount</literal> 维度指定的安装点上。
      </para>
      <para>
       最可能的原因是硬盘发生故障或者在引导过程中发生了临时故障所以并未挂载。
      </para>
      <para>
       其他可能的原因是文件系统损坏，导致其无法被挂载。
      </para>
     </entry>
     <entry>
      <para>
       重新启动节点，查看文件系统是否仍然未安装。
      </para>
      <para>
       如果文件系统已损坏，请参阅用于“硬盘 URE 错误”警报的过程以擦除并重新格式化硬盘。
      </para>
     </entry>
    </row>
    <row>
     <entry>Swift uptime-monitor status</entry>
     <entry>swiftlm-uptime-monitor 使用 Keystone（keystone-get-token），Swift（rest-api）或 Swift 的健康检查时发送错误时的警报。</entry>
     <entry>swiftlm-uptime-monitor 无法从 Keystone 获取令牌，或者无法从 Swift Object-Storage API 获得成功的响应。</entry>
     <entry>
      <para>
       检查 Keystone 服务是否正在运行：
      </para>
      <orderedlist xml:id="ol_obr_2zp_mx">
       <listitem>
        <para>
         登录 &lcm;。
        </para>
       </listitem>
       <listitem>
        <para>
         检查 Keystone 服务的状态：
        </para>
<screen>cd ~/scratch/ansible/next/ardana/ansible/
         ansible-playbook -i hosts/verb_hosts keystone-status.yml</screen>
       </listitem>
       <listitem>
        <para>
         如果它没有运行，请启动该服务：
        </para>
<screen>cd ~/scratch/ansible/next/ardana/ansible/
         ansible-playbook -i hosts/verb_hosts keystone-start.yml</screen>
       </listitem>
       <listitem>
        <para>
         如果需要进一步帮助解决 Keystone 服务问题，请联系支持团队。
        </para>
       </listitem>
      </orderedlist>
      <para>
       检查 Swift 是否正在运行：
      </para>
      <orderedlist xml:id="ol_pbr_2zp_mx">
       <listitem>
        <para>
         登录 &lcm;。
        </para>
       </listitem>
       <listitem>
        <para>
         检查 Keystone 服务的状态：
        </para>
<screen>cd ~/scratch/ansible/next/ardana/ansible/
         ansible-playbook -i hosts/verb_hosts swift-status.yml</screen>
       </listitem>
       <listitem>
        <para>
         如果它没有运行，请启动该服务：
        </para>
<screen>cd ~/scratch/ansible/next/ardana/ansible/
         ansible-playbook -i hosts/verb_hosts swift-start.yml</screen>
       </listitem>
      </orderedlist>
      <para>
       重新启动 swift-uptime-monitor，如下所示：
      </para>
      <orderedlist xml:id="ol_qbr_2zp_mx">
       <listitem>
        <para>
         登录到运行 swift-proxy-server 服务的第一台服务器。使用下面的这个脚本来确定这是哪个主机：
        </para>
<screen>cd ~/scratch/ansible/next/ardana/ansible/
         ansible-playbook -i hosts/verb_hosts swift-status.yml --limit SWF-PRX[0]</screen>
       </listitem>
       <listitem>
        <para>
         使用以下命令重新启动 swift-uptime-monitor：
        </para>
        <screen>sudo systemctl restart swiftlm-uptime-monitor</screen>
       </listitem>
      </orderedlist>
     </entry>
    </row>
    <row>
     <entry>Swift Keystone server connect</entry>
     <entry>无法与 Keystone 服务打开套接字（用于令牌验证）时发出的警报</entry>
     <entry>身份服务（Keystone）服务器可能已关闭。另一个可能的原因是报告问题的主机与 Keystone 服务器或 <literal>haproxy</literal> 进程之间的网络没有向 Keystone 转发请求。</entry>
     <entry><literal>URL</literal> 维度包含虚拟 IP 地址的名称。使用 cURL 或类似程序确认是否可以对虚拟 IP 地址建立连接。检查 <literal>haproxy</literal> 是否正在运行。检查 Keystone 服务是否正常工作。</entry>
    </row>
    <row>
     <entry>Swift service listening on ip and port</entry>
     <entry>当 swift 服务没有侦听正确的端口或 IP 时发出的警报。</entry>
     <entry>Swift服务可能已关闭。</entry>
     <entry>
      <para>
       根据 <literal>hostname</literal> 维度，检查受影响主机上的 Swift 服务的状态。
      </para>
      <orderedlist xml:id="ol_l4w_tys_lx">
       <listitem>
        <para>
         登录 &lcm;。
        </para>
       </listitem>
       <listitem>
        <para>
         运行 Swift 状态脚本以确认状态：
        </para>
<screen>cd ~/scratch/ansible/next/ardana/ansible/
         ansible-playbook -i hosts/verb_hosts swift-status.yml --limit &lt;hostname&gt;</screen>
       </listitem>
      </orderedlist>
      <para>
       如果确定了问题，您可以使用以下步骤停止并重新启动 Swift 服务：
      </para>
      <orderedlist xml:id="ol_rbr_2zp_mx">
       <listitem>
        <para>
         登录 &lcm;。
        </para>
       </listitem>
       <listitem>
        <para>
         在受影响的节点上停止 Swift 服务：
        </para>
<screen>cd ~/scratch/ansible/next/ardana/ansible/
         ansible-playbook -i hosts/verb_hosts swift-stop.yml --limit &lt;hostname&gt;</screen>
       </listitem>
       <listitem>
        <para>
         在受影响的节点上重新启动 Swift 服务：
        </para>
<screen>cd ~/scratch/ansible/next/ardana/ansible/
         ansible-playbook -i hosts/verb_hosts swift-start.yml --limit &lt;hostname&gt;</screen>
       </listitem>
      </orderedlist>
     </entry>
    </row>
    <row>
     <entry>Swift rings checksum</entry>
     <entry>如果 swift 环校验不在所有主机上都匹配时发出的警报。</entry>
     <entry>
      <para>
       每个节点上的 Swift 环文件必须相同。这些文件位于 <literal>/etc/swift/*.ring.gz</literal>
      </para>
      <para>
       如果您刚刚更改了任何环并且仍在部署更改，触发此警报是正常的。
      </para>
     </entry>
     <entry>
      <para>
       如果您刚刚更改了任何 Swift 环，如果您等到更改完成后，此警报可能会自行清除。如果没有，请继续执行这些步骤。
      </para>
      <para>
       使用 <literal>sudo swift-recon --md5</literal> 查找哪个节点已过时的环。
      </para>
      <para>
       使用以下步骤运行 <literal>swift-reconfigure.yml</literal> 脚本。这应该将同一组环部署到每个节点。
      </para>
      <orderedlist xml:id="ol_sbr_2zp_mx">
       <listitem>
        <para>
         登录 &lcm;。
        </para>
       </listitem>
       <listitem>
        <para>
         对受影响的主机运行 Swift 重配置脚本：
<!-- ERROR -->
        </para>
<screen>cd ~/scratch/ansible/next/ardana/ansible/
         ansible-playbook -i hosts/verb_hosts swift-reconfigure.yml</screen>
       </listitem>
      </orderedlist>
     </entry>
    </row>
    <row>
     <entry>Swift memcached server connect</entry>
     <entry>无法与指定的 memcached 服务器打开套接字时发出的警报。</entry>
     <entry>服务器可能已关闭。运行服务器的 memcached 守护程序可能已停止。</entry>
     <entry>
      <para>
       如果服务器已关闭，请重新启动它。
      </para>
      <para>
       如果 memcached 已停止，您可以使用 <literal>memcached-start.yml</literal> 脚本重新启动它，使用以下步骤。如果此操作失败，则重新启动节点来重新启动该进程。
      </para>
      <orderedlist xml:id="ol_tbr_2zp_mx">
       <listitem>
        <para>
         登录 &lcm;。
        </para>
       </listitem>
       <listitem>
        <para>
         对受影响的主机运行 memcached 启动脚本：
        </para>
<screen>cd ~/scratch/ansible/next/ardana/ansible/
         ansible-playbook -i hosts/verb_hosts memcached-start.yml --limit &lt;hostname&gt;</screen>
       </listitem>
      </orderedlist>
      <para>
       如果服务器和 memcached 正在运行，则可能存在阻塞端口 11211 的网络问题。
      </para>
      <para>
       如果您在不同服务器上看到偶发警报，则系统可能资源不足。请联系 HPE 支持部门获取建议。
      </para>
     </entry>
    </row>
    <row>
     <entry>Swift individual disk usage exceeds 80%</entry>
     <entry>当 Swift 使用的硬盘驱动器的利用率超过 80％ 时发出的警报。</entry>
     <entry>通常，所有硬盘驱动器将以大致相同的速率填充。如果单个硬盘驱动器的填充速度比其他驱动器快，则表明复制进程存在问题。</entry>
     <entry>
      <para>
       如果许多/大多数硬盘驱动器已满 80％，则需要向系统添加更多节点或删除现有对象。
      </para>
      <para>
       如果一个硬盘驱动器的使用率明显高于其他硬盘驱动器的平均值（超过 30％），则应检查 Swift 进程是否在服务器上运行（使用以下步骤），并查找与主机相关的警报。否则继续监视情况。
      </para>
      <orderedlist xml:id="ol_ubr_2zp_mx">
       <listitem>
        <para>
         登录 &lcm;。
        </para>
       </listitem>
       <listitem>
        <para>
         运行 Swift status：
        </para>
<screen>cd ~/scratch/ansible/next/ardana/ansible/
         ansible-playbook -i hosts/verb_hosts swift-status.yml</screen>
       </listitem>
      </orderedlist>
     </entry>
    </row>
    <row>
     <entry>Swift individual disk usage exceeds 90%</entry>
     <entry>当 Swift 使用的硬盘驱动器的利用率超过 90％ 时发出的警报。</entry>
     <entry>通常，所有硬盘驱动器将以大致相同的速率填充。如果单个硬盘驱动器的填充速度比其他驱动器快，则表明复制进程存在问题。</entry>
     <entry>
      <para>
       如果一个硬盘驱动器的使用率明显高于其他硬盘驱动器的平均值（超过 30％），则应检查 Swift 进程是否在服务器上运行，使用以下步骤：
      </para>
      <orderedlist xml:id="ol_vbr_2zp_mx">
       <listitem>
        <para>
         登录 &lcm;。
        </para>
       </listitem>
       <listitem>
        <para>
         运行 Swift status:
        </para>
<screen>cd ~/scratch/ansible/next/ardana/ansible/
         ansible-playbook -i hosts/verb_hosts swift-status.yml</screen>
       </listitem>
      </orderedlist>
      <para>
       还要查找与主机相关的警报。单个硬盘驱动器填充可能表明复制进程的问题。
      </para>
      <para>
       使用 <literal>--limit</literal> 参数重新启动该主机上的 Swift 以指定主机：
      </para>
      <orderedlist xml:id="ol_wbr_2zp_mx">
       <listitem>
        <para>
         登录 &lcm;。
        </para>
       </listitem>
       <listitem>
        <para>
         停止 Swift 服务：
        </para>
<screen>cd ~/scratch/ansible/next/ardana/ansible/
         ansible-playbook -i hosts/verb_hosts swift-stop.yml --limit &lt;hostname&gt;</screen>
       </listitem>
       <listitem>
        <para>
         重新启动 Swift 服务：
        </para>
<screen>cd ~/scratch/ansible/next/ardana/ansible/
         ansible-playbook -i hosts/verb_hosts swift-start.yml --limit &lt;hostname&gt;</screen>
       </listitem>
      </orderedlist>
      <para>
       如果利用率未返回与其他硬盘驱动器类似的值，则可以重新格式化硬盘驱动器。您应该只有在所有硬盘驱动器的平均利用率低于 80％ 时才应执行此操作。要格式化硬盘驱动器，请联系 HPE 支持以获取说明。
      </para>
     </entry>
    </row>
    <row>
     <entry>Swift total disk usage exceeds 80%</entry>
     <entry>当 Swift 硬盘驱动器的平均硬盘利用率超过 80％ 利用率时发出警报。</entry>
     <entry>系统中对象的数量和大小开始填充可用硬盘空间。帐户和容器存储包含在硬盘利用率中，但是与对象相比，这通常占用 1-2％ 的空间，因此对象存储是硬盘空间的主要用户。</entry>
     <entry>
      <para>
       您需要向系统添加更多节点或删除现有对象以保持 80％ 以下的利用率。
      </para>
      <note>
       <para>
        如果删除项目/帐户，该帐户中的对象直到一周之后才会被 <literal>account-reaper</literal> 进程删除，因此这不是快速释放空间的好方法。
       </para>
      </note>
     </entry>
    </row>
    <row>
     <entry>Swift total disk usage exceeds 90%</entry>
     <entry>当 Swift 硬盘驱动器的平均硬盘利用率超过 90％ 时发出的警报。</entry>
     <entry>系统中对象的数量和大小开始填充可用硬盘空间。帐户和容器存储包含在硬盘利用率中，但是与对象相比，这通常占用 1-2％ 的空间，因此对象存储是硬盘空间的主要用户。</entry>
     <entry>
      <para>
       如果硬盘驱动器已满 90％，则必须立即停止将新对象放入系统的所有应用程序。此时，您可以删除对象或添加更多服务器。
      </para>
      <para>
       使用以下步骤，还应将 <literal>fallocate_reserve</literal> 值设置为高于硬盘驱动器上当前可用空间的值。这将阻止创建更多对象。
      </para>
      <orderedlist xml:id="ol_xbr_2zp_mx">
       <listitem>
        <para>
         登录 &lcm;。
        </para>
       </listitem>
       <listitem>
        <para>
         编辑下面的配置文件，并将 <literal>fallocate_reserve</literal> 的值更改为高于硬盘驱动器上当前可用空间的值：
        </para>
        <screen>~/openstack/my_cloud/config/swift/account-server.conf.j2
         ~/openstack/my_cloud/config/swift/container-server.conf.j2
         ~/openstack/my_cloud/config/swift/object-server.conf.j2</screen>
       </listitem>
       <listitem>
        <para>
         将更改提交给 git：
        </para>
        <screen>git add -A
         git commit -a -m "changing Swift fallocate_reserve value"</screen>
       </listitem>
       <listitem>
        <para>
         运行配置处理器：
        </para>
        <screen>cd ~/openstack/ardana/ansible
         ansible-playbook -i hosts/localhost config-processor-run.yml</screen>
       </listitem>
       <listitem>
        <para>
         更新部署目录：
        </para>
        <screen>cd ~/openstack/ardana/ansible
         ansible-playbook -i hosts/localhost ready-deployment.yml</screen>
       </listitem>
       <listitem>
        <para>
         运行 Swift 重配置脚本来部署更改：
        </para>
<screen>cd ~/scratch/ansible/next/ardana/ansible/
         ansible-playbook -i hosts/verb_hosts swift-reconfigure.yml</screen>
       </listitem>
      </orderedlist>
      <para>
       如果允许文件系统变满，则无法删除对象或向系统添加更多节点。这是因为添加节点时系统需要一些可用空间来处理复制进程。没有可用空间，复制进程无法工作。
      </para>
     </entry>
    </row>
    <row>
     <entry>Swift service per-minute availability</entry>
     <entry>如果 swift 服务报告在前一分钟不可用时发出的警报。</entry>
     <entry><literal>swiftlm-uptime-monitor</literal> 服务在第一个代理服务器上运行。它监视 Swift 端点并报告延迟数据。如果端点停止报告，则会生成此警报。</entry>
     <entry>
      <para>
       端点可能会停止运行的原因有很多。检查：
      </para>
      <itemizedlist xml:id="ul_ghg_4hc_4v">
       <listitem>
        <para>
         <literal>haproxy</literal> 是否在控制节点上运行？
        </para>
       </listitem>
       <listitem>
        <para>
         <literal>swift-proxy-server</literal> 是否在 Swift 代理服务器上运行？
        </para>
       </listitem>
      </itemizedlist>
     </entry>
    </row>
    <row>
     <entry>Swift rsync connect</entry>
     <entry>如果无法向指定的 rsync 服务器打开套接字时发出的警报</entry>
     <entry>无法联系指定节点上的 rsync 守护程序。最可能的原因是节点已关闭。rsync 服务也可能已在节点上停止。</entry>
     <entry>
      <para>
       如果服务器已关闭，请重新启动服务器。
      </para>
      <para>
       尝试使用此命令重新启动 rsync：
      </para>
      <screen>systemctl restart rsync.service</screen>
     </entry>
    </row>
    <row>
     <entry>Swift smart array controller status</entry>
     <entry>如果 Smart Array 出现故障时发出的警报。</entry>
     <entry>
      <para>
       Smart Array 或 Smart HBA 控制器出现故障或控制器组件（如电池）出现故障或缓存被禁用。
      </para>
     </entry>
     <entry>
      <para>
       登录到报告的主机并运行以下命令以查看控制器的状态：
      </para>
      <screen>sudo hpssacli
       =&gt; controller show all detail</screen>
      <para>
       对于硬件故障（例如电池故障），请更换发生故障的组件。如果禁用缓存，请重新启用缓存。
      </para>
     </entry>
    </row>
    <row>
     <entry>Swift physical drive status</entry>
     <entry>如果物理驱动器出现故障时发出的警报。</entry>
     <entry>服务器上的硬盘驱动器发生故障或有警告。</entry>
     <entry>
      <para>
       登录到报告并运行这些命令以查找驱动器的状态：
      </para>
      <screen>sudo hpssacli
       =&gt; ctrl slot=1 pd all show</screen>
      <para>
       更换任何损坏的驱动器。
      </para>
     </entry>
    </row>
    <row>
     <entry>Swift logical drive status</entry>
     <entry>如果逻辑驱动器出现故障，则发出警报。</entry>
     <entry>服务器上的 LUN 已损坏或已失败。</entry>
     <entry>
      <para>
       登录报告的主机，执行以下命令查找 LUN 的状态：
      </para>
      <screen>sudo hpssacli
       =&gt; ctrl slot=1 ld all show
       =&gt; ctrl slot=1 pd all show</screen>
      <para>
       更换任何损坏的驱动器。
      </para>
     </entry>
    </row>
    <row>
     <entry>Process Check</entry>
     <entry>特定进程未在运行时发出的警报。</entry>
     <entry>如果 <literal>service</literal> 维度是 <literal>object-storage</literal>，请参阅“Swift Service”警报的描述以了解可能的原因。</entry>
    </row>
    <row>
     <entry>HTTP Status</entry>
     <entry>指定的 HTTP 端点关闭或无法访问时发出的警报。</entry>
     <entry>如果 <literal>service</literal> 维度是 <literal>object-store</literal>，请参阅“Swift host socket connect”警报的描述以了解可能的原因。</entry>
     <entry>如果 <literal>service</literal> 维度是 <literal>object-storage</literal>，请参阅“Swift host socket connect”警报的描述以了解可能的原因。</entry>
<!-- ERROR -->
    </row>
    <row>
     <entry>Service Log Directory Size</entry>
     <entry>服务日志目录在使用超出配额的硬盘空间。</entry>
     <entry>这可能是因为服务被设置成 <literal>DEBUG</literal> 而不是 <literal>INFO</literal> 等级。另外可能的原因是一个重复的错误信息填满了日志文件。最后，这也可能是因为日志滚动没有被设置正确，因而旧的日志没有被正常删除。</entry>
     <entry>找到消耗过多硬盘空间的服务，查看日志。如果有 <literal>DEBUG</literal> 日志条目，请把日志等级设为 <literal>INFO</literal>。如果日志在重复记录一个错误信息，请解决该错误信息。如果旧的日志文件存在，配置日志滚动设置来自动移除它们。如果需要，您也可以选择备份后手动删除旧的日志。</entry>
    </row>
    <row>
     <entry morerows="8">block-storage</entry>
     <entry>Process Check</entry>
     <entry>
      <para>
       由 <literal>component</literal> 维度指定的不同 Cinder 服务警报：
      </para>
      <itemizedlist xml:id="ul_ybr_2zp_mx">
       <listitem>
        <para>
         cinder-api
        </para>
       </listitem>
       <listitem>
        <para>
         cinder-backup
        </para>
       </listitem>
       <listitem>
        <para>
         cinder-scheduler
        </para>
       </listitem>
       <listitem>
        <para>
         cinder-volume
        </para>
       </listitem>
      </itemizedlist>
     </entry>
     <entry>进程已崩溃。</entry>
     <entry>
      <para>
       在受影响的节点上重新启动该进程。检查相关日志。
      </para>
      <orderedlist xml:id="ol_yhz_rpk_pw">
       <listitem>
        <para>
         登录 &lcm;。
        </para>
       </listitem>
       <listitem>
        <para>
         运行 cinder-start.yml 脚本以重新启动进程：
        </para>
<screen>cd ~/scratch/ansible/next/ardana/ansible
         ansible-playbook -i hosts/verb_hosts cinder-start.yml --limit &lt;hostname&gt;</screen>
        <note>
         <para>
          <literal>--limit &lt;hostname&gt;</literal> 开关是可选的。如果包含它，那么您应该使用的 <literal>&lt;hostname&gt;</literal> 是引发警报的主机。
         </para>
        </note>
       </listitem>
      </orderedlist>
     </entry>
    </row>
    <row>
     <entry>Process Check</entry>
     <entry>
      <para>
       特定进程未在运行时发出的警报。
      </para>
      <screen>process_name=cinder-backup</screen>
     </entry>
     <entry>进程已崩溃。</entry>
     <entry>如果服务已迁移，则警报可能不正确。在重新启动服务之前，验证该服务是否要在此节点上运行。查看关联的日志。</entry>
    </row>
    <row>
     <entry>Process Check</entry>
     <entry>
      <para>
       特定进程未在运行时发出的警报。
      </para>
      <screen>process_name=cinder-scheduler</screen>
     </entry>
     <entry>进程已崩溃。</entry>
     <entry>
      <para>
       在受影响的节点上重新启动该进程。检查相关日志。
      </para>
      <orderedlist xml:id="ol_zbr_2zp_mx">
       <listitem>
        <para>
         登录 &lcm;。
        </para>
       </listitem>
       <listitem>
        <para>
         运行 cinder-start.yml 脚本以重新启动进程：
        </para>
<screen>cd ~/scratch/ansible/next/ardana/ansible
         ansible-playbook -i hosts/verb_hosts cinder-start.yml --limit &lt;hostname&gt;</screen>
        <note>
         <para>
          <literal>--limit &lt;hostname&gt;</literal> 开关是可选的。如果包含它，那么您应该使用的 <literal>&lt;hostname&gt;</literal> 是引发警报的主机。
         </para>
        </note>
       </listitem>
      </orderedlist>
     </entry>
    </row>
    <row>
     <entry>Process Check</entry>
     <entry>
      <para>
       特定进程未在运行时发出的警报。
      </para>
      <screen>process_name=cinder-volume</screen>
     </entry>
     <entry>进程已崩溃。</entry>
     <entry>如果服务已迁移，则警报可能不正确。在重新启动服务之前，验证该服务是否要在此节点上运行。查看关联的日志。</entry>
    </row>
    <row>
     <entry>Cinder backup running &lt;hostname&gt; check</entry>
     <entry>Cinder 备份单件检查。</entry>
     <entry>
      <para>
       备份进程可能：
      </para>
      <itemizedlist xml:id="ul_acr_2zp_mx">
       <listitem>
        <para>
         在不应该运行的节点上运行，或
        </para>
       </listitem>
       <listitem>
        <para>
         未在应该运行的节点上运行
        </para>
       </listitem>
      </itemizedlist>
     </entry>
     <entry>
      <para/>
      <orderedlist xml:id="ol_ztz_mpk_pw">
       <listitem>
        <para>
         登录 &lcm;。
        </para>
       </listitem>
       <listitem>
        <para>
         运行此脚本以迁移服务：
        </para>
<screen>cd ~/scratch/ansible/next/ardana/ansible/
         ansible-playbook -i hosts/verb_hosts cinder-migrate-volume.yml</screen>
       </listitem>
      </orderedlist>
     </entry>
    </row>
    <row>
     <entry>Cinder volume running &lt;hostname&gt; check</entry>
     <entry>Cinder 备份单件检查。</entry>
     <entry>
      <para>
       <literal>cinder-volume</literal> 可能：
      </para>
      <itemizedlist xml:id="ul_bcr_2zp_mx">
       <listitem>
        <para>
         在不应该运行的节点上运行，或
        </para>
       </listitem>
       <listitem>
        <para>
         未在应该运行的节点上运行
        </para>
       </listitem>
      </itemizedlist>
     </entry>
     <entry>
      <para>
       运行 <literal>cinder-migrate-volume.yml</literal> 脚本将卷和备份迁移到正确的节点：
      </para>
      <orderedlist xml:id="ol_kgy_4pk_pw">
       <listitem>
        <para>
         登录 &lcm;。
        </para>
       </listitem>
       <listitem>
        <para>
         运行此脚本以迁移服务：
        </para>
<screen>cd ~/scratch/ansible/next/ardana/ansible/
         ansible-playbook -i hosts/verb_hosts cinder-migrate-volume.yml</screen>
       </listitem>
      </orderedlist>
     </entry>
    </row>
    <row>
     <entry>Storage faulty lun check</entry>
     <entry>
      <para>
       如果使用 smartarray 的 HPE 服务器上的本地 LUN 不正常时会发出的警报。
      </para>
      <note vendor="hpe">
       <para>
        This alarm only pertains to those using &hpe; servers with smartarray.
       </para>
      </note>
     </entry>
     <entry>服务器上的 LUN 已损坏或已失败。</entry>
     <entry>
      <para>
       登录报告的主机，执行以下命令查看 LUN 的状态：
      </para>
      <screen>sudo hpssacli
       =&gt; ctrl slot=1 ld all show
       =&gt; ctrl slot=1 pd all show</screen>
      <para>
       更换任何损坏的驱动器。
      </para>
     </entry>
    </row>
    <row>
     <entry>Storage faulty drive check</entry>
     <entry>
      <para>
       如果使用 smartarray 的 HPE 服务器上的本地 LUN 不正常时会发出时警报。
      </para>
      <note vendor="hpe">
       <para>
        This alarm only pertains to those using &hpe; servers with smartarray.
       </para>
      </note>
     </entry>
     <entry>服务器上的硬盘驱动器发生故障或有警告。</entry>
     <entry>
      <para>
       登录到报告并运行这些命令以查找驱动器的状态：
      </para>
      <screen>sudo hpssacli
       =&gt; ctrl slot=1 pd all show</screen>
      <para>
       更换任何损坏的驱动器。
      </para>
     </entry>
    </row>
    <row>
     <entry>Service Log Directory Size</entry>
     <entry>服务日志目录在使用超出配额的硬盘空间。</entry>
     <entry>这可能是因为服务被设置成 <literal>DEBUG</literal> 而不是 <literal>INFO</literal> 等级。另外可能的原因是一个重复的错误信息填满了日志文件。最后，这也可能是因为日志滚动没有被设置正确，因而旧的日志没有被正常删除。</entry>
     <entry>找到消耗过多硬盘空间的服务，查看日志。如果有 <literal>DEBUG</literal> 日志条目，请把日志等级设为 <literal>INFO</literal>。如果日志在重复记录一个错误信息，请解决该错误信息。如果旧的日志文件存在，配置日志滚动设置来自动移除它们。如果需要，您也可以选择备份后手动删除旧的日志。</entry>
    </row>
    <!---->
<!-- carl 2018-05-25: Ceph alarm table rows; replace with other storage alarm
     information
     <row>
     <entry morerows="20">ceph-storage</entry>
     <entry>Process Check</entry>
     <entry>
      <para>
       由 <literal>component</literal> 维度指定的不同 Ceph 服务警报：
       <literal>component</literal> dimension:
      </para>
      <itemizedlist xml:id="ul_vyv_1z5_qv">
       <listitem>
        <para>
         &lt;cluster_name&gt;-osd.&lt;id&gt;
        </para>
       </listitem>
       <listitem>
        <para>
         &lt;cluster_name&gt;-mon.&lt;id&gt;
        </para>
       </listitem>
       <listitem>
        <para>
         &lt;cluster_name&gt;-radosgw.&lt;id&gt;
        </para>
       </listitem>
      </itemizedlist>
     </entry>
     <entry>进程已崩溃。</entry>
     <entry>
      <para>
       用以下步骤重启受影响的节点的进程：
      </para>
      <orderedlist xml:id="ol_gcr_2zp_mx">
       <listitem>
        <para>
         登录 &lcm;。
        </para>
       </listitem>
       <listitem>
        <para>
         对受影响的节点使用 Ceph 启动脚本：
        </para>
<screen>cd ~/scratch/ansible/next/ardana/ansible
ansible-playbook -i hosts/verb_hosts ceph-start.yml -/-limit &lt;hostname&gt;</screen>
       </listitem>
      </orderedlist>
     </entry>
    </row>
    <row>
     <entry>Degraded Ceph cluster alert</entry>
     <entry>Alarms if ceph cluster status is not ok.</entry>
     <entry>
      <para>
       The likely cause is one of the following possibilities:
      </para>
      <itemizedlist xml:id="ul_bxh_xkt_2y">
       <listitem>
        <para>
         OSDs being down
        </para>
       </listitem>
       <listitem>
        <para>
         Clock skew
        </para>
       </listitem>
       <listitem>
        <para>
         PG errors
        </para>
       </listitem>
      </itemizedlist>
     </entry>
     <entry>
      <para>
       You should check the following:
      </para>
      <itemizedlist xml:id="ul_l35_xkt_2y">
       <listitem>
        <para>
         cluster overall status, by running the following command:
        </para>
        <screen>ceph -s</screen>
       </listitem>
       <listitem>
        <para>
         the reason, which can be found in the command output
        </para>
       </listitem>
      </itemizedlist>
      <para>
       Based on reason, you might have to perform specific actions to correct
       the failure. For e.g. if cluster is in HEALTH_WARN state because one of
       OSD being down then you need to recover failed OSD. In general, you
       might like to perform following steps to triage the reason of failure:
      </para>
      <itemizedlist xml:id="ul_m35_xkt_2y">
       <listitem>
        <para>
         Run <emphasis role="bold">'ceph -s'</emphasis> command to get cluster
         status
        </para>
       </listitem>
       <listitem>
        <para>
         Observing following logs:
         <emphasis role="bold">/var/log/ceph/&lt;cluster
          name&gt;-cluster.log</emphasis> on monitor nodes,
         <emphasis role="bold"
                   >/var/log/ceph/ceph-mon.&lt;hostname&gt;.log</emphasis>
         </para>
        </listitem>
        <listitem>
         <para>
          Run <emphasis role="bold">'ceph osd tree'</emphasis> command to check
          OSD status
         </para>
        </listitem>
       </itemizedlist>
      </entry>
     </row>
     <row>
      <entry>Broken Ceph cluster alert</entry>
      <entry>
       <para>
        Alarms if the Ceph cluster is unusable.
       </para>
       <para>
        Generates alarm only when cluster is in fatal error state, where no I/O
        operations can be performed.
       </para>
      </entry>
      <entry>
       <para>
        The cause may be one or more of the following possibilities:
       </para>
       <itemizedlist xml:id="ul_vdh_zkt_2y">
        <listitem>
         <para>
          All OSDs being down
         </para>
        </listitem>
        <listitem>
         <para>
          Ceph Monitor quorum is broken
         </para>
        </listitem>
        <listitem>
         <para>
          Cluster is full
         </para>
        </listitem>
       </itemizedlist>
      </entry>
      <entry>
       <para>
        Please observe following:
       </para>
       <itemizedlist xml:id="ul_ym4_zkt_2y">
        <listitem>
         <para>
          cluster overall status (by running the command 'ceph -s')
         </para>
        </listitem>
        <listitem>
         <para>
          reason (can be found in command output)
         </para>
        </listitem>
       </itemizedlist>
       <para>
        Based on reason, you might have to perform specific actions to correct
        the failure. For e.g. if cluster is in HEALTH_ERR state because of a
        broken Ceph Monitor quorum, check whether the ceph-monitor services are
        up and running and are reachable from each other. In general, you might
        like to perform following steps to triage the reason of failure:
       </para>
       <itemizedlist xml:id="ul_zm4_zkt_2y">
        <listitem>
         <para>
          Run <emphasis role="bold">'ceph -s'</emphasis> command to get cluster
          status
         </para>
        </listitem>
        <listitem>
         <para>
          Observing following logs:
          <emphasis role="bold">/var/log/ceph/&lt;cluster
           name&gt;-cluster.log</emphasis> on monitor nodes,
          <emphasis role="bold"
                    >/var/log/ceph/ceph-mon.&lt;hostname&gt;.log</emphasis>
          </para>
         </listitem>
         <listitem>
          <para>
           Run <emphasis role="bold">'ceph osd tree'</emphasis> command to check
           OSD status
          </para>
         </listitem>
        </itemizedlist>
       </entry>
      </row>
      <row>
       <entry>Degraded RGW availability alert</entry>
       <entry>Alarms if any of the Ceph RADOS gateways is not reachable.</entry>
       <entry>
        <para>
         The likely cause can be one or more of the following conditions on
         <emphasis
          role="bold">one or more</emphasis> of your
         Ceph RADOS gateway nodes:
        </para>
        <itemizedlist xml:id="ul_uzs_blt_2y">
         <listitem>
          <para>
           Failure in <literal>radosgw</literal> service
          </para>
         </listitem>
         <listitem>
          <para>
           Failure in <literal>apache2</literal> service
          </para>
         </listitem>
         <listitem>
          <para>
           Network connectivity issues
          </para>
         </listitem>
        </itemizedlist>
       </entry>
       <entry>
        <para>
         The mitigation plan depends on the type of error reported by the
         plugin.When a failure is reported, the following parameters should be
         checked:
        </para>
        <orderedlist xml:id="ol_fn3_clt_2y">
         <listitem>
          <para>
           Ensure the following services on the radosgw node are running:
          </para>
         </listitem>
        </orderedlist>
        <itemizedlist xml:id="ul_gn3_clt_2y">
         <listitem>
          <itemizedlist xml:id="ul_hn3_clt_2y">
           <listitem>
            <para>
             radosgw
            </para>
           </listitem>
           <listitem>
            <para>
             apache2
            </para>
           </listitem>
          </itemizedlist>
         </listitem>
        </itemizedlist>
        <para>
         2. Ensure that the radosgw site in apache is enabled.
        </para>
        <para>
         3. Verify the radosgw apache site configuration
         (<emphasis role="bold"
                    >/etc/apache2/sites-available/rgw.conf</emphasis>)
         is available/accessible and has
         '<emphasis role="bold">Listen</emphasis>' directive.
        </para>
        <para>
         4. If the issue still persists, check the radosgw service logs (i.e.
         <emphasis
          role="bold">/var/log/ceph/radosgw.log</emphasis>)
         for more details.
        </para>
       </entry>
      </row>
      <row>
       <entry>Broken RGW availability alert</entry>
       <entry>Alarms if all of the Ceph RADOS gateways are not reachable.</entry>
       <entry>
        <para>
         The likely cause can be one or more of the following conditions on
         <emphasis
          role="bold">all</emphasis> of your Ceph RADOS
         gateway nodes:
        </para>
        <itemizedlist xml:id="ul_lq2_2lt_2y">
         <listitem>
          <para>
           Failure in <literal>radosgw</literal> service
          </para>
         </listitem>
         <listitem>
          <para>
           Failure in <literal>apache2</literal> service
          </para>
         </listitem>
         <listitem>
          <para>
           Network connectivity issues
          </para>
         </listitem>
        </itemizedlist>
       </entry>
       <entry>
        <para>
         The mitigation plan depends on the type of error reported by the
         plugin.When a failure is reported, the following parameters should be
         checked:
        </para>
        <orderedlist xml:id="ol_fsl_2lt_2y">
         <listitem>
          <para>
           Ensure the following services on the radosgw node are running:
          </para>
         </listitem>
        </orderedlist>
        <itemizedlist xml:id="ul_gsl_2lt_2y">
         <listitem>
          <itemizedlist xml:id="ul_hsl_2lt_2y">
           <listitem>
            <para>
             radosgw
            </para>
           </listitem>
           <listitem>
            <para>
             apache2
            </para>
           </listitem>
          </itemizedlist>
         </listitem>
        </itemizedlist>
        <para>
         2. Ensure that the radosgw site in apache is enabled.
        </para>
        <para>
         3. Verify the radosgw apache site configuration
         (<emphasis role="bold"
                    >/etc/apache2/sites-available/rgw.conf</emphasis>)
         is available/accessible and has
         '<emphasis role="bold">Listen</emphasis>' directive.
        </para>
        <para>
         4. If the issue still persists, check the radosgw service logs (i.e.
         <emphasis
          role="bold">/var/log/ceph/radosgw.log</emphasis>)
         for more details.
        </para>
       </entry>
      </row>
      <row>
       <entry>OSD failure alert</entry>
       <entry>Alarms if any of the OSDs are down.</entry>
       <entry>
        <para>
         The likely cause can be one or more of the following conditions on
         <emphasis
          role="bold">one or more</emphasis> of your
         Ceph OSD nodes:
        </para>
        <itemizedlist xml:id="ul_n4l_flt_2y">
         <listitem>
          <para>
           disk failure
          </para>
         </listitem>
         <listitem>
          <para>
           service failure
          </para>
         </listitem>
         <listitem>
          <para>
           network connectivity failure
          </para>
         </listitem>
         <listitem>
          <para>
           host failure
          </para>
         </listitem>
        </itemizedlist>
       </entry>
       <entry>
        <para>
         Depending on the cause of the failure it will be useful to:
        </para>
        <itemizedlist xml:id="ul_mtt_flt_2y">
         <listitem>
          <para>
           Check node hosting OSD disk is accessible or not.
          </para>
         </listitem>
         <listitem>
          <para>
           Check node connectivity to ensure that monitors can be accessed from
           the node hosting failed OSD disk
          </para>
         </listitem>
         <listitem>
          <para>
           Check disk failure. The system log
           <emphasis role="bold"
                     >/var/log/syslog</emphasis>
           might provide useful information.
          </para>
         </listitem>
         <listitem>
          <para>
           Check service status. The
           <emphasis role="bold"
                     >/var/log/ceph/ceph-osd.&lt;id&gt;.log</emphasis>
           provides useful information for the respective osd services
          </para>
         </listitem>
        </itemizedlist>
        <para>
         Most of cases, OSD going in down state is because of either disk failure
         or service not running as expected. In case of disk failure, you might
         like to replace disk.
        </para>
       </entry>
      </row>
      <row>
       <entry>All OSD failure alert</entry>
       <entry>Alarms if all the OSDs are down.</entry>
       <entry>
        <para>
         The likely cause can be one or more of the following conditions on
         <emphasis
          role="bold">all</emphasis> of your Ceph OSD
         nodes:
        </para>
        <itemizedlist xml:id="ul_y2t_glt_2y">
         <listitem>
          <para>
           disk failure
          </para>
         </listitem>
         <listitem>
          <para>
           service failure
          </para>
         </listitem>
         <listitem>
          <para>
           network connectivity failure
          </para>
         </listitem>
         <listitem>
          <para>
           host failure
          </para>
         </listitem>
        </itemizedlist>
       </entry>
       <entry>
        <para>
         Depending on the cause of the failure it will be useful to:
        </para>
        <itemizedlist xml:id="ul_oyz_glt_2y">
         <listitem>
          <para>
           Check node hosting OSD disk is accessible or not.
          </para>
         </listitem>
         <listitem>
          <para>
           Check node connectivity to ensure that monitors can be accessed from
           the node hosting failed OSD disk
          </para>
         </listitem>
         <listitem>
          <para>
           Check disk failure. The system log
           <emphasis role="bold"
                     >/var/log/syslog</emphasis>
           might provide useful information.
          </para>
         </listitem>
         <listitem>
          <para>
           Check service status. The
           <emphasis role="bold"
                     >/var/log/ceph/ceph-osd.&lt;id&gt;.log</emphasis>
           provides useful information for the respective osd services
          </para>
         </listitem>
        </itemizedlist>
        <para>
         Most of cases, OSD going in down state is because of either disk failure
         or service not running as expected. In case of disk failure, you might
         like to replace disk.
        </para>
       </entry>
      </row>
      <row>
       <entry>Unutilized active OSD alert</entry>
       <entry>Alarms if any Ceph OSDs are up but not part of the cluster.</entry>
       <entry>The likely cause is the cluster has one or more OSDs that are running but not part
        of the cluster, which results in unutilized disk space.</entry>
       <entry>
        <para>
         Run the following command on any ceph monitor node to identify the OSDs
         that are up and out:
        </para>
        <screen>ceph osd tree | grep -e 'up.* 0'</screen>
        <para>
         The above command lists out nodes that are up but out of the cluster.You
         can make them a part of the cluster by running the below command:
        </para>
        <screen>ceph osd in &lt;osd.id&gt;</screen>
       </entry>
      </row>
      <row>
       <entry>Lower threshold (75% filled) Ceph Cluster capacity alert</entry>
       <entry>Alarms if the used capacity of the Ceph cluster exceeds 75% of the total
        capacity.</entry>
       <entry>Generates alarm if the used capacity of the Ceph cluster exceeds 75% of the total
        capacity.</entry>
       <entry>Increase the cluster capacity by adding more OSDs to the cluster. To add more OSDs
        update the disk input model and run ceph playbooks</entry>
      </row>
      <row>
       <entry>High threshold (85% filled) Ceph Cluster capacity alert</entry>
       <entry>Alarms if the used capacity of the ceph cluster exceeds 85% of the total
        capacity,</entry>
       <entry>Generates alarm if the used capacity of the ceph cluster exceeds 85% of the total
        capacity.</entry>
       <entry>Increase the cluster capacity by adding more OSDs to the cluster. To add more OSDs
        update the disk input model and run ceph playbooks</entry>
      </row>
      <row>
       <entry>Degraded Ceph monitor quorum alert</entry>
       <entry>Alarms if any of the Monitors are not in quorum.</entry>
       <entry>
        <para>
         The likely cause can be one or more of the following conditions on
         <emphasis
          role="bold">one or more</emphasis> of your
         Ceph monitor nodes:
        </para>
        <itemizedlist xml:id="ul_ecv_mlt_2y">
         <listitem>
          <para>
           Monitor services are down
          </para>
         </listitem>
         <listitem>
          <para>
           Network connectivity issues
          </para>
         </listitem>
         <listitem>
          <para>
           Clock skew
          </para>
         </listitem>
        </itemizedlist>
       </entry>
       <entry>
        <para>
         Mitigation plan depends on which state the quorum is in. The message
         shows the monitors which are out of quorum.For the monitors which are
         not in quorum:
        </para>
        <itemizedlist xml:id="ul_r3b_nlt_2y">
         <listitem>
          <para>
           Check the status of monitor services.
          </para>
         </listitem>
         <listitem>
          <para>
           Check the network connectivity between the monitor nodes having
           monitors out of quorum and other monitor nodes.
          </para>
         </listitem>
         <listitem>
          <para>
           Check the logs of the monitors in their respective nodes.
          </para>
         </listitem>
         <listitem>
          <para>
           In case of clock skew, make sure all the nodes running the monitor
           services are in sync with ntp server.
          </para>
         </listitem>
        </itemizedlist>
       </entry>
      </row>
      <row>
       <entry>Broken Ceph monitor quorum alert</entry>
       <entry>Alarms if all of the Monitors are not in quorum.</entry>
       <entry>
        <para>
         The likely cause can be one or more of the following conditions on
         <emphasis
          role="bold">all</emphasis> of your Ceph
         monitor nodes:
        </para>
        <itemizedlist xml:id="ul_ksz_nlt_2y">
         <listitem>
          <para>
           Monitor services are down
          </para>
         </listitem>
         <listitem>
          <para>
           Network connectivity issues
          </para>
         </listitem>
         <listitem>
          <para>
           Clock skew
          </para>
         </listitem>
        </itemizedlist>
       </entry>
       <entry>
        <para>
         Mitigation plan depends on which state the quorum is in. The message
         shows the monitors which are out of quorum.For the monitors which are
         not in quorum:
        </para>
        <itemizedlist xml:id="ul_dxf_4lt_2y">
         <listitem>
          <para>
           Check the status of monitor services.
          </para>
         </listitem>
         <listitem>
          <para>
           Check the network connectivity between the monitor nodes having
           monitors out of quorum and other monitor nodes.
          </para>
         </listitem>
         <listitem>
          <para>
           Check the logs of the monitors in their respective nodes.
          </para>
         </listitem>
         <listitem>
          <para>
           In case of clock skew, make sure all the nodes running the monitor
           services are in sync with ntp server.
          </para>
         </listitem>
        </itemizedlist>
       </entry>
      </row>
      <row>
       <entry>Degraded Ceph monitor connectivity alert</entry>
       <entry>Alarms if <emphasis role="bold">one or more</emphasis> of the Ceph Monitors are not
        reachable from the other Ceph nodes.</entry>
       <entry>
        <para>
         The likely cause can be one or more of the following conditions on
         <emphasis
          role="bold">one or more</emphasis> of your
         Ceph monitor nodes:
        </para>
        <itemizedlist xml:id="ul_h4s_qnt_2y">
         <listitem>
          <para>
           Network connectivity issue between the given node and the monitor
          </para>
         </listitem>
         <listitem>
          <para>
           Monitor service is down
          </para>
         </listitem>
         <listitem>
          <para>
           Firewall issues
          </para>
         </listitem>
        </itemizedlist>
        <para>
         in <emphasis role="bold">one or more</emphasis> monitor nodes
        </para>
       </entry>
       <entry>
        <para>
         The mitigation plan depends on multiple factors, like:
        </para>
        <orderedlist xml:id="ol_l31_rnt_2y">
         <listitem>
          <para>
           Only one node reports a monitor unreachable
          </para>
         </listitem>
         <listitem>
          <para>
           All nodes in the Ceph cluster report a specific monitor as unreachable
          </para>
         </listitem>
        </orderedlist>
        <para>
         Please observe the overall cluster health, as this might point to the
         cause of all Ceph nodes reporting a specific monitor node as
         unreachable. In this scenario, ensure that:
        </para>
        <orderedlist xml:id="ol_m31_rnt_2y">
         <listitem>
          <para>
           Monitor service on the (unreachable) node is up and running.
          </para>
         </listitem>
         <listitem>
          <para>
           Verify the firewall rules on the node to ensure that it is not
           blocking traffic to the Monitor service from all the Ceph nodes.
          </para>
         </listitem>
        </orderedlist>
        <para>
         We can be in this state because of one or combinations of following
         reasons:
        </para>
        <orderedlist xml:id="ol_n31_rnt_2y">
         <listitem>
          <para>
           Monitor node is down
          </para>
         </listitem>
         <listitem>
          <para>
           A specific host is not able to access specific set of monitor
          </para>
         </listitem>
        </orderedlist>
        <para>
         In case of first one, you might be seeing same reported status from
         other nodes as well. In this case, you are strongly advice to diagnose
         specific monitor to check various aspects like network connectivity,
         service status etc. For e.g. if
         <emphasis
          role="bold">levelDB</emphasis> is full then
         monitor service shuts itself down which you can identify by looking at
         log files. In case of second one, you might like to check whether
         network connectivity or firewall is fine between the respective node and
         monitor node. Prime reason of failure is expected to
         be<emphasis role="bold"> network connectivity</emphasis> only in this
         case.
        </para>
       </entry>
      </row>
      <row>
       <entry>Broken Ceph monitor connectivity alert</entry>
       <entry>Alarms if <emphasis role="bold">all</emphasis> of the Ceph Monitors are not
        reachable from the other Ceph nodes.</entry>
       <entry>
        <para>
         The likely cause can be one or more of the following conditions on
         <emphasis
          role="bold">all</emphasis> of your Ceph
         monitor nodes:
        </para>
        <itemizedlist xml:id="ul_owg_snt_2y">
         <listitem>
          <para>
           Network connectivity issue between the specified node and the monitor
          </para>
         </listitem>
         <listitem>
          <para>
           Monitor service is down
          </para>
         </listitem>
         <listitem>
          <para>
           Firewall issues
          </para>
         </listitem>
        </itemizedlist>
       </entry>
       <entry>
        <para>
         The mitigation plan depends on multiple factors, like:
        </para>
        <orderedlist xml:id="ol_ogp_snt_2y">
         <listitem>
          <para>
           Only one node reports a monitor unreachable
          </para>
         </listitem>
         <listitem>
          <para>
           All nodes in the Ceph cluster report a specific monitor as unreachable
          </para>
         </listitem>
        </orderedlist>
        <para>
         Please observe the overall cluster health, as this might point to the
         cause of all Ceph nodes reporting a specific monitor node as
         unreachable. In this scenario, ensure that:
        </para>
        <orderedlist xml:id="ol_pgp_snt_2y">
         <listitem>
          <para>
           Monitor service on the (unreachable) node is up and running.
          </para>
         </listitem>
         <listitem>
          <para>
           Verify the firewall rules on the node to ensure that it is not
           blocking traffic to the Monitor service from all the Ceph nodes.
          </para>
         </listitem>
        </orderedlist>
        <para>
         We can be in this state because of one or combinations of following
         reasons:
        </para>
        <orderedlist xml:id="ol_qgp_snt_2y">
         <listitem>
          <para>
           Monitor node is down
          </para>
         </listitem>
         <listitem>
          <para>
           A specific host is not able to access specific set of monitor
          </para>
         </listitem>
        </orderedlist>
        <para>
         In case of first one, you might be seeing same reported status from
         other nodes as well. In this case, you are strongly advice to diagnose
         specific monitor to check various aspects like network connectivity,
         service status etc. For e.g. if
         <emphasis
          role="bold">levelDB</emphasis> is full then
         monitor service shuts itself down which you can identify by looking at
         log files. In case of second one, you might like to check whether
         network connectivity or firewall is fine between the respective node and
         monitor node. Prime reason of failure is expected to
         be<emphasis role="bold"> network connectivity</emphasis> only in this
         case.
        </para>
       </entry>
      </row>
      <row>
       <entry>Recommended OSD memory alert</entry>
       <entry>
        <para>
         Alarms if the recommended memory configuration for OSD nodes is not met.
        </para>
        <para>
         The recommendation is <emphasis role="bold">1GB RAM</emphasis> per
         <emphasis
          role="bold">1TB of data disk</emphasis>.
         </para>
        </entry>
        <entry>The specified OSD node does not adhere to the minimum recommended memory-disk ratio
         of 1GB RAM per 1TB of data disk.</entry>
        <entry>
         <para>
          Having low RAM is not going to cause system failure and hence it does
          not warrant attention to make the cluster functional. However, note that
          falling short in RAM per host can cause signficant performance
          bottleneck and you might observe following:
         </para>
         <orderedlist xml:id="ol_wtd_5nt_2y">
          <listitem>
           <para>
            Throughput of cluster is low
           </para>
          </listitem>
          <listitem>
           <para>
            Timeout in high I/O traffic scenarios
           </para>
          </listitem>
          <listitem>
           <para>
            Recovering of failed OSD node or rebuiliding of data is taking longer
            duration
           </para>
          </listitem>
         </orderedlist>
         <para>
          If you see the alarm please allocate more RAM for the given host node.
         </para>
        </entry>
       </row>
       <row>
        <entry>Recommended OSD-Journal disk ratio alert</entry>
        <entry>
         <para>
          Alarms if the recommended ratio of OSD to journal disks is not adhered
          to.
         </para>
         <para>
          The recommendation is a maximum of<emphasis role="bold"> 4
           OSDs</emphasis> per journal disk.
         </para>
        </entry>
        <entry>The specified Ceph OSD node does not adhere to the minimum recommended ratio of 4
         OSDs per journal disk.</entry>
        <entry>
         <para>
          If you see the alarms then please check input model to figure out
          following:
         </para>
         <orderedlist xml:id="ol_urk_vnt_2y">
          <listitem>
           <para>
            No disk is left with explicit declaration to use journal disk
           </para>
          </listitem>
          <listitem>
           <para>
            Number of disks referring same journal disk is not exceeding
            <emphasis
             role="bold">4:1 ratio</emphasis>
            </para>
           </listitem>
          </orderedlist>
          <para>
           It might be required to re-define disk model for OSD nodes and
           reconfigure services using ceph playbooks.
          </para>
         </entry>
        </row>
        <row>
         <entry>Recommended Ceph Public network NIC speed</entry>
         <entry>
          <para>
           Alarms if recommended Ceph public network NIC speed is not met.
          </para>
          <para>
           Additionally if there are non Ceph networks detected on the same NIC the
           alarm is raised.
          </para>
          <para>
           The recommended NIC speeds are:
          </para>
          <para>
           <emphasis role="bold">10Gb/s</emphasis> for dedicated public network
           NICs,
          </para>
          <para>
           <emphasis role="bold">40Gb/s</emphasis> for shared public/private
           network NIC
          </para>
         </entry>
         <entry>
          <para>
           The node specified by the <literal>hostname</literal> dimension's Ceph
           public network NIC speed is less than the minimum recommended speed
          </para>
          <para>
           (or)
          </para>
          <para>
           Non Ceph networks are detected on the same NIC as the public network
           NIC.
          </para>
         </entry>
         <entry>
          <para>
           Having low NIC speeds is not going to cause system failure and hence it
           does not warrant attention to make cluster functional. However, note
           that falling short in NIC speeds can cause
           <emphasis role="bold">signficant
            performance</emphasis><emphasis
            role="bold">bottleneck</emphasis>
           and you might observe following:
          </para>
          <orderedlist xml:id="ol_oy5_wnt_2y">
           <listitem>
            <para>
             Throughput of cluster is low
            </para>
           </listitem>
           <listitem>
            <para>
             Timeout in high I/O traffic scenarios
            </para>
           </listitem>
          </orderedlist>
          <para>
           If you see the alarm please switch to a high speed network interface as
           per recommendations
          </para>
         </entry>
        </row>
        <row>
         <entry>Recommended Ceph Private network NIC speed</entry>
         <entry>
          <para>
           Alarms if recommended Ceph private network NIC speed is not met.
          </para>
          <para>
           Additionally if there are non ceph networks detected on the same NIC the
           alarm is raised.
          </para>
          <para>
           The recommended NIC speeds are:
          </para>
          <para>
           <emphasis role="bold">10Gb/s</emphasis> for dedicated private network
           NICs,
          </para>
          <para>
           <emphasis role="bold">40Gb/s</emphasis> for shared public/private
           network NIC
          </para>
         </entry>
         <entry>
          <para>
           The node specified by the <literal>hostname</literal> dimension's Ceph
           private network NIC speed is less than the minimum recommended speed.
          </para>
          <para>
           (or)
          </para>
          <para>
           Non Ceph networks are detected on the same NIC as the private network
           NIC.
          </para>
         </entry>
         <entry>
          <para>
           Having low NIC speeds is not going to cause system failure and hence it
           does not warrant attention to make cluster functional. However, note
           that falling short in NIC speeds can cause
           <emphasis role="bold">signficant</emphasis><emphasis role="bold"
                                                                >performance</emphasis><emphasis role="bold">bottleneck</emphasis>
           and you might observe following:
          </para>
          <orderedlist xml:id="ol_tcd_ynt_2y">
           <listitem>
            <para>
             Throughput of cluster is low
            </para>
           </listitem>
           <listitem>
            <para>
             Timeout in high I/O traffic scenarios
            </para>
           </listitem>
          </orderedlist>
          <para>
           If you see the alarm please switch to a high speed network interface as
           per recommendations
          </para>
         </entry>
        </row>
        <row>
         <entry>Cephlm probe check</entry>
         <entry>Alarms if the <emphasis role="bold">cephlm-probe</emphasis> tool cannot execute a
          monitoring task</entry>
         <entry>
          <para>
           The likely cause can be one or more of the following conditions on
           <emphasis
            role="bold">one or more</emphasis> of your
           Ceph nodes, specified by the <literal>hostname</literal> dimension:
          </para>
          <itemizedlist xml:id="ul_afg_b4t_2y">
           <listitem>
            <para>
             Ceph configuration errors
            </para>
           </listitem>
           <listitem>
            <para>
             Command timeouts
            </para>
           </listitem>
          </itemizedlist>
         </entry>
         <entry>
          <para>
           Depending on the alarm message it is recommended to:
          </para>
          <itemizedlist xml:id="ul_dxl_b4t_2y">
           <listitem>
            <para>
             Check for any errors in ceph configuration files
            </para>
           </listitem>
           <listitem>
            <para>
             Modify the command timeout (default 30 seconds) by editing the
             configurable parameters in ceph deployment playbooks and
             re-configuring ceph
            </para>
           </listitem>
           <listitem>
            <para>
             In case of ceph commands getting timed out repeatedly, please check if
             "<emphasis role="bold">ceph -s</emphasis>" command is working fine on
             the node. It is observed that when monitor nodes are down the command
             fails due to timeouts. In such cases, you will need to fix the monitor
             issues
            </para>
           </listitem>
          </itemizedlist>
         </entry>
        </row>
        <row>
         <entry>Cephlm monitor check</entry>
         <entry>Alarms if monasca cephlm check plugin fails to run or it cannot collect/process the
          generated metrics.</entry>
         <entry>
          <para>
           The likely cause can be one or more of the following conditions on
           <emphasis
            role="bold">one or more</emphasis> of your
           Ceph monitor nodes, specified by the <literal>hostname</literal>
           dimension:
          </para>
          <itemizedlist xml:id="ul_pcg_c4t_2y">
           <listitem>
            <para>
             Monasca collector failures
            </para>
           </listitem>
           <listitem>
            <para>
             Stale metrics
            </para>
           </listitem>
           <listitem>
            <para>
             Stuck cron jobs
            </para>
           </listitem>
           <listitem>
            <para>
             System time out of sync
            </para>
           </listitem>
          </itemizedlist>
         </entry>
         <entry>
          <para>
           Depending on the alarm message it is recommended to:
          </para>
          <itemizedlist xml:id="ul_wdm_c4t_2y">
           <listitem>
            <para>
             Check the <emphasis role="bold">/var/log/monasca/collector.log
              </emphasis>and look for any issues pertaining to ceph metrics
            </para>
           </listitem>
           <listitem>
            <para>
             Check the contents of
             <emphasis role="bold">/var/cache/cephlm</emphasis> on the node to see
             if the files are getting updated at regular intervals (should not have
             old files &gt; 6 minutes)
            </para>
           </listitem>
           <listitem>
            <para>
             Check <emphasis role="bold">/var/log/syslog</emphasis> for cephlm cron
             job errors
            </para>
           </listitem>
          </itemizedlist>
         </entry>
        </row>
        <row>
         <entry>Service Log Directory Size</entry>
         <entry>服务日志目录在使用超出配额的硬盘空间。</entry>
         <entry>这可能是因为服务被设置成 <literal>DEBUG</literal> 而不是 <literal>INFO</literal> 等级。另外可能的原因是一个重复的错误信息填满了日志文件。最后，这也可能是因为日志滚动没有被设置正确，因而旧的日志没有被正常删除。</entry>
         <entry>找到消耗过多硬盘空间的服务，查看日志。如果有 <literal>DEBUG</literal> 日志条目，请把日志等级设为 <literal>INFO</literal>。如果日志在重复记录一个错误信息，请解决该错误信息。如果旧的日志文件存在，配置日志滚动设置来自动移除它们。如果需要，您也可以选择备份后手动删除旧的日志。</entry>
        </row>
-->

</tbody>
      </tgroup>
     </informaltable>
    </section>
    
