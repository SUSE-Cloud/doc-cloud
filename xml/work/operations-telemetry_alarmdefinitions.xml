<?xml version="1.0"?>
<!DOCTYPE section [
<!ENTITY % entities SYSTEM "entities.ent"> %entities;
]>
<section xmlns="http://docbook.org/ns/docbook" xmlns:xi="http://www.w3.org/2001/XInclude"
         xmlns:xlink="http://www.w3.org/1999/xlink" xml:id="telemetry_alarmdefinitions">
 <title>遥测（Telemetry）警报</title>
 <para>
  以下警报在 &productname; &opscon; 的遥测（Telemetry）分类中出现。
 </para>
 <informaltable colsep="1" rowsep="1">
  <tgroup cols="5">
   <colspec colname="c1" colnum="1"/>
   <colspec colname="c2" colnum="2"/>
   <colspec colname="c3" colnum="3"/>
   <colspec colname="c4" colnum="4"/>
   <colspec colname="c5" colnum="5"/>
   <thead>
    <row>
     <entry>服务</entry>
     <entry>警报名称</entry>
     <entry>描述</entry>
     <entry>可能的原因</entry>
     <entry>故障排除步骤</entry>
    </row>
   </thead>
   <tbody>
    <row>
     <entry morerows="4">telemetry</entry>
     <entry>Process Check</entry>
     <entry>
      <para>
       <literal>ceilometer-agent-notification</literal> 进程未运行时的警报。
      </para>
     </entry>
     <entry>进程已崩溃。</entry>
     <entry>
      <para>
       查看警报主机上以下位置中的日志以了解原因：
      </para>
      <screen>/var/log/ceilometer/ceilometer-agent-notification-json.log</screen>
      <para>
       用以下步骤重启受影响的节点的进程：
      </para>
      <orderedlist>
       <listitem>
        <para>
         登录 &lcm;。
        </para>
       </listitem>
       <listitem>
        <para>
         对受影响的节点使用 Ceilometer 启动脚本：
        </para>
<screen>&prompt.ardana;cd ~/scratch/ansible/next/ardana/ansible
&prompt.ardana;ansible-playbook -i hosts/verb_hosts ceilometer-start.yml --limit &lt;hostname&gt;</screen>
       </listitem>
      </orderedlist>
     </entry>
    </row>
    <row>
     <entry>Process Check</entry>
     <entry>
      <para>
       <literal>ceilometer-polling</literal> 进程未运行时的警报。
      </para>
     </entry>
     <entry>进程已崩溃。</entry>
     <entry>
      <para>
       查看警报主机上以下位置中的日志以了解原因：
      </para>
      <screen>/var/log/ceilometer/ceilometer-polling-json.log</screen>
      <para>
       用以下步骤重启受影响的节点的进程：
      </para>
      <orderedlist>
       <listitem>
        <para>
         登录 &lcm;。
        </para>
       </listitem>
       <listitem>
        <para>
         对受影响的节点使用 Ceilometer 启动脚本：
        </para>
<screen>&prompt.ardana;cd ~/scratch/ansible/next/ardana/ansible
&prompt.ardana;ansible-playbook -i hosts/verb_hosts ceilometer-start.yml --limit &lt;hostname&gt;</screen>
       </listitem>
      </orderedlist>
     </entry>
    </row>
    <row>
     <entry>Process Check</entry>
     <entry><literal>ceilometer-api</literal> 进程未运行时的警报。</entry>
     <entry>进程已崩溃。</entry>
     <entry>
      <para>
       查看警报主机上以下位置中的日志以了解原因：
      </para>
      <screen>/var/log/ceilometer/ceilometer-api-json.log</screen>
      <para>
       用以下步骤重启受影响的节点的进程：
      </para>
      <orderedlist>
       <listitem>
        <para>
         登录 &lcm;。
        </para>
       </listitem>
       <listitem>
        <para>
         对受影响的节点使用 Ceilometer 启动脚本：
        </para>
<screen>&prompt.ardana;cd ~/scratch/ansible/next/ardana/ansible
&prompt.ardana;ansible-playbook -i hosts/verb_hosts ceilometer-start.yml --limit &lt;hostname&gt;</screen>
       </listitem>
      </orderedlist>
     </entry>
    </row>
    <row>
     <entry>HTTP Status</entry>
     <entry>
      <para>
       指定的 HTTP 端点关闭或无法访问时发出的警报。
      </para>
      <screen>endpoint_type=host_endpoint</screen>
     </entry>
     <entry><literal>hostname</literal> 中定义的主机上的 Ceilometer API 已关闭或无法访问。</entry>
     <entry>
      <para>
       使用以下步骤在受影响的节点上重新启动 Apache：
      </para>
      <orderedlist xml:id="ol_a53_gct_lx">
       <listitem>
        <para>
         登录 &lcm;。
        </para>
       </listitem>
       <listitem>
        <para>
         使用此脚本确认 Apache 的状态：
        </para>
<screen>&prompt.ardana;cd ~/scratch/ansible/next/ardana/ansible
&prompt.ardana;ansible-playbook -i hosts/verb_hosts FND-AP2-status.yml</screen>
       </listitem>
       <listitem>
        <para>
         如有必要，停止 Apache 服务：
        </para>
<screen>&prompt.ardana;cd ~/scratch/ansible/next/ardana/ansible
&prompt.ardana;ansible-playbook -i hosts/verb_hosts FND-AP2-stop.yml --limit &lt;hostname&gt;</screen>
       </listitem>
       <listitem>
        <para>
         对受影响的节点使用此脚本重新启动 Apache：
        </para>
<screen>&prompt.ardana;cd ~/scratch/ansible/next/ardana/ansible
&prompt.ardana;ansible-playbook -i hosts/verb_hosts FND-AP2-start.yml --limit &lt;hostname&gt;</screen>
       </listitem>
      </orderedlist>
     </entry>
    </row>
    <row>
     <entry>HTTP Status</entry>
     <entry>
      <para>
       指定的 HTTP 端点关闭或无法访问时发出的警报。
      </para>
      <screen>endpoint_type=internal</screen>
     </entry>
     <entry>内部虚拟 IP 地址上的 Ceilometer API 已关闭或无法访问。</entry>
     <entry>
      <para>
       如果在所有节点上出现 <literal>http_status</literal> 的错误，请使用以下步骤在所有控制器上重新启动 Apache：
      </para>
      <orderedlist>
       <listitem>
        <para>
         登录 &lcm;。
        </para>
       </listitem>
       <listitem>
        <para>
         使用此脚本确认 Apache 的状态：
        </para>
<screen>&prompt.ardana;cd ~/scratch/ansible/next/ardana/ansible
&prompt.ardana;ansible-playbook -i hosts/verb_hosts FND-AP2-status.yml</screen>
       </listitem>
       <listitem>
        <para>
         如有必要，停止 Apache 服务：
        </para>
<screen>&prompt.ardana;cd ~/scratch/ansible/next/ardana/ansible
&prompt.ardana;ansible-playbook -i hosts/verb_hosts FND-AP2-stop.yml --limit &lt;hostname&gt;</screen>
       </listitem>
       <listitem>
        <para>
         对控制器节点使用此脚本以重新启动 Apache：
        </para>
<screen>&prompt.ardana;cd ~/scratch/ansible/next/ardana/ansible
&prompt.ardana;ansible-playbook -i hosts/verb_hosts FND-AP2-start.yml --limit &lt;hostname&gt;</screen>
       </listitem>
      </orderedlist>
      <para>
       如果特定主机发生 <literal>http_status</literal> 错误并无遥测错误，那么应该是 haproxy 问题并且需要重新启动。
      </para>
      <orderedlist>
       <listitem>
        <para>
         登录 &lcm;。
        </para>
       </listitem>
       <listitem>
        <para>
         使用此脚本确认 haproxy 的状态：
        </para>
<screen>&prompt.ardana;cd ~/scratch/ansible/next/ardana/ansible
&prompt.ardana;ansible-playbook -i hosts/verb_hosts FND-CLU-status.yml --limit &lt;hostname&gt;</screen>
       </listitem>
       <listitem>
        <para>
         如有必要，停止 haproxy 服务：
        </para>
<screen>&prompt.ardana;cd ~/scratch/ansible/next/ardana/ansible
&prompt.ardana;ansible-playbook -i hosts/verb_hosts FND-CLU-stop.yml --limit &lt;hostname&gt;</screen>
       </listitem>
       <listitem>
        <para>
         使用此脚本以重新启动 haproxy：
        </para>
<screen>&prompt.ardana;cd ~/scratch/ansible/next/ardana/ansible
&prompt.ardana;ansible-playbook -i hosts/verb_hosts FND-CLU-start.yml --limit &lt;hostname&gt;</screen>
       </listitem>
      </orderedlist>
      <para>
       要进一步排除故障，请通过 SSH 连接到受影响的主机并查看以下 Ceilometer 访问日志：
      </para>
      <screen>/var/log/ceilometer/ceilometer_modwsgi.log
       /var/log/ceilometer/ceilometer-api.log</screen>
     </entry>
    </row>
    <row>
     <entry>metering</entry>
     <entry>Service Log Directory Size</entry>
     <entry>服务日志目录在使用超出配额的硬盘空间。</entry>
     <entry>这可能是因为服务被设置成 <literal>DEBUG</literal> 而不是 <literal>INFO</literal> 等级。另外可能的原因是一个重复的错误信息填满了日志文件。
      最后，这也可能是因为日志轮替没有被设置正确，因而旧的日志没有被正常删除。</entry>
     <entry>找到消耗过多硬盘空间的服务，查看日志。如果有 <literal>DEBUG</literal> 日志条目，请把日志等级设为 <literal>INFO</literal>。
      如果日志在重复记录一个错误信息，请解决该错误信息。如果旧的日志文件存在，配置日志轮替设置来自动移除它们。如果需要，您也可以选择备份后手动删除旧的日志。</entry>
    </row>
    <!---->
    <row>
     <entry morerows="3">kafka</entry>
     <entry>Kafka Persister Metric Consumer Lag</entry>
     <entry>当 Persister 使用者组无法跟上参数主题上的传入消息时发出的警报。</entry>
     <entry>系统运行缓慢或负载过重。</entry>
     <entry>
      <para>
       用以下步骤验证所有 monasca-persister 服务都在线：
      </para>
      <orderedlist xml:id="ol_z3p_2ff_wv">
       <listitem>
        <para>
         登录 &lcm;
        </para>
       </listitem>
       <listitem>
        <para>
         用此脚本验证所有 <literal>monasca-persister</literal> 服务都在线：
        </para>
<screen>cd ~/scratch/ansible/next/ardana/ansible
         ansible-playbook -i hosts/verb_hosts monasca-status.yml --tags monasca-persister</screen>
       </listitem>
      </orderedlist>
      <para>
       寻找各种系统的高负载。 此警报可以针对多个主题或多个主机触发。 哪些警报被触发可以帮助诊断可能的原因。 例如，如果警报在一台机器上发出警报，则它可能是机器。 如果跨多台机器的一个主题可能是该主题的使用者，等等。
      </para>
     </entry>
    </row>
    <row>
     <entry>Kafka Alarm Transition Consumer Lag</entry>
     <entry>当指定的使用者组跟不上警报状态转换主题上的传入消息时发出的警报。</entry>
     <entry>系统运行缓慢或负载过重。</entry>
     <entry>
      <para>
       用以下步骤验证 monasca-thresh 和 monasca-notification 服务都在线：
      </para>
      <para>
       寻找各种系统的高负载。 此警报可以针对多个主题或多个主机触发。 哪些警报被触发可以帮助诊断可能的原因。 例如：
      </para>
      <itemizedlist>
       <listitem>
        <para>
         如果所有报警都在同一台机器上，则机器可能出现故障。
        </para>
       </listitem>
       <listitem>
        <para>
         如果在多台计算机之间共享一个主题，则该主题的使用者可能出现故障。
        </para>
       </listitem>
      </itemizedlist>
     </entry>
    </row>
    <row>
     <entry>Kafka Kronos Consumer Lag</entry>
     <entry>当 Kronos 使用者组无法跟上参数主题上的传入消息时发出的警报。</entry>
     <entry>系统运行缓慢或负载过重。</entry>
     <entry>
      <para>
       寻找各种系统的高负载。 此警报可以针对多个主题或多个主机触发。 哪些警报被触发可以帮助诊断可能的原因。 例如：
      </para>
      <itemizedlist>
       <listitem>
        <para>
         如果所有报警都在同一台机器上，则机器可能出现故障。
        </para>
       </listitem>
       <listitem>
        <para>
         如果在多台计算机之间共享一个主题，则该主题的使用者可能出现故障。
        </para>
       </listitem>
      </itemizedlist>
     </entry>
    </row>
    <row>
     <entry>Process Check</entry>
     <entry>
      <para>
       特定进程未在运行时发出的警报。
      </para>
      <screen>process_name = kafka.Kafka</screen>
     </entry>
     <entry/>
     <entry>
      <para>
       用以下步骤重启受影响的节点的进程：
      </para>
      <orderedlist xml:id="ol_lrq_lzp_mx">
       <listitem>
        <para>
         登录 &lcm;。
        </para>
       </listitem>
       <listitem>
        <para>
         使用此脚本停止 kafka 服务：
        </para>
<screen>cd ~/scratch/ansible/next/ardana/ansible
         ansible-playbook -i hosts/verb_hosts monasca-start.yml --tags kafka</screen>
       </listitem>
       <listitem>
        <para>
         使用此脚本重启 kafka 服务：
        </para>
<screen>cd ~/scratch/ansible/next/ardana/ansible
         ansible-playbook -i hosts/verb_hosts monasca-status.yml --tags kafka</screen>
       </listitem>
      </orderedlist>
      <para>
       查看此位置的日志：
      </para>
      <screen>/var/log/kafka/server.log</screen>
     </entry>
    </row>
    <!---->
    <row>
     <entry morerows="7">logging</entry>
     <entry>Beaver Memory Usage</entry>
     <entry>Beaver 使用的内存比预期多。 这可能表明它无法转发消息并且其队列正在填满。 如果您持续看到此信息，请参阅故障排除指南。</entry>
     <entry>系统过载或服务内存泄漏。</entry>
     <entry>登录到报告的主机以调查高内存用户。</entry>
    </row>
    <row>
     <entry>Audit Log Partition Low Watermark</entry>
     <entry><literal>/var/audit</literal> 硬盘空间使用量已越过低水位线。如果达到高水位线，将运行 logrotate 以释放硬盘空间。如果需要，调整 <literal>var_audit_low_watermark_percent</literal>。</entry>
     <entry>这可能是因为服务被设置成 <literal>DEBUG</literal> 而不是 <literal>INFO</literal> 等级。另外可能的原因是一个重复的错误信息填满了日志文件。最后，这也可能是因为日志轮替没有被设置正确，因而旧的日志没有被正常删除。</entry>
     <entry>找到消耗过多硬盘空间的服务，查看日志。如果有 <literal>DEBUG</literal> 日志条目，请把日志等级设为 <literal>INFO</literal>。如果日志在重复记录一个错误信息，请解决该错误信息。如果旧的日志文件存在，配置日志轮替设置来自动移除它们。如果需要，您也可以选择备份后手动删除旧的日志。</entry>
    </row>
    <row>
     <entry>Audit Log Partition High Watermark</entry>
     <entry><literal>/var/audit</literal> 硬盘空间不足。将运行 logrotate 以释放硬盘空间。如果需要，调整 <literal>var_audit_high_watermark_percent</literal>。</entry>
     <entry>这可能是因为服务被设置成 <literal>DEBUG</literal> 而不是 <literal>INFO</literal> 等级。另外可能的原因是一个重复的错误信息填满了日志文件。最后，这也可能是因为日志轮替没有被设置正确，因而旧的日志没有被正常删除。</entry>
     <entry>找到消耗过多硬盘空间的服务，查看日志。如果有 <literal>DEBUG</literal> 日志条目，请把日志等级设为 <literal>INFO</literal>。如果日志在重复记录一个错误信息，请解决该错误信息。如果旧的日志文件存在，配置日志轮替设置来自动移除它们。如果需要，您也可以选择备份后手动删除旧的日志。</entry>
    </row>
    <row>
     <entry>&elasticsearch; Unassigned Shards</entry>
     <entry>
      <screen>component = elasticsearch</screen>
      <para>
       &elasticsearch; 未分配的分片数大于0。
      </para>
     </entry>
     <entry>环境可能配置错误。</entry>
     <entry>
      <para>
       要查找未分配的分片，请从 <literal>~/scratch/ansible/next/ardana/ansible</literal> 目录在 &lcm; 上运行以下命令：
      </para>
<screen>cd ~/scratch/ansible/next/ardana/ansible
       ansible -i hosts/verb_hosts LOG-SVR[0] -m shell -a "curl localhost:9200/_cat/shards?pretty -s" | grep UNASSIGNED</screen>
      <para>
       这应该显示哪些分片未分配，如下所示：
      </para>
      <screen>logstash-2015.10.21 4 p UNASSIGNED 11412371  3.2gb 10.241.67.11 Keith Kilham</screen>
      <para>
       最后一列显示 &elasticsearch; 使用的未分配的分片所在节点的名称。 要查找实际主机名，请运行：
      </para>
<screen>cd ~/scratch/ansible/next/ardana/ansible
       ansible -i hosts/verb_hosts LOG-SVR[0] -m shell -a "curl localhost:9200/_nodes/_all/name?pretty -s"</screen>
      <para>
       找到主机名后，您可以尝试以下操作：
      </para>
      <orderedlist xml:id="ol_bcg_3zp_mx">
       <listitem>
        <para>
         确保节点没有磁盘空间不足，并在需要时释放空间。
        </para>
       </listitem>
       <listitem>
        <para>
         重启节点（请谨慎使用，因为这可能会影响其他服务）。
        </para>
       </listitem>
       <listitem>
        <para>
         用以下指令检查以确保所有版本的 &elasticsearch; 相同：
        </para>
<screen>cd ~/scratch/ansible/next/ardana/ansible
         ansible -i hosts/verb_hosts LOG-SVR -m shell -a "curl localhost:9200/_nodes/_local/name?pretty -s" | grep version</screen>
       </listitem>
       <listitem>
        <para>
         联系客户支持。
        </para>
       </listitem>
      </orderedlist>
     </entry>
    </row>
    <row>
     <entry>&elasticsearch; Number of Log Entries</entry>
     <entry>
      <screen>component = elasticsearch</screen>
      <para>
       &elasticsearch; 日志条目数。
      </para>
     </entry>
     <entry>日志条目的数量太多。</entry>
     <entry>如果日志条目的数量太大（例如，大于 40,000），则旧版本的Kibana（版本 3 及更早版本）可能会挂起，并且页面大小需要足够小（大约 20,000 个结果），因为如果它更大 （例如，200,000），它可能会挂起浏览器，但 Kibana 4 不应该有这个问题。</entry>
    </row>
    <row>
     <entry>&elasticsearch; Field Data Evictions</entry>
     <entry>
      <screen>component = elasticsearch</screen>
      <para>
       &elasticsearch; Field Data Eviction 数大于0。
      </para>
     </entry>
     <entry>有可能有 Field Data Evictions，即使它远未达到设置的限制。</entry>
     <entry>
      <para>
       <literal>elasticsearch_indices_fielddata_cache_size</literal> 默认设置为 <literal>unbounded</literal>。 如果用户设置的此值不足，则可能需要增加此配置参数或将其设置为无限制并使用以下步骤运行重新配置：
      </para>
      <orderedlist xml:id="ol_ccg_3zp_mx">
       <listitem>
        <para>
         登录 &lcm;。
        </para>
       </listitem>
       <listitem>
        <para>
         编辑下面的配置文件，并将 <literal>elasticsearch_indices_fielddata_cache_size</literal> 的值更改为所需的值：
        </para>
        <screen>~/openstack/my_cloud/config/logging/main.yml</screen>
       </listitem>
       <listitem>
        <para>
         将更改提交给 git：
        </para>
        <screen>git add -A
git commit -a -m "changing &elasticsearch; fielddata cache size"</screen>
       </listitem>
       <listitem>
        <para>
         运行配置处理器：
        </para>
        <screen>cd ~/openstack/ardana/ansible
         ansible-playbook -i hosts/localhost config-processor-run.yml</screen>
       </listitem>
       <listitem>
        <para>
         更新部署目录：
        </para>
        <screen>cd ~/openstack/ardana/ansible
         ansible-playbook -i hosts/localhost ready-deployment.yml</screen>
       </listitem>
       <listitem>
        <para>
         运行 Logging 重配置脚本以部署更改：
        </para>
<screen>cd ~/scratch/ansible/next/ardana/ansible/
         ansible-playbook -i hosts/verb_hosts kronos-reconfigure.yml</screen>
       </listitem>
      </orderedlist>
     </entry>
    </row>
    <row>
     <entry>Service Log Directory Size</entry>
     <entry>
      <para>
       服务日志目录在使用超出配额的硬盘空间。
      </para>
     </entry>
     <entry>这可能是因为服务被设置成 <literal>DEBUG</literal> 而不是 <literal>INFO</literal> 等级。另外可能的原因是一个重复的错误信息填满了日志文件。最后，这也可能是因为日志轮替没有被设置正确，因而旧的日志没有被正常删除。</entry>
     <entry>找到消耗过多硬盘空间的服务，查看日志。如果有 <literal>DEBUG</literal> 日志条目，请把日志等级设为 <literal>INFO</literal>。如果日志在重复记录一个错误信息，请解决该错误信息。如果旧的日志文件存在，配置日志轮替设置来自动移除它们。如果需要，您也可以选择备份后手动删除旧的日志。</entry>
    </row>
    <row>
     <entry>Process Check</entry>
     <entry>
      <para>
       由 <literal>component</literal> 维度指定的不同 logging 服务警报：
      </para>
      <itemizedlist xml:id="ul_dcg_3zp_mx">
       <listitem>
        <para>
         elasticsearch
        </para>
       </listitem>
       <listitem>
        <para>
         logstash
        </para>
       </listitem>
       <listitem>
        <para>
         beaver
        </para>
       </listitem>
       <listitem>
        <para>
         apache2
        </para>
       </listitem>
       <listitem>
        <para>
         kibana
        </para>
       </listitem>
      </itemizedlist>
     </entry>
     <entry>进程已崩溃。</entry>
     <entry>进程已崩溃。</entry>
     <entry>
      <para>
       在受影响的节点上，尝试重新启动该进程。
      </para>
      <para>
       如果 <command>elasticsearch</command> 进程已崩溃，使用：
      </para>
      <screen>sudo systemctl restart elasticsearch</screen>
      <para>
       如果 logstash 进程已崩溃，使用：
      </para>
      <screen>sudo systemctl restart logstash</screen>
      <para>
       可以使用类似的命令重新启动其余进程，如下所示：
      </para>
      <screen>sudo systemctl restart beaver
       sudo systemctl restart apache2
       sudo systemctl restart kibana</screen>
     </entry>
    </row>
    <!---->
    <row>
     <entry morerows="2">monasca-transform</entry>
     <entry>Process Check</entry>
     <entry>
      <screen>process_name = pyspark</screen>
     </entry>
     <entry>服务进程已崩溃。</entry>
     <entry>
      <para>
       在受影响的节点上重启进程，查看日志。
      </para>
      <para>
       一旦 <literal>monasca-transform</literal> 进程开始处理流就会创建的 <literal>spark-worker</literal> 的子进程。 如果进程与 pyspark 进程一起仅在一个节点上失败，那么 <literal>spark-worker</literal> 很可能无法连接到当选领导者的 <literal>spark-master</literal> 服务。 在这种情况下，应该在受影响的节点上启动 <literal>spark-worker</literal> 服务。 如果在多个节点上发生问题，检查 <literal>spark-worker</literal>，<literal>spark-master</literal> 和 <literal>monasca-transform</literal> 服务和日志。 如果 <literal>monasca-transform</literal> 或 <literal>spark</literal> 服务被中断，则此进程可能在十分钟内不会运行（流处理间隔）。
      </para>
     </entry>
    </row>
    <row>
     <entry>Process Check</entry>
     <entry>
      <screen>process_name = org.apache.spark.executor.CoarseGrainedExecutorBackend</screen>
     </entry>
     <entry>服务进程已崩溃。</entry>
     <entry>
      <para>
       在受影响的节点上重启进程，查看日志。
      </para>
      <para>
       一旦 <literal>monasca-transform</literal> 进程开始处理流就会创建的 <literal>spark-worker</literal> 的子进程。 如果进程与 pyspark 进程一起仅在一个节点上失败，那么 <literal>spark-worker</literal> 很可能无法连接到当选领导者的 <literal>spark-master</literal> 服务。 在这种情况下，应该在受影响的节点上启动 <literal>spark-worker</literal> 服务。 如果在多个节点上发生问题，检查 <literal>spark-worker</literal>，<literal>spark-master</literal> 和 <literal>monasca-transform</literal> 服务和日志。 如果 <literal>monasca-transform</literal> 或 <literal>spark</literal> 服务被中断，则此进程可能在十分钟内不会运行（流处理间隔）。
      </para>
     </entry>
    </row>
    <row>
     <entry>Process Check</entry>
     <entry>
      <screen>process_name = monasca-transform</screen>
     </entry>
     <entry>服务进程已崩溃。</entry>
     <entry>
      <para>
       在受影响的节点上重启服务，查看日志。
      </para>
     </entry>
    </row>
    <!---->
    <row>
     <entry morerows="12">monitoring</entry>
     <entry>HTTP Status</entry>
     <entry>
      <screen>component = monasca-persister</screen>
      <para>
       Persister Health Check
      </para>
     </entry>
     <entry>该进程已崩溃或依赖已消失。</entry>
     <entry>
      <para>
       如果进程崩溃，请使用以下步骤重新启动它。 如果依赖服务已关闭，请解决该问题。
      </para>
      <para>
       用以下步骤重启受影响的节点的进程：
      </para>
      <orderedlist xml:id="ol_ecg_3zp_mx">
       <listitem>
        <para>
         登录 &lcm;。
        </para>
       </listitem>
       <listitem>
        <para>
         使用此脚本检查 <literal>monasca-api</literal> 是否在所有节点上运行： with
         this playbook:
        </para>
<screen>cd ~/scratch/ansible/next/ardana/ansible
         ansible-playbook -i hosts/verb_hosts monasca-status.yml --tags monasca-persister</screen>
       </listitem>
       <listitem>
        <para>
         对受影响的节点使用 Monasca 启动脚本重启它：
         it:
        </para>
<screen>cd ~/scratch/ansible/next/ardana/ansible
         ansible-playbook -i hosts/verb_hosts monasca-start.yml --tags persister</screen>
       </listitem>
       <listitem>
        <para>
         使用此脚本验证它是否在所有节点上运行：
        </para>
<screen>cd ~/scratch/ansible/next/ardana/ansible
         ansible-playbook -i hosts/verb_hosts monasca-status.yml --tags monasca-persister</screen>
       </listitem>
      </orderedlist>
      <para>
       检查相关日志。
      </para>
     </entry>
    </row>
    <row>
     <entry>HTTP Status</entry>
     <entry>
      <screen>component = monasca-api</screen>
      <para>
       API 健康检查
      </para>
     </entry>
     <entry>该进程已崩溃或依赖已消失。</entry>
     <entry>
      <para>
       如果进程崩溃，请使用以下步骤重新启动它。 如果依赖服务已关闭，请解决该问题。
      </para>
      <para>
       用以下步骤重启受影响的节点的进程：
      </para>
      <orderedlist>
       <listitem>
        <para>
         登录 &lcm;。
        </para>
       </listitem>
       <listitem>
        <para>
         使用此脚本检查 <literal>monasca-api</literal> 是否在所有节点上运行：
        </para>
<screen>cd ~/scratch/ansible/next/ardana/ansible
         ansible-playbook -i hosts/verb_hosts monasca-status.yml --tags monasca-api</screen>
       </listitem>
       <listitem>
        <para>
         对受影响的节点使用 Monasca 启动脚本重启它：
        </para>
<screen>cd ~/scratch/ansible/next/ardana/ansible
         ansible-playbook -i hosts/verb_hosts monasca-start.yml --tags monasca-api</screen>
       </listitem>
       <listitem>
        <para>
         使用此脚本验证它是否在所有节点上运行：
        </para>
<screen>cd ~/scratch/ansible/next/ardana/ansible
         ansible-playbook -i hosts/verb_hosts monasca-status.yml --tags monasca-api</screen>
       </listitem>
      </orderedlist>
      <para>
       检查相关日志。
      </para>
     </entry>
    </row>
    <row>
     <entry>Monasca Agent Collection Time</entry>
     <entry>Alarms when the elapsed time the <literal>monasca-agent</literal> takes to collect
      metrics is high.</entry>
     <entry>机器负载重或卡住的 agent 插件。</entry>
     <entry>
      <para>
       解决机器上的负载问题。 如果需要，请使用以下步骤重新启动 agent：
      </para>
      <para>
       使用以下步骤在受影响的节点上重新启动 agent：
      </para>
      <orderedlist xml:id="ol_gcg_3zp_mx">
       <listitem>
        <para>
         登录 &lcm;。
        </para>
       </listitem>
       <listitem>
        <para>
         使用此脚本检查 <literal>monasca-agent</literal> 是否在所有节点上运行：
        </para>
<screen>cd ~/scratch/ansible/next/ardana/ansible
         ansible-playbook -i hosts/verb_hosts monasca-agent-status.yml</screen>
       </listitem>
       <listitem>
        <para>
         对受影响的节点使用 Monasca 启动脚本重启它：
        </para>
<screen>cd ~/scratch/ansible/next/ardana/ansible
         ansible-playbook -i hosts/verb_hosts monasca-start.yml --limit &lt;hostname&gt;</screen>
       </listitem>
       <listitem>
        <para>
         使用此脚本验证它是否在所有节点上运行：
        </para>
<screen>cd ~/scratch/ansible/next/ardana/ansible
         ansible-playbook -i hosts/verb_hosts monasca-agent-status.yml</screen>
       </listitem>
      </orderedlist>
      <para>
       检查相关日志。
      </para>
     </entry>
    </row>
    <row>
     <entry>Process Check</entry>
     <entry>
      <para>
       特定进程未在运行时发出的警报。
      </para>
      <screen>component = kafka</screen>
     </entry>
     <entry>进程已崩溃。</entry>
     <entry>
      <para>
       用以下步骤重启受影响的节点的进程：
      </para>
      <orderedlist>
       <listitem>
        <para>
         登录 &lcm;。
        </para>
       </listitem>
       <listitem>
        <para>
         使用此脚本检查 Kafka 是否在所有节点上运行：
        </para>
<screen>cd ~/scratch/ansible/next/ardana/ansible
         ansible-playbook -i hosts/verb_hosts monasca-status.yml --tags kafka</screen>
       </listitem>
       <listitem>
        <para>
         对受影响的节点使用 Monasca 启动脚本来重启它：
        </para>
<screen>cd ~/scratch/ansible/next/ardana/ansible
         ansible-playbook -i hosts/verb_hosts monasca-start.yml --tags kafka</screen>
       </listitem>
       <listitem>
        <para>
         使用此脚本验证 Kafka 是否在所有节点上运行：
        </para>
<screen>cd ~/scratch/ansible/next/ardana/ansible
         ansible-playbook -i hosts/verb_hosts monasca-status.yml --tags kafka</screen>
       </listitem>
      </orderedlist>
      <para>
       检查相关日志。
      </para>
     </entry>
    </row>
    <row>
     <entry>Process Check</entry>
     <entry>
      <para>
       特定进程未在运行时发出的警报。
      </para>
      <screen>process_name = monasca-notification</screen>
     </entry>
     <entry>进程已崩溃。</entry>
     <entry>
      <para>
       用以下步骤重启受影响的节点的进程：
      </para>
      <orderedlist>
       <listitem>
        <para>
         登录 &lcm;。
        </para>
       </listitem>
       <listitem>
        <para>
         使用此脚本检查 <literal>monasca-api</literal> 是否在所有节点上运行： with
         this playbook:
        </para>
<screen>cd ~/scratch/ansible/next/ardana/ansible
         ansible-playbook -i hosts/verb_hosts monasca-status.yml --tags notification</screen>
       </listitem>
       <listitem>
        <para>
         对受影响的节点使用 Monasca 启动脚本重启它：
         it:
        </para>
<screen>cd ~/scratch/ansible/next/ardana/ansible
         ansible-playbook -i hosts/verb_hosts monasca-start.yml --tags notification</screen>
       </listitem>
       <listitem>
        <para>
         使用此脚本验证它是否在所有节点上运行：
        </para>
<screen>cd ~/scratch/ansible/next/ardana/ansible
         ansible-playbook -i hosts/verb_hosts monasca-status.yml --tags notification</screen>
       </listitem>
      </orderedlist>
      <para>
       检查相关日志。
      </para>
     </entry>
    </row>
    <row>
     <entry>Process Check</entry>
     <entry>
      <para>
       特定进程未在运行时发出的警报。
      </para>
      <screen>process_name = monasca-agent</screen>
     </entry>
     <entry>进程已崩溃。</entry>
     <entry>
      <para>
       使用以下步骤在受影响的节点上重新启动 agent：
      </para>
      <orderedlist>
       <listitem>
        <para>
         登录 &lcm;。
        </para>
       </listitem>
       <listitem>
        <para>
         使用此脚本检查 <literal>monasca-agent</literal> 是否在所有节点上运行： with
         this playbook:
        </para>
<screen>cd ~/scratch/ansible/next/ardana/ansible
         ansible-playbook -i hosts/verb_hosts monasca-agent-status.yml</screen>
       </listitem>
       <listitem>
        <para>
         对受影响的节点使用 Monasca 启动脚本重启它：
         it:
        </para>
<screen>cd ~/scratch/ansible/next/ardana/ansible
         ansible-playbook -i hosts/verb_hosts monasca-start.yml --limit &lt;hostname&gt;</screen>
       </listitem>
       <listitem>
        <para>
         使用此脚本验证它是否在所有节点上运行：
        </para>
<screen>cd ~/scratch/ansible/next/ardana/ansible
         ansible-playbook -i hosts/verb_hosts monasca-agent-status.yml</screen>
       </listitem>
      </orderedlist>
      <para>
       检查相关日志。
      </para>
     </entry>
    </row>
    <row>
     <entry>Process Check</entry>
     <entry>
      <para>
       特定进程未在运行时发出的警报。
      </para>
      <screen>process_name = monasca-api</screen>
     </entry>
     <entry>进程已崩溃。</entry>
     <entry>
      <para>
       > 用以下步骤重启受影响的节点的进程：
       </para>
       <orderedlist>
        <listitem>
         <para>
          登录 &lcm;。
         </para>
        </listitem>
        <listitem>
         <para>
          使用此脚本检查 <literal>monasca-api</literal> 是否在所有节点上运行： with
          this playbook:
         </para>
<screen>cd ~/scratch/ansible/next/ardana/ansible
          ansible-playbook -i hosts/verb_hosts monasca-status.yml --tags monasca-api</screen>
        </listitem>
        <listitem>
         <para>
          对受影响的节点使用 Monasca 启动脚本重启它：
          it:
         </para>
<screen>cd ~/scratch/ansible/next/ardana/ansible
          ansible-playbook -i hosts/verb_hosts monasca-start.yml --tags monasca-api</screen>
        </listitem>
        <listitem>
         <para>
          使用此脚本验证它是否在所有节点上运行：
         </para>
<screen>cd ~/scratch/ansible/next/ardana/ansible
          ansible-playbook -i hosts/verb_hosts monasca-status.yml --tags monasca-api</screen>
        </listitem>
       </orderedlist>
       <para>
        检查相关日志。
       </para>
      </entry>
     </row>
     <row>
      <entry>Process Check</entry>
      <entry>
       <para>
        特定进程未在运行时发出的警报。
       </para>
       <screen>process_name = monasca-persister</screen>
      </entry>
      <entry>进程已崩溃。</entry>
      <entry>
       <para>
        用以下步骤重启受影响的节点的进程：
       </para>
       <orderedlist>
        <listitem>
         <para>
          登录 &lcm;。
         </para>
        </listitem>
        <listitem>
         <para>
          使用此脚本检查 <literal>monasca-api</literal> 是否在所有节点上运行： with
          this playbook:
         </para>
<screen>cd ~/scratch/ansible/next/ardana/ansible
          ansible-playbook -i hosts/verb_hosts monasca-status.yml --tags monasca-persister</screen>
        </listitem>
        <listitem>
         <para>
          对受影响的节点使用 Monasca 启动脚本重启它：
          it:
         </para>
<screen>cd ~/scratch/ansible/next/ardana/ansible
          ansible-playbook -i hosts/verb_hosts monasca-start.yml --tags persister</screen>
        </listitem>
        <listitem>
         <para>
          使用此脚本验证它是否在所有节点上运行：
         </para>
<screen>cd ~/scratch/ansible/next/ardana/ansible
          ansible-playbook -i hosts/verb_hosts monasca-status.yml --tags monasca-persister</screen>
        </listitem>
       </orderedlist>
       <para>
        检查相关日志。
       </para>
      </entry>
     </row>
     <row>
      <entry>Process Check</entry>
      <entry>
       <para>
        特定进程未在运行时发出的警报。
       </para>
       <screen>process_name = backtype.storm.daemon.nimbus
        component = apache-storm</screen>
      </entry>
      <entry>进程已崩溃。</entry>
      <entry>
       <para>
        查看所有 storm 主机上 <literal>/var/log/storm</literal> 目录中的日志以查找根本原因。
       </para>
      <note>
       <para>
        包含阈值引擎日志记录的日志位于第 2 和第 3 个控制器节点上。
       </para>
      </note>
       <para>
        使用以下步骤重新启动 monasca-thresh, if necessary,：
       </para>
       <orderedlist>
        <listitem>
         <para>
          登录 &lcm;。
         </para>
        </listitem>
        <listitem>
         <para>
          使用此脚本检查 <literal>monasca-thresh</literal> 是否在所有节点上运行：
         </para>
<screen>cd ~/scratch/ansible/next/ardana/ansible
          ansible-playbook -i hosts/verb_hosts monasca-status.yml --tags thresh</screen>
        </listitem>
        <listitem>
         <para>
          对受影响的节点使用 Monasca 启动脚本重启它：
          it:
         </para>
<screen>cd ~/scratch/ansible/next/ardana/ansible
          ansible-playbook -i hosts/verb_hosts monasca-start.yml --tags thresh</screen>
        </listitem>
        <listitem>
         <para>
          使用此脚本验证它是否在所有节点上运行：
         </para>
<screen>cd ~/scratch/ansible/next/ardana/ansible
          ansible-playbook -i hosts/verb_hosts monasca-status.yml --tags thresh</screen>
        </listitem>
       </orderedlist>
      </entry>
     </row>
     <row>
      <entry>Process Check</entry>
      <entry>
       <para>
        特定进程未在运行时发出的警报。
       </para>
       <screen>process_name = backtype.storm.daemon.supervisor
        component = apache-storm</screen>
      </entry>
      <entry>进程已崩溃。</entry>
      <entry>
       <para>
        查看所有 storm 主机上 <literal>/var/log/storm</literal> 目录中的日志以查找根本原因。
       </para>
      <note>
       <para>
        包含阈值引擎日志记录的日志位于第 2 和第 3 个控制器节点上。
       </para>
      </note>
       <para>
        使用以下步骤重新启动 monasca-thresh：
       </para>
       <orderedlist>
        <listitem>
         <para>
          登录 &lcm;。
         </para>
        </listitem>
        <listitem>
         <para>
          停止 monasca-thresh 服务：
         </para>
<screen>cd ~/scratch/ansible/next/ardana/ansible
          ansible-playbook -i hosts/verb_hosts monasca-stop.yml --tags thresh</screen>
        </listitem>
        <listitem>
         <para>
          重新启动 monasca-thresh 服务：
         </para>
<screen>cd ~/scratch/ansible/next/ardana/ansible
          ansible-playbook -i hosts/verb_hosts monasca-start.yml --tags thresh</screen>
        </listitem>
        <listitem>
         <para>
          使用此脚本验证它是否在所有节点上运行：
         </para>
<screen>cd ~/scratch/ansible/next/ardana/ansible
          ansible-playbook -i hosts/verb_hosts monasca-status.yml --tags thresh</screen>
        </listitem>
       </orderedlist>
      </entry>
     </row>
     <row>
      <entry>Process Check</entry>
      <entry>
       <para>
        特定进程未在运行时发出的警报。
       </para>
       <screen>process_name = backtype.storm.daemon.worker
        component = apache-storm</screen>
      </entry>
      <entry>进程已崩溃。</entry>
      <entry>
       <para>
        查看所有 storm 主机上 <literal>/var/log/storm</literal> 目录中的日志以查找根本原因。
       </para>
      <note>
       <para>
        包含阈值引擎日志记录的日志位于第 2 和第 3 个控制器节点上。
       </para>
      </note>
       <para>
        使用以下步骤重新启动 monasca-thresh：
       </para>
       <orderedlist>
        <listitem>
         <para>
          登录 &lcm;。
         </para>
        </listitem>
        <listitem>
         <para>
          停止 monasca-thresh 服务：
         </para>
<screen>cd ~/scratch/ansible/next/ardana/ansible
          ansible-playbook -i hosts/verb_hosts monasca-stop.yml --tags thresh</screen>
        </listitem>
        <listitem>
         <para>
          重新启动 monasca-thresh 服务：
         </para>
<screen>cd ~/scratch/ansible/next/ardana/ansible
          ansible-playbook -i hosts/verb_hosts monasca-start.yml --tags thresh</screen>
        </listitem>
        <listitem>
         <para>
          使用此脚本验证它是否在所有节点上运行：
         </para>
<screen>cd ~/scratch/ansible/next/ardana/ansible
          ansible-playbook -i hosts/verb_hosts monasca-status.yml --tags thresh</screen>
        </listitem>
       </orderedlist>
      </entry>
     </row>
     <row>
      <entry>Process Check</entry>
      <entry>
       <para>
        特定进程未在运行时发出的警报。
       </para>
       <screen>process_name = monasca-thresh
        component = apache-storm</screen>
      </entry>
      <entry>进程已崩溃。</entry>
      <entry>
       <para>
        用以下步骤重启受影响的节点的进程：
       </para>
       <orderedlist>
        <listitem>
         <para>
          登录 &lcm;。
         </para>
        </listitem>
        <listitem>
         <para>
          使用此脚本检查 <literal>monasca-thresh</literal> 是否在所有节点上运行：
         </para>
<screen>cd ~/scratch/ansible/next/ardana/ansible
          ansible-playbook -i hosts/verb_hosts monasca-status.yml --tags thresh</screen>
        </listitem>
        <listitem>
         <para>
          对受影响的节点使用 Monasca 启动脚本来重启它：
         </para>
<screen>cd ~/scratch/ansible/next/ardana/ansible
          ansible-playbook -i hosts/verb_hosts monasca-start.yml --tags thresh</screen>
        </listitem>
        <listitem>
         <para>
          使用此脚本验证它是否在所有节点上运行：
         </para>
<screen>cd ~/scratch/ansible/next/ardana/ansible
          ansible-playbook -i hosts/verb_hosts monasca-status.yml --tags thresh</screen>
        </listitem>
       </orderedlist>
       <para>
        检查相关日志。
       </para>
      </entry>
     </row>
     <row>
      <entry>Service Log Directory Size</entry>
      <entry>服务日志目录在使用超出配额的硬盘空间。</entry>
      <entry>由 <literal>path</literal> 维度指示的服务日志目录超过 2.5 GB 配额。</entry>
      <entry>找到消耗过多硬盘空间的服务，查看日志。如果有 <literal>DEBUG</literal> 日志条目，请把日志等级设为 <literal>INFO</literal>。如果日志在重复记录一个错误信息，请解决该错误信息。如果旧的日志文件存在，配置日志轮替设置来自动移除它们。如果需要，您也可以选择备份后手动删除旧的日志。</entry>
     </row>
       </tbody>
      </tgroup>
     </informaltable>
    </section>
    
