<?xml version="1.0"?>
<!DOCTYPE section [
 <!ENTITY % entities SYSTEM "entity-decl.ent"> %entities;
]>
<section xmlns="http://docbook.org/ns/docbook" xmlns:xi="http://www.w3.org/2001/XInclude"
        xmlns:xlink="http://www.w3.org/1999/xlink" xml:id="troubleshooting_blockstorage">
 <title>Block Storage Troubleshooting</title>
 <para>
  The block storage service utilizes &ostack; &o_blockstore; and can integrate with
  multiple back-ends including 3Par, &ses;, and &ceph;. Failures may exist at the &o_blockstore; API level,
  an operation may fail, or you may see an alarm trigger in the monitoring
  service. These may be caused by configuration problems, network issues, or
  issues with your servers or storage back-ends. The purpose of this page and
  section is to describe how the service works, where to find additional
  information, some of the common problems that come up, and how to address
  them.
 </para>
 <section xml:id="logs">
  <title>Where to find information</title>
  <para>
   When debugging block storage issues it is helpful to understand the
   deployment topology and know where to locate the logs with additional
   information.
  </para>
  <para>
   The &o_blockstore; service consists of:
  </para>
  <itemizedlist>
   <listitem>
    <para>
     An API service, typically deployed and active on the controller nodes.
    </para>
   </listitem>
   <listitem>
    <para>
     A scheduler service, also typically deployed and active on the controller
     nodes.
    </para>
   </listitem>
   <listitem>
    <para>
     A volume service, which is deployed on all of the controller nodes but
     only active on one of them.
    </para>
   </listitem>
   <listitem>
    <para>
     A backup service, which is deployed on the same controller node as the
     volume service.
    </para>
   </listitem>
  </itemizedlist>
  <informalfigure>
   <mediaobject>
    <imageobject role="fo">
     <imagedata fileref="media-hos.docs-troubleshooting-cinder_topology.png" width="75%"/>
    </imageobject>
    <imageobject role="html">
     <imagedata fileref="media-hos.docs-troubleshooting-cinder_topology.png"/>
    </imageobject>
   </mediaobject>
  </informalfigure>
  <para>
   You can refer to your configuration files (usually located in
   <literal>~/openstack/my_cloud/definition/</literal> on the &clm;) for
   specifics about where your services are located. They will usually be
   located on the controller nodes.
  </para>
  <para>
   &o_blockstore; uses a &mariadb; database and communicates between components by
   consuming messages from a RabbitMQ message service.
  </para>
  <para>
   The &o_blockstore; API service is layered underneath a HAProxy service and accessed
   using a virtual IP address maintained using keepalived.
  </para>
  <para>
   If any of the &o_blockstore; components is not running on its intended host then an
   alarm will be raised. Details on how to resolve these alarms can be found on
   our <xref linkend="alarmdefinitions"/> page. You should check the logs for
   the service on the appropriate nodes. All &o_blockstore; logs are stored in
   <literal>/var/log/cinder/</literal> and all log entries above
   <literal>INFO</literal> level are also sent to the centralized logging
   service. For details on how to change the logging level of the &o_blockstore;
   service, see <xref linkend="central_log_configure_services"/>.
  </para>
  <para>
   In order to get the full context of an error you may need to examine the
   full log files on individual nodes. Note that if a component runs on more
   than one node you will need to review the logs on each of the nodes that
   component runs on. Also remember that as logs rotate that the time interval
   you are interested in may be in an older log file.
  </para>
  <para>
   <emphasis role="bold">Log locations:</emphasis>
  </para>
  <para>
   <literal>/var/log/cinder/cinder-api.log</literal> - Check this log if you
   have endpoint or connectivity issues
  </para>
  <para>
   <literal>/var/log/cinder/cinder-scheduler.log</literal> - Check this log if
   the system cannot assign your volume to a back-end
  </para>
  <para>
   <literal>/var/log/cinder/cinder-backup.log</literal> - Check this log if you
   have backup or restore issues
  </para>
  <para>
   <literal>/var/log/cinder-cinder-volume.log</literal> - Check here for
   failures during volume creation
  </para>
  <para>
   <literal>/var/log/nova/nova-compute.log</literal> - Check here for failures
   with attaching volumes to compute instances
  </para>
  <para>
   You can also check the logs for the database and/or the RabbitMQ service if
   your cloud exhibits database or messaging errors.
  </para>
  <para>
   If the API servers are up and running but the API is not reachable then
   checking the HAProxy logs on the active keepalived node would be the place
   to look.
  </para>
  <para>
   If you have errors attaching volumes to compute instances using the Nova API
   then the logs would be on the compute node associated with the instance. You
   can use the following command to determine which node is hosting the
   instance:
  </para>
<screen>nova show &lt;instance_uuid&gt;</screen>
  <para>
   Then you can check the logs located at
   <literal>/var/log/nova/nova-compute.log</literal> on that compute node.
  </para>
 </section>
 <section xml:id="volume_states">
  <title>Understanding the &o_blockstore; volume states</title>
  <para>
   Once the topology is understood, if the issue with the &o_blockstore; service
   relates to a specific volume then you should have a good understanding of
   what the various states a volume can be in are. The states are:
  </para>
  <itemizedlist>
   <listitem>
    <para>
     attaching
    </para>
   </listitem>
   <listitem>
    <para>
     available
    </para>
   </listitem>
   <listitem>
    <para>
     backing-up
    </para>
   </listitem>
   <listitem>
    <para>
     creating
    </para>
   </listitem>
   <listitem>
    <para>
     deleting
    </para>
   </listitem>
   <listitem>
    <para>
     downloading
    </para>
   </listitem>
   <listitem>
    <para>
     error
    </para>
   </listitem>
   <listitem>
    <para>
     error attaching
    </para>
   </listitem>
   <listitem>
    <para>
     error deleting
    </para>
   </listitem>
   <listitem>
    <para>
     error detaching
    </para>
   </listitem>
   <listitem>
    <para>
     error extending
    </para>
   </listitem>
   <listitem>
    <para>
     error restoring
    </para>
   </listitem>
   <listitem>
    <para>
     in-use
    </para>
   </listitem>
   <listitem>
    <para>
     extending
    </para>
   </listitem>
   <listitem>
    <para>
     restoring
    </para>
   </listitem>
   <listitem>
    <para>
     restoring backup
    </para>
   </listitem>
   <listitem>
    <para>
     retyping
    </para>
   </listitem>
   <listitem>
    <para>
     uploading
    </para>
   </listitem>
  </itemizedlist>
  <para>
   The common states are <literal>in-use</literal> which indicates a volume is
   currently attached to a compute instance and <literal>available</literal>
   means the volume is created on a back-end and is free to be attached to an
   instance. All <literal>-ing</literal> states are transient and represent a
   transition. If a volume stays in one of those states for too long indicating
   it is stuck, or if it fails and goes into an error state, you should check
   for failures in the logs.
  </para>
 </section>
 <section xml:id="idg-all-operations-troubleshooting-ts_blockstorage-xml-7">
  <title>Initial troubleshooting steps</title>
  <para>
   These should be the initial troubleshooting steps you go through.
  </para>
  <procedure>
   <step>
    <para>
     If you have noticed an issue with the service, you should check your
     monitoring system for any alarms that may have triggered. See
     <xref linkend="alarmdefinitions"/> for resolution steps for those alarms.
    </para>
   </step>
   <step>
    <para>
     Check if the &o_blockstore; API service is active by listing the available volumes
     from the &clm;:
    </para>
<screen>source ~/service.osrc
openstack volume list</screen>
   </step>
   <step>
    <para>
     Run a basic diagnostic from the &clm;:
    </para>
<screen>cd ~/scratch/ansible/next/ardana/ansible
ansible-playbook -i hosts/verb_hosts _cinder_post_check.yml</screen>
    <para>
     This ansible playbook will list all volumes, create a 1 GB volume and then
     delete it using the v1 and v2 APIs, which will exercise basic &o_blockstore;
     capability.
    </para>
   </step>
  </procedure>
 </section>
 <section xml:id="common_issues">
  <title>Common failures</title>
  <para>
   <emphasis role="bold">Alerts from the &o_blockstore; service</emphasis>
  </para>
  <para>
   Check for alerts associated with the block storage service, noting that
   these could include alerts related to the server nodes being down, alerts
   related to the messaging and database services, or the HAProxy and
   keepalived services, as well as alerts directly attributed to the block
   storage service.
  </para>
  <para>
   The &opscon; provides a web UI method for checking
   alarms.
  </para>
  <para>
   <emphasis role="bold">&o_blockstore; volume service is down</emphasis>
  </para>
  <para>
   The &o_blockstore; volume service could be down if the server hosting the
   volume service fails. (Running the command <command>cinder
   service-list</command> will show the state of the volume service.) In this
   case you should follow the documented procedure linked below to start the
   volume service on another controller node. See <xref
   linkend="sec.operation.manage-block-storage"/> for details.
  </para>
  <para>
   <emphasis role="bold">Creating a &o_blockstore; bootable volume fails</emphasis>
  </para>
  <para>
   When creating a bootable volume from an image, your &o_blockstore; volume must be
   larger than the Virtual Size (raw size) of your image or creation will fail
   with an error.
  </para>
  <para>
   An error like this error would appear in
   <literal>cinder-volume.log</literal> file:
  </para>
<screen>'2016-06-14 07:44:00.954 25834 ERROR oslo_messaging.rpc.dispatcher ImageCopyFailure: Failed to copy image to volume: qemu-img: /dev/disk/by-path/ip-192.168.92.5:3260-iscsi-iqn.2003-10.com.lefthandnetworks:mg-ses:146:volume-c0e75c66-a20a-4368-b797-d70afedb45cc-lun-0: error while converting raw: Device is too small
2016-06-14 07:44:00.954 25834 ERROR oslo_messaging.rpc.dispatcher'</screen>
  <para>
   In an example where creating a 1GB bootable volume fails, your image may
   look like this:
  </para>
<screen>$ qemu-img info /tmp/image.qcow2
image: /tmp/image.qcow2
file format: qcow2
virtual size: 1.5G (1563295744 bytes)
disk size: 354M
cluster_size: 65536
...</screen>
  <para>
   In this case, note that the image format is qcow2 and hte virtual size is
   1.5GB, which is greater than the size of the bootable volume. Even though
   the compressed image size is less than 1GB, this bootable volume creation
   will fail.
  </para>
  <para>
   When creating your disk model for nodes that will have the cinder volume
   role make sure that there is sufficient disk space allocated for a temporary
   space for image conversion if you will be creating bootable volumes. You
   should allocate enough space to the filesystem as would be needed to cater
   for the raw size of images to be used for bootable volumes - for example
   Windows images can be quite large in raw format.
  </para>
  <para>
   By default, &o_blockstore; uses <literal>/var/lib/cinder</literal> for image
   conversion and this will be on the root filesystem unless it is explicitly
   separated. You can ensure there is enough space by ensuring that the root
   file system is sufficiently large, or by creating a logical volume mounted
   at <literal>/var/lib/cinder</literal> in the disk model when installing the
   system.
  </para>
  <para>
   If your system is already installed, use these steps to update this:
  </para>
  <orderedlist>
   <listitem>
    <para>
     Edit the configuration item <literal>image_conversion_dir</literal> in
     <literal>cinder.conf.j2</literal> to point to another location with more
     disk space. Make sure that the new directory location has the same
     ownership and permissions as <literal>/var/lib/cinder</literal>
     (owner:cinder group:cinder. mode 0750).
    </para>
   </listitem>
   <listitem>
    <para>
     Then run this playbook:
    </para>
<screen>cd ~/scratch/ansible/next/ardana/ansible
ansible-playbook -i hosts/verb_hosts cinder-reconfigure.yml</screen>
   </listitem>
  </orderedlist>
  <para>
   <emphasis role="bold">API-level failures</emphasis>
  </para>
  <para>
   If the API is inaccessible, determine if the API service is running on the
   target node. If it is not, check to see why the API service is not running in
   the log files. If it is running okay, check if the HAProxy service is
   functioning properly.
  </para>
  <note>
   <para>
    After a controller node is rebooted, you must make sure to run the
    <literal>ardana-start.yml</literal> playbook to ensure all the services are
    up and running. For more information, see
    <xref linkend="recover_downed_cluster"/>.
   </para>
  </note>
  <para>
   If the API service is returning an error code, look for the error message in
   the API logs on all API nodes. Successful completions would be logged like
   this:
  </para>
<screen>2016-04-25 10:09:51.107 30743 INFO eventlet.wsgi.server [<emphasis role="bold">req-a14cd6f3-6c7c-4076-adc3-48f8c91448f6</emphasis>
dfb484eb00f94fb39b5d8f5a894cd163 7b61149483ba4eeb8a05efa92ef5b197 - - -] 192.168.186.105 - - [25/Apr/2016
10:09:51] "GET /v2/7b61149483ba4eeb8a05efa92ef5b197/volumes/detail HTTP/1.1" <emphasis role="bold">200</emphasis> 13915 0.235921</screen>
  <para>
   where <literal>200</literal> represents HTTP status 200 for a successful
   completion. Look for a line with your status code and then examine all
   entries associated with the request id. The request ID in the successful
   completion is highlighted in bold above.
  </para>
  <para>
   The request may have failed at the scheduler or at the volume or backup
   service and you should also check those logs at the time interval of
   interest, noting that the log file of interest may be on a different node.
  </para>
  <para>
   <emphasis role="bold">Operations that do not complete</emphasis>
  </para>
  <para>
   If you have started an operation, such as creating or deleting a volume,
   that does not complete, the &o_blockstore; volume may be stuck in a state. You
   should follow the procedures for detaling with stuck volumes.
  </para>
  <para>
   There are six transitory states that a volume can get stuck in:
  </para>
  <informaltable colsep="1" rowsep="1">
   <tgroup cols="2">
    <colspec colname="c1" colnum="1"/>
    <colspec colname="c2" colnum="2"/>
    <thead>
     <row>
      <entry>State</entry>
      <entry>Description</entry>
     </row>
    </thead>
    <tbody>
     <row>
      <entry>creating</entry>
      <entry>The &o_blockstore; volume manager has sent a request
                                                  to a back-end driver to create a volume, but has
                                                  not received confirmation that the volume is
                                                  available.</entry>
     </row>
     <row>
      <entry>attaching</entry>
      <entry>&o_blockstore; has received a request from Nova to
                                                  make a volume available for attaching to an
                                                  instance but has not received confirmation from
                                                  Nova that the attachment is complete.</entry>
     </row>
     <row>
      <entry>detaching</entry>
      <entry>&o_blockstore; has received notification from Nova
                                                  that it will detach a volume from an instance but
                                                  has not received notification that the detachment
                                                  is complete.</entry>
     </row>
     <row>
      <entry>deleting</entry>
      <entry>&o_blockstore; has received a request to delete a
                                                  volume but has not completed the
                                                  operation.</entry>
     </row>
     <row>
      <entry>backing-up</entry>
      <entry>&o_blockstore; backup manager has started to back a
                                                  volume up to Swift, or some other backup target,
                                                  but has not completed the operation.</entry>
     </row>
     <row>
      <entry>restoring</entry>
      <entry>&o_blockstore; backup manager has started to restore
                                                  a volume from Swift, or some other backup target,
                                                  but has not completed the operation.</entry>
     </row>
    </tbody>
   </tgroup>
  </informaltable>
  <para>
   At a high level, the steps that you would take to address any of these
   states are similar:
  </para>
  <orderedlist>
   <listitem>
    <para>
     Confirm that the volume is actually stuck, and not just temporarily
     blocked.
    </para>
   </listitem>
   <listitem>
    <para>
     Where possible, remove any resources being held by the volume. For
     example, if a volume is stuck detaching it may be necessary to remove
     associated iSCSI or DM devices on the compute node.
    </para>
   </listitem>
   <listitem>
    <para>
     Reset the state of the volume to an appropriate state, for example to
     <literal>available</literal> or <literal>error</literal>.
    </para>
   </listitem>
   <listitem>
    <para>
     Do any final cleanup. For example, if you reset the state to
     <literal>error</literal> you can then delete the volume.
    </para>
   </listitem>
  </orderedlist>
  <para>
   The next sections will describe specific steps you can take for volumes
   stuck in each of the transitory states.
  </para>
  <para>
   <emphasis role="bold">Volumes stuck in Creating</emphasis>
  </para>
  <para>
   Broadly speaking, there are two possible scenarios where a volume would get
   stuck in <literal>creating</literal>. The <literal>cinder-volume</literal>
   service could have thrown an exception while it was attempting to create the
   volume, and failed to handle the exception correctly. Or the volume back-end
   could have failed, or gone offline, after it received the request from
   &o_blockstore; to create the volume.
  </para>
  <para>
   These two cases are different in that for the second case you will need to
   determine the reason the back-end is offline and restart it. Often, when the
   back-end has been restarted, the volume will move from
   <literal>creating</literal> to <literal>available</literal> so your issue
   will be resolved.
  </para>
  <para>
   If you can create volumes successfully on the same back-end as the volume
   stuck in <literal>creating</literal> then the back-end is not down. So you
   will need to reset the state for the volume and then delete it.
  </para>
  <para>
   To reset the state of a volume you can use the <literal>cinder
   reset-state</literal> command. You can use either the UUID or the volume
   name of the stuck volume.
  </para>
  <para>
   For example, here is a volume list where we have a stuck volume:
  </para>
<screen>$ cinder list
+--------------------------------------+-----------+------+------+-------------+------------+
|                  ID                  |   Status  | Name | Size | Volume Type |Attached to |
+--------------------------------------+-----------+------+------+-------------+------------+
| 14b76133-e076-4bd3-b335-fa67e09e51f6 | creating  | vol1 |  1   |      -      |            |
+--------------------------------------+-----------+------+------+-------------+------------+</screen>
  <para>
   You can reset the state by using the <literal>cinder reset-state</literal>
   command, like this:
  </para>
<screen>cinder reset-state --state error 14b76133-e076-4bd3-b335-fa67e09e51f6</screen>
  <para>
   Confirm that with another listing:
  </para>
<screen>$ cinder list
+--------------------------------------+-----------+------+------+-------------+------------+
|                  ID                  |   Status  | Name | Size | Volume Type |Attached to |
+--------------------------------------+-----------+------+------+-------------+------------+
| 14b76133-e076-4bd3-b335-fa67e09e51f6 | error     | vol1 |  1   |      -      |            |
+--------------------------------------+-----------+------+------+-------------+------------+</screen>
  <para>
   You can then delete the volume:
  </para>
<screen>$ cinder delete 14b76133-e076-4bd3-b335-fa67e09e51f6
Request to delete volume 14b76133-e076-4bd3-b335-fa67e09e51f6 has been accepted.</screen>
  <para>
   <emphasis role="bold">Volumes stuck in Deleting</emphasis>
  </para>
  <para>
   If a volume is stuck in the deleting state then the request to delete the
   volume may or may not have been sent to and actioned by the back-end. If you
   can identify volumes on the back-end then you can examine the back-end to
   determine whether the volume is still there or not. Then you can decide
   which of the following paths you can take. It may also be useful to
   determine whether the back-end is responding, either by checking for recent
   volume create attempts, or creating and deleting a test volume.
  </para>
  <para>
   The first option is to reset the state of the volume to
   <literal>available</literal> and then attempt to delete the volume again.
  </para>
  <para>
   The second option is to reset the state of the volume to
   <literal>error</literal> and then delete the volume.
  </para>
  <para>
   If you have reset the volume state to <literal>error</literal> then the volume
   may still be consuming storage on the back-end. If that is the case then you
   will need to delete it from the back-end using your back-end's specific tool.
  </para>
  <para>
   <emphasis role="bold">Volumes stuck in Attaching</emphasis>
  </para>
  <para>
   The most complicated situation to deal with is where a volume is stuck
   either in attaching or detaching, because as well as dealing with the state
   of the volume in &o_blockstore; and the back-end, you have to deal with exports from
   the back-end, imports to the compute node, and attachments to the compute
   instance.
  </para>
  <para>
   The two options you have here are to make sure that all exports and imports
   are deleted and to reset the state of the volume to
   <literal>available</literal> or to make sure all of the exports and imports
   are correct and to reset the state of the volume to
   <literal>in-use</literal>.
  </para>
  <para>
   A volume that is in attaching state should never have been made available to
   a compute instance and therefore should not have any data written to it, or
   in any buffers between the compute instance and the volume back-end. In that
   situation, it is often safe to manually tear down the devices exported on
   the back-end and imported on the compute host and then reset the volume state
   to <literal>available</literal>.
  </para>
  <para>
   You can use the management features of the back-end you are using to locate
   the compute host to where the volume is being exported.
  </para>
  <para>
   <emphasis role="bold">Volumes stuck in Detaching</emphasis>
  </para>
  <para>
   The steps in dealing with a volume stuck in <literal>detaching</literal>
   state are very similar to those for a volume stuck in
   <literal>attaching</literal>. However, there is the added consideration that
   the volume was attached to, and probably servicing, I/O from a compute
   instance. So you must take care to ensure that all buffers are properly
   flushed before detaching the volume.
  </para>
  <para>
   When a volume is stuck in <literal>detaching</literal>, the output from a
   <literal>cinder list</literal> command will include the UUID for the
   instance to which the volume was attached. From that you can identify the
   compute host that is running the instance using the <literal>nova
   show</literal> command.
  </para>
  <para>
   For example, here are some snippets:
  </para>
<screen>$ cinder list
+--------------------------------------+-----------+-----------------------+-----------------+
|                  ID                  |   Status  |       Name            |   Attached to   |
+--------------------------------------+-----------+-----------------------+-----------------+
| 85384325-5505-419a-81bb-546c69064ec2 | detaching |        vol1           | 4bedaa76-78ca-… |
+--------------------------------------+-----------+-----------------------+-----------------+</screen>
<screen>$ nova show 4bedaa76-78ca-4fe3-806a-3ba57a9af361|grep host
| OS-EXT-SRV-ATTR:host                 | mycloud-cp1-comp0005-mgmt
| OS-EXT-SRV-ATTR:hypervisor_hostname  | mycloud-cp1-comp0005-mgmt
| hostId                               | 61369a349bd6e17611a47adba60da317bd575be9a900ea590c1be816</screen>
  <para>
   The first thing to check in this case is whether the instance is still
   importing the volume. Use <literal>virsh list</literal> and <literal>virsh
   dumpxml</literal> as described in the section above. If the XML for the
   instance has a reference to the device, then you should reset the volume
   state to <literal>in-use</literal> and attempt the <literal>cinder
   detach</literal> operation again.
  </para>
<screen>$ cinder reset-state --state in-use --attach-status attached 85384325-5505-419a-81bb-546c69064ec2</screen>
  <para>
   If the volume gets stuck detaching again, there may be a more fundamental
   problem, which is outside the scope of this document and you should contact
   the Support team.
  </para>
  <para>
   If the volume is not referenced in the XML for the instance then you should
   remove any devices on the compute node and back-end and then reset the state
   of the volume to <literal>available</literal>.
  </para>
<screen>$ cinder reset-state --state available --attach-status detached 85384325-5505-419a-81bb-546c69064ec2</screen>
  <para>
   You can use the management features of the back-end you are using to locate
   the compute host to where the volume is being exported.
  </para>
  <para>
   <emphasis role="bold">Volumes stuck in restoring</emphasis>
  </para>
  <para>
   Restoring a &o_blockstore; volume from backup will be as slow as backing it up. So
   you must confirm that the volume is actually stuck by examining the
   <literal>cinder-backup.log</literal>. For example:
  </para>
<screen># tail -f cinder-backup.log |grep 162de6d5-ba92-4e36-aba4-e37cac41081b
2016-04-27 12:39:14.612 6689 DEBUG swiftclient [req-0c65ec42-8f9d-430a-b0d5-05446bf17e34 - -
2016-04-27 12:39:15.533 6689 DEBUG cinder.backup.chunkeddriver [req-0c65ec42-8f9d-430a-b0d5-
2016-04-27 12:39:15.566 6689 DEBUG requests.packages.urllib3.connectionpool [req-0c65ec42-
2016-04-27 12:39:15.567 6689 DEBUG swiftclient [req-0c65ec42-8f9d-430a-b0d5-05446bf17e34 - - -</screen>
  <para>
   If you determine that the volume is genuinely stuck in
   <literal>detaching</literal> then you must follow the procedure described in
   the detaching section above to remove any volumes that remain exported from
   the back-end and imported on the controller node. Remember that in this case
   the volumes will be imported and mounted on the controller node running
   <literal>cinder-backup</literal>. So you do not have to search for the
   correct compute host. Also remember that no instances are involved so you do
   not need to confirm that the volume is not imported to any instances.
  </para>
 </section>
 <section xml:id="debugging_attachment">
  <title>Debugging volume attachment</title>
  <para>
   In an error case, it is possible for a &o_blockstore; volume to fail to complete an
   operation and revert back to its initial state. For example, attaching a
   &o_blockstore; volume to a Nova instance, so you would follow the steps above to
   examine the Nova compute logs for the attach request.
  </para>
 </section>
 <section xml:id="errors_creating">
  <title>Errors creating volumes</title>
  <para>
   If you are creating a volume and it goes into the <literal>ERROR</literal>
   state, a common error to see is <literal>No valid host was found</literal>.
   This means that the scheduler could not schedule your volume to a back-end.
   You should check that the volume service is up and running. You can use this
   command:
  </para>
<screen>$ sudo cinder-manage service list
Binary           Host                                 Zone             Status     State Updated At
cinder-scheduler ha-volume-manager                    nova             enabled    :-)   2016-04-25 11:39:30
cinder-volume    ha-volume-manager@ses1               nova             enabled    XXX   2016-04-25 11:27:26
cinder-backup    ha-volume-manager                    nova             enabled    :-)   2016-04-25 11:39:28</screen>
  <para>
   In this example, the state of <literal>XXX</literal> indicates that the
   service is down.
  </para>
  <para>
   If the service is up, next check that the back-end has sufficient space. You
   can use this command to show the available and total space on each back-end:
  </para>
<screen>cinder get-pools –detail</screen>
  <para>
   If your deployment is using volume types, verify that the
   <literal>volume_backend_name</literal> in your
   <literal>cinder.conf</literal> file matches the
   <literal>volume_backend_name</literal> for the volume type you selected.
  </para>
  <para>
   You can verify the back-end name on your volume type by using this command:
  </para>
<screen>openstack volume type list</screen>
  <para>
   Then list the details about your volume type. For example:
  </para>
<screen>$ openstack volume type show dfa8ecbd-8b95-49eb-bde7-6520aebacde0
+---------------------------------+--------------------------------------+
| Field                           | Value                                |
+---------------------------------+--------------------------------------+
| description                     | None                                 |
| id                              | dfa8ecbd-8b95-49eb-bde7-6520aebacde0 |
| is_public                       | True                                 |
| name                            | my3par                               |
| os-volume-type-access:is_public | True                                 |
| properties                      | volume_backend_name='3par'           |
+---------------------------------+--------------------------------------+</screen>
 </section>
 <section xml:id="idg-all-operations-troubleshooting-ts_blockstorage-xml-12">
  <title>Diagnosing back-end issues</title>
  <para>
   You can find further troubleshooting steps for specific back-end types by
   vising these pages:
  </para>
 </section>
</section>
