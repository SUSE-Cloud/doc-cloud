<?xml version="1.0"?>
<!DOCTYPE chapter [
 <!ENTITY % entities SYSTEM "entity-decl.ent"> %entities;
]>
<chapter xml:id="multipath-boot-from-san" xmlns="http://docbook.org/ns/docbook" version="5.1"
 xmlns:xi="http://www.w3.org/2001/XInclude" xmlns:xlink="http://www.w3.org/1999/xlink">
 <title>Boot from SAN and Multipath Configuration</title>
 <section xml:id="multipath-overview">
  <title>Introduction</title>
  <para>
   For information about supported hardware for multipathing, see
   <xref
    linkend="hw-support-hardwareconfig"/>.
  </para>
  <important xml:id="boot-from-san-LUN0">
   <para>
    When exporting a LUN to a node for boot from SAN, you should ensure that
    <emphasis>LUN 0</emphasis> is assigned to the LUN and configure any setup
    dialog that is necessary in the firmware to consume this LUN 0 for OS boot.
   </para>
  </important>
  <important xml:id="boot-from-san-host-persona">
   <para>
    Any hosts that are connected to 3PAR storage must have a <literal>host
    persona</literal> of <literal>2-generic-alua</literal> set on the 3PAR.
    Refer to the 3PAR documentation for the steps necessary to check this and
    change if necessary.
   </para>
  </important>
  <para>
   iSCSI boot from SAN is not supported. For more information on the use of
   cinder with multipath, see <xref linkend="sec-3par-multipath"/>.
  </para>
  <para>
   To allow &product; to use volumes from a SAN, you have to specify
   configuration options for both the installation and the OS configuration
   phase. In all cases, the devices that are utilized are devices for which
   multipath is configured.
  </para>
 </section>
 <section>
  <title>Install Phase Configuration</title>
  <para>
   For FC connected nodes and for FCoE nodes where the network processor used
   is from the Emulex family such as for the 650FLB, the following changes are
   required.
  </para>
<!-- removed in SCRD-8942
  <procedure>
   <step>
    <para>
     In each stanza of the <filename>servers.yml</filename> insert a line
     stating <literal>boot-from-san: true</literal>
    </para>
<screen>- id: controller2
      ip-addr: 192.168.10.4
      role: CONTROLLER-ROLE
      server-group: RACK2
      nic-mapping: HP-DL360-4PORT</screen>
    <para>
     This uses the disk <filename>/dev/mapper/mpatha</filename> as the default
     device on which to install the OS.
    </para>
   </step>
   <step>
    <para>
     In the disk input models, specify the devices that will be used via their
     multipath names (which will be of the form
     <filename>/dev/mapper/mpatha</filename>,
     <filename>/dev/mapper/mpathb</filename>, etc.).
    </para>
<screen>
    volume-groups:
      - name: ardana-vg
        physical-volumes:

          # NOTE: 'sda_root' is a templated value. This value is checked in
          # os-config and replaced by the partition actually used on sda
          #for example sda1 or sda5
          - /dev/mapper/mpatha_root

...
      - name: vg-comp
        physical-volumes:
          - /dev/mapper/mpathb
</screen>
   </step>
  </procedure>
-->
  <para>
   Instead of using Cobbler, you need to provision a baremetal node manually
   using the following procedure.
  </para>
  <procedure>
   <step>
    <para>
     During manual installation of &cloudos;, select the desired SAN disk and
     create an LVM partitioning scheme that meets &cloud; requirements: it has
     an <literal>ardana-vg</literal> volume group and an
     <literal>ardana-vg-root</literal> logical volume. For more information on
     partitioning, see <xref linkend="sec-depl-adm-inst-partitioning"/>.
    </para>
   </step>
   <step>
    <para>
     Open the <filename>/etc/multipath/bindings</filename> file and map the
     expected device name to the SAN disk selected during installation. In
     &cloud;, the naming convention is mpatha, mpathb, and so on. For example:
    </para>
    <screen>mpatha-part1 360000000030349030-part1
mpatha-part2 360000000030349030-part2
mpatha-part3 360000000030349030-part3

mpathb-part1 360000000030349000-part1
mpathb-part2 360000000030349000-part2</screen>
   </step>
   <step>
    <para>
     Reboot to enable the changes.
    </para>
   </step>
   <step>
    <para>
     Assign a static IP to the node:
    </para>
    <substeps>
     <step>
      <para>
       Use the <command>ip addr</command> command to list active network
       interfaces on your system:
      </para>
<screen>1: lo: &lt;LOOPBACK,UP,LOWER_UP&gt; mtu 65536 qdisc noqueue state UNKNOWN
    link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00
    inet 127.0.0.1/8 scope host lo
       valid_lft forever preferred_lft forever
    inet6 ::1/128 scope host
       valid_lft forever preferred_lft forever
2: eno1: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc mq state UP qlen 1000
    link/ether f0:92:1c:05:89:70 brd ff:ff:ff:ff:ff:ff
    inet 10.13.111.178/26 brd 10.13.111.191 scope global eno1
       valid_lft forever preferred_lft forever
    inet6 fe80::f292:1cff:fe05:8970/64 scope link
       valid_lft forever preferred_lft forever
3: eno2: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc mq state UP qlen 1000
    link/ether f0:92:1c:05:89:74 brd ff:ff:ff:ff:ff:ff
</screen>
     </step>
     <step>
      <para>
       Identify the network interface that matches the MAC address of your
       server and edit the corresponding configuration file in
       <filename>/etc/sysconfig/network-scripts</filename>. For example, for
       the <systemitem>eno1</systemitem> interface, open the
       <systemitem>/etc/sysconfig/network-scripts/ifcfg-eno1</systemitem> file
       and edit <replaceable>IPADDR</replaceable> and
       <replaceable>NETMASK</replaceable> values to match your environment.
       The <replaceable>IPADDR</replaceable> is used in the corresponding
       stanza in <filename>servers.yml</filename>. You may also need to set
       <literal>BOOTPROTO</literal> to <literal>none</literal>:
      </para>
<screen>TYPE=Ethernet
BOOTPROTO=none
DEFROUTE=yes
PEERDNS=yes
PEERROUTES=yes
IPV4_FAILURE_FATAL=no
IPV6INIT=yes
IPV6_AUTOCONF=yes
IPV6_DEFROUTE=yes
IPV6_PEERDNS=yes
IPV6_PEERROUTES=yes
IPV6_FAILURE_FATAL=no
NAME=eno1
UUID=360360aa-12aa-444a-a1aa-ee777a3a1a7a
DEVICE=eno1
ONBOOT=yes
NETMASK=255.255.255.192
IPADDR=10.10.100.10
</screen>
     </step>
     <step>
      <para>
       Reboot the &slsa; node and ensure that it can be accessed from the
       &clm;.
      </para>
     </step>
    </substeps>
   </step>
   <step>
    <para>
     Add the <literal>ardana</literal> user and home directory:
    </para>
<screen>
&prompt.root;useradd -m -d /var/lib/ardana -U ardana
</screen>
   </step>
   <step>
    <para>
     Allow the user <literal>ardana</literal> to run <command>sudo</command>
     without a password by creating the
     <filename>/etc/sudoers.d/ardana</filename> file with the following
     configuration:
    </para>
<screen>ardana ALL=(ALL) NOPASSWD:ALL</screen>
   </step>
   <step>
    <para>
     When you start installation using the &clm;, or if you are adding a &slsa;
     node to an existing cloud, copy the &clm; public key to the &slsa; node to
     enable passwordless SSH access. One way of doing this is to copy the file
     <filename>~/.ssh/authorized_keys</filename> from another node in the cloud
     to the same location on the &slsa; node. If you are installing a new
     cloud, this file will be available on the nodes after running the
     <filename>bm-reimage.yml</filename> playbook. Ensure that there is global
     read access to the file
     <filename>/var/lib/ardana/.ssh/authorized_keys</filename>.
    </para>
    <para>
     Use the following command to test passwordless SSH from the deployer and
     check the ability to remotely execute sudo commands:
    </para>
<screen>ssh stack@<replaceable>SLES_NODE_IP</replaceable> "sudo tail -5 /var/log/messages"</screen>
   </step>
  </procedure>
 <section xml:id="depl-cloud">
  <title>Deploying the Cloud</title>
  <procedure>
   <step>
    <para>
     In
     <filename>openstack/my_cloud/config/multipath/multipath_settings.yml</filename>,
     set <literal>manual_multipath_conf</literal> to <literal>True</literal> so
     that <literal>multipath.conf</literal> on manually installed nodes is not
     overwritten.
    </para>
   </step>
   <step>
    <para>
     Commit the changes.
    </para>
    <screen>&prompt.user;cd ~/openstack
&prompt.user;git add -A
&prompt.user;git commit -m "multipath config"</screen>
   </step>
   <step>
    <para>
     Run <literal>config-processor</literal> and
     <literal>ready-deployment</literal>.
    </para>
    <screen>&prompt.user;cd ~/openstack/ardana/ansible
&prompt.ardana;ansible-playbook -i hosts/localhost config-processor-run.yml
&prompt.user;ansible-playbook -i hosts/localhost ready-deployment.yml
    </screen>
   </step>
   <step>
    <para>
     Ensure that all existing partitions on the nodes are wiped prior to
     installation by running the <filename>wipe_disks.yml</filename> playbook.
    </para>
    <note>
     <para>
      Confirm that your root partition disk is not listed in disks to be wiped.
     </para>
    </note>
    <screen>&prompt.user;cd ~/scratch/ansible/next/ardana/ansible
&prompt.user;ansible-playbook -i hosts/verb_hosts wipe_disks.yml</screen>
   </step>
   <step>
    <para>
     Run the <filename>site.yml</filename> playbook:
    </para>
    <screen>&prompt.user;cd ~/scratch/ansible/next/ardana/ansible
&prompt.user;ansible-playbook -i hosts/verb_hosts site.yml</screen>
   </step>
  </procedure>
 </section>
</section>
</chapter>
