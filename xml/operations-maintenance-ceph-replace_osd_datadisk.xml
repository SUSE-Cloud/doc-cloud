<?xml version="1.0"?>
<?xml-stylesheet href="urn:x-suse:xslt:profiling:docbook51-profile.xsl"
 type="text/xml"
 title="Profiling step"?>
<!DOCTYPE section [
 <!ENTITY % entities SYSTEM "entities.ent"> %entities;
]>
<!---->
<section xmlns="http://docbook.org/ns/docbook" xmlns:xi="http://www.w3.org/2001/XInclude" xmlns:xlink="http://www.w3.org/1999/xlink" xml:id="topic_ryt_gt4_zv">
 <title>Replacing the Data Disk in a Ceph OSD Node</title>
 <para>
  Maintenance steps for replacing a data disk in a Ceph OSD node.
 </para>
 <para>
  The following steps can be used to replace a failed data disk in a Ceph OSD
  node.
 </para>
 <orderedlist>
  <listitem>
   <para>
    Once the failed data disk has been identified, determine what the
    corresponding journal partition and the affected OSD ID is by SSH'ing to
    the Ceph OSD node and using this command:
   </para>
<screen>sudo ceph-disk list | grep <replaceable>FAILED_DISK</replaceable></screen>
   <para>
    Example, where the failed data disk is <literal>/dev/sde</literal>:
   </para>
<screen>sudo ceph-disk list | grep /dev/sde
/dev/sde :
 /dev/sde1 ceph data, active, cluster ceph, osd.5, journal /dev/sdg3
 /dev/sdg3 ceph journal, for /dev/sde1</screen>
   <para>
    In this example, the affected OSD ID is <literal>5</literal> and the
    journal partition is <literal>/dev/sdg3</literal>.
   </para>
  </listitem>
  <listitem>
   <para>
    Determine that the OSD that is consuming the failed drive is still running
    by using this command:
   </para>
<screen>ceph osd tree --cluster <replaceable>CEPH_CLUSTER</replaceable> | grep -i osd.<replaceable>OSD_NUMBER</replaceable></screen>
  </listitem>
  <listitem>
   <para>
    If the OSD is still running, check the status and stop the OSD service by
    using this command:
   </para>
<screen>sudo systemctl status ceph-osd@<replaceable>OSD_NUMBER</replaceable>
sudo systemctl stop ceph-osd@<replaceable>OSD_NUMBER</replaceable></screen>
  </listitem>
  <listitem>
   <para>
    Check if the OSD disk is mounted and if it is, unmount it:
   </para>
<screen>sudo df -h
sudo umount <replaceable>SOURCE</replaceable> | <replaceable>DIRECTORY</replaceable></screen>
  </listitem>
  <listitem>
   <para>
    You will also need to remove the mount instruction for the failed disk from
    the fstab file. You can determine what the failed drive's UUID is by using
    this command:
   </para>
<screen>sudo blkid | grep "ceph data"</screen>
   <para>
    Example output, where the UUID
    <literal>51c32ca1-0ee1-4c75-bfbc-5d113a4b4402</literal> corresponds to the
    data disk being replaced in our series of examples
    (<literal>/dev/sde</literal>).
   </para>
<screen>$ sudo blkid |grep "ceph data"
/dev/sdc1: UUID="45b7c7e8-4f2d-4226-989b-c83e7e9e2596" TYPE="xfs" PARTLABEL="ceph data" PARTUUID="6c37a311-eabf-4872-bd8e-7d426585432e"
/dev/sdf1: UUID="1cfb4de2-fe21-4e20-9a6e-ad45e90949ae" TYPE="xfs" PARTLABEL="ceph data" PARTUUID="209b9af4-8d25-4d60-a430-2934ceca6042"
/dev/sde1: UUID="51c32ca1-0ee1-4c75-bfbc-5d113a4b4402" TYPE="xfs" PARTLABEL="ceph data" PARTUUID="dfcd410d-9c56-4bb1-93c6-496bae29de5f"</screen>
  </listitem>
  <listitem>
   <para>
    Remove the line in the <literal>/etc/fstab</literal> file pertaining to the
    failed disk.
   </para>
  </listitem>
  <listitem>
   <para>
    Delete the journal partition for the OSD and instruct the kernel to reload
    the table:
   </para>
<screen>(echo d; echo <replaceable>PARTITION_NUMBER</replaceable>; echo w) | sudo fdisk <replaceable>FAILED_DISK</replaceable>
sudo partprobe</screen>
  </listitem>
  <listitem>
   <para>
    Remove the failed drive and insert a new, replacement, physical drive.
    After replacing the drive, wait for some time so that the new drive becomes
    stable. Ensure that there are no pre-existing partitions on the new disk.
   </para>
  </listitem>
  <listitem>
   <para>
    Verify that replacement data disk gets the same device name and then
    proceed with the steps below to create a new OSD with the replacement data
    disk.
   </para>
  </listitem>
  <listitem>
   <para>
    Make the OSD out, ensure that the OSD has stopped and if not, stop it with
    this command:
   </para>
<screen>ceph osd out osd.&lt;osd-number&gt; --cluster &lt;ceph-cluster&gt;
sudo systemctl status ceph-osd@&lt;osd-number&gt;
sudo systemctl stop ceph-osd@&lt;osd-number&gt;</screen>
  </listitem>
  <listitem>
   <para>
    Remove this OSD from the CRUSH map:
   </para>
<screen>ceph osd crush remove osd.<replaceable>OSD_NUMBER</replaceable> --cluster <replaceable>CEPH_CLUSTER</replaceable></screen>
  </listitem>
  <listitem>
   <para>
    As soon as it is removed from the CRUSH map, Ceph starts making PG copies
    that were located on the failed disk and it places these PG on other disks.
    So a recovery process will start, which can be observed by periodically
    executing this command:
   </para>
<screen>ceph -s --cluster <replaceable>CEPH_CLUSTER</replaceable></screen>
   <para>
    Once the <literal>active+remapped</literal> and
    <literal>active+recovering</literal> PGs have become
    <literal>active+clean</literal> then the data recovery can be considered
    complete.
   </para>
  </listitem>
  <listitem>
   <para>
    Now delete the keyrings for that OSD and finally remove the OSD with these
    commands:
   </para>
<screen>ceph auth del osd.<replaceable>OSD_NUMBER</replaceable> --cluster <replaceable>CEPH_CLUSTER</replaceable>
ceph osd rm <replaceable>OSD_NUMBER</replaceable> --cluster <replaceable>CEPH_CLUSTER</replaceable></screen>
  </listitem>
  <listitem>
   <para>
    Remove the OSD's directories with this command. Ensure you run this against
    the affected OSD node only.
   </para>
<screen>sudo rmdir /var/lib/ceph/osd/<replaceable>CEPH_CLUSTER</replaceable>-<replaceable>OSD_NUMBER</replaceable></screen>
   <para>
    where <replaceable>CEPH_CLUSTER</replaceable> should be replaced with the
    name of your Ceph cluster and
    <replaceable>OSD_NUMBER</replaceable> should be replaced with the number
    identified in step #3 earlier. <!-- FIXME: <xref/> to step 3 -->
    For example: In the earlier example, where OSD ID 5 is the affected one
    (and Ceph cluster name is <literal>ceph</literal>), the following command
    should be executed:
   </para>
<screen>sudo rmdir /var/lib/ceph/osd/ceph-5</screen>
  </listitem>
  <listitem>
   <para>
    Log in to the lifecycle manager.
   </para>
  </listitem>
  <listitem>
   <para>
    Execute the ceph-deploy.yml playbook against the affected OSD node so that
    the OSD service on that node will be started:
   </para>
<screen>cd ~/scratch/ansible/next/hos/ansible
ansible-playbook -i hosts/verb_hosts ceph-deploy.yml --limit <replaceable>OSD_NODE_WITH_REPLACED_DISK</replaceable></screen>
  </listitem>
 </orderedlist>
</section>
