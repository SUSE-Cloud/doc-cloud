<?xml version="1.0"?>
<!DOCTYPE section [
 <!ENTITY % entities SYSTEM "entity-decl.ent"> %entities;
]>
<section xmlns="http://docbook.org/ns/docbook" xmlns:xi="http://www.w3.org/2001/XInclude"
  xmlns:xlink="http://www.w3.org/1999/xlink" xml:id="storage_alarmdefinitions">
 <title>Storage Alarms</title>
 <para>
  These alarms show under the Storage section of the &productname; &opscon;.
 </para>
 <informaltable colsep="1" rowsep="1">
  <tgroup cols="5">
   <colspec colname="c1" colnum="1"/>
   <colspec colname="c2" colnum="2"/>
   <colspec colname="c3" colnum="3"/>
   <colspec colname="c4" colnum="4"/>
   <colspec colname="c5" colnum="5"/>
   <thead>
    <row>
     <entry>Service</entry>
     <entry>Alarm Name</entry>
     <entry>Description</entry>
     <entry>Likely Cause</entry>
     <entry>Mitigation Tasks to Perform</entry>
    </row>
   </thead>
   <tbody>
    <row>
     <entry morerows="25">object-storage</entry>
     <entry>swiftlm-scan monitor</entry>
     <entry>Alarms if <literal>swiftlm-scan</literal> cannot execute a monitoring task.</entry>
     <entry>The <literal>swiftlm-scan</literal> program is used to monitor and measure a number
            of metrics. If it is unable to monitor or measure something, it raises this
            alarm.</entry>
     <entry>
      <para>
       Click on the alarm to examine the <literal>Details</literal> field and
       look for a <literal>msg</literal> field. The text may explain the error
       problem. To view/confirm this, you can also log into the host specified
       by the <literal>hostname</literal> dimension, and then run this command:
      </para>
<screen>sudo swiftlm-scan | python -mjson.tool</screen>
      <para>
       The <literal>msg</literal> field is contained in the
       <literal>value_meta</literal> item.
      </para>
     </entry>
    </row>
    <row>
     <entry>Swift account replicator last completed in 12 hours</entry>
     <entry>Alarms if an <literal>account-replicator</literal> process did not complete a
            replication cycle within the last 12 hours.</entry>
     <entry>This can indicate that the <literal>account-replication</literal> process is
            stuck.</entry>
     <entry>
      <para>
       SSH to the affected host and restart the process with this command:
      </para>
<screen>sudo systemctl restart swift-account-replicator</screen>
      <para>
       Another cause of this problem may be that a file system may be corrupt.
       Look for sign of this in these logs on the affected node:
      </para>
<screen>/var/log/swift/swift.log
/var/log/kern.log</screen>
      <para>
       The file system may need to be wiped, contact &serviceteam; for advice on
       the best way to do that if needed. You can then reformat the file system
       with these steps:
      </para>
      <orderedlist xml:id="ol_jbr_2zp_mx">
       <listitem>
        <para>
         Log in to the &lcm;.
        </para>
       </listitem>
       <listitem>
        <para>
         Run the Swift deploy playbook against the affected node, which will
         format the wiped file system:
        </para>
<screen>cd ~/scratch/ansible/next/ardana/ansible/
ansible-playbook -i hosts/verb_hosts swift-deploy.yml --limit &lt;hostname&gt;</screen>
       </listitem>
      </orderedlist>
     </entry>
    </row>
    <row>
     <entry>Swift container replicator last completed in 12 hours</entry>
     <entry>Alarms if a container-replicator process did not complete a replication cycle
            within the last 12 hours</entry>
     <entry>This can indicate that the container-replication process is stuck.</entry>
     <entry>
      <para>
       SSH to the affected host and restart the process with this command:
      </para>
<screen>sudo systemctl restart swift-container-replicator</screen>
      <para>
       Another cause of this problem may be that a file system may be corrupt.
       Look for sign of this in these logs on the affected node:
      </para>
<screen>/var/log/swift/swift.log
/var/log/kern.log</screen>
      <para>
       The file system may need to be wiped, contact &serviceteam; for advice on
       the best way to do that if needed. You can then reformat the file system
       with these steps:
      </para>
      <orderedlist xml:id="ol_kbr_2zp_mx">
       <listitem>
        <para>
         Log in to the &lcm;.
        </para>
       </listitem>
       <listitem>
        <para>
         Run the Swift deploy playbook against the affected node, which will
         format the wiped file system:
        </para>
<screen>cd ~/scratch/ansible/next/ardana/ansible/
ansible-playbook -i hosts/verb_hosts swift-deploy.yml --limit &lt;hostname&gt;</screen>
       </listitem>
      </orderedlist>
     </entry>
    </row>
    <row>
     <entry>Swift object replicator last completed in 24 hours</entry>
     <entry>Alarms if an object-replicator process did not complete a replication cycle within
            the last 24 hours</entry>
     <entry>This can indicate that the object-replication process is stuck.</entry>
     <entry>
      <para>
       SSH to the affected host and restart the process with this command:
      </para>
<screen>sudo systemctl restart swift-account-replicator</screen>
      <para>
       Another cause of this problem may be that a file system may be corrupt.
       Look for sign of this in these logs on the affected node:
      </para>
<screen>/var/log/swift/swift.log
/var/log/kern.log</screen>
      <para>
       The file system may need to be wiped, contact &serviceteam; for advice on
       the best way to do that if needed. You can then reformat the file system
       with these steps:
      </para>
      <orderedlist xml:id="ol_lbr_2zp_mx">
       <listitem>
        <para>
         Log in to the &lcm;.
        </para>
       </listitem>
       <listitem>
        <para>
         Run the Swift deploy playbook against the affected node, which will
         format the wiped file system:
        </para>
<screen>cd ~/scratch/ansible/next/ardana/ansible/
ansible-playbook -i hosts/verb_hosts swift-deploy.yml --limit &lt;hostname&gt;</screen>
       </listitem>
      </orderedlist>
     </entry>
    </row>
    <row>
     <entry>Swift configuration file ownership</entry>
     <entry>Alarms if files/directories in <literal>/etc/swift</literal> are not owned by
            Swift.</entry>
     <entry>For files in <literal>/etc/swift</literal>, somebody may have manually edited or
            created a file.</entry>
     <entry>
      <para>
       For files in <literal>/etc/swift</literal>, use this command to change
       the file ownership:
      </para>
<screen>sudo chown swift.swift /etc/swift/, /etc/swift/*</screen>
     </entry>
    </row>
    <row>
     <entry>Swift data filesystem ownership</entry>
     <entry>Alarms if files/directories in <literal>/srv/node</literal> are not owned by
            Swift.</entry>
     <entry>For directories in <literal>/srv/node/*</literal>, it may happen that the root
            partition was reimaged or reinstalled and the UID assigned to the Swift user changes.
            The directories and files are then not owned by the UID assigned to the Swift
            user.</entry>
     <entry>For directories and files in <literal>/srv/node/*</literal>, compare the swift UID
            of this system and other systems and the UID of the owner of
              <literal>/srv/node/*</literal>. If possible, make the UID of the Swift user match the
            directories/files. Otherwise, change the ownership of all files and directories under
            the <literal>/srv/node</literal> path using a similar command as above.</entry>
    </row>
    <row>
     <entry>Drive URE errors detected</entry>
     <entry>Alarms if <literal>swift-drive-audit</literal> reports an unrecoverable read error
            on a drive used by the Swift service.</entry>
     <entry>An unrecoverable read error has occurred when Swift attempted to access a
            directory.</entry>
     <entry>
      <para>
       The UREs reported only apply to file system metadata (that is, directory
       structures). For UREs in object files, the Swift system automatically
       deletes the file and replicates a fresh copy from one of the other
       replicas.
      </para>
      <para>
       UREs are a normal feature of large disk drives. It does not mean that
       the drive has failed. However, if you get regular UREs on a specific
       drive, then this may indicate that the drive has indeed failed and
       should be replaced.
      </para>
      <para>
       You can use standard XFS repair actions to correct the UREs in the file
       system.
      </para>
      <para>
       If the XFS repair fails, you should wipe the GPT table as follows (where
       &lt;drive_name&gt; is replaced by the actual drive name):
      </para>
<screen>sudo dd if=/dev/zero of=/dev/sd&lt;drive_name&gt; bs=$((1024*1024)) count=1</screen>
      <para>
       Then, follow the steps below which will reformat the drive, remount it,
       and restart Swift services on the affected node.
      </para>
      <orderedlist xml:id="ol_mbr_2zp_mx">
       <listitem>
        <para>
         Log in to the &lcm;.
        </para>
       </listitem>
       <listitem>
        <para>
         Run the Swift reconfigure playbook, specifying the affected node:
        </para>
<screen>cd ~/scratch/ansible/next/ardana/ansible/
ansible-playbook -i hosts/verb_hosts _swift-configure.yml --limit &lt;hostname&gt;</screen>
       </listitem>
      </orderedlist>
      <para>
       It is safe to reformat drives containing Swift data because Swift
       maintains other copies of the data (usually, Swift is configured to have
       three replicas of all data).
      </para>
     </entry>
    </row>
    <row>
     <entry>Swift service</entry>
     <entry>Alarms if a Swift process, specified by the <literal>component</literal> field, is
            not running. </entry>
     <entry>A daemon specified by the <literal>component</literal> dimension on the host
            specified by the <literal>hostname</literal> dimension has stopped running.</entry>
     <entry>
      <para>
       Examine the <literal>/var/log/swift/swift.log</literal> file for
       possible error messages related the Swift process. The process in
       question is listed in the alarm dimensions in the
       <literal>component</literal> dimension.
      </para>
      <para>
       Restart Swift processes by running the
       <literal>swift-start.yml</literal> playbook, with these steps:
      </para>
      <orderedlist xml:id="ol_nbr_2zp_mx">
       <listitem>
        <para>
         Log in to the &lcm;.
        </para>
       </listitem>
       <listitem>
        <para>
         Run the Swift start playbook against the affected host:
        </para>
<screen>cd ~/scratch/ansible/next/ardana/ansible/
ansible-playbook -i hosts/verb_hosts swift-start.yml --limit &lt;hostname&gt;</screen>
       </listitem>
      </orderedlist>
     </entry>
    </row>
    <row>
     <entry>Swift filesystem mount point status</entry>
     <entry>Alarms if a file system/drive used by Swift is not correctly mounted.</entry>
     <entry>
      <para>
       The device specified by the <literal>device</literal> dimension is not
       correctly mounted at the mountpoint specified by the
       <literal>mount</literal> dimension.
      </para>
      <para>
       The most probable cause is that the drive has failed or that it had a
       temporary failure during the boot process and remained unmounted.
      </para>
      <para>
       Other possible causes are a file system corruption that prevents the
       device from being mounted.
      </para>
     </entry>
     <entry>
      <para>
       Reboot the node and see if the file system remains unmounted.
      </para>
      <para>
       If the file system is corrupt, see the process used for the "Drive URE
       errors" alarm to wipe and reformat the drive.
      </para>
     </entry>
    </row>
    <row>
     <entry>Swift uptime-monitor status</entry>
     <entry>Alarms if the swiftlm-uptime-monitor has errors using Keystone
            (keystone-get-token), Swift (rest-api) or Swift's healthcheck.</entry>
     <entry>The swiftlm-uptime-monitor cannot get a token from Keystone or cannot get a
            successful response from the Swift Object-Storage API.</entry>
     <entry>
      <para>
       Check that the Keystone service is running:
      </para>
      <orderedlist xml:id="ol_obr_2zp_mx">
       <listitem>
        <para>
         Log in to the &lcm;.
        </para>
       </listitem>
       <listitem>
        <para>
         Check the status of the Keystone service:
        </para>
<screen>cd ~/scratch/ansible/next/ardana/ansible/
ansible-playbook -i hosts/verb_hosts keystone-status.yml</screen>
       </listitem>
       <listitem>
        <para>
         If it is not running, start the service:
        </para>
<screen>cd ~/scratch/ansible/next/ardana/ansible/
ansible-playbook -i hosts/verb_hosts keystone-start.yml</screen>
       </listitem>
       <listitem>
        <para>
         Contact the support team if further assistance troubleshooting the
         Keystone service is needed.
        </para>
       </listitem>
      </orderedlist>
      <para>
       Check that Swift is running:
      </para>
      <orderedlist xml:id="ol_pbr_2zp_mx">
       <listitem>
        <para>
         Log in to the &lcm;.
        </para>
       </listitem>
       <listitem>
        <para>
         Check the status of the Keystone service:
        </para>
<screen>cd ~/scratch/ansible/next/ardana/ansible/
ansible-playbook -i hosts/verb_hosts swift-status.yml</screen>
       </listitem>
       <listitem>
        <para>
         If it is not running, start the service:
        </para>
<screen>cd ~/scratch/ansible/next/ardana/ansible/
ansible-playbook -i hosts/verb_hosts swift-start.yml</screen>
       </listitem>
      </orderedlist>
      <para>
       Restart the swiftlm-uptime-montitor as follows:
      </para>
      <orderedlist xml:id="ol_qbr_2zp_mx">
       <listitem>
        <para>
         Log into the first server running the swift-proxy-server service. Use
         this playbook below to determine whcih host this is:
        </para>
<screen>cd ~/scratch/ansible/next/ardana/ansible/
ansible-playbook -i hosts/verb_hosts swift-status.yml --limit SWF-PRX[0]</screen>
       </listitem>
       <listitem>
        <para>
         Restart the swiftlm-uptime-montitor with this command:
        </para>
<screen>sudo systemctl restart swiftlm-uptime-monitor</screen>
       </listitem>
      </orderedlist>
     </entry>
    </row>
    <row>
     <entry>Swift Keystone server connect</entry>
     <entry>Alarms if a socket cannot be opened to the Keystone service (used for token
            validation)</entry>
     <entry>The Identity service (Keystone) server may be down. Another possible cause is that
            the network between the host reporting the problem and the Keystone server or the
              <literal>haproxy</literal> process is not forwarding requests to Keystone.</entry>
     <entry>The <literal>URL</literal> dimension contains the name of the virtual IP address.
            Use cURL or a similar program to confirm that a connection can or cannot be made to the
            virtual IP address. Check that <literal>haproxy</literal> is running. Check that the
            Keystone service is working.</entry>
    </row>
    <row>
     <entry>Swift service listening on ip and port</entry>
     <entry>Alarms when a swift service is not listening on the correct port or ip.</entry>
     <entry>The Swift service may be down.</entry>
     <entry>
      <para>
       Verify the status of the Swift service on the affected host, as
       specified by the <literal>hostname</literal> dimension.
      </para>
      <orderedlist xml:id="ol_l4w_tys_lx">
       <listitem>
        <para>
         Log in to the &lcm;.
        </para>
       </listitem>
       <listitem>
        <para>
         Run the Swift status playbook to confirm status:
        </para>
<screen>cd ~/scratch/ansible/next/ardana/ansible/
ansible-playbook -i hosts/verb_hosts swift-status.yml --limit &lt;hostname&gt;</screen>
       </listitem>
      </orderedlist>
      <para>
       If an issue is determined, you can stop and restart the Swift service
       with these steps:
      </para>
      <orderedlist xml:id="ol_rbr_2zp_mx">
       <listitem>
        <para>
         Log in to the &lcm;.
        </para>
       </listitem>
       <listitem>
        <para>
         Stop the Swift service on the affected host:
        </para>
<screen>cd ~/scratch/ansible/next/ardana/ansible/
ansible-playbook -i hosts/verb_hosts swift-stop.yml --limit &lt;hostname&gt;</screen>
       </listitem>
       <listitem>
        <para>
         Restart the Swift service on the affected host:
        </para>
<screen>cd ~/scratch/ansible/next/ardana/ansible/
ansible-playbook -i hosts/verb_hosts swift-start.yml --limit &lt;hostname&gt;</screen>
       </listitem>
      </orderedlist>
     </entry>
    </row>
    <row>
     <entry>Swift rings checksum</entry>
     <entry>Alarms if the swift rings checksums do not match on all hosts.</entry>
     <entry>
      <para>
       The Swift ring files must be the same on every node. The files are
       located in <literal>/etc/swift/*.ring.gz</literal>
      </para>
      <para>
       If you have just changed any of the rings and you are still deploying
       the change, it is normal for this alarm to trigger.
      </para>
     </entry>
     <entry>
      <para>
       If you have just changed any of your Swift rings, if you wait until the
       changes complete then this alarm will likely clear on its own. If it
       does not, then continue with these steps.
      </para>
      <para>
       Use <literal>sudo swift-recon --md5</literal> to find which node has
       outdated rings.
      </para>
      <para>
       Run the <literal>swift-reconfigure.yml</literal> playbook, using the
       steps below. This should deploy the same set of rings to every node.
      </para>
      <orderedlist xml:id="ol_sbr_2zp_mx">
       <listitem>
        <para>
         Log in to the &lcm;.
        </para>
       </listitem>
       <listitem>
        <para>
         Run the Swift start playbook against the affected host:
        </para>
<screen>cd ~/scratch/ansible/next/ardana/ansible/
ansible-playbook -i hosts/verb_hosts swift-reconfigure.yml</screen>
       </listitem>
      </orderedlist>
     </entry>
    </row>
    <row>
     <entry>Swift memcached server connect</entry>
     <entry>Alarms if a socket cannot be opened to the specified memcached server.</entry>
     <entry>The server may be down. The memcached deamon running the server may have
            stopped.</entry>
     <entry>
      <para>
       If the server is down, restart it.
      </para>
      <para>
       If memcached has stopped, you can restart it by using the
       <literal>memcached-start.yml</literal> playbook, using the steps below.
       If this fails, rebooting the node will restart the process.
      </para>
      <orderedlist xml:id="ol_tbr_2zp_mx">
       <listitem>
        <para>
         Log in to the &lcm;.
        </para>
       </listitem>
       <listitem>
        <para>
         Run the memcached start playbook against the affected host:
        </para>
<screen>cd ~/scratch/ansible/next/ardana/ansible/
ansible-playbook -i hosts/verb_hosts memcached-start.yml --limit &lt;hostname&gt;</screen>
       </listitem>
      </orderedlist>
      <para>
       If the server is running and memcached is running, there may be a
       network problem blocking port 11211.
      </para>
      <para>
       If you see sporadic alarms on different servers, the system may be
       running out of resources. Contact HPE Support for advice.
      </para>
     </entry>
    </row>
    <row>
     <entry>Swift individual disk usage exceeds 80%</entry>
     <entry>Alarms when a disk drive used by Swift exceeds 80% utilization.</entry>
     <entry>Generally all disk drives will fill roughly at the same rate. If an individual disk
            drive becomes filled faster than other drives it can indicate a problem with the
            replication process.</entry>
     <entry>
      <para>
       If many/most of your disk drives are 80% full you need to add more nodes
       to your system or delete existing objects.
      </para>
      <para>
       If one disk drive is noticeably (more than 30%) more utilized than the
       average of other disk drives, you should check that Swift processes are
       working on the server (use the steps below) and also look for alarms
       related to the host. Otherwise continue to monitor the situation.
      </para>
      <orderedlist xml:id="ol_ubr_2zp_mx">
       <listitem>
        <para>
         Log in to the &lcm;.
        </para>
       </listitem>
       <listitem>
        <para>
         Run the Swift status:
        </para>
<screen>cd ~/scratch/ansible/next/ardana/ansible/
ansible-playbook -i hosts/verb_hosts swift-status.yml</screen>
       </listitem>
      </orderedlist>
     </entry>
    </row>
    <row>
     <entry>Swift individual disk usage exceeds 90%</entry>
     <entry>Alarms when a disk drive used by Swift exceeds 90% utilization.</entry>
     <entry>Generally all disk drives will fill roughly at the same rate. If an individual disk
            drive becomes filled faster than other drives it can indicate a problem with the
            replication process.</entry>
     <entry>
      <para>
       If one disk drive is noticeably (more than 30%) more utilized than the
       average of other disk drives, you should check that Swift processes are
       working on the server, using these steps:
      </para>
      <orderedlist xml:id="ol_vbr_2zp_mx">
       <listitem>
        <para>
         Log in to the &lcm;.
        </para>
       </listitem>
       <listitem>
        <para>
         Run the Swift status:
        </para>
<screen>cd ~/scratch/ansible/next/ardana/ansible/
ansible-playbook -i hosts/verb_hosts swift-status.yml</screen>
       </listitem>
      </orderedlist>
      <para>
       Also look for alarms related to the host. An individual disk drive
       filling can indicate a problem with the replication process.
      </para>
      <para>
       Restart Swift on that host using the <literal>--limit</literal> argument
       to target the host:
      </para>
      <orderedlist xml:id="ol_wbr_2zp_mx">
       <listitem>
        <para>
         Log in to the &lcm;.
        </para>
       </listitem>
       <listitem>
        <para>
         Stop the Swift service:
        </para>
<screen>cd ~/scratch/ansible/next/ardana/ansible/
ansible-playbook -i hosts/verb_hosts swift-stop.yml --limit &lt;hostname&gt;</screen>
       </listitem>
       <listitem>
        <para>
         Start the Swift service back up:
        </para>
<screen>cd ~/scratch/ansible/next/ardana/ansible/
ansible-playbook -i hosts/verb_hosts swift-start.yml --limit &lt;hostname&gt;</screen>
       </listitem>
      </orderedlist>
      <para>
       If the utilization does not return to similar values as other disk
       drives, you can reformat the disk drive. You should only do this if the
       average utilization of all disk drives is less than 80%. To format a
       disk drive contact HPE Support for instructions.
      </para>
     </entry>
    </row>
    <row>
     <entry>Swift total disk usage exceeds 80%</entry>
     <entry>Alarms when the average disk utilization of Swift disk drives exceeds 80%
            utilization.</entry>
     <entry>The number and size of objects in your system is beginning to fill the available
            disk space. Account and container storage is included in disk utilization. However, this
            generally consumes 1-2% of space compared to objects, so object storage is the dominate
            consumer of disk space.</entry>
     <entry>
      <para>
       You need to add more nodes to your system or delete existing objects to
       remain under 80% utilization.
      </para>
      <note>
       <para>
        If you delete a project/account, the objects in that account are not
        removed until a week later by the <literal>account-reaper</literal>
        process, so this is not a good way of quickly freeing up space.
       </para>
      </note>
     </entry>
    </row>
    <row>
     <entry>Swift total disk usage exceeds 90%</entry>
     <entry>Alarms when the average disk utilization of Swift disk drives exceeds 90%
            utilization.</entry>
     <entry>The number and size of objects in your system is beginning to fill the available
            disk space. Account and container storage is included in disk utilization. However, this
            generally consumes 1-2% of space compared to objects, so object storage is the dominate
            consumer of disk space.</entry>
     <entry>
      <para>
       If your disk drives are 90% full you must immediately stop all
       applications that put new objects into the system. At that point you can
       either delete objects or add more servers.
      </para>
      <para>
       Using the steps below, you should also set the
       <literal>fallocate_reserve</literal> value to a value higher than the
       currently available space on disk drives. This will prevent more objects
       being created.
      </para>
      <orderedlist xml:id="ol_xbr_2zp_mx">
       <listitem>
        <para>
         Log in to the &lcm;.
        </para>
       </listitem>
       <listitem>
        <para>
         Edit the configuration files below and change the value for
         <literal>fallocate_reserve</literal> to a value higher than the
         currently available space on the disk drives:
        </para>
<screen>~/openstack/my_cloud/config/swift/account-server.conf.j2
~/openstack/my_cloud/config/swift/container-server.conf.j2
~/openstack/my_cloud/config/swift/object-server.conf.j2</screen>
       </listitem>
       <listitem>
        <para>
         Commit the changes to git:
        </para>
<screen>git add -A
git commit -a -m "changing Swift fallocate_reserve value"</screen>
       </listitem>
       <listitem>
        <para>
         Run the configuration processor:
        </para>
<screen>cd ~/openstack/ardana/ansible
ansible-playbook -i hosts/localhost config-processor-run.yml</screen>
       </listitem>
       <listitem>
        <para>
         Update your deployment directory:
        </para>
<screen>cd ~/openstack/ardana/ansible
ansible-playbook -i hosts/localhost ready-deployment.yml</screen>
       </listitem>
       <listitem>
        <para>
         Run the Swift reconfigure playbook to deploy the change:
        </para>
<screen>cd ~/scratch/ansible/next/ardana/ansible/
ansible-playbook -i hosts/verb_hosts swift-reconfigure.yml</screen>
       </listitem>
      </orderedlist>
      <para>
       If you allow your file systems to become full, you will be unable to
       delete objects or add more nodes to the system. This is because the
       system needs some free space to handle the replication process when
       adding nodes. With no free space, the replication process cannot work.
      </para>
     </entry>
    </row>
    <row>
     <entry>Swift service per-minute availability</entry>
     <entry>Alarms if the swift service reports unavailable for the previous minute.</entry>
     <entry>The <literal>swiftlm-uptime-monitor</literal> service runs on the first proxy
            server. It monitors the Swift endpoint and reports latency data. If the endpoint stops
            reporting, it generates this alarm.</entry>
     <entry>
      <para>
       There are many reasons why the endpoint may stop running. Check:
      </para>
      <itemizedlist xml:id="ul_ghg_4hc_4v">
       <listitem>
        <para>
         Is <literal>haproxy</literal> running on the control nodes?
        </para>
       </listitem>
       <listitem>
        <para>
         Is <literal>swift-proxy-server</literal> running on the Swift proxy
         servers?
        </para>
       </listitem>
      </itemizedlist>
     </entry>
    </row>
    <row>
     <entry>Swift rsync connect</entry>
     <entry>Alarms if a socket cannot be opened to the specified rsync server</entry>
     <entry>The rsync daemon on the specified node cannot be contacted. The most probable cause
            is that the node is down. The rsync service might also have been stopped on the
            node.</entry>
     <entry>
      <para>
       Reboot the server if it is down.
      </para>
      <para>
       Attempt to restart rsync with this command:
      </para>
<screen>systemctl restart rsync.service</screen>
     </entry>
    </row>
    <row>
     <entry>Swift smart array controller status</entry>
     <entry>Alarms if there is a failure in the Smart Array.</entry>
     <entry>
      <para>
       The Smart Array or Smart HBA controller has a fault or a component of
       the controller (such as a battery) is failed or caching is disabled.
      </para>
      <para>
       The HPE Smart Storage Administrator (HPE SSA) CLI component will have to
       be installed for SSACLI status to be reported. To download and install
       the SSACLI utility, please refer to: <link
       xlink:href="https://support.hpe.com/hpsc/swd/public/detail?swItemId=MTX_3d16386b418a443388c18da82f"/>
      </para>
     </entry>
     <entry>
      <para>
       Log in to the reported host and run these commands to find out the
       status of the controllers:
      </para>
<screen>sudo hpssacli
=&gt; controller show all detail</screen>
      <para>
       For hardware failures (such as failed battery), replace the failed
       component. If the cache is disabled, reenable the cache.
      </para>
     </entry>
    </row>
    <row>
     <entry>Swift physical drive status</entry>
     <entry>Alarms if there is a failure in the Physical Drive.</entry>
     <entry>A disk drive on the server has failed or has warnings.</entry>
     <entry>
      <para>
       Log in to the reported and run these commands to find out the status of
       the drive:
      </para>
<screen>sudo hpssacli
=&gt; ctrl slot=1 pd all show</screen>
      <para>
       Replace any broken drives.
      </para>
     </entry>
    </row>
    <row>
     <entry>Swift logical drive status</entry>
     <entry>Alarms if there is a failure in the Logical Drive.</entry>
     <entry>A LUN on the server is degraded or has failed.</entry>
     <entry>
      <para>
       Log in to the reported host and run these commands to find out the
       status of the LUN:
      </para>
<screen>sudo hpssacli
=&gt; ctrl slot=1 ld all show
=&gt; ctrl slot=1 pd all show</screen>
      <para>
       Replace any broken drives.
      </para>
     </entry>
    </row>
    <row>
     <entry>Process Check</entry>
     <entry>Alarms when the specified process is not running.</entry>
     <entry>If the <literal>service</literal> dimension is <literal>object-store</literal>, see
            the description of the "Swift Service" alarm for possible causes.</entry>
     <entry>If the <literal>service</literal> dimension is <literal>object-storage</literal>,
            see the description of the "Swift Service" alarm for possible mitigation tasks.</entry>
    </row>
    <row>
     <entry>HTTP Status</entry>
     <entry>Alarms when the specified HTTP endpoint is down or not reachable.</entry>
     <entry>If the <literal>service</literal> dimension is <literal>object-store</literal>, see
            the description of the "Swift host socket connect" alarm for possible causes.</entry>
     <entry>If the <literal>service</literal> dimension is <literal>object-storage</literal>,
            see the description of the "Swift host socket connect" alarm for possible mitigation
            tasks.</entry>
    </row>
    <row>
     <entry>Service Log Directory Size</entry>
     <entry>Service log directory consuming more disk than its quota.</entry>
     <entry>This could be due to a service set to <literal>DEBUG</literal> instead of
              <literal>INFO</literal> level. Another reason could be due to a repeating error
            message filling up the log files. Finally, it could be due to log rotate not configured
            properly so old log files are not being deleted properly.</entry>
     <entry>Find the service that is consuming too much disk space. Look at the logs. If
              <literal>DEBUG</literal> log entries exist, set the logging level to
              <literal>INFO</literal>. If the logs are repeatedly logging an error message, do what
            is needed to resolve the error. If old log files exist, configure log rotate to remove
            them. You could also choose to remove old log files by hand after backing them up if
            needed.</entry>
    </row>
    <row>
     <entry morerows="8">block-storage</entry>
     <entry>Process Check</entry>
     <entry>
      <para>
       Separate alarms for each of these Cinder services, specified by the
       <literal>component</literal> dimension:
      </para>
      <itemizedlist xml:id="ul_ybr_2zp_mx">
       <listitem>
        <para>
         cinder-api
        </para>
       </listitem>
       <listitem>
        <para>
         cinder-backup
        </para>
       </listitem>
       <listitem>
        <para>
         cinder-scheduler
        </para>
       </listitem>
       <listitem>
        <para>
         cinder-volume
        </para>
       </listitem>
      </itemizedlist>
     </entry>
     <entry>Process crashed.</entry>
     <entry>
      <para>
       Restart the process on the affected node. Review the associated logs.
      </para>
      <orderedlist xml:id="ol_yhz_rpk_pw">
       <listitem>
        <para>
         Log in to the &lcm;.
        </para>
       </listitem>
       <listitem>
        <para>
         Run the cinder-start.yml playbook to start the process back up:
        </para>
<screen>cd ~/scratch/ansible/next/ardana/ansible
ansible-playbook -i hosts/verb_hosts cinder-start.yml --limit &lt;hostname&gt;</screen>
        <note>
         <para>
          The <literal>--limit &lt;hostname&gt;</literal> switch is optional.
          If it is included, then the <literal>&lt;hostname&gt;</literal> you
          should use is the host where the alarm was raised.
         </para>
        </note>
       </listitem>
      </orderedlist>
     </entry>
    </row>
    <row>
     <entry>Process Check</entry>
     <entry>
      <para>
       Alarms when the specified process is not running.
      </para>
<screen>process_name=cinder-backup</screen>
     </entry>
     <entry>Process crashed.</entry>
     <entry>Alert may be incorrect if the service has migrated. Validate that the service is
            intended to be running on this node before restarting the service. Review the associated
            logs.</entry>
    </row>
    <row>
     <entry>Process Check</entry>
     <entry>
      <para>
       Alarms when the specified process is not running.
      </para>
<screen>process_name=cinder-scheduler</screen>
     </entry>
     <entry>Process crashed.</entry>
     <entry>
      <para>
       Restart the process on the affected node. Review the associated logs.
      </para>
      <orderedlist xml:id="ol_zbr_2zp_mx">
       <listitem>
        <para>
         Log in to the &lcm;.
        </para>
       </listitem>
       <listitem>
        <para>
         Run the cinder-start.yml playbook to start the process back up:
        </para>
<screen>cd ~/scratch/ansible/next/ardana/ansible
ansible-playbook -i hosts/verb_hosts cinder-start.yml --limit &lt;hostname&gt;</screen>
        <note>
         <para>
          The <literal>--limit &lt;hostname&gt;</literal> switch is optional.
          If it is included, then the <literal>&lt;hostname&gt;</literal> you
          should use is the host where the alarm was raised.
         </para>
        </note>
       </listitem>
      </orderedlist>
     </entry>
    </row>
    <row>
     <entry>Process Check</entry>
     <entry>
      <para>
       Alarms when the specified process is not running.
      </para>
<screen>process_name=cinder-volume</screen>
     </entry>
     <entry>Process crashed.</entry>
     <entry>Alert may be incorrect if the service has migrated. Validate that the service is
            intended to be running on this node before restarting the service. Review the associated
            logs.</entry>
    </row>
    <row>
     <entry>Cinder backup running &lt;hostname&gt; check</entry>
     <entry>Cinder backup singleton check.</entry>
     <entry>
      <para>
       Backup process is either:
      </para>
      <itemizedlist xml:id="ul_acr_2zp_mx">
       <listitem>
        <para>
         running on a node it should not be on, or
        </para>
       </listitem>
       <listitem>
        <para>
         not running on a node it should be on
        </para>
       </listitem>
      </itemizedlist>
     </entry>
     <entry>
      <para/>
      <orderedlist xml:id="ol_ztz_mpk_pw">
       <listitem>
        <para>
         Log in to the &lcm;.
        </para>
       </listitem>
       <listitem>
        <para>
         Run this playbook to migrate the service:
        </para>
<screen>cd ~/scratch/ansible/next/ardana/ansible/
ansible-playbook -i hosts/verb_hosts cinder-migrate-volume.yml</screen>
       </listitem>
      </orderedlist>
     </entry>
    </row>
    <row>
     <entry>Cinder volume running &lt;hostname&gt; check</entry>
     <entry>Cinder volume singleton check.</entry>
     <entry>
      <para>
       The <literal>cinder-volume</literal> process is either:
      </para>
      <itemizedlist xml:id="ul_bcr_2zp_mx">
       <listitem>
        <para>
         running on a node it should not be on, or
        </para>
       </listitem>
       <listitem>
        <para>
         not running on a node it should be on
        </para>
       </listitem>
      </itemizedlist>
     </entry>
     <entry>
      <para>
       Run the <literal>cinder-migrate-volume.yml</literal> playbook to migrate
       the volume and backup to correct node:
      </para>
      <orderedlist xml:id="ol_kgy_4pk_pw">
       <listitem>
        <para>
         Log in to the &lcm;.
        </para>
       </listitem>
       <listitem>
        <para>
         Run this playbook to migrate the service:
        </para>
<screen>cd ~/scratch/ansible/next/ardana/ansible/
ansible-playbook -i hosts/verb_hosts cinder-migrate-volume.yml</screen>
       </listitem>
      </orderedlist>
     </entry>
    </row>
    <row>
     <entry>Storage faulty lun check</entry>
     <entry>
      <para>
       Alarms if local LUNs on your HPE servers using smartarray are not OK.
      </para>
     </entry>
     <entry>A LUN on the server is degraded or has failed.</entry>
     <entry>
      <para>
       Log in to the reported host and run these commands to find out the
       status of the LUN:
      </para>
<screen>sudo hpssacli
=&gt; ctrl slot=1 ld all show
=&gt; ctrl slot=1 pd all show</screen>
      <para>
       Replace any broken drives.
      </para>
     </entry>
    </row>
    <row>
     <entry>Storage faulty drive check</entry>
     <entry>
      <para>
       Alarms if the local disk drives on your HPE servers using smartarray
       are not OK.
      </para>
     </entry>
     <entry>A disk drive on the server has failed or has warnings.</entry>
     <entry>
      <para>
       Log in to the reported and run these commands to find out the status of
       the drive:
      </para>
<screen>sudo hpssacli
=&gt; ctrl slot=1 pd all show</screen>
      <para>
       Replace any broken drives.
      </para>
     </entry>
    </row>
    <row>
     <entry>Service Log Directory Size</entry>
     <entry>Service log directory consuming more disk than its quota.</entry>
     <entry>This could be due to a service set to <literal>DEBUG</literal> instead of
              <literal>INFO</literal> level. Another reason could be due to a repeating error
            message filling up the log files. Finally, it could be due to log rotate not configured
            properly so old log files are not being deleted properly.</entry>
     <entry>Find the service that is consuming too much disk space. Look at the logs. If
              <literal>DEBUG</literal> log entries exist, set the logging level to
              <literal>INFO</literal>. If the logs are repeatedly logging an error message, do what
            is needed to resolve the error. If old log files exist, configure log rotate to remove
            them. You could also choose to remove old log files by hand after backing them up if
            needed.</entry>
    </row>
<!---->
<!-- carl 2018-05-25: Ceph alarm table rows; replace with other storage alarm
     information
     <row>
     <entry morerows="20">ceph-storage</entry>
     <entry>Process Check</entry>
     <entry>
      <para>
       Separate alarms for each of these Ceph services, specified by the
       <literal>component</literal> dimension:
      </para>
      <itemizedlist xml:id="ul_vyv_1z5_qv">
       <listitem>
        <para>
         &lt;cluster_name&gt;-osd.&lt;id&gt;
        </para>
       </listitem>
       <listitem>
        <para>
         &lt;cluster_name&gt;-mon.&lt;id&gt;
        </para>
       </listitem>
       <listitem>
        <para>
         &lt;cluster_name&gt;-radosgw.&lt;id&gt;
        </para>
       </listitem>
      </itemizedlist>
     </entry>
     <entry>Process crashed.</entry>
     <entry>
      <para>
       Restart the process on the affected node using these steps:
      </para>
      <orderedlist xml:id="ol_gcr_2zp_mx">
       <listitem>
        <para>
         Log in to the &lcm;.
        </para>
       </listitem>
       <listitem>
        <para>
         Use the Ceph start playbook against the affected node:
        </para>
<screen>cd ~/scratch/ansible/next/ardana/ansible
ansible-playbook -i hosts/verb_hosts ceph-start.yml -/-limit &lt;hostname&gt;</screen>
       </listitem>
      </orderedlist>
     </entry>
    </row>
    <row>
     <entry>Degraded Ceph cluster alert</entry>
     <entry>Alarms if ceph cluster status is not ok.</entry>
     <entry>
      <para>
       The likely cause is one of the following possibilities:
      </para>
      <itemizedlist xml:id="ul_bxh_xkt_2y">
       <listitem>
        <para>
         OSDs being down
        </para>
       </listitem>
       <listitem>
        <para>
         Clock skew
        </para>
       </listitem>
       <listitem>
        <para>
         PG errors
        </para>
       </listitem>
      </itemizedlist>
     </entry>
     <entry>
      <para>
       You should check the following:
      </para>
      <itemizedlist xml:id="ul_l35_xkt_2y">
       <listitem>
        <para>
         cluster overall status, by running the following command:
        </para>
<screen>ceph -s</screen>
       </listitem>
       <listitem>
        <para>
         the reason, which can be found in the command output
        </para>
       </listitem>
      </itemizedlist>
      <para>
       Based on reason, you might have to perform specific actions to correct
       the failure. For e.g. if cluster is in HEALTH_WARN state because one of
       OSD being down then you need to recover failed OSD. In general, you
       might like to perform following steps to triage the reason of failure:
      </para>
      <itemizedlist xml:id="ul_m35_xkt_2y">
       <listitem>
        <para>
         Run <emphasis role="bold">'ceph -s'</emphasis> command to get cluster
         status
        </para>
       </listitem>
       <listitem>
        <para>
         Observing following logs:
         <emphasis role="bold">/var/log/ceph/&lt;cluster
         name&gt;-cluster.log</emphasis> on monitor nodes,
         <emphasis role="bold"
                    >/var/log/ceph/ceph-mon.&lt;hostname&gt;.log</emphasis>
        </para>
       </listitem>
       <listitem>
        <para>
         Run <emphasis role="bold">'ceph osd tree'</emphasis> command to check
         OSD status
        </para>
       </listitem>
      </itemizedlist>
     </entry>
    </row>
    <row>
     <entry>Broken Ceph cluster alert</entry>
     <entry>
      <para>
       Alarms if the Ceph cluster is unusable.
      </para>
      <para>
       Generates alarm only when cluster is in fatal error state, where no I/O
       operations can be performed.
      </para>
     </entry>
     <entry>
      <para>
       The cause may be one or more of the following possibilities:
      </para>
      <itemizedlist xml:id="ul_vdh_zkt_2y">
       <listitem>
        <para>
         All OSDs being down
        </para>
       </listitem>
       <listitem>
        <para>
         Ceph Monitor quorum is broken
        </para>
       </listitem>
       <listitem>
        <para>
         Cluster is full
        </para>
       </listitem>
      </itemizedlist>
     </entry>
     <entry>
      <para>
       Please observe following:
      </para>
      <itemizedlist xml:id="ul_ym4_zkt_2y">
       <listitem>
        <para>
         cluster overall status (by running the command 'ceph -s')
        </para>
       </listitem>
       <listitem>
        <para>
         reason (can be found in command output)
        </para>
       </listitem>
      </itemizedlist>
      <para>
       Based on reason, you might have to perform specific actions to correct
       the failure. For e.g. if cluster is in HEALTH_ERR state because of a
       broken Ceph Monitor quorum, check whether the ceph-monitor services are
       up and running and are reachable from each other. In general, you might
       like to perform following steps to triage the reason of failure:
      </para>
      <itemizedlist xml:id="ul_zm4_zkt_2y">
       <listitem>
        <para>
         Run <emphasis role="bold">'ceph -s'</emphasis> command to get cluster
         status
        </para>
       </listitem>
       <listitem>
        <para>
         Observing following logs:
         <emphasis role="bold">/var/log/ceph/&lt;cluster
         name&gt;-cluster.log</emphasis> on monitor nodes,
         <emphasis role="bold"
                    >/var/log/ceph/ceph-mon.&lt;hostname&gt;.log</emphasis>
        </para>
       </listitem>
       <listitem>
        <para>
         Run <emphasis role="bold">'ceph osd tree'</emphasis> command to check
         OSD status
        </para>
       </listitem>
      </itemizedlist>
     </entry>
    </row>
    <row>
     <entry>Degraded RGW availability alert</entry>
     <entry>Alarms if any of the Ceph RADOS gateways is not reachable.</entry>
     <entry>
      <para>
       The likely cause can be one or more of the following conditions on
       <emphasis
                role="bold">one or more</emphasis> of your
       Ceph RADOS gateway nodes:
      </para>
      <itemizedlist xml:id="ul_uzs_blt_2y">
       <listitem>
        <para>
         Failure in <literal>radosgw</literal> service
        </para>
       </listitem>
       <listitem>
        <para>
         Failure in <literal>apache2</literal> service
        </para>
       </listitem>
       <listitem>
        <para>
         Network connectivity issues
        </para>
       </listitem>
      </itemizedlist>
     </entry>
     <entry>
      <para>
       The mitigation plan depends on the type of error reported by the
       plugin.When a failure is reported, the following parameters should be
       checked:
      </para>
      <orderedlist xml:id="ol_fn3_clt_2y">
       <listitem>
        <para>
         Ensure the following services on the radosgw node are running:
        </para>
       </listitem>
      </orderedlist>
      <itemizedlist xml:id="ul_gn3_clt_2y">
       <listitem>
        <itemizedlist xml:id="ul_hn3_clt_2y">
         <listitem>
          <para>
           radosgw
          </para>
         </listitem>
         <listitem>
          <para>
           apache2
          </para>
         </listitem>
        </itemizedlist>
       </listitem>
      </itemizedlist>
      <para>
       2. Ensure that the radosgw site in apache is enabled.
      </para>
      <para>
       3. Verify the radosgw apache site configuration
       (<emphasis role="bold"
                >/etc/apache2/sites-available/rgw.conf</emphasis>)
       is available/accessible and has
       '<emphasis role="bold">Listen</emphasis>' directive.
      </para>
      <para>
       4. If the issue still persists, check the radosgw service logs (i.e.
       <emphasis
                role="bold">/var/log/ceph/radosgw.log</emphasis>)
       for more details.
      </para>
     </entry>
    </row>
    <row>
     <entry>Broken RGW availability alert</entry>
     <entry>Alarms if all of the Ceph RADOS gateways are not reachable.</entry>
     <entry>
      <para>
       The likely cause can be one or more of the following conditions on
       <emphasis
                role="bold">all</emphasis> of your Ceph RADOS
       gateway nodes:
      </para>
      <itemizedlist xml:id="ul_lq2_2lt_2y">
       <listitem>
        <para>
         Failure in <literal>radosgw</literal> service
        </para>
       </listitem>
       <listitem>
        <para>
         Failure in <literal>apache2</literal> service
        </para>
       </listitem>
       <listitem>
        <para>
         Network connectivity issues
        </para>
       </listitem>
      </itemizedlist>
     </entry>
     <entry>
      <para>
       The mitigation plan depends on the type of error reported by the
       plugin.When a failure is reported, the following parameters should be
       checked:
      </para>
      <orderedlist xml:id="ol_fsl_2lt_2y">
       <listitem>
        <para>
         Ensure the following services on the radosgw node are running:
        </para>
       </listitem>
      </orderedlist>
      <itemizedlist xml:id="ul_gsl_2lt_2y">
       <listitem>
        <itemizedlist xml:id="ul_hsl_2lt_2y">
         <listitem>
          <para>
           radosgw
          </para>
         </listitem>
         <listitem>
          <para>
           apache2
          </para>
         </listitem>
        </itemizedlist>
       </listitem>
      </itemizedlist>
      <para>
       2. Ensure that the radosgw site in apache is enabled.
      </para>
      <para>
       3. Verify the radosgw apache site configuration
       (<emphasis role="bold"
                >/etc/apache2/sites-available/rgw.conf</emphasis>)
       is available/accessible and has
       '<emphasis role="bold">Listen</emphasis>' directive.
      </para>
      <para>
       4. If the issue still persists, check the radosgw service logs (i.e.
       <emphasis
                role="bold">/var/log/ceph/radosgw.log</emphasis>)
       for more details.
      </para>
     </entry>
    </row>
    <row>
     <entry>OSD failure alert</entry>
     <entry>Alarms if any of the OSDs are down.</entry>
     <entry>
      <para>
       The likely cause can be one or more of the following conditions on
       <emphasis
                role="bold">one or more</emphasis> of your
       Ceph OSD nodes:
      </para>
      <itemizedlist xml:id="ul_n4l_flt_2y">
       <listitem>
        <para>
         disk failure
        </para>
       </listitem>
       <listitem>
        <para>
         service failure
        </para>
       </listitem>
       <listitem>
        <para>
         network connectivity failure
        </para>
       </listitem>
       <listitem>
        <para>
         host failure
        </para>
       </listitem>
      </itemizedlist>
     </entry>
     <entry>
      <para>
       Depending on the cause of the failure it will be useful to:
      </para>
      <itemizedlist xml:id="ul_mtt_flt_2y">
       <listitem>
        <para>
         Check node hosting OSD disk is accessible or not.
        </para>
       </listitem>
       <listitem>
        <para>
         Check node connectivity to ensure that monitors can be accessed from
         the node hosting failed OSD disk
        </para>
       </listitem>
       <listitem>
        <para>
         Check disk failure. The system log
         <emphasis role="bold"
                    >/var/log/syslog</emphasis>
         might provide useful information.
        </para>
       </listitem>
       <listitem>
        <para>
         Check service status. The
         <emphasis role="bold"
                    >/var/log/ceph/ceph-osd.&lt;id&gt;.log</emphasis>
         provides useful information for the respective osd services
        </para>
       </listitem>
      </itemizedlist>
      <para>
       Most of cases, OSD going in down state is because of either disk failure
       or service not running as expected. In case of disk failure, you might
       like to replace disk.
      </para>
     </entry>
    </row>
    <row>
     <entry>All OSD failure alert</entry>
     <entry>Alarms if all the OSDs are down.</entry>
     <entry>
      <para>
       The likely cause can be one or more of the following conditions on
       <emphasis
                role="bold">all</emphasis> of your Ceph OSD
       nodes:
      </para>
      <itemizedlist xml:id="ul_y2t_glt_2y">
       <listitem>
        <para>
         disk failure
        </para>
       </listitem>
       <listitem>
        <para>
         service failure
        </para>
       </listitem>
       <listitem>
        <para>
         network connectivity failure
        </para>
       </listitem>
       <listitem>
        <para>
         host failure
        </para>
       </listitem>
      </itemizedlist>
     </entry>
     <entry>
      <para>
       Depending on the cause of the failure it will be useful to:
      </para>
      <itemizedlist xml:id="ul_oyz_glt_2y">
       <listitem>
        <para>
         Check node hosting OSD disk is accessible or not.
        </para>
       </listitem>
       <listitem>
        <para>
         Check node connectivity to ensure that monitors can be accessed from
         the node hosting failed OSD disk
        </para>
       </listitem>
       <listitem>
        <para>
         Check disk failure. The system log
         <emphasis role="bold"
                    >/var/log/syslog</emphasis>
         might provide useful information.
        </para>
       </listitem>
       <listitem>
        <para>
         Check service status. The
         <emphasis role="bold"
                    >/var/log/ceph/ceph-osd.&lt;id&gt;.log</emphasis>
         provides useful information for the respective osd services
        </para>
       </listitem>
      </itemizedlist>
      <para>
       Most of cases, OSD going in down state is because of either disk failure
       or service not running as expected. In case of disk failure, you might
       like to replace disk.
      </para>
     </entry>
    </row>
    <row>
     <entry>Unutilized active OSD alert</entry>
     <entry>Alarms if any Ceph OSDs are up but not part of the cluster.</entry>
     <entry>The likely cause is the cluster has one or more OSDs that are running but not part
            of the cluster, which results in unutilized disk space.</entry>
     <entry>
      <para>
       Run the following command on any ceph monitor node to identify the OSDs
       that are up and out:
      </para>
<screen>ceph osd tree | grep -e 'up.* 0'</screen>
      <para>
       The above command lists out nodes that are up but out of the cluster.You
       can make them a part of the cluster by running the below command:
      </para>
<screen>ceph osd in &lt;osd.id&gt;</screen>
     </entry>
    </row>
    <row>
     <entry>Lower threshold (75% filled) Ceph Cluster capacity alert</entry>
     <entry>Alarms if the used capacity of the Ceph cluster exceeds 75% of the total
            capacity.</entry>
     <entry>Generates alarm if the used capacity of the Ceph cluster exceeds 75% of the total
            capacity.</entry>
     <entry>Increase the cluster capacity by adding more OSDs to the cluster. To add more OSDs
            update the disk input model and run ceph playbooks</entry>
    </row>
    <row>
     <entry>High threshold (85% filled) Ceph Cluster capacity alert</entry>
     <entry>Alarms if the used capacity of the ceph cluster exceeds 85% of the total
            capacity,</entry>
     <entry>Generates alarm if the used capacity of the ceph cluster exceeds 85% of the total
            capacity.</entry>
     <entry>Increase the cluster capacity by adding more OSDs to the cluster. To add more OSDs
            update the disk input model and run ceph playbooks</entry>
    </row>
    <row>
     <entry>Degraded Ceph monitor quorum alert</entry>
     <entry>Alarms if any of the Monitors are not in quorum.</entry>
     <entry>
      <para>
       The likely cause can be one or more of the following conditions on
       <emphasis
                role="bold">one or more</emphasis> of your
       Ceph monitor nodes:
      </para>
      <itemizedlist xml:id="ul_ecv_mlt_2y">
       <listitem>
        <para>
         Monitor services are down
        </para>
       </listitem>
       <listitem>
        <para>
         Network connectivity issues
        </para>
       </listitem>
       <listitem>
        <para>
         Clock skew
        </para>
       </listitem>
      </itemizedlist>
     </entry>
     <entry>
      <para>
       Mitigation plan depends on which state the quorum is in. The message
       shows the monitors which are out of quorum.For the monitors which are
       not in quorum:
      </para>
      <itemizedlist xml:id="ul_r3b_nlt_2y">
       <listitem>
        <para>
         Check the status of monitor services.
        </para>
       </listitem>
       <listitem>
        <para>
         Check the network connectivity between the monitor nodes having
         monitors out of quorum and other monitor nodes.
        </para>
       </listitem>
       <listitem>
        <para>
         Check the logs of the monitors in their respective nodes.
        </para>
       </listitem>
       <listitem>
        <para>
         In case of clock skew, make sure all the nodes running the monitor
         services are in sync with ntp server.
        </para>
       </listitem>
      </itemizedlist>
     </entry>
    </row>
    <row>
     <entry>Broken Ceph monitor quorum alert</entry>
     <entry>Alarms if all of the Monitors are not in quorum.</entry>
     <entry>
      <para>
       The likely cause can be one or more of the following conditions on
       <emphasis
                role="bold">all</emphasis> of your Ceph
       monitor nodes:
      </para>
      <itemizedlist xml:id="ul_ksz_nlt_2y">
       <listitem>
        <para>
         Monitor services are down
        </para>
       </listitem>
       <listitem>
        <para>
         Network connectivity issues
        </para>
       </listitem>
       <listitem>
        <para>
         Clock skew
        </para>
       </listitem>
      </itemizedlist>
     </entry>
     <entry>
      <para>
       Mitigation plan depends on which state the quorum is in. The message
       shows the monitors which are out of quorum.For the monitors which are
       not in quorum:
      </para>
      <itemizedlist xml:id="ul_dxf_4lt_2y">
       <listitem>
        <para>
         Check the status of monitor services.
        </para>
       </listitem>
       <listitem>
        <para>
         Check the network connectivity between the monitor nodes having
         monitors out of quorum and other monitor nodes.
        </para>
       </listitem>
       <listitem>
        <para>
         Check the logs of the monitors in their respective nodes.
        </para>
       </listitem>
       <listitem>
        <para>
         In case of clock skew, make sure all the nodes running the monitor
         services are in sync with ntp server.
        </para>
       </listitem>
      </itemizedlist>
     </entry>
    </row>
    <row>
     <entry>Degraded Ceph monitor connectivity alert</entry>
     <entry>Alarms if <emphasis role="bold">one or more</emphasis> of the Ceph Monitors are not
            reachable from the other Ceph nodes.</entry>
     <entry>
      <para>
       The likely cause can be one or more of the following conditions on
       <emphasis
                role="bold">one or more</emphasis> of your
       Ceph monitor nodes:
      </para>
      <itemizedlist xml:id="ul_h4s_qnt_2y">
       <listitem>
        <para>
         Network connectivity issue between the given node and the monitor
        </para>
       </listitem>
       <listitem>
        <para>
         Monitor service is down
        </para>
       </listitem>
       <listitem>
        <para>
         Firewall issues
        </para>
       </listitem>
      </itemizedlist>
      <para>
       in <emphasis role="bold">one or more</emphasis> monitor nodes
      </para>
     </entry>
     <entry>
      <para>
       The mitigation plan depends on multiple factors, like:
      </para>
      <orderedlist xml:id="ol_l31_rnt_2y">
       <listitem>
        <para>
         Only one node reports a monitor unreachable
        </para>
       </listitem>
       <listitem>
        <para>
         All nodes in the Ceph cluster report a specific monitor as unreachable
        </para>
       </listitem>
      </orderedlist>
      <para>
       Please observe the overall cluster health, as this might point to the
       cause of all Ceph nodes reporting a specific monitor node as
       unreachable. In this scenario, ensure that:
      </para>
      <orderedlist xml:id="ol_m31_rnt_2y">
       <listitem>
        <para>
         Monitor service on the (unreachable) node is up and running.
        </para>
       </listitem>
       <listitem>
        <para>
         Verify the firewall rules on the node to ensure that it is not
         blocking traffic to the Monitor service from all the Ceph nodes.
        </para>
       </listitem>
      </orderedlist>
      <para>
       We can be in this state because of one or combinations of following
       reasons:
      </para>
      <orderedlist xml:id="ol_n31_rnt_2y">
       <listitem>
        <para>
         Monitor node is down
        </para>
       </listitem>
       <listitem>
        <para>
         A specific host is not able to access specific set of monitor
        </para>
       </listitem>
      </orderedlist>
      <para>
       In case of first one, you might be seeing same reported status from
       other nodes as well. In this case, you are strongly advice to diagnose
       specific monitor to check various aspects like network connectivity,
       service status etc. For e.g. if
       <emphasis
                role="bold">levelDB</emphasis> is full then
       monitor service shuts itself down which you can identify by looking at
       log files. In case of second one, you might like to check whether
       network connectivity or firewall is fine between the respective node and
       monitor node. Prime reason of failure is expected to
       be<emphasis role="bold"> network connectivity</emphasis> only in this
       case.
      </para>
     </entry>
    </row>
    <row>
     <entry>Broken Ceph monitor connectivity alert</entry>
     <entry>Alarms if <emphasis role="bold">all</emphasis> of the Ceph Monitors are not
            reachable from the other Ceph nodes.</entry>
     <entry>
      <para>
       The likely cause can be one or more of the following conditions on
       <emphasis
                role="bold">all</emphasis> of your Ceph
       monitor nodes:
      </para>
      <itemizedlist xml:id="ul_owg_snt_2y">
       <listitem>
        <para>
         Network connectivity issue between the specified node and the monitor
        </para>
       </listitem>
       <listitem>
        <para>
         Monitor service is down
        </para>
       </listitem>
       <listitem>
        <para>
         Firewall issues
        </para>
       </listitem>
      </itemizedlist>
     </entry>
     <entry>
      <para>
       The mitigation plan depends on multiple factors, like:
      </para>
      <orderedlist xml:id="ol_ogp_snt_2y">
       <listitem>
        <para>
         Only one node reports a monitor unreachable
        </para>
       </listitem>
       <listitem>
        <para>
         All nodes in the Ceph cluster report a specific monitor as unreachable
        </para>
       </listitem>
      </orderedlist>
      <para>
       Please observe the overall cluster health, as this might point to the
       cause of all Ceph nodes reporting a specific monitor node as
       unreachable. In this scenario, ensure that:
      </para>
      <orderedlist xml:id="ol_pgp_snt_2y">
       <listitem>
        <para>
         Monitor service on the (unreachable) node is up and running.
        </para>
       </listitem>
       <listitem>
        <para>
         Verify the firewall rules on the node to ensure that it is not
         blocking traffic to the Monitor service from all the Ceph nodes.
        </para>
       </listitem>
      </orderedlist>
      <para>
       We can be in this state because of one or combinations of following
       reasons:
      </para>
      <orderedlist xml:id="ol_qgp_snt_2y">
       <listitem>
        <para>
         Monitor node is down
        </para>
       </listitem>
       <listitem>
        <para>
         A specific host is not able to access specific set of monitor
        </para>
       </listitem>
      </orderedlist>
      <para>
       In case of first one, you might be seeing same reported status from
       other nodes as well. In this case, you are strongly advice to diagnose
       specific monitor to check various aspects like network connectivity,
       service status etc. For e.g. if
       <emphasis
                role="bold">levelDB</emphasis> is full then
       monitor service shuts itself down which you can identify by looking at
       log files. In case of second one, you might like to check whether
       network connectivity or firewall is fine between the respective node and
       monitor node. Prime reason of failure is expected to
       be<emphasis role="bold"> network connectivity</emphasis> only in this
       case.
      </para>
     </entry>
    </row>
    <row>
     <entry>Recommended OSD memory alert</entry>
     <entry>
      <para>
       Alarms if the recommended memory configuration for OSD nodes is not met.
      </para>
      <para>
       The recommendation is <emphasis role="bold">1GB RAM</emphasis> per
       <emphasis
                role="bold">1TB of data disk</emphasis>.
      </para>
     </entry>
     <entry>The specified OSD node does not adhere to the minimum recommended memory-disk ratio
            of 1GB RAM per 1TB of data disk.</entry>
     <entry>
      <para>
       Having low RAM is not going to cause system failure and hence it does
       not warrant attention to make the cluster functional. However, note that
       falling short in RAM per host can cause signficant performance
       bottleneck and you might observe following:
      </para>
      <orderedlist xml:id="ol_wtd_5nt_2y">
       <listitem>
        <para>
         Throughput of cluster is low
        </para>
       </listitem>
       <listitem>
        <para>
         Timeout in high I/O traffic scenarios
        </para>
       </listitem>
       <listitem>
        <para>
         Recovering of failed OSD node or rebuiliding of data is taking longer
         duration
        </para>
       </listitem>
      </orderedlist>
      <para>
       If you see the alarm please allocate more RAM for the given host node.
      </para>
     </entry>
    </row>
    <row>
     <entry>Recommended OSD-Journal disk ratio alert</entry>
     <entry>
      <para>
       Alarms if the recommended ratio of OSD to journal disks is not adhered
       to.
      </para>
      <para>
       The recommendation is a maximum of<emphasis role="bold"> 4
       OSDs</emphasis> per journal disk.
      </para>
     </entry>
     <entry>The specified Ceph OSD node does not adhere to the minimum recommended ratio of 4
            OSDs per journal disk.</entry>
     <entry>
      <para>
       If you see the alarms then please check input model to figure out
       following:
      </para>
      <orderedlist xml:id="ol_urk_vnt_2y">
       <listitem>
        <para>
         No disk is left with explicit declaration to use journal disk
        </para>
       </listitem>
       <listitem>
        <para>
         Number of disks referring same journal disk is not exceeding
         <emphasis
                    role="bold">4:1 ratio</emphasis>
        </para>
       </listitem>
      </orderedlist>
      <para>
       It might be required to re-define disk model for OSD nodes and
       reconfigure services using ceph playbooks.
      </para>
     </entry>
    </row>
    <row>
     <entry>Recommended Ceph Public network NIC speed</entry>
     <entry>
      <para>
       Alarms if recommended Ceph public network NIC speed is not met.
      </para>
      <para>
       Additionally if there are non Ceph networks detected on the same NIC the
       alarm is raised.
      </para>
      <para>
       The recommended NIC speeds are:
      </para>
      <para>
       <emphasis role="bold">10Gb/s</emphasis> for dedicated public network
       NICs,
      </para>
      <para>
       <emphasis role="bold">40Gb/s</emphasis> for shared public/private
       network NIC
      </para>
     </entry>
     <entry>
      <para>
       The node specified by the <literal>hostname</literal> dimension's Ceph
       public network NIC speed is less than the minimum recommended speed
      </para>
      <para>
       (or)
      </para>
      <para>
       Non Ceph networks are detected on the same NIC as the public network
       NIC.
      </para>
     </entry>
     <entry>
      <para>
       Having low NIC speeds is not going to cause system failure and hence it
       does not warrant attention to make cluster functional. However, note
       that falling short in NIC speeds can cause
       <emphasis role="bold">signficant
       performance</emphasis><emphasis
                role="bold">bottleneck</emphasis>
       and you might observe following:
      </para>
      <orderedlist xml:id="ol_oy5_wnt_2y">
       <listitem>
        <para>
         Throughput of cluster is low
        </para>
       </listitem>
       <listitem>
        <para>
         Timeout in high I/O traffic scenarios
        </para>
       </listitem>
      </orderedlist>
      <para>
       If you see the alarm please switch to a high speed network interface as
       per recommendations
      </para>
     </entry>
    </row>
    <row>
     <entry>Recommended Ceph Private network NIC speed</entry>
     <entry>
      <para>
       Alarms if recommended Ceph private network NIC speed is not met.
      </para>
      <para>
       Additionally if there are non ceph networks detected on the same NIC the
       alarm is raised.
      </para>
      <para>
       The recommended NIC speeds are:
      </para>
      <para>
       <emphasis role="bold">10Gb/s</emphasis> for dedicated private network
       NICs,
      </para>
      <para>
       <emphasis role="bold">40Gb/s</emphasis> for shared public/private
       network NIC
      </para>
     </entry>
     <entry>
      <para>
       The node specified by the <literal>hostname</literal> dimension's Ceph
       private network NIC speed is less than the minimum recommended speed.
      </para>
      <para>
       (or)
      </para>
      <para>
       Non Ceph networks are detected on the same NIC as the private network
       NIC.
      </para>
     </entry>
     <entry>
      <para>
       Having low NIC speeds is not going to cause system failure and hence it
       does not warrant attention to make cluster functional. However, note
       that falling short in NIC speeds can cause
       <emphasis role="bold">signficant</emphasis><emphasis role="bold"
                >performance</emphasis><emphasis role="bold">bottleneck</emphasis>
       and you might observe following:
      </para>
      <orderedlist xml:id="ol_tcd_ynt_2y">
       <listitem>
        <para>
         Throughput of cluster is low
        </para>
       </listitem>
       <listitem>
        <para>
         Timeout in high I/O traffic scenarios
        </para>
       </listitem>
      </orderedlist>
      <para>
       If you see the alarm please switch to a high speed network interface as
       per recommendations
      </para>
     </entry>
    </row>
    <row>
     <entry>Cephlm probe check</entry>
     <entry>Alarms if the <emphasis role="bold">cephlm-probe</emphasis> tool cannot execute a
            monitoring task</entry>
     <entry>
      <para>
       The likely cause can be one or more of the following conditions on
       <emphasis
                role="bold">one or more</emphasis> of your
       Ceph nodes, specified by the <literal>hostname</literal> dimension:
      </para>
      <itemizedlist xml:id="ul_afg_b4t_2y">
       <listitem>
        <para>
         Ceph configuration errors
        </para>
       </listitem>
       <listitem>
        <para>
         Command timeouts
        </para>
       </listitem>
      </itemizedlist>
     </entry>
     <entry>
      <para>
       Depending on the alarm message it is recommended to:
      </para>
      <itemizedlist xml:id="ul_dxl_b4t_2y">
       <listitem>
        <para>
         Check for any errors in ceph configuration files
        </para>
       </listitem>
       <listitem>
        <para>
         Modify the command timeout (default 30 seconds) by editing the
         configurable parameters in ceph deployment playbooks and
         re-configuring ceph
        </para>
       </listitem>
       <listitem>
        <para>
         In case of ceph commands getting timed out repeatedly, please check if
         "<emphasis role="bold">ceph -s</emphasis>" command is working fine on
         the node. It is observed that when monitor nodes are down the command
         fails due to timeouts. In such cases, you will need to fix the monitor
         issues
        </para>
       </listitem>
      </itemizedlist>
     </entry>
    </row>
    <row>
     <entry>Cephlm monitor check</entry>
     <entry>Alarms if monasca cephlm check plugin fails to run or it cannot collect/process the
            generated metrics.</entry>
     <entry>
      <para>
       The likely cause can be one or more of the following conditions on
       <emphasis
                role="bold">one or more</emphasis> of your
       Ceph monitor nodes, specified by the <literal>hostname</literal>
       dimension:
      </para>
      <itemizedlist xml:id="ul_pcg_c4t_2y">
       <listitem>
        <para>
         Monasca collector failures
        </para>
       </listitem>
       <listitem>
        <para>
         Stale metrics
        </para>
       </listitem>
       <listitem>
        <para>
         Stuck cron jobs
        </para>
       </listitem>
       <listitem>
        <para>
         System time out of sync
        </para>
       </listitem>
      </itemizedlist>
     </entry>
     <entry>
      <para>
       Depending on the alarm message it is recommended to:
      </para>
      <itemizedlist xml:id="ul_wdm_c4t_2y">
       <listitem>
        <para>
         Check the <emphasis role="bold">/var/log/monasca/collector.log
         </emphasis>and look for any issues pertaining to ceph metrics
        </para>
       </listitem>
       <listitem>
        <para>
         Check the contents of
         <emphasis role="bold">/var/cache/cephlm</emphasis> on the node to see
         if the files are getting updated at regular intervals (should not have
         old files &gt; 6 minutes)
        </para>
       </listitem>
       <listitem>
        <para>
         Check <emphasis role="bold">/var/log/syslog</emphasis> for cephlm cron
         job errors
        </para>
       </listitem>
      </itemizedlist>
     </entry>
    </row>
    <row>
     <entry>Service Log Directory Size</entry>
     <entry>Service log directory consuming more disk than its quota.</entry>
     <entry>This could be due to a service set to <literal>DEBUG</literal> instead of
              <literal>INFO</literal> level. Another reason could be due to a repeating error
            message filling up the log files. Finally, it could be due to log rotate not configured
            properly so old log files are not being deleted properly.</entry>
     <entry>Find the service that is consuming too much disk space. Look at the logs. If
              <literal>DEBUG</literal> log entries exist, set the logging level to
              <literal>INFO</literal>. If the logs are repeatedly logging an error message, do what
            is needed to resolve the error. If old log files exist, configure log rotate to remove
            them. You could also choose to remove old log files by hand after backing them up if
            needed.</entry>
    </row>
-->

</tbody>
  </tgroup>
 </informaltable>
</section>
