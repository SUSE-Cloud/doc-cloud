<?xml version="1.0"?>
<!DOCTYPE section [
 <!ENTITY % entities SYSTEM "entities.ent"> %entities;
]>
<section xmlns="http://docbook.org/ns/docbook" xmlns:xi="http://www.w3.org/2001/XInclude"
        xmlns:xlink="http://www.w3.org/1999/xlink" xml:id="recoverrabbit">
 <title>Understanding and Recovering RabbitMQ after Failure</title>
 <para>
  RabbitMQ is the message queue service that runs on each of your controller
  nodes and brokers communication between multiple services in your
  &kw-hos-phrase; cloud environment. It is important for cloud operators to
  understand how different troubleshooting scenarios affect RabbitMQ so they
  can minimize downtime in their environments. We are going to discuss multiple
  scenarios and how it affects RabbitMQ. We will also explain how you can
  recover from them if there are issues.
 </para>

 <section xml:id="idg-all-operations-troubleshooting-recover_rabbit-xml-7">
  <title>How upgrades affect RabbitMQ</title>
  <para>
   There are two types of upgrades within &kw-hos; -- major and minor. The
   effect that the upgrade process has on RabbitMQ depends on these types.
  </para>
  <para>
   A major upgrade is defined by an erlang change or major version upgrade of
   RabbitMQ. A minor upgrade would be an upgrade where RabbitMQ stays within
   the same version, such as v3.4.3 to v.3.4.6.
  </para>
  <para>
   During both types of upgrades there may be minor blips in the authentication
   process of client services as the accounts are recreated.
  </para>
  <para>
   <emphasis role="bold">RabbitMQ during a major upgrade</emphasis>
  </para>
  <para>
   There will be a RabbitMQ service outage while the upgrade is performed.
  </para>
  <para>
   During the upgrade, high availability consistency is compromised -- all but
   the primary node will go down and will be reset, meaning their database
   copies are deleted. The primary node is not taken down until the last step
   and then it is upgrade. The database of users and permissions is maintained
   during this process. Then the other nodes are brought back into the cluster
   and resynchronized.
  </para>
  <para>
   <emphasis role="bold">RabbitMQ during a minor upgrade</emphasis>
  </para>
  <para>
   Minor upgrades are performed node by node. This "rolling" process means
   there should be no overall service outage because each node is taken out of
   its cluster in turn, its database is reset, and then it is added back to the
   cluster and resynchronized.
  </para>
 </section>
 <section xml:id="idg-all-operations-troubleshooting-recover_rabbit-xml-8">
  <title>How RabbitMQ is affected by other operational processes</title>
  <para>
   There are operational tasks, such as <xref linkend="stop_restart"/>, where
   you use the <literal>ardana-stop.yml</literal> and
   <literal>ardana-start.yml</literal> playbooks to gracefully restart your cloud.
   If you use these playbooks, and there are no errors associated with them
   forcing you to troubleshoot further, then RabbitMQ is brought down
   gracefully and brought back up. There is nothing special to note regarding
   RabbitMQ in these normal operational processes.
  </para>
  <para>
   However, there are other scenarios where an understanding of RabbitMQ is
   important when a graceful shutdown did not occur.
  </para>
  <para>
   These examples that follow assume you are using one of the entry-scale
   models where RabbitMQ is hosted on your controller node cluster. If you are
   using a mid-scale model or have a dedicated cluster that RabbitMQ lives on
   you may need to alter the steps accordingly. To determine which nodes
   RabbitMQ is on you can use the <literal>rabbit-status.yml</literal> playbook
   from your &lcm;.
  </para>
<screen>cd ~/scratch/ansible/next/ardana/ansible
ansible-playbook -i hosts/verb_hosts rabbitmq-status.yml</screen>
  <para>
   <emphasis role="bold">Your entire control plane cluster goes down</emphasis>
  </para>
  <para>
   If you have a scenario where all of your controller nodes went down, either
   manually or via another process such as a power outage, then an
   understanding of how RabbitMQ should be brought back up is important. Follow
   these steps to recover RabbitMQ on your controller node cluster in these
   cases:
  </para>
  <procedure>
   <step>
    <para>
     The order in which the nodes went down is key here. Locate the last node
     to go down as this will be used as the primary node when bringing the
     RabbitMQ cluster back up. You can review the timestamps in the
     <literal>/var/log/rabbitmq</literal> log file to determine what the last
     node was.
    </para>
    <note>
     <para>
      The "primary" status of a node is transient, it only applies for the
      duration that this process is running. There is no long-term distinction
      between any of the nodes in your cluster. The primary node is simply
      the one that owns the RabbitMQ configuration database that will be
      synchronized across the cluster.
     </para>
    </note>
   </step>
   <step>
    <para>
     Run the <literal>ardana-start.yml</literal> playbook specifying the primary
     node (aka the last node down determined in the first step):
    </para>
<screen>cd ~/scratch/ansible/next/ardana/ansible
ansible-playbook -i hosts/verb_hosts ardana-start.yml -e rabbit_primary_hostname=&lt;hostname&gt;</screen>
    <note>
     <para>
      The <literal>&lt;hostname&gt;</literal> value will be the "shortname" for
      your node, as found in the <literal>/etc/hosts</literal> file.
     </para>
    </note>
   </step>
  </procedure>
  <para>
   <emphasis role="bold">If one of your controller nodes goes down</emphasis>
  </para>
  <para>
   First step here is to determine whether the controller that went down is the
   primary RabbitMQ host or not. The primary host is going to be the first host
   member in the <literal>FND-RMQ</literal> group in the file below on your
   &lcm;:
  </para>
<screen>~/scratch/ansible/next/ardana/ansible/hosts/verb_hosts</screen>
  <para>
   In this example below, <literal>ardana-cp1-c1-m1-mgmt</literal> would be the
   primary:
  </para>
<screen>[FND-RMQ-ccp-cluster1:children]
ardana-cp1-c1-m1-mgmt
ardana-cp1-c1-m2-mgmt
ardana-cp1-c1-m3-mgmt</screen>
  <para>
   If your primary RabbitMQ controller node has gone down and you need to bring
   it back up, you can follow these steps. In this playbook you are using the
   <literal>rabbit_primary_hostname</literal> parameter to specify the hostname
   for one of the other controller nodes in your environment hosting RabbitMQ,
   which will service as the primary node in the recovery. You will also use
   the <literal>--limit</literal> parameter to specify the controller node you
   are attempting to bring back up.
  </para>
<screen>cd ~/scratch/ansible/next/ardana/ansible
ansible-playbook -i hosts/verb_hosts ardana-start.yml -e rabbit_primary_hostname=&lt;new_primary_hostname&gt; --limit &lt;hostname_of_node_you_are_bringing_up&gt;</screen>
  <para>
   If the node you need to bring back is <emphasis role="bold">not</emphasis>
   the primary RabbitMQ node then you can just run the
   <literal>ardana-start.yml</literal> playbook with the
   <literal>--limit</literal> parameter and your node should recover:
  </para>
<screen>cd ~/scratch/ansible/next/ardana/ansible
ansible-playbook -i hosts/verb_hosts ardana-start.yml --limit &lt;hostname_of_node_you_are_bringing_up&gt;</screen>
  <para>
   <emphasis role="bold">If you are replacing one or more of your controller
   nodes</emphasis>
  </para>
  <para>
   The same general process noted above is used if you are removing or
   replacing one or more of your controller nodes.
  </para>
  <para>
   If your node needs minor hardware repairs, but does not need to be replaced
   with a new node, you should use the <literal>ardana-stop.yml</literal> playbook
   with the <literal>--limit</literal> parameter to stop services on that node
   prior to removing it from the cluster.
  </para>
  <procedure>
   <step>
    <para>
     Log into the &lcm;.
    </para>
   </step>
   <step>
    <para>
     Run the <literal>rabbitmq-stop.yml</literal> playbook, specifying the
     hostname of the node you are removing, which will remove the node from the
     RabbitMQ cluster:
    </para>
<screen>cd ~/scratch/ansible/next/ardana/ansible
ansible-playbook -i hosts/verb_hosts rabbitmq-stop.yml --limit &lt;hostname_of_node_you_are_removing&gt;</screen>
   </step>
   <step>
    <para>
     Run the <literal>ardana-stop.yml</literal> playbook, again specifying the
     hostname of the node you are removing, which will stop the rest of the
     services and prepare it to be removed:
    </para>
<screen>cd ~/scratch/ansible/next/ardana/ansible
ansible-playbook -i hosts/verb_hosts ardana-stop.yml --limit &lt;hostname_of_node_you_are_removing&gt;</screen>
   </step>
  </procedure>
  <para>
   If your node cannot be repaired and needs to be replaced with another
   baremetal node, the RabbitMQ cluster needs any references to the node
   removed from it. This is because RabbitMQ associates a cookie with each node
   in the cluster which is derived, in part, by the specific hardware.
   Replacing a hard drive in a node should be fine, but an exchanged
   motherboard or replacing it with another node entirely will potentially
   trigger a breakage. Under these circumstances, the running RabbitMQ cluster
   must be edited from a running RabbitMQ node with these steps:
  </para>
  <procedure>
   <step>
    <para>
     SSH to a running RabbitMQ cluster node.
    </para>
   </step>
   <step>
    <para>
     Run this command to force the cluster to forget the node you are removing,
     which will remove all refernces to it:
    </para>
<screen>sudo rabbitmqctl forget_cluster_node rabbit@&lt;hostname_of_node_you_are_removing&gt;</screen>
   </step>
   <step>
    <para>
     You can then confirm that the node has been removed with this command:
    </para>
<screen>sudo rabbitmqctl cluster_status</screen>
   </step>
  </procedure>
  <para>
   If the node you are removing/replacing is your primary host then when you
   are adding it to your cluster then you will want to ensure that you specify
   a new primary host when doing so, as follows:
  </para>
<screen>cd ~/scratch/ansible/next/ardana/ansible
ansible-playbook -i hosts/verb_hosts ardana-start.yml -e rabbit_primary_hostname=&lt;new_primary_hostname&gt; --limit &lt;hostname_of_node_you_are_adding&gt;</screen>
  <para>
   If the node you are removing/replacing is not your primary host then you can
   add it as follows:
  </para>
<screen>cd ~/scratch/ansible/next/ardana/ansible
ansible-playbook -i hosts/verb_hosts ardana-start.yml --limit &lt;hostname_of_node_you_are_adding&gt;</screen>
  <para>
   <emphasis role="bold">If one of your controller nodes has rebooted or
   temporarily lost power</emphasis>
  </para>
  <para>
   After a single reboot, RabbitMQ will not automatically restart. This is by
   design to protect your RabbitMQ cluster. To restart RabbitMQ, you should
   follow the process below.
  </para>
  <para>
   If the rebooted node was your primary RabbitMQ host, you will specify a
   different primary hostname using one of the other nodes in your cluster:
  </para>
<screen>cd ~/scratch/ansible/next/ardana/ansible
ansible-playbook -i hosts/verb_hosts ardana-start.yml -e rabbit_primary_hostname=&lt;new_primary_hostname&gt; --limit &lt;hostname_of_node_that_rebooted&gt;</screen>
  <para>
   If the rebooted node was not the primary RabbitMQ host then you can just
   start it back up with this playbook:
  </para>
<screen>cd ~/scratch/ansible/next/ardana/ansible
ansible-playbook -i hosts/verb_hosts ardana-start.yml --limit &lt;hostname_of_node_that_rebooted&gt;</screen>
 </section>
 <section xml:id="recovery">
  <title>Recovering RabbitMQ</title>
  <para>
   In this section we will show you how to check the status of RabbitMQ and how
   to do a variety of disaster recovery procedures.
  </para>
  <para>
   <emphasis role="bold">Verifying the status of RabbitMQ</emphasis>
  </para>
  <para>
   You can verify the status of RabbitMQ on each of your controller nodes by
   using the following steps:
  </para>
  <procedure>
   <step>
    <para>
     Log in to the &lcm;.
    </para>
   </step>
   <step>
    <para>
     Run the <literal>rabbitmq-status.yml</literal> playbook:
    </para>
<screen>cd ~/scratch/ansible/next/ardana/ansible
ansible-playbook -i hosts/verb_hosts rabbitmq-status.yml</screen>
   </step>
   <step>
    <para>
     If all is well, you should see an output similar to the following:
    </para>
<screen>PLAY RECAP ********************************************************************
rabbitmq | status | Check RabbitMQ running hosts in cluster ------------- 2.12s
rabbitmq | status | Check RabbitMQ service running ---------------------- 1.69s
rabbitmq | status | Report status of RabbitMQ --------------------------- 0.32s
-------------------------------------------------------------------------------
Total: ------------------------------------------------------------------ 4.36s
ardana-cp1-c1-m1-mgmt  : ok=2    changed=0    unreachable=0    failed=0
ardana-cp1-c1-m2-mgmt  : ok=2    changed=0    unreachable=0    failed=0
ardana-cp1-c1-m3-mgmt  : ok=2    changed=0    unreachable=0    failed=0</screen>
   </step>
  </procedure>
  <para>
   If one or more of your controller nodes are having RabbitMQ issues then
   continue reading, looking for the scenario that best matches yours.
  </para>
  <para>
   <emphasis role="bold">RabbitMQ recovery after a small network
   outage</emphasis>
  </para>
  <para>
   In the case of a transient network outage, the version of RabbitMQ included
   with &kw-hos-phrase; is likely to recover automatically without any further
   action needed. However, if yours does not and the
   <literal>rabbitmq-status.yml</literal> playbook is reporting an issue then
   use the scenarios below to resolve your issues.
  </para>
  <para>
   <emphasis role="bold">All of your controller nodes have gone down and using
   other methods have not brought RabbitMQ back up</emphasis>
  </para>
  <para>
   If your RabbitMQ cluster is irrecoverable and you need rapid service
   recovery because other methods either cannot resolve the issue or you do not
   have time to investigate more nuanced approaches then we provide a disaster
   recovery playbook for you to use. This playbook will tear down and reset any
   RabbitMQ services. This does have an extreme effect on your services. The
   process will ensure that the RabbitMQ cluster is recreated.
  </para>
  <procedure>
   <step>
    <para>
     Log in to your &lcm;.
    </para>
   </step>
   <step>
    <para>
     Run the RabbitMQ disaster recovery playbook. This generally takes around
     two minutes.
    </para>
<screen>cd ~/scratch/ansible/next/ardana/ansible
ansible-playbook -i hosts/verb_hosts rabbitmq-disaster-recovery.yml</screen>
   </step>
   <step>
    <para>
     Run the reconfigure playbooks for both Cinder (Block Storage) and Heat
     (Orchestration), if those services are present in your cloud. These
     services are affected when the fan-out queues are not recovered correctly.
     The reconfigure generally takes around five minutes.
    </para>
<screen>cd ~/scratch/ansible/next/ardana/ansible
ansible-playbook -i hosts/verb_hosts cinder-reconfigure.yml
ansible-playbook -i hosts/verb_hosts heat-reconfigure.yml
ansible-playbook -i hosts/verb_hosts logging-server-configure.yml</screen>
   </step>
   <step>
    <para>
     If you need to do a safe recovery of all the services in your environment
     then you can use this playbook. This is a more lengthy process as all
     services are inspected.
    </para>
<screen>cd ~/scratch/ansible/next/ardana/ansible
ansible-playbook -i hosts/verb_hosts ardana-reconfigure.yml</screen>
   </step>
  </procedure>
  <para>
   <emphasis role="bold">One of your controller nodes has gone down and using
   other methods have not brought RabbitMQ back up</emphasis>
  </para>
  <para>
   This disaster recovery procedure has the same caveats as the preceding one,
   but the steps differ.
  </para>
  <para>
   If your primary RabbitMQ controller node has gone down and you need to
   perform a disaster recovery, use this playbook from your &lcm;:
  </para>
<screen>cd ~/scratch/ansible/next/ardana/ansible
ansible-playbook -i hosts/verb_hosts rabbitmq-disaster-recovery.yml -e rabbit_primary_hostname=&lt;new_primary_hostname&gt; --limit &lt;hostname_of_node_that_needs_recovered&gt;</screen>
  <para>
   If the controller node is not your primary, you can use this playbook:
  </para>
<screen>cd ~/scratch/ansible/next/ardana/ansible
ansible-playbook -i hosts/verb_hosts rabbitmq-disaster-recovery.yml --limit &lt;hostname_of_node_that_needs_recovered&gt;</screen>
  <para>
   No reconfigure playbooks are needed because all of the fan-out exchanges are
   maintained by the running members of your RabbitMQ cluster.
  </para>
 </section>
</section>
