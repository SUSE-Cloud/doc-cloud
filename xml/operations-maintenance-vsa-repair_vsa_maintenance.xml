<?xml version="1.0"?>
<?xml-stylesheet href="urn:x-suse:xslt:profiling:docbook51-profile.xsl"
 type="text/xml"
 title="Profiling step"?>
<!DOCTYPE section [
 <!ENTITY % entities SYSTEM "entities.ent"> %entities;
]>
<section xmlns="http://docbook.org/ns/docbook" xmlns:xi="http://www.w3.org/2001/XInclude" xmlns:xlink="http://www.w3.org/1999/xlink" xml:id="topic_e32_tm2_rt">
 <title>Repairing a VSA Node</title>
 <para>
  Repairing a VSA node temporarily for maintenance.
 </para>
 <para>
  This process is used when you want to remove a VSA node from a cluster or
  management group for maintenance, using the HPE StoreVirtual Management
  Console (CMC) utility.
 </para>
 <para>
  After maintenance, the VSA node is added back to the cluster.
 </para>
 <section xml:id="idg-all-operations-maintenance-vsa-repair_vsa_maintenance-xml-6">
  <title>Notes</title>
  <para>
   In order for the Block Storage volumes to remain available even after the
   VSA storage node goes offline, you must ensure that the VSA network RAID
   configuration/data protection aspects are taken care of prior to removing a
   VSA node. See the following sections of the HPE StoreVirtual Storage Online
   Help (available in the StoreVirtual Management Console) prior to configuring
   RAID:
  </para>
  <itemizedlist>
   <listitem>
    <para>
     Using disk RAID with Network RAID in a cluster
    </para>
   </listitem>
   <listitem>
    <para>
     Planning the RAID configuration
    </para>
   </listitem>
   <listitem>
    <para>
     Data protection
    </para>
   </listitem>
  </itemizedlist>
  <para>
   You must configure RAID before adding the storage system to the management
   group. You must set the data protection level while creating the volume
   type.
  </para>
  <para>
   See the following pages for more information:
  </para>
  <itemizedlist>
   <listitem>
    <para>
     HPE LeftHand/StoreVirtual driver:
     <link xlink:href="http://docs.openstack.org/liberty/config-reference/content/HP-LeftHand-StoreVirtual-driver.html"/>
    </para>
   </listitem>
   <listitem>
    <para>
     <xref linkend="creating_voltype"/>
    </para>
   </listitem>
  </itemizedlist>
 </section>
 <section>
  <title>Repair a Disk used by VSA</title>
  <para>
   Repairing a storage system allows you to perform maintenance on or repair a
   storage system that contains volumes configured for data protection levels
   other than Network RAID-0. Repairing a storage system has the advantage of
   triggering only one resynchronization of the data on the storage system,
   rather than a complete restripe of the data on the cluster. Resynchronizing
   the data is a shorter operation than a restripe.
  </para>
  <orderedlist>
   <listitem>
    <para>
     [OPTIONAL] If manager is running on the VSA node you are going to repair
     then you have to first stop manager. To stop manager do the following:
    </para>
    <orderedlist>
     <listitem>
      <para>
       Right click on VSA node and select Stop Manager. It will pop-up warning
       "Are you sure you want to stop the manager running on ..".
      </para>
     </listitem>
     <listitem>
      <para>
       Click OK to stop manager.
      </para>
      <para>
       Once the manger is stopped, start a manager on a different VSA node
       where manager is not already running to maintain quorum and the best
       fault tolerance, if necessary. To start manager, right click on VSA node
       and select Start Manager. Please ensure there is an odd number of
       managers running to avoid split brain syndrome.
      </para>
     </listitem>
    </orderedlist>
   </listitem>
   <listitem>
    <para>
     Right-click the storage system, and select Repair Storage System.
    </para>
   </listitem>
   <listitem>
    <para>
     From the Repair Storage System window, select the item that describes the
     problem to solve. See the "Repairing a storage system" section of the
     <emphasis role="bold">HPE StoreVirtual Storage Online Help</emphasis>
     (available in the StoreVirtual Management Console) for more information.
    </para>
   </listitem>
  </orderedlist>
 </section>
 <section>
  <title>Repair NIC of Host Machine Running VSA</title>
  <para>
   Due to network issue, you will lose connectivity to VSA. Thus showing
   Storage system (VSA) offline in CMC. This is more of physical NIC failure of
   Host machine. Repair the NIC of Host machine and reboot.
  </para>
 </section>
 <section>
  <title>Repair VSA Appliance Networking Issues</title>
  <para>
   Due to network issue, you will lose connectivity to VSA. Thus showing
   Storage system (VSA) offline in CMC. Login to VSA node and try to fix
   networking issue using virsh commands.
  </para>
<screen>sudo virsh --help</screen>
 </section>
 <section>
  <title>Excluding the VSA Node while Maintenance</title>
  <para>
   Even after the VSA node is under maintenance, it should continue to remain
   in the<literal> data/servers.yml</literal> file. You should also create a
   file (for example offline-vsa) on the &lcm; with a list of VSA
   nodes that are currently offline, so that these nodes get excluded from any
   further reconfiguration or upgrade. Perform the following steps to remove
   the VSA node for maintenance:
  </para>
  <orderedlist>
   <listitem>
    <para>
     Login to the &lcm;.
    </para>
   </listitem>
   <listitem>
    <para>
     Verify the node that needs to be kept under maintenance from <literal>
     ~/scratch/ansible/next/ardana/ansible/host_vars</literal> directory.
    </para>
<screen>ls ~/scratch/ansible/next/ardana/ansible/host_vars</screen>
   </listitem>
   <listitem>
    <para>
     Create a file (for example: offline-vsa) and enter the node information
     that needs to be placed under maintenance. For example, you want to place
     <literal>ardana-cp1-vsa0004-mgmt</literal> under maintenance, you must use
     the host name along with ! (for example:
     <literal>!ardana-cp1-vsa0004-mgmt</literal>).
    </para>
<screen>cat offline-vsa
!ardana-cp1-vsa0004-mgmt</screen>
   </listitem>
   <listitem>
    <para>
     When performing &productname; lifecycle management
     deployments/reconfigurations/upgrades or any other operation, specify the
     name of this file (prepend @ to the file ) in a --limit option to prevent
     the access of the VSA nodes that are offline till maintenance activity is
     done.
    </para>
<screen>ansible-playbook -i hosts/verb_hosts vsa-status.yml --limit &lt;@file location&gt;</screen>
    <para>
     In the following example offline-vsa file has VSA node name and the file
     is created under <literal>~/</literal>.
    </para>
<screen>ansible-playbook -i hosts/verb_hosts vsa-status.yml --limit @~/offline-vsa</screen>
   </listitem>
  </orderedlist>
 </section>
 <section>
  <title>Host Operating System Corrupted</title>
  <para>
   If operating system of Host machine where VSA appliance is running gets
   corrupted then you have to reimage VSA node and re-deploy VSA appliance on
   this node. VSA node IP and VSA appliance IP remain same. Please consult HPSD
   team for this. OS recovery is performed with HPSD support intervention.
  </para>
 </section>
</section>
