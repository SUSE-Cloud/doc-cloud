<?xml version="1.0"?>
<?xml-stylesheet href="urn:x-suse:xslt:profiling:docbook51-profile.xsl"
 type="text/xml"
 title="Profiling step"?>
<!DOCTYPE section [
 <!ENTITY % entities SYSTEM "entities.ent"> %entities;
]>
<section xmlns="http://docbook.org/ns/docbook" xmlns:xi="http://www.w3.org/2001/XInclude" xmlns:xlink="http://www.w3.org/1999/xlink" xml:id="remove_datadisk_osd">
 <title>Removing a Data Disk from a Ceph OSD Node</title>
 <para>
  Steps for removing data disks from your OSD nodes. To remove a data disks
  from your existing OSD nodes, these steps will show you how to.
 </para>
 <section xml:id="idg-all-operations-maintenance-ceph-remove_osd_datadisk-xml-6">
  <title>Removing a Data Disk from an Object Storage Daemon (OSD) Node</title>
  <orderedlist>
   <listitem>
    <para>
     Log in to the OSD node you want to remove the data disk from.
    </para>
   </listitem>
   <listitem>
    <para>
     Figure out the OSD number for the corresponding disk using this command:
    </para>
<screen>sudo ceph-disk list | grep &lt;disk-to-be-removed&gt; | grep osd</screen>
   </listitem>
   <listitem>
    <para>
     Using the output from the command in the previous step, locate the OSD
     number.
    </para>
    <para>
     In the example below, the OSD number is <literal>4</literal>:
    </para>
<screen>/dev/sdn1 ceph data, active, cluster ceph, osd.4, journal /dev/sdj1</screen>
   </listitem>
   <listitem>
    <para>
     Use the following commands to take the OSD node out of the Ceph cluster
     and observe the cluster rebalance itself:
    </para>
<screen>ceph osd out &lt;osd_number&gt;
ceph -w</screen>
    <para>
     The placement group states will change from
     <literal>active+clean</literal> to <literal>active</literal>, some
     <literal>degraded</literal> objects, and then finally back to
     <literal>active+clean</literal> once the migration completes. You can
     press CTRL+C to exit.
    </para>
   </listitem>
   <listitem>
    <para>
     Use the following commands to stop the OSD service, remove it from the
     CRUSH map, delete its authentication key, and remove the OSD node:
    </para>
<screen>sudo systemctl stop ceph-osd@&lt;osd_number&gt;
ceph osd crush remove osd.&lt;osd_number&gt;
ceph auth del osd.&lt;osd_number&gt;
ceph osd rm &lt;osd_number&gt;</screen>
   </listitem>
   <listitem>
    <para>
     Unmount the OSD data disk:
    </para>
<screen>sudo umount &lt;mount_path_of_datadisk&gt;</screen>
   </listitem>
   <listitem>
    <para>
     This step will remove the OSD's directories and should be performed with
     caution:
    </para>
<screen>sudo rm -rf /var/lib/ceph/osd/&lt;ceph_cluster&gt;-&lt;osd_number&gt;</screen>
    <para>
     where:
    </para>
    <informaltable colsep="1" rowsep="1">
     <tgroup cols="2">
      <colspec colname="c1" colnum="1" colwidth="1.0*"/>
      <colspec colname="c2" colnum="2" colwidth="1.0*"/>
      <thead>
       <row>
        <entry>Variable</entry>
        <entry>Description</entry>
       </row>
      </thead>
      <tbody>
       <row>
        <entry>&lt;ceph_cluster&gt;</entry>
        <entry><para>This should be replaced with the name of your Ceph cluster.</para></entry>
       </row>
       <row>
        <entry>&lt;osd_number&gt;</entry>
        <entry><para>This should be replaced with the number identified in step #3 earlier.</para></entry>
       </row>
      </tbody>
     </tgroup>
    </informaltable>
   </listitem>
   <listitem>
    <para>
     Remove the mount instructions for the same disk(s) from the
     <literal>/etc/fstab</literal> file. For example, to remove the data disk
     <literal>/dev/sde</literal>, find out its UUID by executing this command:
    </para>
<screen>sudo blkid | grep "ceph data"</screen>
    <para>
     Example output:
    </para>
<screen>$ sudo blkid |grep "ceph data"
/dev/sdc1: UUID="45b7c7e8-4f2d-4226-989b-c83e7e9e2596" TYPE="xfs" PARTLABEL="ceph data" PARTUUID="6c37a311-eabf-4872-bd8e-7d426585432e"
/dev/sdf1: UUID="1cfb4de2-fe21-4e20-9a6e-ad45e90949ae" TYPE="xfs" PARTLABEL="ceph data" PARTUUID="209b9af4-8d25-4d60-a430-2934ceca6042"
/dev/sde1: UUID="51c32ca1-0ee1-4c75-bfbc-5d113a4b4402" TYPE="xfs" PARTLABEL="ceph data" PARTUUID="dfcd410d-9c56-4bb1-93c6-496bae29de5f"</screen>
    <para>
     In the example above, the UUID
     <literal>51c32ca1-0ee1-4c75-bfbc-5d113a4b4402</literal> corresponds to the
     data disk being removed (i.e. <literal>/dev/sde</literal>. Remove the line
     in the <literal>/etc/fstab</literal> file having the entry starting with
     the text <literal>UUID="51c32ca1-0ee1-4c75-bfbc-5d113a4b4402"</literal>.
    </para>
   </listitem>
   <listitem>
    <para>
     Delete the journal partition for each OSD number removed and instruct the
     kernel to reload the table as follows:
    </para>
<screen>(echo d; echo &lt;partition-number&gt;; echo w) | sudo fdisk &lt;disk&gt;
sudo partprobe

Example:
In step #3 above, /dev/sdj1 journal partition can be removed by executing below commands:
$ (echo d; echo 1; echo w) | sudo fdisk /dev/sdj
$ sudo partprobe</screen>
   </listitem>
   <listitem>
    <para>
     Repeat steps 1-9 for each OSD node in the Ceph cluster one at a time.
    </para>
   </listitem>
   <listitem>
    <para>
     Log in to the &lcm;.
    </para>
   </listitem>
   <listitem>
    <para>
     Edit the
     <literal>~/openstack/my_cloud/definition/data/disks_osd.yml</literal> file
     and remove the details for the corresponding data disk that you removed
     earlier.
    </para>
    <para>
     Example:
    </para>
<screen>- name: ceph-osd-data-and-journal
  devices:
     - name: /dev/sdn
  consumer:
     name: ceph
     attrs:
     usage: data
     journal_disk: /dev/sdj</screen>
   </listitem>
   <listitem>
    <para>
     Commit your configuration to the local Git repository (see
     <xref linkend="using_git"/>), as follows:
    </para>
<screen>cd ~/openstack/ardana/ansible
git add -A
git commit -m "Removing OSD data disk"</screen>
   </listitem>
   <listitem>
    <para>
     Run the configuration processor:
    </para>
<screen>cd ~/openstack/ardana/ansible
ansible-playbook -i hosts/localhost config-processor-run.yml</screen>
   </listitem>
   <listitem>
    <para>
     Update your deployment directory:
    </para>
<screen>cd ~/openstack/ardana/ansible
ansible-playbook -i hosts/localhost ready-deployment.yml</screen>
   </listitem>
   <listitem>
    <para>
     Check the status of Ceph services with this playbook:
    </para>
<screen>cd ~/scratch/ansible/next/ardana/ansible
ansible-playbook -i hosts/verb_hosts ceph-status.yml</screen>
   </listitem>
  </orderedlist>
 </section>
</section>
