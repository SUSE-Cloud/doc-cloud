<?xml version="1.0"?>
<?xml-stylesheet href="urn:x-suse:xslt:profiling:docbook51-profile.xsl"
 type="text/xml"
 title="Profiling step"?>
<!DOCTYPE section [
 <!ENTITY % entities SYSTEM "entity-decl.ent"> %entities;
]>
<section xmlns="http://docbook.org/ns/docbook" xmlns:xi="http://www.w3.org/2001/XInclude" xmlns:xlink="http://www.w3.org/1999/xlink" xml:id="full_recovery">
 <title>Full Disaster Recovery</title>
 <para>
  In this disaster scenario, you have lost everything in the cloud, including
  &swift;. Restoring from a &swift; backup is not possible because &swift; is
  gone.
 </para>
 <section>
  <title>Install and Set Up a &clm; Node</title>
  <para>
   Before beginning the process of restoring your whole cloud, you need to
   install and set up a &clm; node as though you are creating a new
   cloud. There are several steps in that process:
  </para>
  <procedure>
   <step>
    <para>
     Install &clm; software
    </para>
   </step>
   <step>
    <para>
     Prepare the &clm;, which includes installing the necessary packages
    </para>
   </step>
   <step>
    <para>
     Initialize the &clm;
    </para>
   </step>
   <step>
    <para>
     Restore your OpenStack git repository
    </para>
   </step>
   <step>
    <para>
     Adjust input model settings if the hardware setup has changed
    </para>
   </step>
  </procedure>
  <para>
   The following sections cover these steps in detail.
  </para>
 </section>
 <section>
  <title>Install and prepare the &clm;</title>
  <para>
   To ensure that you use the same version of &productname; that was previously
   loaded on your &clm;, download and install the &clm; software using the
   instructions from <xref linkend="sec.depl.adm_inst.add_on"/>.
  </para>
  <para>
   Prepare the &clm; following the steps in the <literal>Before You
   Start</literal> section of <xref linkend="install_gui"/>.
  </para>
 </section>
 <section>
  <title>Restore the &clm; from a remote backup</title>
  <procedure>
   <step>
    <para>
     Log in to the &clm;.
    </para>
   </step>
   <step>
    <para>
     Retrieve the &clm; backups from the remote server, which were created and
     saved during <xref linkend="manual_backup_setup"/>.
    </para>
<screen>&prompt.ardana;scp <replaceable>USER@REMOTE_SERVER</replaceable>:<replaceable>TAR_ARCHIVE</replaceable></screen>
   </step>
   <step>
    <para>
     Untar the TAR archives to overwrite the seven required locations:
    </para>
    <itemizedlist>
     <listitem>
      <para>
       home
      </para>
     </listitem>
     <listitem>
      <para>
       ssh
      </para>
     </listitem>
     <listitem>
      <para>
       shadow
      </para>
     </listitem>
     <listitem>
      <para>
       passwd
      </para>
     </listitem>
     <listitem>
      <para>
       group
      </para>
     </listitem>
     <listitem>
      <para>
       cobbler
      </para>
     </listitem>
     <listitem>
      <para>
       www_cobbler
      </para>
     </listitem>
    </itemizedlist>
<screen>&prompt.ardana;sudo tar -z --incremental --extract --ignore-zeros \
--warning=none --overwrite --directory <replaceable>RESTORE_TARGET</replaceable> -f <replaceable>BACKUP_TARGET</replaceable>.tar.gz</screen>
    <para>
     For example, where BACKUP_TARGET=/etc/ssh/
    </para>
<screen>&prompt.ardana;sudo tar -z --incremental --extract --ignore-zeros \
--warning=none --overwrite --directory /etc/ssh/ -f ssh.tar.gz</screen>
    <para>
     With BACKUP_TARGET=/etc/passwd
    </para>
<screen>&prompt.ardana;sudo tar -z --incremental --extract --ignore-zeros \
--warning=none --overwrite --directory /etc/ -f passwd.tar.gz</screen>
   </step>
   <step>
    <para>
     Run this playbook to deploy your cloud:
    </para>
<screen>&prompt.ardana;cd ~/scratch/ansible/next/ardana/ansible
&prompt.ardana;ansible-playbook -i hosts/verb_hosts site.yml</screen>
   </step>
   <step>
    <para>
     You can now perform the procedures to restore &mariadb; and &swift;.
    </para>
   </step>
  </procedure>
 </section>
 <section>
  <title>Restore &mariadb; from a remote backup</title>
  <procedure>
   <step>
    <para>
     Log in to the first node running the &mariadb; service.
    </para>
   </step>
   <step>
    <para>
     Retrieve the &mariadb; backup that was created with
     <xref
       linkend="mariadb_database_backup"/>.
    </para>
   </step>
   <step>
    <para>
     Create a temporary directory and extract the TAR archive (for example,
     <filename>mydb.tar.gz</filename>).
    </para>
<screen>&prompt.ardana;mkdir /tmp/mysql_restore; sudo tar -z --incremental \
--extract --ignore-zeros --warning=none --overwrite --directory /tmp/mysql_restore/ \
-f mydb.tar.gz</screen>
   </step>
   <step>
    <para>
     Verify that the files have been restored on the controller.
    </para>
<screen>&prompt.ardana;sudo du -shx /tmp/mysql_restore/*
16K     /tmp/mysql_restore/aria_log.00000001
4.0K    /tmp/mysql_restore/aria_log_control
3.4M    /tmp/mysql_restore/barbican
8.0K    /tmp/mysql_restore/ceilometer
4.2M    /tmp/mysql_restore/cinder
2.9M    /tmp/mysql_restore/designate
129M    /tmp/mysql_restore/galera.cache
2.1M    /tmp/mysql_restore/glance
4.0K    /tmp/mysql_restore/grastate.dat
4.0K    /tmp/mysql_restore/gvwstate.dat
2.6M    /tmp/mysql_restore/heat
752K    /tmp/mysql_restore/horizon
4.0K    /tmp/mysql_restore/ib_buffer_pool
76M     /tmp/mysql_restore/ibdata1
128M    /tmp/mysql_restore/ib_logfile0
128M    /tmp/mysql_restore/ib_logfile1
12M     /tmp/mysql_restore/ibtmp1
16K     /tmp/mysql_restore/innobackup.backup.log
313M    /tmp/mysql_restore/keystone
716K    /tmp/mysql_restore/magnum
12M     /tmp/mysql_restore/mon
8.3M    /tmp/mysql_restore/monasca_transform
0       /tmp/mysql_restore/multi-master.info
11M     /tmp/mysql_restore/mysql
4.0K    /tmp/mysql_restore/mysql_upgrade_info
14M     /tmp/mysql_restore/nova
4.4M    /tmp/mysql_restore/nova_api
14M     /tmp/mysql_restore/nova_cell0
3.6M    /tmp/mysql_restore/octavia
208K    /tmp/mysql_restore/opsconsole
38M     /tmp/mysql_restore/ovs_neutron
8.0K    /tmp/mysql_restore/performance_schema
24K     /tmp/mysql_restore/tc.log
4.0K    /tmp/mysql_restore/test
8.0K    /tmp/mysql_restore/winchester
4.0K    /tmp/mysql_restore/xtrabackup_galera_info</screen>
   </step>
  </procedure>
 </section>
 <section>
  <title>Restore &swift; from a remote backup</title>
  <procedure>
   <step>
    <para>
     Log in to the first &swift; Proxy (<literal>SWF-PRX[0]</literal>) node.
    </para>
    <para>
     To find the first &swift; Proxy node:
    </para>
    <substeps>
     <step>
      <para>
       On the &clm;
      </para>
<screen>&prompt.ardana;cd  ~/scratch/ansible/next/ardana/ansible
&prompt.ardana;ansible-playbook -i hosts/verb_hosts swift-status.yml \
--limit SWF-PRX[0]</screen>
      <para>
       At the end of the output, you will see something like the following
       example:
      </para>
<screen><?dbsuse-fo font-size="0.65em"?>...
Jun 18 20:01:49 ardana-qe102-cp1-c1-m1-mgmt swiftlm-uptime-mon[3985]: 'uptime-mon - INFO : Metric:keystone-get-token:max-latency: 0.679254770279 (at 1529352109.66)'
Jun 18 20:01:49 ardana-qe102-cp1-c1-m1-mgmt swiftlm-uptime-mon[3985]: 'uptime-mon - INFO : Metric:keystone-get-token:avg-latency: 0.679254770279 (at 1529352109.66)'

PLAY RECAP ********************************************************************
ardana-qe102-cp1-c1-m1 : ok=12 changed=0 unreachable=0 failed=0```</screen>
     </step>
     <step>
      <para>
       Find the first node name and its IP address. For example:
      </para>
<screen>&prompt.ardana;cat /etc/hosts | grep ardana-qe102-cp1-c1-m1</screen>
     </step>
    </substeps>
   </step>
   <step>
    <para>
     Retrieve (<command>scp</command>) the &swift; backup that was created with
     <xref linkend="swift_ring_backup"/>.
    </para>
   </step>
   <step>
    <para>
     Create a temporary directory and extract the TAR archive (for example,
     <filename>swring.tar.gz</filename>).
    </para>
    <screen>&prompt.ardana;mkdir /tmp/swift_builder_dir_restore; sudo tar -z \
--incremental --extract --ignore-zeros --warning=none --overwrite --directory \
/tmp/swift_builder_dir_restore/  -f swring.tar.gz</screen>
   </step>
   <step>
    <para>
     Log in to the &clm;.
    </para>
   </step>
   <step>
    <para>
     Stop the &swift; service:
    </para>
<screen>&prompt.ardana;cd ~/scratch/ansible/next/ardana/ansible
&prompt.ardana;ansible-playbook -i hosts/verb_hosts swift-stop.yml</screen>
   </step>
   <step>
    <para>
     Log back in to the first &swift; Proxy (<literal>SWF-PRX[0]</literal>)
     node, which was determined in <xref linkend="swift-nodes"/>.
    </para>
   </step>
   <step>
    <para>
     Copy the restored files.
    </para>
<screen>&prompt.ardana;sudo cp -pr /tmp/swift_builder_dir_restore/<replaceable>CLOUD_NAME</replaceable>/<replaceable>CONTROL_PLANE_NAME</replaceable>/builder_dir/* \
    /etc/swiftlm/<replaceable>CLOUD_NAME</replaceable>/<replaceable>CONTROL_PLANE_NAME</replaceable>/builder_dir/</screen>
    <para>For example</para>
<screen>&prompt.ardana;sudo cp -pr /tmp/swift_builder_dir_restore/entry-scale-kvm/control-plane-1/builder_dir/* \
    /etc/swiftlm/entry-scale-kvm/control-plane-1/builder_dir/</screen>
   </step>
   <step>
    <para>
     Log back in to the &clm;.
    </para>
   </step>
   <step>
    <para>
     Reconfigure the &swift; service:
    </para>
<screen>&prompt.ardana;cd ~/scratch/ansible/next/ardana/ansible
&prompt.ardana;ansible-playbook -i hosts/verb_hosts swift-reconfigure.yml</screen>
   </step>
  </procedure>
 </section>
</section>
