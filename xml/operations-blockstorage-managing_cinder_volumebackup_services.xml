<?xml version="1.0"?>
<?xml-stylesheet href="urn:x-suse:xslt:profiling:docbook51-profile.xsl"
 type="text/xml"
 title="Profiling step"?>
<!DOCTYPE section [
 <!ENTITY % entities SYSTEM "entities.ent"> %entities;
]>
<section xmlns="http://docbook.org/ns/docbook" xmlns:xi="http://www.w3.org/2001/XInclude" xmlns:xlink="http://www.w3.org/1999/xlink" xml:id="sec.operation.manage-block-storage">
 <title>Managing &o_blockstore; Volume and Backup Services</title>
 <important>
  <title>Use Only When Needed</title>
  <para>
   If the host running the <literal>cinder-volume</literal> service fails for
   any reason, it should be restarted as quickly as possible. Often, the host
   running &o_blockstore; services also runs high availability (HA) services
   such as &mariadb; and &rabbit;. These HA services are at risk while one of the
   nodes in the cluster is down. If it will take a significant amount of time
   to recover the failed node, then you may migrate the
   <literal>cinder-volume</literal> service and its backup service to one of
   the other controller nodes. When the node has been recovered, you should
   migrate the <literal>cinder-volume</literal> service and its backup service
   to the original (default) node.
  </para>
  <para>
   The <literal>cinder-volume</literal> service and its backup service migrate
   as a pair. If you migrate the <literal>cinder-volume</literal> service, its
   backup service will also be migrated.
  </para>
 </important>
 <section xml:id="idg-all-operations-blockstorage-managing_cinder_volumebackup_services-xml-5">
  <title>Migrating the cinder-volume service</title>
  <para>
   The following steps will migrate the cinder-volume service and its backup service.
  </para>
  <orderedlist>
   <listitem>
    <para>
     Log in to the &clm; node.
    </para>
   </listitem>
   <listitem>
    <para>
     Determine the host index numbers for each of your control plane nodes.
     This host index number will be used in a later step. They can be obtained
     by running this playbook:
    </para>
<screen>cd ~/scratch/ansible/next/ardana/ansible
ansible-playbook -i hosts/verb_hosts cinder-show-volume-hosts.yml</screen>
    <para>
     Here is an example snippet showing the output of a single three node
     control plane, with the host index numbers in bold:
    </para>
<screen>TASK: [_CND-CMN | show_volume_hosts | Show Cinder Volume hosts index and hostname] ***
ok: [ardana-cp1-c1-m1] =&gt; (item=(0, 'ardana-cp1-c1-m1')) =&gt; {
    "item": [
        <emphasis role="bold">0</emphasis>,
        "ardana-cp1-c1-m1"
    ],
    "msg": "Index 0 Hostname ardana-cp1-c1-m1"
}
ok: [ardana-cp1-c1-m1] =&gt; (item=(1, 'ardana-cp1-c1-m2')) =&gt; {
    "item": [
        <emphasis role="bold">1</emphasis>,
        "ardana-cp1-c1-m2"
    ],
    "msg": "Index 1 Hostname ardana-cp1-c1-m2"
}
ok: [ardana-cp1-c1-m1] =&gt; (item=(2, 'ardana-cp1-c1-m3')) =&gt; {
    "item": [
        <emphasis role="bold">2</emphasis>,
        "ardana-cp1-c1-m3"
    ],
    "msg": "Index 2 Hostname ardana-cp1-c1-m3"
}</screen>
   </listitem>
   <listitem>
    <para>
     Locate the control plane fact file for the control plane you need to
     migrate the service from. It will be located in the following directory:
    </para>
<screen>/etc/ansible/facts.d/</screen>
    <para>
     These fact files use the following naming convention:
    </para>
<screen>cinder_volume_run_location_<emphasis>&lt;control_plane_name&gt;</emphasis>.fact</screen>
   </listitem>
   <listitem>
    <para>
     Edit the fact file to include the host index number of the control plane
     node you wish to migrate the <literal>cinder-volume</literal> services to.
     For example, if they currently reside on your first controller node, host
     index 0, and you wish to migrate them to your second controller, you would
     change the value in the fact file to <literal>1</literal>.
    </para>
   </listitem>
   <listitem>
    <para>
     If you are using data encryption on your &clm;, ensure you
     have included the encryption key in your environment variables. For more
     information see <xref linkend="password_encryption"/>.
    </para>
<screen>export HOS_USER_PASSWORD_ENCRYPT_KEY=&lt;encryption key&gt;</screen>
   </listitem>
   <listitem>
    <para>
     After you have edited the control plane fact file, run the &o_blockstore; volume
     migration playbook for the control plane nodes involved in the migration.
     At minimum this includes the one to start cinder-volume manager on and the
     one on which to stop it:
    </para>
<screen>cd ~/scratch/ansible/next/ardana/ansible
ansible-playbook -i hosts/verb_hosts cinder-migrate-volume.yml --limit=&lt;limit_pattern1,limit_pattern2&gt;</screen>
    <note>
     <para>
      <emphasis>&lt;limit_pattern&gt;</emphasis> is the pattern used to limit
      the hosts that are selected to those within a specific control plane. For
      example, with the nodes in the snippet shown above,
      <literal>--limit=&gt;ardana-cp1-c1-m1,ardana-cp1-c1-m2&lt;</literal>
     </para>
    </note>
   </listitem>
   <listitem>
    <para>
     Even though the playbook summary reports no errors, you may disregard
     informational messages such as:
    </para>
    <screen>msg: Marking ardana_notify_cinder_restart_required to be cleared from the fact cache</screen>
   </listitem>
   <listitem>
    <para>
     Ensure that once your maintenance or other tasks are completed that you
     migrate the <literal>cinder-volume</literal> services back to their
     original node using these same steps.
    </para>
   </listitem>
  </orderedlist>
 </section>
</section>
