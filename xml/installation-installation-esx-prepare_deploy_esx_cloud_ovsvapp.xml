<?xml version="1.0"?>
<!DOCTYPE section [
 <!ENTITY % entities SYSTEM "entities.ent"> %entities;
]>
<section xml:id="prepare_deploy_esx_computes_ovsvapp"
 xmlns="http://docbook.org/ns/docbook" version="5.1"
 xmlns:xi="http://www.w3.org/2001/XInclude"
 xmlns:xlink="http://www.w3.org/1999/xlink">
 <title>Prepare and Deploy ESX Computes and OVSvAPPs</title>
 <para>
  The following sections describe the procedure to install and configure ESX
  compute and OVSvAPPs on vCenter.
 </para>
 <para>
  <emphasis role="bold">Preparation for ESX Cloud Deployment</emphasis>
 </para>
 <para>
  This section describes the procedures to prepare and deploy the ESX computes
  and OVSvAPPs for deployment.
 </para>
 <procedure>
  <step>
   <para>
    Login to the &lcm;.
   </para>
  </step>
  <step>
   <para>
    Source <literal>service.osrc</literal>.
   </para>
  </step>
  <step>
   <para>
    <xref linkend="sec.esx.register-vcenter"/>
   </para>
  </step>
  <step>
   <para>
    <xref linkend="sec.esx.activate-cluster"/>
   </para>
    <substeps>
     <step>
     <para>
      <xref linkend="sec.esx.modify-volume-config"/>
     </para>
    </step>
    <step>
     <para>
      <xref linkend="sec.esx.commit-your-cloud"/>
     </para>
    </step>
    <step>
     <para>
      <xref linkend="sec.esx.deploy-compute-proxy-ovsvapps"/>
     </para>
    </step>
   </substeps>
  </step>
 </procedure>
 <section>
  <title>Manage vCenters and Clusters</title>
 <para>
  The following section describes the detailed procedure on managing the
  vCenters and clusters.
 </para>
 </section>
 <section xml:id="sec.esx.register-vcenter">
  <title>Register a vCenter Server</title>
 <note>
  <itemizedlist>
   <listitem>
    <para>
     If ESX related roles are added in the input model after the controller
     cluster is up, you must run <literal>ardana-reconfigure.yml</literal> before
     adding the eon resource-manager.
    </para>
   </listitem>
   <listitem>
    <para>
     Make sure to provide single quotes (<literal>' '</literal>) or backslash
     (<literal>\</literal>) for the special characters
     (<literal>&amp; ! ; " ' () | \ &gt;</literal>) that are used when setting a
     password for vCenter registration . For example:
     <literal>'2the!Moon'</literal> or <literal>2the\!Moon</literal>. You can
     use either of the formats to set the vCenter password.
    </para>
   </listitem>
  </itemizedlist>
 </note>
 <para>
  vCenter provides centralized management of virtual host and virtual machines
  from a single console.
 </para>
 <procedure>
  <step>
   <para>
    Add a vCenter using EON python client.
   </para>
<screen># eon resource-manager-add [--name &lt;RESOURCE_MANAGER_NAME&gt;] \
  --ip-address &lt;RESOURCE_MANAGER_IP_ADDR&gt; \
  --username &lt;RESOURCE_MANAGER_USERNAME&gt; \
  --password &lt;RESOURCE_MANAGER_PASSWORD&gt; \
  [--port &lt;RESOURCE_MANAGER_PORT&gt;] \
  --type &lt;RESOURCE_MANAGER_TYPE&gt;</screen>
   <para>
    where:
   </para>
   <itemizedlist>
    <listitem>
     <para>
      Resource Manager Name - the identical name of the vCenter server.
     </para>
    </listitem>
    <listitem>
     <para>
      Resource Manager IP address - the IP address of the vCenter server.
     </para>
    </listitem>
    <listitem>
     <para>
      Resource Manager Username - the admin privilege username for the vCenter.
     </para>
    </listitem>
    <listitem>
     <para>
      Resource Manager Password - the password for the above username.
     </para>
    </listitem>
    <listitem>
     <para>
      Resource Manager Port - the vCenter server port. By default it is 443.
     </para>
    </listitem>
    <listitem>
     <para>
      Resource Manager Type - specify <literal>vcenter</literal>.
     </para>
     <important>
      <para>
       Please do not change the vCenter Port unless you are certain it is
       required to do so.
      </para>
     </important>
    </listitem>
   </itemizedlist>
   <para>
    <emphasis role="bold">Sample Output:</emphasis>
   </para>
<screen># eon resource-manager-add --name vc01 --ip-address 10.1.200.38 \
	--username administrator@vsphere.local \
	--password init123# --port 443 --type vcenter
+-------------+--------------------------------------+
| Property    | Value                                |
+-------------+--------------------------------------+
| ID          | BC9DED4E-1639-481D-B190-2B54A2BF5674 |
| IPv4Address | 10.1.200.38                          |
| Name        | vc01                                 |
| Password    | &lt;SANITIZED&gt;                          |
| Port        | 443                                  |
| State       | registered                           |
| Type        | vcenter                              |
| Username    | administrator@vsphere.local          |
+-------------+--------------------------------------+</screen>
  </step>
 </procedure>
 </section>
 <section>
  <title>Show vCenter</title>
 <procedure>
  <step>
   <para>
    Show vCenter using EON python client.
   </para>
<screen># eon resource-manager-show &lt;RESOURCE_MANAGER_ID&gt;</screen>
   <para>
    <emphasis role="bold">Sample Output:</emphasis>
   </para>
<screen>eon resource-manager-show BC9DED4E-1639-481D-B190-2B54A2BF5674
+-------------+--------------------------------------+
| Property    | Value                                |
+-------------+--------------------------------------+
| Clusters    | Cluster1, virtClust, Cluster2        |
| Datacenters | DC1, DC2                             |
| ID          | BC9DED4E-1639-481D-B190-2B54A2BF5674 |
| IPv4Address | 10.1.200.38                          |
| Name        | vc01                                 |
| Password    | &lt;SANITIZED&gt;                          |
| Port        | 443                                  |
| State       | registered                           |
| Type        | vcenter                              |
| Username    | administrator@vsphere.local          |
+-------------+--------------------------------------+</screen>
  </step>
 </procedure>
 </section>
 <section xml:id="sec.esx.register-network">
  <title>Create and edit an activation template</title>
 <para>
  This involves getting a sample network information template and modifying the
  details of the template. You will use the template to register the cloud
  network configuration for the vCenter.
 </para>
 <procedure>
  <step>
   <para>
    Execute the following command to generate a sample activation template:
   </para>
<screen>
<?dbsuse-fo font-size="0.70em"?>
eon get-activation-template [--filename &lt;ACTIVATION_JSON_NAME&gt;] \
  --type &lt;RESOURCE_TYPE&gt;</screen>
   <para>
    <emphasis role="bold">Sample Output:</emphasis>
   </para>
<screen>eon get-activation-template --filename activationtemplate.json --type esxcluster
---------------------------------------------------------------
Saved the sample network file in /home/user/activationtemplate.json
---------------------------------------------------------------</screen>
  </step>
  <step>
   <para>
    Change to the <literal>/home/user/</literal> directory:
   </para>
<screen>cd /home/user/</screen>
  </step>
  <step>
   <para>
    Modify the template (json file) as per your environment. See
    the sample file in <xref linkend="topic4797cgikdjm"/>.
   </para>
  </step>
 </procedure>
 </section>
 <section xml:id="sec.esx.activate-cluster">
  <title>Activate Clusters</title>
 <para>
  This involves using the activation template to register the cloud network
  configuration for the vCenter.
 </para>
 <para>
  This process spawns one compute proxy VM per cluster and one OVSvApp VM per
  host and configures the networking as defined in the JSON template. This
  process also updates the input model with the service VM details, executes
  the Ansible playbooks on the new nodes, and performs a post activation check
  on the service VMs.
 </para>
 <procedure>
  <step>
   <para>
    Activate the cluster for the selected vCenter.
   </para>
<screen># eon resource-activate &lt;RESOURCE_ID&gt; \
	--config-json /home/user/&lt;ACTIVATION_JSON_NAME&gt;</screen>
   <note>
    <para>
     To retrieve the resource ID, execute <literal>eon resource-list</literal>.
    </para>
   </note>
   <note>
    <para>
     Activating the first cluster in a datacenter requires you to provide an
     activation template JSON. You can provide the same activation template or
     a new activation template to activate the subsequent clusters.
    </para>
   </note>
   <note>
    <para>
     Minimum disk space required for a cluster activation is (number of hosts
     in that cluster +1 )* 45 GB.
    </para>
   </note>
  </step>
  <step>
   <para>
    Execute the following command to view the status of the cluster:
   </para>
<screen>eon resource-list
+--------------+-----------+-------------+-------------+------------+-------+------------+------------+
| ID           | Name      | Moid        | Rsrc Mgr ID | IP Address | Port  | Type       | State      |
+--------------+-----------+-------------+-------------+------------+-------+------------+------------+
| 1228f...e426 | Cluster2  | domain-c184 | BC9D...5674 | UNSET      | UNSET | esxcluster | imported   |
| 46971...0487 | virtClust | domain-c943 | BC9D...5674 | UNSET      | UNSET | esxcluster | activated  |
| a3003...7fb0 | Cluster1  | domain-c21  | BC9D...5674 | UNSET      | UNSET | esxcluster | imported   |
+--------------+-----------+-------------+-------------+------------+-------+------------+------------+</screen>
   <para>
    When the state is <literal>activated</literal>, the input model is updated
    with the service VM details, a git commit has been performed, the required
    playbooks and the post activation checks are completed successfully.
   </para>
   <note>
    <para>
     In multi-region setups, monitoring services will be deployed/running in a
     shared control-plane, unlike the normal deployment, where all services
     will be in one single control-plane. During ESX cluster activation in the
     multi-region scenarios, <literal>generate_hosts_file</literal> monitoring
     playbooks will be skipped due to absence of monitoring services in the ESX
     control-plane. To overcome this, you must run the playbook manually
     without limit.
    </para>
<screen>cd ~/scratch/ansible/next/ardana/ansible/
ansible-playbook -i hosts/verb_hosts site.yml --tag "generate_hosts_file"</screen>
   </note>
   <note>
    <para>
     Whenever an ESX compute is activated or deactivated, you must run the
     following playbook to reflect the changes in opsconsole.
    </para>
<screen>cd scratch/ansible/next/ardana/ansible/
ansible-playbook -i hosts/verb_hosts ops-console-reconfigure.yml </screen>
   </note>
  </step>
 </procedure>
 </section>
 <section xml:id="sec.esx.modify-volume-config">
  <title>Modify the Volume Configuration File</title>
 <para>
  Once the cluster is activated you must configure the volume.
 </para>
 <para>
  Perform the following steps to modify the volume configuration files:
 </para>
 <procedure>
  <step>
   <para>
    Change the directory. The <literal>cinder.conf.j2</literal> is present in
    following directories:
   </para>
<screen>cd ~/openstack/ardana/ansible/roles/_CND-CMN/templates</screen>
   <para>
    OR
   </para>
<screen>cd ~/openstack/my_cloud/config/cinder</screen>
   <para>
    It is recommended to modify the <literal>cinder.conf.j2</literal> available
    in <literal>~/openstack/my_cloud/config/cinder</literal>
   </para>
  </step>
  <step>
   <para>
    Modify the <literal>cinder.conf.j2</literal> as follows:
   </para>
<screen># Configure the enabled backends
enabled_backends=&lt;unique-section-name&gt;

# Start of section for VMDK block storage
#
# If you have configured VMDK Block storage for cinder you must
# uncomment this section, and replace all strings in angle brackets
# with the correct values for vCenter you have configured. You
# must also add the section name to the list of values in the
# 'enabled_backends' variable above. You must provide unique section
# each time you configure a new backend.

#[&lt;unique-section-name&gt;]
#vmware_api_retry_count = 10
#vmware_tmp_dir = /tmp
#vmware_image_transfer_timeout_secs = 7200
#vmware_task_poll_interval = 0.5
#vmware_max_objects_retrieval = 100
#vmware_volume_folder = cinder-volumes
#volume_driver = cinder.volume.drivers.vmware.vmdk.VMwareVcVmdkDriver
#vmware_host_ip = &lt;ip_address_of_vcenter&gt;
#vmware_host_username = &lt;vcenter_username&gt;
#vmware_host_password = &lt;password&gt;
#
#volume_backend_name = &lt;vmdk-backend-name&gt;
#
# End of section for VMDK block storage</screen>
  </step>
 </procedure>
 </section>
 <section xml:id="sec.esx.commit-your-cloud">
  <title>Commit your Cloud Definition</title>
 <procedure>
  <step>
   <para>
    Add the cloud deployment definition to Git:
   </para>
<screen>&prompt.user;cd ~/openstack/ardana/ansible
&prompt.user;git add -A
&prompt.user;git commit -m 'Adding ESX Configurations or other commit message'</screen>
  </step>
  <step>
   <para>
    Prepare your environment for deployment:
   </para>
<screen>&prompt.user;ansible-playbook -i hosts/localhost config-processor-run.yml
&prompt.user;ansible-playbook -i hosts/localhost ready-deployment.yml
&prompt.user;cd /home/stack/scratch/ansible/next/ardana/ansible</screen>
  </step>
 </procedure>
 </section>
 <section xml:id="sec.esx.deploy-compute-proxy-ovsvapps">
  <title>Configuring VMDK block storage</title>
 <para>
  Execute the following command to configure VMDK block storage:
 </para>
<screen>ansible-playbook -i hosts/verb_hosts cinder-reconfigure.yml</screen>
 <para>
  If there are more backends defined, you must create and use
  specific volume type to ensure that volume is created in ESX, as shown below:
 </para>
<screen># cinder type-create "ESX VMDK Storage"
...
# cinder type-key "ESX VMDK Storage" \
set volume_backend_name=&lt;name of VMDK backend selected during installation&gt;
...
# cinder create --volume-type "ESX VMDK Storage" 1
...</screen>
 </section>
 <section>
  <title>Validate the block storage</title>
  <para>
   You can validate that the VMDK block storage is added to the cloud
   successfully using the following command:
  </para>
 <screen># cinder service-list</screen>
<screen>
<?dbsuse-fo font-size="0.70em"?>
+------------------+-------------------+------+---------+-------+------------------+-----------+
|      Binary      |        Host       | Zone |  Status | State |    Updated_at    | Disabled  |
|                  |                   |      |         |       |                  |  Reason   |
+------------------+-------------------+------+---------+-------+------------------+-----------+
|  cinder-backup   | ha-volume-manager | nova | enabled |   up  | 2016-06-14T08:...|     -     |
| cinder-scheduler |  " -manager       | nova | enabled |  down | 2016-06-13T08:...|     -     |
| cinder-scheduler |  " -manager       | nova | enabled |   up  | 2016-06-14T08:...|     -     |
|  cinder-volume   |  " -manager       | nova | enabled |  down | 2016-06-13T07:...|     -     |
|  cinder-volume   |  " -manager@vmdk1 | nova | enabled |   up  | 2016-06-14T08:...|     -     |
+------------------+-------------------+------+---------+-------+---------------------+--------+</screen>
 </section>
 <section xml:id="sec.esx.verify">
  <title>Validate the compute</title>
  <para>
   You can validate that the ESX compute cluster is added to the cloud
   successfully using the following command:
  </para>
<screen>#  nova service-list</screen>
<screen>
<?dbsuse-fo font-size="0.65em"?>
+-----+------------------+-------------------------+----------+---------+-------+-----------+----------+
| Id  | Binary           | Host                    | Zone     | Status  | State | Updated   | Disabled |
|     |                  |                         |          |         |       |           | Reason   |
+-----+------------------+-------------------------+----------+---------+-------+-----------+----------+
| 3   | nova-conductor   | esxhos-joh-core-m1-mgmt | internal | enabled | up    | date:time | -        |
| 63  | nova-scheduler   |  " -core-m1-mgmt        | internal | enabled | up    | date:time | -        |
| 66  | nova-conductor   |  " -core-m2-mgmt        | internal | enabled | up    | date:time | -        |
| 111 | nova-conductor   |  " -core-m3-mgmt        | internal | enabled | up    | date:time | -        |
| 129 | nova-scheduler   |  " -core-m3-mgmt        | internal | enabled | up    | date:time | -        |
| 132 | nova-consoleauth |  " -core-m1-mgmt        | internal | enabled | up    | date:time | -        |
| 135 | nova-scheduler   |  " -core-m2-mgmt        | internal | enabled | up    | date:time | -        |
| 138 | nova-compute     |  " -esx-comp0001-mgmt   | nova     | enabled | up    | date:time | -        |
+-----+------------------+-------------------------+----------+---------+-------+-----------+----------+</screen>
  <para>
   You can validate the hypervisor list using the following command:
  </para>
<screen># nova hypervisor-list</screen>
<screen>+----+-------------------------------------------------+-------+----------+
| ID | Hypervisor hostname                             | State | Status   |
+----+-------------------------------------------------+-------+----------+
| 9  | domain-c40.9FDCFA66-6677-42A1-83FF-16DC32448021 | up    | enabled  |
+----+-------------------------------------------------+-------+----------+</screen>
 </section>
 <section>
  <title>Validate the neutron</title>
  <para>
   You can validate that the networking of the ESX cluster using the following
   command:
  </para>
<screen># neutron agent-list</screen>
<screen>
<?dbsuse-fo font-size="0.65em"?>
+------------+--------------+--------------------------+-------+--------------+------------------------+
| id         | agent_type   | host                     | alive | adm_state_up |binary                  |
+------------+--------------+--------------------------+-------+--------------+------------------------+
| 097b...77f | Metadata     | esxhos-joh-core-m3-mgmt  | :-)   | True         | neutron-metadata-agt   |
| 2b25...3e4 | DHCP         |  " -core-m3-mgmt         | :-)   | True         | neutron-dhcp-agt       |
| 4384...09e | OVSvApp      |  " -esx-ovsvapp0002-mgmt | :-)   | True         | ovsvapp-agt            |
| 4f0b...73e | L3           |  " -core-m3-mgmt         | :-)   | True         | neutron-vpn-agt        |
| 6db7...cb2 | OVSvApp      |  " -esx-ovsvapp0001-mgmt | :-)   | True         | ovsvapp-agt            |
| 700f...2db | DHCP         |  " -core-m1-mgmt         | :-)   | True         | neutron-dhcp-agt       |
| a723...0b6 | DHCP         |  " -core-m2-mgmt         | :-)   | True         | neutron-dhcp-agt       |
| bd51...e64 | L3           |  " -core-m1-mgmt         | :-)   | True         | neutron-vpn-agt        |
| e3d4...29e | Open vSwitch |  " -core-m2-mgmt         | :-)   | True         | neutron-openvswitch-agt|
| e5d7...3f0 | Open vSwitch |  " -core-m1-mgmt         | :-)   | True         | neutron-openvswitch-agt|
| e6a4...7bd | Metadata     |  " -core-m1-mgmt         | :-)   | True         | neutron-metadata-agt   |
| e734...12e | Metadata     |  " -core-m2-mgmt         | :-)   | True         | neutron-metadata-agt   |
| f019...5bd | L3 agent     |  " -core-m2-mgmt         | :-)   | True         | neutron-vpn-agen       |
| fc83...7bc | Open vSwitch |  " -core-m3-mgmt         | :-)   | True         | neutron-openvswitch-agt|
+------------+--------------+--------------------------+-------+--------------+------------------------+</screen>
 </section>
</section>
