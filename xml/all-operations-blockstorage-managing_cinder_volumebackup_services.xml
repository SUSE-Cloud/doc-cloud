<?xml version="1.0"?>
<!DOCTYPE section [
 <!ENTITY % entities SYSTEM "entities.ent"> %entities;
]>
<!---->
<section xmlns="http://docbook.org/ns/docbook" xmlns:xi="http://www.w3.org/2001/XInclude" xmlns:xlink="http://www.w3.org/1999/xlink" xml:id="topic_l1p_vpy_jt"><title>Managing Cinder Volume and Backup
    Services</title><abstract><para></para>
</abstract>
    <!---->
    <sidebar xml:id="warning"><title>When to use these steps</title><para>This operation should be used rarely and for good reason.</para>
<para>If the host running the <literal>cinder-volume</literal> service fails for any reason it
        should be restarted as quickly as possible. Often the host running Cinder services also runs
        high availability (HA) services such as MySQL and RabbitMQ. These HA services are at risk
        while one of the nodes in the cluster is down. If it will take a significant amount of time
        to recover the failed node, then you may migrate the <literal>cinder-volume</literal> service
        to one of the other controller nodes. When the node has been recovered you should migrate
        the <literal>cinder-volume</literal> service back to the original (default) node.</para>
</sidebar>
    <sidebar xml:id="idg-all-operations-blockstorage-managing_cinder_volumebackup_services-xml-5"><title>Migrating the cinder-volume service</title><para>Use the following steps to migrate the cinder-volume service.</para>
<orderedlist xml:id="ol_ck1_wdq_kx">
        <listitem><para>Log in to the lifecycle manager node.</para>
</listitem>
        <listitem><para>Determine the host index numbers for each of your control plane nodes. This host index
          number will be used in a later step. They can be obtained by running this
            playbook:</para>
<screen>cd ~/scratch/ansible/next/hos/ansible
ansible-playbook -i hosts/verb_hosts cinder-show-volume-hosts.yml</screen><para>Here is an example snippet showing the output of a single three node control plane, with the host
            index numbers in
            bold:</para>
<screen>TASK: [_CND-CMN | show_volume_hosts | Show Cinder Volume hosts index and hostname] ***
ok: [helion-cp1-c1-m1] =&gt; (item=(0, 'helion-cp1-c1-m1')) =&gt; {
    "item": [
        <emphasis role="bold">0</emphasis>,
        "helion-cp1-c1-m1"
    ],
    "msg": "Index 0 Hostname helion-cp1-c1-m1"
}
ok: [helion-cp1-c1-m1] =&gt; (item=(1, 'helion-cp1-c1-m2')) =&gt; {
    "item": [
        <emphasis role="bold">1</emphasis>,
        "helion-cp1-c1-m2"
    ],
    "msg": "Index 1 Hostname helion-cp1-c1-m2"
}
ok: [helion-cp1-c1-m1] =&gt; (item=(2, 'helion-cp1-c1-m3')) =&gt; {
    "item": [
        <emphasis role="bold">2</emphasis>,
        "helion-cp1-c1-m3"
    ],
    "msg": "Index 2 Hostname helion-cp1-c1-m3"
}</screen>
</listitem>
        <listitem><para>Locate the control plane fact file for the control plane you need to migrate the service
          from. It will be located in the following
            directory:</para>
<screen>/etc/ansible/facts.d/</screen><para>These fact files use the
            following naming
          convention:</para>
<screen>cinder_volume_run_location_<emphasis>&lt;control_plane_name&gt;</emphasis>.fact</screen></listitem>
        <listitem><para>Edit the fact file to include the host index number of the control plane node you wish
          to migrate the <literal>cinder-volume</literal> services to. For example, if they currently
          reside on your first controller node, host index 0, and you wish to migrate them to your
          second controller, you would change the value in the fact file to <literal>1</literal>.</para>
</listitem>
        <listitem><para>If you are using data encryption on your lifecycle manager, ensure you have included the
          encryption key in your environment
          variables:</para>
<screen>export HOS_USER_PASSWORD_ENCRYPT_KEY=&lt;encryption key&gt;</screen></listitem>
        <listitem><para>Once you have edited the control plane fact file, run the Cinder volume migration
          playbook for the control plane nodes involved in the migration. This at minimum includes
          the one to start cinder-volume manager on and the one on which to stop it:
              </para>
<screen>cd ~/scratch/ansible/next/hos/ansible
ansible-playbook -i hosts/verb_hosts cinder-migrate-volume.yml --limit=&lt;limit_pattern1,limit_pattern2&gt;</screen></listitem>
        <listitem><para>Ensure that once your maintenance or other tasks are completed that you migrate the
            <literal>cinder-volume</literal> services back to their original node using these same
          steps.</para>
</listitem>
      </orderedlist></sidebar>
  </section>
