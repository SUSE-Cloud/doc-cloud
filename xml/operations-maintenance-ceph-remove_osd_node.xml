<?xml version="1.0"?>
<?xml-stylesheet href="urn:x-suse:xslt:profiling:docbook51-profile.xsl"
 type="text/xml"
 title="Profiling step"?>
<!DOCTYPE section [
 <!ENTITY % entities SYSTEM "entities.ent"> %entities;
]>
<section xmlns="http://docbook.org/ns/docbook" xmlns:xi="http://www.w3.org/2001/XInclude" xmlns:xlink="http://www.w3.org/1999/xlink" xml:id="remove_osd_node">
 <title>Removing a Ceph OSD Node</title>
 <para>
  Removing a Ceph Object Storage Daemon (OSD) node allows you to remove
  capacity.
 </para>
 <para>
  Perform the following steps to remove a Ceph Object Storage Daemon (OSD) node
  from a cluster:
 </para>
 <orderedlist>
  <listitem>
   <para>
    Log in to the OSD node that you wish to remove.
   </para>
  </listitem>
  <listitem>
   <para>
    Use the following command to retrieve and make note of the OSD numbers on
    the OSD node to be removed:
   </para>
<screen>ceph osd tree</screen>
   <para>
    A sample of OSD node is as follows:
   </para>
<screen>$ ceph osd tree
# id    weight  type name       up/down reweight
-1      6       root default
-2      3               host ceph-ccp-ceph0003-mgmt
0       1                       osd.0   up      1
3       1                       osd.3   up      1
6       1                       osd.6   up      1
<emphasis role="bold">-4      3               host ceph-ccp-ceph0001-mgmt
1       1                       osd.1   up      1
4       1                       osd.4   up      1
7       1                       osd.7   up      1</emphasis>
-3      3               host ceph-ccp-ceph0002-mgmt
2       1                       osd.2   up      1
5       1                       osd.5   up      1
8       1                       osd.8   up      1</screen>
   <para>
    For example, to remove the OSD node
    <literal>ceph-ccp-ceph0001-mgmt</literal>, the OSD numbers 1, 4, and 7 are
    relevant.
   </para>
  </listitem>
  <listitem>
   <para>
    Log in to the lifecycle manager.
   </para>
  </listitem>
  <listitem>
   <para>
    Run the following command to stop the OSDs running on the OSD node to be
    removed:
   </para>
<screen>cd ~/scratch/ansible/next/hos/ansible
ansible-playbook -i hosts/verb_hosts ceph-stop.yml --limit &lt;osd_node_to_remove&gt;</screen>
   <para>
    Replace the <literal>&lt;osd_node_to_remove&gt;</literal> string with the
    hostname of the node that you wish to remove. For example
    <literal>'ceph-ccp-ceph0001-mgmt'</literal>.
   </para>
  </listitem>
  <listitem>
   <para>
    Log back in to the OSD node you are removing and run the following command
    for each OSD number (obtained from step #2 above):
   </para>
<screen>ceph osd out &lt;osd-number&gt; --cluster &lt;ceph_cluster&gt;
ceph auth del osd.&lt;osd-number&gt; --cluster &lt;ceph_cluster&gt;
ceph osd crush remove osd.&lt;osd-number&gt; --cluster &lt;ceph_cluster&gt;
ceph osd rm &lt;osd-number&gt; --cluster &lt;ceph_cluster&gt;</screen>
  </listitem>
  <listitem>
   <para>
    From the same OSD node, remove the OSD host from the Ceph CRUSH map by
    executing the following command:
   </para>
<screen>ceph osd crush remove &lt;host-name&gt;</screen>
  </listitem>
  <listitem>
   <para>
    Log in to the lifecycle manager and remove the OSD host fentry rom the
    input model in the
    <literal>~/helion/my_cloud/definition/data/servers.yml</literal> file.
   </para>
  </listitem>
  <listitem>
   <para>
    Check your
    <literal>~/helion/my_cloud/definition/data/control_plane.yml</literal>
    file and ensure that the <literal>member-count</literal>,
    <literal>min-count</literal>, and <literal>max-count</literal> values (if
    used) represent the number of OSD nodes that will remain after the node is
    removed.
   </para>
  </listitem>
  <listitem>
   <para>
    Commit your configuration to the local Git repository (see
    <xref linkend="using_git"/>), as follows:
   </para>
<screen>cd ~/helion/hos/ansible
git add -A
git commit -m "removing OSD node"</screen>
  </listitem>
  <listitem>
   <para>
    Run the configuration processor:
   </para>
<screen>cd ~/helion/hos/ansible
ansible-playbook -i hosts/localhost config-processor-run.yml</screen>
  </listitem>
  <listitem>
   <para>
    Update your deployment directory:
   </para>
<screen>cd ~/helion/hos/ansible
ansible-playbook -i hosts/localhost ready-deployment.yml</screen>
  </listitem>
  <listitem>
   <para>
    Remove the OSD node from Cobbler:
   </para>
<screen>sudo cobbler system remove --name=&lt;node&gt;</screen>
  </listitem>
  <listitem>
   <para>
    Run the Cobbler deploy playbook:
   </para>
<screen>cd ~/helion/hos/ansible
ansible-playbook -i hosts/localhost cobbler-deploy.yml</screen>
  </listitem>
  <listitem>
   <para>
    Log in to the OSD node and unmount the data and journal disks. Also,
    remove the mount instructions for the same disks from the
    <literal>/etc/fstab</literal> file.
   </para>
  </listitem>
  <listitem>
   <para>
    On the OSD node, remove the OSD service directory:
   </para>
<screen>sudo rm -rf /var/lib/ceph/osd/ceph-{osd-number}</screen>
   <para>
    Example:
   </para>
<screen>sudo rm -rf /var/lib/ceph/osd/ceph-1</screen>
  </listitem>
 </orderedlist>
 <section xml:id="idg-all-operations-maintenance-ceph-remove_osd_node-xml-7">
  <title>Removing a Ceph OSD Node from Monitoring</title>
  <para>
   Once a Ceph OSD node has been removed, any related monitoring alarms will
   trigger. Take the following additional steps to take to resolve this issue.
  </para>
  <para>
   You will want to SSH to each of the Monasca API servers and edit the
   <literal>/etc/monasca/agent/conf.d/host_alive.yaml</literal> file and remove
   references to the Ceph OSD node(s) you removed. This will require
   <literal>sudo</literal> access.
  </para>
  <para>
   Once you have removed the references on each of your Monasca API servers you
   then need to restart the monasca-agent on each of those servers with the
   following command:
  </para>
<screen>sudo service monasca-agent restart</screen>
  <para>
   With the Ceph OSD node references removed from your Monasca API servers and
   the corrensponding monasca-agents restarted, you can then delete the
   corresponding alarms to finish this process. To do so, we recommend using
   the Monasca CLI, which should be installed by default on each of your
   Monasca API servers:
  </para>
 <screen>monasca alarm-list --metric-dimensions hostname=&lt;ceph osd node deleted&gt;</screen>
  <para>
   You can then delete the alarm with this command:
  </para>
<screen>monasca alarm-delete &lt;alarm ID&gt;</screen>
 </section>
</section>
