<?xml version="1.0"?>
<!DOCTYPE section [
 <!ENTITY % entities SYSTEM "entities.ent"> %entities;
]>
<section xml:id="nsx-vsphere-baremetal"
 xmlns="http://docbook.org/ns/docbook" version="5.1"
 xmlns:xi="http://www.w3.org/2001/XInclude"
 xmlns:xlink="http://www.w3.org/1999/xlink">
 <title>Integrating with NSX for vSphere on Baremetal</title>
 <para>
  This section describes the installation steps and requirements for
  integrating with NSX for vSphere on baremetal physical hardware.
 </para>
 <section>
  <title>Pre-Integration Checklist</title>
  <para>
   The following installation and integration instructions assumes an
   understanding of VMware's ESXI and vSphere products for setting up virtual
   environments.
  </para>
  <para>
   Please review the following requirements for the VMware vSphere environment.
  </para>
  <para>
   <emphasis role="bold">Software Requirements</emphasis>
  </para>
  <para>
   Before you install or upgrade NSX, verify your software versions. The
   following are the required versions.
  </para>
  <informaltable>
   <tgroup cols="2">
    <colspec colnum="1" colname="1" colwidth="67*"/>
    <colspec colnum="2" colname="2" colwidth="33*"/>
    <thead>
     <row>
      <entry>
       <para>
        Software
       </para>
      </entry>
      <entry>
       <para>
        Version
       </para>
      </entry>
     </row>
    </thead>
    <tbody>
     <row>
      <entry>
       <para>
        &productname;
       </para>
      </entry>
      <entry>
       <para>
        8
       </para>
      </entry>
     </row>
     <row>
      <entry>
       <para>
        VMware NSX-v Manager
       </para>
      </entry>
      <entry>
       <para>
        6.3.4 or higher
       </para>
      </entry>
     </row>
     <row>
      <entry>
       <para>
        VMWare NSX-v &o_netw; Plugin
       </para>
      </entry>
      <entry>
       <para>
        Pike Release (TAG=11.0.0)
       </para>
      </entry>
     </row>
     <row>
      <entry>
       <para>
        VMWare ESXi and vSphere Appliance (vSphere web Client)
       </para>
      </entry>
      <entry>
       <para>
        6.0 or higher
       </para>
      </entry>
     </row>
    </tbody>
   </tgroup>
  </informaltable>
  <para>
   A vCenter server (appliance) is required to manage the vSphere environment.
   It is recommended that you install a vCenter appliance as an ESX virtual
   machine.
  </para>
  <important>
   <para>
    Each ESXi compute cluster is required to have shared storage between the
    hosts in the cluster, otherwise attempts to create instances through
    nova-compute will fail.
   </para>
  </important>
 </section>
 <section>
  <title>Installing on Baremetal</title>
  <para>
   &ostack; can be deployed in two ways: on baremetal (physical hardware) or in
   an ESXi virtual environment on virtual machines. The following instructions
   describe how to install &ostack; on baremetal nodes with vCenter and NSX
   Manager running as virtual machines. For instructions on virtual machine
   installation, see <xref linkend="nsx-vsphere-vm"/>.
  </para>
  <para>
   This deployment example will consist of two ESXi clusters at minimum: a
   <literal>control-plane</literal> cluster and a <literal>compute</literal>
   cluster. The control-plane cluster must have 3 ESXi hosts minimum (due to
   VMware's recommendation that each NSX controller virtual machine is on a
   separate host). The compute cluster must have 2 ESXi hosts minimum. There
   can be multiple compute clusters. The following table outlines the virtual
   machine specifications to be built in the control-plane cluster:
  </para>
  <table xml:id="nsx-hw-reqs-bm">
   <title>NSX Hardware Requirements for Baremetal Integration</title>
   <tgroup cols="6">
    <colspec colnum="1" colname="1" colwidth="35*"/>
    <colspec colnum="2" colname="2" colwidth="13*"/>
    <colspec colnum="3" colname="3" colwidth="13*"/>
    <colspec colnum="4" colname="4" colwidth="13*"/>
    <colspec colnum="5" colname="5" colwidth="13*"/>
    <colspec colnum="6" colname="6" colwidth="13"/>
    <thead>
     <row>
      <entry>
       <para>
        Virtual Machine Role
       </para>
      </entry>
      <entry>
       <para>
        Required Number
       </para>
      </entry>
      <entry>
       <para>
        Disk
       </para>
      </entry>
      <entry>
       <para>
        Memory
       </para>
      </entry>
      <entry>
       <para>
        Network
       </para>
      </entry>
      <entry>
       <para>
        CPU
       </para>
      </entry>
     </row>
    </thead>
    <tbody>
     <row>
      <entry>
       <para>
        Compute virtual machines
       </para>
      </entry>
      <entry>
       <para>
        1 per compute cluster
       </para>
      </entry>
      <entry>
       <para>
        80GB
       </para>
      </entry>
      <entry>
       <para>
        4GB
       </para>
      </entry>
      <entry>
       <para>
        3 VMXNET Virtual Network Adapters
       </para>
      </entry>
      <entry>
       <para>
        2 vCPU
       </para>
      </entry>
     </row>
     <row>
      <entry>
       <para>
        NSX Edge Gateway/DLR/Metadata-proxy appliances
       </para>
      </entry>
      <entry>
       <para></para>
      </entry>
      <entry>
       <para>
        Autogenerated by NSXv
       </para>
      </entry>
      <entry>
       <para>
        Autogenerated by NSXv
       </para>
      </entry>
      <entry>
       <para>
        Autogenerated by NSXv
       </para>
      </entry>
      <entry>
       <para>
        Autogenerated by NSXv
       </para>
      </entry>
     </row>
    </tbody>
   </tgroup>
  </table>
  <para>
   In addition to the ESXi hosts, it is recommended that there is one physical
   host for the &lcm; node and three physical hosts for the controller nodes.
  </para>
  <section>
   <title>Network Requirements</title>
   <para>
    NSX-v requires the following for networking:
   </para>
   <itemizedlist>
    <listitem>
     <para>
      The ESXi hosts, vCenter, and the NSX Manager appliance must resolve DNS
      lookup.
     </para>
    </listitem>
    <listitem>
     <para>
      The ESXi host must have the NTP service configured and enabled.
     </para>
    </listitem>
    <listitem>
     <para>
      Jumbo frames must be enabled on the switch ports that the ESXi hosts are
      connected to.
     </para>
    </listitem>
    <listitem>
     <para>
      The ESXi hosts must have at least 2 physical network cards each.
     </para>
    </listitem>
   </itemizedlist>
  </section>
  <section>
   <title>Network Model</title>
   <para>
    The model in these instructions requires the following networks:
   </para>
   <variablelist>
    <varlistentry>
     <term>ESXi Hosts and vCenter</term>
     <listitem>
      <para>
       This is the network that the ESXi hosts and vCenter use to route traffic
       with.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>NSX Management</term>
     <listitem>
      <para>
       The network which the NSX controllers and NSX Manager will use.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>NSX VTEP Pool</term>
     <listitem>
      <para>
       The network that NSX uses to create endpoints for VxLAN tunnels.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>Management</term>
     <listitem>
      <para>
       The network that &ostack; uses for deployment and maintenance of the
       cloud.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>Internal API (optional)</term>
     <listitem>
      <para>
       The network group that will be used for management (private API) traffic
       within the cloud.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>External API</term>
     <listitem>
      <para>
       This is the network that users will use to make requests to the cloud.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>External VM</term>
     <listitem>
      <para>
       VLAN-backed provider network for external access to guest VMs (floating
       IPs).
      </para>
     </listitem>
    </varlistentry>
   </variablelist>
  </section>
  <section>
   <title>vSphere port security settings</title>
   <para>
    Even though the &ostack; deployment is on baremetal, it is still necessary
    to define each VLAN within a vSphere Distributed Switch for the &o_comp;
    compute proxy virtual machine. Therefore, the vSphere port security
    settings are shown in the table below.
   </para>
   <informaltable>
    <tgroup cols="4">
     <colspec colnum="1" colname="1" colwidth="40*"/>
     <colspec colnum="2" colname="2" colwidth="10*"/>
     <colspec colnum="3" colname="3" colwidth="10*"/>
     <colspec colnum="4" colname="4" colwidth="40*"/>
     <thead>
      <row>
       <entry>
        <para>
         Network Group
        </para>
       </entry>
       <entry>
        <para>
         VLAN Type
        </para>
       </entry>
       <entry>
        <para>
         Interface
        </para>
       </entry>
       <entry>
        <para>
         vSphere Port Group Security Settings
        </para>
       </entry>
      </row>
     </thead>
     <tbody>
      <row>
       <entry>
        <para>
         IPMI
        </para>
       </entry>
       <entry>
        <para>
         Untagged
        </para>
       </entry>
       <entry>
        <para>
         N/A
        </para>
       </entry>
       <entry>
        <para>
         N/A
        </para>
       </entry>
      </row>
      <row>
       <entry>
        <para>
         ESXi Hosts and vCenter
        </para>
       </entry>
       <entry>
        <para>
         Tagged
        </para>
       </entry>
       <entry>
        <para>
         N/A
        </para>
       </entry>
       <entry>
        <para>
         Defaults
        </para>
       </entry>
      </row>
      <row>
       <entry>
        <para>
         NSX Manager
        </para>
        <para>
         Must be able to reach ESXi Hosts and vCenter
        </para>
       </entry>
       <entry>
        <para>
         Tagged
        </para>
       </entry>
       <entry>
        <para>
         N/A
        </para>
       </entry>
       <entry>
        <para>
         Defaults
        </para>
       </entry>
      </row>
      <row>
       <entry>
        <para>
         NSX VTEP Pool
        </para>
       </entry>
       <entry>
        <para>
         Tagged
        </para>
       </entry>
       <entry>
        <para>
         N/A
        </para>
       </entry>
       <entry>
        <para>
         Defaults
        </para>
       </entry>
      </row>
      <row>
       <entry>
        <para>
         Management
        </para>
       </entry>
       <entry>
        <para>
         Tagged or Untagged
        </para>
       </entry>
       <entry>
        <para>
         bond0
        </para>
       </entry>
       <entry>
        <itemizedlist>
         <listitem>
          <para>
           <emphasis role="bold">Promiscuous Mode</emphasis>: Accept
          </para>
         </listitem>
         <listitem>
          <para>
           <emphasis role="bold">MAC Address Changes</emphasis>: Reject
          </para>
         </listitem>
         <listitem>
          <para>
           <emphasis role="bold">Forged Transmits</emphasis>:Reject
          </para>
         </listitem>
        </itemizedlist>
       </entry>
      </row>
      <row>
       <entry>
        <para>
         Internal API (Optional, may be combined with the Management Network.
         If network segregation is required for security reasons, you can keep
         this as a separate network.)
        </para>
       </entry>
       <entry>
        <para>
         Tagged
        </para>
       </entry>
       <entry>
        <para>
         bond0
        </para>
       </entry>
       <entry>
        <itemizedlist>
         <listitem>
          <para>
           <emphasis role="bold">Promiscuous Mode</emphasis>: Accept
          </para>
         </listitem>
         <listitem>
          <para>
           <emphasis role="bold">MAC Address Changes</emphasis>: Reject
          </para>
         </listitem>
         <listitem>
          <para>
           <emphasis role="bold">Forged Transmits</emphasis>: Accept
          </para>
         </listitem>
        </itemizedlist>
       </entry>
      </row>
      <row>
       <entry>
        <para>
         External API (Public)
        </para>
       </entry>
       <entry>
        <para>
         Tagged
        </para>
       </entry>
       <entry>
        <para>
         N/A
        </para>
       </entry>
       <entry>
        <itemizedlist>
         <listitem>
          <para>
           <emphasis role="bold">Promiscuous Mode</emphasis>: Accept
          </para>
         </listitem>
         <listitem>
          <para>
           <emphasis role="bold">MAC Address Changes</emphasis>: Reject
          </para>
         </listitem>
         <listitem>
          <para>
           <emphasis role="bold">Forged Transmits</emphasis>: Accept
          </para>
         </listitem>
        </itemizedlist>
       </entry>
      </row>
      <row>
       <entry>
        <para>
         External VM
        </para>
       </entry>
       <entry>
        <para>
         Tagged
        </para>
       </entry>
       <entry>
        <para>
         N/A
        </para>
       </entry>
       <entry>
        <itemizedlist>
         <listitem>
          <para>
           <emphasis role="bold">Promiscuous Mode</emphasis>: Accept
          </para>
         </listitem>
         <listitem>
          <para>
           <emphasis role="bold">MAC Address Changes</emphasis>: Reject
          </para>
         </listitem>
         <listitem>
          <para>
           <emphasis role="bold">Forged Transmits</emphasis>: Accept
          </para>
         </listitem>
        </itemizedlist>
       </entry>
      </row>
     </tbody>
    </tgroup>
   </informaltable>
  </section>
  <section>
   <title>Configuring the vSphere Environment</title>
   <para>
    Before deploying &ostack; with NSX-v, the VMware vSphere environment must
    be properly configured, including setting up vSphere distributed switches
    and port groups. For detailed instructions, see
    <xref linkend="install_esx_ovsvapp"/>.
   </para>
   <para>
    Installing and configuring the VMware NSX Manager and creating the NSX
    network within the vSphere environment is covered below.
   </para>
   <para>
    Before proceeding with the installation, ensure that the following are
    configured in the vSphere environment.
   </para>
   <itemizedlist>
    <listitem>
     <para>
      The vSphere datacenter is configured with at least two clusters, one
      <emphasis role="bold">control-plane</emphasis> cluster and one
      <emphasis
     role="bold">compute</emphasis> cluster.
     </para>
    </listitem>
    <listitem>
     <para>
      Verify that all software, hardware, and networking requirements have been
      met.
     </para>
    </listitem>
    <listitem>
     <para>
      Ensure the vSphere distributed virtual switches (DVS) are configured for
      each cluster.
     </para>
    </listitem>
   </itemizedlist>
   <note>
    <para>
     The MTU setting for each DVS should be set to 1600. NSX should
     automatically apply this setting to each DVS during the setup process.
     Alternatively, the setting can be manually applied to each DVS before
     setup if desired.
    </para>
   </note>
   <para>
    Make sure there is a copy of the &cloudos; <filename>.iso</filename> in the
    <literal>ardana</literal> home directory,
    <filename>var/lib/ardana</filename>, and that it is called
    <filename>sles12sp3.iso</filename>.
   </para>
   <para>
    Install the <literal>open-vm-tools</literal> package.
   </para>
<screen>&prompt.sudo;zypper install open-vm-tools</screen>
   <section>
    <title>Install NSX Manager</title>
    <para>
     The NSX Manager is the centralized network management component of NSX. It
     provides a single point of configuration and REST API entry-points.
    </para>
    <para>
     The NSX Manager is installed as a virtual appliance on one of the ESXi
     hosts within the vSphere environment. This guide will cover installing the
     appliance on one of the ESXi hosts within the control-plane cluster. For
     more detailed information, refer to
     <link
   xlink:href="https://pubs.vmware.com/NSX-62/index.jsp?topic=%2Fcom.vmware.nsx.install.doc%2FGUID-D8578F6E-A40C-493A-9B43-877C2B75ED52.html">VMware's
     NSX Installation Guide.</link>
    </para>
    <para>
     To install the NSX Manager, download the virtual appliance from
     <link
  xlink:href="https://www.vmware.com/go/download-nsx-vsphere">VMware</link>
     and deploy the appliance within vCenter onto one of the ESXi hosts. For
     information on deploying appliances within vCenter, refer to VMware's
     documentation for ESXi
     <link
  xlink:href="https://pubs.vmware.com/vsphere-55/index.jsp?topic=%2Fcom.vmware.vsphere.vm_admin.doc%2FGUID-AFEDC48B-C96F-4088-9C1F-4F0A30E965DE.html">5.5</link>
     or
     <link
  xlink:href="https://pubs.vmware.com/vsphere-60/index.jsp?topic=%2Fcom.vmware.vsphere.vm_admin.doc%2FGUID-AFEDC48B-C96F-4088-9C1F-4F0A30E965DE.html">6.0</link>.
    </para>
    <para>
     During the deployment of the NSX Manager appliance, be aware of the
     following:
    </para>
    <para>
     When prompted, select <guimenu>Accept extra configuration
     options</guimenu>. This will present options for configuring IPv4 and IPv6
     addresses, the default gateway, DNS, NTP, and SSH properties during the
     installation, rather than configuring these settings manually after the
     installation.
    </para>
    <itemizedlist>
     <listitem>
      <para>
       Choose an ESXi host that resides within the control-plane cluster.
      </para>
     </listitem>
     <listitem>
      <para>
       Ensure that the network mapped port group is the DVS port group that
       represents the VLAN the NSX Manager will use for its networking (in this
       example it is labeled as the <literal>NSX Management</literal> network).
      </para>
     </listitem>
    </itemizedlist>
    <note>
     <para>
      The IP address assigned to the NSX Manager must be able to resolve
      reverse DNS.
     </para>
    </note>
    <para>
     Power on the NSX Manager virtual machine after it finishes deploying and
     wait for the operating system to fully load. When ready, carry out the
     following steps to have the NSX Manager use single sign-on (SSO) and to
     register the NSX Manager with vCenter:
    </para>
    <procedure>
     <step>
      <para>
       Open a web browser and enter the hostname or IP address that was
       assigned to the NSX Manager during setup.
      </para>
     </step>
     <step>
      <para>
       Log in with the username <literal>admin</literal> and the password set
       during the deployment.
      </para>
     </step>
     <step>
      <para>
       After logging in, click on <guimenu>Manage vCenter
       Registration</guimenu>.
      </para>
     </step>
     <step>
      <para>
       Configure the NSX Manager to connect to the vCenter server.
      </para>
     </step>
     <step>
      <para>
       Configure NSX manager for single sign on (SSO) under the <guimenu>Lookup
       Server URL</guimenu> section.
      </para>
     </step>
    </procedure>
    <note>
     <para>
      When configuring SSO, use <literal>Lookup Service Port 443</literal> for
      vCenter version 6.0. Use <literal>Lookup Service Port 7444</literal> for
      vCenter version 5.5.
     </para>
     <para>
      SSO makes vSphere and NSX more secure by allowing the various components
      to communicate with each other through a secure token exchange mechanism,
      instead of requiring each component to authenticate a user separately.
      For more details, refer to VMware's documentation on
      <link
   xlink:href="https://pubs.vmware.com/NSX-62/index.jsp?topic=%2Fcom.vmware.nsx.install.doc%2FGUID-523B0D77-AAB9-4535-B326-1716967EC0D2.html">Configure
      Single Sign-On</link>.
     </para>
    </note>
    <para>
     Both the <literal>Lookup Service URL</literal> and the <literal>vCenter
     Server</literal> sections should have a status of
     <literal>connected</literal> when configured properly.
    </para>
    <para>
     Log into the vSphere Web Client (log out and and back in if already logged
     in). The NSX Manager will appear under the <guimenu>Networking &amp;
     Security</guimenu> section of the client.
    </para>
    <note>
     <para>
      The <guimenu>Networking &amp; Security</guimenu> section will not appear
      under the vSphere desktop client. Use of the web client is required for
      the rest of this process.
     </para>
    </note>
   </section>
   <section>
    <title>Add NSX Controllers</title>
    <para>
     The NSX controllers serve as the central control point for all logical
     switches within the vSphere environment's network, and they maintain
     information about all hosts, logical switches (VXLANs), and distributed
     logical routers.
    </para>
    <para>
     NSX controllers will each be deployed as a virtual appliance on the ESXi
     hosts within the control-plane cluster to form the NSX Controller cluster.
     For details about NSX controllers and the NSX control plane in general,
     refer to
     <link
  xlink:href="https://pubs.vmware.com/NSX-62/index.jsp?topic=%2Fcom.vmware.nsx.install.doc%2FGUID-4E0FEE83-CF2C-45E0-B0E6-177161C3D67C.html">VMware's
     NSX documentation</link>.
    </para>
    <important>
     <para>
      Whatever the size of the NSX deployment, the following conditions must be
      met:
     </para>
     <itemizedlist>
      <listitem>
       <para>
        Each NSX Controller cluster must contain three controller nodes. Having
        a different number of controller nodes is not supported.
       </para>
      </listitem>
      <listitem>
       <para>
        Before deploying NSX Controllers, you must deploy an NSX Manager
        appliance and register vCenter with NSX Manager.
       </para>
      </listitem>
      <listitem>
       <para>
        Determine the IP pool settings for your controller cluster, including
        the gateway and IP address range. DNS settings are optional.
       </para>
      </listitem>
      <listitem>
       <para>
        The NSX Controller IP network must have connectivity to the NSX Manager
        and to the management interfaces on the ESXi hosts.
       </para>
      </listitem>
     </itemizedlist>
    </important>
    <para>
     Log in to the vSphere web client and do the following steps to add the NSX
     controllers:
    </para>
    <procedure>
     <step>
      <para>
       In vCenter, navigate to <guimenu>Home</guimenu>, select
       <menuchoice><guimenu>Networking &amp;
       Security</guimenu><guimenu>Installation</guimenu></menuchoice>, and then
       select the <guimenu>Management</guimenu> tab.
      </para>
     </step>
     <step>
      <para>
       In the <guimenu>NSX Controller nodes</guimenu> section, click the
       <guimenu>Add Node</guimenu> icon represented by a green plus sign.
      </para>
     </step>
     <step>
      <para>
       Enter the NSX Controller settings appropriate to your environment. If
       you are following this example, use the control-plane clustered ESXi
       hosts and control-plane DVS port group for the controller settings.
      </para>
     </step>
     <step>
      <para>
       If it has not already been done, create an IP pool for the NSX
       Controller cluster with at least three IP addressess by clicking
       <guimenu>New IP Pool</guimenu>. Individual controllers can be in
       separate IP subnets, if necessary.
      </para>
     </step>
     <step>
      <para>
       Click <guimenu>OK</guimenu> to deploy the controller. After the first
       controller is completely deployed, deploy two additional controllers.
      </para>
     </step>
    </procedure>
    <important>
     <para>
      Three NSX controllers is mandatory. VMware recommends configuring a DRS
      anti-affinity rule to prevent the controllers from residing on the same
      ESXi host. See more information about
      <link
    xlink:href="https://pubs.vmware.com/vsphere-51/index.jsp?topic=%2Fcom.vmware.vsphere.resmgmt.doc%2FGUID-FF28F29C-8B67-4EFF-A2EF-63B3537E6934.html">DRS
      Affinity Rules</link>.
     </para>
    </important>
   </section>
   <section>
    <title>Prepare Clusters for NSX Management</title>
    <para>
     During <guimenu>Host Preparation</guimenu>, the NSX Manager:
    </para>
    <itemizedlist>
     <listitem>
      <para>
       Installs the NSX kernel modules on ESXi hosts that are members of
       vSphere clusters
      </para>
     </listitem>
     <listitem>
      <para>
       Builds the NSX control-plane and management-plane infrastructure
      </para>
     </listitem>
    </itemizedlist>
    <para>
     The NSX kernel modules are packaged in <filename>VIB</filename> (vSphere
     Installation Bundle) files. They run within the hypervisor kernel and
     provide services such as distributed routing, distributed firewall, and
     VXLAN bridging capabilities. These files are installed on a per-cluster
     level, and the setup process deploys the required software on all ESXi
     hosts in the target cluster. When a new ESXi host is added to the cluster,
     the required software is automatically installed on the newly added host.
    </para>
    <para>
     Before beginning the NSX host preparation process, make sure of the
     following in your environment:
    </para>
    <itemizedlist>
     <listitem>
      <para>
       Register vCenter with NSX Manager and deploy the NSX controllers.
      </para>
     </listitem>
     <listitem>
      <para>
       Verify that DNS reverse lookup returns a fully qualified domain name
       when queried with the IP address of NSX Manager.
      </para>
     </listitem>
     <listitem>
      <para>
       Verify that the ESXi hosts can resolve the DNS name of vCenter server.
      </para>
     </listitem>
     <listitem>
      <para>
       Verify that the ESXi hosts can connect to vCenter Server on port 80.
      </para>
     </listitem>
     <listitem>
      <para>
       Verify that the network time on vCenter Server and the ESXi hosts is
       synchronized.
      </para>
     </listitem>
     <listitem>
      <para>
       For each vSphere cluster that will participate in NSX, verify that the
       ESXi hosts within each respective cluster are attached to a common VDS.
      </para>
      <para>
       For example, given a deployment with two clusters named Host1 and Host2.
       Host1 is attached to VDS1 and VDS2. Host2 is attached to VDS1 and VDS3.
       When you prepare a cluster for NSX, you can only associate NSX with VDS1
       on the cluster. If you add another host (Host3) to the cluster and Host3
       is not attached to VDS1, it is an invalid configuration, and Host3 will
       not be ready for NSX functionality.
      </para>
     </listitem>
     <listitem>
      <para>
       If you have vSphere Update Manager (VUM) in your environment, you must
       disable it before preparing clusters for network virtualization. For
       information on how to check if VUM is enabled and how to disable it if
       necessary, see the
       <link
    xlink:href="http://kb.vmware.com/kb/2053782">VMware knowledge
       base</link>.
      </para>
     </listitem>
     <listitem>
      <para>
       In the vSphere web client, ensure that the cluster is in the resolved
       state (listed under the <guimenu>Host Preparation</guimenu> tab). If the
       Resolve option does not appear in the cluster's Actions list, then it is
       in a resolved state.
      </para>
     </listitem>
    </itemizedlist>
    <para>
     To prepare the vSphere clusters for NSX:
    </para>
    <procedure>
     <step>
      <para>
       In vCenter, select
       <menuchoice><guimenu>Home</guimenu><guimenu>Networking &amp;
       Security</guimenu><guimenu>Installation</guimenu></menuchoice>, and then
       select the <guimenu>Host Preparation</guimenu> tab.
      </para>
     </step>
     <step>
      <para>
       Continuing with the example in these instructions, click on the
       <guimenu>Actions</guimenu> button (gear icon) and select
       <guimenu>Install</guimenu> for both the control-plane cluster and
       compute cluster (if you are using something other than this example,
       then only install on the clusters that require NSX logical switching,
       routing, and firewalls).
      </para>
     </step>
     <step>
      <para>
       Monitor the installation until the <literal>Installation
       Status</literal> column displays a green check mark.
      </para>
      <important>
       <para>
        While installation is in progress, do not deploy, upgrade, or uninstall
        any service or component.
       </para>
      </important>
      <important>
       <para>
        If the <literal>Installation Status</literal> column displays a red
        warning icon and says <literal>Not Ready</literal>, click
        <guimenu>Resolve</guimenu>. Clicking <guimenu>Resolve</guimenu> might
        result in a reboot of the host. If the installation is still not
        successful, click the warning icon. All errors will be displayed. Take
        the required action and click <guimenu>Resolve</guimenu> again.
       </para>
      </important>
     </step>
     <step>
      <para>
       To verify the VIBs (<filename>esx-vsip</filename> and
       <filename>esx-vxlan</filename>) are installed and registered, SSH into
       an ESXi host within the prepared cluster. List the names and versions of
       the VIBs installed by running the following command:
      </para>
<screen>&prompt.user;esxcli software vib list | grep esx</screen>
<screen>...
esx-vsip      6.0.0-0.0.2732470    VMware  VMwareCertified   2015-05-29
esx-vxlan     6.0.0-0.0.2732470    VMware  VMwareCertified   2015-05-29
...
   </screen>
     </step>
    </procedure>
    <important>
     <para>
      After host preparation:
     </para>
     <itemizedlist>
      <listitem>
       <para>
        A host reboot is not required
       </para>
      </listitem>
      <listitem>
       <para>
        If you add a host to a prepared cluster, the NSX VIBs are automatically
        installed on the host.
       </para>
      </listitem>
      <listitem>
       <para>
        If you move a host to an unprepared cluster, the NSX VIBs are
        automatically uninstalled from the host. In this case, a host reboot is
        required to complete the uninstall process.
       </para>
      </listitem>
     </itemizedlist>
    </important>
   </section>
   <section>
    <title>Configure VXLAN Transport Parameters</title>
    <para>
     VXLAN is configured on a per-cluster basis, where each vSphere cluster
     that is to participate in NSX is mapped to a vSphere Distributed Virtual
     Switch (DVS). When mapping a vSphere cluster to a DVS, each ESXi host in
     that cluster is enabled for logical switches. The settings chosen in this
     section will be used in creating the VMkernel interface.
    </para>
    <para>
     Configuring transport parameters involves selecting a DVS, a VLAN ID, an
     MTU size, an IP addressing mechanism, and a NIC teaming policy. The MTU
     for each switch must be set to 1550 or higher. By default, it is set to
     1600 by NSX. This is also the recommended setting for integration with
     &ostack;.
    </para>
    <para>
     To configure the VXLAN transport parameters:
    </para>
    <procedure>
     <step>
      <para>
       In the vSphere web client, navigate to
       <menuchoice><guimenu>Home</guimenu><guimenu>Networking &amp;
       Security</guimenu><guimenu>Installation</guimenu></menuchoice>.
      </para>
     </step>
     <step>
      <para>
       Select the <guimenu>Host Preparation</guimenu> tab.
      </para>
     </step>
     <step>
      <para>
       Click the <guimenu>Configure</guimenu> link in the VXLAN column.
      </para>
     </step>
     <step>
      <para>
       Enter the required information.
      </para>
     </step>
     <step>
      <para>
       If you have not already done so, create an IP pool for the VXLAN tunnel
       end points (VTEP) by clicking <guimenu>New IP Pool</guimenu>:
      </para>
     </step>
     <step>
      <para>
       Click <guimenu>OK</guimenu> to create the VXLAN network.
      </para>
     </step>
    </procedure>
    <para>
     When configuring the VXLAN transport network, consider the following:
    </para>
    <itemizedlist>
     <listitem>
      <para>
       Use a NIC teaming policy that best suits the environment being built.
       <literal>Load Balance - SRCID</literal> as the VMKNic teaming policy is
       usually the most flexible out of all the available options. This allows
       each host to have a VTEP vmkernel interface for each dvuplink on the
       selected distributed switch (two dvuplinks gives two VTEP interfaces per
       ESXi host).
      </para>
     </listitem>
     <listitem>
      <para>
       Do not mix different teaming policies for different portgroups on a VDS
       where some use Etherchannel or Link Aggregation Control Protocol (LACPv1
       or LACPv2) and others use a different teaming policy. If uplinks are
       shared in these different teaming policies, traffic will be interrupted.
       If logical routers are present, there will be routing problems. Such a
       configuration is not supported and should be avoided.
      </para>
     </listitem>
     <listitem>
      <para>
       For larger environments it may be better to use DHCP for the VMKNic IP
       Addressing.
      </para>
     </listitem>
     <listitem>
      <para>
       For more information and further guidance, see the
       <link
xlink:href="https://communities.vmware.com/docs/DOC-27683">VMware
       NSX for vSphere Network Virtualization Design Guide</link>.
      </para>
     </listitem>
    </itemizedlist>
   </section>
   <section>
    <title>Assign Segment ID Pool</title>
    <para>
     Each VXLAN tunnel will need a segment ID to isolate its network traffic.
     Therefore, it is necessary to configure a segment ID pool for the NSX
     VXLAN network to use. If an NSX controller is not deployed within the
     vSphere environment, a multicast address range must be added to spread
     traffic across the network and avoid overloading a single multicast
     address.
    </para>
    <para>
     For the purposes of the example in these instructions, do the following
     steps to assign a segment ID pool. Otherwise, follow best practices as
     outlined in
     <link
  xlink:href="https://pubs.vmware.com/NSX-62/index.jsp?topic=%2Fcom.vmware.nsx.install.doc%2FGUID-7B33DE72-78A7-448C-A61C-9B41D1EB12AD.html">VMware's
     documentation</link>.
    </para>
    <procedure>
     <step>
      <para>
       In the vSphere web client, navigate to
       <menuchoice><guimenu>Home</guimenu><guimenu>Networking &amp;
       Security</guimenu><guimenu>Installation</guimenu></menuchoice>.
      </para>
     </step>
     <step>
      <para>
       Select the <guimenu>Logical Network Preparation</guimenu> tab.
      </para>
     </step>
     <step>
      <para>
       Click <guimenu>Segment ID</guimenu>, and then <guimenu>Edit</guimenu>.
      </para>
     </step>
     <step>
      <para>
       Click <guimenu>OK</guimenu> to save your changes.
      </para>
     </step>
    </procedure>
   </section>
   <section>
    <title>Assign Segment ID Pool</title>
    <para>
     Each VXLAN tunnel will need a segment ID to isolate its network traffic.
     Therefore, it is necessary to configure a segment ID pool for the NSX
     VXLAN network to use. If an NSX controller is not deployed within the
     vSphere environment, a multicast address range must be added to spread
     traffic across the network and avoid overloading a single multicast
     address.
    </para>
    <para>
     For the purposes of the example in these instructions, do the following
     steps to assign a segment ID pool. Otherwise, follow best practices as
     outlined in
     <link
  xlink:href="https://pubs.vmware.com/NSX-62/index.jsp?topic=%2Fcom.vmware.nsx.install.doc%2FGUID-7B33DE72-78A7-448C-A61C-9B41D1EB12AD.html">VMware's
     documentation</link>.
    </para>
    <procedure>
     <step>
      <para>
       In the vSphere web client, navigate to
       <menuchoice><guimenu>Home</guimenu><guimenu>Networking &amp;
       Security</guimenu><guimenu>Installation</guimenu></menuchoice>.
      </para>
     </step>
     <step>
      <para>
       Select the <guimenu>Logical Network Preparation</guimenu> tab.
      </para>
     </step>
     <step>
      <para>
       Click <guimenu>Segment ID</guimenu>, and then <guimenu>Edit</guimenu>.
      </para>
     </step>
     <step>
      <para>
       Click <guimenu>OK</guimenu> to save your changes.
      </para>
     </step>
    </procedure>
   </section>
   <section>
    <title>Create a Transport Zone</title>
    <para>
     A transport zone controls which hosts a logical switch can reach and has
     the following characteristics.
    </para>
    <itemizedlist>
     <listitem>
      <para>
       It can span one or more vSphere clusters.
      </para>
     </listitem>
     <listitem>
      <para>
       Transport zones dictate which clusters can participate in the use of a
       particular network. Therefore they dictate which VMs can participate in
       the use of a particular network.
      </para>
     </listitem>
     <listitem>
      <para>
       A vSphere NSX environment can contain one or more transport zones based
       on the environment's requirements.
      </para>
     </listitem>
     <listitem>
      <para>
       A host cluster can belong to multiple transport zones.
      </para>
     </listitem>
     <listitem>
      <para>
       A logical switch can belong to only one transport zone.
      </para>
     </listitem>
    </itemizedlist>
    <note>
     <para>
      &ostack; has only been verified to work with a single transport zone
      within a vSphere NSX-v environment. Other configurations are currently
      not supported.
     </para>
    </note>
    <para>
     For more information on transport zones, refer to
     <link
   xlink:href="https://pubs.vmware.com/NSX-62/topic/com.vmware.nsx.install.doc/GUID-0B3BD895-8037-48A8-831C-8A8986C3CA42.html">VMware's
     Add A Transport Zone</link>.
    </para>
    <para>
     To create a transport zone:
    </para>
    <procedure>
     <step>
      <para>
       In the vSphere web client, navigate to
       <menuchoice><guimenu>Home</guimenu><guimenu>Networking &amp;
       Security</guimenu><guimenu>Installation</guimenu></menuchoice>.
      </para>
     </step>
     <step>
      <para>
       Select the <guimenu>Logical Network Preparation</guimenu> tab.
      </para>
     </step>
     <step>
      <para>
       Click <guimenu>Transport Zones</guimenu>, and then click the
       <guimenu>New Transport Zone</guimenu> (New Logical Switch) icon.
      </para>
     </step>
     <step>
      <para>
       In the <guimenu>New Transport Zone</guimenu> dialog box, type a name and
       an optional description for the transport zone.
      </para>
     </step>
     <step>
      <para>
       For these example instructions, select the control plane mode as
       <literal>Unicast</literal>.
      </para>
      <note>
       <para>
        Whether there is a controller in the environment or if the environment
        is going to use multicast addresses will determine the control plane
        mode to select:
       </para>
       <itemizedlist>
        <listitem>
         <para>
          <literal>Unicast</literal> (what this set of instructions uses): The
          control plane is handled by an NSX controller. All unicast traffic
          leverages optimized headend replication. No multicast IP addresses or
          special network configuration is required.
         </para>
        </listitem>
        <listitem>
         <para>
          <literal>Multicast</literal>: Multicast IP addresses in the physical
          network are used for the control plane. This mode is recommended only
          when upgrading from older VXLAN deployments. Requires PIM/IGMP in the
          physical network.
         </para>
        </listitem>
        <listitem>
         <para>
          <literal>Hybrid</literal>: Offloads local traffic replication to the
          physical network (L2 multicast). This requires IGMP snooping on the
          first-hop switch and access to an IGMP querier in each VTEP subnet,
          but does not require PIM. The first-hop switch handles traffic
          replication for the subnet.
         </para>
        </listitem>
       </itemizedlist>
      </note>
     </step>
     <step>
      <para>
       Select the clusters to be added to the transport zone.
      </para>
     </step>
     <step>
      <para>
       Click <guimenu>OK</guimenu> to save your changes.
      </para>
     </step>
    </procedure>
   </section>
   <section>
    <title>Deploying &productname;</title>
    <para>
     With vSphere environment setup completed, the &ostack; can be deployed.
     The following sections will cover creating virtual machines within the
     vSphere environment, configuring the cloud model and integrating NSX-v
     &o_netw; core plugin into the &ostack;:
    </para>
    <procedure>
     <step>
      <para>
       Create the virtual machines
      </para>
     </step>
     <step>
      <para>
       Deploy the &lcm;
      </para>
     </step>
     <step>
      <para>
       Configure the &o_netw; environment with NSX-v
      </para>
     </step>
     <step>
      <para>
       Modify the cloud input model
      </para>
     </step>
     <step>
      <para>
       Set up the parameters
      </para>
     </step>
     <step>
      <para>
       Deploy the Operating System with Cobbler
      </para>
     </step>
     <step>
      <para>
       Deploy the cloud
      </para>
     </step>
    </procedure>
   </section>
   <section>
    <title>Deploying &productname; on Baremetal</title>
    <para>
     Within the vSphere environment, create the &ostack; compute proxy virtual
     machines. There needs to be one &o_netw; compute proxy virtual machine per
     ESXi compute cluster.
    </para>
    <para>
     For the minimum NSX hardware requirements, refer to
     <xref
    linkend="nsx-hw-reqs-bm"/>. Also be aware of the networking
     model to use for the VM network interfaces, see
     <xref linkend="nsx-interface-reqs"/>:
    </para>
    <para>
     If ESX VMs are to be used as &o_comp; compute proxy nodes, set up three
     LAN interfaces in each virtual machine as shown in the table below. There
     is at least one &o_comp; compute proxy node per cluster.
    </para>
    <table xml:id="nsx-interface-reqs">
     <title>NSX Interface Requirements</title>
     <tgroup cols="2">
      <colspec colnum="1" colname="1" colwidth="50*"/>
      <colspec colnum="2" colname="2" colwidth="50*"/>
      <thead>
       <row>
        <entry>
         <para>
          Network Group
         </para>
        </entry>
        <entry>
         <para>
          Interface
         </para>
        </entry>
       </row>
      </thead>
      <tbody>
       <row>
        <entry>
         <para>
          Management
         </para>
        </entry>
        <entry>
         <para>
          <literal>eth0</literal>
         </para>
        </entry>
       </row>
       <row>
        <entry>
         <para>
          External API
         </para>
        </entry>
        <entry>
         <para>
          <literal>eth1</literal>
         </para>
        </entry>
       </row>
       <row>
        <entry>
         <para>
          Internal API
         </para>
        </entry>
        <entry>
         <para>
          <literal>eth2</literal>
         </para>
        </entry>
       </row>
      </tbody>
     </tgroup>
    </table>
    <section>
     <title>Advanced Configuration Option</title>
     <important>
      <para>
       Within vSphere for each in the virtual machine:
      </para>
      <itemizedlist>
       <listitem>
        <para>
         In the <guimenu>Options</guimenu> section, under <guimenu>Advanced
         configuration parameters</guimenu>, ensure that
         <literal>disk.EnableUUIDoption</literal> is set to
         <literal>true</literal>.
        </para>
       </listitem>
       <listitem>
        <para>
         If the option does not exist, it must be added. This option is
         required for the &ostack; deployment.
        </para>
       </listitem>
       <listitem>
        <para>
         If the option is not specified, then the deployment will fail when
         attempting to configure the disks of each virtual machine.
        </para>
       </listitem>
      </itemizedlist>
     </important>
    </section>
<!-- Setting Up the &lcm; -->
    <section>
     <xi:include xpointer="element(/1/4/1)" href="installation-kvm_xpointer.xml"/>
     <xi:include xpointer="element(/1/4/2)" href="installation-kvm_xpointer.xml"/>
    </section>
<!-- Setting Up NSX -->
    <section>
     <title>Configure the Neutron Environment with NSX-v</title>
     <para>
      In summary, integrating NSX with vSphere has four major steps:
     </para>
     <procedure>
      <step>
       <para>
        Modify the input model to define the server roles, servers, network
        roles and networks. <xref linkend="nsx-modify-input-model"/>
       </para>
      </step>
      <step>
       <para>
        Set up the parameters needed for &o_netw; and &o_comp; to communicate
        with the ESX and NSX Manager. <xref linkend="nsx-deploy-os-cobbler"/>
       </para>
      </step>
      <step>
       <para>
        Do the steps to deploy the cloud. <xref linkend="nsx-deploy-cloud"/>
       </para>
      </step>
     </procedure>
     <section>
      <title>Third-Party Import of VMware NSX-v Into &o_netw; and Neutronclient</title>
      <para>
       To import the NSX-v &o_netw; core-plugin into &lcm;, run the third-party
       import playbook.
      </para>
<screen>&prompt.ardana;cd ~/openstack/ardana/ansible
&prompt.ardana;ansible-playbook -i hosts/localhost third-party-import.yml</screen>
     </section>
     <section>
      <title>Modify the Input Model</title>
      <para>
       After the third-party import has completed successfully, modify the
       input model:
      </para>
      <procedure>
       <step>
        <para>
         Prepare for input model changes
        </para>
       </step>
       <step>
        <para>
         Define the servers and server roles needed for a NSX-v cloud.
        </para>
       </step>
       <step>
        <para>
         Define the necessary networks and network groups
        </para>
       </step>
       <step>
        <para>
         Specify the services needed to be deployed on the &lcm; controllers
         and the &o_comp; ESX compute proxy nodes.
        </para>
       </step>
       <step>
        <para>
         Commit the changes and run the configuration processor.
        </para>
       </step>
      </procedure>
      <section>
       <title>Prepare for Input Model Changes</title>
       <para>
        The previous steps created a modified &productname; tarball with the
        NSX-v core plugin in the &o_netw; and <literal>neutronclient</literal>
        venvs. The <filename>tar</filename> file can now be extracted and the
        <filename>ardana-init.bash</filename> script can be run to set up the
        deployment files and directories. If a modified
        <filename>tar</filename> file was not created, then extract the tar
        from the /media/cdrom/ardana location.
       </para>
       <para>
        To run the <filename>ardana-init.bash</filename> script which is
        included in the build, use this commands:
       </para>
<screen>&prompt.ardana;~/ardana/hos-init.bash</screen>
      </section>
      <section>
       <title>Create the Input Model</title>
       <para>
        Copy the example input model to
        <filename>~/openstack/my_cloud/definition/</filename> directory:
       </para>
<screen>&prompt.ardana;cd ~/ardana-extensions/ardana-extensions-nsx/vmware/examples/models/entry-scale-nsx
&prompt.ardana;cp -R entry-scale-nsx ~/openstack/my_cloud/definition</screen>
       <para>
        Refer to the reference input model in
        <filename>ardana-extensions/ardana-extensions-nsx/vmware/examples/models/entry-scale-nsx/</filename>
        for details about how these definitions should be made. The main
        differences between this model and the standard &lcm; input models are:
       </para>
       <itemizedlist>
        <listitem>
         <para>
          Only the neutron-server is deployed. No other neutron agents are
          deployed.
         </para>
        </listitem>
        <listitem>
         <para>
          Additional parameters need to be set in
          <filename>pass_through.yml</filename> and
          <filename>nsx/nsx_config.yml</filename>.
         </para>
        </listitem>
        <listitem>
         <para>
          Nova ESX compute proxy nodes may be ESX virtual machines.
         </para>
        </listitem>
       </itemizedlist>
       <section>
        <title>Set up the Parameters</title>
        <para>
         The special parameters needed for the NSX-v integrations are set in
         the files <filename>pass_through.yml</filename> and
         <filename>nsx/nsx_config.yml</filename>. They are in the
         <filename>~/openstack/my_cloud/definition/data</filename> directory.
        </para>
        <para>
         Parameters in <filename>pass_through.yml</filename> are in the sample
         input model in the
         <filename>ardana-extensions/ardana-extensions-nsx/vmware/examples/models/entry-scale-nsx/</filename>
         directory. The comments in the sample input model file describe how to
         locate the values of the required parameters.
        </para>
<screen>#
# (c) Copyright 2017 SUSE LLC
#
# Licensed under the Apache License, Version 2.0 (the "License"); you may
# not use this file except in compliance with the License. You may obtain
# a copy of the License at
#
# http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
# WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
# License for the specific language governing permissions and limitations
# under the License.
#
---
product:
  version: 2
pass-through:
  global:
    vmware:
      - username: <replaceable>VCENTER_ADMIN_USERNAME</replaceable>
        ip: <replaceable>VCENTER_IP</replaceable>
        port: 443
        cert_check: false
        # The password needs to be encrypted using the script
        # openstack/ardana/ansible/ardanaencrypt.py on the deployer:
        #
        # $ cd ~/openstack/ardana/ansible
        # $ export ARDANA_USER_PASSWORD_ENCRYPT_KEY=<replaceable>ENCRYPTION_KEY</replaceable>
        # $ ./ardanaencrypt.py
        #
        # The script will prompt for the vCenter password. The string
        # generated is the encrypted password. Enter the string
        # enclosed by double-quotes below.
        password: &quot;<replaceable>ENCRYPTED_PASSWD_FROM_ARDANAENCRYPT</replaceable>&quot;

        # The id is is obtained by the URL
        # https://<replaceable>VCENTER_IP</replaceable>/mob/?moid=ServiceInstance&amp;doPath=content%2eabout,
        # field instanceUUID.
        id: <replaceable>VCENTER_UUID</replaceable>
  servers:
    -
      # Here the 'id' refers to the name of the node running the
      # esx-compute-proxy. This is identical to the 'servers.id' in
      # servers.yml. There should be one esx-compute-proxy node per ESX
      # resource pool.
      id: esx-compute1
      data:
        vmware:
          vcenter_cluster: <replaceable>VMWARE_CLUSTER1_NAME</replaceable>
          vcenter_id: <replaceable>VCENTER_UUID</replaceable>
    -
      id: esx-compute2
      data:
        vmware:
          vcenter_cluster: <replaceable>VMWARE_CLUSTER2_NAME</replaceable>
          vcenter_id: <replaceable>VCENTER_UUID</replaceable>
  </screen>
        <para>
         There are parameters in <filename>nsx/nsx_config.yml</filename>. The
         comments describes how to retrieve the values.
        </para>
<screen># (c) Copyright 2017 SUSE LLC
#
# Licensed under the Apache License, Version 2.0 (the &quot;License&quot;); you may
# not use this file except in compliance with the License. You may obtain
# a copy of the License at
#
# http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an &quot;AS IS&quot; BASIS, WITHOUT
# WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
# License for the specific language governing permissions and limitations
# under the License.
#
---
  product:
    version: 2
  configuration-data:
    - name: NSX-CONFIG-CP1
      services:
        - nsx
      data:
        # (Required) URL for NSXv manager (e.g - https://management_ip).
        manager_uri: 'https://<replaceable>NSX_MGR_IP</replaceable>

        # (Required) NSXv username.
        user: 'admin'

        # (Required) Encrypted NSX Manager password.
        # Password encryption is done by the script
        # ~/openstack/ardana/ansible/ardanaencrypt.py on the deployer:
        #
        # $ cd ~/openstack/ardana/ansible
        # $ export ARDANA_USER_PASSWORD_ENCRYPT_KEY=<replaceable>ENCRYPTION_KEY</replaceable>
        # $ ./ardanaencrypt.py
        #
        # NOTE: Make sure that the NSX Manager password is encrypted with the same key
        # used to encrypt the VCenter password.
        #
        # The script will prompt for the NSX Manager password. The string
        # generated is the encrypted password. Enter the string enclosed
        # by double-quotes below.
        password: &quot;<replaceable>ENCRYPTED_NSX_MGR_PASSWD_FROM_ARDANAENCRYPT</replaceable>&quot;
        # (Required) datacenter id for edge deployment.
        # Retrieved using
        #    http://<replaceable>VCENTER_IP_ADDR</replaceable>/mob/?moid=ServiceInstance&amp;doPath=content
        # click on the value from the rootFolder property. The datacenter_moid is
        # the value of the childEntity property.
        # The vCenter-ip-address comes from the file pass_through.yml in the
        # input model under &quot;pass-through.global.vmware.ip&quot;.
        datacenter_moid: 'datacenter-21'
        # (Required) id of logic switch for physical network connectivity.
        # How to retrieve
        # 1. Get to the same page where the datacenter_moid is found.
        # 2. Click on the value of the rootFolder property.
        # 3. Click on the value of the childEntity property
        # 4. Look at the network property. The external network is
        #    network associated with EXTERNAL VM in VCenter.
        external_network: 'dvportgroup-74'
        # (Required) clusters ids containing OpenStack hosts.
        # Retrieved using http://<replaceable>VCENTER_IP_ADDR</replaceable>/mob, click on the value
        # from the rootFolder property. Then click on the value of the
        # hostFolder property. Cluster_moids are the values under childEntity
        # property of the compute clusters.
        cluster_moid: 'domain-c33,domain-c35'
        # (Required) resource-pool id for edge deployment.
        resource_pool_id: 'resgroup-67'
        # (Optional) datastore id for edge deployment. If not needed,
        # do not declare it.
        # datastore_id: 'datastore-117'

        # (Required) network scope id of the transport zone.
        # To get the vdn_scope_id, in the vSphere web client from the Home
        # menu:
        #   1. click on Networking &amp; Security
        #   2. click on installation
        #   3. click on the Logical Netowrk Preparation tab.
        #   4. click on the Transport Zones button.
        #   5. Double click on the transport zone being configure.
        #   6. Select Manage tab.
        #   7. The vdn_scope_id will appear at the end of the URL.
        vdn_scope_id: 'vdnscope-1'

        # (Optional) Dvs id for VLAN based networks. If not needed,
        # do not declare it.
        # dvs_id: 'dvs-68'

        # (Required) backup_edge_pool: backup edge pools management range,
        # - edge_type>[edge_size]:<replaceable>MINIMUM_POOLED_EDGES</replaceable>:<replaceable>MAXIMUM_POOLED_EDGES</replaceable>
        # - edge_type: service (service edge) or  vdr (distributed edge)
        # - edge_size:  compact ,  large (by default),  xlarge  or  quadlarge
        backup_edge_pool: 'service:compact:4:10,vdr:compact:4:10'

        # (Optional) mgt_net_proxy_ips: management network IP address for
        # metadata proxy. If not needed, do not declare it.
        # mgt_net_proxy_ips: '10.142.14.251,10.142.14.252'

        # (Optional) mgt_net_proxy_netmask: management network netmask for
        # metadata proxy. If not needed, do not declare it.
        # mgt_net_proxy_netmask: '255.255.255.0'

        # (Optional) mgt_net_moid: Network ID for management network connectivity
        # Do not declare if not used.
        # mgt_net_moid: 'dvportgroup-73'

        # ca_file: Name of the certificate file. If insecure is set to True,
        # then this parameter is ignored. If insecure is set to False and this
        # parameter is not defined, then the system root CAs will be used
        # to verify the server certificate.
        ca_file: a/nsx/certificate/file

        # insecure:
        # If true (default), the NSXv server certificate is not verified.
        # If false, then the default CA truststore is used for verification.
        # This option is ignored if &quot;ca_file&quot; is set
        insecure: True
        # (Optional) edge_ha: if true, will duplicate any edge pool resources
        # Default to False if undeclared.
        # edge_ha: False
        # (Optional) spoofguard_enabled:
        # If True (default), indicates NSXV spoofguard component is used to
        # implement port-security feature.
        # spoofguard_enabled: True
        # (Optional) exclusive_router_appliance_size:
        # Edge appliance size to be used for creating exclusive router.
        # Valid values: 'compact', 'large', 'xlarge', 'quadlarge'
        # Defaults to 'compact' if not declared.  # exclusive_router_appliance_size:
        'compact'
       </screen>
       </section>
      </section>
      <section>
       <title>Commit Changes and Run the Configuration Processor</title>
       <para>
        Commit your changes with the input model and the required configuration
        values added to the <filename>pass_through.yml</filename> and
        <filename>nsx/nsx_config.yml</filename> files.
       </para>
<screen><?dbsuse-fo font-size="0.70em"?>
&prompt.ardana;cd ~/openstack/my_cloud/definition
&prompt.ardana;git commit -A -m &quot;Configuration changes for NSX deployment&quot;
&prompt.ardana;cd ~/openstack/ardana/ansible
&prompt.ardana;ansible-playbook -i hosts/localhost config-processor-run.yml \
 -e \encrypt=&quot;&quot; -e rekey=&quot;&quot;</screen>
       <para>
        If the playbook <filename>config-processor-run.yml</filename> fails,
        there is an error in the input model. Fix the error and repeat the
        above steps.
       </para>
      </section>
     </section>
     <section>
      <title>Deploying the Operating System with Cobbler</title>
      <procedure>
       <step>
        <para>
         From the &lcm;, run Cobbler to install the operating system on the
         nodes after it has to be deployed:
        </para>
<screen>&prompt.ardana;cd ~/openstack/ardana/ansible
&prompt.ardana;ansible-playbook -i hosts/localhost cobbler-deploy.yml
   </screen>
       </step>
       <step>
        <para>
         Verify the nodes that will have an operating system installed by
         Cobbler by running this command:
        </para>
<screen>&prompt.sudo;cobbler system find --netboot-enabled=1</screen>
       </step>
       <step>
        <para>
         Reimage the nodes using Cobbler. Do not use Cobbler to reimage the
         nodes running as ESX virtual machines. The command below is run on a
         setup where the &o_comp; ESX compute proxies are VMs. Controllers 1,
         2, and 3 are running on physical servers.
        </para>
<screen>&prompt.ardana;cd ~/openstack/ardana/ansible
&prompt.ardana;ansible-playbook -i hosts/localhost bm-reimage.yml -e \
   nodelist=controller1,controller2,controller3</screen>
       </step>
       <step>
        <para>
         When the playbook has completed, each controller node should have an
         operating system installed with an IP address configured on
         <literal>eth0</literal>.
        </para>
       </step>
       <step>
        <para>
         After your controller nodes have been completed, you should install
         the operating system on your &o_comp; compute proxy virtual machines.
         Each configured virtual machine should be able to PXE boot into the
         operating system installer.
        </para>
       </step>
       <step>
        <para>
         From within the vSphere environment, power on each &o_comp; compute
         proxy virtual machine and watch for it to PXE boot into the OS
         installer via its console.
        </para>
        <substeps>
         <step>
          <para>
           If successful, the virtual machine will have the operating system
           automatically installed and will then automatically power off.
          </para>
         </step>
         <step>
          <para>
           When the virtual machine has powered off, power it on and let it
           boot into the operating system.
          </para>
         </step>
        </substeps>
       </step>
       <step>
        <para>
         Verify network settings after deploying the operating system to each
         node.
        </para>
        <itemizedlist>
         <listitem>
          <para>
           Verify that the NIC bus mapping specified in the cloud model input
           file
           (<filename>~/ardana/my_cloud/definition/data/nic_mappings.yml</filename>)
           matches the NIC bus mapping on each &ostack; node.
          </para>
          <para>
           Check the NIC bus mapping with this command:
          </para>
<screen>&prompt.sudo;cobbler system list</screen>
         </listitem>
         <listitem>
          <para>
           After the playbook has completed, each controller node should have
           an operating system installed with an IP address configured on eth0.
          </para>
         </listitem>
        </itemizedlist>
       </step>
       <step>
        <para>
         When the ESX compute proxy nodes are VMs, install the operating system
         if you have not already done so.
        </para>
       </step>
      </procedure>
     </section>
     <section>
      <title>Deploying the Cloud</title>
      <para>
       When the configuration processor has completed successfully, the cloud
       can be deployed. Set the ARDANA_USER_PASSWORD_ENCRYPT_KEY environment
       variable before running <filename>site.yml</filename>.
      </para>
<screen>&prompt.ardana;cd ~/openstack/ardana/ansible
&prompt.ardana;ansible-playbook -i hosts/localhost ready-deployment.yml
&prompt.ardana;cd ~/scratch/ansible/next/ardana/ansible
&prompt.ardana;export ARDANA_USER_PASSWORD_ENCRYPT_KEY=<replaceable>PASSWORD_KEY</replaceable>
&prompt.ardana;ansible-playbook -i hosts/verb_hosts site.yml
&prompt.ardana;ansible-playbook -i hosts/verb_hosts ardana-cloud-configure.yml</screen>
      <para>
       <replaceable>PASSWORD_KEY</replaceable> in the <literal>export</literal>
       command is the key used to encrypt the passwords for vCenter and NSX
       Manager.
      </para>
     </section>
    </section>
   </section>
  </section>
 </section>
</section>
