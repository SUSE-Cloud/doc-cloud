<?xml version="1.0"?>
<!DOCTYPE section [
 <!ENTITY % entities SYSTEM "entities.ent"> %entities;
]>
<section xml:id="ceph_storage_usage"
 xmlns="http://docbook.org/ns/docbook" version="5.1"
 xmlns:xi="http://www.w3.org/2001/XInclude"
 xmlns:xlink="http://www.w3.org/1999/xlink">
 <title>Usage of Ceph Storage</title>
 <para>
  Ceph is a versatile storage technology that facilitates the consumption of
  storage by using multiple protocols. Ceph is closely integrated with
  &kw-hos-tm; services to support various storage use cases such as the use of
  storage pools for cinder-volume and glance data store. &kw-hos-tm; further
  simplifies administrator tasks by providing automated playbooks for various
  storage use cases.
 </para>
 <itemizedlist xml:id="ul_gx3_sdd_jw">
  <listitem>
   <para>
    Creating storage pools
   </para>
  </listitem>
  <listitem>
   <para>
    Deploying Ceph client components on client nodes
   </para>
  </listitem>
  <listitem>
   <para>
    Configuring OpenStack services with storage pools
   </para>
  </listitem>
 </itemizedlist>
 <para>
  The following section guides you through the following common scenarios that
  you might come across while consuming Ceph for various storage use cases.
 </para>
 <orderedlist xml:id="ol_ix2_bdd_jw">
  <listitem>
   <para>
    <!-- FIXME: <link xlink:href="#ceph_storage_usage/setup-deployer-node">Set Up Your Deployer as a Ceph Client Node</link> -->
   </para>
  </listitem>
  <listitem>
   <para>
    <!-- FIXME: <link xlink:href="#ceph_storage_usage/using-core-ceph-openstack-service">Using Core Ceph for OpenStack Services</link> -->
   </para>
   <orderedlist xml:id="ol_kxs_nqf_kw">
    <listitem>
     <para>
    <!-- FIXME: <link xlink:href="#ceph_storage_usage/preq">Prerequisites</link> -->
     </para>
    </listitem>
    <listitem>
     <para>
    <!-- FIXME: <link xlink:href="#ceph_storage_usage/glance-data-store">Use Ceph Storage Pools as Glance Data Stores</link> -->
     </para>
    </listitem>
    <listitem>
     <para>
    <!-- FIXME: <link xlink:href="#ceph_storage_usage/cinder-volume-backend">Use Ceph Storage Pools as Cinder Volume Backends</link> -->
     </para>
    </listitem>
    <listitem>
     <para>
    <!-- FIXME: <link xlink:href="#ceph_storage_usage/cinder-backup-device">Use Ceph Storage Pools as Cinder Backup Devices</link> -->
     </para>
    </listitem>
   </orderedlist>
  </listitem>
  <listitem>
   <para>
    <!-- FIXME: <link xlink:href="#ceph_storage_usage/rados-gw-object-storage">Use RADOS Gateway to Access Objects Using S3/Swift API</link> -->
   </para>
  </listitem>
 </orderedlist>
 <para>
  Although you can also use Ceph storage pools as ephemeral storage backends
  for Nova, this practice is not formally supported.
 </para>
 <section>
  <title>Set Up Your Deployer as a Ceph Client Node</title>
  <para>
   By default, your deployer node is not configured as a Ceph client node. As a
   result, you cannot start your Ceph operation here. It is usually more
   helpful to set up your deployer node as an admin client node to perform
   various Ceph operations. This will help you manage the Ceph cluster without
   logging in to the monitor node. You can turn your deployer node into a
   client node by executing the following playbook.
  </para>
<screen>cd ~/scratch/ansible/next/hos/ansible
ansible-playbook -i hosts/verb_hosts ceph-setup-deployer-as-client.yml</screen>
  <para>
   After you complete the preceding playbook, your deployer node will be
   configured with an admin key ring and thus act as an admin node.
  </para>
 </section>
 <section>
  <title>Using Core Ceph OpenStack Services</title>
  <para>
   The use of Cinder for glance, cinder-volume, and cinder-backup requires pool
   creation and setting up ceph-clients on respective OpenStack service nodes.
   To simplify the workflow, the playbook
   <literal>ceph-client-prepare.yml</literal> is provided. This playbook reads
   pool configurations following the specification provided in
   <literal>ceph_user_model.yml</literal> and sets up an OpenStack client node.
   Perform the following steps:
  </para>
  <procedure>
   <step>
    <para>
     Define the user model.
    </para>
   </step>
   <step>
    <para>
     Run <literal>hosts/verb_hosts ceph-client-prepare.yml</literal>.
    </para>
    <para>
     The default user model provided with &kw-hos-tm; is mentioned below. The
     default configuration contains pool configuration for cinder-volume,
     glance, and cinder-backup. You can edit the file if you do not intend to
     use Ceph for all services or want to change pool attributes based on your
     cluster configuration. For example, if you have large number of disks then
     you can increase the Placement Group (PG) number for a given pool.
    </para>
<screen>---

product:
   version: 2

ceph_user_models:
    - user:
        name: cinder
        type: openstack
        secret_id: 457eb676-33da-42ec-9a8c-9293d545c337
      pools:
        - name: volumes
          attrs:
            creation_policy: eager
            type: replicated
            replica_size: 3
            permission: rwx
            pg: 100
          usage:
            consumer: cinder-volume
        - name: vms
          attrs:
            creation_policy: eager
            type: replicated
            replica_size: 3
            permission: rwx
            pg: 100
          usage:
            consumer: nova
        - name: images
          attrs:
            creation_policy: lazy
            permission: rx
    - user:
        name: glance
        type: openstack
      pools:
        - name: images
          attrs:
            creation_policy: eager
            type: replicated
            replica_size: 3
            permission: rwx
            pg: 128
          usage:
            consumer: glance-datastore
    - user:
        name: cinder-backup
        type: openstack
      pools:
        - name: backups
          attrs:
            creation_policy: eager
            type: replicated
            replica_size: 3
            permission: rwx
            pg: 128
          usage:
            consumer: cinder-backup</screen>
    <para>
     The following table provides the descriptions of the preceding parameters.
    </para>
    <informaltable xml:id="simpletable_k14_jyf_kw">
     <tgroup cols="4">
      <thead>
       <row>
        <entry>Parameter</entry>
        <entry>Value</entry>
        <entry>Description</entry>
        <entry>Recommendation</entry>
       </row>
      </thead>
      <tbody>
       <row>
        <entry>user.name</entry>
        <entry>A user-defined string.</entry>
        <entry>
         <para>
          Defines the name of the user who can access a set of pools. A user is
          created with same name in the Ceph system.
         </para>
        </entry>
        <entry>
         <para>
          Retain the default names for some of the well-known OpenStack users
          as described in the default user model. For example, cinder user for
          volume access who needs to access volumes, vms, and the images pool,
          as defined in the default model.
         </para>
        </entry>
       </row>
       <row>
        <entry>user.type</entry>
        <entry>OpenStack | user.</entry>
        <entry>
         <para>
          Indicates whether the user is specific for OpenStack services. This
          parameter does not have semantic implications.
         </para>
        </entry>
        <entry>
         <para>
          Use <emphasis role="bold">openstack</emphasis> for pools used by
          cinder-volume, cinder-backup, glance, and Nova services. For other
          services, you can choose <emphasis role="bold">user</emphasis>.
         </para>
        </entry>
       </row>
       <row>
        <entry>user.secret_id</entry>
        <entry>UUID instance.</entry>
        <entry>A unique UUID. </entry>
        <entry>
         <para>
          Generate a value for your cluster before setting up the Cinder
          client. The <literal>libvirt</literal> process needs this value to
          access the cluster while attaching a block device from Cinder.
         </para>
        </entry>
       </row>
       <row>
        <entry>pools.name</entry>
        <entry>A user-defined string.</entry>
        <entry>A user-defined name.</entry>
        <entry>
         <para>
          The name has to be unique in the Ceph namespace.
         </para>
        </entry>
       </row>
       <row>
        <entry>pools.attrs.creation_policy</entry>
        <entry>eager | lazy</entry>
        <entry>
         <para>
          If creation_policy is "eager," the playbooks will create the user. If
          the creation_policy is set to "lazy," the pool will be created
          externally (not by Ceph ansible playbooks) out of band.
         </para>
        </entry>
        <entry>
         <para>
          Retain default values for pools meant to be consumed by OpenStack
          services.
         </para>
        </entry>
       </row>
       <row>
        <entry>pools.attrs.type</entry>
        <entry>Replicated.</entry>
        <entry>
         <para>
          Defines the type of pool. Ceph supports erasure-coded and replicated
          pools.
         </para>
        </entry>
        <entry>
         <para>
          Retain the value as replicated if you do want to use
          <literal>ceph-client-prepare.yml</literal> for pool creation.
         </para>
         <para>
          This does not mean that you cannot create an erasure-code pool with
          your cluster deployed using &kw-hos-tm;. It just means that
          erasure-code pools are not officially supported.
          <!--and you have to take help of technical support.-->
         </para>
        </entry>
       </row>
       <row>
        <entry>pools.attrs.replica_size</entry>
        <entry>1..n </entry>
        <entry>Replica count.</entry>
        <entry>
         <para>
          Use 3 as the replica count because it is used in most common
          scenarios providing the right level of protection and is the minimum
          setting that &kw-hos; supports.
         </para>
        </entry>
       </row>
       <row>
        <entry>pools.attrs.permission</entry>
        <entry>rwx</entry>
        <entry>
         <para>
          Read, write, and execute the permission a user will have on given
          pool.
         </para>
        </entry>
        <entry>
         <para>
          Retain the values for OpenStack user as mentioned in the default user
          model. Altering these values might affect your client setup
          configuration.
         </para>
        </entry>
       </row>
       <row>
        <entry>pools.attrs.pg</entry>
        <entry>Placement group number.</entry>
        <entry>Number of placement groups.</entry>
        <entry>
         <para>
          The default value is 128. You can change the value of this parameter
          if your disk size is larger.
         </para>
        </entry>
       </row>
       <row>
        <entry>pools.usage.consumer</entry>
        <entry>Predefined choices.</entry>
        <entry>
         <para>
          Defines pool consumers, and indicates which services are expected to
          consume a given pool with which access. It has semantic meaning for
          the following predefined choices:
         </para>
         <orderedlist xml:id="ol_zp1_s5d_lw">
          <listitem>
           <para>
            nova
           </para>
          </listitem>
          <listitem>
           <para>
            glance-datastore
           </para>
          </listitem>
          <listitem>
           <para>
            cinder-volume
           </para>
          </listitem>
          <listitem>
           <para>
            cinder-back-up
           </para>
          </listitem>
         </orderedlist>
        </entry>
        <entry>
         <para>
          Do not pick the value from glance-datastore, nova, cinder-volume, or
          cinder-backup for user-defined pools. For user-defined pools, you can
          skip this parameter.
         </para>
        </entry>
       </row>
      </tbody>
     </tgroup>
    </informaltable>
   </step>
  </procedure>
  </section>
 <section>
  <title>Prerequisites</title>
  <para>
   To use Ceph as a volume backend, the nodes running these services should
   have the Ceph client installed on them. Use the
   <literal>ceph-client-prepare.yml</literal> playbook to deploy the Ceph
   client on these nodes.
  </para>
<screen>cd ~/scratch/ansible/next/hos/ansible
ansible-playbook -i hosts/verb_hosts ceph-client-prepare.yml</screen>
  <para>
   This playbook also creates Ceph users and Ceph pools on the resource nodes.
  </para>
  <note>
   <para>
    The following steps install packages and configure the existing client
    nodes (such as the Cinder, Glance, and Nova Compute nodes) required to use
    the Ceph cluster. For any new client nodes added later on that need to be
    configured to use the Ceph cluster, just execute the preceding playbook
    with the addition of the <literal>--limit &lt;new-client-node&gt;</literal>
    switch.
   </para>
  </note>
 </section>
 <section>
  <title>Use Ceph Storage Pools as Glance Data Stores</title>
  <para>
   To enable Ceph as a Glance data store, perform the following steps:
  </para>
  <procedure>
   <step>
    <para>
     Log in to the lifecycle manager.
    </para>
   </step>
   <step>
    <para>
     Make the following changes in the
     <literal>~/helion/my_cloud/config/glance/glance-api.conf.j2</literal>
     file:
    </para>
     <substeps>
	<step>
      <para>
       Copy the following content and paste it into the
       <literal>glance-api.conf.j2</literal> file under
       <emphasis role="bold">[glance_store]</emphasis>:
      </para>
<screen><emphasis role="bold">[glance_store]</emphasis>
default_store = rbd
stores = rbd
rbd_store_pool = images
rbd_store_user = glance
rbd_store_ceph_conf = /etc/ceph/ceph.conf
rbd_store_chunk_size = 8</screen>
     </step>
     <step>
      <para>
       In the same file, comment out the following references to Swift:
      </para>
<screen>stores = {{ glance_stores }}
default_store = {{ glance_default_store }}</screen>
	</step>
     </substeps>
    </step>
   <step>
    <para>
     <!-- FIXME: use important tag? - sknorr, 2018-01-02 -->
     <emphasis role="bold">IMPORTANT:</emphasis> If you have preexisting images
     in your Glance repo and want to use Ceph exclusively as a backend, use the
     Glance CLI or other tool to back up your images before configuring Ceph as
     your Glance backend:
    </para>
     <substeps>
	<step>
      <para>
       Snapshot or delete all Nova instances using those images.
      </para>
     </step>
     <step>
      <para>
       Download the images locally that you want to save.
      </para>
     </step>
     <step>
      <para>
       Delete all the images from Glance.
      </para>
	</step>
     </substeps>
    <para>
     After you finish the Ceph configuration you will need to add those images
     again.
    </para>
   </step>
   <step>
    <para>
     Commit your configuration to the local git repo
     (<xref linkend="using_git"/>), as follows:
    </para>
<screen>cd ~/helion/hos/ansible
git add -A
git commit -m "My config or other commit message"</screen>
   </step>
   <step>
    <para>
     Run the configuration processor:
    </para>
<screen>cd ~/helion/hos/ansible
ansible-playbook -i hosts/localhost config-processor-run.yml</screen>
   </step>
   <step>
    <para>
     Update your deployment directory:
    </para>
<screen>cd ~/helion/hos/ansible
ansible-playbook -i hosts/localhost ready-deployment.yml</screen>
   </step>
   <step>
    <para>
     Run the Glance reconfigure playbook to configure Ceph as a Glance backend:
    </para>
<screen>cd ~/scratch/ansible/next/hos/ansible
ansible-playbook -i hosts/verb_hosts glance-reconfigure.yml</screen>
   </step>
  </procedure>
  <para>
   After you execute the preceding command successfully, the glance is
   configured to use Ceph as a data store. You can use glance operations to
   upload images, which will be stored in the Ceph cluster.
  </para>
 </section>
 <section>
  <title>Use Ceph Storage Pools as Cinder Volume Backends</title>
  <para>
   To enable Ceph as a Cinder volume backend, follow these steps:
  </para>
  <procedure>
   <step>
    <para>
     Log in to the lifecycle manager.
    </para>
   </step>
   <step>
    <para>
     Make the following changes to the
     <literal>~/helion/my_cloud/config/cinder/cinder.conf.j2</literal> file:
    </para>
     <substeps>
		<step>
      <para>
       Add your Ceph backend to the <literal>enabled_backends</literal>
       section:
      </para>
<screen># Configure the enabled backends
enabled_backends=ceph1</screen>
      <important>
       <para>
        If you are using multiple backend types, you can use a comma-delimited
        list here. For example, if you are going to use both VSA and Ceph
        backends, specify <literal>enabled_backends=vsa-1,ceph1</literal>.
       </para>
      </important>
     </step>
     <step>
      <para>
       [optional] If you want your volumes to use a default volume type, enter
       the name of the volume type in the <literal>[DEFAULT]</literal> section
       with the following syntax. <emphasis role="bold">Make note of this value
       because you will need it when you create your volume type
       later.</emphasis>
      </para>
      <important>
       <para>
        If you do not specify a default type, then your volumes will default to
        a nonredundant RAID configuration. We recommend that you create a
        volume type that meets your environments needs and specify it here.
       </para>
      </important>
<screen>[DEFAULT]
# Set the default volume type
default_volume_type = &lt;your new volume type&gt;</screen>
     </step>
     <step>
      <para>
       Uncomment the <literal>ceph</literal> section and supply the values to
       match your cluster information. If you have more than one cluster, you
       will need to add another similar section with respective values. The
       following example adds only one cluster.
      </para>
<screen>[ceph1]
rbd_secret_uuid = &lt;secret-uuid&gt;
rbd_user = &lt;ceph-cinder-user&gt;
rbd_pool = &lt;ceph-cinder-volume-pool&gt;
rbd_ceph_conf = &lt;ceph-config-file&gt;
rbd_cluster_name = &lt;cluster_name&gt;
volume_driver = cinder.volume.drivers.rbd.RBDDriver
volume_backend_name = &lt;ceph-backend-name&gt;</screen>
      <para>
       This example has the following values:
      </para>
      <informaltable xml:id="ceph_volume" colsep="1" rowsep="1">
       <tgroup cols="2">
        <colspec colname="c1" colnum="1"/>
        <colspec colname="c2" colnum="2"/>
        <thead>
         <row>
          <entry>Value</entry>
          <entry>Description</entry>
         </row>
        </thead>
        <tbody>
         <row>
          <entry>rbd_secret_uuid</entry>
          <entry>
           <para>
            The secret_id value from the
            <literal>~/helion/my_cloud/config/ceph/user_model.yml</literal>
            file, as highlighted here:
           </para>
<screen>- user:
    name: <emphasis role="bold">cinder</emphasis>
    type: openstack
    secret_id: <emphasis role="bold">&lt;secret ID will be here&gt;</emphasis>
pools:
    - name: volumes</screen>
           <important>
            <para>
             You should generate and use your own secret ID. You can use any
             UUID generation tool.
            </para>
           </important>
          </entry>
         </row>
         <row>
          <entry>rbd_user</entry>
          <entry>
           <para>
            The username value from the
            <literal>~/helion/my_cloud/config/ceph/user_model.yml</literal>
            file, as highlighted here:
           </para>
<screen>- user:
    name: <emphasis role="bold">cinder</emphasis>
    type: openstack
    secret_id: &lt;secret ID will be here&gt;
pools:
    - name: volumes</screen>
          </entry>
         </row>
         <row>
          <entry>rbd_pool</entry>
          <entry>
           <para>
            The pool name value from the
            <literal>~/helion/my_cloud/config/ceph/user_model.yml</literal>
            file, as highlighted here:
           </para>
<screen>- user:
    name: cinder
    type: openstack
    secret_id: 457eb676-33da-42ec-9a8c-9293d545c337
pools:
    - name: <emphasis role="bold">volumes</emphasis></screen>
          </entry>
         </row>
         <row>
          <entry>rbd_ceph_conf</entry>
          <entry>
           <para>
            The Ceph configuration file location, usually
            <literal>/etc/ceph/ceph.conf</literal>.
           </para>
          </entry>
         </row>
         <row>
          <entry> rbd_cluster_name</entry>
          <entry>Name of the Ceph cluster.</entry>
         </row>
         <row>
          <entry>volume_driver</entry>
          <entry>
           <para>
            The Cinder volume driver. Leave this as the default value specified
            for Ceph.
           </para>
          </entry>
         </row>
         <row>
          <entry>volume_backend_name</entry>
          <entry>
           <para>
            The name given to the Ceph backend.
           </para>
          </entry>
         </row>
        </tbody>
       </tgroup>
      </informaltable>
		</step>
	 </substeps>
    </step>
   <step>
    <para>
     To enable attaching Ceph volumes to Nova provisioned instances, make the
     following changes to the
     <literal>~/helion/my_cloud/config/nova/kvm-hypervisor.conf.j2</literal>
     file:
    </para>
     <substeps>
		<step>
      <para>
       Uncomment the Ceph backend lines and edit them as follows:
      </para>
<screen>[libvirt]
rbd_user = &lt;ceph-user&gt;
rbd_secret_uuid = &lt;secret-uuid&gt;</screen>
      <para>
       The values are as follows:
      </para>
      <informaltable xml:id="nova_volume" colsep="1" rowsep="1">
       <tgroup cols="2">
        <colspec colname="c1" colnum="1"/>
        <colspec colname="c2" colnum="2"/>
        <thead>
         <row>
          <entry>Value</entry>
          <entry>Description</entry>
         </row>
        </thead>
        <tbody>
         <row>
          <entry>rbd_user</entry>
          <entry>
           <para>
            The username value from the
            <literal>~/helion/my_cloud/config/ceph/user_model.yml</literal>
            file, as highlighted here:
           </para>
<screen>- user:
  name: <emphasis role="bold">cinder</emphasis>
  type: openstack
  secret_id: 457eb676-33da-42ec-9a8c-9293d545c337</screen>
          </entry>
         </row>
         <row>
          <entry>rbd_secret_uuid</entry>
          <entry>
           <para>
            The secret_id value from the
            <literal>~/helion/my_cloud/config/ceph/user_model.yml</literal> fil
            e, as highlighted here:
           </para>
<screen>- user:
  name: <emphasis role="bold">cinder</emphasis>
  type: openstack
  secret_id: <emphasis role="bold">457eb676-33da-42ec-9a8c-9293d545c337</emphasis></screen>
          </entry>
         </row>
        </tbody>
       </tgroup>
      </informaltable>
      <note>
       <para>
        To attach a volume provisioned out of a newly added Ceph backend to an
        existing OpenStack instance, the instance must be rebooted after the
        new backend has been added.
       </para>
      </note>
	</step>
     </substeps>
    <important>
     <para>
      Do not use <literal>backend_host</literal> variable in
      <literal>cinder.conf</literal> file. If <literal>backend_host</literal>
      is set, it will override the [DEFAULT]/host value which &kw-hos-phrase;
      is dependent on.
     </para>
    </important>
   </step>
   <step>
    <para>
     Commit your configuration to the local git repo (<xref linkend="using_git"/>), as follows:
    </para>
<screen>cd ~/helion/hos/ansible
git add -A
git commit -m "My config or other commit message"</screen>
   </step>
   <step>
    <para>
     Run the configuration processor:
    </para>
<screen>cd ~/helion/hos/ansible
ansible-playbook -i hosts/localhost config-processor-run.yml</screen>
   </step>
   <step>
    <para>
     Update your deployment directory:
    </para>
<screen>cd ~/helion/hos/ansible
ansible-playbook -i hosts/localhost ready-deployment.yml</screen>
   </step>
   <step>
    <para>
     Run the Cinder reconfigure playbook:
    </para>
<screen>cd ~/scratch/ansible/next/hos/ansible
ansible-playbook -i hosts/verb_hosts cinder-reconfigure.yml</screen>
   </step>
   <step>
    <para>
     If Nova has been configured to attach Ceph backend volumes, run the Nova
     reconfigure playbook:
    </para>
<screen>cd ~/scratch/ansible/next/hos/ansible
ansible-playbook -i hosts/verb_hosts nova-reconfigure.yml</screen>
   </step>
  </procedure>
  <para>
   After you execute the preceding command successfully, the cinder-volume is
   configured to use Ceph as a data store. You can use volume lifecycle
   operations as described at <citetitle>FIXME: broken external
   xref</citetitle>
  </para>
 </section>
 <section>
  <title>Use Ceph Storage Pools as Cinder Backup Devices</title>
  <para>
   To enable Cinder backup devices for Ceph, make the following changes to the
   <literal>~/helion/my_cloud/config/cinder/cinder.conf.j2</literal> file:
  </para>
  <procedure>
   <step>
    <para>
     Uncomment the <literal>ceph backup</literal> section and supply the
     values:
    </para>
<screen>[DEFAULT]
backup_driver = cinder.backup.drivers.ceph
backup_ceph_conf = &lt;ceph-config-file&gt;
backup_ceph_user = &lt;ceph-backup-user&gt;
backup_ceph_pool = &lt;ceph-backup-pool&gt;</screen>
    <para>
     The values are as follows:
    </para>
    <informaltable xml:id="ceph_backup" colsep="1" rowsep="1">
     <tgroup cols="2">
      <colspec colname="c1" colnum="1"/>
      <colspec colname="c2" colnum="2"/>
      <thead>
       <row>
        <entry>Value</entry>
        <entry>Description</entry>
       </row>
      </thead>
      <tbody>
       <row>
        <entry>backup_driver</entry>
        <entry>
         <para>
          The Cinder volume driver. Leave this as the default value specified
          for Ceph.
         </para>
        </entry>
       </row>
       <row>
        <entry>backup_ceph_conf</entry>
        <entry>
         <para>
          The Ceph configuration file location, usually:
          <literal>/etc/ceph/ceph.conf</literal>.
         </para>
        </entry>
       </row>
       <row>
        <entry>backup_ceph_user</entry>
        <entry>
         <para>
          The username value from the
          <literal>~/helion/my_cloud/config/ceph/user_model.yml</literal> file,
          as highlighted here:
         </para>
<screen>- user:
    name: <emphasis role="bold">cinder-backup</emphasis>
    type: openstack
pools:
    - name: backups</screen>
        </entry>
       </row>
       <row>
        <entry>backup_ceph_pool</entry>
        <entry>
         <para>
          The pool name value from the
          <literal>~/helion/my_cloud/config/ceph/user_model.yml</literal> file,
          as highlighted here:
         </para>
<screen>pools:
    - name: <emphasis role="bold">backups</emphasis></screen>
        </entry>
       </row>
      </tbody>
     </tgroup>
    </informaltable>
   </step>
   <step>
    <para>
     Commit your configuration to the local git repo
     (<xref linkend="using_git"/>), as follows:
    </para>
<screen>cd ~/helion/hos/ansible
git add -A
git commit -m "My config or other commit message"</screen>
   </step>
   <step>
    <para>
     Run the configuration processor:
    </para>
<screen>cd ~/helion/hos/ansible
ansible-playbook -i hosts/localhost config-processor-run.yml</screen>
   </step>
   <step>
    <para>
     Update your deployment directory:
    </para>
<screen>cd ~/helion/hos/ansible
ansible-playbook -i hosts/localhost ready-deployment.yml</screen>
   </step>
   <step>
    <para>
     Run the Cinder reconfigure playbook:
    </para>
<screen>cd ~/scratch/ansible/next/hos/ansible
ansible-playbook -i hosts/verb_hosts cinder-reconfigure.yml </screen>
   </step>
  </procedure>
  <para>
   After you execute the preceding command successfully, the cinder-backup
   service is configured to use Ceph as a data store. You can use volume
   lifecycle operations as described in
   <!-- FIXME: <xref href="../../operations/blockstorage/creating_voltype.xml#creating_voltype/associate_volumetype"/> -->
   .
  </para>
 </section>
 <section>
  <title>Use RADOS Gateway to Access Objects Using S3/Swift API</title>
  <para>
   RADOS Gateway provides object storage functionality through S3 and Swift
   API. It has a built-in authentication mechanism and it can use Keystone as
   an authentication backend, unlike the OpenStack Services, where Keystone is
   the only authentication backend. Users primarily see the following
   combinations of object accessibility:
  </para>
  <orderedlist>
   <listitem>
    <para>
     Keystone users using:
    </para>
    <orderedlist xml:id="ol_uc3_dfd_jw">
     <listitem>
      <para>
       Swift API
      </para>
     </listitem>
     <listitem>
      <para>
       S3 API
      </para>
     </listitem>
    </orderedlist>
   </listitem>
   <listitem>
    <para>
     RADOS Gateway users using:
    </para>
    <orderedlist xml:id="ol_pf1_cfd_jw">
     <listitem>
      <para>
       Swift API
      </para>
     </listitem>
     <listitem>
      <para>
       S3 API
      </para>
     </listitem>
    </orderedlist>
   </listitem>
  </orderedlist>
  <para>
   Here's a pictorial view of preceding perspective:
  </para>
  <mediaobject xml:id="image_lf4_tmt_jw">
   <imageobject role="fo">
    <imagedata fileref="media-ceph-ceph-rgw.png" width="75%" format="PNG"/>
   </imageobject>
   <imageobject role="html">
    <imagedata fileref="media-ceph-ceph-rgw.png"/>
   </imageobject>
  </mediaobject>
  <para>
   <!-- FIXME: the 1.a, 2.b, ... mentions -->
   The first view uses Keystone as the authentication backend while the second
   one uses RADOS Gateway (internal authentication backend). Default deployment
   of RADOS Gateway in &kw-hos-tm; &kw-hos-version; enables 1.a, 2.a, and 2.b
   only.
  </para>
  <para>
   Enabling choice 1.b for Keystone users accessing S3 API requires Keystone to
   be the default authentication backend. In this case all user authentication
   goes first to Keystone, with failover to RADOS Gateway even for non-Keystone
   users (such as in case 2.b), but this situation causes a serious performance
   drop for RADOS Gateway users.
  </para>
  <para>
   Lab tests performed on a 10 TB Ceph cluster have shown that enabling S3 API
   access for Keystone users degrades the performance of S3 API access for
   RADOS Gateway users roughly by 50%. We ran the rest-bench tool for 10
   seconds with a concurrency of 16 and an object size of 4096 (4K) bytes on
   Ceph deployed using the <literal>entry-scale-kvm-ceph</literal> input model.
   Our lab finding was that the system performed an average of 1350 (object
   size 4K) write operations when Keystone was enabled as the default
   authentication backend, in contrast to 2500 (object size 4K) operations in
   the default configuration, which was a degradation of roughly 49%. Note that
   these numbers are for illustration purposes to provide an insight on the
   degree of degradation. However, storing all credentials in Keystone provides
   the advantage of reducing the maintenance burden. It is not required to
   create or manage credentials in a database for S3 authentication. You can
   use standard administrative tools such as Horizon instead. Aspect 1.b does
   not apply if you do not intend to enable S3 API access for Keystone users.
  </para>
  <section>
   <title>Steps to Access S3 API for RADOS Gateway Users</title>
   <para>
    Perform these steps:
   </para>
   <procedure>
    <step>
     <para>
      Create a user by executing the following command:
     </para>
<screen>sudo radosgw-admin user create --uid="{username}" --display-name="{Display Name}" --email="{email}"</screen>
     <para>
      Example:
     </para>
<screen>sudo radosgw-admin user create --uid=rgwuser --display-name="RadosGatewayUser" --email=rgwuser@test.com</screen>
    </step>
    <step>
     <para>
      Perform the object operation. You can use S3 clients. Example:
      <literal>python-boto</literal>.
     </para>
    </step>
   </procedure>
   </section>
  <section>
   <title>Steps to Access Swift API for RADOS Gateway Users</title>
   <para>
    Perform these steps:
   </para>
   <procedure>
    <step>
     <para>
      Create a user by executing the following command:
     </para>
<screen>sudo radosgw-admin user create --uid="{username}" --display-name="{Display Name}" --email="{email}"</screen>
     <para>
      Example:
     </para>
<screen>sudo radosgw-admin user create --uid=rgwuser --display-name="RadosGatewayUser" --email=rgwuser@test.com</screen>
    </step>
    <step>
     <para>
      Create a subuser by executing the following command:
     </para>
<screen>sudo radosgw-admin subuser create --uid={username} --subuser={username}:{subusername} --access=full --key-type=swift --gen-secret</screen>
     <para>
      Example:
     </para>
<screen>sudo radosgw-admin subuser create --uid="rgwuser" --subuser="rgwuser:rgwswiftuser" --access=full --key-type=swift --gen-secret</screen>
    </step>
    <step>
     <para>
      Perform the object operation using Swift CLI or any compatible client.
     </para>
    </step>
   </procedure>
   </section>
  <section>
   <title>Steps to Access Swift API for Keystone Users</title>
   <para>
    When RADOS Gateway is deployed per the default
    <literal>entry-scale-kvm-ceph</literal> model, it exists alongside
    OpenStack Swift (this is because the Keystone service type for RADOS
    Gateway defaults to <literal>ceph-object-store</literal>). In this
    (coexisting) mode, OpenStack Horizon communicates only with OpenStack Swift
    for all the object store operations. However, the OpenStack Swift command
    line interface can be tuned to communicate with both OpenStack Swift and
    RADOS Gateway as follows:
   </para>
   <itemizedlist xml:id="ul_p4x_4zd_jw">
    <listitem>
     <para>
      To interact with Swift (default):
     </para>
<screen><literal>export OS_SERVICE_TYPE=object-store</literal></screen>
    </listitem>
    <listitem>
     <para>
      To interact with RADOS Gateway:
     </para>
<screen> <literal>export OS_SERVICE_TYPE=ceph-object-store</literal></screen>
    </listitem>
   </itemizedlist>
   <para>
    You can use Swift CLI to perform object operations on Ceph.
   </para>
   <para>
    <emphasis role="bold">Example</emphasis>:
   </para>
   <para>
    Log in to the monitor node to list the objects in Ceph using the
    credentials available in the <literal>~/service.osrc</literal> file.
   </para>
<screen>source ~/service.osrc</screen>
   <para>
    Execute the following command to list objects in the Ceph object store:
   </para>
<screen>export OS_SERVICE_TYPE=ceph-object-store
swift list</screen>
   <para>
    Execute the following command to list objects in the Swift object store:
   </para>
<screen>export OS_SERVICE_TYPE=object-store
swift list</screen>
  </section>
  <section>
   <title>Steps to Access S3 API for Keystone Users</title>
   <para>
    By default, a Keystone user can access the RADOS Gateway functionality
    using the Swift API. To configure a Keystone user for S3 API to access the
    RADOS Gateway, perform the following steps:
   </para>
   <procedure>
    <step>
     <para>
      Login to lifecycle manager.
     </para>
    </step>
    <step>
     <para>
      Edit the<literal> ~/helion/my_cloud/config/ceph/settings.yml</literal>
      file and set the <literal>rgw_s3_auth_use_keystone</literal> value to
      <emphasis role="bold">true</emphasis>.
     </para>
<screen>rgw_s3_auth_use_keystone: true</screen>
    </step>
    <step>
     <para>
      Commit your configuration to local repo.
     </para>
<screen>cd ~/helion/hos/ansible
git add -A
git commit -m &lt;"commit message"&gt;</screen>
    </step>
    <step>
     <para>
      Run ready-deployment playbook.
     </para>
<screen>cd ~/helion/hos/ansible
ansible-playbook -i hosts/localhost ready-deployment.yml</screen>
    </step>
    <step>
     <para>
      Run the ceph-reconfigure playbook.
     </para>
<screen>cd ~/scratch/ansible/next/hos/ansible
ansible-playbook -i hosts/verb_hosts ceph-reconfigure.yml</screen>
     <para>
      <emphasis role="bold">Create the EC2 credentials</emphasis>
     </para>
      <substeps>
	  <step>
       <para>
        Login to a controller node and source the admin user credentials.
       </para>
<screen>source ~/service.osrc</screen>
      </step>
      <step>
       <para>
        Execute the following commands to generate the EC2 credentials for the
        OpenStack user.
       </para>
<screen>openstack ec2 credentials create --project &lt;project-name&gt; --user &lt;user-name&gt;</screen>
       <para>
        For example, to generate the EC2 credentials for user demo for the
        project demo.
       </para>
<screen>openstack ec2 credentials create --project demo --user demo</screen>
	</step>
	</substeps>
	</step>
   </procedure>
   <important>
    <para>
     <!-- FIXME: Should this be removed? - sknorr, 2017-12-29 -->
     Please contact the Professional Services team for details on how to
     perform the preceding steps.
    </para>
   </important>
  </section>
 </section>
</section>
