<?xml version="1.0"?>
<!DOCTYPE chapter [
 <!ENTITY % entities SYSTEM "entity-decl.ent"> %entities;
]>
<chapter xml:id="install-rhel-compute-node"
 xmlns="http://docbook.org/ns/docbook" version="5.1"
 xmlns:xi="http://www.w3.org/2001/XInclude"
 xmlns:xlink="http://www.w3.org/1999/xlink">
 <title>Using &rhla; as a &compnode;</title>
 <para>
  This section outlines how to install a &rhla; &compnode; as a member of a
  new or existing cloud created with &productname;.
 </para>
 <section xml:id="sec-install-rhel-compute-node-provision-deployer">
  <title>Setting Up a Deployer for &rhla;</title>
  <para>
   Ensure that your environment includes a Subscription Management Tool (SMT)
   server. Run the following steps on that server to configure an RPM
   repository for use by &rhla; clients.
  </para>
  <procedure>
   <step>
    <para>
     Enable SMT mirroring of the external CentOS 7.5 repository to distribute
     RHEL-compatible packages to nodes in the cloud.
    </para>
<screen>&prompt.sudo;smt-setup-custom-repos --name CentOS --description "CentOS 7.5" --productid 100682 \
  --exturl https://download.opensuse.org/repositories/systemsmanagement:/Ardana:/9:/CentOS:/7.5/CentOS_7.5/
&prompt.sudo;smt-repos -e CentOS
&prompt.sudo;smt-sync
&prompt.sudo;smt-mirror</screen>
   </step>
   <step>
    <para>
     CentOS RPM packages will now be stored on the SMT server in
     <filename>/srv/www/htdocs/repo/RPMMD/CentOS</filename>. If your deployer
     node also operates as your SMT server, publish this content to other nodes
     with the following command:
    </para>
<screen>&prompt.sudo;ln -s /srv/www/htdocs/repo/RPMMD/CentOS /opt/ardana_packager/ardana/rhel7/yum/centos</screen>
    <para>
     Or, if your SMT server is hosted separately from your deployer, follow the
     same steps for this repository as you are doing to mirror other repos, ensuring
     that it results in hosting the repo on the deployer at
     <filename>/opt/ardana_packager/ardana/rhel7/yum/centos</filename>.
    </para>
   </step>
   <step>
    <para>
     Add this new repo as a yum package source to cloud nodes by populating
     <filename>/etc/yum.repos.d/ardana-centos.repo</filename> in each
     &rhla; system with the following contents:
    </para>
<screen>[ardana-centos]
name=Ardana CentOS Repository
baseurl=http://<replaceable>DEPLOYER-IP</replaceable>:79/ardana/rhel7/yum/centos
enabled=1
gpgcheck=0</screen>
   </step>
   <step>
    <para>
     Add the new repository to the deployer node as well (and accept its
     certificate), but with lower priority than other repositories. This
     ensures that packages present in both repos are preferably installed
     from the &sles; repositories.
    </para>
<screen>&prompt.sudo;zypper ar -p 100 file:/opt/ardana_packager/ardana/rhel7/yum/centos centos
&prompt.sudo;zypper ref</screen>
   </step>
  </procedure>
  <para>
   &ostack; components are installed on &rhel; &compnode;s
   via virtualenv (venv). To facilitate this, certain packages
   must be installed from the <systemitem>ardana-centos</systemitem>
   repository on the deployer node:
  </para>
<screen>for i in nova neutron monasca_agent
  do sudo zypper in -y venv-openstack-$i-rhel-x86_64
done</screen>
  <para>
   Once these packages are installed, they will populate a new package
   directory which must be prepared to serve as a &rhla; yum repository:
  </para>
<screen>&prompt.sudo;create_index --dir=/opt/ardana_packager/ardana-9/rhel_venv/x86_64</screen>
 </section>
 <section xml:id="sec-install-rhel-compute-node-yum-ardana">
  <title>Package Dependencies for &rhla; &compnode;s</title>
  <para>
   The original release of &rhla; 7.5 requires bug fix advisory
   <literal>RHBA-2018:2198</literal> to be installed for proper DVR
   functionality. (If this is not done, attempts to create Floating IP
   resources for VMs on &rhla; &compnode;s will fail, as documented in the
   <link xlink:href="http://access.redhat.com/solutions/3449511">Red Hat
   Knowledge Base</link>.) This update can be downloaded via an active &rhla;
   license, and should be applied after the node's operating system is
   installed. If this results in a change to the kernel version (to the
   target of 3.10.0-862), a reboot will be required.
  </para>
  <para>
   To use the &ostack-current; version of
   <literal>nova-compute</literal> and <literal>qemu-kvm</literal>,
   virtualization packages with versions of at least 2.10.0 must be installed
   on &rhla; nodes.
  </para>
  <para>
   The RPM files needed for these packages must be
   provided before executing <filename>site.yml</filename>. It is expected
   that a &rh; Virtualization product subscription will be used to provide
   these RPM packages, ensuring that the latest version is installed and
   that continuous updates will be available.
  </para>
  <itemizedlist>
   <listitem>
    <para>
     <filename>qemu-img-rhev</filename>
    </para>
   </listitem>
   <listitem>
    <para>
     <filename>qemu-kvm-common-rhev</filename>
    </para>
   </listitem>
   <listitem>
    <para>
     <filename>qemu-kvm-rhev</filename>
    </para>
   </listitem>
   <listitem>
    <para>
     <filename>qemu-kvm-tools-rhev</filename>
    </para>
   </listitem>
  </itemizedlist>
 </section>
 <section xml:id="sec-install-rhel-compute-node-deploying">
  <title>Deploying &rhla; &compnode;s</title>
  <para>
   SELinux policy updates are needed for
   <literal>nova-compute</literal> to work properly. SELinux policy updates can be
   enabled by changing a flag in default configuration. This needs to be
   done on the &clm; <emphasis>before</emphasis> <literal>nova-compute</literal>
   is installed on &rhla; nodes.
  </para>
  <procedure>
   <step>
    <para>
     On the &clm; node, edit the following file:
    </para>
<screen>cd ~/openstack/ardana/ansible
vi roles/NOV-CMP-KVM/defaults/main.yml</screen>
   </step>
   <step>
    <para>
     Set the SELinux flag to <literal>true</literal>:
    </para>
<screen>nova_rhel_compute_apply_selinux_policy_updates: true</screen>
   </step>
   <step>
    <para>
     Save and close the file.
    </para>
   </step>
   <step>
    <para>
     Commit the change to Git:
    </para>
<screen>git commit -a --allow-empty -m "Enable SELinux policy updates for compute nodes"</screen>
   </step>
   <step>
    <para>
     Publish the changes so that they are included in playbook runs:
    </para>
<screen>ansible-playbook -i hosts/localhost ready-deployment.yml</screen>
   </step>
  </procedure>
  <para>
   If your existing cloud is fully installed and the new &rhla; compute node is
   being added to it, update the <filename>/etc/hosts</filename> table on
   existing nodes to include the new compute node.
  </para>
<screen>cd ~/scratch/ansible/next/ardana/ansible
ansible-playbook -i hosts/verb_hosts site.yml --tag "generate_hosts_file"</screen>
  <para>
   Continue to deploy &ostack; services on &rhla; nodes by providing the
   names of each &rhla; node (as defined in <filename>hosts/verb_hosts</filename>)
   in the following command:
  </para>
<screen>cd ~/scratch/ansible/next/ardana/ansible
ansible-playbook -i hosts/verb_hosts site.yml -l "<replaceable>NODE1_NAME</replaceable>,<replaceable>NODE2_NAME</replaceable>"</screen>
  <para>
   If you have used an encryption password when running the configuration
   processor, include the additional parameter <command>--ask-vault-pass</command>
   to the end of the above command.
  </para>
 </section>
</chapter>
