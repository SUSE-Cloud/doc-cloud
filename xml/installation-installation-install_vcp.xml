<?xml version="1.0"?>
<!DOCTYPE chapter [
 <!ENTITY % entities SYSTEM "entities.ent"> %entities;
 <!-- HPE used a little check mark image for this, but + is more ASCII-compatible. -->
 <!ENTITY check "<emphasis role='bold' xmlns='http://docbook.org/ns/docbook'>+</emphasis>">
]>
<chapter xml:id="install_vcp"
 xmlns="http://docbook.org/ns/docbook" version="5.1"
 xmlns:xi="http://www.w3.org/2001/XInclude"
 xmlns:xlink="http://www.w3.org/1999/xlink">
 <title>HLM-Hypervisor instructions</title>
 <section xml:id="d1e43">
  <title>Introduction</title>
  <para>
   You can read an overview of the design on the
   <link xlink:href="https://wiki.hpcloud.net/display/core/HLM+creation+of+VMs">Helion
   lifecycle manager creation of VMs</link> page. This page describes the
   steps needed to set up and use a HLM-Hypervisor (previously known as a
   VM-factory) and related functionality of &kw-hos-phrase;. This comprises
   model changes and running a couple of new playbooks to bring up the VMs.
  </para>
 </section>
 <section xml:id="d1e51">
  <title>Model changes</title>
  <section xml:id="d1e55">
   <title>passthrough-network-groups</title>
   <para>
    <!-- FIXME: The heck is PB3? a version number like "prebeta 3"? (appears
    multiple times in this document) - sknorr, 2017-12-29 -->
    With PB3 the specification of
    <literal>passthrough-network-groups</literal> is now
    supported within the Interfaces section of the input model.
   </para>
   <para>
    A <literal>passthrough-network</literal> group is a network group that can
    be accessed by a VM hosted by the hypervisor system regardless of whether
    the network group is required on the hypervisor system itself.
   </para>
   <variablelist>
    <varlistentry>
     <term>network-groups:</term>
     <listitem>
      <para>
       List of network groups that this server may require access to.
      </para>
      <para>
       Access to the network is only configured if the network-group is
       required by at least one service hosted on this server.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>forced-network-groups:</term>
     <listitem>
      <para>
       List of network-groups that this server requires access to.
      </para>
      <para>
       Access to the network is configured regardless of whether the
       network-group is required by any service hosted on this server.
      </para>
      <para>
       Network groups should not appear in both
       <literal>network-groups</literal> and
       <literal>forced-network-groups</literal>.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>passthrough-network-groups:</term>
     <listitem>
      <para>
       List of network-groups that need to be passed through this server to
       hosted VMs.
      </para>
      <para>
       No access is configured for this server if the network group is listed
       <literal>only</literal> in
       <literal>passthrough-network-groups</literal>.
      </para>
      <para>
       The <literal>passthrough-network-groups</literal> may include networks
       from either <literal>network-groups</literal> or
       <literal>forced-network-groups</literal> but may also include network
       groups not used directly by this server. If a
       <literal>passthrough-network-groups</literal> is listed in either of
       <literal>network-groups</literal> or
       <literal>forced-network-groups</literal>the access to the network
       group will be such that both the server and hosted VMs are network
       peers.
      </para>
     </listitem>
    </varlistentry>
   </variablelist>
   <section xml:id="d1e146">
    <title>Example:</title>
    <para>
     The following hypervisor interface-model example specifies :
    </para>
    <itemizedlist>
     <listitem>
      <para>
       <literal>bond0</literal>
      </para>
      <itemizedlist>
       <listitem>
        <para>
         <literal>PXE</literal> available to hypervisor and Hosted-VMs
        </para>
       </listitem>
       <listitem>
        <para>
         <literal>CLM</literal> available to hypervisor and Hosted-VMs
        </para>
       </listitem>
       <listitem>
        <para>
         <literal>CAN</literal> available to Hosted-VMs only
        </para>
       </listitem>
      </itemizedlist>
     </listitem>
     <listitem>
      <para>
       <literal>bond1</literal>
      </para>
      <itemizedlist>
       <listitem>
        <para>
         <literal>VxLAN-VLAN1-TUL</literal>,
         <literal>EXT</literal> and
         <literal>VLAN2-TUL</literal> available to Hosted-VMs only
        </para>
       </listitem>
      </itemizedlist>
     </listitem>
    </itemizedlist>
<screen>  interface-models:
    - name: HLM-HYPERVISOR-INTERFACES
      network-interfaces:
        - name: BOND0
          device:
            name: bond0
          bond-data:
            options:
              mode: active-backup
              miimon: 200
              primary: hed5
            provider: linux
            devices:
              - name: hed5
          network-groups:
            - PXE
            - CLM
          passthrough-network-groups:
            - PXE
            - CLM
            - CAN
        - name: BOND1
          device:
            name: bond1
          bond-data:
            options:
              mode: active-backup
              miimon: 200
              primary: hed1
            provider: linux
            devices:
              - name: hed1
          passthrough-network-groups:
            - VxLAN-VLAN1-TUL
            - EXT
            - VLAN2-TUL</screen>
   </section>
  </section>
  <section xml:id="d1e225">
   <title>hlm-hypervisor</title>
   <para>
    With PB3 we now support the specification
    of <literal>hlm-hypervisor</literal> Boolean within the
    Servers section of the input model. This setting is used to identify nodes
    that can host VMs (non-NOVA).
   </para>
   <para>
    This is related to the specification
    of <literal>hypervisor-id</literal> that is required to identify servers
    to be <literal>instantiated</literal> as hosted-VMs.
   </para>
   <itemizedlist>
    <listitem>
     <para>
      Any server referenced as a target of a <literal>hypervisor-id</literal>
      must have <literal>hlm-hypervisor</literal> set to True.
     </para>
    </listitem>
    <listitem>
     <para>
      A server with <literal>hlm-hypervisor</literal> of True setting may or
      may not have Helion lifecycle manager Hosted-VMs associated with it.
     </para>
    </listitem>
   </itemizedlist>
   <section xml:id="d1e283">
    <title>Example</title>
    <para>
     In the following example from <literal>servers.yml</literal>
    </para>
    <itemizedlist>
     <listitem>
      <para>
       a single server (<literal>hypervisor1)</literal> is declared as
       supporting Hosted-VMs using <literal>hlm-hypervisor: True</literal>
      </para>
     </listitem>
     <listitem>
      <para>
       three nodes
       (<literal>controller1</literal>, <literal>controller2</literal> and
       <literal>controller3</literal>)
       are declared as Hosted-VMs on<literal>hypervisor1</literal>
       (<literal>hypervisor-id: hypervisor1</literal>)
      </para>
     </listitem>
    </itemizedlist>
<screen>    - id: hypervisor1
      ip-addr: 10.225.107.74
      role: HLM-HYPERVISOR-ROLE
      server-group: RACK2
      hlm-hypervisor: True
      nic-mapping: HP-BL460-6PORT
      mac-addr: 9c:7e:96:22:9e:d8
      ilo-ip: 10.1.18.42
      ilo-password: whatever
      ilo-user: whatever

    - id: controller1
      ip-addr: 10.225.107.75
      role: CONTROLLER-ROLE
      hypervisor-id: hypervisor1
      nic-mapping: VIRTUAL-1-PORT

    - id: controller2
      ip-addr: 10.225.107.76
      role: CONTROLLER-ROLE
      hypervisor-id: hypervisor1
      nic-mapping: VIRTUAL-1-PORT

    - id: controller3
      ip-addr: 10.225.107.77
      role: CONTROLLER-ROLE
      hypervisor-id: hypervisor1
      nic-mapping: VIRTUAL-1-PORT

    - id: compute1
      ip-addr: 10.225.107.78
      role: COMPUTE-ROLE
      server-group: RACK1
      nic-mapping: HP-BL460-6PORT
      mac-addr: 6c:9e:96:22:7e:d8
      ilo-ip: 10.1.18.44
      ilo-password: whatever
      ilo-user: whatever</screen>
    <para>
     The basic steps are as follows, with examples below. In all of these
     examples, it is assumed that hed1 is the physical interface to which the
     VMs need to be connected.
    </para>
    <procedure>
     <step>
      <para>
       Create a new network interfaces group for the Hypervisor nodes,
       appropriate to the hardware in use. It is called
       HLM-HYPERVISOR-INTERFACES in this example. You may also want to create
       nic mappings for these nodes.
      </para>
<screen>@@ -94,3 +94,12 @@
           network-groups:
             - MANAGEMENT

+    - name: HLM-HYPERVISOR-INTERFACES
+      network-interfaces:
+        - name: hed1
+          device:
+              name: hed1
+          network-groups:
+            - EXTERNAL-VM
+            - GUEST
+            - MANAGEMENT</screen>
     </step>
     <step>
      <para>
       Create a new disk model for the Hypervisor nodes. Here's an example for
       a Hypervisor node with 4 disks. In this specific example, we've
       included storage to be set-aside in the &kw-hos; input model for
       gluster.
      </para>
      <para>
       You will need to tailor this disk model to the hardware that you are
       using. For example, if your Hypervisor nodes only have one disk, you
       would delete the line referencing <literal>/dev/sdb</literal> from the
       <literal>hlm-vg</literal> volume group definition, and remove the
       <literal>vg-images</literal> volume group altogether.
      </para>
<screen>
<?dbsuse-fo font-size="0.70em"?>
  ---
  product:
    version: 2
  disk-models:
  - name: HLM-HYPERVISOR-DISKS
    volume-groups:
      - name: hlm-vg
        physical-volumes:
         - /dev/sda_root
         - /dev/sdb
        logical-volumes:
          - name: root
            size: 35%
            fstype: ext4
            mount: /
          - name: log
            size: 50%
            mount: /var/log
            fstype: ext4
            mkfs-opts: -O large_file
          - name: crash
            size: 10%
            mount: /var/crash
            fstype: ext4
            mkfs-opts: -O large_file
      # optional VG dedicated to VM images to keep VM IOPS off the OS disk
      # The consumer stanza allows user defined location of where to store the qcows
      - name: hlm-images
        physical-volumes:
          - /dev/sdc
          - /dev/sdd
        logical-volumes:
          - name: hlm-images
            size: 95%
            mount: /var/lib/hlm-images
            fstype: ext4
            mkfs-opts: -O large_file
            consumer:
        name: hlm-hypervisor
        usage: hlm-hypervisor-images

    # Optionally define some storage in the input model to be used
    # by gluster
    device-groups:
      - name: gluster
        devices:
          - name: /dev/sde
          - name: /dev/sdf
        consumer:
          name: gluster
          suppress-warnings: True</screen>
      <para>
       Note the <literal>suppress-warnings: True</literal> flag for "gluster".
       This indicates to the configuration processor that is should not issue
       warnings when it cannot find a "gluster" service definition.
      </para>
     </step>
     <step>
      <para>
       Create a new role that uses this interface and disk model.
      </para>
<screen>@@ -34,3 +34,7 @@
     - name: DEPLOYER-ROLE
       interface-model: DEPLOYER-INTERFACES
       disk-model: DEPLOYER-DISKS
+
+    - name: HLM-HYPERVISOR-ROLE
+      interface-model: HLM-HYPERVISOR-INTERFACES
+      disk-model: HLM-HYPERVISOR-DISKS</screen>
     </step>
     <step>
      <para>
       Define a new type of node in the "clusters" section of control_plane.yml
       so that nodes with this role will get their own CP group.
      </para>
<screen>@@ -135,3 +135,10 @@
             - ntp-client
             - vsa

+        - name: hypervisor
+          resource-prefix: vmf
+          server-role: HLM-HYPERVISOR-ROLE
+          allocation-policy: strict
+          min-count: 0
+          service-components:
+            - lifecycle-manager-target
+            - ntp-server</screen>
      <para>
       If the lifecycle manager is also a hypervisor then you should add the
       component <literal>lifecycle-manager</literal> instead of
       <literal>lifecycle-manager-target</literal>. It is required that the
       hypervisor nodes have the <literal>ntp-server</literal> component.
      </para>
     </step>
     <step>
      <para>
       Assign the HLM-HYPERVISOR-ROLE to the appropriate nodes in servers.yml
      </para>
       <substeps>
		<step>
        <para>
         If the lifecycle manager node is also going to be a Hypervisor, then
         give it the HLM-HYPERVISOR-ROLE in servers.yml
        </para>
       </step>
       <step>
        <para>
         Having a baremetal node that is both a Hypervisor and a Controller is
         not supported.
        </para>
       </step>
       <step>
        <para>
         Also add definitions for all of the control plane VMs that you intend
         to create later. Note that you don't need iLO information or
         mac-addresses for VMs.
        </para>
       </step>
       <step>
        <para>
         You will need to modify the IP addresses etc. in this example to match
         your machine.
        </para>
<screen>@@ -80,7 +80,7 @@

+    - id: hypervisor1
+      ip-addr: 10.225.107.74
+      role: HLM-HYPERVISOR-ROLE
+      hlm-hypervisor: True
+      server-group: RACK2
+      nic-mapping: HP-BL460-6PORT
+      mac-addr: 9c:7e:96:22:9e:d8
+      ilo-ip: 10.1.18.42
+      ilo-password: whatever
+      ilo-user: whatever
+
+    - id: controller1
+      ip-addr: 10.225.107.75
+      role: CONTROLLER-ROLE
+      hypervisor-id: hypervisor1
+      nic-mapping: VIRTUAL-1-PORT
+
+    - id: controller2
+      ip-addr: 10.225.107.76
+      role: CONTROLLER-ROLE
+      hypervisor-id: hypervisor1
+      nic-mapping: VIRTUAL-1-PORT
+
+    - id: controller3
+      ip-addr: 10.225.107.77
+      role: CONTROLLER-ROLE
+      hypervisor-id: hypervisor1
+      nic-mapping: VIRTUAL-1-PORT
+
+    - id: compute1
+      ip-addr: 10.225.107.78
+      role: COMPUTE-ROLE
+      server-group: RACK1
+      nic-mapping: HP-BL460-6PORT
+      mac-addr: 6c:9e:96:22:7e:d8
+      ilo-ip: 10.1.18.44
+      ilo-password: whatever
+      ilo-user: whatever</screen>
       </step>
	</substeps>
	 </step>
     <step>
      <para>
       Add a definition of this new nic-mapping to
       <literal>nic_mappings.yml</literal>:
      </para>
<screen>    - name: VIRTUAL-1-PORT
      physical-ports:
      - bus-address: '0000:01:01.0'
        logical-name: hed1
        type: simple-port</screen>
      <para>
       See the section
       <link xlink:href="#nic_mapping_for_hlm_virtual_controllers">NIC Mapping
       for Helion lifecycle manager Virtual-Controllers</link> below for more
       information and examples.
      </para>
     </step>
     <step>
      <para>
       You will also need to make model changes for the vms. The vm role above
       is CONTROLLER-ROLE. You need to add information to the model that
       specifies:
      </para>
       <substeps>
	  <step>
        <para>
         The number of vcpus - this is done by adding a
         <literal>cpu-model</literal> to the vm role with a
         <literal>vm-size</literal> stanza
        </para>
       </step>
       <step>
        <para>
         The amount of RAM to be used - this is done by adding a
         <literal>memory-model</literal> to the vm role with a
         <literal>vm-size</literal> stanza
        </para>
       </step>
       <step>
        <para>
         The sizes of the virtual disks - this is done by adding a
         <literal>vm-size</literal> stanza to the disk-model used with the
         vm-role.
        </para>
       </step>
	 </substeps>
	 <para>
       The CONTROLLER-ROLE definition will look like:
      </para>
<screen>    - name: CONTROLLER-ROLE
      interface-model: CONTROLLER-INTERFACES
      disk-model: CONTROLLER-DISKS
      cpu-model: CONTROLLER-CPU
      memory-model: CONTROLLER-MEMORY</screen>
      <para>
       The cpu-model will look like the following - place it in a yaml file and
       set <literal>vcpus</literal> to the value that you want to use:
      </para>
<screen>---
  product:
    version: 2

  cpu-models:
   - name: CONTROLLER-CPU
     vm-size:
        vcpus: 4</screen>
      <para>
       The memory-model will look like the following - place it in a yaml file
       and set <literal>ram</literal> to the value that you want to use (valid
       values are in "KMG"):
      </para>
<screen>---
  product:
    version: 2
  memory-models:
   - name: CONTROLLER-MEMORY
     vm-size:
       ram: 16G</screen>
      <para>
       The <literal>disk-model</literal> will have a stanza like the following
       added to it:
      </para>
<screen>  disk-models:
  - name: CONTROLLER-DISKS
    vm-size:
      disks:
      - name: /dev/vda_root
        size: 1T
      - name: /dev/vdb
        size: 1T
      - name: /dev/vdc
        size: 1T
      - name: /dev/vdd
        size: 1T</screen>
      <para>
       <literal>disks</literal> is a list of all the physical-volumes in
       volume-groups and devices in device-groups used in the disk-model, one
       entry for each with the name in the <literal>name:</literal> field and
       the size in the <literal>size:</literal> field. Valid values for size
       are <literal>KMGT</literal>
      </para>
     </step>
     <step>
      <para>
       Note hat the network that is associated with ansible traffic needs be on
       the network interface in the vm which is on the "lowest" pci address. In
       the nic-mapping description below this would be hed1.
      </para>
     </step>
     <step>
      <para>
       You do need to have a <literal>vda_root</literal> described in your
       input model,
       <!-- FIXME: dangling half sentence, already broken in DITA original. -->
      </para>
     </step>
    </procedure>
    </section>
  </section>
 </section>
 <section xml:id="d1e603">
  <title>Bootstrap Instructions</title>
  <para>
   Start with one bare-metal node and install it directly from the
   &kw-hos-phrase; iso for example through virtual media on its iLO. Follow the
   usual install instructions for example
   <!-- FIXME: <xref keyref="install_entryscale_kvm"/> --> but make sure you
   use a model with the changes described above. Follow all the usual steps,
   including cobbler-deploy, bm-reimage and config-processor-run. Stop after
   running <literal>ready-deployment.yml</literal> though, and move to the
   instructions in the next section, to bring up the virtual control plane
   nodes.
  </para>
 </section>
 <section xml:id="d1e637">
  <title>Customizing for VCP</title>
  <para>
   The <literal>ready-deployment.yml</literal> playbook will have
   created <literal>~/scratch/ansible/next/hos/ansible</literal> so change to
   that directory and perform the rest of your work from that directory.
  </para>
<screen>cd ~/scratch/ansible/next/hos/ansible</screen>
  <para>
   There are a number of installation choices at this point:
  </para>
  <orderedlist>
   <listitem>
    <para>
     You are installing a VCP system which is fully described and managed by
     input model.
    </para>
   </listitem>
   <listitem>
    <para>
     You are installing a VCP system where additional steps need to be
     performed after provisioning the VCP VMs before
     running <literal>site.yml</literal>
    </para>
   </listitem>
   <listitem>
    <para>
     You are installing a VCP system where additional steps need to be
     performed between configuring the hypervisor nodes, deploying the
     hypervisors, provisioning the VCP VMs, or running site.yml
    </para>
   </listitem>
  </orderedlist>
  <section xml:id="d1e683">
   <title>Installing a fully input model managed Virtual Control Plane based &kw-hos; Cloud</title>
   <para>
    The plays to setup the hlm-hypervisor have been encapsulated into a single
    play that can be run before running site.yml to bring up the full cloud.
   </para>
<screen>cd ~/scratch/ansible/next/hos/ansible
ansible-playbook -i hosts/verb_hosts hlm-hypervisor-setup.yml
ansible-playbook -i hosts/verb_hosts site.yml</screen>
   <para>
    This will configure and deploy the Helion lifecycle manager Hypervisor
    nodes, provision the VCP VMs, and fully deploy the &kw-hos; Cloud.
   </para>
  </section>
  <section xml:id="d1e717">
   <title>Provisioning just the Virtual Control Plane VMs</title>
   <para>
    If your &kw-hos; Cloud installation requires that you perform additional
    actions between the provisioning of the VCP VMs and the running of the
    site.yml playbook then you can run the hlm-hypervisor-setup.yml playbook to
    provision the VCP VMs only, as follows:
   </para>
<screen>cd ~/scratch/ansible/next/hos/ansible
ansible-playbook -i hosts/verb_hosts hlm-hypervisor-setup.yml</screen>
   <para>
    This will configure and deploy the Helion lifecycle manager Hypervisor
    nodes, and then provision the VCP VMs. Once you have performed the
    additional actions you can complete the installation by running
    the <literal>site.yml</literal> playbook.
   </para>
  </section>
  <section xml:id="d1e748">
   <title>Manually running each phase of the Virtual Control Plane provisioning</title>
   <para>
    If you need explicit control over when the different phases of the
    configuration and deployment of the HLM Hypervisor nodes and the
    provisioning of the VCP VMs happen then you can run through the following
    steps, performing any required additional actions before moving on to the
    next step.
   </para>
   <section xml:id="d1e755">
    <title>Configure OS and Networking on HLM Hypervisor nodes</title>
<screen>cd ~/scratch/ansible/next/hos/ansible
ansible-playbook -i hosts/verb_hosts osconfig-run.yml --limit=hlm-hypervisors</screen>
    <para>
     This will configure the OS environment and any relevant networking
     infrastructure on the HLM Hypervisor nodes that will be needed to support
     the associated VCP VMs.
    </para>
   </section>
   <section xml:id="d1e792">
    <title>Deploy and Configure the HLM Hypervisor nodes</title>
<screen>cd ~/scratch/ansible/next/hos/ansible
ansible-playbook -i hosts/verb_hosts hlm-hypervisor-deploy.yml</screen>
    <para>
     This will install and configure any additional software that needs to be
     in place before you can provision the VCP VMs.
    </para>
    <para>
     Provision the VCP VMs
    </para>
<screen>cd ~/scratch/ansible/next/hos/ansible
ansible-playbook -i hosts/verb_hosts hlm-hypervisor-vms-deploy.yml</screen>
    <para>
     This will provision and start the VCP VMs, but will not perform any
     additional setup operations on them.
    </para>
    <para>
     When you have completed any additional actions you can then
     run <literal>site.yml</literal> to complete the installation of the
     &kw-hos; Cloud.
    </para>
   </section>
  </section>
 </section>
 <section xml:id="nic_mapping_for_hlm_virtual_controllers">
  <title>NIC Mapping for HLM Virtual-Controllers</title>
  <para>
   For a bare-metal system the set
   of <literal>nic-mappings</literal> specified
   within <literal>nic_mappings.yml</literal> file define the mapping of PCI
   bus-addresses to device name.
  </para>
  <para>
   For a HLM Virtual-Controller the set
   of <literal>nic-mappings</literal> specified
   within <literal>nic_mappings.yml</literal> file defines the mapping of PCI
   bus-addresses to device name, but is also used to construct the HLM
   Virtual-Controller VM with network interfaces at those PCI bus-addresses.
  </para>
  <para>
   When defining the network-interface model for a HLM
   Virtual-Controller it will become apparent how many network interfaces are
   required for each controller and therefore how many NICs need to be defined
   in the <literal>nic-mappings</literal> for this
   VM.
  </para>
  <para>
   It is recommended that you define a
   separate <literal>nic-mappings</literal> for each number of NICs that you
   reference. For example the following defines three mappings, one
   with a single NIC, a second with two NICs and a third with eight
   NICs:
  </para>
<screen>...
  nic-mappings:
...

    - name: HOS-VIRTUAL-ONE-PORT
      physical-ports:
        - logical-name: hed1
          bus-address: '0000:01:01.0'
          type: simple-port
    - name: HOS-VIRTUAL-TWO-PORT
      physical-ports:
        - logical-name: hed1
          type: simple-port
          bus-address: "0000:01:01.0"
        - logical-name: hed2
          type: simple-port
          bus-address: "0000:01:02.0"

    - name: HOS-VIRTUAL-EIGHT-PORT
      physical-ports:
        - bus-address: '0000:01:01.0'
          logical-name: hed1
          type: simple-port
        - bus-address: '0000:01:02.0'
          logical-name: hed2
          type: simple-port
        - bus-address: '0000:01:03.0'
          logical-name: hed3
          type: simple-port
        - bus-address: '0000:01:04.0'
          logical-name: hed4
          type: simple-port
        - bus-address: '0000:01:05.0'
          logical-name: hed5
          type: simple-port
        - bus-address: '0000:01:06.0'
          logical-name: hed6
          type: simple-port
        - bus-address: '0000:01:07.0'
          logical-name: hed7
          type: simple-port
        - bus-address: '0000:01:08.0'
          logical-name: hed8
          type: simple-port</screen>
  <section xml:id="d1e956">
   <title>Recommended Mappings</title>
   <para>
    It is recommended that the nic-mappings for HLM Virtual-Controllers use the
    PCI-bus addresses of the
    form <literal>"0000:01:<replaceable>XX</replaceable>.0"</literal>,
    where <replaceable>XX</replaceable> is the
    value <literal>01..1f</literal> (1..31 in hexadecimal).
   </para>
   <para>
    This style isolates the virtual NIC interfaces onto PCI
    bus <literal>01</literal> and should prevent bus-address clashes with
    other PCI devices on the system.
   </para>
  </section>
 </section>
 <section xml:id="d1e983">
  <title>Notification of actions to be taken post upgrade</title>
  <para>
   As part of an upgrade play actions that need to be taken - that cannot be
   automatically done since sharing the host with other vms will be placed
   in <emphasis role="bold">/var/run/hos</emphasis>.
  </para>
  <para>
   The actions that are currently notified are
  </para>
<screen>/var/run/hos/reload_apparmor</screen>
<screen>/var/run/hos/reload_libvirtd</screen>
<screen>/var/run/hos/restart_libvirtd
</screen>
 </section>
 <section xml:id="d1e1009">
  <title>Reboot of a HLM-Hypervisor node</title>
  <para>
   Rebooting a HLM-Hypervisor node follows the standard procedure in the
   &kw-hos; documentation .
  </para>
  <para>
   The sequence here is to target the &kw-hos; vms first then the host itself -
   then shutdown the vms before reboot. Note that there is an ansible host
   group in verb hosts called &lt;hlm-hypervisor node name&gt;-vms which
   targets the vms on the node with name of hlm-hypervisor-node-name. In this
   case <emphasis role="bold">liam-vcp-cp1-vmf-m1-mgmt-vms</emphasis>
  </para>
<screen>
<?dbsuse-fo font-size="0.70em"?>
ansible-playbook -i hosts/verb_hosts --limit liam-vcp-cp1-vmf-m1-mgmt-vms hlm-stop.yml
ansible-playbook -i hosts/verb_hosts --limit liam-vcp-cp1-vmf-m1-mgmt hlm-stop.yml
ansible-playbook -i hosts/verb_hosts --limit liam-vcp-cp1-vmf-m1-mgmt \
hlm-hypervisor-vms-stop.yml # ie. shutdown the vms. </screen>
  <para>
   <emphasis role="bold">reboot node</emphasis>
  </para>
<screen>
<?dbsuse-fo font-size="0.70em"?>
ansible-playbook -i hosts/verb_hosts --limit liam-vcp-cp1-vmf-m1-mgmt hlm-hypervisor-vms-start.yml
   (the vms should now be on autoboot)
ansible-playbook -i hosts/verb_hosts --limit liam-vcp-cp1-vmf-m1-mgmt-vms hlm-start.yml
ansible-playbook -i hosts/verb_hosts --limit liam-vcp-cp1-vmf-m1-mgmt hlm-start.yml</screen>
<!-- FIXME: comment from DITA orginal: "[Gerry Fahy tested and] I think 1 more step is necessary here i.e. ..."-->
 </section>
 <section xml:id="d1e1053">
  <title>Staged install on HLM-Hypervisor individual playbooks.</title>
  <para>
   This is a call out of the set of various playbooks to allow for a staged
   deployment of the hlm-hypervisor to allow for NFV vms to be installed as
   well at an appropriate time.
  </para>
  <para>
   Once the node has been installed and the input model created from the NFV
   seed VM, the next stage is to configure this node and deploy the other nodes
   in the control plane.
  </para>
  <para>
   The install of the other BM nodes can follow standard practice of running
   the cobbler deploy and the bm-reimage playbooks - then you need to run the
   osconfig playbook and hlm-hypervisor on these nodes.
  </para>
  <para>
   This brings up the networking on the nodes and deploys a minimal set of
   packages to allow for virtual machine deployment. This is done by
  </para>
<screen>
<?dbsuse-fo font-size="0.70em"?>
ansible-playbook -i hosts/verb_hosts --limit hlm-hypervisors osconfig-run.yml
ansible-playbook -i hosts/verb_hosts --limit hlm-hypervisors hlm-hypervisors-deploy.yml</screen>
  <para>
   At this stage you now have the nodes up with the networking configured and
   the basic set of packages installed to allow for subsequent deploy of the
   NFV vms.
  </para>
  <para>
   Once this is done we then would need to deploy the &kw-hos; vms - this can
   be done with
  </para>
<screen>
<?dbsuse-fo font-size="0.70em"?>
ansible-playbook - i hosts/verb_hosts --limit hlm-hypervisors hlm-hypervisor-vms-deploy.yml</screen>
  <para>
   Note that these 3 plays have been encapsulated into a single play that can
   be run with
  </para>
<screen>
<?dbsuse-fo font-size="0.70em"?>
ansible-playbook -i hosts/verb_hosts hlm-hypervisor-setup.yml</screen>
  <para>
   At this point you now have all the nodes to be able to deploy the cloud -
   this can now be achieved by running
  </para>
<screen>
<?dbsuse-fo font-size="0.70em"?>
ansible-playbook -i hosts/verb_hosts site.yml</screen>
 </section>
 <section xml:id="d1e1125">
  <title>Virtual Controller Replacement</title>
  <para>
   There are several use cases here, all related to repair or replacement of
   VCP infrastructure. Essentially in each case you start by recovering the
   hypervisor (if necessary) and then create new VMs to replace the ones that
   were lost. Once they complete
   <literal>hlm-hypervisor-vms-deploy.yml</literal>, it is the equivalent of
   having just completed <literal>bm-reimage.yml</literal> for a physical node
   replacement, and can proceed with the existing instructions for controller
   recovery.
  </para>
  <orderedlist>
   <listitem>
    <para>
     Repair or replacement of a single bad VM. One that has been damaged or
     lost, not just a simple reboot.
    </para>
   </listitem>
   <listitem>
    <para>
     Of a single hypervisor, not causing for example RabbitMQ to lose quorum or
     similar effects, because all the VMs on this host are part of 3-node
     clusters and the other nodes are still up.
    </para>
   </listitem>
   <listitem>
    <para>
     Disaster recovery - loss of all hypervisors (and therefore control plane
     VMs)
    </para>
   </listitem>
  </orderedlist>
  <para>
   <emphasis role="bold">Use case 1 - single bad VM.</emphasis>
  </para>
<screen>
<?dbsuse-fo font-size="0.70em"?>
ansible-playbook -i hosts/verb_hosts --limit hlm-hypervisors osconfig-run.yml
ansible-playbook -i hosts/verb_hosts --limit hlm-hypervisors hlm-hypervisors-deploy.yml
ansible-playbook - i hosts/verb_hosts --limit hlm-hypervisors hlm-hypervisor-vms-deploy.yml</screen>
  <para>
   <emphasis role="bold">Use case 2 - single lost hypervisor but services are
   still running on the rest of the control plane</emphasis>
  </para>
  <para>
   This starts with recovering the hypervisor, which is very similar to
   recovering a dead physical controller.
  </para>
  <orderedlist>
   <listitem>
    <para>
     You'll need to update servers.yml and cobbler if the node's mac address
     or ilo details have changed (because of physical repairs/replacements).
    </para>
   </listitem>
   <listitem>
    <para>
     Reimage just this node and then run the hypervisor plays on it
    </para>
<screen>
<?dbsuse-fo font-size="0.70em"?>
ansible-playbook -i hosts/localhost bm-reimage.yml --limit=hypervisor1
ansible-playbook -i hosts/verb_hosts --limit hypervisor1 osconfig-run.yml
ansible-playbook -i hosts/verb_hosts --limit hypervisor1 hlm-hypervisors-deploy.yml
ansible-playbook - i hosts/verb_hosts --limit hypervisor1 hlm-hypervisor-vms-deploy.yml</screen>
    <para>
     At this stage, the VMs are up with a bare operating system, and you should
     follow the physical controller recovery instructions from here.
    </para>
   </listitem>
  </orderedlist>
  <para>
   <emphasis role="bold">Use case 3 - lost all hypervisors and thus the
   complete control plane</emphasis>
  </para>
  <para>
   Currently this use case is not correctly documented, even for a completely
   physical system - see
   <link xlink:href="https://jira.hpcloud.net/browse/DOCS-2921">DOCS-2921</link>.
   However once there is a working procedure for physical it could be adapted
   using similar techniques to the above.
  </para>
 </section>
 <section xml:id="d1e1196">
  <title>Ansible host-specific variables for network-group access</title>
  <para>
   From a 3rd-party view-point there are two important scenarios for accessing
   the network:
  </para>
  <itemizedlist>
   <listitem>
    <para>
     3rd-Party service on a HLM managed Server.
    </para>
   </listitem>
   <listitem>
    <para>
     3rd-Party VM on a HLM-Hypervisor Server.
    </para>
   </listitem>
  </itemizedlist>
  <para>
   Each requires different sub-sets of data to establish the network
   configuration and potentially the end-point to which they need to attach.
   The following table indicates the set of data that is made available, and
   the scenarios that may find this data useful. The network-group
   specifications are provided under an
   attribute: <literal>host.my_network_groups:</literal> containing a list of
   dicts, one per network-group.
  </para>
  <informaltable>
   <tgroup cols="6">
    <colspec colnum="1" colname="c1"/>
    <colspec colnum="2" colname="c2"/>
    <colspec colnum="3" colname="c3"/>
    <colspec colnum="4" colname="c4"/>
    <colspec colnum="5" colname="c5"/>
    <colspec colnum="6" colname="c6"/>
    <thead>
     <row>
      <entry/>
      <entry/>
      <entry/>
      <entry namest="c4" nameend="c5">Consumed by 3rd-Party scenario</entry>
      <entry/>
     </row>
     <row>
      <entry>Index</entry>
      <entry>Attribute</entry>
      <entry>Value</entry>
      <entry>3rd-Party VM</entry>
      <entry>3rd-Party Service</entry>
      <entry>Comment</entry>
     </row>
    </thead>
    <tbody>
     <row>
      <entry>
       <para>
        <literal>&lt;network-group-name&gt;</literal>
       </para>
      </entry>
      <entry/>
      <entry/>
      <entry>
       &check;
      </entry>
      <entry>
       &check;
      </entry>
      <entry>name of the network-group</entry>
     </row>
     <row>
      <entry/>
      <entry>
       <para>
        <literal>network-name</literal>
       </para>
      </entry>
      <entry>
       <para>
        <literal>&lt;network-name&gt;</literal>
       </para>
      </entry>
      <entry>
       &check;
      </entry>
      <entry>
       &check;
      </entry>
      <entry>name of the network</entry>
     </row>
     <row>
      <entry/>
      <entry>
       <para>
        <literal>passthrough-device</literal>
       </para>
      </entry>
      <entry>
       <para>
        <literal>&lt;bridge-name&gt;</literal>
       </para>
      </entry>
      <entry>
       &check;
      </entry>
      <entry/>
      <entry>openvswitch-bridge to attach to for unfiltered access to the specified
                network</entry>
     </row>
     <row>
      <entry/>
      <entry>
      <para>
        <literal>tagged-vlan</literal>
      </para>
      </entry>
      <entry>
       <para>
        <literal>true/false</literal>
       </para>
      </entry>
      <entry>
       &check;
      </entry>
      <entry/>
      <entry>is this network running tagged</entry>
     </row>
     <row>
      <entry/>
      <entry>
       <para>
        <literal>vlanid</literal>
       </para>
      </entry>
      <entry>
       <para>
        <literal>&lt;value&gt;</literal>
       </para>
      </entry>
      <entry>
       &check;
      </entry>
      <entry></entry>
      <entry>which vlanid: is it tagged with</entry>
     </row>
     <row>
      <entry/>
      <entry><literal>device</literal>
      </entry>
      <entry><literal>&lt;dev-name&gt;</literal>
      </entry>
      <entry/>
      <entry>
       &check;
      </entry>
      <entry>Device used for local-services</entry>
     </row>
     <row>
      <entry/>
      <entry>
       <para>
        <literal>address</literal>
       </para>
      </entry>
      <entry>
       <para>
        <literal>&lt;ip-address&gt;</literal>
       </para>
      </entry>
      <entry/>
      <entry>
       &check;
      </entry>
      <entry>Address only used for local-services</entry>
     </row>
     <row>
      <entry/>
      <entry>
       <para>
        <literal>cidr</literal>
       </para>
      </entry>
      <entry>
       <para>
        <literal>&lt;cidr&gt;</literal>
       </para>
      </entry>
      <entry>
       &check;
      </entry>
      <entry>
       &check;
      </entry>
      <entry>CIDR in use by this network</entry>
     </row>
     <row>
      <entry/>
      <entry>
       <para>
        <literal>gateway-ip</literal>
       </para>
      </entry>
      <entry>
       <para>
        <literal>&lt;ip-address&gt;</literal>
       </para>
      </entry>
      <entry>
       &check;
      </entry>
      <entry>
       &check;
      </entry>
      <entry>gateway-ip in use on this network &lt;optional&gt;</entry>
     </row>
     <row>
      <entry/>
      <entry>
       <para>
        <literal>mtu</literal>
       </para>
      </entry>
      <entry>
       <para>
        <literal>&lt;mtu&gt;</literal>
       </para>
      </entry>
      <entry>
       &check;
      </entry>
      <entry>
       &check;
      </entry>
      <entry>the MTU specified or inherited on this network</entry>
     </row>
    </tbody>
   </tgroup>
  </informaltable>
  <para>
   For Example:
  </para>
<screen>host:
    ...
    my_network_groups:
        BLS:
        -   address: 10.244.47.102
            cidr: 10.244.47.0/24
            device: br-vlan3537
            gateway-ip: 10.244.47.1
            network-name: BLS-NET
            passthrough-device: br-bond0
            tagged-vlan: true
            vlanid: 3537
        CAN:
        -   cidr: 10.244.45.0/24
            device: br-bond0
            gateway-ip: 10.244.45.1
            network-name: CAN-NET
            passthrough-device: br-bond0
            tagged-vlan: true
            vlanid: 3535
        CLM:
        -   address: 10.244.44.102
            cidr: 10.244.44.0/24
            device: vlan3534
            gateway-ip: 10.244.44.1
            network-name: CLM-NET
            passthrough-device: br-bond0
            tagged-vlan: true
            vlanid: 3534
        PXE:
        -   address: 10.244.43.102
            cidr: 10.244.43.0/24
            device: br-bond0
            gateway-ip: 10.244.43.1
            network-name: PXE-NET
            passthrough-device: br-bond0
            tagged-vlan: false
            vlanid: 3533
        VLAN1-TUL:
        -   device: br-bond1
            network-name: VLAN1-TUL-NET
            passthrough-device: br-bond1
            tagged-vlan: false
        VLAN2-TUL:
        -   device: br-bond1
            network-name: VLAN2-TUL-NET
            passthrough-device: br-bond1
            tagged-vlan: false
        VxLAN-TUL:
        -   cidr: 10.244.48.0/24
            device: br-bond1
            gateway-ip: 10.244.48.1
            network-name: VxLAN-TUL-NET
            passthrough-device: br-bond1
            tagged-vlan: false
            vlanid: 3538</screen>
  <para>
   Which decodes to mean:
  </para>
  <!-- FIXME: The literals in this list look wrong... - sknorr, 2017-12-29 -->
  <itemizedlist>
   <listitem>
    <para>
     BLS: available locally (<literal>address: 10.244.47.102</literal>) and
     passthrough to hosted-VMs
     (<literal>passthrough-device: br-bond0, vlanid: 3537, cidr:, gateway: </literal>)
    </para>
   </listitem>
   <listitem>
    <para>
     CAN: not-available locally, but passthrough to Hosted-VMs
     (passthrough-device: <literal>br-bond0, vlanid: 3535, cidr:, gateway: </literal>)
    </para>
   </listitem>
   <listitem>
    <para>
     CLM: available locally (<literal>address: 10.244.44.102</literal>) and
     passthrough to hosted-VMs
     (<literal>passthrough-device: br-bond0, vlanid: 3534, cidr:, gateway: </literal>)
    </para>
   </listitem>
   <listitem>
    <para>
     PXE: available locally (<literal>address: 10.244.43.1102</literal>) and
     passthrough to hosted-VMs
     (passthrough-device: <literal>br-bond0, vlanid: 3533, cidr:, gateway:</literal>)
    </para>
   </listitem>
   <listitem>
    <para>
     VLAN1-TUL: not available locally, but passthrough to Hosted-VMs
     (<literal>passthrough-device: br-bond1, tagged-vlan: false</literal>)
    </para>
   </listitem>
   <listitem>
    <para>
     VLAN2-TUL: not available locally, but passthrough to Hosted-VMs
     (<literal>passthrough-device: br-bond1, tagged-vlan: false</literal>)
    </para>
   </listitem>
   <listitem>
    <para>
     VxLAN-TUL: not available locally, but passthrough to Hosted-VMs
     (<literal>passthrough-device: br-bond1, tagged-vlan: false, cidr:,
     gateway:</literal>)
    </para>
   </listitem>
  </itemizedlist>
  <para>
   A 3rd-Party application can provide an ansible-playbook that
   processes the specified set of ansible-vars and extracts the information
   required to support the configuration of the 3rd-party application.
  </para>
  <section xml:id="d1e1813">
   <title>Example 3rd-Party <literal>Ansible</literal> for Network Configuration Access</title>
   <para>
    The following is an example ansible playbook to find the name of
    the <literal>device</literal> associated with the
    <literal>MANAGEMENT</literal> network-group. It can be easily customized to
    select any known network and any attribute of that network shown in the
    table above.
   </para>
<screen>
<?dbsuse-fo font-size="0.70em"?>
#
# (c) Copyright 2016 Hewlett Packard Enterprise Development LP
#
# Licensed under the Apache License, Version 2.0 (the "License"); you may
# not use this file except in compliance with the License. You may obtain
# a copy of the License at
#
# http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
# WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
# License for the specific language governing permissions and limitations
# under the License.
#
# Example playbook to interrogate and recover network specific
# configuration data for Third-Party applications
- name: third-party | network-details | display all available network groups
  debug:
    var: host.my_network_groups
- name: third-party | network-details | select a network group
  set_fact:
    tp_network_group_name: 'MANAGEMENT'
- name: third-party | network-details | display which network group interrogating
  debug:
    var: tp_network_group_name
- name: third-party | network-details | get details of selected network group
  set_fact:
    tp_network_group_data: "{{ host.my_network_groups | item(tp_network_group_name) }}"
- name: third-party | network-details | display details of selected network group
  debug:
    var: tp_network_group_data
- name: third-party | network-details | get 'device' for selected network group
  set_fact:
    tp_network_group_device: "{{ tp_network_group_data[0] | item('device') }}"
- name: third-party | network-details | display 'device' for selected network group
  debug:
    var: tp_network_group_device</screen>
  </section>
 </section>
 <section xml:id="d1e1847">
  <title>Ansible host-specific variables for gluster</title>
  <para>
   If disks have been set aside for gluster in the hypervisor input model as
   outlined earlier, then a list of these disks can be found
   in <literal>host.my_device_groups.gluster.devices</literal>, for example:
  </para>
<screen>host:
    my_device_groups:
        gluster:
        -   consumer:
                name: gluster
                suppress_warnings: true
            devices:
            -   name: /dev/sde
            -   name: /dev/sdf
            name: gluster

</screen>
 </section>
 <section xml:id="d1e1863">
  <title>"Empty" HLM Hypervisor Node</title>
  <para>
   In order to create an empty hlm-hypervisor node then specifying
   <literal>hlm-hypervisor: True</literal> is necessary.
  </para>
<screen>- id: hypervisor1
      ip-addr: 10.225.107.74
      role: HLM-HYPERVISOR-ROLE
      server-group: RACK2
      <emphasis role="bold">hlm-hypervisor: True</emphasis>
      nic-mapping: HP-BL460-6PORT
      mac-addr: 9c:7e:96:22:9e:d8
      ilo-ip: 10.1.18.42
      ilo-password: whatever
      ilo-user: whatever</screen>
  <para>
   In addition, if these nodes are being used in scenarios where there are no
   &kw-hos; servers to consume log data etc., you will need to ensure that in
   the input model the only <literal>common-service-components</literal> would
   be the <literal>lifecycle-manager-target</literal>.
  </para>
 </section>
 <section xml:id="d1e1878">
  <title>Monasca Monitoring of HLM Hypervisors</title>
  <para>
   If your build includes a top-level
   play <literal>hlm-hyperviros-monitoring-deploy.yml</literal> then you have
   the option of enabling Monasca monitoring for the HLM Hypervisor nodes by
   running that playbook as follows:
  </para>
<screen>cd ~/scratch/ansible/next/hos/ansible
ansible-playbook -i hosts/verb_hosts hlm-hypervisor-monitoring-deploy.yml</screen>
  <section xml:id="d1e1894">
   <title>Monitored Metrics</title>
   <para>
    Currently two types of metrics are gathered:
   </para>
   <orderedlist>
    <listitem>
     <para>
      Overall summary state for a HLM Hypervisor node
      - <literal>hlm-hypervisor.vcp_vms</literal>
     </para>
     <orderedlist>
      <listitem>
       <para>
        Reports 0 indicating healthy so long as all monitored VMs, and
        associated networks, are running/active.
       </para>
      </listitem>
      <listitem>
       <para>
        If any of the VMs or associated networks are not running/active,
        reports a value of 1.
       </para>
      </listitem>
      <listitem>
       <para>
        Associated additional dimensions:
       </para>
       <orderedlist>
        <listitem>
         <para>
          service: hlm-hypervisor
         </para>
        </listitem>
        <listitem>
         <para>
          component: vcp
         </para>
        </listitem>
       </orderedlist>
      </listitem>
     </orderedlist>
    </listitem>
    <listitem>
     <para>
      Summary state for each VCP VM running on a HLM Hypervisor node
      - <literal>hlm-hypervisor.vcp_vm.&lt;vm_name&gt;</literal>
     </para>
     <orderedlist>
      <listitem>
       <para>
        Reports 0 indicating health so long as a VM, and its associated
        networks, are running/active.
       </para>
      </listitem>
      <listitem>
       <para>
        If the VM or any of its networks are not running/active, reports a
        value of 1.
       </para>
      </listitem>
      <listitem>
       <para>
        Associated additional dimensions:
       </para>
       <orderedlist>
        <listitem>
         <para>
          service: hlm-hypervisor
         </para>
        </listitem>
        <listitem>
         <para>
          component: vcp_vm
         </para>
        </listitem>
        <listitem>
         <para>
          domain: &lt;vm_name&gt;
         </para>
        </listitem>
       </orderedlist>
      </listitem>
     </orderedlist>
    </listitem>
   </orderedlist>
  </section>
  <section xml:id="d1e1964">
   <title>Configured Alarms</title>
   <para>
    Alarms are configured to track these metrics; an alarm will be triggered if
    the metric starts reporting a state of 1.
   </para>
   <para>
    If an alarm is triggered the metric's measurement value_meta will include a
    detail section outlining the detected issues.
   </para>
  </section>
  <section xml:id="d1e1977">
   <title>HLM Hypervisor Monitoring Configuration</title>
   <para>
    The monitoring mechanism is driven by per-VM JSON configuration files in
    <literal>/etc/hlm-hypervisor/vms</literal> with the following format:
   </para>
<screen>{
    "hhv_vm_config": {
        "name": "&lt;hostname&gt;",
        "domain": "&lt;vm_name&gt;",
        "networks": [
            "&lt;vnet_1&gt;",
            "&lt;vnet_2&gt;"
        ],
        "dimensions": {
            "service": "hlm-hypervisor",
            "component": "vcp_vm",
        	"domain": "&lt;vm_name&gt;"
        },
        "check_type": "vm"
    }
}</screen>
   <para>
    The HLM Hypervisor Monasca detection plugin
    <literal>hhv.py</literal>, exposes two classes,
    <literal>HLMHypervisorSummary</literal> and
    <literal>HLMHypervisorVMs</literal>, which respectively generate the
    appropriate Monasca configuration to support both types of metric.
   </para>
   <para>
    The HLM Hypervisor Monasca check plugin,
    <literal>hhv.py</literal>, processes the configuration generated by the
    detection plugin and updates the relevant metrics.
   </para>
  </section>
 </section>
</chapter>
