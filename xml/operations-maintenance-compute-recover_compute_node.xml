<?xml version="1.0"?>
<?xml-stylesheet href="urn:x-suse:xslt:profiling:docbook51-profile.xsl"
 type="text/xml"
 title="Profiling step"?>
<!DOCTYPE section [
 <!ENTITY % entities SYSTEM "entity-decl.ent"> %entities;
]>
<section xmlns="http://docbook.org/ns/docbook" xmlns:xi="http://www.w3.org/2001/XInclude" xmlns:xlink="http://www.w3.org/1999/xlink" xml:id="recover_computenode">
 <title>Recovering a Compute Node</title>
 <para>
  If one or more of your compute nodes has experienced an issue such as power
  loss or hardware failure, then you need to perform disaster recovery. Here we
  provide different scenarios and how to resolve them to get your cloud
  repaired.
 </para>
 <para>
  Typical scenarios in which you will need to recover a compute node include
  the following:
 </para>
 <itemizedlist>
  <listitem>
   <para>
    The node has failed, either because it has shut down has a hardware
    failure, or for another reason.
   </para>
  </listitem>
  <listitem>
   <para>
    The node is working but the <literal>nova-compute</literal> process is not
    responding, thus instances are working but you cannot manage them (for
    example to delete, reboot, and attach/detach volumes).
   </para>
  </listitem>
  <listitem>
   <para>
    The node is fully operational but monitoring indicates a potential issue
    (such as disk errors) that require down time to fix.
   </para>
  </listitem>
 </itemizedlist>
 <section xml:id="idg-all-operations-maintenance-compute-recover_compute_node-xml-7">
  <title>What to do if your compute node is down</title>
  <para>
   <emphasis role="bold">Compute node has power but is not powered
   on</emphasis>
  </para>
  <para>
   If your compute node has power but is not powered on, use these steps to
   restore the node:
  </para>
  <procedure>
   <step>
    <para>
     Log in to the &clm;.
    </para>
   </step>
   <step>
    <para>
     Obtain the name for your compute node in Cobbler:
    </para>
<screen>&prompt.ardana;sudo cobbler system list</screen>
   </step>
   <step>
    <para>
     Power the node back up with this playbook, specifying the node name from
     Cobbler:
    </para>
<screen>&prompt.ardana;cd ~/openstack/ardana/ansible
&prompt.ardana;ansible-playbook -i hosts/localhost bm-power-up.yml -e nodelist=&lt;node name&gt;</screen>
   </step>
  </procedure>
  <para>
   <emphasis role="bold">Compute node is powered on but services are not
   running on it</emphasis>
  </para>
  <para>
   If your compute node is powered on but you are unsure if services are
   running, you can use these steps to ensure that they are running:
  </para>
  <procedure>
   <step>
    <para>
     Log in to the &clm;.
    </para>
   </step>
   <step>
    <para>
     Confirm the status of the compute service on the node with this playbook:
    </para>
<screen>&prompt.ardana;cd ~/scratch/ansible/next/ardana/ansible
&prompt.ardana;ansible-playbook -i hosts/verb_hosts nova-status.yml --limit &lt;hostname&gt;</screen>
   </step>
   <step>
    <para>
     You can start the compute service on the node with this playbook:
    </para>
<screen>&prompt.ardana;cd ~/scratch/ansible/next/ardana/ansible
&prompt.ardana;ansible-playbook -i hosts/verb_hosts nova-start.yml --limit &lt;hostname&gt;</screen>
   </step>
  </procedure>
 </section>
 <section xml:id="unplanned">
  <title>Scenarios involving disk failures on your compute nodes</title>
  <para>
   Your compute nodes should have a minimum of two disks, one that is used for
   the operating system and one that is used as the data disk. These are
   defined during the installation of your cloud, in the
   <literal>~/openstack/my_cloud/definition/data/disks_compute.yml</literal>
   file on the &clm;. The data disk(s) are where the
   <literal>nova-compute</literal> service lives. Recovery scenarios will
   depend on whether one or the other, or both, of these disks experienced
   failures.
  </para>
  <para>
   <emphasis role="bold">If your operating system disk failed but the data
   disk(s) are okay</emphasis>
  </para>
  <para>
   If you have had issues with the physical volume that nodes your operating
   system you need to ensure that your physical volume is restored and then you
   can use the following steps to restore the operating system:
  </para>
  <procedure>
   <step>
    <para>
     Log in to the &clm;.
    </para>
   </step>
   <step>
    <para>
     Source the administrator credentials:
    </para>
<screen>&prompt.ardana;source ~/service.osrc</screen>
   </step>
   <step>
    <para>
     Obtain the hostname for your compute node, which you will use in
     subsequent commands when <literal>&lt;hostname&gt;</literal> is requested:
    </para>
<screen>&prompt.ardana;openstack host list | grep compute</screen>
   </step>
   <step>
    <para>
     Obtain the status of the <literal>nova-compute</literal> service on that
     node:
    </para>
<screen>&prompt.ardana;nova service-list --host &lt;hostname&gt;</screen>
   </step>
   <step>
    <para>
     You will likely want to disable provisioning on that node to ensure that
     <literal>nova-scheduler</literal> does not attempt to place any additional
     instances on the node while you are repairing it:
    </para>
<screen>&prompt.ardana;nova service-disable --reason "node is being rebuilt" &lt;hostname&gt;</screen>
   </step>
   <step>
    <para>
     Obtain the status of the instances on the compute node:
    </para>
<screen>&prompt.ardana;nova list --host &lt;hostname&gt; --all-tenants</screen>
   </step>
   <step>
    <para>
     Before continuing, you should either evacuate all of the instances off
     your compute node or shut them down. If the instances are booted from
     volumes, then you can use the <literal>nova evacuate</literal> or
     <literal>nova host-evacuate</literal> commands to do this. See
     <xref linkend="liveInstMigration"/> for more details on how to do this.
    </para>
    <para>
     If your instances are not booted from volumes, you will need to stop the
     instances using the <literal>nova stop</literal> command. Because the
     <literal>nova-compute</literal> service is not running on the node you
     will not see the instance status change, but the <literal>Task
     State</literal> for the instance should change to
     <literal>powering-off</literal>.
    </para>
<screen>&prompt.ardana;nova stop &lt;instance_uuid&gt;</screen>
    <para>
     Verify the status of each of the instances using these commands, verifying
     the <literal>Task State</literal> states <literal>powering-off</literal>:
    </para>
<screen>&prompt.ardana;nova list --host &lt;hostname&gt; --all-tenants
&prompt.ardana;nova show &lt;instance_uuid&gt;</screen>
   </step>
   <step>
    <para>
     At this point you should be ready with a functioning hard disk in the node
     that you can use for the operating system. Follow these steps:
    </para>
    <substeps>
     <step>
      <para>
       Obtain the name for your compute node in Cobbler, which you will use in
       subsequent commands when <literal>&lt;node_name&gt;</literal> is
       requested:
      </para>
<screen>&prompt.ardana;sudo cobbler system list</screen>
     </step>
     <step>
      <para>
       Run the following playbook, ensuring that you specify only your UEFI
       &slsa; nodes using the nodelist. This playbook will reconfigure Cobbler
       for the nodes listed.
      </para>
      <screen>&prompt.ardana;cd ~/scratch/ansible/next/ardana/ansible
&prompt.ardana;ansible-playbook prepare-sles-grub2.yml -e nodelist=node1[,node2,node3]</screen>
     </step>
     <step>
      <para>
       Reimage the compute node with this playbook:
      </para>
<screen>&prompt.ardana;cd ~/openstack/ardana/ansible
&prompt.ardana;ansible-playbook -i hosts/localhost bm-reimage.yml -e nodelist=&lt;node name&gt;</screen>
     </step>
    </substeps>
   </step>
   <step>
    <para>
     Once reimaging is complete, use the following playbook to configure the
     operating system and start up services:
    </para>
<screen>&prompt.ardana;cd ~/scratch/ansible/next/ardana/ansible
&prompt.ardana;ansible-playbook -i hosts/verb_hosts site.yml --limit &lt;hostname&gt;</screen>
   </step>
   <step>
    <para>
     You should then ensure any instances on the recovered node are in an
     <literal>ACTIVE</literal> state. If they are not then use the
     <literal>nova start</literal> command to bring them to the
     <literal>ACTIVE</literal> state:
    </para>
<screen>&prompt.ardana;nova list --host &lt;hostname&gt; --all-tenants
&prompt.ardana;nova start &lt;instance_uuid&gt;</screen>
   </step>
   <step>
    <para>
     Reenable provisioning:
    </para>
<screen>&prompt.ardana;nova service-enable &lt;hostname&gt;</screen>
   </step>
   <step>
    <para>
     Start any instances that you had stopped previously:
    </para>
<screen>&prompt.ardana;nova list --host &lt;hostname&gt; --all-tenants
&prompt.ardana;nova start &lt;instance_uuid&gt;</screen>
   </step>
  </procedure>
  <para>
   <emphasis role="bold">If your data disk(s) failed but the operating system
   disk is okay OR if all drives failed</emphasis>
  </para>
  <para>
   In this scenario your instances on the node are lost. First, follow steps 1
   to 5 and 8 to 9 in the previous scenario.
  </para>
  <para>
   After that is complete, use the <literal>nova rebuild</literal> command to
   respawn your instances, which will also ensure that they receive the same IP
   address:
  </para>
<screen>&prompt.ardana;nova list --host &lt;hostname&gt; --all-tenants
&prompt.ardana;nova rebuild &lt;instance_uuid&gt;</screen>
 </section>
</section>
