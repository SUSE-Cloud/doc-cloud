<?xml version="1.0"?>
<!DOCTYPE section [
 <!ENTITY % entities SYSTEM "entities.ent"> %entities;
]>
<section xml:id="idg-installation-installation-ceph-alternative_supported_choice-xml-1"
 xmlns="http://docbook.org/ns/docbook" version="5.1"
 xmlns:xi="http://www.w3.org/2001/XInclude"
 xmlns:xlink="http://www.w3.org/1999/xlink">
 <title>Alternative Supported Choices</title>
 <para>
  This section provides insight on how to alter the
  <literal>entry-scale-kvm-ceph</literal> input model to deploy Ceph with
  various supported options. We recommend that you deploy your supported
  choice only after evaluating all pros and cons. For technical details, please
  consult with the technical support team. The choices available can impact the
  performance and scaling of clusters. Choices are illustrated for reference
  purposes and you can combine one or more of them as needed. The content is
  categorized as follows:
 </para>
 <orderedlist xml:id="ol_sqb_mrg_kw">
  <listitem>
   <para>
    Core Ceph
   </para>
   <itemizedlist xml:id="ul_ejq_yrg_kw">
    <listitem>
     <para>
      <!-- FIXME: <link xlink:href="#config_ceph/deploying-monitor-on-standalone-node">Installing the Monitor Service on Standalone Nodes</link> -->
     </para>
    </listitem>
    <listitem>
     <para>
      <!-- FIXME: <link xlink:href="#config_ceph/single-vlan-for-all-ceph-traffic">Using a Single VLAN for All Ceph Traffic (Management, Client, and Internal OSD)</link> -->
     </para>
    </listitem>
    <listitem>
     <para>
      <!-- FIXME: <link xlink:href="#config_ceph/using-two-vlan">Using Two VLANs: For Management and Client Traffic and for Internal OSD` Traffic</link> -->
     </para>
    </listitem>
   </itemizedlist>
  </listitem>
  <listitem>
   <para>
    RADOS Gateway
   </para>
   <itemizedlist xml:id="ul_ptr_frt_mx">
    <listitem>
     <para>
<!-- FIXME: <link xlink:href="#config_ceph/install-rados-gateway-on-cluster-node-that-host-ceph-monitor">Installing RADOS Gateway on Dedicated Cluster Nodes that Host the Ceph Monitor Service</link> -->
     </para>
    </listitem>
    <listitem>
     <para>
<!-- FIXME: <link xlink:href="#config_ceph/install-rados-gateway-on-controller-nodes">Installing RADOS Gateway on Controller Nodes</link> -->
     </para>
    </listitem>
    <listitem>
     <para>
<!-- FIXME: <link xlink:href="#config_ceph/install-more-two-rados-gateway-servers">Installing More than Two RADOS Gateway Servers</link> -->
     </para>
    </listitem>
   </itemizedlist>
  </listitem>
  <listitem userlevel="INTERNAL">
   <para>
<!-- FIXME: <link xlink:href="#config_ceph/ceph-deployment-vcp">Ceph Deployment with Virtual Control Plane </link> -->
   </para>
  </listitem>
 </orderedlist>
 <section>
  <title>Core Ceph</title>
  <para>
   <emphasis role="bold">Installing the Monitor Service on Standalone
   Nodes</emphasis>
  </para>
  <para>
   The following section provides the procedure for installing the monitor
   service on standalone nodes instead of installing on controller nodes, as
   mentioned in <literal>entry-scale-kvm-ceph</literal>.
  </para>
  <important>
   <para>
    If you want to install the monitor service as a dedicated resource node,
    you must decide before deploying Ceph. &kw-hos-phrase; does not support
    deployment transition. After Ceph is deployed, you cannot migrate the
    monitor service from controller nodes to dedicated resource nodes.
   </para>
  </important>
  <para>
   <emphasis role="bold">Prerequisite</emphasis>
  </para>
  <para>
   Perform the following steps to install the Ceph monitor on a dedicated node.
   Note that the Ceph requires at least three monitoring servers to form a
   cluster in case of a node failure.
  </para>
  <procedure>
   <step>
    <para>
     Log in to the lifecycle manager.
    </para>
   </step>
   <step>
    <para>
     Copy the <literal>entry-scale-kvm-ceph</literal> input model to the
     <literal>~/helion/my_cloud/definition</literal> directory before you begin
     the editing process:
    </para>
<screen>cp -r ~/helion/examples/entry-scale-kvm-ceph/* ~/helion/my_cloud/definition/</screen>
   </step>
   <step>
    <para>
     Edit the <literal>control_plane.yml</literal> to create a new cluster,
     such as with <literal>ceph-mon</literal>, as shown here.
    </para>
<screen>clusters:
  - name: cluster1
    cluster-prefix: c1
    server-role: CONTROLLER-ROLE
    member-count: 3
    allocation-policy: strict
    service-components:
      - lifecycle-manager
      - ntp-server
      ...

  <emphasis role="bold">- name: ceph-mon
    cluster-prefix: ceph-mon
    server-role: CEP-MON-ROLE
    min-count: 3
    allocation-policy: strict
    service-components:
      - ntp-client
      - ceph-monitor</emphasis>

  - name: rgw
    cluster-prefix: rgw
    server-role: RGW-ROLE
    ...</screen>
   </step>
   <step>
    <para>
     Edit the <literal>~/helion/my_cloud/definition/data/servers.yml</literal>
     file to define the Ceph monitor node (monitor services). The following
     example shows three nodes for monitor services. We recommend using an odd
     number of monitor nodes.
    </para>
<screen># Ceph Monitor Nodes
- id: ceph-mon1
  ip-addr: 10.13.111.141
  server-group: RACK1
  role: CEP-MON-ROLE
  nic-mapping: MY-4PORT-SERVER
  mac-addr: "f0:92:1c:05:69:10"
  ilo-ip: 10.12.8.217
  ilo-password: password
  ilo-user: admin

- id: ceph-mon2
  ip-addr: 10.13.111.142
  server-group: RACK2
  role: CEP-MON-ROLE
  nic-mapping: MY-4PORT-SERVER
  mac-addr: "83:92:1c:55:69:b0"
  ilo-ip: 10.12.8.218
  ilo-password: password
  ilo-user: admin

- id: ceph-mon3
  ip-addr: 10.13.111.143
  server-group: RACK3
  role: CEP-MON-ROLE
  nic-mapping: MY-4PORT-SERVER
  mac-addr: "d9:92:1c:25:69:e0"
  ilo-ip: 10.12.8.219
  ilo-password: password
  ilo-user: admin

# Ceph RGW Nodes
- id: rgw1
  ...</screen>
   </step>
   <step>
    <para>
     Edit the
     <literal>~/helion/my_cloud/definition/data/net_interfaces.yml</literal>
     file to define a new network interface set for your Ceph monitors, as
     shown here.
    </para>
<screen>## This defines the interface used for management
## traffic such as logging, monitoring, etc.
- name: CEP-MON-INTERFACES
  network-interfaces:
    - name: BOND0
      device:
          name: bond0
      bond-data:
          options:
              mode: active-backup
              miimon: 200
              primary: hed1
          provider: linux
          devices:
            - name: hed1
            - name: hed2
      network-groups:
        - MANAGEMENT

- name: RGW-INTERFACES
  network-interfaces:
  ...</screen>
   </step>
   <step>
    <para>
     Edit
     <literal>~/helion/my_cloud/definition/data/disks_ceph_monitor.yml</literal>
     to define the disk model for monitor nodes.
    </para>
<screen>disk-models:
- name: CEP-MON-DISKS
  # Disk model to be used for Ceph monitor nodes
  # /dev/sda_root is used as a volume group for /, /var/log and /var/crash
  # sda_root is a templated value to align with whatever partition is really used
  # This value is checked in os config and replaced by the partition actually used
  # on sda e.g. sda1 or sda5

  volume-groups:
    - name: hlm-vg
      physical-volumes:
        - /dev/sda_root

      logical-volumes:
      # The policy is not to consume 100% of the space of each volume group.
      # 5% should be left free for snapshots and to allow for some flexibility.
        - name: root
          size: 30%
          fstype: ext4
          mount: /
        - name: log
          size: 45%
          mount: /var/log
          fstype: ext4
          mkfs-opts: -O large_file
        - name: crash
          size: 20%
          mount: /var/crash
          fstype: ext4
          mkfs-opts: -O large_file
      consumer:
         name: os</screen>
   </step>
   <step>
    <para>
     Edit the
     <literal>~/helion/my_cloud/definition/data/server_roles.yml</literal> file
     to define a new server role for your Ceph monitors:
    </para>
<screen>- name: CEP-MON-ROLE
  interface-model: CEP-MON-INTERFACES
  disk-model: CEP-MON-DISKS</screen>
   </step>
   <step>
    <para>
     Commit your configuration:
    </para>
<screen>cd ~/helion/hos/ansible
git add -A
git commit -m "&lt;commit message&gt;"</screen>
   </step>
   <step>
    <para>
     Run the following playbook to add your nodes into Cobbler:
    </para>
<screen>cd ~/helion/hos/ansible/
ansible-playbook -i hosts/localhost cobbler-deploy.yml</screen>
   </step>
   <step>
    <para>
     To reimage all the nodes using PXE, run the following playbook:
    </para>
<screen>cd ~/helion/hos/ansible/
ansible-playbook -i hosts/localhost bm-reimage.yml</screen>
   </step>
   <step>
    <para>
     Run the configuration processor:
    </para>
<screen>cd ~/helion/hos/ansible/
ansible-playbook -i hosts/localhost config-processor-run.yml</screen>
   </step>
   <step>
    <para>
     Update your deployment directory with this playbook:
    </para>
<screen>cd ~/helion/hos/ansible/
ansible-playbook -i hosts/localhost ready-deployment.yml</screen>
   </step>
   <step>
    <para>
     Deploy these changes:
    </para>
<screen>cd ~/scratch/ansible/next/hos/ansible
ansible-playbook -i hosts/verb_hosts site.yml</screen>
   </step>
 </procedure>
  <para>
   <emphasis role="bold">Using a Single VLAN for All Ceph Traffic (Management,
   Client, and Internal OSD)</emphasis>
  </para>
  <para>
   You can use a single VLAN to transmit all Ceph traffic. This configuration
   is recommended for a small cluster deployment.
  </para>
  <para>
<!--<b>Procedure to configure Entry-scale-kvm-ceph-single-network</b>-->
  </para>
  <para>
   Perform the following steps to configure
   <literal>Entry-scale-kvm-ceph-single-network</literal>.
  </para>
  <procedure>
   <step>
    <para>
     Log in to the lifecycle manager.
    </para>
   </step>
   <step>
    <para>
     Copy the <literal>entry-scale-kvm-ceph</literal> input model to the
     <literal>~/helion/my_cloud/definition</literal> directory before you begin
     the editing process:
    </para>
<screen>cp -r ~/helion/examples/entry-scale-kvm-ceph/* ~/helion/my_cloud/definition/</screen>
   </step>
   <step>
    <para>
     Validate that NIC interfaces are correctly specified in
     <literal>nic_mapping.yml</literal> for servers that are used in the cloud.
    </para>
   </step>
   <step>
    <para>
     Ensure that you have at least two NICs for Ceph nodes to create a bonded
     interface for it.
    </para>
   </step>
   <step>
    <para>
     Validate that your servers are mapped to a correct NIC interface
     specification in <literal>servers.yml</literal>. The following is an
     example of a server node used for OSD deployment:
    </para>
<screen># Ceph OSD Nodes
    - id: osd1
      ip-addr: 192.168.10.9
      role: OSD-ROLE
      server-group: RACK1
      nic-mapping: MY-2PORT-SERVER
      mac-addr: "8b:f6:9e:ca:3b:78"
      ilo-ip: 192.168.9.9
      ilo-password: password
      ilo-user: admin</screen>
   </step>
   <step>
    <para>
     Delete the OSD-INTERNAL and OSD-CLIENT network groups from
     <literal>network_groups.yml</literal>. This is necessary because only the
     management network is used for Ceph traffic, thus OSD-INTERNAL and
     OSD-CLIENT network groups are not required.
    </para>
   </step>
   <step>
    <para>
     Define <literal>net_interfaces.yml</literal> to use only management
     network groups, as shown here.
    </para>
<screen>  - name: CONTROLLER-INTERFACES
      network-interfaces:
        - name: BOND0
          device:
              name: bond0
          bond-data:
              options:
                  mode: active-backup
                  miimon: 200
                  primary: hed3
              provider: linux
              devices:
                - name: hed3
                - name: hed4
          network-groups:
            - EXTERNAL-API
            - EXTERNAL-VM
            - GUEST
            - MANAGEMENT

    - name: COMPUTE-INTERFACES
      network-interfaces:
        - name: BOND0
          device:
              name: bond0
          bond-data:
              options:
                  mode: active-backup
                  miimon: 200
                  primary: hed3
              provider: linux
              devices:
                - name: hed3
                - name: hed4
          network-groups:
            - EXTERNAL-VM
            - GUEST
            - MANAGEMENT

    - name: OSD-INTERFACES
      network-interfaces:
        - name: BOND0
          device:
              name: bond0
          bond-data:
              options:
                  mode: active-backup
                  miimon: 200
                  primary: hed3
              provider: linux
              devices:
                - name: hed3
                - name: hed4
          network-groups:
            - MANAGEMENT

    - name: RGW-INTERFACES
      network-interfaces:
        - name: BOND0
          device:
              name: bond0
          bond-data:
              options:
                  mode: active-backup
                  miimon: 200
                  primary: hed3
              provider: linux
              devices:
                - name: hed3
                - name: hed4
          network-groups:
            - MANAGEMENT</screen>
   </step>
   <step>
    <para>
     Delete VLAN information for OSD-INTERNAL-NET and OSD-CLIENT-NET from
     <literal>networks.yml</literal>. Only Management VLANs are used.
    </para>
   </step>
   <step>
    <para>
     After you set up your configuration files, perform
     <!-- FIXME: steps <emphasis role="bold">8 to 13</emphasis> --> in
     <!-- FIXME: <link xlink:href="#config_ceph/deploying-monitor-on-standalone-node">Deploying the Monitor Service on Standalone Nodes</link> -->.
    </para>
   </step>
 </procedure>
  <para>
   <emphasis role="bold">Using Two VLANs: For Management and Client Traffic and
   for Internal OSD Traffic</emphasis>
  </para>
  <para>
   You can use dual VLANs to transmit Ceph traffic. In this configuration one
   VLAN transmits management and client traffic and the other VLAN transmits
   internal OSD traffic. A separate bonded interface for two VLANs is used with
   four NICs. This configuration provides two aspects:
  </para>
  <itemizedlist xml:id="ul_ypz_qgy_lw">
   <listitem>
    <para>
     Use of two networks, such as VLANs.
    </para>
   </listitem>
   <listitem>
    <para>
     Use of separate bonded interfaces for each VLAN (different from what is
     provided in <literal>entry-scale-kvm-ceph</literal>).
    </para>
   </listitem>
  </itemizedlist>
  <para>
   The use of separate NICs segregates traffic at the interface level and
   requires your server to have at least four NICs. But using a separate bonded
   interface for each VLAN is not mandatory, and thus you can use a single
   bonded interface (or server with only two NICs) for Ceph deployment.
  </para>
  <para>
<!--<b>Procedure to configure Entry-scale-kvm-ceph-dual-network</b>-->
  </para>
  <para>
   Perform the following steps to configure
   <literal>Entry-scale-kvm-ceph-dual-network</literal>.
  </para>
  <procedure>
   <step>
    <para>
     Log in to the lifecycle manager.
    </para>
   </step>
   <step>
    <para>
     Copy the <literal>entry-scale-kvm-ceph</literal> input model to the
     <literal>~/helion/my_cloud/definition</literal> directory before you begin
     the editing process:
    </para>
<screen>cp -r ~/helion/examples/entry-scale-kvm-ceph/* ~/helion/my_cloud/definition/</screen>
   </step>
   <step>
    <para>
     Validate that NIC interfaces are correctly specified in
     <literal>nic_mapping.yml</literal> for servers that are used in the cloud.
     For Ceph OSD nodes, four port servers are required. You can use
     <emphasis role="bold">HP-DL360-4PORT</emphasis> as it is defined in
     <literal>nic_mapping.yml</literal> of
     <literal>entry-scale-kvm-ceph</literal> or define a new NIC mapping (as
     shown here) for new sets of servers having four port servers.
    </para>
<screen> - name: HP-4PORT-SERVER
      physical-ports:
        - logical-name: hed1
          type: simple-port
          bus-address: "0000:07:00.0"

        - logical-name: hed2
          type: simple-port
          bus-address: "0000:08:00.0"

        - logical-name: hed3
          type: simple-port
          bus-address: "0000:09:00.0"

        - logical-name: hed4
          type: simple-port
          bus-address: "0000:0a:00.0"</screen>
   </step>
   <step>
    <para>
     Modify OSD nodes to use four port servers, as shown here. Change the NIC
     mapping attribute from <emphasis role="bold">HP-DL360-4PORT</emphasis> to
     use any other name defined in <literal>nic_mapping.yml</literal>.
    </para>
<screen> # Ceph OSD Nodes
    - id: osd1
      ip-addr: 192.168.10.9
      role: OSD-ROLE
      server-group: RACK1
      nic-mapping: HP-DL360-4PORT
      mac-addr: "8b:f6:9e:ca:3b:78"
      ilo-ip: 192.168.9.9
      ilo-password: password
      ilo-user: admin</screen>
   </step>
   <step>
    <para>
     Delete OSD-CLIENT from <literal>network_groups.yml</literal>. Note that no
     dedicated network group exists for client traffic. Only the management
     network group is used for client traffic.
    </para>
   </step>
   <step>
    <para>
     Edit <literal>net_interfaces.yml</literal> with a bonded NIC as shown
     here.
    </para>
      <screen> - name: CONTROLLER-INTERFACES
      network-interfaces:
        - name: BOND0
          device:
              name: bond0
          bond-data:
              options:
                  mode: active-backup
                  miimon: 200
                  primary: hed1
              provider: linux
              devices:
                - name: hed1
                - name: hed2
          network-groups:
            - EXTERNAL-API
            - EXTERNAL-VM
            - GUEST
            - MANAGEMENT

    - name: COMPUTE-INTERFACES
      network-interfaces:
        - name: BOND0
          device:
              name: bond0
          bond-data:
              options:
                  mode: active-backup
                  miimon: 200
                  primary: hed1
              provider: linux
              devices:
                - name: hed1
                - name: hed2
          network-groups:
            - EXTERNAL-VM
            - GUEST
            - MANAGEMENT

    - name: OSD-INTERFACES
      network-interfaces:
        - name: BOND0
          device:
              name: bond0
          bond-data:
              options:
                  mode: active-backup
                  miimon: 200
                  primary: hed1
              provider: linux
              devices:
                - name: hed1
                - name: hed2
          network-groups:
            - MANAGEMENT
        - name: BOND1
          device:
              name: bond1
          bond-data:
              options:
                  mode: active-backup
                  miimon: 200
                  primary: hed3
              provider: linux
              devices:
                - name: hed3
                - name: hed4
          network-groups:
            - OSD-INTERNAL

    - name: RGW-INTERFACES
     network-interfaces:
        - name: BOND0
          device:
              name: bond0
          bond-data:
              options:
                  mode: active-backup
                  miimon: 200
                  primary: hed1
              provider: linux
              devices:
                - name: hed1
                - name: hed2
          network-groups:
            - MANAGEMENT</screen>
   </step>
   <step>
    <para>
     Delete OSD-CLIENT from <literal>server_groups.yml</literal>.
    </para>
   </step>
   <step>
    <para>
     Delete VLAN information for OSD-CLIENT-NET from
     <literal>networks.yml</literal>. Only management VLANs are used for client
     traffic.
    </para>
   </step>
   <step>
    <para>
     After you set up your configuration files, perform
     <!-- FIXME: steps <emphasis role="bold">8 to 13</emphasis> --> in
     <!-- FIXME: <link xlink:href="#config_ceph/deploying-monitor-on-standalone-node">Installing Monitor on Standalone Node</link> -->.
    </para>
   </step>
 </procedure>
 </section>
 <section>
  <title>RADOS Gateway</title>
  <para>
   <emphasis role="bold">Installing RADOS Gateway on Dedicated Cluster Nodes
   that Host the Ceph Monitor Service</emphasis>
  </para>
  <para>
   You can configure RADOS Gateway to install on one or more dedicated cluster
   nodes hosting the Ceph monitor service as follows:
  </para>
  <procedure>
   <step>
    <para>
     Remove the sections for servers in the
     <literal>~/helion/my_cloud/definition/data/servers.yml</literal> file that
     have the <literal>role: RGW-ROLE</literal> attribute.
    </para>
   </step>
   <step>
    <para>
     Edit the
     <literal>~/helion/my_cloud/definition/data/control_plane.yml</literal>
     file and add the following lines to the
     <literal>service-components</literal> section for the cluster nodes that
     have the <literal>server-role: MON-ROLE</literal> attribute.
    </para>
      <screen>- ceph-radosgw
      - apache2</screen>
   </step>
   <step>
    <para>
     Edit the
     <literal>~/helion/my_cloud/definition/data/net_interfaces.yml</literal>
     file to remove the RGW-INTERFACES section. This section defines RADOS
     Gateway network interfaces, which are not required in this configuration:
    </para>
      <screen> - name: RGW-INTERFACES
   network-interfaces:
     - name: BOND0
       device:
          name: bond0
       bond-data:
          options:
             mode: active-backup
             miimon: 200
             primary: hed3
          provider: linux
          devices:
             - name: hed3
             - name: hed4
       network-groups:
         - MANAGEMENT
         - OSD-CLIENT</screen>
   </step>
   <step>
    <para>
     After you set up your configuration files, perform
     <!-- FIXME: steps <emphasis role="bold">8 to 13</emphasis> --> in
     <!-- FIXME: <link xlink:href="#config_ceph/deploying-monitor-on-standalone-node">Deploying the Monitor on Standalone Nodes</link> -->.
    </para>
   </step>
 </procedure>
  <para>
   <emphasis role="bold">Installing RADOS Gateway on Controller
   Nodes</emphasis>
  </para>
  <para>
   You can configure RADOS Gateway to install on controller nodes. To do this,
   perform the following steps:
  </para>
  <procedure>
   <step>
    <para>
     Remove the sections for servers in the
     <literal>~/helion/my_cloud/definition/data/servers.yml</literal> file that
     have the <literal>role: RGW-ROLE</literal> attribute.
    </para>
   </step>
   <step>
    <para>
     Edit the
     <literal>~/helion/my_cloud/definition/data/net_interfaces.yml</literal>
     file to remove the RGW-INTERFACES section. This section defines RADOS
     Gateway network interfaces, which are not required in this configuration:
    </para>
     <screen> - name: RGW-INTERFACES
   network-interfaces:
     - name: BOND0
       device:
          name: bond0
       bond-data:
          options:
             mode: active-backup
             miimon: 200
             primary: hed3
          provider: linux
          devices:
             - name: hed3
             - name: hed4
       network-groups:
         - MANAGEMENT
         - OSD-CLIENT</screen>
   </step>
   <step>
    <para>
     Edit the
     <literal>~/helion/my_cloud/definition/data/control_plane.yml</literal>
     file and add the following line to <literal>service-components</literal>
     for the cluster with the <literal>server-role: CONTROLLER-ROLE</literal>
     attribute.
    </para>
<screen>- ceph-radosgw</screen>
   </step>
   <step>
    <para>
     After you set up your configuration files, perform
     <!-- FIXME: steps <emphasis role="bold">8 to 13</emphasis> --> in
     <!-- FIXME: <link xlink:href="#config_ceph/deploying-monitor-on-standalone-node">Deploying the Monitor Service on Standalone Nodes</link> -->.
    </para>
   </step>
 </procedure>
  <para>
   <emphasis role="bold">Installing More than Two RADOS Gateway
   Servers</emphasis>
  </para>
  <para>
   To deploy more than two RADOS Gateway servers, you need to add a section to
   the <literal>~/helion/my_cloud/definition/data/servers.yml</literal> file
   for each additional RADOS Gateway node.
  </para>
  <note>
   <para>
    Installing additional RADOS Gateway servers is possible only if RADOS
    Gateway is installed on dedicated cluster nodes or on dedicated cluster
    nodes that host the Ceph monitor service. Additional RADOS Gateway servers
    cannot be added if RADOS Gateway is installed on a controller node.
   </para>
  </note>
 </section>
 <section>
  <title>Ceph Deployment with Virtual Control Plane</title>
  <para>
   &kw-hos-tm; &kw-hos-version; supports the deployment of control plane
   elements on virtual machines which can be co-located on one baremetal
   machine or spread across three baremetal machines. The baremetal machine in
   this context is termed as VM factory host(s). The following aspects must be
   considered while deploying Ceph with the virtual control plane.
  </para>
  <orderedlist xml:id="ol_wg4_s25_mx">
   <listitem>
    <para>
     Deploy OSD and monitor node on a single VM factor host - It is applicable
     to X1 cloud model. The number of VM factory host is one. Therefore, Ceph
     cluster will not have HA support as there will be only one instance of the
     monitor component of Ceph.
    </para>
   </listitem>
   <listitem>
    <para>
     Deploy OSD and monitor on set of three VM factor hosts - It is applicable
     to S1 cloud model.
    </para>
   </listitem>
   <listitem>
    <para>
     Deploy monitor on set of three VM factor hosts but OSD nodes are deployed
     independently as a resource nodes - It is applicable to M1 cloud model.
    </para>
   </listitem>
  </orderedlist>
  <para>
   The following aspects must be considered while deploying Ceph with virtual
   control plane:
  </para>
  <orderedlist xml:id="ol_zgr_djn_kx">
   <listitem>
    <para>
     Scale-out of cluster - Adding a new set of monitor or OSD nodes is not
     validated by the engineering team. Although, technically it is feasible
     but not recommended because it can have a significant performance impact.
     However, one can increase a cluster capacity by adding more disks to the
     VM factory host and configuring them as OSD (a scale-in path to increase
     capacity) nodes.
    </para>
   </listitem>
   <listitem>
    <para>
     Deployment of RADOS Gateway on VM factory host is not formally supported.
    </para>
   </listitem>
   <listitem>
    <para>
     Performance of Ceph components (OSD and monitor nodes) are sensitive to
     compute resources i.e. memory, core CPU, and so on. It is strongly
     recommended to plan and allocate minimum amount of resources for Ceph
     components to avoid resource contention because same set of machines will
     be running the control plane elements and the Ceph components. Starvation
     of resources might causes impact on the performance and the stability of
     the Ceph clusters health. Consider the following resource aspect for
     planning:
    </para>
    <itemizedlist xml:id="ul_a3t_c3n_kx">
     <listitem>
      <para>
       CPU
      </para>
     </listitem>
     <listitem>
      <para>
       RAM
      </para>
     </listitem>
     <listitem>
      <para>
       Disk space for monitoring logs
      </para>
     </listitem>
    </itemizedlist>
    <para>
     The following section focus on the change of the input model for X1 and S1
     model ONLY. The change in the input model for M1 model is similar except
     that OSD node is deployed as the resource nodes. For other aspects of
     cluster management like upgrade, adding new set of disks, stopping and
     starting services and so on, you can follow the similar approach that is
     used for the deployment of Ceph cluster using &kw-hos-tm; .
    </para>
   </listitem>
  </orderedlist>
  <para>
   <emphasis role="bold">Steps to deploy Ceph on VM factory host(s)</emphasis>
  </para>
  <procedure>
   <step>
    <para>
     Log in to the lifecycle manager.
    </para>
   </step>
   <step>
    <para>
     Go to <literal>~/helion/my_cloud/definition/</literal>.
    </para>
    <note>
     <para>
      We assume that you have already copied the cloud model representing
      control element deployment on VM factory host(s).
     </para>
    </note>
   </step>
   <step>
    <para>
     Edit your <literal>control_plane.yml</literal> file to add
     <literal>ceph-osd</literal> and <literal>ceph-monitor</literal> components
     to the vmfactory nodes. For example:
    </para>
      <screen> - name: vmfactory
          resource-prefix: vmf
          server-role: HLM-HYPERVISOR-ROLE
          min-count:
          allocation-policy: strict
          service-components:
            - ntp-server
            - ceph-osd
            - ceph-monitor
            - lifecycle-manager
            - tempest
            - openstack-client</screen>
   </step>
   <step>
    <para>
     Edit <literal>disks_vmfactory.yml</literal> file of VM factory hosts to
     define data and journal disks for OSD. For example, the following disk
     model illustrates the usage of <literal>/dev/sdd</literal>,
     <literal>/dev/sde</literal>, and <literal>/dev/sdf</literal> as data disks
     and <literal>/dev/sdg</literal> as journal disks for OSD. Disks allocated
     to OSD must not be used for any other purpose.
    </para>
      <screen>---
   product:
     version: 2

   disk-models:
     - name: HLM-HYPERVISOR-DISKS
       volume-groups:
         - name: hlm-vg
        physical-volumes:
         - /dev/sda_root
        logical-volumes:
        # The policy is not to consume 100% of the space of each volume group.
        # 5% should be left free for snapshots and to allow for some flexibility.
          - name: root
            size: 35%
            fstype: ext4
            mount: /
          - name: log
            size: 50%
            mount: /var/log
            fstype: ext4
            mkfs-opts: -O large_file
          - name: crash
            size: 10%
            mount: /var/crash
            fstype: ext4
            mkfs-opts: -O large_file
      - name: vg-images
        # this VG is dedicated to libvirt images to keep VM IOPS off the OS disk
        physical-volumes:
          - /dev/sdb
          - /dev/sdc
        logical-volumes:
          - name: images
            size: 95%
            mount: /var/lib/libvirt/images
            fstype: ext4
            mkfs-opts: -O large_file

   device-groups:
      - name: ceph-osd-disks
        devices:
       - name: /dev/sdd
       - name: /dev/sde
       - name: /dev/sdf
        consumer:
           name: ceph
           attrs:
             usage: data
             journal_disk: /dev/sdg</screen>
   </step>
   <step>
    <para>
     Commit your configuration to the local git repo.
    </para>
      <screen>cd ~/helion/hos/ansible
      git add -A
      git commit -m "My config or other commit message"</screen>
   </step>
   <step>
    <para>
     After setting up the configuration files, continue with the installation
     procedure mentioned at
     <!-- FIXME: <xref href="../installing_kvm.xml#install_kvm/config_processor"/> -->.
    </para>
	  <substeps>
	  <step>
       <para>
        Running <literal>ceph-stop.yml</literal> and
        <literal>ceph-start.yml</literal> playbooks on the VM factory host
        stops all the services (OSD and monitor) on the node. There is no
        playbook that can stop only one service.
       </para>
      </step>
	 <step>
       <para>
        If you are deploying Ceph on a single VM factory host (i.e. X1 model),
        make the following changes before deployment.
       </para>
        <substeps>
		<step>
         <para>
          Edit <literal>~/helion/my_cloud/config/ceph/settings.yml</literal>
          file to add the following content.
         </para>
<screen>extra:
     global:
           osd_crush_chooseleaf_type: 0</screen>
        </step>
	   <step>
         <para>
          Commit your configuration to the local git repo and continue with the
          installation procedure mentioned at
          <!-- FIXME: <xref href="../installing_kvm.xml#install_kvm/config_processor"/> -->.
         </para>
        </step>
        </substeps>
	  </step>
	 </substeps>
   </step>
 </procedure>
 </section>
</section>
