<?xml version="1.0" encoding="UTF-8" standalone="no"?>
<chapter xmlns="http://docbook.org/ns/docbook" xmlns:xlink="http://www.w3.org/1999/xlink" xmlns:xi="http://www.w3.org/2001/XInclude" version="5.1">
  <title>Compute</title>
  <info>
   <abstract>
    <para>The OpenStack Compute service allows you to control an
            Infrastructure-as-a-Service (IaaS) cloud computing platform.  It gives you
            control over instances and networks, and allows you to manage access to the
            cloud through users and projects.</para>
  <para>Compute does not include virtualization software. Instead, it defines drivers
            that interact with underlying virtualization mechanisms that run on your host
            operating system, and exposes functionality over a web-based API.</para>
   </abstract>
  </info>
  <chapter xml:id="injecting-the-administrator-password" xml:base="admin-password-injection">
    <title>Injecting the administrator password</title>
    <para>Compute can generate a random administrator (root) password and inject that
            password into an instance. If this feature is enabled, users can run
            <command>ssh</command> to an instance without an <command>ssh</command> keypair.  The random
            password appears in the output of the <command>openstack server create</command>
            command.  You can also view and set the admin password from the dashboard.</para>
    <para>By default, the dashboard will display the <literal>admin</literal> password and allow the
            user to modify it.</para>
    <para>If you do not want to support password injection, disable the password fields
            by editing the dashboard's <literal>local_settings.py</literal> file.</para>
    <screen language="none">OPENSTACK_HYPERVISOR_FEATURES = {
...
    'can_set_password': False,
}</screen>
    <para>For hypervisors that use the libvirt back end (such as KVM, QEMU, and LXC),
            admin password injection is disabled by default. To enable it, set this option
            in <literal>/etc/nova/nova.conf</literal>:</para>
    <screen language="ini">[libvirt]
inject_password=true</screen>
    <para>When enabled, Compute will modify the password of the admin account by editing
            the <literal>/etc/shadow</literal> file inside the virtual machine instance.</para>
    <note>
      <para>Users can only use <command>ssh</command> to access the instance by using the admin
                password if the virtual machine image is a Linux distribution, and it has
                been configured to allow users to use <command>ssh</command> as the root user. This
                is not the case for <link xmlns:xl="http://www.w3.org/1999/xlink" xl:href="http://uec-images.ubuntu.com">Ubuntu cloud images</link>
                which, by default, does not allow users to use <command>ssh</command> to access the
                root account.</para>
    </note>
    <para>When using the XenAPI hypervisor back end, Compute uses the XenAPI agent to
            inject passwords into guests. The virtual machine image must be configured with
            the agent for password injection to work.</para>
    <para>For Windows virtual machines, configure the Windows image to retrieve the admin
            password on boot by installing an agent such as <link xmlns:xl="http://www.w3.org/1999/xlink" xl:href="https://cloudbase.it/cloudbase-init">cloudbase-init</link>.</para>
  </chapter>
  <chapter xml:id="advanced-configuration" xml:base="adv-config">
    <title>Advanced configuration</title>
    <para>OpenStack clouds run on platforms that differ greatly in the capabilities that
            they provide. By default, the Compute service seeks to abstract the underlying
            hardware that it runs on, rather than exposing specifics about the underlying
            host platforms. This abstraction manifests itself in many ways. For example,
            rather than exposing the types and topologies of CPUs running on hosts, the
            service exposes a number of generic CPUs (virtual CPUs, or vCPUs) and allows
            for overcommitting of these. In a similar manner, rather than exposing the
            individual types of network devices available on hosts, generic
            software-powered network ports are provided. These features are designed to
            allow high resource utilization and allows the service to provide a generic
            cost-effective and highly scalable cloud upon which to build applications.</para>
    <para>This abstraction is beneficial for most workloads. However, there are some
            workloads where determinism and per-instance performance are important, if not
            vital. In these cases, instances can be expected to deliver near-native
            performance. The Compute service provides features to improve individual
            instance for these kind of workloads.</para>
    <section xml:id="attaching-physical-pci-devices-to-guests" xml:base="pci-passthrough">
      <title>Attaching physical PCI devices to guests</title>
      <para>The PCI passthrough feature in OpenStack allows full access and direct control
            of a physical PCI device in guests. This mechanism is generic for any kind of
            PCI device, and runs with a Network Interface Card (NIC), Graphics Processing
            Unit (GPU), or any other devices that can be attached to a PCI bus. Correct
            driver installation is the only requirement for the guest to properly use the
            devices.</para>
      <para>Some PCI devices provide Single Root I/O Virtualization and Sharing (SR-IOV)
            capabilities. When SR-IOV is used, a physical device is virtualized and appears
            as multiple PCI devices. Virtual PCI devices are assigned to the same or
            different guests. In the case of PCI passthrough, the full physical device is
            assigned to only one guest and cannot be shared.</para>
      <note>
        <para>For information on attaching virtual SR-IOV devices to guests, refer to the
                <link xmlns:xl="http://www.w3.org/1999/xlink" xl:href="https://docs.openstack.org/neutron/latest/admin/config-sriov.html">Networking Guide</link>.</para>
      </note>
      <para>To enable PCI passthrough, follow the steps below:</para>
      <procedure>
        <step>
          <para>Configure nova-scheduler (Controller)</para>
        </step>
        <step>
          <para>Configure nova-api (Controller)**</para>
        </step>
        <step>
          <para>Configure a flavor (Controller)</para>
        </step>
        <step>
          <para>Enable PCI passthrough (Compute)</para>
        </step>
        <step>
          <para>Configure PCI devices in nova-compute (Compute)</para>
        </step>
      </procedure>
      <note>
        <para>The PCI device with address <literal>0000:41:00.0</literal> is used as an example. This
                will differ between environments.</para>
      </note>
      <section xml:id="configure-nova-scheduler-controller">
        <title>Configure nova-scheduler (Controller)</title>
        <procedure>
          <step>
            <para>Configure <literal>nova-scheduler</literal> as specified in <link xmlns:xl="http://www.w3.org/1999/xlink" xl:href="https://docs.openstack.org/neutron/latest/admin/config-sriov.html#configure-nova-scheduler-controller">Configure nova-scheduler</link>.</para>
          </step>
          <step>
            <para>Restart the <literal>nova-scheduler</literal> service.</para>
          </step>
        </procedure>
      </section>
      <section xml:id="configure-nova-api-controller">
        <title>Configure nova-api (Controller)</title>
        <procedure>
          <step>
            <para>Specify the PCI alias for the device.</para>
            <para>Configure a PCI alias <literal>a1</literal> to request a PCI device with a <literal>vendor_id</literal> of
                        <literal>0x8086</literal> and a <literal>product_id</literal> of <literal>0x154d</literal>. The <literal>vendor_id</literal> and
                        <literal>product_id</literal> correspond the PCI device with address <literal>0000:41:00.0</literal>.</para>
            <para>Edit <literal>/etc/nova/nova.conf</literal>:</para>
            <screen language="ini">[pci]
alias = { "vendor_id":"8086", "product_id":"154d", "device_type":"type-PF", "name":"a1" }</screen>
            <para>For more information about the syntax of <literal>alias</literal>, refer to
                        <xref linkend="../configuration/config"/>.</para>
          </step>
          <step>
            <para>Restart the <literal>nova-api</literal> service.</para>
          </step>
        </procedure>
      </section>
      <section xml:id="configure-a-flavor-controller">
        <title>Configure a flavor (Controller)</title>
        <para>Configure a flavor to request two PCI devices, each with <literal>vendor_id</literal> of
                <literal>0x8086</literal> and <literal>product_id</literal> of <literal>0x154d</literal>:</para>
        <screen language="console"># openstack flavor set m1.large --property "pci_passthrough:alias"="a1:2"</screen>
        <para>For more information about the syntax for <literal>pci_passthrough:alias</literal>, refer to
                <xref linkend="flavors"/>.</para>
      </section>
      <section xml:id="enable-pci-passthrough-compute">
        <title>Enable PCI passthrough (Compute)</title>
        <para>Enable VT-d and IOMMU. For more information, refer to steps one and two in
                <link xmlns:xl="http://www.w3.org/1999/xlink" xl:href="https://docs.openstack.org/neutron/latest/admin/config-sriov.html#create-virtual-functions-compute">Create Virtual Functions</link>.</para>
      </section>
      <section xml:id="configure-pci-devices-compute">
        <title>Configure PCI devices (Compute)</title>
        <procedure>
          <step>
            <para>Configure <literal>nova-compute</literal> to allow the PCI device to pass through to
                        VMs. Edit <literal>/etc/nova/nova.conf</literal>:</para>
            <screen language="ini">[pci]
passthrough_whitelist = { "address": "0000:41:00.0" }</screen>
            <para>Alternatively specify multiple PCI devices using whitelisting:</para>
            <screen language="ini">[pci]
passthrough_whitelist = { "vendor_id": "8086", "product_id": "10fb" }</screen>
            <para>All PCI devices matching the <literal>vendor_id</literal> and <literal>product_id</literal> are added to
                        the pool of PCI devices available for passthrough to VMs.</para>
            <para>For more information about the syntax of <literal>passthrough_whitelist</literal>,
                        refer to <xref linkend="../configuration/config"/>.</para>
          </step>
          <step>
            <para>Specify the PCI alias for the device.</para>
            <para>From the Newton release, to resize guest with PCI device, configure the PCI
                        alias on the compute node as well.</para>
            <para>Configure a PCI alias <literal>a1</literal> to request a PCI device with a <literal>vendor_id</literal> of
                        <literal>0x8086</literal> and a <literal>product_id</literal> of <literal>0x154d</literal>. The <literal>vendor_id</literal> and
                        <literal>product_id</literal> correspond the PCI device with address <literal>0000:41:00.0</literal>.</para>
            <para>Edit <literal>/etc/nova/nova.conf</literal>:</para>
            <screen language="ini">[pci]
alias = { "vendor_id":"8086", "product_id":"154d", "device_type":"type-PF", "name":"a1" }</screen>
            <para>For more information about the syntax of <literal>alias</literal>, refer to <xref linkend="../configuration/config"/>.</para>
          </step>
          <step>
            <para>Restart the <literal>nova-compute</literal> service.</para>
          </step>
        </procedure>
      </section>
      <section xml:id="create-instances-with-pci-passthrough-devices">
        <title>Create instances with PCI passthrough devices</title>
        <para>The <literal>nova-scheduler</literal> selects a destination host that has PCI devices
                available with the specified <literal>vendor_id</literal> and <literal>product_id</literal> that matches the
                <literal>alias</literal> from the flavor.</para>
        <screen language="console"># openstack server create --flavor m1.large --image cirros-0.3.5-x86_64-uec --wait test-pci</screen>
      </section>
    </section>
    <section xml:id="cpu-topologies" xml:base="cpu-topologies">
      <title>CPU topologies</title>
      <para>The NUMA topology and CPU pinning features in OpenStack provide high-level
            control over how instances run on hypervisor CPUs and the topology of virtual
            CPUs available to instances. These features help minimize latency and maximize
            performance.</para>
      <section xml:id="smp-numa-and-smt">
        <title>SMP, NUMA, and SMT</title>
        <variablelist>
          <varlistentry>
            <term>Symmetric multiprocessing (SMP)</term>
            <listitem>
              <para>SMP is a design found in many modern multi-core systems. In an SMP system,
                            there are two or more CPUs and these CPUs are connected by some interconnect.
                            This provides CPUs with equal access to system resources like memory and
                            input/output ports.</para>
            </listitem>
          </varlistentry>
          <varlistentry>
            <term>Non-uniform memory access (NUMA)</term>
            <listitem>
              <para>NUMA is a derivative of the SMP design that is found in many multi-socket
                            systems. In a NUMA system, system memory is divided into cells or nodes that
                            are associated with particular CPUs. Requests for memory on other nodes are
                            possible through an interconnect bus. However, bandwidth across this shared
                            bus is limited. As a result, competition for this resource can incur
                            performance penalties.</para>
            </listitem>
          </varlistentry>
          <varlistentry>
            <term>Simultaneous Multi-Threading (SMT)</term>
            <listitem>
              <para>SMT is a design complementary to SMP. Whereas CPUs in SMP systems share a bus
                            and some memory, CPUs in SMT systems share many more components. CPUs that
                            share components are known as thread siblings.  All CPUs appear as usable
                            CPUs on the system and can execute workloads in parallel. However, as with
                            NUMA, threads compete for shared resources.</para>
            </listitem>
          </varlistentry>
        </variablelist>
        <para>In OpenStack, SMP CPUs are known as <emphasis>cores</emphasis>, NUMA cells or nodes are known as
                <emphasis>sockets</emphasis>, and SMT CPUs are known as <emphasis>threads</emphasis>. For example, a quad-socket,
                eight core system with Hyper-Threading would have four sockets, eight cores per
                socket and two threads per core, for a total of 64 CPUs.</para>
      </section>
      <section xml:id="configuring-compute-nodes-for-instances-with-numa-placement-policies">
        <title>Configuring compute nodes for instances with NUMA placement policies</title>
        <para>Hyper-V is configured by default to allow instances to span multiple NUMA
                nodes, regardless if the instances have been configured to only span N NUMA
                nodes. This behaviour allows Hyper-V instances to have up to 64 vCPUs and 1 TB
                of memory.</para>
        <para>Checking NUMA spanning can easily be done by running this following powershell
                command:</para>
        <screen language="console">(Get-VMHost).NumaSpanningEnabled</screen>
        <para>In order to disable this behaviour, the host will have to be configured to
                disable NUMA spanning. This can be done by executing these following
                powershell commands:</para>
        <screen language="console">Set-VMHost -NumaSpanningEnabled $false
Restart-Service vmms</screen>
        <para>In order to restore this behaviour, execute these powershell commands:</para>
        <screen language="console">Set-VMHost -NumaSpanningEnabled $true
Restart-Service vmms</screen>
        <para>The <literal>vmms</literal> service (Virtual Machine Management Service) is responsible for
                managing the Hyper-V VMs. The VMs will still run while the service is down
                or restarting, but they will not be manageable by the <literal>nova-compute</literal>
                service. In order for the effects of the Host NUMA spanning configuration
                to take effect, the VMs will have to be restarted.</para>
        <para>Hyper-V does not allow instances with a NUMA topology to have dynamic
                memory allocation turned on. The Hyper-V driver will ignore the configured
                <literal>dynamic_memory_ratio</literal> from the given <literal>nova.conf</literal> file when spawning
                instances with a NUMA topology.</para>
      </section>
      <section xml:id="customizing-instance-numa-placement-policies">
        <title>Customizing instance NUMA placement policies</title>
        <important>
          <para>The functionality described below is currently only supported by the
                    libvirt/KVM and Hyper-V driver.</para>
        </important>
        <para>When running workloads on NUMA hosts, it is important that the vCPUs executing
                processes are on the same NUMA node as the memory used by these processes.
                This ensures all memory accesses are local to the node and thus do not consume
                the limited cross-node memory bandwidth, adding latency to memory accesses.
                Similarly, large pages are assigned from memory and benefit from the same
                performance improvements as memory allocated using standard pages. Thus, they
                also should be local. Finally, PCI devices are directly associated with
                specific NUMA nodes for the purposes of DMA. Instances that use PCI or SR-IOV
                devices should be placed on the NUMA node associated with these devices.</para>
        <para>By default, an instance floats across all NUMA nodes on a host. NUMA awareness
                can be enabled implicitly through the use of huge pages or pinned CPUs or
                explicitly through the use of flavor extra specs or image metadata.  In all
                cases, the <literal>NUMATopologyFilter</literal> filter must be enabled. Details on this
                filter are provided in <xref linkend="configuration/schedulers"/> in Nova
                configuration guide.</para>
        <important>
          <para>The NUMA node(s) used are normally chosen at random. However, if a PCI
                    passthrough or SR-IOV device is attached to the instance, then the NUMA
                    node that the device is associated with will be used. This can provide
                    important performance improvements. However, booting a large number of
                    similar instances can result in unbalanced NUMA node usage. Care should
                    be taken to mitigate this issue. See this <link xmlns:xl="http://www.w3.org/1999/xlink" xl:href="http://lists.openstack.org/pipermail/openstack-dev/2016-March/090367.html">discussion</link> for more details.</para>
        </important>
        <important>
          <para>Inadequate per-node resources will result in scheduling failures. Resources
                    that are specific to a node include not only CPUs and memory, but also PCI
                    and SR-IOV resources. It is not possible to use multiple resources from
                    different nodes without requesting a multi-node layout. As such, it may be
                    necessary to ensure PCI or SR-IOV resources are associated with the same
                    NUMA node or force a multi-node layout.</para>
        </important>
        <para>When used, NUMA awareness allows the operating system of the instance to
                intelligently schedule the workloads that it runs and minimize cross-node
                memory bandwidth. To restrict an instance's vCPUs to a single host NUMA node,
                run:</para>
        <screen language="console">$ openstack flavor set m1.large --property hw:numa_nodes=1</screen>
        <para>Some workloads have very demanding requirements for memory access latency or
                bandwidth that exceed the memory bandwidth available from a single NUMA node.
                For such workloads, it is beneficial to spread the instance across multiple
                host NUMA nodes, even if the instance's RAM/vCPUs could theoretically fit on a
                single NUMA node. To force an instance's vCPUs to spread across two host NUMA
                nodes, run:</para>
        <screen language="console">$ openstack flavor set m1.large --property hw:numa_nodes=2</screen>
        <para>The allocation of instances vCPUs and memory from different host NUMA nodes can
                be configured. This allows for asymmetric allocation of vCPUs and memory, which
                can be important for some workloads. To spread the 6 vCPUs and 6 GB of memory
                of an instance across two NUMA nodes and create an asymmetric 1:2 vCPU and
                memory mapping between the two nodes, run:</para>
        <screen language="console">$ openstack flavor set m1.large --property hw:numa_nodes=2
$ openstack flavor set m1.large \  # configure guest node 0
  --property hw:numa_cpus.0=0,1 \
  --property hw:numa_mem.0=2048
$ openstack flavor set m1.large \  # configure guest node 1
  --property hw:numa_cpus.1=2,3,4,5 \
  --property hw:numa_mem.1=4096</screen>
        <note>
          <para>Hyper-V does not support asymmetric NUMA topologies, and the Hyper-V
                    driver will not spawn instances with such topologies.</para>
        </note>
        <para>For more information about the syntax for <literal>hw:numa_nodes</literal>, <literal>hw:numa_cpus.N</literal>
                and <literal>hw:num_mem.N</literal>, refer to the <xref linkend="extra-specs-numa-topology"/> guide.</para>
      </section>
      <section xml:id="customizing-instance-cpu-pinning-policies">
        <title>Customizing instance CPU pinning policies</title>
        <important>
          <para>The functionality described below is currently only supported by the
                    libvirt/KVM driver. Hyper-V does not support CPU pinning.</para>
        </important>
        <para>By default, instance vCPU processes are not assigned to any particular host
                CPU, instead, they float across host CPUs like any other process. This allows
                for features like overcommitting of CPUs. In heavily contended systems, this
                provides optimal system performance at the expense of performance and latency
                for individual instances.</para>
        <para>Some workloads require real-time or near real-time behavior, which is not
                possible with the latency introduced by the default CPU policy. For such
                workloads, it is beneficial to control which host CPUs are bound to an
                instance's vCPUs. This process is known as pinning. No instance with pinned
                CPUs can use the CPUs of another pinned instance, thus preventing resource
                contention between instances. To configure a flavor to use pinned vCPUs, a
                use a dedicated CPU policy. To force this, run:</para>
        <screen language="console">$ openstack flavor set m1.large --property hw:cpu_policy=dedicated</screen>
        <important>
          <para>Host aggregates should be used to separate pinned instances from unpinned
                    instances as the latter will not respect the resourcing requirements of
                    the former.</para>
        </important>
        <para>When running workloads on SMT hosts, it is important to be aware of the impact
                that thread siblings can have. Thread siblings share a number of components
                and contention on these components can impact performance. To configure how
                to use threads, a CPU thread policy should be specified. For workloads where
                sharing benefits performance, use thread siblings. To force this, run:</para>
        <screen language="console">$ openstack flavor set m1.large \
  --property hw:cpu_policy=dedicated \
  --property hw:cpu_thread_policy=require</screen>
        <para>For other workloads where performance is impacted by contention for resources,
                use non-thread siblings or non-SMT hosts. To force this, run:</para>
        <screen language="console">$ openstack flavor set m1.large \
  --property hw:cpu_policy=dedicated \
  --property hw:cpu_thread_policy=isolate</screen>
        <para>Finally, for workloads where performance is minimally impacted, use thread
                siblings if available. This is the default, but it can be set explicitly:</para>
        <screen language="console">$ openstack flavor set m1.large \
  --property hw:cpu_policy=dedicated \
  --property hw:cpu_thread_policy=prefer</screen>
        <para>For more information about the syntax for <literal>hw:cpu_policy</literal> and
                <literal>hw:cpu_thread_policy</literal>, refer to the <xref linkend="flavors"/> guide.</para>
        <para>Applications are frequently packaged as images. For applications that require
                real-time or near real-time behavior, configure image metadata to ensure
                created instances are always pinned regardless of flavor. To configure an
                image to use pinned vCPUs and avoid thread siblings, run:</para>
        <screen language="console">$ openstack image set [IMAGE_ID] \
  --property hw_cpu_policy=dedicated \
  --property hw_cpu_thread_policy=isolate</screen>
        <para>If the flavor specifies a CPU policy of <literal>dedicated</literal> then that policy will be
                used. If the flavor explicitly specifies a CPU policy of <literal>shared</literal> and the
                image specifies no policy or a policy of <literal>shared</literal> then the <literal>shared</literal> policy
                will be used, but if the image specifies a policy of <literal>dedicated</literal> an exception
                will be raised. By setting a <literal>shared</literal> policy through flavor extra-specs,
                administrators can prevent users configuring CPU policies in images and
                impacting resource utilization. To configure this policy, run:</para>
        <screen language="console">$ openstack flavor set m1.large --property hw:cpu_policy=shared</screen>
        <para>If the flavor does not specify a CPU thread policy then the CPU thread policy
                specified by the image (if any) will be used. If both the flavor and image
                specify a CPU thread policy then they must specify the same policy, otherwise
                an exception will be raised.</para>
        <note>
          <para>There is no correlation required between the NUMA topology exposed in the
                    instance and how the instance is actually pinned on the host. This is by
                    design. See this <link xmlns:xl="http://www.w3.org/1999/xlink" xl:href="https://bugs.launchpad.net/nova/+bug/1466780">invalid bug</link> for more information.</para>
        </note>
        <para>For more information about image metadata, refer to the <link xmlns:xl="http://www.w3.org/1999/xlink" xl:href="https://docs.openstack.org/image-guide/image-metadata.html">Image metadata</link>
                guide.</para>
      </section>
      <section xml:id="customizing-instance-cpu-topologies">
        <title>Customizing instance CPU topologies</title>
        <important>
          <para>The functionality described below is currently only supported by the
                    libvirt/KVM driver.</para>
        </important>
        <para>In addition to configuring how an instance is scheduled on host CPUs, it is
                possible to configure how CPUs are represented in the instance itself. By
                default, when instance NUMA placement is not specified, a topology of N
                sockets, each with one core and one thread, is used for an instance, where N
                corresponds to the number of instance vCPUs requested. When instance NUMA
                placement is specified, the number of sockets is fixed to the number of host
                NUMA nodes to use and the total number of instance CPUs is split over these
                sockets.</para>
        <para>Some workloads benefit from a custom topology. For example, in some operating
                systems, a different license may be needed depending on the number of CPU
                sockets. To configure a flavor to use a maximum of two sockets, run:</para>
        <screen language="console">$ openstack flavor set m1.large --property hw:cpu_sockets=2</screen>
        <para>Similarly, to configure a flavor to use one core and one thread, run:</para>
        <screen language="console">$ openstack flavor set m1.large \
  --property hw:cpu_cores=1 \
  --property hw:cpu_threads=1</screen>
        <important>
          <para>If specifying all values, the product of sockets multiplied by cores
                    multiplied by threads must equal the number of instance vCPUs. If specifying
                    any one of these values or the multiple of two values, the values must be a
                    factor of the number of instance vCPUs to prevent an exception. For example,
                    specifying <literal>hw:cpu_sockets=2</literal> on a host with an odd number of cores fails.
                    Similarly, specifying <literal>hw:cpu_cores=2</literal> and <literal>hw:cpu_threads=4</literal> on a host
                    with ten cores fails.</para>
        </important>
        <para>For more information about the syntax for <literal>hw:cpu_sockets</literal>, <literal>hw:cpu_cores</literal>
                and <literal>hw:cpu_threads</literal>, refer to the <xref linkend="flavors"/> guide.</para>
        <para>It is also possible to set upper limits on the number of sockets, cores, and
                threads used. Unlike the hard values above, it is not necessary for this exact
                number to used because it only provides a limit. This can be used to provide
                some flexibility in scheduling, while ensuring certains limits are not
                exceeded. For example, to ensure no more than two sockets are defined in the
                instance topology, run:</para>
        <screen language="console">$ openstack flavor set m1.large --property=hw:cpu_max_sockets=2</screen>
        <para>For more information about the syntax for <literal>hw:cpu_max_sockets</literal>,
                <literal>hw:cpu_max_cores</literal>, and <literal>hw:cpu_max_threads</literal>, refer to the
                <xref linkend="flavors"/> guide.</para>
        <para>Applications are frequently packaged as images. For applications that prefer
                certain CPU topologies, configure image metadata to hint that created instances
                should have a given topology regardless of flavor. To configure an image to
                request a two-socket, four-core per socket topology, run:</para>
        <screen language="console">$ openstack image set [IMAGE_ID] \
  --property hw_cpu_sockets=2 \
  --property hw_cpu_cores=4</screen>
        <para>To constrain instances to a given limit of sockets, cores or threads, use the
                <literal>max_</literal> variants. To configure an image to have a maximum of two sockets and a
                maximum of one thread, run:</para>
        <screen language="console">$ openstack image set [IMAGE_ID] \
  --property hw_cpu_max_sockets=2 \
  --property hw_cpu_max_threads=1</screen>
        <para>The value specified in the flavor is treated as the abolute limit.  The image
                limits are not permitted to exceed the flavor limits, they can only be equal
                to or lower than what the flavor defines. By setting a <literal>max</literal> value for
                sockets, cores, or threads, administrators can prevent users configuring
                topologies that might, for example, incur an additional licensing fees.</para>
        <para>For more information about image metadata, refer to the <link xmlns:xl="http://www.w3.org/1999/xlink" xl:href="https://docs.openstack.org/image-guide/image-metadata.html">Image metadata</link>
                guide.</para>
      </section>
    </section>
    <section xml:id="huge-pages" xml:base="huge-pages">
      <title>Huge pages</title>
      <para>The huge page feature in OpenStack provides important performance improvements
            for applications that are highly memory IO-bound.</para>
      <note>
        <para>Huge pages may also be referred to hugepages or large pages, depending on
                the source. These terms are synonyms.</para>
      </note>
      <section xml:id="pages-the-tlb-and-huge-pages">
        <title>Pages, the TLB and huge pages</title>
        <variablelist>
          <varlistentry>
            <term>Pages</term>
            <listitem>
              <para>Physical memory is segmented into a series of contiguous regions called
                            pages. Each page contains a number of bytes, referred to as the page size.
                            The system retrieves memory by accessing entire pages, rather than byte by
                            byte.</para>
            </listitem>
          </varlistentry>
          <varlistentry>
            <term>Translation Lookaside Buffer (TLB)</term>
            <listitem>
              <para>A TLB is used to map the virtual addresses of pages to the physical addresses
                            in actual memory. The TLB is a cache and is not limitless, storing only the
                            most recent or frequently accessed pages. During normal operation, processes
                            will sometimes attempt to retrieve pages that are not stored in the cache.
                            This is known as a TLB miss and results in a delay as the processor iterates
                            through the pages themselves to find the missing address mapping.</para>
            </listitem>
          </varlistentry>
          <varlistentry>
            <term>Huge Pages</term>
            <listitem>
              <para>The standard page size in x86 systems is 4 kB. This is optimal for general
                            purpose computing but larger page sizes - 2 MB and 1 GB - are also available.
                            These larger page sizes are known as huge pages. Huge pages result in less
                            efficient memory usage as a process will not generally use all memory
                            available in each page. However, use of huge pages will result in fewer
                            overall pages and a reduced risk of TLB misses. For processes that have
                            significant memory requirements or are memory intensive, the benefits of huge
                            pages frequently outweigh the drawbacks.</para>
            </listitem>
          </varlistentry>
          <varlistentry>
            <term>Persistent Huge Pages</term>
            <listitem>
              <para>On Linux hosts, persistent huge pages are huge pages that are reserved
                            upfront. The HugeTLB provides for the mechanism for this upfront
                            configuration of huge pages. The HugeTLB allows for the allocation of varying
                            quantities of different huge page sizes. Allocation can be made at boot time
                            or run time. Refer to the <link xmlns:xl="http://www.w3.org/1999/xlink" xl:href="https://www.kernel.org/doc/Documentation/vm/hugetlbpage.txt">Linux hugetlbfs guide</link> for more information.</para>
            </listitem>
          </varlistentry>
          <varlistentry>
            <term>Transparent Huge Pages (THP)</term>
            <listitem>
              <para>On Linux hosts, transparent huge pages are huge pages that are automatically
                            provisioned based on process requests. Transparent huge pages are provisioned
                            on a best effort basis, attempting to provision 2 MB huge pages if available
                            but falling back to 4 kB small pages if not. However, no upfront
                            configuration is necessary. Refer to the <link xmlns:xl="http://www.w3.org/1999/xlink" xl:href="https://www.kernel.org/doc/Documentation/vm/transhuge.txt">Linux THP guide</link> for more
                            information.</para>
            </listitem>
          </varlistentry>
        </variablelist>
      </section>
      <section xml:id="enabling-huge-pages-on-the-host">
        <title>Enabling huge pages on the host</title>
        <para>Persistent huge pages are required owing to their guaranteed availability.
                However, persistent huge pages are not enabled by default in most environments.
                The steps for enabling huge pages differ from platform to platform and only the
                steps for Linux hosts are described here. On Linux hosts, the number of
                persistent huge pages on the host can be queried by checking <literal>/proc/meminfo</literal>:</para>
        <screen language="console">$ grep Huge /proc/meminfo
AnonHugePages:         0 kB
ShmemHugePages:        0 kB
HugePages_Total:       0
HugePages_Free:        0
HugePages_Rsvd:        0
HugePages_Surp:        0
Hugepagesize:       2048 kB</screen>
        <para>In this instance, there are 0 persistent huge pages (<literal>HugePages_Total</literal>) and 0
                transparent huge pages (<literal>AnonHugePages</literal>) allocated. Huge pages can be
                allocated at boot time or run time. Huge pages require a contiguous area of
                memory - memory that gets increasingly fragmented the long a host is running.
                Identifying contiguous areas of memory is a issue for all huge page sizes, but
                it is particularly problematic for larger huge page sizes such as 1 GB huge
                pages. Allocating huge pages at boot time will ensure the correct number of huge
                pages is always available, while allocating them at run time can fail if memory
                has become too fragmented.</para>
        <para>To allocate huge pages at run time, the kernel boot parameters must be extended
                to include some huge page-specific parameters. This can be achieved by
                modifying <literal>/etc/default/grub</literal> and appending the <literal>hugepagesz</literal>,
                <literal>hugepages</literal>, and <literal>transparent_hugepages=never</literal> arguments to
                <literal>GRUB_CMDLINE_LINUX</literal>. To allocate, for example, 2048 persistent 2 MB huge
                pages at boot time, run:</para>
        <screen language="console"># echo 'GRUB_CMDLINE_LINUX="$GRUB_CMDLINE_LINUX hugepagesz=2M hugepages=2048 transparent_hugepage=never"' &gt; /etc/default/grub
$ grep GRUB_CMDLINE_LINUX /etc/default/grub
GRUB_CMDLINE_LINUX="..."
GRUB_CMDLINE_LINUX="$GRUB_CMDLINE_LINUX hugepagesz=2M hugepages=2048 transparent_hugepage=never"</screen>
        <important>
          <para>Persistent huge pages are not usable by standard host OS processes. Ensure
                    enough free, non-huge page memory is reserved for these processes.</para>
        </important>
        <para>Reboot the host, then validate that huge pages are now available:</para>
        <screen language="console">$ grep "Huge" /proc/meminfo
AnonHugePages:         0 kB
ShmemHugePages:        0 kB
HugePages_Total:    2048
HugePages_Free:     2048
HugePages_Rsvd:        0
HugePages_Surp:        0
Hugepagesize:       2048 kB</screen>
        <para>There are now 2048 2 MB huge pages totalling 4 GB of huge pages. These huge
                pages must be mounted. On most platforms, this happens automatically. To verify
                that the huge pages are mounted, run:</para>
        <screen language="console"># mount | grep huge
hugetlbfs on /dev/hugepages type hugetlbfs (rw)</screen>
        <para>In this instance, the huge pages are mounted at <literal>/dev/hugepages</literal>. This mount
                point varies from platform to platform. If the above command did not return
                anything, the hugepages must be mounted manually. To mount the huge pages at
                <literal>/dev/hugepages</literal>, run:</para>
        <screen language="console"># mkdir -p /dev/hugepages
# mount -t hugetlbfs hugetlbfs /dev/hugepages</screen>
        <para>There are many more ways to configure huge pages, including allocating huge
                pages at run time, specifying varying allocations for different huge page
                sizes, or allocating huge pages from memory affinitized to different NUMA
                nodes. For more information on configuring huge pages on Linux hosts, refer to
                the <link xmlns:xl="http://www.w3.org/1999/xlink" xl:href="https://www.kernel.org/doc/Documentation/vm/hugetlbpage.txt">Linux hugetlbfs guide</link>.</para>
      </section>
      <section xml:id="customizing-instance-huge-pages-allocations">
        <title>Customizing instance huge pages allocations</title>
        <important>
          <para>The functionality described below is currently only supported by the
                    libvirt/KVM driver.</para>
        </important>
        <important>
          <para>For performance reasons, configuring huge pages for an instance will
                    implicitly result in a NUMA topology being configured for the instance.
                    Configuring a NUMA topology for an instance requires enablement of
                    <literal>NUMATopologyFilter</literal>. Refer to <xref linkend="cpu-topologies"/> for more
                    information.</para>
        </important>
        <para>By default, an instance does not use huge pages for its underlying memory.
                However, huge pages can bring important or required performance improvements
                for some workloads. Huge pages must be requested explicitly through the use of
                flavor extra specs or image metadata. To request an instance use huge pages,
                run:</para>
        <screen language="console">$ openstack flavor set m1.large --property hw:mem_page_size=large</screen>
        <para>Different platforms offer different huge page sizes. For example: x86-based
                platforms offer 2 MB and 1 GB huge page sizes. Specific huge page sizes can be
                also be requested, with or without a unit suffix. The unit suffix must be one
                of: Kb(it), Kib(it), Mb(it), Mib(it), Gb(it), Gib(it), Tb(it), Tib(it), KB,
                KiB, MB, MiB, GB, GiB, TB, TiB. Where a unit suffix is not provided, Kilobytes
                are assumed. To request an instance to use 2 MB huge pages, run one of:</para>
        <screen language="console">$ openstack flavor set m1.large --property hw:mem_page_size=2Mb</screen>
        <screen language="console">$ openstack flavor set m1.large --property hw:mem_page_size=2048</screen>
        <para>Enabling huge pages for an instance can have negative consequences for other
                instances by consuming limited huge pages resources. To explicitly request
                an instance use small pages, run:</para>
        <screen language="console">$ openstack flavor set m1.large --property hw:mem_page_size=small</screen>
        <note>
          <para>Explicitly requesting any page size will still result in a NUMA topology
                    being applied to the instance, as described earlier in this document.</para>
        </note>
        <para>Finally, to leave the decision of huge or small pages to the compute driver,
                run:</para>
        <screen language="console">$ openstack flavor set m1.large --property hw:mem_page_size=any</screen>
        <para>For more information about the syntax for <literal>hw:mem_page_size</literal>, refer to the
                <link xmlns:xl="http://www.w3.org/1999/xlink" xl:href="https://docs.openstack.org/admin-guide/compute-flavors.html">Flavors</link> guide.</para>
        <para>Applications are frequently packaged as images. For applications that require
                the IO performance improvements that huge pages provides, configure image
                metadata to ensure instances always request the specific page size regardless
                of flavor. To configure an image to use 1 GB huge pages, run:</para>
        <screen language="console">$ openstack image set [IMAGE_ID]  --property hw_mem_page_size=1GB</screen>
        <para>If the flavor specifies a numerical page size or a page size of "small" the
                image is not allowed to specify a page size and if it does an exception will
                be raised. If the flavor specifies a page size of <literal>any</literal> or <literal>large</literal> then
                any page size specified in the image will be used. By setting a <literal>small</literal>
                page size in the flavor, administrators can prevent users requesting huge
                pages in flavors and impacting resource utilization. To configure this page
                size, run:</para>
        <screen language="console">$ openstack flavor set m1.large --property hw:mem_page_size=small</screen>
        <note>
          <para>Explicitly requesting any page size will still result in a NUMA topology
                    being applied to the instance, as described earlier in this document.</para>
        </note>
        <para>For more information about image metadata, refer to the <link xmlns:xl="http://www.w3.org/1999/xlink" xl:href="https://docs.openstack.org/image-guide/image-metadata.html">Image metadata</link>
                guide.</para>
      </section>
    </section>
  </chapter>
  <chapter xml:id="system-architecture" xml:base="arch">
    <title>System architecture</title>
    <para>OpenStack Compute contains several main components.</para>
    <itemizedlist>
      <listitem>
        <para>The cloud controller represents the global state and interacts with the
                    other components. The <literal>API server</literal> acts as the web services front end for
                    the cloud controller. The <literal>compute controller</literal> provides compute server
                    resources and usually also contains the Compute service.</para>
      </listitem>
      <listitem>
        <para>The <literal>object store</literal> is an optional component that provides storage
                    services; you can also use OpenStack Object Storage instead.</para>
      </listitem>
      <listitem>
        <para>An <literal>auth manager</literal> provides authentication and authorization services when
                    used with the Compute system; you can also use OpenStack Identity as a
                    separate authentication service instead.</para>
      </listitem>
      <listitem>
        <para>A <literal>volume controller</literal> provides fast and permanent block-level storage for
                    the compute servers.</para>
      </listitem>
      <listitem>
        <para>The <literal>network controller</literal> provides virtual networks to enable compute
                    servers to interact with each other and with the public network. You can also
                    use OpenStack Networking instead.</para>
      </listitem>
      <listitem>
        <para>The <literal>scheduler</literal> is used to select the most suitable compute controller to
                    host an instance.</para>
      </listitem>
    </itemizedlist>
    <para>Compute uses a messaging-based, <literal>shared nothing</literal> architecture. All major
            components exist on multiple servers, including the compute, volume, and
            network controllers, and the Object Storage or Image service.  The state of the
            entire system is stored in a database. The cloud controller communicates with
            the internal object store using HTTP, but it communicates with the scheduler,
            network controller, and volume controller using Advanced Message Queuing
            Protocol (AMQP). To avoid blocking a component while waiting for a response,
            Compute uses asynchronous calls, with a callback that is triggered when a
            response is received.</para>
    <section xml:id="hypervisors">
      <title>Hypervisors</title>
      <para>Compute controls hypervisors through an API server. Selecting the best
                hypervisor to use can be difficult, and you must take budget, resource
                constraints, supported features, and required technical specifications into
                account. However, the majority of OpenStack development is done on systems
                using KVM and Xen-based hypervisors. For a detailed list of features and
                support across different hypervisors, see the <link xmlns:xl="http://www.w3.org/1999/xlink" xl:href="https://docs.openstack.org/developer/nova/support-matrix.html">Feature Support Matrix</link>.</para>
      <para>You can also orchestrate clouds using multiple hypervisors in different
                availability zones. Compute supports the following hypervisors:</para>
      <itemizedlist>
        <listitem>
          <para>
            <link xmlns:xl="http://www.w3.org/1999/xlink" xl:href="https://docs.openstack.org/ironic/latest/">Baremetal</link>
          </para>
        </listitem>
        <listitem>
          <para>
            <link xmlns:xl="http://www.w3.org/1999/xlink" xl:href="https://www.docker.io">Docker</link>
          </para>
        </listitem>
        <listitem>
          <para>
            <link xmlns:xl="http://www.w3.org/1999/xlink" xl:href="http://www.microsoft.com/en-us/server-cloud/hyper-v-server/default.aspx">Hyper-V</link>
          </para>
        </listitem>
        <listitem>
          <para>
            <link xmlns:xl="http://www.w3.org/1999/xlink" xl:href="http://www.linux-kvm.org/page/Main_Page">Kernel-based Virtual Machine (KVM)</link>
          </para>
        </listitem>
        <listitem>
          <para>
            <link xmlns:xl="http://www.w3.org/1999/xlink" xl:href="https://linuxcontainers.org/">Linux Containers (LXC)</link>
          </para>
        </listitem>
        <listitem>
          <para>
            <link xmlns:xl="http://www.w3.org/1999/xlink" xl:href="http://wiki.qemu.org/Manual">Quick Emulator (QEMU)</link>
          </para>
        </listitem>
        <listitem>
          <para>
            <link xmlns:xl="http://www.w3.org/1999/xlink" xl:href="http://user-mode-linux.sourceforge.net/">User Mode Linux (UML)</link>
          </para>
        </listitem>
        <listitem>
          <para>
            <link xmlns:xl="http://www.w3.org/1999/xlink" xl:href="http://www.vmware.com/products/vsphere-hypervisor/support.html">VMware vSphere</link>
          </para>
        </listitem>
        <listitem>
          <para>
            <link xmlns:xl="http://www.w3.org/1999/xlink" xl:href="http://www.xen.org/support/documentation.html">Xen</link>
          </para>
        </listitem>
      </itemizedlist>
      <para>For more information about hypervisors, see
                <xref linkend="configuration/hypervisors"/>
                section in the Nova Configuration Reference.</para>
    </section>
    <section xml:id="projects-users-and-roles">
      <title>Projects, users, and roles</title>
      <para>The Compute system is designed to be used by different consumers in the form of
                projects on a shared system, and role-based access assignments.  Roles control
                the actions that a user is allowed to perform.</para>
      <para>Projects are isolated resource containers that form the principal
                organizational structure within the Compute service. They consist of an
                individual VLAN, and volumes, instances, images, keys, and users. A user can
                specify the project by appending <literal>project_id</literal> to their access key.  If no
                project is specified in the API request, Compute attempts to use a project with
                the same ID as the user.</para>
      <para>For projects, you can use quota controls to limit the:</para>
      <itemizedlist>
        <listitem>
          <para>Number of volumes that can be launched.</para>
        </listitem>
        <listitem>
          <para>Number of processor cores and the amount of RAM that can be allocated.</para>
        </listitem>
        <listitem>
          <para>Floating IP addresses assigned to any instance when it launches. This allows
                        instances to have the same publicly accessible IP addresses.</para>
        </listitem>
        <listitem>
          <para>Fixed IP addresses assigned to the same instance when it launches.  This
                        allows instances to have the same publicly or privately accessible IP
                        addresses.</para>
        </listitem>
      </itemizedlist>
      <para>Roles control the actions a user is allowed to perform. By default, most
                actions do not require a particular role, but you can configure them by editing
                the <literal>policy.json</literal> file for user roles. For example, a rule can be defined so
                that a user must have the <literal>admin</literal> role in order to be able to allocate a
                public IP address.</para>
      <para>A project limits users' access to particular images. Each user is assigned a
                user name and password. Keypairs granting access to an instance are enabled for
                each user, but quotas are set, so that each project can control resource
                consumption across available hardware resources.</para>
      <note>
        <para>Earlier versions of OpenStack used the term <literal>tenant</literal> instead of
                    <literal>project</literal>. Because of this legacy terminology, some command-line tools use
                    <literal>--tenant_id</literal> where you would normally expect to enter a project ID.</para>
      </note>
    </section>
    <section xml:id="block-storage">
      <title>Block storage</title>
      <para>OpenStack provides two classes of block storage: ephemeral storage and
                persistent volume.</para>
      <para>Ephemeral storage includes a root ephemeral volume and an additional ephemeral
                volume.</para>
      <para>The root disk is associated with an instance, and exists only for the life of
                this very instance. Generally, it is used to store an instance's root file
                system, persists across the guest operating system reboots, and is removed on
                an instance deletion. The amount of the root ephemeral volume is defined by the
                flavor of an instance.</para>
      <para>In addition to the ephemeral root volume, all default types of flavors, except
                <literal>m1.tiny</literal>, which is the smallest one, provide an additional ephemeral block
                device sized between 20 and 160 GB (a configurable value to suit an
                environment). It is represented as a raw block device with no partition table
                or file system. A cloud-aware operating system can discover, format, and mount
                such a storage device. OpenStack Compute defines the default file system for
                different operating systems as Ext4 for Linux distributions, VFAT for non-Linux
                and non-Windows operating systems, and NTFS for Windows. However, it is
                possible to specify any other filesystem type by using <literal>virt_mkfs</literal> or
                <literal>default_ephemeral_format</literal> configuration options.</para>
      <note>
        <para>For example, the <literal>cloud-init</literal> package included into an Ubuntu's stock
                    cloud image, by default, formats this space as an Ext4 file system and
                    mounts it on <literal>/mnt</literal>. This is a cloud-init feature, and is not an OpenStack
                    mechanism. OpenStack only provisions the raw storage.</para>
      </note>
      <para>A persistent volume is represented by a persistent virtualized block device
                independent of any particular instance, and provided by OpenStack Block
                Storage.</para>
      <para>Only a single configured instance can access a persistent volume.  Multiple
                instances cannot access a persistent volume. This type of configuration
                requires a traditional network file system to allow multiple instances
                accessing the persistent volume. It also requires a traditional network file
                system like NFS, CIFS, or a cluster file system such as GlusterFS. These
                systems can be built within an OpenStack cluster, or provisioned outside of it,
                but OpenStack software does not provide these features.</para>
      <para>You can configure a persistent volume as bootable and use it to provide a
                persistent virtual instance similar to the traditional non-cloud-based
                virtualization system. It is still possible for the resulting instance to keep
                ephemeral storage, depending on the flavor selected. In this case, the root
                file system can be on the persistent volume, and its state is maintained, even
                if the instance is shut down. For more information about this type of
                configuration, see <link xmlns:xl="http://www.w3.org/1999/xlink" xl:href="https://docs.openstack.org/cinder/latest/configuration/block-storage/block-storage-overview.html">Introduction to the Block Storage service</link>
                in the OpenStack Configuration Reference.</para>
      <note>
        <para>A persistent volume does not provide concurrent access from multiple
                    instances. That type of configuration requires a traditional network file
                    system like NFS, or CIFS, or a cluster file system such as GlusterFS. These
                    systems can be built within an OpenStack cluster, or provisioned outside of
                    it, but OpenStack software does not provide these features.</para>
      </note>
    </section>
    <section xml:id="ec2-compatibility-api">
      <title>EC2 compatibility API</title>
      <para>In addition to the native compute API, OpenStack provides an EC2-compatible
                API. This API allows EC2 legacy workflows built for EC2 to work with OpenStack.</para>
      <warning>
        <para>Nova in tree EC2-compatible API is deprecated.  The <link xmlns:xl="http://www.w3.org/1999/xlink" xl:href="https://git.openstack.org/cgit/openstack/ec2-api/">ec2-api project</link> is working to
                    implement the EC2 API.</para>
      </warning>
      <para>You can use numerous third-party tools and language-specific SDKs to interact
                with OpenStack clouds. You can use both native and compatibility APIs. Some of
                the more popular third-party tools are:</para>
      <variablelist>
        <varlistentry>
          <term>Euca2ools</term>
          <listitem>
            <para>A popular open source command-line tool for interacting with the EC2 API.
                            This is convenient for multi-cloud environments where EC2 is the common API,
                            or for transitioning from EC2-based clouds to OpenStack. For more
                            information, see the <link xmlns:xl="http://www.w3.org/1999/xlink" xl:href="http://docs.hpcloud.com/eucalyptus">Eucalyptus Documentation</link>.</para>
          </listitem>
        </varlistentry>
        <varlistentry>
          <term>Hybridfox</term>
          <listitem>
            <para>A Firefox browser add-on that provides a graphical interface to many popular
                            public and private cloud technologies, including OpenStack.  For more
                            information, see the <link xmlns:xl="http://www.w3.org/1999/xlink" xl:href="http://code.google.com/p/hybridfox/">hybridfox site</link>.</para>
          </listitem>
        </varlistentry>
        <varlistentry>
          <term>boto</term>
          <listitem>
            <para>Python library for interacting with Amazon Web Services. You can use this
                            library to access OpenStack through the EC2 compatibility API.  For more
                            information, see the <link xmlns:xl="http://www.w3.org/1999/xlink" xl:href="https://github.com/boto/boto">boto project page on GitHub</link>.</para>
          </listitem>
        </varlistentry>
        <varlistentry>
          <term>fog</term>
          <listitem>
            <para>A Ruby cloud services library. It provides methods to interact with a large
                            number of cloud and virtualization platforms, including OpenStack. For more
                            information, see the <link xmlns:xl="http://www.w3.org/1999/xlink" xl:href="https://rubygems.org/gems/fog">fog site</link>.</para>
          </listitem>
        </varlistentry>
        <varlistentry>
          <term>php-opencloud</term>
          <listitem>
            <para>A PHP SDK designed to work with most OpenStack-based cloud deployments, as
                            well as Rackspace public cloud. For more information, see the <link xmlns:xl="http://www.w3.org/1999/xlink" xl:href="http://www.php-opencloud.com">php-opencloud
                                site</link>.</para>
          </listitem>
        </varlistentry>
      </variablelist>
    </section>
    <section xml:id="building-blocks">
      <title>Building blocks</title>
      <para>In OpenStack the base operating system is usually copied from an image stored
                in the OpenStack Image service. This is the most common case and results in an
                ephemeral instance that starts from a known template state and loses all
                accumulated states on virtual machine deletion. It is also possible to put an
                operating system on a persistent volume in the OpenStack Block Storage volume
                system. This gives a more traditional persistent system that accumulates states
                which are preserved on the OpenStack Block Storage volume across the deletion
                and re-creation of the virtual machine. To get a list of available images on
                your system, run:</para>
      <screen language="console">$ openstack image list
+--------------------------------------+-----------------------------+--------+
| ID                                   | Name                        | Status |
+--------------------------------------+-----------------------------+--------+
| aee1d242-730f-431f-88c1-87630c0f07ba | Ubuntu 14.04 cloudimg amd64 | active |
| 0b27baa1-0ca6-49a7-b3f4-48388e440245 | Ubuntu 14.10 cloudimg amd64 | active |
| df8d56fc-9cea-4dfd-a8d3-28764de3cb08 | jenkins                     | active |
+--------------------------------------+-----------------------------+--------+</screen>
      <para>The displayed image attributes are:</para>
      <variablelist>
        <varlistentry>
          <term>
            <literal>ID</literal>
          </term>
          <listitem>
            <para>Automatically generated UUID of the image</para>
          </listitem>
        </varlistentry>
        <varlistentry>
          <term>
            <literal>Name</literal>
          </term>
          <listitem>
            <para>Free form, human-readable name for image</para>
          </listitem>
        </varlistentry>
        <varlistentry>
          <term>
            <literal>Status</literal>
          </term>
          <listitem>
            <para>The status of the image. Images marked <literal>ACTIVE</literal> are available for use.</para>
          </listitem>
        </varlistentry>
        <varlistentry>
          <term>
            <literal>Server</literal>
          </term>
          <listitem>
            <para>For images that are created as snapshots of running instances, this is the
                            UUID of the instance the snapshot derives from. For uploaded images, this
                            field is blank.</para>
          </listitem>
        </varlistentry>
      </variablelist>
      <para>Virtual hardware templates are called <literal>flavors</literal>. By default, these are
                configurable by admin users, however that behavior can be changed by redefining
                the access controls for <literal>compute_extension:flavormanage</literal> in
                <literal>/etc/nova/policy.json</literal> on the <literal>compute-api</literal> server.</para>
      <para>For a list of flavors that are available on your system:</para>
      <screen language="console">$ openstack flavor list
+-----+-----------+-------+------+-----------+-------+-----------+
| ID  | Name      |   RAM | Disk | Ephemeral | VCPUs | Is_Public |
+-----+-----------+-------+------+-----------+-------+-----------+
| 1   | m1.tiny   |   512 |    1 |         0 |     1 | True      |
| 2   | m1.small  |  2048 |   20 |         0 |     1 | True      |
| 3   | m1.medium |  4096 |   40 |         0 |     2 | True      |
| 4   | m1.large  |  8192 |   80 |         0 |     4 | True      |
| 5   | m1.xlarge | 16384 |  160 |         0 |     8 | True      |
+-----+-----------+-------+------+-----------+-------+-----------+</screen>
    </section>
    <section xml:id="compute-service-architecture">
      <title>Compute service architecture</title>
      <para>These basic categories describe the service architecture and information about
                the cloud controller.</para>
      <para>At the heart of the cloud framework is an API server, which makes command and
                control of the hypervisor, storage, and networking programmatically available
                to users.</para>
      <para>The API endpoints are basic HTTP web services which handle authentication,
                authorization, and basic command and control functions using various API
                interfaces under the Amazon, Rackspace, and related models. This enables API
                compatibility with multiple existing tool sets created for interaction with
                offerings from other vendors. This broad compatibility prevents vendor lock-in.</para>
      <para>A messaging queue brokers the interaction between compute nodes (processing),
                the networking controllers (software which controls network infrastructure),
                API endpoints, the scheduler (determines which physical hardware to allocate to
                a virtual resource), and similar components. Communication to and from the
                cloud controller is handled by HTTP requests through multiple API endpoints.</para>
      <para>A typical message passing event begins with the API server receiving a request
                from a user. The API server authenticates the user and ensures that they are
                permitted to issue the subject command. The availability of objects implicated
                in the request is evaluated and, if available, the request is routed to the
                queuing engine for the relevant workers.  Workers continually listen to the
                queue based on their role, and occasionally their type host name. When an
                applicable work request arrives on the queue, the worker takes assignment of
                the task and begins executing it. Upon completion, a response is dispatched to
                the queue which is received by the API server and relayed to the originating
                user.  Database entries are queried, added, or removed as necessary during the
                process.</para>
      <para>Compute workers manage computing instances on host machines. The API dispatches
                commands to compute workers to complete these tasks:</para>
      <itemizedlist>
        <listitem>
          <para>Run instances</para>
        </listitem>
        <listitem>
          <para>Delete instances (Terminate instances)</para>
        </listitem>
        <listitem>
          <para>Reboot instances</para>
        </listitem>
        <listitem>
          <para>Attach volumes</para>
        </listitem>
        <listitem>
          <para>Detach volumes</para>
        </listitem>
        <listitem>
          <para>Get console output</para>
        </listitem>
      </itemizedlist>
      <para>The Network Controller manages the networking resources on host machines. The
                API server dispatches commands through the message queue, which are
                subsequently processed by Network Controllers. Specific operations include:</para>
      <itemizedlist>
        <listitem>
          <para>Allocating fixed IP addresses</para>
        </listitem>
        <listitem>
          <para>Configuring VLANs for projects</para>
        </listitem>
        <listitem>
          <para>Configuring networks for compute nodes</para>
        </listitem>
      </itemizedlist>
    </section>
  </chapter>
  <chapter xml:id="select-hosts-where-instances-are-launched" xml:base="availability-zones">
    <title>Select hosts where instances are launched</title>
    <para>With the appropriate permissions, you can select which host instances are
            launched on and which roles can boot instances on this host.</para>
    <procedure>
      <step>
        <para>To select the host where instances are launched, use the
                    <literal>--availability-zone ZONE:HOST:NODE</literal> parameter on the <command>openstack
                        server create</command> command.</para>
        <para>For example:</para>
        <screen language="console">$ openstack server create --image IMAGE --flavor m1.tiny \
  --key-name KEY --availability-zone ZONE:HOST:NODE \
  --nic net-id=UUID SERVER</screen>
        <note>
          <para>HOST and NODE are optional parameters. In such cases, use the
                        <literal>--availability-zone ZONE::NODE</literal>, <literal>--availability-zone ZONE:HOST</literal> or
                        <literal>--availability-zone ZONE</literal>.</para>
        </note>
      </step>
      <step>
        <para>To specify which roles can launch an instance on a specified host, enable
                    the <literal>create:forced_host</literal> option in the <literal>policy.json</literal> file. By default,
                    this option is enabled for only the admin role. If you see <literal>Forbidden (HTTP
403)</literal> in return, then you are not using admin credentials.</para>
      </step>
      <step>
        <para>To view the list of valid zones, use the <command>openstack availability
                        zone list</command> command.</para>
        <screen language="console">$ openstack availability zone list
+-----------+-------------+
| Zone Name | Zone Status |
+-----------+-------------+
| zone1     | available   |
| zone2     | available   |
+-----------+-------------+</screen>
      </step>
      <step>
        <para>To view the list of valid compute hosts, use the <command>openstack host
                        list</command> command.</para>
        <screen language="console">$ openstack host list
+----------------+-------------+----------+
| Host Name      | Service     | Zone     |
+----------------+-------------+----------+
| compute01      | compute     | nova     |
| compute02      | compute     | nova     |
+----------------+-------------+----------+</screen>
      </step>
      <step>
        <para>To view the list of valid compute nodes, use the <command>openstack
                        hypervisor list</command> command.</para>
        <screen language="console">$ openstack hypervisor list
+----+---------------------+
| ID | Hypervisor Hostname |
+----+---------------------+
|  1 | server2             |
|  2 | server3             |
|  3 | server4             |
+----+---------------------+</screen>
      </step>
    </procedure>
  </chapter>
  <chapter xml:id="section-configuring-compute-migrations" xml:base="configuring-migrations">
    <title>Configure live migrations</title>
    <para>Migration enables an administrator to move a virtual machine instance from one
            compute host to another. A typical scenario is planned maintenance on the
            source host, but migration can also be useful to redistribute the load when
            many VM instances are running on a specific physical machine.</para>
    <para>This document covers live migrations using the
            <link xmlns:xl="http://www.w3.org/1999/xlink" xl:href="#configuring-migrations-kvm-libvirt">KVM-libvirt</link> and
            <link xmlns:xl="http://www.w3.org/1999/xlink" xl:href="#configuring-migrations-xenserver">XenServer</link> hypervisors.</para>
    <note>
      <para>Not all Compute service hypervisor drivers support live-migration, or
                support all live-migration features.</para>
      <para>Consult the <link xmlns:xl="http://www.w3.org/1999/xlink" xl:href="https://docs.openstack.org/developer/nova/support-matrix.html">Hypervisor Support Matrix</link> to
                determine which hypervisors support live-migration.</para>
      <para>See the <link xmlns:xl="http://www.w3.org/1999/xlink" xl:href="https://docs.openstack.org/ocata/config-reference/compute/hypervisors.html">Hypervisor configuration pages</link>
                for details on hypervisor-specific configuration settings.</para>
    </note>
    <para>The migration types are:</para>
    <itemizedlist>
      <listitem>
        <para><emphasis role="bold">Non-live migration</emphasis>, also known as cold migration or simply migration.</para>
        <para>The instance is shut down, then moved to another hypervisor and restarted.
                    The instance recognizes that it was rebooted, and the application running on
                    the instance is disrupted.</para>
        <para>This section does not cover cold migration.</para>
      </listitem>
      <listitem>
        <para>
          <emphasis role="bold">Live migration</emphasis>
        </para>
        <para>The instance keeps running throughout the migration.  This is useful when it
                    is not possible or desirable to stop the application running on the instance.</para>
        <para>Live migrations can be classified further by the way they treat instance
                    storage:</para>
        <itemizedlist>
          <listitem>
            <para><emphasis role="bold">Shared storage-based live migration</emphasis>. The instance has ephemeral disks
                            that are located on storage shared between the source and destination
                            hosts.</para>
          </listitem>
          <listitem>
            <para><emphasis role="bold">Block live migration</emphasis>, or simply block migration.  The instance has
                            ephemeral disks that are not shared between the source and destination
                            hosts.  Block migration is incompatible with read-only devices such as
                            CD-ROMs and Configuration Drive (config_drive).</para>
          </listitem>
          <listitem>
            <para><emphasis role="bold">Volume-backed live migration</emphasis>. Instances use volumes rather than
                            ephemeral disks.</para>
          </listitem>
        </itemizedlist>
        <para>Block live migration requires copying disks from the source to the
                    destination host. It takes more time and puts more load on the network.
                    Shared-storage and volume-backed live migration does not copy disks.</para>
      </listitem>
    </itemizedlist>
    <note>
      <para>In a multi-cell cloud, instances can be live migrated to a
                different host in the same cell, but not across cells.</para>
    </note>
    <para>The following sections describe how to configure your hosts for live migrations
            using the KVM and XenServer hypervisors.</para>
    <section xml:id="configuring-migrations-kvm-libvirt">
      <title>KVM-libvirt</title>
      <section xml:id="configuring-migrations-kvm-general">
        <title>General configuration</title>
        <para>To enable any type of live migration, configure the compute hosts according to
                    the instructions below:</para>
        <procedure>
          <step>
            <para>Set the following parameters in <literal>nova.conf</literal> on all compute hosts:</para>
            <itemizedlist>
              <listitem>
                <para>
                  <literal>vncserver_listen=0.0.0.0</literal>
                </para>
                <para>You must not make the VNC server listen to the IP address of its compute
                                    host, since that addresses changes when the instance is migrated.</para>
                <important>
                  <para>Since this setting allows VNC clients from any IP address to connect to
                                        instance consoles, you must take additional measures like secure
                                        networks or firewalls to prevent potential attackers from gaining
                                        access to instances.</para>
                </important>
              </listitem>
              <listitem>
                <para><literal>instances_path</literal> must have the same value for all compute hosts. In
                                    this guide, the value <literal>/var/lib/nova/instances</literal> is assumed.</para>
              </listitem>
            </itemizedlist>
          </step>
          <step>
            <para>Ensure that name resolution on all compute hosts is identical, so that they
                            can connect each other through their hostnames.</para>
            <para>If you use <literal>/etc/hosts</literal> for name resolution and enable SELinux, ensure
                            that <literal>/etc/hosts</literal> has the correct SELinux context:</para>
            <screen language="console"># restorecon /etc/hosts</screen>
          </step>
          <step>
            <para>Enable password-less SSH so that root on one compute host can log on to any
                            other compute host without providing a password.  The <literal>libvirtd</literal> daemon,
                            which runs as root, uses the SSH protocol to copy the instance to the
                            destination and can't know the passwords of all compute hosts.</para>
            <para>You may, for example, compile root's public SSH keys on all compute hosts
                            into an <literal>authorized_keys</literal> file and deploy that file to the compute hosts.</para>
          </step>
          <step>
            <para>Configure the firewalls to allow libvirt to communicate between compute
                            hosts.</para>
            <para>By default, libvirt uses the TCP port range from 49152 to 49261 for copying
                            memory and disk contents. Compute hosts must accept connections in this
                            range.</para>
            <para>For information about ports used by libvirt, see the <link xmlns:xl="http://www.w3.org/1999/xlink" xl:href="http://libvirt.org/remote.html#Remote_libvirtd_configuration">libvirt documentation</link>.</para>
            <important>
              <para>Be mindful of the security risks introduced by opening ports.</para>
            </important>
          </step>
        </procedure>
      </section>
      <section xml:id="configuring-migrations-kvm-block-and-volume-migration">
        <title>Block migration, volume-based live migration</title>
        <para>No additional configuration is required for block migration and volume-backed
                    live migration.</para>
        <para>Be aware that block migration adds load to the network and storage subsystems.</para>
      </section>
      <section xml:id="configuring-migrations-kvm-shared-storage">
        <title>Shared storage</title>
        <para>Compute hosts have many options for sharing storage, for example NFS, shared
                    disk array LUNs, Ceph or GlusterFS.</para>
        <para>The next steps show how a regular Linux system might be configured as an NFS v4
                    server for live migration.  For detailed information and alternative ways to
                    configure NFS on Linux, see instructions for <link xmlns:xl="http://www.w3.org/1999/xlink" xl:href="https://help.ubuntu.com/community/SettingUpNFSHowTo">Ubuntu</link>, <link xmlns:xl="http://www.w3.org/1999/xlink" xl:href="https://access.redhat.com/documentation/en-US/Red_Hat_Enterprise_Linux/7/html/Storage_Administration_Guide/nfs-serverconfig.html">RHEL and derivatives</link>
                    or <link xmlns:xl="http://www.w3.org/1999/xlink" xl:href="https://www.suse.com/documentation/sles-12/book_sle_admin/data/sec_nfs_configuring-nfs-server.html">SLES and OpenSUSE</link>.</para>
        <procedure>
          <step>
            <para>Ensure that UID and GID of the nova user are identical on the compute hosts
                            and the NFS server.</para>
          </step>
          <step>
            <para>Create a directory with enough disk space for all instances in the cloud,
                            owned by user nova. In this guide, we assume <literal>/var/lib/nova/instances</literal>.</para>
          </step>
          <step>
            <para>Set the execute/search bit on the <literal>instances</literal> directory:</para>
            <screen language="console">$ chmod o+x /var/lib/nova/instances</screen>
            <para>This  allows qemu to access the <literal>instances</literal> directory tree.</para>
          </step>
          <step>
            <para>Export <literal>/var/lib/nova/instances</literal> to the compute hosts. For example, add
                            the following line to <literal>/etc/exports</literal>:</para>
            <screen language="ini">/var/lib/nova/instances *(rw,sync,fsid=0,no_root_squash)</screen>
            <para>The asterisk permits access to any NFS client. The option <literal>fsid=0</literal> exports
                            the instances directory as the NFS root.</para>
          </step>
        </procedure>
        <para>After setting up the NFS server, mount the remote filesystem on all compute
                    hosts.</para>
        <procedure>
          <step>
            <para>Assuming the NFS server's hostname is <literal>nfs-server</literal>, add this line to
                            <literal>/etc/fstab</literal> to mount the NFS root:</para>
            <screen language="console">nfs-server:/ /var/lib/nova/instances nfs4 defaults 0 0</screen>
          </step>
          <step>
            <para>Test NFS by mounting the instances directory and check access permissions
                            for the nova user:</para>
            <screen language="console">$ sudo mount -a -v
$ ls -ld /var/lib/nova/instances/
drwxr-xr-x. 2 nova nova 6 Mar 14 21:30 /var/lib/nova/instances/</screen>
          </step>
        </procedure>
      </section>
      <section xml:id="configuring-migrations-kvm-advanced">
        <title>Advanced configuration for KVM and QEMU</title>
        <para>Live migration copies the instance's memory from the source to the destination
                    compute host. After a memory page has been copied, the instance may write to it
                    again, so that it has to be copied again.  Instances that frequently write to
                    different memory pages can overwhelm the memory copy process and prevent the
                    live migration from completing.</para>
        <para>This section covers configuration settings that can help live migration of
                    memory-intensive instances succeed.</para>
        <procedure>
          <step>
            <para>
              <emphasis role="bold">Live migration completion timeout</emphasis>
            </para>
            <para>The Compute service aborts a migration when it has been running for too
                            long.  The timeout is calculated based on the instance size, which is the
                            instance's memory size in GiB. In the case of block migration, the size of
                            ephemeral storage in GiB is added.</para>
            <para>The timeout in seconds is the instance size multiplied by the configurable
                            parameter <literal>live_migration_completion_timeout</literal>, whose default is 800. For
                            example, shared-storage live migration of an instance with 8GiB memory will
                            time out after 6400 seconds.</para>
          </step>
          <step>
            <para>
              <emphasis role="bold">Live migration progress timeout</emphasis>
            </para>
            <para>The Compute service also aborts a live migration when it detects that memory
                            copy is not making progress for a certain time. You can set this time, in
                            seconds, through the configurable parameter
                            <literal>live_migration_progress_timeout</literal>.</para>
            <para>In Ocata, the default value of <literal>live_migration_progress_timeout</literal> is 0,
                            which disables progress timeouts. You should not change this value, since
                            the algorithm that detects memory copy progress has been determined to be
                            unreliable. It may be re-enabled in future releases.</para>
          </step>
          <step>
            <para>
              <emphasis role="bold">Instance downtime</emphasis>
            </para>
            <para>Near the end of the memory copy, the instance is paused for a short time so
                            that the remaining few pages can be copied without interference from
                            instance memory writes. The Compute service initializes this time to a small
                            value that depends on the instance size, typically around 50 milliseconds.
                            When it notices that the memory copy does not make sufficient progress, it
                            increases the time gradually.</para>
            <para>You can influence the instance downtime algorithm with the help of three
                            configuration variables on the compute hosts:</para>
            <screen language="ini">live_migration_downtime = 500
live_migration_downtime_steps = 10
live_migration_downtime_delay = 75</screen>
            <para><literal>live_migration_downtime</literal> sets the maximum permitted downtime for a live
                            migration, in <emphasis>milliseconds</emphasis>.  The default is 500.</para>
            <para><literal>live_migration_downtime_steps</literal> sets the total number of adjustment steps
                            until <literal>live_migration_downtime</literal> is reached.  The default is 10 steps.</para>
            <para><literal>live_migration_downtime_delay</literal> sets the time interval between two
                            adjustment steps in <emphasis>seconds</emphasis>. The default is 75.</para>
          </step>
          <step>
            <para>
              <emphasis role="bold">Auto-convergence</emphasis>
            </para>
            <para>One strategy for a successful live migration of a memory-intensive instance
                            is slowing the instance down. This is called auto-convergence.  Both libvirt
                            and QEMU implement this feature by automatically throttling the instance's
                            CPU when memory copy delays are detected.</para>
            <para>Auto-convergence is disabled by default.  You can enable it by setting
                            <literal>live_migration_permit_auto_converge=true</literal>.</para>
            <important>
              <para>Before enabling auto-convergence, make sure that the instance's
                                application tolerates a slow-down.</para>
              <para>Be aware that auto-convergence does not guarantee live migration success.</para>
            </important>
          </step>
          <step>
            <para>
              <emphasis role="bold">Post-copy</emphasis>
            </para>
            <para>Live migration of a memory-intensive instance is certain to succeed when you
                            enable post-copy. This feature, implemented by libvirt and QEMU, activates
                            the virtual machine on the destination host before all of its memory has
                            been copied.  When the virtual machine accesses a page that is missing on
                            the destination host, the resulting page fault is resolved by copying the
                            page from the source host.</para>
            <para>Post-copy is disabled by default. You can enable it by setting
                            <literal>live_migration_permit_post_copy=true</literal>.</para>
            <para>When you enable both auto-convergence and post-copy, auto-convergence
                            remains disabled.</para>
            <important>
              <para>The page faults introduced by post-copy can slow the instance down.</para>
              <para>When the network connection between source and destination host is
                                interrupted, page faults cannot be resolved anymore and the instance is
                                rebooted.</para>
            </important>
          </step>
        </procedure>
        <para>The full list of live migration configuration parameters is documented in the
                    <xref linkend="../configuration/config"/></para>
      </section>
    </section>
    <section xml:id="configuring-migrations-xenserver">
      <title>XenServer</title>
      <section xml:id="configuring-migrations-xenserver-shared-storage">
        <title>Shared storage</title>
        <para>
          <emphasis role="bold">Prerequisites</emphasis>
        </para>
        <itemizedlist>
          <listitem>
            <para><emphasis role="bold">Compatible XenServer hypervisors</emphasis>.</para>
            <para>For more information, see the <link xmlns:xl="http://www.w3.org/1999/xlink" xl:href="http://docs.vmd.citrix.com/XenServer/6.0.0/1.0/en_gb/reference.html#pooling_homogeneity_requirements">Requirements for Creating Resource Pools</link>
                            section of the XenServer Administrator's Guide.</para>
          </listitem>
          <listitem>
            <para><emphasis role="bold">Shared storage</emphasis>.</para>
            <para>An NFS export, visible to all XenServer hosts.</para>
            <note>
              <para>For the supported NFS versions, see the <link xmlns:xl="http://www.w3.org/1999/xlink" xl:href="http://docs.vmd.citrix.com/XenServer/6.0.0/1.0/en_gb/reference.html#id1002701">NFS VHD</link>
                                    section of the XenServer Administrator's Guide.</para>
            </note>
          </listitem>
        </itemizedlist>
        <para>To use shared storage live migration with XenServer hypervisors, the hosts must
                    be joined to a XenServer pool. To create that pool, a host aggregate must be
                    created with specific metadata. This metadata is used by the XAPI plug-ins to
                    establish the pool.</para>
        <procedure>
          <step>
            <para>Add an NFS VHD storage to your master XenServer, and set it as the default
                            storage repository. For more information, see NFS VHD in the XenServer
                            Administrator's Guide.</para>
          </step>
          <step>
            <para>Configure all compute nodes to use the default storage repository (<literal>sr</literal>)
                            for pool operations. Add this line to your <literal>nova.conf</literal> configuration files
                            on all compute nodes:</para>
            <screen language="ini">sr_matching_filter=default-sr:true</screen>
          </step>
          <step>
            <para>Create a host aggregate. This command creates the aggregate, and then
                            displays a table that contains the ID of the new aggregate</para>
            <screen language="console">$ openstack aggregate create --zone AVAILABILITY_ZONE POOL_NAME</screen>
            <para>Add metadata to the aggregate, to mark it as a hypervisor pool</para>
            <screen language="console">$ openstack aggregate set --property hypervisor_pool=true AGGREGATE_ID

$ openstack aggregate set --property operational_state=created AGGREGATE_ID</screen>
            <para>Make the first compute node part of that aggregate</para>
            <screen language="console">$ openstack aggregate add host AGGREGATE_ID MASTER_COMPUTE_NAME</screen>
            <para>The host is now part of a XenServer pool.</para>
          </step>
          <step>
            <para>Add hosts to the pool</para>
            <screen language="console">$ openstack aggregate add host AGGREGATE_ID COMPUTE_HOST_NAME</screen>
            <note>
              <para>The added compute node and the host will shut down to join the host to
                                the XenServer pool. The operation will fail if any server other than the
                                compute node is running or suspended on the host.</para>
            </note>
          </step>
        </procedure>
      </section>
      <section xml:id="configuring-migrations-xenserver-block-migration">
        <title>Block migration</title>
        <itemizedlist>
          <listitem>
            <para><emphasis role="bold">Compatible XenServer hypervisors</emphasis>.</para>
            <para>The hypervisors must support the Storage XenMotion feature.  See your
                            XenServer manual to make sure your edition has this feature.</para>
            <note>
              <itemizedlist>
                <listitem>
                  <para>To use block migration, you must use the <literal>--block-migrate</literal> parameter
                                            with the live migration command.</para>
                </listitem>
                <listitem>
                  <para>Block migration works only with EXT local storage storage repositories,
                                            and the server must not have any volumes attached.</para>
                </listitem>
              </itemizedlist>
            </note>
          </listitem>
        </itemizedlist>
      </section>
    </section>
  </chapter>
  <chapter xml:id="cpu-topologies" xml:base="cpu-topologies">
    <title>CPU topologies</title>
    <para>The NUMA topology and CPU pinning features in OpenStack provide high-level
            control over how instances run on hypervisor CPUs and the topology of virtual
            CPUs available to instances. These features help minimize latency and maximize
            performance.</para>
    <section xml:id="smp-numa-and-smt">
      <title>SMP, NUMA, and SMT</title>
      <variablelist>
        <varlistentry>
          <term>Symmetric multiprocessing (SMP)</term>
          <listitem>
            <para>SMP is a design found in many modern multi-core systems. In an SMP system,
                            there are two or more CPUs and these CPUs are connected by some interconnect.
                            This provides CPUs with equal access to system resources like memory and
                            input/output ports.</para>
          </listitem>
        </varlistentry>
        <varlistentry>
          <term>Non-uniform memory access (NUMA)</term>
          <listitem>
            <para>NUMA is a derivative of the SMP design that is found in many multi-socket
                            systems. In a NUMA system, system memory is divided into cells or nodes that
                            are associated with particular CPUs. Requests for memory on other nodes are
                            possible through an interconnect bus. However, bandwidth across this shared
                            bus is limited. As a result, competition for this resource can incur
                            performance penalties.</para>
          </listitem>
        </varlistentry>
        <varlistentry>
          <term>Simultaneous Multi-Threading (SMT)</term>
          <listitem>
            <para>SMT is a design complementary to SMP. Whereas CPUs in SMP systems share a bus
                            and some memory, CPUs in SMT systems share many more components. CPUs that
                            share components are known as thread siblings.  All CPUs appear as usable
                            CPUs on the system and can execute workloads in parallel. However, as with
                            NUMA, threads compete for shared resources.</para>
          </listitem>
        </varlistentry>
      </variablelist>
      <para>In OpenStack, SMP CPUs are known as <emphasis>cores</emphasis>, NUMA cells or nodes are known as
                <emphasis>sockets</emphasis>, and SMT CPUs are known as <emphasis>threads</emphasis>. For example, a quad-socket,
                eight core system with Hyper-Threading would have four sockets, eight cores per
                socket and two threads per core, for a total of 64 CPUs.</para>
    </section>
    <section xml:id="configuring-compute-nodes-for-instances-with-numa-placement-policies">
      <title>Configuring compute nodes for instances with NUMA placement policies</title>
      <para>Hyper-V is configured by default to allow instances to span multiple NUMA
                nodes, regardless if the instances have been configured to only span N NUMA
                nodes. This behaviour allows Hyper-V instances to have up to 64 vCPUs and 1 TB
                of memory.</para>
      <para>Checking NUMA spanning can easily be done by running this following powershell
                command:</para>
      <screen language="console">(Get-VMHost).NumaSpanningEnabled</screen>
      <para>In order to disable this behaviour, the host will have to be configured to
                disable NUMA spanning. This can be done by executing these following
                powershell commands:</para>
      <screen language="console">Set-VMHost -NumaSpanningEnabled $false
Restart-Service vmms</screen>
      <para>In order to restore this behaviour, execute these powershell commands:</para>
      <screen language="console">Set-VMHost -NumaSpanningEnabled $true
Restart-Service vmms</screen>
      <para>The <literal>vmms</literal> service (Virtual Machine Management Service) is responsible for
                managing the Hyper-V VMs. The VMs will still run while the service is down
                or restarting, but they will not be manageable by the <literal>nova-compute</literal>
                service. In order for the effects of the Host NUMA spanning configuration
                to take effect, the VMs will have to be restarted.</para>
      <para>Hyper-V does not allow instances with a NUMA topology to have dynamic
                memory allocation turned on. The Hyper-V driver will ignore the configured
                <literal>dynamic_memory_ratio</literal> from the given <literal>nova.conf</literal> file when spawning
                instances with a NUMA topology.</para>
    </section>
    <section xml:id="customizing-instance-numa-placement-policies">
      <title>Customizing instance NUMA placement policies</title>
      <important>
        <para>The functionality described below is currently only supported by the
                    libvirt/KVM and Hyper-V driver.</para>
      </important>
      <para>When running workloads on NUMA hosts, it is important that the vCPUs executing
                processes are on the same NUMA node as the memory used by these processes.
                This ensures all memory accesses are local to the node and thus do not consume
                the limited cross-node memory bandwidth, adding latency to memory accesses.
                Similarly, large pages are assigned from memory and benefit from the same
                performance improvements as memory allocated using standard pages. Thus, they
                also should be local. Finally, PCI devices are directly associated with
                specific NUMA nodes for the purposes of DMA. Instances that use PCI or SR-IOV
                devices should be placed on the NUMA node associated with these devices.</para>
      <para>By default, an instance floats across all NUMA nodes on a host. NUMA awareness
                can be enabled implicitly through the use of huge pages or pinned CPUs or
                explicitly through the use of flavor extra specs or image metadata.  In all
                cases, the <literal>NUMATopologyFilter</literal> filter must be enabled. Details on this
                filter are provided in <xref linkend="configuration/schedulers"/> in Nova
                configuration guide.</para>
      <important>
        <para>The NUMA node(s) used are normally chosen at random. However, if a PCI
                    passthrough or SR-IOV device is attached to the instance, then the NUMA
                    node that the device is associated with will be used. This can provide
                    important performance improvements. However, booting a large number of
                    similar instances can result in unbalanced NUMA node usage. Care should
                    be taken to mitigate this issue. See this <link xmlns:xl="http://www.w3.org/1999/xlink" xl:href="http://lists.openstack.org/pipermail/openstack-dev/2016-March/090367.html">discussion</link> for more details.</para>
      </important>
      <important>
        <para>Inadequate per-node resources will result in scheduling failures. Resources
                    that are specific to a node include not only CPUs and memory, but also PCI
                    and SR-IOV resources. It is not possible to use multiple resources from
                    different nodes without requesting a multi-node layout. As such, it may be
                    necessary to ensure PCI or SR-IOV resources are associated with the same
                    NUMA node or force a multi-node layout.</para>
      </important>
      <para>When used, NUMA awareness allows the operating system of the instance to
                intelligently schedule the workloads that it runs and minimize cross-node
                memory bandwidth. To restrict an instance's vCPUs to a single host NUMA node,
                run:</para>
      <screen language="console">$ openstack flavor set m1.large --property hw:numa_nodes=1</screen>
      <para>Some workloads have very demanding requirements for memory access latency or
                bandwidth that exceed the memory bandwidth available from a single NUMA node.
                For such workloads, it is beneficial to spread the instance across multiple
                host NUMA nodes, even if the instance's RAM/vCPUs could theoretically fit on a
                single NUMA node. To force an instance's vCPUs to spread across two host NUMA
                nodes, run:</para>
      <screen language="console">$ openstack flavor set m1.large --property hw:numa_nodes=2</screen>
      <para>The allocation of instances vCPUs and memory from different host NUMA nodes can
                be configured. This allows for asymmetric allocation of vCPUs and memory, which
                can be important for some workloads. To spread the 6 vCPUs and 6 GB of memory
                of an instance across two NUMA nodes and create an asymmetric 1:2 vCPU and
                memory mapping between the two nodes, run:</para>
      <screen language="console">$ openstack flavor set m1.large --property hw:numa_nodes=2
$ openstack flavor set m1.large \  # configure guest node 0
  --property hw:numa_cpus.0=0,1 \
  --property hw:numa_mem.0=2048
$ openstack flavor set m1.large \  # configure guest node 1
  --property hw:numa_cpus.1=2,3,4,5 \
  --property hw:numa_mem.1=4096</screen>
      <note>
        <para>Hyper-V does not support asymmetric NUMA topologies, and the Hyper-V
                    driver will not spawn instances with such topologies.</para>
      </note>
      <para>For more information about the syntax for <literal>hw:numa_nodes</literal>, <literal>hw:numa_cpus.N</literal>
                and <literal>hw:num_mem.N</literal>, refer to the <xref linkend="extra-specs-numa-topology"/> guide.</para>
    </section>
    <section xml:id="customizing-instance-cpu-pinning-policies">
      <title>Customizing instance CPU pinning policies</title>
      <important>
        <para>The functionality described below is currently only supported by the
                    libvirt/KVM driver. Hyper-V does not support CPU pinning.</para>
      </important>
      <para>By default, instance vCPU processes are not assigned to any particular host
                CPU, instead, they float across host CPUs like any other process. This allows
                for features like overcommitting of CPUs. In heavily contended systems, this
                provides optimal system performance at the expense of performance and latency
                for individual instances.</para>
      <para>Some workloads require real-time or near real-time behavior, which is not
                possible with the latency introduced by the default CPU policy. For such
                workloads, it is beneficial to control which host CPUs are bound to an
                instance's vCPUs. This process is known as pinning. No instance with pinned
                CPUs can use the CPUs of another pinned instance, thus preventing resource
                contention between instances. To configure a flavor to use pinned vCPUs, a
                use a dedicated CPU policy. To force this, run:</para>
      <screen language="console">$ openstack flavor set m1.large --property hw:cpu_policy=dedicated</screen>
      <important>
        <para>Host aggregates should be used to separate pinned instances from unpinned
                    instances as the latter will not respect the resourcing requirements of
                    the former.</para>
      </important>
      <para>When running workloads on SMT hosts, it is important to be aware of the impact
                that thread siblings can have. Thread siblings share a number of components
                and contention on these components can impact performance. To configure how
                to use threads, a CPU thread policy should be specified. For workloads where
                sharing benefits performance, use thread siblings. To force this, run:</para>
      <screen language="console">$ openstack flavor set m1.large \
  --property hw:cpu_policy=dedicated \
  --property hw:cpu_thread_policy=require</screen>
      <para>For other workloads where performance is impacted by contention for resources,
                use non-thread siblings or non-SMT hosts. To force this, run:</para>
      <screen language="console">$ openstack flavor set m1.large \
  --property hw:cpu_policy=dedicated \
  --property hw:cpu_thread_policy=isolate</screen>
      <para>Finally, for workloads where performance is minimally impacted, use thread
                siblings if available. This is the default, but it can be set explicitly:</para>
      <screen language="console">$ openstack flavor set m1.large \
  --property hw:cpu_policy=dedicated \
  --property hw:cpu_thread_policy=prefer</screen>
      <para>For more information about the syntax for <literal>hw:cpu_policy</literal> and
                <literal>hw:cpu_thread_policy</literal>, refer to the <xref linkend="flavors"/> guide.</para>
      <para>Applications are frequently packaged as images. For applications that require
                real-time or near real-time behavior, configure image metadata to ensure
                created instances are always pinned regardless of flavor. To configure an
                image to use pinned vCPUs and avoid thread siblings, run:</para>
      <screen language="console">$ openstack image set [IMAGE_ID] \
  --property hw_cpu_policy=dedicated \
  --property hw_cpu_thread_policy=isolate</screen>
      <para>If the flavor specifies a CPU policy of <literal>dedicated</literal> then that policy will be
                used. If the flavor explicitly specifies a CPU policy of <literal>shared</literal> and the
                image specifies no policy or a policy of <literal>shared</literal> then the <literal>shared</literal> policy
                will be used, but if the image specifies a policy of <literal>dedicated</literal> an exception
                will be raised. By setting a <literal>shared</literal> policy through flavor extra-specs,
                administrators can prevent users configuring CPU policies in images and
                impacting resource utilization. To configure this policy, run:</para>
      <screen language="console">$ openstack flavor set m1.large --property hw:cpu_policy=shared</screen>
      <para>If the flavor does not specify a CPU thread policy then the CPU thread policy
                specified by the image (if any) will be used. If both the flavor and image
                specify a CPU thread policy then they must specify the same policy, otherwise
                an exception will be raised.</para>
      <note>
        <para>There is no correlation required between the NUMA topology exposed in the
                    instance and how the instance is actually pinned on the host. This is by
                    design. See this <link xmlns:xl="http://www.w3.org/1999/xlink" xl:href="https://bugs.launchpad.net/nova/+bug/1466780">invalid bug</link> for more information.</para>
      </note>
      <para>For more information about image metadata, refer to the <link xmlns:xl="http://www.w3.org/1999/xlink" xl:href="https://docs.openstack.org/image-guide/image-metadata.html">Image metadata</link>
                guide.</para>
    </section>
    <section xml:id="customizing-instance-cpu-topologies">
      <title>Customizing instance CPU topologies</title>
      <important>
        <para>The functionality described below is currently only supported by the
                    libvirt/KVM driver.</para>
      </important>
      <para>In addition to configuring how an instance is scheduled on host CPUs, it is
                possible to configure how CPUs are represented in the instance itself. By
                default, when instance NUMA placement is not specified, a topology of N
                sockets, each with one core and one thread, is used for an instance, where N
                corresponds to the number of instance vCPUs requested. When instance NUMA
                placement is specified, the number of sockets is fixed to the number of host
                NUMA nodes to use and the total number of instance CPUs is split over these
                sockets.</para>
      <para>Some workloads benefit from a custom topology. For example, in some operating
                systems, a different license may be needed depending on the number of CPU
                sockets. To configure a flavor to use a maximum of two sockets, run:</para>
      <screen language="console">$ openstack flavor set m1.large --property hw:cpu_sockets=2</screen>
      <para>Similarly, to configure a flavor to use one core and one thread, run:</para>
      <screen language="console">$ openstack flavor set m1.large \
  --property hw:cpu_cores=1 \
  --property hw:cpu_threads=1</screen>
      <important>
        <para>If specifying all values, the product of sockets multiplied by cores
                    multiplied by threads must equal the number of instance vCPUs. If specifying
                    any one of these values or the multiple of two values, the values must be a
                    factor of the number of instance vCPUs to prevent an exception. For example,
                    specifying <literal>hw:cpu_sockets=2</literal> on a host with an odd number of cores fails.
                    Similarly, specifying <literal>hw:cpu_cores=2</literal> and <literal>hw:cpu_threads=4</literal> on a host
                    with ten cores fails.</para>
      </important>
      <para>For more information about the syntax for <literal>hw:cpu_sockets</literal>, <literal>hw:cpu_cores</literal>
                and <literal>hw:cpu_threads</literal>, refer to the <xref linkend="flavors"/> guide.</para>
      <para>It is also possible to set upper limits on the number of sockets, cores, and
                threads used. Unlike the hard values above, it is not necessary for this exact
                number to used because it only provides a limit. This can be used to provide
                some flexibility in scheduling, while ensuring certains limits are not
                exceeded. For example, to ensure no more than two sockets are defined in the
                instance topology, run:</para>
      <screen language="console">$ openstack flavor set m1.large --property=hw:cpu_max_sockets=2</screen>
      <para>For more information about the syntax for <literal>hw:cpu_max_sockets</literal>,
                <literal>hw:cpu_max_cores</literal>, and <literal>hw:cpu_max_threads</literal>, refer to the
                <xref linkend="flavors"/> guide.</para>
      <para>Applications are frequently packaged as images. For applications that prefer
                certain CPU topologies, configure image metadata to hint that created instances
                should have a given topology regardless of flavor. To configure an image to
                request a two-socket, four-core per socket topology, run:</para>
      <screen language="console">$ openstack image set [IMAGE_ID] \
  --property hw_cpu_sockets=2 \
  --property hw_cpu_cores=4</screen>
      <para>To constrain instances to a given limit of sockets, cores or threads, use the
                <literal>max_</literal> variants. To configure an image to have a maximum of two sockets and a
                maximum of one thread, run:</para>
      <screen language="console">$ openstack image set [IMAGE_ID] \
  --property hw_cpu_max_sockets=2 \
  --property hw_cpu_max_threads=1</screen>
      <para>The value specified in the flavor is treated as the abolute limit.  The image
                limits are not permitted to exceed the flavor limits, they can only be equal
                to or lower than what the flavor defines. By setting a <literal>max</literal> value for
                sockets, cores, or threads, administrators can prevent users configuring
                topologies that might, for example, incur an additional licensing fees.</para>
      <para>For more information about image metadata, refer to the <link xmlns:xl="http://www.w3.org/1999/xlink" xl:href="https://docs.openstack.org/image-guide/image-metadata.html">Image metadata</link>
                guide.</para>
    </section>
  </chapter>
  <chapter xml:id="compute-service-node-firewall-requirements" xml:base="default-ports">
    <title>Compute service node firewall requirements</title>
    <para>Console connections for virtual machines, whether direct or through a
            proxy, are received on ports <literal>5900</literal> to <literal>5999</literal>. The firewall on each
            Compute service node must allow network traffic on these ports.</para>
    <para>This procedure modifies the iptables firewall to allow incoming
            connections to the Compute services.</para>
    <para>
      <emphasis role="bold">Configuring the service-node firewall</emphasis>
    </para>
    <procedure>
      <step>
        <para>Log in to the server that hosts the Compute service, as root.</para>
      </step>
      <step>
        <para>Edit the <literal>/etc/sysconfig/iptables</literal> file, to add an INPUT rule that
                    allows TCP traffic on ports from <literal>5900</literal> to <literal>5999</literal>. Make sure the new
                    rule appears before any INPUT rules that REJECT traffic:</para>
        <screen language="console">-A INPUT -p tcp -m multiport --dports 5900:5999 -j ACCEPT</screen>
      </step>
      <step>
        <para>Save the changes to the <literal>/etc/sysconfig/iptables</literal> file, and restart the
                    <literal>iptables</literal> service to pick up the changes:</para>
        <screen language="console">$ service iptables restart</screen>
      </step>
      <step>
        <para>Repeat this process for each Compute service node.</para>
      </step>
    </procedure>
  </chapter>
  <chapter xml:id="managing-the-cloud-with-euca2ools" xml:base="euca2ools">
    <title>Managing the cloud with euca2ools</title>
    <para>The <literal>euca2ools</literal> command-line tool provides a command line interface to EC2
            API calls. For more information, see the <link xmlns:xl="http://www.w3.org/1999/xlink" xl:href="http://docs.hpcloud.com/eucalyptus/">Official Eucalyptus Documentation</link>.</para>
  </chapter>
  <chapter xml:id="evacuate-instances" xml:base="evacuate">
    <title>Evacuate instances</title>
    <para>If a hardware malfunction or other error causes a cloud compute node to fail,
            you can evacuate instances to make them available again. You can optionally
            include the target host on the <command>nova evacuate</command> command. If you omit
            the host, the scheduler chooses the target host.</para>
    <para>To preserve user data on the server disk, configure shared storage on the
            target host. When you evacuate the instance, Compute detects whether shared
            storage is available on the target host. Also, you must validate that the
            current VM host is not operational. Otherwise, the evacuation fails.</para>
    <procedure>
      <step>
        <para>To find a host for the evacuated instance, list all hosts:</para>
        <screen language="console">$ openstack host list</screen>
      </step>
      <step>
        <para>Evacuate the instance. You can use the <literal>--password PWD</literal> option to pass the
                    instance password to the command. If you do not specify a password, the
                    command generates and prints one after it finishes successfully. The
                    following command evacuates a server from a failed host to <literal>HOST_B</literal>.</para>
        <screen language="console">$ nova evacuate EVACUATED_SERVER_NAME HOST_B</screen>
        <para>The command rebuilds the instance from the original image or volume and
                    returns a password. The command preserves the original configuration, which
                    includes the instance ID, name, uid, IP address, and so on.</para>
        <screen language="console">+-----------+--------------+
| Property  |    Value     |
+-----------+--------------+
| adminPass | kRAJpErnT4xZ |
+-----------+--------------+</screen>
      </step>
      <step>
        <para>To preserve the user disk data on the evacuated server, deploy Compute with
                    a shared file system. To configure your system, see
                    <xref linkend="section-configuring-compute-migrations"/>.  The following example does
                    not change the password.</para>
        <screen language="console">$ nova evacuate EVACUATED_SERVER_NAME HOST_B --on-shared-storage</screen>
      </step>
    </procedure>
  </chapter>
  <chapter xml:id="manage-flavors" xml:base="flavors2">
    <title>Manage flavors</title>
    <para>In OpenStack, flavors define the compute, memory, and storage capacity of nova
            computing instances. To put it simply, a flavor is an available hardware
            configuration for a server. It defines the <emphasis>size</emphasis> of a virtual server that can
            be launched.</para>
    <note>
      <para>Flavors can also determine on which compute host a flavor can be used to
                launch an instance. For information about customizing flavors, refer to
                <xref linkend="flavors"/>.</para>
    </note>
    <para>A flavor consists of the following parameters:</para>
    <variablelist>
      <varlistentry>
        <term>Flavor ID</term>
        <listitem>
          <para>Unique ID (integer or UUID) for the new flavor. If specifying 'auto', a UUID
                        will be automatically generated.</para>
        </listitem>
      </varlistentry>
      <varlistentry>
        <term>Name</term>
        <listitem>
          <para>Name for the new flavor.</para>
        </listitem>
      </varlistentry>
      <varlistentry>
        <term>VCPUs</term>
        <listitem>
          <para>Number of virtual CPUs to use.</para>
        </listitem>
      </varlistentry>
      <varlistentry>
        <term>Memory MB</term>
        <listitem>
          <para>Amount of RAM to use (in megabytes).</para>
        </listitem>
      </varlistentry>
      <varlistentry>
        <term>Root Disk GB</term>
        <listitem>
          <para>Amount of disk space (in gigabytes) to use for the root (<literal>/</literal>) partition.
                        This property is required.</para>
          <para>The root disk is an ephemeral disk that the base image is copied into. When
                        booting from a persistent volume it is not used. The <literal>0</literal> size is a special
                        case which uses the native base image size as the size of the ephemeral root
                        volume. However, in this case the filter scheduler cannot select the compute
                        host based on the virtual image size. As a result, <literal>0</literal> should only be used
                        for volume booted instances or for testing purposes. Volume-backed instances
                        can be enforced for flavors with zero root disk via the
                        <literal>os_compute_api:servers:create:zero_disk_flavor</literal> policy rule.</para>
        </listitem>
      </varlistentry>
      <varlistentry>
        <term>Ephemeral Disk GB</term>
        <listitem>
          <para>Amount of disk space (in gigabytes) to use for the ephemeral partition. If
                        unspecified, the value is <literal>0</literal> by default.  Ephemeral disks offer machine
                        local disk storage linked to the lifecycle of a VM instance. When a VM is
                        terminated, all data on the ephemeral disk is lost. Ephemeral disks are not
                        included in any snapshots.</para>
        </listitem>
      </varlistentry>
      <varlistentry>
        <term>Swap</term>
        <listitem>
          <para>Amount of swap space (in megabytes) to use. If unspecified, the value is
                        <literal>0</literal> by default.</para>
        </listitem>
      </varlistentry>
      <varlistentry>
        <term>RXTX Factor</term>
        <listitem>
          <para>Optional property that allows servers with a different bandwidth be created
                        with the RXTX Factor. The default value is <literal>1.0</literal>. That is, the new
                        bandwidth is the same as that of the attached network. The RXTX Factor is
                        available only for Xen or NSX based systems.</para>
        </listitem>
      </varlistentry>
      <varlistentry>
        <term>Is Public</term>
        <listitem>
          <para>Boolean value defines whether the flavor is available to all users.  Defaults
                        to <literal>True</literal>.</para>
        </listitem>
      </varlistentry>
      <varlistentry>
        <term>Extra Specs</term>
        <listitem>
          <para>Key and value pairs that define on which compute nodes a flavor can run.
                        These pairs must match corresponding pairs on the compute nodes. It can be
                        used to implement special resources, such as flavors that run on only compute
                        nodes with GPU hardware.</para>
        </listitem>
      </varlistentry>
    </variablelist>
    <para>As of Newton, there are no default flavors.  The following table lists the
            default flavors for Mitaka and earlier.</para>
    <informaltable>
      <tgroup cols="4">
        <colspec colname="c0" colwidth="12"/>
        <colspec colname="c1" colwidth="9"/>
        <colspec colname="c2" colwidth="15"/>
        <colspec colname="c3" colwidth="15"/>
        <thead>
          <row>
            <entry>
              <para>Flavor</para>
            </entry>
            <entry>
              <para>VCPUs</para>
            </entry>
            <entry>
              <para>Disk (in GB)</para>
            </entry>
            <entry>
              <para>RAM (in MB)</para>
            </entry>
          </row>
        </thead>
        <tbody>
          <row>
            <entry>
              <para>m1.tiny</para>
            </entry>
            <entry>
              <para>1</para>
            </entry>
            <entry>
              <para>1</para>
            </entry>
            <entry>
              <para>512</para>
            </entry>
          </row>
          <row>
            <entry>
              <para>m1.small</para>
            </entry>
            <entry>
              <para>1</para>
            </entry>
            <entry>
              <para>20</para>
            </entry>
            <entry>
              <para>2048</para>
            </entry>
          </row>
          <row>
            <entry>
              <para>m1.medium</para>
            </entry>
            <entry>
              <para>2</para>
            </entry>
            <entry>
              <para>40</para>
            </entry>
            <entry>
              <para>4096</para>
            </entry>
          </row>
          <row>
            <entry>
              <para>m1.large</para>
            </entry>
            <entry>
              <para>4</para>
            </entry>
            <entry>
              <para>80</para>
            </entry>
            <entry>
              <para>8192</para>
            </entry>
          </row>
          <row>
            <entry>
              <para>m1.xlarge</para>
            </entry>
            <entry>
              <para>8</para>
            </entry>
            <entry>
              <para>160</para>
            </entry>
            <entry>
              <para>16384</para>
            </entry>
          </row>
        </tbody>
      </tgroup>
    </informaltable>
    <para>You can create and manage flavors with the <command>openstack flavor</command> commands
            provided by the <literal>python-openstackclient</literal> package.</para>
    <section xml:id="create-a-flavor">
      <title>Create a flavor</title>
      <procedure>
        <step>
          <para>List flavors to show the ID and name, the amount of memory, the amount of
                        disk space for the root partition and for the ephemeral partition, the swap,
                        and the number of virtual CPUs for each flavor:</para>
          <screen language="console">$ openstack flavor list</screen>
        </step>
        <step>
          <para>To create a flavor, specify a name, ID, RAM size, disk size, and the number
                        of VCPUs for the flavor, as follows:</para>
          <screen language="console">$ openstack flavor create FLAVOR_NAME --id FLAVOR_ID \
    --ram RAM_IN_MB --disk ROOT_DISK_IN_GB --vcpus NUMBER_OF_VCPUS</screen>
          <note>
            <para>Unique ID (integer or UUID) for the new flavor. If specifying 'auto', a
                            UUID will be automatically generated.</para>
          </note>
          <para>Here is an example with additional optional parameters filled in that
                        creates a public <literal>extra_tiny</literal> flavor that automatically gets an ID
                        assigned, with 256 MB memory, no disk space, and one VCPU. The rxtx-factor
                        indicates the slice of bandwidth that the instances with this flavor can use
                        (through the Virtual Interface (vif) creation in the hypervisor):</para>
          <screen language="console">$ openstack flavor create --public m1.extra_tiny --id auto \
    --ram 256 --disk 0 --vcpus 1 --rxtx-factor 1</screen>
        </step>
        <step>
          <para>If an individual user or group of users needs a custom flavor that you do
                        not want other projects to have access to, you can change the flavor's
                        access to make it a private flavor.  See <link xmlns:xl="http://www.w3.org/1999/xlink" xl:href="https://docs.openstack.org/ops-guide/ops-user-facing-operations.html#private-flavors">Private Flavors in the OpenStack
                            Operations Guide</link>.</para>
          <para>For a list of optional parameters, run this command:</para>
          <screen language="console">$ openstack help flavor create</screen>
        </step>
        <step>
          <para>After you create a flavor, assign it to a project by specifying the flavor
                        name or ID and the project ID:</para>
          <screen language="console">$ nova flavor-access-add FLAVOR TENANT_ID</screen>
        </step>
        <step>
          <para>In addition, you can set or unset <literal>extra_spec</literal> for the existing flavor.
                        The <literal>extra_spec</literal> metadata keys can influence the instance directly when it
                        is launched. If a flavor sets the <literal>extra_spec key/value
quota:vif_outbound_peak=65536</literal>, the instance's outbound peak bandwidth I/O
                        should be less than or equal to 512 Mbps. There are several aspects that can
                        work for an instance including <emphasis>CPU limits</emphasis>, <emphasis>Disk tuning</emphasis>, <emphasis>Bandwidth I/O</emphasis>,
                        <emphasis>Watchdog behavior</emphasis>, and <emphasis>Random-number generator</emphasis>.  For information about
                        supporting metadata keys, see <xref linkend="flavors"/>.</para>
          <para>For a list of optional parameters, run this command:</para>
          <screen language="console">$ nova help flavor-key</screen>
        </step>
      </procedure>
    </section>
    <section xml:id="delete-a-flavor">
      <title>Delete a flavor</title>
      <para>Delete a specified flavor, as follows:</para>
      <screen language="console">$ openstack flavor delete FLAVOR_ID</screen>
    </section>
  </chapter>
  <chapter xml:id="flavors" xml:base="flavors">
    <title>Flavors</title>
    <para>Admin users can use the <command>openstack flavor</command> command to customize and
            manage flavors. To see information for this command, run:</para>
    <screen language="console">$ openstack flavor --help
Command "flavor" matches:
  flavor create
  flavor delete
  flavor list
  flavor set
  flavor show
  flavor unset</screen>
    <note>
      <para>Configuration rights can be delegated to additional users by redefining
                the access controls for <literal>os_compute_api:os-flavor-manage</literal> in
                <literal>/etc/nova/policy.json</literal> on the <literal>nova-api</literal> server.</para>
    </note>
    <para>Flavors define these elements:</para>
    <informaltable xml:id="index-0">
      <tgroup cols="2">
        <colspec colname="c0" colwidth="13"/>
        <colspec colname="c1" colwidth="63"/>
        <thead>
          <row>
            <entry>
              <para>Element</para>
            </entry>
            <entry>
              <para>Description</para>
            </entry>
          </row>
        </thead>
        <tbody>
          <row>
            <entry>
              <para>Name</para>
            </entry>
            <entry>
              <para>A descriptive name. XX.SIZE_NAME is typically not required,
                                though some third party tools may rely on it.</para>
            </entry>
          </row>
          <row>
            <entry>
              <para>Memory MB</para>
            </entry>
            <entry>
              <para>Instance memory in megabytes.</para>
            </entry>
          </row>
          <row>
            <entry>
              <para>Disk</para>
            </entry>
            <entry>
              <para>Virtual root disk size in gigabytes. This is an ephemeral disk that the base image is copied into. When booting from a persistent volume it is not used. The "0" size is a special case which uses the native base image size as the size of the
                                ephemeral root volume. However, in this case the filter
                                scheduler cannot select the compute host based on the virtual
                                image size. Therefore 0 should only be used for volume booted
                                instances or for testing purposes.</para>
            </entry>
          </row>
          <row>
            <entry>
              <para>Ephemeral</para>
            </entry>
            <entry>
              <para>Specifies the size of a secondary ephemeral data disk. This
                                is an empty, unformatted disk and exists only for the life of the instance. Default value is <literal>0</literal>.</para>
            </entry>
          </row>
          <row>
            <entry>
              <para>Swap</para>
            </entry>
            <entry>
              <para>Optional swap space allocation for the instance. Default
                                value is <literal>0</literal>.</para>
            </entry>
          </row>
          <row>
            <entry>
              <para>VCPUs</para>
            </entry>
            <entry>
              <para>Number of virtual CPUs presented to the instance.</para>
            </entry>
          </row>
          <row>
            <entry>
              <para>RXTX Factor</para>
            </entry>
            <entry>
              <para>Optional property allows created servers to have a different
                                bandwidth cap than that defined in the network they are attached to. This factor is multiplied by the rxtx_base property of the network. Default value is <literal>1.0</literal>. That is, the same
                                as attached network. This parameter is only available for Xen
                                or NSX based systems.</para>
            </entry>
          </row>
          <row>
            <entry>
              <para>Is Public</para>
            </entry>
            <entry>
              <para>Boolean value, whether flavor is available to all users or private to the project it was created in. Defaults to <literal>True</literal>.</para>
            </entry>
          </row>
          <row>
            <entry>
              <para>Extra Specs</para>
            </entry>
            <entry>
              <para>Key and value pairs that define on which compute nodes a flavor can run. These pairs must match corresponding pairs on the compute nodes. Use to implement special resources, such as flavors that run on only compute nodes with GPU hardware.</para>
            </entry>
          </row>
        </tbody>
      </tgroup>
    </informaltable>
    <note>
      <para>Flavor customization can be limited by the hypervisor in use. For example
                the libvirt driver enables quotas on CPUs available to a VM, disk tuning,
                bandwidth I/O, watchdog behavior, random number generator device control,
                and instance VIF traffic control.</para>
    </note>
    <section xml:id="is-public">
      <title>Is Public</title>
      <para>Flavors can be assigned to particular projects. By default, a flavor is public
                and available to all projects. Private flavors are only accessible to those on
                the access list and are invisible to other projects. To create and assign a
                private flavor to a project, run this command:</para>
      <screen language="console">$ openstack flavor create --private p1.medium --id auto --ram 512 --disk 40 --vcpus 4</screen>
    </section>
    <section xml:id="extra-specs">
      <title>Extra Specs</title>
      <variablelist>
        <varlistentry>
          <term>CPU limits</term>
          <listitem>
            <para>You can configure the CPU limits with control parameters with the <literal>nova</literal>
                            client. For example, to configure the I/O limit, use:</para>
            <screen language="console">$ openstack flavor set FLAVOR-NAME \
    --property quota:read_bytes_sec=10240000 \
    --property quota:write_bytes_sec=10240000</screen>
            <para>Use these optional parameters to control weight shares, enforcement intervals
                            for runtime quotas, and a quota for maximum allowed bandwidth:</para>
            <itemizedlist>
              <listitem>
                <para><literal>cpu_shares</literal>: Specifies the proportional weighted share for the domain.
                                    If this element is omitted, the service defaults to the OS provided
                                    defaults. There is no unit for the value; it is a relative measure based on
                                    the setting of other VMs. For example, a VM configured with value 2048 gets
                                    twice as much CPU time as a VM configured with value 1024.</para>
              </listitem>
              <listitem>
                <para><literal>cpu_shares_level</literal>: On VMware, specifies the allocation level. Can be
                                    <literal>custom</literal>, <literal>high</literal>, <literal>normal</literal>, or <literal>low</literal>. If you choose <literal>custom</literal>, set
                                    the number of shares using <literal>cpu_shares_share</literal>.</para>
              </listitem>
              <listitem>
                <para><literal>cpu_period</literal>: Specifies the enforcement interval (unit: microseconds)
                                    for QEMU and LXC hypervisors. Within a period, each VCPU of the domain is
                                    not allowed to consume more than the quota worth of runtime. The value
                                    should be in range <literal>[1000, 1000000]</literal>.  A period with value 0 means no
                                    value.</para>
              </listitem>
              <listitem>
                <para><literal>cpu_limit</literal>: Specifies the upper limit for VMware machine CPU allocation
                                    in MHz. This parameter ensures that a machine never uses more than the
                                    defined amount of CPU time. It can be used to enforce a limit on the
                                    machine's CPU performance.</para>
              </listitem>
              <listitem>
                <para><literal>cpu_reservation</literal>: Specifies the guaranteed minimum CPU reservation in
                                    MHz for VMware. This means that if needed, the machine will definitely get
                                    allocated the reserved amount of CPU cycles.</para>
              </listitem>
              <listitem>
                <para><literal>cpu_quota</literal>: Specifies the maximum allowed bandwidth (unit:
                                    microseconds). A domain with a negative-value quota indicates that the
                                    domain has infinite bandwidth, which means that it is not bandwidth
                                    controlled. The value should be in range <literal>[1000, 18446744073709551]</literal> or
                                    less than 0. A quota with value 0 means no value. You can use this feature
                                    to ensure that all vCPUs run at the same speed. For example:</para>
                <screen language="console">$ openstack flavor set FLAVOR-NAME \
    --property quota:cpu_quota=10000 \
    --property quota:cpu_period=20000</screen>
                <para>In this example, an instance of <literal>FLAVOR-NAME</literal> can only consume a maximum
                                    of 50% CPU of a physical CPU computing capability.</para>
              </listitem>
            </itemizedlist>
          </listitem>
        </varlistentry>
        <varlistentry>
          <term>Memory limits</term>
          <listitem>
            <para>For VMware, you can configure the memory limits with control parameters.</para>
            <para>Use these optional parameters to limit the memory allocation, guarantee
                            minimum memory reservation, and to specify shares used in case of resource
                            contention:</para>
            <itemizedlist>
              <listitem>
                <para><literal>memory_limit</literal>: Specifies the upper limit for VMware machine memory
                                    allocation in MB. The utilization of a virtual machine will not exceed this
                                    limit, even if there are available resources. This is typically used to
                                    ensure a consistent performance of virtual machines independent of
                                    available resources.</para>
              </listitem>
              <listitem>
                <para><literal>memory_reservation</literal>: Specifies the guaranteed minimum memory reservation
                                    in MB for VMware. This means the specified amount of memory will definitely
                                    be allocated to the machine.</para>
              </listitem>
              <listitem>
                <para><literal>memory_shares_level</literal>: On VMware, specifies the allocation level.  This
                                    can be <literal>custom</literal>, <literal>high</literal>, <literal>normal</literal> or <literal>low</literal>. If you choose
                                    <literal>custom</literal>, set the number of shares using <literal>memory_shares_share</literal>.</para>
              </listitem>
              <listitem>
                <para><literal>memory_shares_share</literal>: Specifies the number of shares allocated in the
                                    event that <literal>custom</literal> is used. There is no unit for this value. It is a
                                    relative measure based on the settings for other VMs.  For example:</para>
                <screen language="console">$ openstack flavor set FLAVOR-NAME \
    --property quota:memory_shares_level=custom \
    --property quota:memory_shares_share=15</screen>
              </listitem>
            </itemizedlist>
          </listitem>
        </varlistentry>
        <varlistentry>
          <term>Disk I/O limits</term>
          <listitem>
            <para>For VMware, you can configure the resource limits for disk with control
                            parameters.</para>
            <para>Use these optional parameters to limit the disk utilization, guarantee disk
                            allocation, and to specify shares used in case of resource contention. This
                            allows the VMware driver to enable disk allocations for the running instance.</para>
            <itemizedlist>
              <listitem>
                <para><literal>disk_io_limit</literal>: Specifies the upper limit for disk utilization in I/O
                                    per second. The utilization of a virtual machine will not exceed this
                                    limit, even if there are available resources. The default value is -1 which
                                    indicates unlimited usage.</para>
              </listitem>
              <listitem>
                <para><literal>disk_io_reservation</literal>: Specifies the guaranteed minimum disk allocation
                                    in terms of Input/output Operations Per Second (IOPS).</para>
              </listitem>
              <listitem>
                <para><literal>disk_io_shares_level</literal>: Specifies the allocation level. This can be
                                    <literal>custom</literal>, <literal>high</literal>, <literal>normal</literal> or <literal>low</literal>.  If you choose custom, set the
                                    number of shares using <literal>disk_io_shares_share</literal>.</para>
              </listitem>
              <listitem>
                <para><literal>disk_io_shares_share</literal>: Specifies the number of shares allocated in the
                                    event that <literal>custom</literal> is used.  When there is resource contention, this
                                    value is used to determine the resource allocation.</para>
                <para>The example below sets the <literal>disk_io_reservation</literal> to 2000 IOPS.</para>
                <screen language="console">$ openstack flavor set FLAVOR-NAME \
    --property quota:disk_io_reservation=2000</screen>
              </listitem>
            </itemizedlist>
          </listitem>
        </varlistentry>
        <varlistentry>
          <term>Disk tuning</term>
          <listitem>
            <para>Using disk I/O quotas, you can set maximum disk write to 10 MB per second for
                            a VM user. For example:</para>
            <screen language="console">$ openstack flavor set FLAVOR-NAME \
    --property quota:disk_write_bytes_sec=10485760</screen>
            <para>The disk I/O options are:</para>
            <itemizedlist>
              <listitem>
                <para>
                  <literal>disk_read_bytes_sec</literal>
                </para>
              </listitem>
              <listitem>
                <para>
                  <literal>disk_read_iops_sec</literal>
                </para>
              </listitem>
              <listitem>
                <para>
                  <literal>disk_write_bytes_sec</literal>
                </para>
              </listitem>
              <listitem>
                <para>
                  <literal>disk_write_iops_sec</literal>
                </para>
              </listitem>
              <listitem>
                <para>
                  <literal>disk_total_bytes_sec</literal>
                </para>
              </listitem>
              <listitem>
                <para>
                  <literal>disk_total_iops_sec</literal>
                </para>
              </listitem>
            </itemizedlist>
          </listitem>
        </varlistentry>
        <varlistentry>
          <term>Bandwidth I/O</term>
          <listitem>
            <para>The vif I/O options are:</para>
            <itemizedlist>
              <listitem>
                <para>
                  <literal>vif_inbound_average</literal>
                </para>
              </listitem>
              <listitem>
                <para>
                  <literal>vif_inbound_burst</literal>
                </para>
              </listitem>
              <listitem>
                <para>
                  <literal>vif_inbound_peak</literal>
                </para>
              </listitem>
              <listitem>
                <para>
                  <literal>vif_outbound_average</literal>
                </para>
              </listitem>
              <listitem>
                <para>
                  <literal>vif_outbound_burst</literal>
                </para>
              </listitem>
              <listitem>
                <para>
                  <literal>vif_outbound_peak</literal>
                </para>
              </listitem>
            </itemizedlist>
            <para>Incoming and outgoing traffic can be shaped independently. The bandwidth
                            element can have at most, one inbound and at most, one outbound child
                            element. If you leave any of these child elements out, no quality of service
                            (QoS) is applied on that traffic direction. So, if you want to shape only the
                            network's incoming traffic, use inbound only (and vice versa). Each element
                            has one mandatory attribute average, which specifies the average bit rate on
                            the interface being shaped.</para>
            <para>There are also two optional attributes (integer): <literal>peak</literal>, which specifies
                            the maximum rate at which a bridge can send data (kilobytes/second), and
                            <literal>burst</literal>, the amount of bytes that can be burst at peak speed (kilobytes).
                            The rate is shared equally within domains connected to the network.</para>
            <para>The example below sets network traffic bandwidth limits for existing flavor
                            as follows:</para>
            <itemizedlist>
              <listitem>
                <para>Outbound traffic:</para>
                <itemizedlist>
                  <listitem>
                    <para>average: 262 Mbps (32768 kilobytes/second)</para>
                  </listitem>
                  <listitem>
                    <para>peak: 524 Mbps (65536 kilobytes/second)</para>
                  </listitem>
                  <listitem>
                    <para>burst: 65536 kilobytes</para>
                  </listitem>
                </itemizedlist>
              </listitem>
              <listitem>
                <para>Inbound traffic:</para>
                <itemizedlist>
                  <listitem>
                    <para>average: 262 Mbps (32768 kilobytes/second)</para>
                  </listitem>
                  <listitem>
                    <para>peak: 524 Mbps (65536 kilobytes/second)</para>
                  </listitem>
                  <listitem>
                    <para>burst: 65536 kilobytes</para>
                  </listitem>
                </itemizedlist>
              </listitem>
            </itemizedlist>
            <screen language="console">$ openstack flavor set FLAVOR-NAME \
    --property quota:vif_outbound_average=32768 \
    --property quota:vif_outbound_peak=65536 \
    --property quota:vif_outbound_burst=65536 \
    --property quota:vif_inbound_average=32768 \
    --property quota:vif_inbound_peak=65536 \
    --property quota:vif_inbound_burst=65536</screen>
            <note>
              <para>All the speed limit values in above example are specified in
                                kilobytes/second. And burst values are in kilobytes. Values were converted
                                using 'Data rate units on Wikipedia
                                &lt;<link xmlns:xl="http://www.w3.org/1999/xlink" xl:href="https://en.wikipedia.org/wiki/Data_rate_units"/>&gt;`_.</para>
            </note>
          </listitem>
        </varlistentry>
        <varlistentry>
          <term>Watchdog behavior</term>
          <listitem>
            <para>For the libvirt driver, you can enable and set the behavior of a virtual
                            hardware watchdog device for each flavor. Watchdog devices keep an eye on the
                            guest server, and carry out the configured action, if the server hangs. The
                            watchdog uses the i6300esb device (emulating a PCI Intel 6300ESB). If
                            <literal>hw:watchdog_action</literal> is not specified, the watchdog is disabled.</para>
            <para>To set the behavior, use:</para>
            <screen language="console">$ openstack flavor set FLAVOR-NAME --property hw:watchdog_action=ACTION</screen>
            <para>Valid ACTION values are:</para>
            <itemizedlist>
              <listitem>
                <para><literal>disabled</literal>: (default) The device is not attached.</para>
              </listitem>
              <listitem>
                <para><literal>reset</literal>: Forcefully reset the guest.</para>
              </listitem>
              <listitem>
                <para><literal>poweroff</literal>: Forcefully power off the guest.</para>
              </listitem>
              <listitem>
                <para><literal>pause</literal>: Pause the guest.</para>
              </listitem>
              <listitem>
                <para><literal>none</literal>: Only enable the watchdog; do nothing if the server hangs.</para>
              </listitem>
            </itemizedlist>
            <note>
              <para>Watchdog behavior set using a specific image's properties will override
                                behavior set using flavors.</para>
            </note>
          </listitem>
        </varlistentry>
        <varlistentry>
          <term>Random-number generator</term>
          <listitem>
            <para>If a random-number generator device has been added to the instance through
                            its image properties, the device can be enabled and configured using:</para>
            <screen language="console">$ openstack flavor set FLAVOR-NAME \
    --property hw_rng:allowed=True \
    --property hw_rng:rate_bytes=RATE-BYTES \
    --property hw_rng:rate_period=RATE-PERIOD</screen>
            <para>Where:</para>
            <itemizedlist>
              <listitem>
                <para>RATE-BYTES: (integer) Allowed amount of bytes that the guest can read from
                                    the host's entropy per period.</para>
              </listitem>
              <listitem>
                <para>RATE-PERIOD: (integer) Duration of the read period in seconds.</para>
              </listitem>
            </itemizedlist>
          </listitem>
        </varlistentry>
        <varlistentry>
          <term>CPU topology</term>
          <listitem>
            <para>For the libvirt driver, you can define the topology of the processors in the
                            virtual machine using properties. The properties with <literal>max</literal> limit the
                            number that can be selected by the user with image properties.</para>
            <screen language="console">$ openstack flavor set FLAVOR-NAME \
    --property hw:cpu_sockets=FLAVOR-SOCKETS \
    --property hw:cpu_cores=FLAVOR-CORES \
    --property hw:cpu_threads=FLAVOR-THREADS \
    --property hw:cpu_max_sockets=FLAVOR-SOCKETS \
    --property hw:cpu_max_cores=FLAVOR-CORES \
    --property hw:cpu_max_threads=FLAVOR-THREADS</screen>
            <para>Where:</para>
            <itemizedlist>
              <listitem>
                <para>FLAVOR-SOCKETS: (integer) The number of sockets for the guest VM. By
                                    default, this is set to the number of vCPUs requested.</para>
              </listitem>
              <listitem>
                <para>FLAVOR-CORES: (integer) The number of cores per socket for the guest VM. By
                                    default, this is set to <literal>1</literal>.</para>
              </listitem>
              <listitem>
                <para>FLAVOR-THREADS: (integer) The number of threads per core for the guest VM.
                                    By default, this is set to <literal>1</literal>.</para>
              </listitem>
            </itemizedlist>
          </listitem>
        </varlistentry>
        <varlistentry>
          <term>CPU pinning policy</term>
          <listitem>
            <para>For the libvirt driver, you can pin the virtual CPUs (vCPUs) of instances to
                            the host's physical CPU cores (pCPUs) using properties. You can further
                            refine this by stating how hardware CPU threads in a simultaneous
                            multithreading-based (SMT) architecture be used. These configurations will
                            result in improved per-instance determinism and performance.</para>
            <note>
              <para>SMT-based architectures include Intel processors with Hyper-Threading
                                technology. In these architectures, processor cores share a number of
                                components with one or more other cores. Cores in such architectures are
                                commonly referred to as hardware threads, while the cores that a given
                                core share components with are known as thread siblings.</para>
            </note>
            <note>
              <para>Host aggregates should be used to separate these pinned instances from
                                unpinned instances as the latter will not respect the resourcing
                                requirements of the former.</para>
            </note>
            <screen>$ openstack flavor set FLAVOR-NAME \
    --property hw:cpu_policy=CPU-POLICY \
    --property hw:cpu_thread_policy=CPU-THREAD-POLICY</screen>
            <para>Valid CPU-POLICY values are:</para>
            <itemizedlist>
              <listitem>
                <para><literal>shared</literal>: (default) The guest vCPUs will be allowed to freely float
                                    across host pCPUs, albeit potentially constrained by NUMA policy.</para>
              </listitem>
              <listitem>
                <para><literal>dedicated</literal>: The guest vCPUs will be strictly pinned to a set of host
                                    pCPUs. In the absence of an explicit vCPU topology request, the drivers
                                    typically expose all vCPUs as sockets with one core and one thread.  When
                                    strict CPU pinning is in effect the guest CPU topology will be setup to
                                    match the topology of the CPUs to which it is pinned. This option implies
                                    an overcommit ratio of 1.0. For example, if a two vCPU guest is pinned to a
                                    single host core with two threads, then the guest will get a topology of
                                    one socket, one core, two threads.</para>
              </listitem>
            </itemizedlist>
            <para>Valid CPU-THREAD-POLICY values are:</para>
            <itemizedlist>
              <listitem>
                <para><literal>prefer</literal>: (default) The host may or may not have an SMT architecture.
                                    Where an SMT architecture is present, thread siblings are preferred.</para>
              </listitem>
              <listitem>
                <para><literal>isolate</literal>: The host must not have an SMT architecture or must emulate a
                                    non-SMT architecture. If the host does not have an SMT architecture, each
                                    vCPU is placed on a different core as expected. If the host does have an
                                    SMT architecture - that is, one or more cores have thread siblings - then
                                    each vCPU is placed on a different physical core. No vCPUs from other
                                    guests are placed on the same core. All but one thread sibling on each
                                    utilized core is therefore guaranteed to be unusable.</para>
              </listitem>
              <listitem>
                <para><literal>require</literal>: The host must have an SMT architecture. Each vCPU is allocated
                                    on thread siblings. If the host does not have an SMT architecture, then it
                                    is not used. If the host has an SMT architecture, but not enough cores with
                                    free thread siblings are available, then scheduling fails.</para>
              </listitem>
            </itemizedlist>
            <note>
              <para>The <literal>hw:cpu_thread_policy</literal> option is only valid if <literal>hw:cpu_policy</literal> is
                                set to <literal>dedicated</literal>.</para>
            </note>
          </listitem>
        </varlistentry>
      </variablelist>
      <variablelist>
        <varlistentry>
          <term>NUMA topology</term>
          <listitem>
            <para>For the libvirt driver, you can define the host NUMA placement for the
                            instance vCPU threads as well as the allocation of instance vCPUs and memory
                            from the host NUMA nodes. For flavors whose memory and vCPU allocations are
                            larger than the size of NUMA nodes in the compute hosts, the definition of a
                            NUMA topology allows hosts to better utilize NUMA and improve performance of
                            the instance OS.</para>
            <screen language="console">$ openstack flavor set FLAVOR-NAME \
    --property hw:numa_nodes=FLAVOR-NODES \
    --property hw:numa_cpus.N=FLAVOR-CORES \
    --property hw:numa_mem.N=FLAVOR-MEMORY</screen>
            <para>Where:</para>
            <itemizedlist>
              <listitem>
                <para>FLAVOR-NODES: (integer) The number of host NUMA nodes to restrict execution
                                    of instance vCPU threads to. If not specified, the vCPU threads can run on
                                    any number of the host NUMA nodes available.</para>
              </listitem>
              <listitem>
                <para>N: (integer) The instance NUMA node to apply a given CPU or memory
                                    configuration to, where N is in the range <literal>0</literal> to <literal>FLAVOR-NODES - 1</literal>.</para>
              </listitem>
              <listitem>
                <para>FLAVOR-CORES: (comma-separated list of integers) A list of instance vCPUs
                                    to map to instance NUMA node N. If not specified, vCPUs are evenly divided
                                    among available NUMA nodes.</para>
              </listitem>
              <listitem>
                <para>FLAVOR-MEMORY: (integer) The number of MB of instance memory to map to
                                    instance NUMA node N. If not specified, memory is evenly divided among
                                    available NUMA nodes.</para>
              </listitem>
            </itemizedlist>
            <note>
              <para><literal>hw:numa_cpus.N</literal> and <literal>hw:numa_mem.N</literal> are only valid if
                                <literal>hw:numa_nodes</literal> is set. Additionally, they are only required if the
                                instance's NUMA nodes have an asymmetrical allocation of CPUs and RAM
                                (important for some NFV workloads).</para>
            </note>
            <note>
              <para>The <literal>N</literal> parameter is an index of <emphasis>guest</emphasis> NUMA nodes and may not
                                correspond to <emphasis>host</emphasis> NUMA nodes. For example, on a platform with two NUMA
                                nodes, the scheduler may opt to place guest NUMA node 0, as referenced in
                                <literal>hw:numa_mem.0</literal> on host NUMA node 1 and vice versa.  Similarly, the
                                integers used for <literal>FLAVOR-CORES</literal> are indexes of <emphasis>guest</emphasis> vCPUs and may
                                not correspond to <emphasis>host</emphasis> CPUs. As such, this feature cannot be used to
                                constrain instances to specific host CPUs or NUMA nodes.</para>
            </note>
            <warning>
              <para>If the combined values of <literal>hw:numa_cpus.N</literal> or <literal>hw:numa_mem.N</literal> are
                                greater than the available number of CPUs or memory respectively, an
                                exception is raised.</para>
            </warning>
          </listitem>
        </varlistentry>
        <varlistentry>
          <term>Large pages allocation</term>
          <listitem>
            <para>You can configure the size of large pages used to back the VMs.</para>
            <screen>$ openstack flavor set FLAVOR-NAME \
    --property hw:mem_page_size=PAGE_SIZE</screen>
            <para>Valid <literal>PAGE_SIZE</literal> values are:</para>
            <itemizedlist>
              <listitem>
                <para><literal>small</literal>: (default) The smallest page size is used. Example: 4 KB on x86.</para>
              </listitem>
              <listitem>
                <para><literal>large</literal>: Only use larger page sizes for guest RAM. Example: either 2 MB
                                    or 1 GB on x86.</para>
              </listitem>
              <listitem>
                <para><literal>any</literal>: It is left up to the compute driver to decide. In this case, the
                                    libvirt driver might try to find large pages, but fall back to small pages.
                                    Other drivers may choose alternate policies for <literal>any</literal>.</para>
              </listitem>
              <listitem>
                <para>pagesize: (string) An explicit page size can be set if the workload has
                                    specific requirements. This value can be an integer value for the page size
                                    in KB, or can use any standard suffix. Example: <literal>4KB</literal>, <literal>2MB</literal>,
                                    <literal>2048</literal>, <literal>1GB</literal>.</para>
              </listitem>
            </itemizedlist>
            <note>
              <para>Large pages can be enabled for guest RAM without any regard to whether the
                                guest OS will use them or not. If the guest OS chooses not to use huge
                                pages, it will merely see small pages as before. Conversely, if a guest OS
                                does intend to use huge pages, it is very important that the guest RAM be
                                backed by huge pages. Otherwise, the guest OS will not be getting the
                                performance benefit it is expecting.</para>
            </note>
          </listitem>
        </varlistentry>
        <varlistentry>
          <term>PCI passthrough</term>
          <listitem>
            <para>You can assign PCI devices to a guest by specifying them in the flavor.</para>
            <screen>$ openstack flavor set FLAVOR-NAME \
    --property pci_passthrough:alias=ALIAS:COUNT</screen>
            <para>Where:</para>
            <itemizedlist>
              <listitem>
                <para>ALIAS: (string) The alias which correspond to a particular PCI device class
                                    as configured in the nova configuration file (see
                                    <xref linkend="../configuration/config"/>).</para>
              </listitem>
              <listitem>
                <para>COUNT: (integer) The amount of PCI devices of type ALIAS to be assigned to
                                    a guest.</para>
              </listitem>
            </itemizedlist>
          </listitem>
        </varlistentry>
        <varlistentry>
          <term>Secure Boot</term>
          <listitem>
            <para>When your Compute services use the Hyper-V hypervisor, you can enable secure
                            boot for Windows and Linux instances.</para>
            <screen>$ openstack flavor set FLAVOR-NAME \
    --property os:secure_boot=SECURE_BOOT_OPTION</screen>
            <para>Valid <literal>SECURE_BOOT_OPTION</literal> values are:</para>
            <itemizedlist>
              <listitem>
                <para><literal>required</literal>: Enable Secure Boot for instances running with this flavor.</para>
              </listitem>
              <listitem>
                <para><literal>disabled</literal> or <literal>optional</literal>: (default) Disable Secure Boot for instances
                                    running with this flavor.</para>
              </listitem>
            </itemizedlist>
          </listitem>
        </varlistentry>
      </variablelist>
    </section>
  </chapter>
  <chapter xml:id="huge-pages" xml:base="huge-pages">
    <title>Huge pages</title>
    <para>The huge page feature in OpenStack provides important performance improvements
            for applications that are highly memory IO-bound.</para>
    <note>
      <para>Huge pages may also be referred to hugepages or large pages, depending on
                the source. These terms are synonyms.</para>
    </note>
    <section xml:id="pages-the-tlb-and-huge-pages">
      <title>Pages, the TLB and huge pages</title>
      <variablelist>
        <varlistentry>
          <term>Pages</term>
          <listitem>
            <para>Physical memory is segmented into a series of contiguous regions called
                            pages. Each page contains a number of bytes, referred to as the page size.
                            The system retrieves memory by accessing entire pages, rather than byte by
                            byte.</para>
          </listitem>
        </varlistentry>
        <varlistentry>
          <term>Translation Lookaside Buffer (TLB)</term>
          <listitem>
            <para>A TLB is used to map the virtual addresses of pages to the physical addresses
                            in actual memory. The TLB is a cache and is not limitless, storing only the
                            most recent or frequently accessed pages. During normal operation, processes
                            will sometimes attempt to retrieve pages that are not stored in the cache.
                            This is known as a TLB miss and results in a delay as the processor iterates
                            through the pages themselves to find the missing address mapping.</para>
          </listitem>
        </varlistentry>
        <varlistentry>
          <term>Huge Pages</term>
          <listitem>
            <para>The standard page size in x86 systems is 4 kB. This is optimal for general
                            purpose computing but larger page sizes - 2 MB and 1 GB - are also available.
                            These larger page sizes are known as huge pages. Huge pages result in less
                            efficient memory usage as a process will not generally use all memory
                            available in each page. However, use of huge pages will result in fewer
                            overall pages and a reduced risk of TLB misses. For processes that have
                            significant memory requirements or are memory intensive, the benefits of huge
                            pages frequently outweigh the drawbacks.</para>
          </listitem>
        </varlistentry>
        <varlistentry>
          <term>Persistent Huge Pages</term>
          <listitem>
            <para>On Linux hosts, persistent huge pages are huge pages that are reserved
                            upfront. The HugeTLB provides for the mechanism for this upfront
                            configuration of huge pages. The HugeTLB allows for the allocation of varying
                            quantities of different huge page sizes. Allocation can be made at boot time
                            or run time. Refer to the <link xmlns:xl="http://www.w3.org/1999/xlink" xl:href="https://www.kernel.org/doc/Documentation/vm/hugetlbpage.txt">Linux hugetlbfs guide</link> for more information.</para>
          </listitem>
        </varlistentry>
        <varlistentry>
          <term>Transparent Huge Pages (THP)</term>
          <listitem>
            <para>On Linux hosts, transparent huge pages are huge pages that are automatically
                            provisioned based on process requests. Transparent huge pages are provisioned
                            on a best effort basis, attempting to provision 2 MB huge pages if available
                            but falling back to 4 kB small pages if not. However, no upfront
                            configuration is necessary. Refer to the <link xmlns:xl="http://www.w3.org/1999/xlink" xl:href="https://www.kernel.org/doc/Documentation/vm/transhuge.txt">Linux THP guide</link> for more
                            information.</para>
          </listitem>
        </varlistentry>
      </variablelist>
    </section>
    <section xml:id="enabling-huge-pages-on-the-host">
      <title>Enabling huge pages on the host</title>
      <para>Persistent huge pages are required owing to their guaranteed availability.
                However, persistent huge pages are not enabled by default in most environments.
                The steps for enabling huge pages differ from platform to platform and only the
                steps for Linux hosts are described here. On Linux hosts, the number of
                persistent huge pages on the host can be queried by checking <literal>/proc/meminfo</literal>:</para>
      <screen language="console">$ grep Huge /proc/meminfo
AnonHugePages:         0 kB
ShmemHugePages:        0 kB
HugePages_Total:       0
HugePages_Free:        0
HugePages_Rsvd:        0
HugePages_Surp:        0
Hugepagesize:       2048 kB</screen>
      <para>In this instance, there are 0 persistent huge pages (<literal>HugePages_Total</literal>) and 0
                transparent huge pages (<literal>AnonHugePages</literal>) allocated. Huge pages can be
                allocated at boot time or run time. Huge pages require a contiguous area of
                memory - memory that gets increasingly fragmented the long a host is running.
                Identifying contiguous areas of memory is a issue for all huge page sizes, but
                it is particularly problematic for larger huge page sizes such as 1 GB huge
                pages. Allocating huge pages at boot time will ensure the correct number of huge
                pages is always available, while allocating them at run time can fail if memory
                has become too fragmented.</para>
      <para>To allocate huge pages at run time, the kernel boot parameters must be extended
                to include some huge page-specific parameters. This can be achieved by
                modifying <literal>/etc/default/grub</literal> and appending the <literal>hugepagesz</literal>,
                <literal>hugepages</literal>, and <literal>transparent_hugepages=never</literal> arguments to
                <literal>GRUB_CMDLINE_LINUX</literal>. To allocate, for example, 2048 persistent 2 MB huge
                pages at boot time, run:</para>
      <screen language="console"># echo 'GRUB_CMDLINE_LINUX="$GRUB_CMDLINE_LINUX hugepagesz=2M hugepages=2048 transparent_hugepage=never"' &gt; /etc/default/grub
$ grep GRUB_CMDLINE_LINUX /etc/default/grub
GRUB_CMDLINE_LINUX="..."
GRUB_CMDLINE_LINUX="$GRUB_CMDLINE_LINUX hugepagesz=2M hugepages=2048 transparent_hugepage=never"</screen>
      <important>
        <para>Persistent huge pages are not usable by standard host OS processes. Ensure
                    enough free, non-huge page memory is reserved for these processes.</para>
      </important>
      <para>Reboot the host, then validate that huge pages are now available:</para>
      <screen language="console">$ grep "Huge" /proc/meminfo
AnonHugePages:         0 kB
ShmemHugePages:        0 kB
HugePages_Total:    2048
HugePages_Free:     2048
HugePages_Rsvd:        0
HugePages_Surp:        0
Hugepagesize:       2048 kB</screen>
      <para>There are now 2048 2 MB huge pages totalling 4 GB of huge pages. These huge
                pages must be mounted. On most platforms, this happens automatically. To verify
                that the huge pages are mounted, run:</para>
      <screen language="console"># mount | grep huge
hugetlbfs on /dev/hugepages type hugetlbfs (rw)</screen>
      <para>In this instance, the huge pages are mounted at <literal>/dev/hugepages</literal>. This mount
                point varies from platform to platform. If the above command did not return
                anything, the hugepages must be mounted manually. To mount the huge pages at
                <literal>/dev/hugepages</literal>, run:</para>
      <screen language="console"># mkdir -p /dev/hugepages
# mount -t hugetlbfs hugetlbfs /dev/hugepages</screen>
      <para>There are many more ways to configure huge pages, including allocating huge
                pages at run time, specifying varying allocations for different huge page
                sizes, or allocating huge pages from memory affinitized to different NUMA
                nodes. For more information on configuring huge pages on Linux hosts, refer to
                the <link xmlns:xl="http://www.w3.org/1999/xlink" xl:href="https://www.kernel.org/doc/Documentation/vm/hugetlbpage.txt">Linux hugetlbfs guide</link>.</para>
    </section>
    <section xml:id="customizing-instance-huge-pages-allocations">
      <title>Customizing instance huge pages allocations</title>
      <important>
        <para>The functionality described below is currently only supported by the
                    libvirt/KVM driver.</para>
      </important>
      <important>
        <para>For performance reasons, configuring huge pages for an instance will
                    implicitly result in a NUMA topology being configured for the instance.
                    Configuring a NUMA topology for an instance requires enablement of
                    <literal>NUMATopologyFilter</literal>. Refer to <xref linkend="cpu-topologies"/> for more
                    information.</para>
      </important>
      <para>By default, an instance does not use huge pages for its underlying memory.
                However, huge pages can bring important or required performance improvements
                for some workloads. Huge pages must be requested explicitly through the use of
                flavor extra specs or image metadata. To request an instance use huge pages,
                run:</para>
      <screen language="console">$ openstack flavor set m1.large --property hw:mem_page_size=large</screen>
      <para>Different platforms offer different huge page sizes. For example: x86-based
                platforms offer 2 MB and 1 GB huge page sizes. Specific huge page sizes can be
                also be requested, with or without a unit suffix. The unit suffix must be one
                of: Kb(it), Kib(it), Mb(it), Mib(it), Gb(it), Gib(it), Tb(it), Tib(it), KB,
                KiB, MB, MiB, GB, GiB, TB, TiB. Where a unit suffix is not provided, Kilobytes
                are assumed. To request an instance to use 2 MB huge pages, run one of:</para>
      <screen language="console">$ openstack flavor set m1.large --property hw:mem_page_size=2Mb</screen>
      <screen language="console">$ openstack flavor set m1.large --property hw:mem_page_size=2048</screen>
      <para>Enabling huge pages for an instance can have negative consequences for other
                instances by consuming limited huge pages resources. To explicitly request
                an instance use small pages, run:</para>
      <screen language="console">$ openstack flavor set m1.large --property hw:mem_page_size=small</screen>
      <note>
        <para>Explicitly requesting any page size will still result in a NUMA topology
                    being applied to the instance, as described earlier in this document.</para>
      </note>
      <para>Finally, to leave the decision of huge or small pages to the compute driver,
                run:</para>
      <screen language="console">$ openstack flavor set m1.large --property hw:mem_page_size=any</screen>
      <para>For more information about the syntax for <literal>hw:mem_page_size</literal>, refer to the
                <link xmlns:xl="http://www.w3.org/1999/xlink" xl:href="https://docs.openstack.org/admin-guide/compute-flavors.html">Flavors</link> guide.</para>
      <para>Applications are frequently packaged as images. For applications that require
                the IO performance improvements that huge pages provides, configure image
                metadata to ensure instances always request the specific page size regardless
                of flavor. To configure an image to use 1 GB huge pages, run:</para>
      <screen language="console">$ openstack image set [IMAGE_ID]  --property hw_mem_page_size=1GB</screen>
      <para>If the flavor specifies a numerical page size or a page size of "small" the
                image is not allowed to specify a page size and if it does an exception will
                be raised. If the flavor specifies a page size of <literal>any</literal> or <literal>large</literal> then
                any page size specified in the image will be used. By setting a <literal>small</literal>
                page size in the flavor, administrators can prevent users requesting huge
                pages in flavors and impacting resource utilization. To configure this page
                size, run:</para>
      <screen language="console">$ openstack flavor set m1.large --property hw:mem_page_size=small</screen>
      <note>
        <para>Explicitly requesting any page size will still result in a NUMA topology
                    being applied to the instance, as described earlier in this document.</para>
      </note>
      <para>For more information about image metadata, refer to the <link xmlns:xl="http://www.w3.org/1999/xlink" xl:href="https://docs.openstack.org/image-guide/image-metadata.html">Image metadata</link>
                guide.</para>
    </section>
  </chapter>
  <chapter xml:id="live-migrate-instances" xml:base="live-migration-usage">
    <title>Live-migrate instances</title>
    <para>Live-migrating an instance means moving its virtual machine to a different
            OpenStack Compute server while the instance continues running.  Before starting
            a live-migration, review the chapter
            <xref linkend="section-configuring-compute-migrations"/>. It covers the configuration
            settings required to enable live-migration, but also reasons for migrations and
            non-live-migration options.</para>
    <para>The instructions below cover shared-storage and volume-backed migration.  To
            block-migrate instances, add the command-line option
            <literal>-block-migrate</literal> to the <command>nova live-migration</command> command,
            and <literal>--block-migration</literal> to the <command>openstack server migrate</command>
            command.</para>
    <section xml:id="section-manual-selection-of-dest">
      <title>Manual selection of the destination host</title>
      <procedure>
        <step>
          <para>Obtain the ID of the instance you want to migrate:</para>
          <screen language="console">$ openstack server list

+--------------------------------------+------+--------+-----------------+------------+
| ID                                   | Name | Status | Networks        | Image Name |
+--------------------------------------+------+--------+-----------------+------------+
| d1df1b5a-70c4-4fed-98b7-423362f2c47c | vm1  | ACTIVE | private=a.b.c.d | ...        |
| d693db9e-a7cf-45ef-a7c9-b3ecb5f22645 | vm2  | ACTIVE | private=e.f.g.h | ...        |
+--------------------------------------+------+--------+-----------------+------------+</screen>
        </step>
        <step>
          <para>Determine on which host the instance is currently running. In this example,
                        <literal>vm1</literal> is running on <literal>HostB</literal>:</para>
          <screen language="console">$ openstack server show d1df1b5a-70c4-4fed-98b7-423362f2c47c

+----------------------+--------------------------------------+
| Field                | Value                                |
+----------------------+--------------------------------------+
| ...                  | ...                                  |
| OS-EXT-SRV-ATTR:host | HostB                                |
| ...                  | ...                                  |
| addresses            | a.b.c.d                              |
| flavor               | m1.tiny                              |
| id                   | d1df1b5a-70c4-4fed-98b7-423362f2c47c |
| name                 | vm1                                  |
| status               | ACTIVE                               |
| ...                  | ...                                  |
+----------------------+--------------------------------------+</screen>
        </step>
        <step>
          <para>Select the compute node the instance will be migrated to. In this example,
                        we will migrate the instance to <literal>HostC</literal>, because <literal>nova-compute</literal> is
                        running on it:</para>
          <screen language="console">$ openstack compute service list

+----+------------------+-------+----------+---------+-------+----------------------------+
| ID | Binary           | Host  | Zone     | Status  | State | Updated At                 |
+----+------------------+-------+----------+---------+-------+----------------------------+
|  3 | nova-conductor   | HostA | internal | enabled | up    | 2017-02-18T09:42:29.000000 |
|  4 | nova-scheduler   | HostA | internal | enabled | up    | 2017-02-18T09:42:26.000000 |
|  5 | nova-consoleauth | HostA | internal | enabled | up    | 2017-02-18T09:42:29.000000 |
|  6 | nova-compute     | HostB | nova     | enabled | up    | 2017-02-18T09:42:29.000000 |
|  7 | nova-compute     | HostC | nova     | enabled | up    | 2017-02-18T09:42:29.000000 |
+----+------------------+-------+----------+---------+-------+----------------------------+</screen>
        </step>
        <step>
          <para>Check that <literal>HostC</literal> has enough resources for migration:</para>
          <screen language="console">$ openstack host show HostC

+-------+------------+-----+-----------+---------+
| Host  | Project    | CPU | Memory MB | Disk GB |
+-------+------------+-----+-----------+---------+
| HostC | (total)    |  16 |     32232 |     878 |
| HostC | (used_now) |  22 |     21284 |     422 |
| HostC | (used_max) |  22 |     21284 |     422 |
| HostC | p1         |  22 |     21284 |     422 |
| HostC | p2         |  22 |     21284 |     422 |
+-------+------------+-----+-----------+---------+</screen>
          <itemizedlist>
            <listitem>
              <para><literal>cpu</literal>: Number of CPUs</para>
            </listitem>
            <listitem>
              <para><literal>memory_mb</literal>: Total amount of memory, in MB</para>
            </listitem>
            <listitem>
              <para><literal>disk_gb</literal>: Total amount of space for NOVA-INST-DIR/instances, in GB</para>
            </listitem>
          </itemizedlist>
          <para>In this table, the first row shows the total amount of resources available
                        on the physical server. The second line shows the currently used resources.
                        The third line shows the maximum used resources. The fourth line and below
                        shows the resources available for each project.</para>
        </step>
        <step>
          <para>Migrate the instance:</para>
          <screen language="console">$ openstack server migrate d1df1b5a-70c4-4fed-98b7-423362f2c47c --live HostC</screen>
        </step>
        <step>
          <para>Confirm that the instance has been migrated successfully:</para>
          <screen language="console">$ openstack server show d1df1b5a-70c4-4fed-98b7-423362f2c47c

+----------------------+--------------------------------------+
| Field                | Value                                |
+----------------------+--------------------------------------+
| ...                  | ...                                  |
| OS-EXT-SRV-ATTR:host | HostC                                |
| ...                  | ...                                  |
+----------------------+--------------------------------------+</screen>
          <para>If the instance is still running on <literal>HostB</literal>, the migration failed. The
                        <literal>nova-scheduler</literal> and <literal>nova-conductor</literal> log files on the controller and
                        the <literal>nova-compute</literal> log file on the source compute host can help pin-point
                        the problem.</para>
        </step>
      </procedure>
    </section>
    <section xml:id="auto-selection-of-dest">
      <title>Automatic selection of the destination host</title>
      <para>To leave the selection of the destination host to the Compute service, use the
                nova command-line client.</para>
      <procedure>
        <step>
          <para>Obtain the instance ID as shown in step 1 of the section
                        <link xmlns:xl="http://www.w3.org/1999/xlink" xl:href="#section-manual-selection-of-dest">Manual selection of the destination host</link>.</para>
        </step>
        <step>
          <para>Leave out the host selection steps 2, 3, and 4.</para>
        </step>
        <step>
          <para>Migrate the instance:</para>
          <screen language="console">$ nova live-migration d1df1b5a-70c4-4fed-98b7-423362f2c47c</screen>
        </step>
      </procedure>
    </section>
    <section xml:id="monitoring-the-migration">
      <title>Monitoring the migration</title>
      <procedure>
        <step>
          <para>Confirm that the instance is migrating:</para>
          <screen language="console">$ openstack server show d1df1b5a-70c4-4fed-98b7-423362f2c47c

+----------------------+--------------------------------------+
| Field                | Value                                |
+----------------------+--------------------------------------+
| ...                  | ...                                  |
| status               | MIGRATING                            |
| ...                  | ...                                  |
+----------------------+--------------------------------------+</screen>
        </step>
        <step>
          <para>Check progress</para>
          <para>Use the nova command-line client for nova's migration monitoring feature.
                        First, obtain the migration ID:</para>
          <screen language="console">$ nova server-migration-list d1df1b5a-70c4-4fed-98b7-423362f2c47c
+----+-------------+-----------  (...)
| Id | Source Node | Dest Node | (...)
+----+-------------+-----------+ (...)
| 2  | -           | -         | (...)
+----+-------------+-----------+ (...)</screen>
          <para>For readability, most output columns were removed. Only the first column,
                        <emphasis role="bold">Id</emphasis>, is relevant.  In this example, the migration ID is 2. Use this to
                        get the migration status.</para>
          <screen language="console">$ nova server-migration-show d1df1b5a-70c4-4fed-98b7-423362f2c47c 2
+------------------------+--------------------------------------+
| Property               | Value                                |
+------------------------+--------------------------------------+
| created_at             | 2017-03-08T02:53:06.000000           |
| dest_compute           | controller                           |
| dest_host              | -                                    |
| dest_node              | -                                    |
| disk_processed_bytes   | 0                                    |
| disk_remaining_bytes   | 0                                    |
| disk_total_bytes       | 0                                    |
| id                     | 2                                    |
| memory_processed_bytes | 65502513                             |
| memory_remaining_bytes | 786427904                            |
| memory_total_bytes     | 1091379200                           |
| server_uuid            | d1df1b5a-70c4-4fed-98b7-423362f2c47c |
| source_compute         | compute2                             |
| source_node            | -                                    |
| status                 | running                              |
| updated_at             | 2017-03-08T02:53:47.000000           |
+------------------------+--------------------------------------+</screen>
          <para>The output shows that the migration is running. Progress is measured by the
                        number of memory bytes that remain to be copied. If this number is not
                        decreasing over time, the migration may be unable to complete, and it may be
                        aborted by the Compute service.</para>
          <note>
            <para>The command reports that no disk bytes are processed, even in the event
                            of block migration.</para>
          </note>
        </step>
      </procedure>
    </section>
    <section xml:id="what-to-do-when-the-migration-times-out">
      <title>What to do when the migration times out</title>
      <para>During the migration process, the instance may write to a memory page after
                that page has been copied to the destination. When that happens, the same page
                has to be copied again. The instance may write to memory pages faster than they
                can be copied, so that the migration cannot complete.  The Compute service will
                cancel it when the <literal>live_migration_completion_timeout</literal>, a configuration
                parameter, is reached.</para>
      <para>The following remarks assume the KVM/Libvirt hypervisor.</para>
      <section xml:id="how-to-know-that-the-migration-timed-out">
        <title>How to know that the migration timed out</title>
        <para>To determine that the migration timed out, inspect the <literal>nova-compute</literal> log
                    file on the source host. The following log entry shows that the migration timed
                    out:</para>
        <screen language="console"># grep WARNING.*d1df1b5a-70c4-4fed-98b7-423362f2c47c /var/log/nova/nova-compute.log
...
WARNING nova.virt.libvirt.migration [req-...] [instance: ...]
live migration not completed after 1800 sec</screen>
        <para>The Compute service also cancels migrations when the memory copy seems to make
                    no progress. Ocata disables this feature by default, but it can be enabled
                    using the configuration parameter <literal>live_migration_progress_timeout</literal>. Should
                    this be the case, you may find the following message in the log:</para>
        <screen language="console">WARNING nova.virt.libvirt.migration [req-...] [instance: ...]
live migration stuck for 150 sec</screen>
      </section>
      <section xml:id="addressing-migration-timeouts">
        <title>Addressing migration timeouts</title>
        <para>To stop the migration from putting load on infrastructure resources like
                    network and disks, you may opt to cancel it manually.</para>
        <screen language="console">$ nova live-migration-abort INSTANCE_ID MIGRATION_ID</screen>
        <para>To make live-migration succeed, you have several options:</para>
        <itemizedlist>
          <listitem>
            <para>
              <emphasis role="bold">Manually force-complete the migration</emphasis>
            </para>
            <screen language="console">$ nova live-migration-force-complete INSTANCE_ID MIGRATION_ID</screen>
            <para>The instance is paused until memory copy completes.</para>
            <important>
              <para>Since the pause impacts time keeping on the instance and not all
                                applications tolerate incorrect time settings, use this approach with
                                caution.</para>
            </important>
          </listitem>
          <listitem>
            <para>
              <emphasis role="bold">Enable auto-convergence</emphasis>
            </para>
            <para>Auto-convergence is a Libvirt feature. Libvirt detects that the migration is
                            unlikely to complete and slows down its CPU until the memory copy process is
                            faster than the instance's memory writes.</para>
            <para>To enable auto-convergence, set
                            <literal>live_migration_permit_auto_converge=true</literal> in <literal>nova.conf</literal> and restart
                            <literal>nova-compute</literal>. Do this on all compute hosts.</para>
            <important>
              <para>One possible downside of auto-convergence is the slowing down of the
                                instance.</para>
            </important>
          </listitem>
          <listitem>
            <para>
              <emphasis role="bold">Enable post-copy</emphasis>
            </para>
            <para>This is a Libvirt feature. Libvirt detects that the migration does not
                            progress and responds by activating the virtual machine on the destination
                            host before all its memory has been copied. Access to missing memory pages
                            result in page faults that are satisfied from the source host.</para>
            <para>To enable post-copy, set <literal>live_migration_permit_post_copy=true</literal> in
                            <literal>nova.conf</literal> and restart <literal>nova-compute</literal>. Do this on all compute hosts.</para>
            <para>When post-copy is enabled, manual force-completion does not pause the
                            instance but switches to the post-copy process.</para>
            <important>
              <para>Possible downsides:</para>
              <itemizedlist>
                <listitem>
                  <para>When the network connection between source and destination is
                                        interrupted, page faults cannot be resolved anymore, and the virtual
                                        machine is rebooted.</para>
                </listitem>
                <listitem>
                  <para>Post-copy may lead to an increased page fault rate during migration,
                                        which can slow the instance down.</para>
                </listitem>
              </itemizedlist>
            </important>
          </listitem>
        </itemizedlist>
      </section>
    </section>
  </chapter>
  <chapter xml:id="logging" xml:base="manage-logs">
    <title>Logging</title>
    <section xml:id="logging-module">
      <title>Logging module</title>
      <para>Logging behavior can be changed by creating a configuration file. To specify
                the configuration file, add this line to the <literal>/etc/nova/nova.conf</literal> file:</para>
      <screen language="ini">log-config=/etc/nova/logging.conf</screen>
      <para>To change the logging level, add <literal>DEBUG</literal>, <literal>INFO</literal>, <literal>WARNING</literal>, or <literal>ERROR</literal>
                as a parameter.</para>
      <para>The logging configuration file is an INI-style configuration file, which must
                contain a section called <literal>logger_nova</literal>. This controls the behavior of the
                logging facility in the <literal>nova-*</literal> services. For example:</para>
      <screen language="ini">[logger_nova]
level = INFO
handlers = stderr
qualname = nova</screen>
      <para>This example sets the debugging level to <literal>INFO</literal> (which is less verbose than
                the default <literal>DEBUG</literal> setting).</para>
      <para>For more about the logging configuration syntax, including the <literal>handlers</literal> and
                <literal>quaname</literal> variables, see the <link xmlns:xl="http://www.w3.org/1999/xlink" xl:href="https://docs.python.org/release/2.7/library/logging.html#configuration-file-format">Python documentation</link>
                on logging configuration files.</para>
      <para>For an example of the <literal>logging.conf</literal> file with various defined handlers, see
                the <link xmlns:xl="http://www.w3.org/1999/xlink" xl:href="https://docs.openstack.org/oslo.log/latest/admin/example_nova.html">Example Configuration File for nova</link>.</para>
    </section>
    <section xml:id="syslog">
      <title>Syslog</title>
      <para>OpenStack Compute services can send logging information to syslog. This is
                useful if you want to use rsyslog to forward logs to a remote machine.
                Separately configure the Compute service (nova), the Identity service
                (keystone), the Image service (glance), and, if you are using it, the Block
                Storage service (cinder) to send log messages to syslog.  Open these
                configuration files:</para>
      <itemizedlist>
        <listitem>
          <para>
            <literal>/etc/nova/nova.conf</literal>
          </para>
        </listitem>
        <listitem>
          <para>
            <literal>/etc/keystone/keystone.conf</literal>
          </para>
        </listitem>
        <listitem>
          <para>
            <literal>/etc/glance/glance-api.conf</literal>
          </para>
        </listitem>
        <listitem>
          <para>
            <literal>/etc/glance/glance-registry.conf</literal>
          </para>
        </listitem>
        <listitem>
          <para>
            <literal>/etc/cinder/cinder.conf</literal>
          </para>
        </listitem>
      </itemizedlist>
      <para>In each configuration file, add these lines:</para>
      <screen language="ini">debug = False
use_syslog = True
syslog_log_facility = LOG_LOCAL0</screen>
      <para>In addition to enabling syslog, these settings also turn off debugging output
                from the log.</para>
      <note>
        <para>Although this example uses the same local facility for each service
                    (<literal>LOG_LOCAL0</literal>, which corresponds to syslog facility <literal>LOCAL0</literal>), we
                    recommend that you configure a separate local facility for each service, as
                    this provides better isolation and more flexibility. For example, you can
                    capture logging information at different severity levels for different
                    services. syslog allows you to define up to eight local facilities,
                    <literal>LOCAL0, LOCAL1, ..., LOCAL7</literal>. For more information, see the syslog
                    documentation.</para>
      </note>
    </section>
    <section xml:id="rsyslog">
      <title>Rsyslog</title>
      <para>rsyslog is useful for setting up a centralized log server across multiple
                machines. This section briefly describe the configuration to set up an rsyslog
                server. A full treatment of rsyslog is beyond the scope of this book. This
                section assumes rsyslog has already been installed on your hosts (it is
                installed by default on most Linux distributions).</para>
      <para>This example provides a minimal configuration for <literal>/etc/rsyslog.conf</literal> on the
                log server host, which receives the log files</para>
      <screen language="console"># provides TCP syslog reception
$ModLoad imtcp
$InputTCPServerRun 1024</screen>
      <para>Add a filter rule to <literal>/etc/rsyslog.conf</literal> which looks for a host name.  This
                example uses COMPUTE_01 as the compute host name:</para>
      <screen language="none">:hostname, isequal, "COMPUTE_01" /mnt/rsyslog/logs/compute-01.log</screen>
      <para>On each compute host, create a file named <literal>/etc/rsyslog.d/60-nova.conf</literal>, with
                the following content:</para>
      <screen language="none"># prevent debug from dnsmasq with the daemon.none parameter
*.*;auth,authpriv.none,daemon.none,local0.none -/var/log/syslog
# Specify a log level of ERROR
local0.error    @@172.20.1.43:1024</screen>
      <para>Once you have created the file, restart the <literal>rsyslog</literal> service. Error-level
                log messages on the compute hosts should now be sent to the log server.</para>
    </section>
    <section xml:id="serial-console">
      <title>Serial console</title>
      <para>The serial console provides a way to examine kernel output and other system
                messages during troubleshooting if the instance lacks network connectivity.</para>
      <para>Read-only access from server serial console is possible using the
                <literal>os-GetSerialOutput</literal> server action. Most cloud images enable this feature by
                default. For more information, see <xref linkend="compute-common-errors-and-fixes"/>.</para>
      <para>OpenStack Juno and later supports read-write access using the serial console
                using the <literal>os-GetSerialConsole</literal> server action. This feature also requires a
                websocket client to access the serial console.</para>
      <procedure>
        <step>
          <para>On a compute node, edit the <literal>/etc/nova/nova.conf</literal> file:</para>
          <para>In the <literal>[serial_console]</literal> section, enable the serial console:</para>
          <screen language="ini">[serial_console]
# ...
enabled = true</screen>
        </step>
        <step>
          <para>In the <literal>[serial_console]</literal> section, configure the serial console proxy
                        similar to graphical console proxies:</para>
          <screen language="ini">[serial_console]
# ...
base_url = ws://controller:6083/
listen = 0.0.0.0
proxyclient_address = MANAGEMENT_INTERFACE_IP_ADDRESS</screen>
          <para>The <literal>base_url</literal> option specifies the base URL that clients receive from the
                        API upon requesting a serial console. Typically, this refers to the host
                        name of the controller node.</para>
          <para>The <literal>listen</literal> option specifies the network interface nova-compute should
                        listen on for virtual console connections. Typically, 0.0.0.0 will enable
                        listening on all interfaces.</para>
          <para>The <literal>proxyclient_address</literal> option specifies which network interface the
                        proxy should connect to. Typically, this refers to the IP address of the
                        management interface.</para>
          <para>When you enable read-write serial console access, Compute will add serial
                        console information to the Libvirt XML file for the instance. For example:</para>
          <screen language="xml">&lt;console type='tcp'&gt;
  &lt;source mode='bind' host='127.0.0.1' service='10000'/&gt;
  &lt;protocol type='raw'/&gt;
  &lt;target type='serial' port='0'/&gt;
  &lt;alias name='serial0'/&gt;
&lt;/console&gt;</screen>
        </step>
      </procedure>
      <procedure>
        <step>
          <para>Use the <command>nova get-serial-proxy</command> command to retrieve the websocket
                        URL for the serial console on the instance:</para>
          <screen language="console">$ nova get-serial-proxy INSTANCE_NAME</screen>
          <informaltable>
            <tgroup cols="2">
              <colspec colname="c0" colwidth="9"/>
              <colspec colname="c1" colwidth="65"/>
              <tbody>
                <row>
                  <entry>
                    <para>Type</para>
                  </entry>
                  <entry>
                    <para>Url</para>
                  </entry>
                </row>
                <row>
                  <entry>
                    <para>serial</para>
                  </entry>
                  <entry>
                    <para>ws://127.0.0.1:6083/?token=18510769-71ad-4e5a-8348-4218b5613b3d</para>
                  </entry>
                </row>
              </tbody>
            </tgroup>
          </informaltable>
          <para>Alternatively, use the API directly:</para>
          <screen language="console">$ curl -i 'http://&lt;controller&gt;:8774/v2.1/&lt;tenant_uuid&gt;/servers/&lt;instance_uuid&gt;/action' \
  -X POST \
  -H "Accept: application/json" \
  -H "Content-Type: application/json" \
  -H "X-Auth-Project-Id: &lt;project_id&gt;" \
  -H "X-Auth-Token: &lt;auth_token&gt;" \
  -d '{"os-getSerialConsole": {"type": "serial"}}'</screen>
        </step>
        <step>
          <para>Use Python websocket with the URL to generate <literal>.send</literal>, <literal>.recv</literal>, and
                        <literal>.fileno</literal> methods for serial console access. For example:</para>
          <screen language="python">import websocket
ws = websocket.create_connection(
    'ws://127.0.0.1:6083/?token=18510769-71ad-4e5a-8348-4218b5613b3d',
    subprotocols=['binary', 'base64'])</screen>
        </step>
      </procedure>
      <para>Alternatively, use a <link xmlns:xl="http://www.w3.org/1999/xlink" xl:href="https://github.com/larsks/novaconsole/">Python websocket client</link>.</para>
      <note>
        <para>When you enable the serial console, typical instance logging using the
                    <command>nova console-log</command> command is disabled. Kernel output and other
                    system messages will not be visible unless you are actively viewing the
                    serial console.</para>
      </note>
    </section>
  </chapter>
  <chapter xml:id="section-manage-the-cloud" xml:base="manage-the-cloud">
    <title>Manage the cloud</title>
    <section xml:id="managing-the-cloud-with-euca2ools" xml:base="euca2ools">
      <title>Managing the cloud with euca2ools</title>
      <para>The <literal>euca2ools</literal> command-line tool provides a command line interface to EC2
            API calls. For more information, see the <link xmlns:xl="http://www.w3.org/1999/xlink" xl:href="http://docs.hpcloud.com/eucalyptus/">Official Eucalyptus Documentation</link>.</para>
    </section>
    <section xml:id="show-usage-statistics-for-hosts-and-instances" xml:base="common/nova-show-usage-statistics-for-hosts-instances">
      <title>Show usage statistics for hosts and instances</title>
      <para>You can show basic statistics on resource usage for hosts and instances.</para>
      <note>
        <para>For more sophisticated monitoring, see the
                <link xmlns:xl="http://www.w3.org/1999/xlink" xl:href="https://launchpad.net/ceilometer">ceilometer</link> project. You can
                also use tools, such as <link xmlns:xl="http://www.w3.org/1999/xlink" xl:href="http://ganglia.info/">Ganglia</link> or
                <link xmlns:xl="http://www.w3.org/1999/xlink" xl:href="http://graphite.wikidot.com/">Graphite</link>, to gather more detailed
                data.</para>
      </note>
      <section xml:id="show-host-usage-statistics">
        <title>Show host usage statistics</title>
        <para>The following examples show the host usage statistics for a host called
                <literal>devstack</literal>.</para>
        <itemizedlist>
          <listitem>
            <para>List the hosts and the nova-related services that run on them:</para>
            <screen language="console">$ openstack host list
+-----------+-------------+----------+
| Host Name | Service     | Zone     |
+-----------+-------------+----------+
| devstack  | conductor   | internal |
| devstack  | compute     | nova     |
| devstack  | cert        | internal |
| devstack  | network     | internal |
| devstack  | scheduler   | internal |
| devstack  | consoleauth | internal |
+-----------+-------------+----------+</screen>
          </listitem>
          <listitem>
            <para>Get a summary of resource usage of all of the instances running on the host:</para>
            <screen language="console">$ openstack host show devstack
+----------+----------------------------------+-----+-----------+---------+
| Host     | Project                          | CPU | MEMORY MB | DISK GB |
+----------+----------------------------------+-----+-----------+---------+
| devstack | (total)                          | 2   | 4003      | 157     |
| devstack | (used_now)                       | 3   | 5120      | 40      |
| devstack | (used_max)                       | 3   | 4608      | 40      |
| devstack | b70d90d65e464582b6b2161cf3603ced | 1   | 512       | 0       |
| devstack | 66265572db174a7aa66eba661f58eb9e | 2   | 4096      | 40      |
+----------+----------------------------------+-----+-----------+---------+</screen>
            <para>The <literal>CPU</literal> column shows the sum of the virtual CPUs for instances running on
                        the host.</para>
            <para>The <literal>MEMORY MB</literal> column shows the sum of the memory (in MB) allocated to the
                        instances that run on the host.</para>
            <para>The <literal>DISK GB</literal> column shows the sum of the root and ephemeral disk sizes (in
                        GB) of the instances that run on the host.</para>
            <para>The row that has the value <literal>used_now</literal> in the <literal>PROJECT</literal> column shows the
                        sum of the resources allocated to the instances that run on the host, plus
                        the resources allocated to the virtual machine of the host itself.</para>
            <para>The row that has the value <literal>used_max</literal> in the <literal>PROJECT</literal> column shows the
                        sum of the resources allocated to the instances that run on the host.</para>
            <note>
              <para>These values are computed by using information about the flavors of the
                            instances that run on the hosts. This command does not query the CPU
                            usage, memory usage, or hard disk usage of the physical host.</para>
            </note>
          </listitem>
        </itemizedlist>
      </section>
      <section xml:id="show-instance-usage-statistics">
        <title>Show instance usage statistics</title>
        <itemizedlist>
          <listitem>
            <para>Get CPU, memory, I/O, and network statistics for an instance.</para>
            <procedure>
              <step>
                <para>List instances:</para>
                <screen language="console">$ openstack server list
+----------+----------------------+--------+------------+-------------+------------------+------------+
| ID       | Name                 | Status | Task State | Power State | Networks         | Image Name |
+----------+----------------------+--------+------------+-------------+------------------+------------+
| 84c6e... | myCirrosServer       | ACTIVE | None       | Running     | private=10.0.0.3 | cirros     |
| 8a995... | myInstanceFromVolume | ACTIVE | None       | Running     | private=10.0.0.4 | ubuntu     |
+----------+----------------------+--------+------------+-------------+------------------+------------+</screen>
              </step>
              <step>
                <para>Get diagnostic statistics:</para>
                <screen language="console">$ nova diagnostics myCirrosServer
+---------------------------+--------+
| Property                  | Value  |
+---------------------------+--------+
| memory                    | 524288 |
| memory-actual             | 524288 |
| memory-rss                | 6444   |
| tap1fec8fb8-7a_rx         | 22137  |
| tap1fec8fb8-7a_rx_drop    | 0      |
| tap1fec8fb8-7a_rx_errors  | 0      |
| tap1fec8fb8-7a_rx_packets | 166    |
| tap1fec8fb8-7a_tx         | 18032  |
| tap1fec8fb8-7a_tx_drop    | 0      |
| tap1fec8fb8-7a_tx_errors  | 0      |
| tap1fec8fb8-7a_tx_packets | 130    |
| vda_errors                | -1     |
| vda_read                  | 2048   |
| vda_read_req              | 2      |
| vda_write                 | 182272 |
| vda_write_req             | 74     |
+---------------------------+--------+</screen>
              </step>
            </procedure>
          </listitem>
          <listitem>
            <para>Get summary statistics for each project:</para>
            <screen language="console">$ openstack usage list
Usage from 2013-06-25 to 2013-07-24:
+---------+---------+--------------+-----------+---------------+
| Project | Servers | RAM MB-Hours | CPU Hours | Disk GB-Hours |
+---------+---------+--------------+-----------+---------------+
| demo    | 1       | 344064.44    | 672.00    | 0.00          |
| stack   | 3       | 671626.76    | 327.94    | 6558.86       |
+---------+---------+--------------+-----------+---------------+</screen>
          </listitem>
        </itemizedlist>
        <para>System administrators can use the <command>openstack</command> and <command>euca2ools</command>
            commands to manage their clouds.</para>
    <para>The <literal>openstack</literal> client and <literal>euca2ools</literal> can be used by all users, though
            specific commands might be restricted by the Identity service.</para>
    <para>
      <emphasis role="bold">Managing the cloud with the openstack client</emphasis>
    </para>
    <procedure>
      <step>
        <para>The <literal>python-openstackclient</literal> package provides an <literal>openstack</literal> shell that
                    enables Compute API interactions from the command line. Install the client,
                    and provide your user name and password (which can be set as environment
                    variables for convenience), for the ability to administer the cloud from the
                    command line.</para>
        <para>To install python-openstackclient, follow the instructions in the <link xmlns:xl="http://www.w3.org/1999/xlink" xl:href="https://docs.openstack.org/user-guide/common/cli-install-openstack-command-line-clients.html">OpenStack
                        User Guide</link>.</para>
      </step>
      <step>
        <para>Confirm the installation was successful:</para>
        <screen language="console">$ openstack help
usage: openstack [--version] [-v | -q] [--log-file LOG_FILE] [-h] [--debug]
           [--os-cloud &lt;cloud-config-name&gt;]
           [--os-region-name &lt;auth-region-name&gt;]
           [--os-cacert &lt;ca-bundle-file&gt;] [--verify | --insecure]
           [--os-default-domain &lt;auth-domain&gt;]
           ...</screen>
        <para>Running <command>openstack help</command> returns a list of <literal>openstack</literal> commands
                    and parameters. To get help for a subcommand, run:</para>
        <screen language="console">$ openstack help SUBCOMMAND</screen>
        <para>For a complete list of <literal>openstack</literal> commands and parameters, see the
                    <link xmlns:xl="http://www.w3.org/1999/xlink" xl:href="https://docs.openstack.org/cli-reference/openstack.html">OpenStack Command-Line Reference</link>.</para>
      </step>
      <step>
        <para>Set the required parameters as environment variables to make running
                    commands easier. For example, you can add <literal>--os-username</literal> as an
                    <literal>openstack</literal> option, or set it as an environment variable. To set the user
                    name, password, and project as environment variables, use:</para>
        <screen language="console">$ export OS_USERNAME=joecool
$ export OS_PASSWORD=coolword
$ export OS_TENANT_NAME=coolu</screen>
      </step>
      <step>
        <para>The Identity service gives you an authentication endpoint, which Compute
                    recognizes as <literal>OS_AUTH_URL</literal>:</para>
        <screen language="console">$ export OS_AUTH_URL=http://hostname:5000/v2.0</screen>
      </step>
    </procedure>
      </section>
    </section>
  </chapter>
  <chapter xml:id="section-manage-compute-users" xml:base="manage-users">
    <title>Manage Compute users</title>
    <para>Access to the Euca2ools (ec2) API is controlled by an access key and a secret
            key. The user's access key needs to be included in the request, and the request
            must be signed with the secret key. Upon receipt of API requests, Compute
            verifies the signature and runs commands on behalf of the user.</para>
    <para>To begin using Compute, you must create a user with the Identity service.</para>
  </chapter>
  <chapter xml:id="manage-volumes" xml:base="manage-volumes">
    <title>Manage volumes</title>
    <para>Depending on the setup of your cloud provider, they may give you an endpoint to
            use to manage volumes, or there may be an extension under the covers. In either
            case, you can use the <literal>openstack</literal> CLI to manage volumes.</para>
    <table xml:id="id1">
      <title>openstack volume commands</title>
      <tgroup cols="2">
        <colspec colname="c0" colwidth="50"/>
        <colspec colname="c1" colwidth="50"/>
        <thead>
          <row>
            <entry>
              <para>Command</para>
            </entry>
            <entry>
              <para>Description</para>
            </entry>
          </row>
        </thead>
        <tbody>
          <row>
            <entry>
              <para>server add volume</para>
            </entry>
            <entry>
              <para>Attach a volume to a server.</para>
            </entry>
          </row>
          <row>
            <entry>
              <para>volume create</para>
            </entry>
            <entry>
              <para>Add a new volume.</para>
            </entry>
          </row>
          <row>
            <entry>
              <para>volume delete</para>
            </entry>
            <entry>
              <para>Remove or delete a volume.</para>
            </entry>
          </row>
          <row>
            <entry>
              <para>server remove volume</para>
            </entry>
            <entry>
              <para>Detach or remove a volume from a server.</para>
            </entry>
          </row>
          <row>
            <entry>
              <para>volume list</para>
            </entry>
            <entry>
              <para>List all the volumes.</para>
            </entry>
          </row>
          <row>
            <entry>
              <para>volume show</para>
            </entry>
            <entry>
              <para>Show details about a volume.</para>
            </entry>
          </row>
          <row>
            <entry>
              <para>snapshot create</para>
            </entry>
            <entry>
              <para>Add a new snapshot.</para>
            </entry>
          </row>
          <row>
            <entry>
              <para>snapshot delete</para>
            </entry>
            <entry>
              <para>Remove a snapshot.</para>
            </entry>
          </row>
          <row>
            <entry>
              <para>snapshot list</para>
            </entry>
            <entry>
              <para>List all the snapshots.</para>
            </entry>
          </row>
          <row>
            <entry>
              <para>snapshot show</para>
            </entry>
            <entry>
              <para>Show details about a snapshot.</para>
            </entry>
          </row>
          <row>
            <entry>
              <para>volume type create</para>
            </entry>
            <entry>
              <para>Create a new volume type.</para>
            </entry>
          </row>
          <row>
            <entry>
              <para>volume type delete</para>
            </entry>
            <entry>
              <para>Delete a specific flavor</para>
            </entry>
          </row>
          <row>
            <entry>
              <para>volume type list</para>
            </entry>
            <entry>
              <para>Print a list of available 'volume types'.</para>
            </entry>
          </row>
        </tbody>
      </tgroup>
    </table>
    <para>For example, to list IDs and names of volumes, run:</para>
    <screen language="console">$ openstack volume list
+--------+--------------+-----------+------+-------------+
| ID     | Display Name | Status    | Size | Attached to |
+--------+--------------+-----------+------+-------------+
| 86e6cb | testnfs      | available |    1 |             |
| e389f7 | demo         | available |    1 |             |
+--------+--------------+-----------+------+-------------+</screen>
  </chapter>
  <chapter xml:id="migrate-instances" xml:base="migration">
    <title>Migrate instances</title>
    <para>When you want to move an instance from one compute host to another, you can use
            the <command>openstack server migrate</command> command. The scheduler chooses the
            destination compute host based on its settings. This process does not assume
            that the instance has shared storage available on the target host. If you are
            using SSH tunneling, you must ensure that each node is configured with SSH key
            authentication so that the Compute service can use SSH to move disks to other
            nodes. For more information, see <xref linkend="cli-os-migrate-cfg-ssh"/>.</para>
    <procedure>
      <step>
        <para>To list the VMs you want to migrate, run:</para>
        <screen language="console">$ openstack server list</screen>
      </step>
      <step>
        <para>Use the <command>openstack server migrate</command> command.</para>
        <screen language="console">$ openstack server migrate --live TARGET_HOST VM_INSTANCE</screen>
      </step>
      <step>
        <para>To migrate an instance and watch the status, use this example script:</para>
        <screen language="bash">#!/bin/bash

# Provide usage
usage() {
echo "Usage: $0 VM_ID"
exit 1
}

[[ $# -eq 0 ]] &amp;&amp; usage

# Migrate the VM to an alternate hypervisor
echo -n "Migrating instance to alternate host"
VM_ID=$1
openstack server migrate $VM_ID
VM_OUTPUT=$(openstack server show $VM_ID)
VM_STATUS=$(echo "$VM_OUTPUT" | grep status | awk '{print $4}')
while [[ "$VM_STATUS" != "VERIFY_RESIZE" ]]; do
echo -n "."
sleep 2
VM_OUTPUT=$(openstack server show $VM_ID)
VM_STATUS=$(echo "$VM_OUTPUT" | grep status | awk '{print $4}')
done
nova resize-confirm $VM_ID
echo " instance migrated and resized."
echo;

# Show the details for the VM
echo "Updated instance details:"
openstack server show $VM_ID

# Pause to allow users to examine VM details
read -p "Pausing, press &lt;enter&gt; to exit."</screen>
      </step>
    </procedure>
    <note>
      <para>If you see the following error, it means you are either running the command
                with the wrong credentials, such as a non-admin user, or the <literal>policy.json</literal>
                file prevents migration for your user:</para>
      <screen>ERROR (Forbidden): Policy doesn't allow compute_extension:admin_actions:migrate to be performed. (HTTP 403)``</screen>
    </note>
    <note>
      <para>If you see the following error, similar to this message, SSH tunneling was
                not set up between the compute nodes:</para>
      <screen>ProcessExecutionError: Unexpected error while running command.
Stderr: u Host key verification failed.\r\n</screen>
    </note>
    <para>The instance is booted from a new host, but preserves its configuration
            including instance ID, name, IP address, any metadata, and other properties.</para>
  </chapter>
  <chapter xml:id="networking-with-nova-network" xml:base="networking-nova">
    <title>Networking with nova-network</title>
    <important>
      <para><literal>nova-network</literal> was deprecated in the OpenStack Newton release.  In Ocata
                and future releases, you can start <literal>nova-network</literal> only with a cells v1
                configuration. This is not a recommended configuration for deployment.</para>
    </important>
    <para>Understanding the networking configuration options helps you design the best
            configuration for your Compute instances.</para>
    <para>You can choose to either install and configure <literal>nova-network</literal> or use the
            OpenStack Networking service (neutron). This section contains a brief overview
            of <literal>nova-network</literal>. For more information about OpenStack Networking, see
            <link xmlns:xl="http://www.w3.org/1999/xlink" xl:href="https://docs.openstack.org/admin-guide/networking.html">Networking</link>.</para>
    <section xml:id="networking-concepts">
      <title>Networking concepts</title>
      <para>Compute assigns a private IP address to each VM instance. Compute makes a
                distinction between fixed IPs and floating IP. Fixed IPs are IP addresses that
                are assigned to an instance on creation and stay the same until the instance is
                explicitly terminated. Floating IPs are addresses that can be dynamically
                associated with an instance. A floating IP address can be disassociated and
                associated with another instance at any time. A user can reserve a floating IP
                for their project.</para>
      <note>
        <para>Currently, Compute with <literal>nova-network</literal> only supports Linux bridge
                    networking that allows virtual interfaces to connect to the outside network
                    through the physical interface.</para>
      </note>
      <para>The network controller with <literal>nova-network</literal> provides virtual networks to
                enable compute servers to interact with each other and with the public network.
                Compute with <literal>nova-network</literal> supports the following network modes, which are
                implemented as Network Manager types:</para>
      <variablelist>
        <varlistentry>
          <term>Flat Network Manager</term>
          <listitem>
            <para>In this mode, a network administrator specifies a subnet. IP addresses for VM
                            instances are assigned from the subnet, and then injected into the image on
                            launch. Each instance receives a fixed IP address from the pool of available
                            addresses. A system administrator must create the Linux networking bridge
                            (typically named <literal>br100</literal>, although this is configurable) on the systems
                            running the <literal>nova-network</literal> service. All instances of the system are
                            attached to the same bridge, which is configured manually by the network
                            administrator.</para>
          </listitem>
        </varlistentry>
      </variablelist>
      <note>
        <para>Configuration injection currently only works on Linux-style systems that
                    keep networking configuration in <literal>/etc/network/interfaces</literal>.</para>
      </note>
      <variablelist>
        <varlistentry>
          <term>Flat DHCP Network Manager</term>
          <listitem>
            <para>In this mode, OpenStack starts a DHCP server (dnsmasq) to allocate IP
                            addresses to VM instances from the specified subnet, in addition to manually
                            configuring the networking bridge. IP addresses for VM instances are assigned
                            from a subnet specified by the network administrator.</para>
            <para>Like flat mode, all instances are attached to a single bridge on the compute
                            node. Additionally, a DHCP server configures instances depending on
                            single-/multi-host mode, alongside each <literal>nova-network</literal>.  In this mode,
                            Compute does a bit more configuration. It attempts to bridge into an Ethernet
                            device (<literal>flat_interface</literal>, eth0 by default). For every instance, Compute
                            allocates a fixed IP address and configures dnsmasq with the MAC ID and IP
                            address for the VM.  Dnsmasq does not take part in the IP address allocation
                            process, it only hands out IPs according to the mapping done by Compute.
                            Instances receive their fixed IPs with the <command>dhcpdiscover</command> command.
                            These IPs are not assigned to any of the host's network interfaces, only to
                            the guest-side interface for the VM.</para>
            <para>In any setup with flat networking, the hosts providing the <literal>nova-network</literal>
                            service are responsible for forwarding traffic from the private network. They
                            also run and configure dnsmasq as a DHCP server listening on this bridge,
                            usually on IP address 10.0.0.1 (see <link xmlns:xl="http://www.w3.org/1999/xlink" xl:href="#compute-dnsmasq">DHCP server: dnsmasq</link>). Compute can
                            determine the NAT entries for each network, although sometimes NAT is not
                            used, such as when the network has been configured with all public IPs, or if
                            a hardware router is used (which is a high availability option). In this
                            case, hosts need to have <literal>br100</literal> configured and physically connected to any
                            other nodes that are hosting VMs. You must set the <literal>flat_network_bridge</literal>
                            option or create networks with the bridge parameter in order to avoid raising
                            an error. Compute nodes have iptables or ebtables entries created for each
                            project and instance to protect against MAC ID or IP address spoofing and ARP
                            poisoning.</para>
          </listitem>
        </varlistentry>
      </variablelist>
      <note>
        <para>In single-host Flat DHCP mode you will be able to ping VMs through their
                    fixed IP from the <literal>nova-network</literal> node, but you cannot ping them from the
                    compute nodes. This is expected behavior.</para>
      </note>
      <variablelist>
        <varlistentry>
          <term>VLAN Network Manager</term>
          <listitem>
            <para>This is the default mode for OpenStack Compute. In this mode, Compute creates
                            a VLAN and bridge for each project. For multiple-machine installations, the
                            VLAN Network Mode requires a switch that supports VLAN tagging (IEEE 802.1Q).
                            The project gets a range of private IPs that are only accessible from inside
                            the VLAN.  In order for a user to access the instances in their project, a
                            special VPN instance (code named <literal>cloudpipe</literal>) needs to be created.  Compute
                            generates a certificate and key for the user to access the VPN and starts the
                            VPN automatically. It provides a private network segment for each project's
                            instances that can be accessed through a dedicated VPN connection from the
                            internet. In this mode, each project gets its own VLAN, Linux networking
                            bridge, and subnet.</para>
            <para>The subnets are specified by the network administrator, and are assigned
                            dynamically to a project when required. A DHCP server is started for each
                            VLAN to pass out IP addresses to VM instances from the subnet assigned to the
                            project. All instances belonging to one project are bridged into the same
                            VLAN for that project. OpenStack Compute creates the Linux networking bridges
                            and VLANs when required.</para>
          </listitem>
        </varlistentry>
      </variablelist>
      <para>These network managers can co-exist in a cloud system. However, because you
                cannot select the type of network for a given project, you cannot configure
                multiple network types in a single Compute installation.</para>
      <para>All network managers configure the network using network drivers. For example,
                the Linux L3 driver (<literal>l3.py</literal> and <literal>linux_net.py</literal>), which makes use of
                <literal>iptables</literal>, <literal>route</literal> and other network management facilities, and the
                libvirt <link xmlns:xl="http://www.w3.org/1999/xlink" xl:href="http://libvirt.org/formatnwfilter.html">network filtering facilities</link>. The driver is not tied to any
                particular network manager; all network managers use the same driver. The
                driver usually initializes only when the first VM lands on this host node.</para>
      <para>All network managers operate in either single-host or multi-host mode.  This
                choice greatly influences the network configuration. In single-host mode, a
                single <literal>nova-network</literal> service provides a default gateway for VMs and hosts a
                single DHCP server (dnsmasq). In multi-host mode, each compute node runs its
                own <literal>nova-network</literal> service. In both cases, all traffic between VMs and the
                internet flows through <literal>nova-network</literal>. Each mode has benefits and drawbacks.
                For more on this, see the Network Topology section in the <link xmlns:xl="http://www.w3.org/1999/xlink" xl:href="https://docs.openstack.org/ops-guide/arch-network-design.html#network-topology">OpenStack Operations
                    Guide</link>.</para>
      <para>All networking options require network connectivity to be already set up
                between OpenStack physical nodes. OpenStack does not configure any physical
                network interfaces. All network managers automatically create VM virtual
                interfaces. Some network managers can also create network bridges such as
                <literal>br100</literal>.</para>
      <para>The internal network interface is used for communication with VMs. The
                interface should not have an IP address attached to it before OpenStack
                installation, it serves only as a fabric where the actual endpoints are VMs and
                dnsmasq. Additionally, the internal network interface must be in
                <literal>promiscuous</literal> mode, so that it can receive packets whose target MAC address
                is the guest VM, not the host.</para>
      <para>All machines must have a public and internal network interface (controlled by
                these options: <literal>public_interface</literal> for the public interface, and
                <literal>flat_interface</literal> and <literal>vlan_interface</literal> for the internal interface with flat
                or VLAN managers). This guide refers to the public network as the external
                network and the private network as the internal or project network.</para>
      <para>For flat and flat DHCP modes, use the <command>nova network-create</command> command to
                create a network:</para>
      <screen language="console">$ nova network-create vmnet \
  --fixed-range-v4 10.0.0.0/16 --fixed-cidr 10.0.20.0/24 --bridge br100</screen>
      <para>This example uses the following parameters:</para>
      <variablelist>
        <varlistentry>
          <term>
            <literal>--fixed-range-v4</literal>
          </term>
          <listitem>
            <para>Specifies the network subnet.</para>
          </listitem>
        </varlistentry>
        <varlistentry>
          <term>
            <literal>--fixed-cidr</literal>
          </term>
          <listitem>
            <para>Specifies a range of fixed IP addresses to allocate, and can be a subset of
                            the <literal>--fixed-range-v4</literal> argument.</para>
          </listitem>
        </varlistentry>
        <varlistentry>
          <term>
            <literal>--bridge</literal>
          </term>
          <listitem>
            <para>Specifies the bridge device to which this network is connected on every
                            compute node.</para>
          </listitem>
        </varlistentry>
      </variablelist>
    </section>
    <section xml:id="compute-dnsmasq">
      <title>DHCP server: dnsmasq</title>
      <para>The Compute service uses <link xmlns:xl="http://www.w3.org/1999/xlink" xl:href="http://www.thekelleys.org.uk/dnsmasq/doc.html">dnsmasq</link> as the DHCP server when
                using either Flat DHCP Network Manager or VLAN Network Manager. For Compute to
                operate in IPv4/IPv6 dual-stack mode, use at least dnsmasq v2.63. The
                <literal>nova-network</literal> service is responsible for starting dnsmasq processes.</para>
      <para>The behavior of dnsmasq can be customized by creating a dnsmasq configuration
                file. Specify the configuration file using the <literal>dnsmasq_config_file</literal>
                configuration option:</para>
      <screen language="ini">dnsmasq_config_file=/etc/dnsmasq-nova.conf</screen>
      <para>For more information about creating a dnsmasq configuration file, see the
                <xref linkend="../configuration/config"/>, and <link xmlns:xl="http://www.w3.org/1999/xlink" xl:href="http://www.thekelleys.org.uk/dnsmasq/docs/dnsmasq.conf.example">the dnsmasq documentation</link>.</para>
      <para>Dnsmasq also acts as a caching DNS server for instances. You can specify the
                DNS server that dnsmasq uses by setting the <literal>dns_server</literal> configuration option
                in <literal>/etc/nova/nova.conf</literal>. This example configures dnsmasq to use Google's
                public DNS server:</para>
      <screen language="ini">dns_server=8.8.8.8</screen>
      <para>Dnsmasq logs to syslog (typically <literal>/var/log/syslog</literal> or <literal>/var/log/messages</literal>,
                depending on Linux distribution). Logs can be useful for troubleshooting,
                especially in a situation where VM instances boot successfully but are not
                reachable over the network.</para>
      <para>Administrators can specify the starting point IP address to reserve with the
                DHCP server (in the format n.n.n.n) with this command:</para>
      <screen language="console">$ nova-manage fixed reserve --address IP_ADDRESS</screen>
      <para>This reservation only affects which IP address the VMs start at, not the fixed
                IP addresses that <literal>nova-network</literal> places on the bridges.</para>
    </section>
    <section xml:id="configure-compute-to-use-ipv6-addresses">
      <title>Configure Compute to use IPv6 addresses</title>
      <para>If you are using OpenStack Compute with <literal>nova-network</literal>, you can put Compute
                into dual-stack mode, so that it uses both IPv4 and IPv6 addresses for
                communication. In dual-stack mode, instances can acquire their IPv6 global
                unicast addresses by using a stateless address auto-configuration mechanism
                [RFC 4862/2462]. IPv4/IPv6 dual-stack mode works with both <literal>VlanManager</literal> and
                <literal>FlatDHCPManager</literal> networking modes.</para>
      <para>In <literal>VlanManager</literal> networking mode, each project uses a different 64-bit global
                routing prefix. In <literal>FlatDHCPManager</literal> mode, all instances use one 64-bit
                global routing prefix.</para>
      <para>This configuration was tested with virtual machine images that have an IPv6
                stateless address auto-configuration capability. This capability is required
                for any VM to run with an IPv6 address. You must use an EUI-64 address for
                stateless address auto-configuration. Each node that executes a <literal>nova-*</literal>
                service must have <literal>python-netaddr</literal> and <literal>radvd</literal> installed.</para>
      <procedure>
        <step>
          <para>For every node running a <literal>nova-*</literal> service, install <literal>python-netaddr</literal>:</para>
          <screen language="console"># apt-get install python-netaddr</screen>
        </step>
        <step>
          <para>For every node running <literal>nova-network</literal>, install <literal>radvd</literal> and configure
                        IPv6 networking:</para>
          <screen language="console"># apt-get install radvd
# echo 1 &gt; /proc/sys/net/ipv6/conf/all/forwarding
# echo 0 &gt; /proc/sys/net/ipv6/conf/all/accept_ra</screen>
        </step>
        <step>
          <para>On all nodes, edit the <literal>nova.conf</literal> file and specify <literal>use_ipv6 = True</literal>.</para>
        </step>
        <step>
          <para>Restart all <literal>nova-*</literal> services.</para>
        </step>
      </procedure>
      <para>You can use the following options with the <command>nova network-create</command>
                command:</para>
      <itemizedlist>
        <listitem>
          <para>Add a fixed range for IPv6 addresses to the <command>nova network-create</command>
                        command. Specify <literal>public</literal> or <literal>private</literal> after the <literal>network-create</literal>
                        parameter.</para>
          <screen language="console">$ nova network-create public --fixed-range-v4 FIXED_RANGE_V4 \
  --vlan VLAN_ID --vpn VPN_START --fixed-range-v6 FIXED_RANGE_V6</screen>
        </listitem>
        <listitem>
          <para>Set the IPv6 global routing prefix by using the <literal>--fixed_range_v6</literal>
                        parameter. The default value for the parameter is <literal>fd00::/48</literal>.</para>
          <para>When you use <literal>FlatDHCPManager</literal>, the command uses the original
                        <literal>--fixed_range_v6</literal> value. For example:</para>
          <screen language="console">$ nova network-create public  --fixed-range-v4 10.0.2.0/24 \
  --fixed-range-v6 fd00:1::/48</screen>
        </listitem>
        <listitem>
          <para>When you use <literal>VlanManager</literal>, the command increments the subnet ID to create
                        subnet prefixes. Guest VMs use this prefix to generate their IPv6 global
                        unicast addresses. For example:</para>
          <screen language="console">$ nova network-create public --fixed-range-v4 10.0.1.0/24 --vlan 100 \
  --vpn 1000 --fixed-range-v6 fd00:1::/48</screen>
        </listitem>
      </itemizedlist>
      <table xml:id="id8">
        <title>Description of IPv6 configuration options</title>
        <tgroup cols="2">
          <colspec colname="c0" colwidth="50"/>
          <colspec colname="c1" colwidth="50"/>
          <thead>
            <row>
              <entry>
                <para>Configuration option = Default value</para>
              </entry>
              <entry>
                <para>Description</para>
              </entry>
            </row>
            <row>
              <entry>
                <para>[DEFAULT]</para>
              </entry>
              <entry/>
            </row>
          </thead>
          <tbody>
            <row>
              <entry>
                <para>fixed_range_v6 = fd00::/48</para>
              </entry>
              <entry>
                <para>(StrOpt) Fixed IPv6 address block</para>
              </entry>
            </row>
            <row>
              <entry>
                <para>gateway_v6 = None</para>
              </entry>
              <entry>
                <para>(StrOpt) Default IPv6 gateway</para>
              </entry>
            </row>
            <row>
              <entry>
                <para>ipv6_backend = rfc2462</para>
              </entry>
              <entry>
                <para>(StrOpt) Backend to use for IPv6 generation</para>
              </entry>
            </row>
            <row>
              <entry>
                <para>use_ipv6 = False</para>
              </entry>
              <entry>
                <para>(BoolOpt) Use IPv6</para>
              </entry>
            </row>
          </tbody>
        </tgroup>
      </table>
    </section>
    <section xml:id="metadata-service">
      <title>Metadata service</title>
      <para>Compute uses a metadata service for virtual machine instances to retrieve
                instance-specific data. Instances access the metadata service at
                <literal>http://169.254.169.254</literal>. The metadata service supports two sets of APIs: an
                OpenStack metadata API and an EC2-compatible API. Both APIs are versioned by
                date.</para>
      <para>To retrieve a list of supported versions for the OpenStack metadata API, make a
                GET request to <literal>http://169.254.169.254/openstack</literal>:</para>
      <screen language="console">$ curl http://169.254.169.254/openstack
2012-08-10
2013-04-04
2013-10-17
latest</screen>
      <para>To list supported versions for the EC2-compatible metadata API, make a GET
                request to <literal>http://169.254.169.254</literal>:</para>
      <screen language="console">$ curl http://169.254.169.254
1.0
2007-01-19
2007-03-01
2007-08-29
2007-10-10
2007-12-15
2008-02-01
2008-09-01
2009-04-04
latest</screen>
      <para>If you write a consumer for one of these APIs, always attempt to access the
                most recent API version supported by your consumer first, then fall back to an
                earlier version if the most recent one is not available.</para>
      <para>Metadata from the OpenStack API is distributed in JSON format. To retrieve the
                metadata, make a GET request to
                <literal>http://169.254.169.254/openstack/2012-08-10/meta_data.json</literal>:</para>
      <screen language="console">$ curl http://169.254.169.254/openstack/2012-08-10/meta_data.json</screen>
      <screen language="json">{
   "uuid": "d8e02d56-2648-49a3-bf97-6be8f1204f38",
   "availability_zone": "nova",
   "hostname": "test.novalocal",
   "launch_index": 0,
   "meta": {
      "priority": "low",
      "role": "webserver"
   },
   "project_id": "f7ac731cc11f40efbc03a9f9e1d1d21f",
   "public_keys": {
       "mykey": "ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAAAgQDYVEprvtYJXVOBN0XNKV\
                 VRNCRX6BlnNbI+USLGais1sUWPwtSg7z9K9vhbYAPUZcq8c/s5S9dg5vTH\
                 bsiyPCIDOKyeHba4MUJq8Oh5b2i71/3BISpyxTBH/uZDHdslW2a+SrPDCe\
                 uMMoss9NFhBdKtDkdG9zyi0ibmCP6yMdEX8Q== Generated by Nova\n"
   },
   "name": "test"
}</screen>
      <para>Instances also retrieve user data (passed as the <literal>user_data</literal> parameter in the
                API call or by the <literal>--user_data</literal> flag in the <command>openstack server
                    create</command> command) through the metadata service, by making a GET request to
                <literal>http://169.254.169.254/openstack/2012-08-10/user_data</literal>:</para>
      <screen language="console">$ curl http://169.254.169.254/openstack/2012-08-10/user_data
#!/bin/bash
echo 'Extra user data here'</screen>
      <para>The metadata service has an API that is compatible with version 2009-04-04 of
                the <link xmlns:xl="http://www.w3.org/1999/xlink" xl:href="http://docs.amazonwebservices.com/AWSEC2/2009-04-04/UserGuide/AESDG-chapter-instancedata.html">Amazon EC2 metadata service</link>.
                This means that virtual machine images designed for EC2 will work properly with
                OpenStack.</para>
      <para>The EC2 API exposes a separate URL for each metadata element. Retrieve a
                listing of these elements by making a GET query to
                <literal>http://169.254.169.254/2009-04-04/meta-data/</literal>:</para>
      <screen language="console">$ curl http://169.254.169.254/2009-04-04/meta-data/
ami-id
ami-launch-index
ami-manifest-path
block-device-mapping/
hostname
instance-action
instance-id
instance-type
kernel-id
local-hostname
local-ipv4
placement/
public-hostname
public-ipv4
public-keys/
ramdisk-id
reservation-id
security-groups</screen>
      <screen language="console">$ curl http://169.254.169.254/2009-04-04/meta-data/block-device-mapping/
ami</screen>
      <screen language="console">$ curl http://169.254.169.254/2009-04-04/meta-data/placement/
availability-zone</screen>
      <screen language="console">$ curl http://169.254.169.254/2009-04-04/meta-data/public-keys/
0=mykey</screen>
      <para>Instances can retrieve the public SSH key (identified by keypair name when a
                user requests a new instance) by making a GET request to
                <literal>http://169.254.169.254/2009-04-04/meta-data/public-keys/0/openssh-key</literal>:</para>
      <screen language="console">$ curl http://169.254.169.254/2009-04-04/meta-data/public-keys/0/openssh-key
ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAAAgQDYVEprvtYJXVOBN0XNKVVRNCRX6BlnNbI+US\
LGais1sUWPwtSg7z9K9vhbYAPUZcq8c/s5S9dg5vTHbsiyPCIDOKyeHba4MUJq8Oh5b2i71/3B\
ISpyxTBH/uZDHdslW2a+SrPDCeuMMoss9NFhBdKtDkdG9zyi0ibmCP6yMdEX8Q== Generated\
by Nova</screen>
      <para>Instances can retrieve user data by making a GET request to
                <literal>http://169.254.169.254/2009-04-04/user-data</literal>:</para>
      <screen language="console">$ curl http://169.254.169.254/2009-04-04/user-data
#!/bin/bash
echo 'Extra user data here'</screen>
      <para>The metadata service is implemented by either the <literal>nova-api</literal> service or the
                <literal>nova-api-metadata</literal> service. Note that the <literal>nova-api-metadata</literal> service is
                generally only used when running in multi-host mode, as it retrieves
                instance-specific metadata. If you are running the <literal>nova-api</literal> service, you
                must have <literal>metadata</literal> as one of the elements listed in the <literal>enabled_apis</literal>
                configuration option in <literal>/etc/nova/nova.conf</literal>. The default <literal>enabled_apis</literal>
                configuration setting includes the metadata service, so you do not need to
                modify it.</para>
      <para>Hosts access the service at <literal>169.254.169.254:80</literal>, and this is translated to
                <literal>metadata_host:metadata_port</literal> by an iptables rule established by the
                <literal>nova-network</literal> service. In multi-host mode, you can set <literal>metadata_host</literal> to
                <literal>127.0.0.1</literal>.</para>
      <para>For instances to reach the metadata service, the <literal>nova-network</literal> service must
                configure iptables to NAT port <literal>80</literal> of the <literal>169.254.169.254</literal> address to the
                IP address specified in <literal>metadata_host</literal> (this defaults to <literal>$my_ip</literal>, which
                is the IP address of the <literal>nova-network</literal> service) and port specified in
                <literal>metadata_port</literal> (which defaults to <literal>8775</literal>) in <literal>/etc/nova/nova.conf</literal>.</para>
      <note>
        <para>The <literal>metadata_host</literal> configuration option must be an IP address, not a host
                    name.</para>
      </note>
      <para>The default Compute service settings assume that <literal>nova-network</literal> and
                <literal>nova-api</literal> are running on the same host. If this is not the case, in the
                <literal>/etc/nova/nova.conf</literal> file on the host running <literal>nova-network</literal>, set the
                <literal>metadata_host</literal> configuration option to the IP address of the host where
                <literal>nova-api</literal> is running.</para>
      <table xml:id="id9">
        <title>Description of metadata configuration options</title>
        <tgroup cols="2">
          <colspec colname="c0" colwidth="50"/>
          <colspec colname="c1" colwidth="50"/>
          <thead>
            <row>
              <entry>
                <para>Configuration option = Default value</para>
              </entry>
              <entry>
                <para>Description</para>
              </entry>
            </row>
            <row>
              <entry>
                <para>[DEFAULT]</para>
              </entry>
              <entry/>
            </row>
          </thead>
          <tbody>
            <row>
              <entry>
                <para>metadata_cache_expiration = 15</para>
              </entry>
              <entry>
                <para>(IntOpt) Time in seconds to cache metadata; 0 to disable metadata
                                    caching entirely (not recommended). Increasing this should improve
                                    response times of the metadata API when under heavy load. Higher values
                                    may increase memory usage and result in longer times for host metadata
                                    changes to take effect.</para>
              </entry>
            </row>
            <row>
              <entry>
                <para>metadata_host = $my_ip</para>
              </entry>
              <entry>
                <para>(StrOpt) The IP address for the metadata API server</para>
              </entry>
            </row>
            <row>
              <entry>
                <para>metadata_listen = 0.0.0.0</para>
              </entry>
              <entry>
                <para>(StrOpt) The IP address on which the metadata API will listen.</para>
              </entry>
            </row>
            <row>
              <entry>
                <para>metadata_listen_port = 8775</para>
              </entry>
              <entry>
                <para>(IntOpt) The port on which the metadata API will listen.</para>
              </entry>
            </row>
            <row>
              <entry>
                <para>metadata_manager = nova.api.manager.MetadataManager</para>
              </entry>
              <entry>
                <para>(StrOpt) OpenStack metadata service manager</para>
              </entry>
            </row>
            <row>
              <entry>
                <para>metadata_port = 8775</para>
              </entry>
              <entry>
                <para>(IntOpt) The port for the metadata API port</para>
              </entry>
            </row>
            <row>
              <entry>
                <para>metadata_workers = None</para>
              </entry>
              <entry>
                <para>(IntOpt) Number of workers for metadata service. The default will be the number of CPUs available.</para>
              </entry>
            </row>
            <row>
              <entry>
                <para>vendordata_driver = nova.api.metadata.vendordata_json.JsonFileVendorData</para>
              </entry>
              <entry>
                <para>(StrOpt) Driver to use for vendor data</para>
              </entry>
            </row>
            <row>
              <entry>
                <para>vendordata_jsonfile_path = None</para>
              </entry>
              <entry>
                <para>(StrOpt) File to load JSON formatted vendor data from</para>
              </entry>
            </row>
          </tbody>
        </tgroup>
      </table>
    </section>
    <section xml:id="enable-ping-and-ssh-on-vms">
      <title>Enable ping and SSH on VMs</title>
      <para>You need to enable <literal>ping</literal> and <literal>ssh</literal> on your VMs for network access.  This
                can be done with either the <command>nova</command> or <command>euca2ools</command> commands.</para>
      <note>
        <para>Run these commands as root only if the credentials used to interact with
                    <literal>nova-api</literal> are in <literal>/root/.bashrc</literal>. If the EC2 credentials in the
                    <literal>.bashrc</literal> file are for an unprivileged user, you must run these commands
                    as that user instead.</para>
      </note>
      <para>Enable ping and SSH with <command>openstack security group rule create</command>
                commands:</para>
      <screen language="console">$ openstack security group rule create --protocol icmp default
$ openstack security group rule create --protocol tcp --dst-port 22:22 default</screen>
      <para>Enable ping and SSH with <literal>euca2ools</literal>:</para>
      <screen language="console">$ euca-authorize -P icmp -t -1:-1 -s 0.0.0.0/0 default
$ euca-authorize -P tcp -p 22 -s 0.0.0.0/0 default</screen>
      <para>If you have run these commands and still cannot ping or SSH your instances,
                check the number of running <literal>dnsmasq</literal> processes, there should be two. If not,
                kill the processes and restart the service with these commands:</para>
      <screen language="console"># killall dnsmasq
# service nova-network restart</screen>
    </section>
    <section xml:id="configure-public-floating-ip-addresses">
      <title>Configure public (floating) IP addresses</title>
      <para>This section describes how to configure floating IP addresses with
                <literal>nova-network</literal>. For information about doing this with OpenStack Networking,
                see <link xmlns:xl="http://www.w3.org/1999/xlink" xl:href="https://docs.openstack.org/neutron/latest/admin/archives/adv-features.html#l3-routing-and-nat">L3-routing-and-NAT</link>.</para>
      <section xml:id="private-and-public-ip-addresses">
        <title>Private and public IP addresses</title>
        <para>In this section, the term floating IP address is used to refer to an IP
                    address, usually public, that you can dynamically add to a running virtual
                    instance.</para>
        <para>Every virtual instance is automatically assigned a private IP address.  You can
                    choose to assign a public (or floating) IP address instead.  OpenStack Compute
                    uses network address translation (NAT) to assign floating IPs to virtual
                    instances.</para>
        <para>To be able to assign a floating IP address, edit the <literal>/etc/nova/nova.conf</literal>
                    file to specify which interface the <literal>nova-network</literal> service should bind public
                    IP addresses to:</para>
        <screen language="ini">public_interface=VLAN100</screen>
        <para>If you make changes to the <literal>/etc/nova/nova.conf</literal> file while the
                    <literal>nova-network</literal> service is running, you will need to restart the service to
                    pick up the changes.</para>
        <note>
          <para>Floating IPs are implemented by using a source NAT (SNAT rule in iptables),
                        so security groups can sometimes display inconsistent behavior if VMs use
                        their floating IP to communicate with other VMs, particularly on the same
                        physical host. Traffic from VM to VM across the fixed network does not have
                        this issue, and so this is the recommended setup. To ensure that traffic
                        does not get SNATed to the floating range, explicitly set:</para>
          <screen language="ini">dmz_cidr=x.x.x.x/y</screen>
          <para>The <literal>x.x.x.x/y</literal> value specifies the range of floating IPs for each pool of
                        floating IPs that you define. This configuration is also required if the VMs
                        in the source group have floating IPs.</para>
        </note>
      </section>
      <section xml:id="enable-ip-forwarding">
        <title>Enable IP forwarding</title>
        <para>IP forwarding is disabled by default on most Linux distributions. You will need
                    to enable it in order to use floating IPs.</para>
        <note>
          <para>IP forwarding only needs to be enabled on the nodes that run
                        <literal>nova-network</literal>. However, you will need to enable it on all compute nodes
                        if you use <literal>multi_host</literal> mode.</para>
        </note>
        <para>To check if IP forwarding is enabled, run:</para>
        <screen language="console">$ cat /proc/sys/net/ipv4/ip_forward
0</screen>
        <para>Alternatively, run:</para>
        <screen language="console">$ sysctl net.ipv4.ip_forward
net.ipv4.ip_forward = 0</screen>
        <para>In these examples, IP forwarding is disabled.</para>
        <para>To enable IP forwarding dynamically, run:</para>
        <screen language="console"># sysctl -w net.ipv4.ip_forward=1</screen>
        <para>Alternatively, run:</para>
        <screen language="console"># echo 1 &gt; /proc/sys/net/ipv4/ip_forward</screen>
        <para>To make the changes permanent, edit the <literal>/etc/sysctl.conf</literal> file and update
                    the IP forwarding setting:</para>
        <screen language="ini">net.ipv4.ip_forward = 1</screen>
        <para>Save the file and run this command to apply the changes:</para>
        <screen language="console"># sysctl -p</screen>
        <para>You can also apply the changes by restarting the network service:</para>
        <itemizedlist>
          <listitem>
            <para>on Ubuntu, Debian:</para>
            <screen language="console"># /etc/init.d/networking restart</screen>
          </listitem>
          <listitem>
            <para>on RHEL, Fedora, CentOS, openSUSE and SLES:</para>
            <screen language="console"># service network restart</screen>
          </listitem>
        </itemizedlist>
      </section>
      <section xml:id="create-a-list-of-available-floating-ip-addresses">
        <title>Create a list of available floating IP addresses</title>
        <para>Compute maintains a list of floating IP addresses that are available for
                    assigning to instances. Use the <command>nova-manage floating</command> commands to
                    perform floating IP operations:</para>
        <itemizedlist>
          <listitem>
            <para>Add entries to the list:</para>
            <screen language="console"># nova-manage floating create --pool nova --ip_range 68.99.26.170/31</screen>
          </listitem>
          <listitem>
            <para>List the floating IP addresses in the pool:</para>
            <screen language="console"># openstack floating ip list</screen>
          </listitem>
          <listitem>
            <para>Create specific floating IPs for either a single address or a subnet:</para>
            <screen language="console"># nova-manage floating create --pool POOL_NAME --ip_range CIDR</screen>
          </listitem>
          <listitem>
            <para>Remove floating IP addresses using the same parameters as the create command:</para>
            <screen language="console"># openstack floating ip delete CIDR</screen>
          </listitem>
        </itemizedlist>
        <para>For more information about how administrators can associate floating IPs with
                    instances, see <link xmlns:xl="http://www.w3.org/1999/xlink" xl:href="https://docs.openstack.org/python-openstackclient/latest/cli/command-objects/ip-floating.html">ip floating</link> in the python-openstackclient User Documentation.</para>
      </section>
      <section xml:id="automatically-add-floating-ips">
        <title>Automatically add floating IPs</title>
        <para>You can configure <literal>nova-network</literal> to automatically allocate and assign a
                    floating IP address to virtual instances when they are launched. Add this line
                    to the <literal>/etc/nova/nova.conf</literal> file:</para>
        <screen language="ini">auto_assign_floating_ip=True</screen>
        <para>Save the file, and restart <literal>nova-network</literal></para>
        <note>
          <para>If this option is enabled, but all floating IP addresses have already been
                        allocated, the <command>openstack server create</command> command will fail.</para>
        </note>
      </section>
    </section>
    <section xml:id="remove-a-network-from-a-project">
      <title>Remove a network from a project</title>
      <para>You cannot delete a network that has been associated to a project. This section
                describes the procedure for dissociating it so that it can be deleted.</para>
      <para>In order to disassociate the network, you will need the ID of the project it
                has been associated to. To get the project ID, you will need to be an
                administrator.</para>
      <para>Disassociate the network from the project using the <command>nova-manage
                    project scrub</command> command, with the project ID as the final parameter:</para>
      <screen language="console"># nova-manage project scrub --project ID</screen>
    </section>
    <section xml:id="multiple-interfaces-for-instances-multinic">
      <title>Multiple interfaces for instances (multinic)</title>
      <para>The multinic feature allows you to use more than one interface with your
                instances. This is useful in several scenarios:</para>
      <itemizedlist>
        <listitem>
          <para>SSL Configurations (VIPs)</para>
        </listitem>
        <listitem>
          <para>Services failover/HA</para>
        </listitem>
        <listitem>
          <para>Bandwidth Allocation</para>
        </listitem>
        <listitem>
          <para>Administrative/Public access to your instances</para>
        </listitem>
      </itemizedlist>
      <para>Each VIP represents a separate network with its own IP block. Every network
                mode has its own set of changes regarding multinic usage:</para>
      <mediaobject>
        <imageobject role="fo">
          <imagedata fileref="SCH_5007_V00_NUAC-multi_nic_OpenStack-Flat-manager.jpg" width="600"/>
        </imageobject>
        <imageobject role="html">
          <imagedata fileref="SCH_5007_V00_NUAC-multi_nic_OpenStack-Flat-manager.jpg" width="600"/>
        </imageobject>
      </mediaobject>
      <mediaobject>
        <imageobject role="fo">
          <imagedata fileref="SCH_5007_V00_NUAC-multi_nic_OpenStack-Flat-DHCP-manager.jpg" width="600"/>
        </imageobject>
        <imageobject role="html">
          <imagedata fileref="SCH_5007_V00_NUAC-multi_nic_OpenStack-Flat-DHCP-manager.jpg" width="600"/>
        </imageobject>
      </mediaobject>
      <mediaobject>
        <imageobject role="fo">
          <imagedata fileref="SCH_5007_V00_NUAC-multi_nic_OpenStack-VLAN-manager.jpg" width="600"/>
        </imageobject>
        <imageobject role="html">
          <imagedata fileref="SCH_5007_V00_NUAC-multi_nic_OpenStack-VLAN-manager.jpg" width="600"/>
        </imageobject>
      </mediaobject>
      <section xml:id="using-multinic">
        <title>Using multinic</title>
        <para>In order to use multinic, create two networks, and attach them to the project
                    (named <literal>project</literal> on the command line):</para>
        <screen language="console">$ nova network-create first-net --fixed-range-v4 20.20.0.0/24 --project-id $your-project
$ nova network-create second-net --fixed-range-v4 20.20.10.0/24 --project-id $your-project</screen>
        <para>Each new instance will now receive two IP addresses from their respective DHCP
                    servers:</para>
        <screen language="console">$ openstack server list
+---------+----------+--------+-----------------------------------------+------------+
|ID       | Name     | Status | Networks                                | Image Name |
+---------+----------+--------+-----------------------------------------+------------+
| 1234... | MyServer | ACTIVE | network2=20.20.0.3; private=20.20.10.14 | cirros     |
+---------+----------+--------+-----------------------------------------+------------+</screen>
        <note>
          <para>Make sure you start the second interface on the instance, or it won't be
                        reachable through the second IP.</para>
        </note>
        <para>This example demonstrates how to set up the interfaces within the instance.
                    This is the configuration that needs to be applied inside the image.</para>
        <para>Edit the <literal>/etc/network/interfaces</literal> file:</para>
        <screen language="bash"># The loopback network interface
auto lo
iface lo inet loopback

auto eth0
iface eth0 inet dhcp

auto eth1
iface eth1 inet dhcp</screen>
        <para>If the Virtual Network Service Neutron is installed, you can specify the
                    networks to attach to the interfaces by using the <literal>--nic</literal> flag with the
                    <command>openstack server create</command> command:</para>
        <screen language="console">$ openstack server create --image ed8b2a37-5535-4a5f-a615-443513036d71 \
  --flavor 1 --nic net-id=NETWORK1_ID --nic net-id=NETWORK2_ID test-vm1</screen>
      </section>
    </section>
    <section xml:id="troubleshooting-networking">
      <title>Troubleshooting Networking</title>
      <section xml:id="cannot-reach-floating-ips">
        <title>Cannot reach floating IPs</title>
      </section>
      <section xml:id="problem">
        <title>Problem</title>
        <para>You cannot reach your instances through the floating IP address.</para>
      </section>
      <section xml:id="solution">
        <title>Solution</title>
        <itemizedlist>
          <listitem>
            <para>Check that the default security group allows ICMP (ping) and SSH (port 22),
                            so that you can reach the instances:</para>
            <screen language="console">$ openstack security group rule list default
+--------------------------------------+-------------+-----------+-----------------+-----------------------+
| ID                                   | IP Protocol | IP Range  | Port Range      | Remote Security Group |
+--------------------------------------+-------------+-----------+-----------------+-----------------------+
| 63536865-e5b6-4df1-bac5-ca6d97d8f54d | tcp         | 0.0.0.0/0 | 22:22           | None                  |
| e9d3200f-647a-4293-a9fc-e65ceee189ae | icmp        | 0.0.0.0/0 | type=1:code=-1  | None                  |
+--------------------------------------+-------------+-----------+-----------------+-----------------------+</screen>
          </listitem>
          <listitem>
            <para>Check the NAT rules have been added to iptables on the node that is running
                            <literal>nova-network</literal>:</para>
            <screen language="console"># iptables -L -nv -t nat \
    -A nova-network-PREROUTING -d 68.99.26.170/32 -j DNAT --to-destination 10.0.0.3 \
    -A nova-network-floating-snat -s 10.0.0.3/32 -j SNAT --to-source 68.99.26.170</screen>
          </listitem>
          <listitem>
            <para>Check that the public address (<literal>68.99.26.170</literal> in this example), has been
                            added to your public interface. You should see the address in the listing
                            when you use the <command>ip addr</command> command:</para>
            <screen language="console">$ ip addr
2: eth0: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc mq state UP qlen 1000
link/ether xx:xx:xx:17:4b:c2 brd ff:ff:ff:ff:ff:ff
inet 13.22.194.80/24 brd 13.22.194.255 scope global eth0
inet 68.99.26.170/32 scope global eth0
inet6 fe80::82b:2bf:fe1:4b2/64 scope link
valid_lft forever preferred_lft forever</screen>
            <note>
              <para>You cannot use <literal>SSH</literal> to access an instance with a public IP from within
                                the same server because the routing configuration does not allow it.</para>
            </note>
          </listitem>
          <listitem>
            <para>Use <literal>tcpdump</literal> to identify if packets are being routed to the inbound
                            interface on the compute host. If the packets are reaching the compute hosts
                            but the connection is failing, the issue may be that the packet is being
                            dropped by reverse path filtering. Try disabling reverse-path filtering on
                            the inbound interface. For example, if the inbound interface is <literal>eth2</literal>,
                            run:</para>
            <screen language="console"># sysctl -w net.ipv4.conf.ETH2.rp_filter=0</screen>
            <para>If this solves the problem, add the following line to <literal>/etc/sysctl.conf</literal> so
                            that the reverse-path filter is persistent:</para>
            <screen language="ini">net.ipv4.conf.rp_filter=0</screen>
          </listitem>
        </itemizedlist>
      </section>
      <section xml:id="temporarily-disable-firewall">
        <title>Temporarily disable firewall</title>
      </section>
      <section xml:id="id2">
        <title>Problem</title>
        <para>Networking issues prevent administrators accessing or reaching VM's through
                    various pathways.</para>
      </section>
      <section xml:id="id3">
        <title>Solution</title>
        <para>You can disable the firewall by setting this option in <literal>/etc/nova/nova.conf</literal>:</para>
        <screen language="ini">firewall_driver=nova.virt.firewall.NoopFirewallDriver</screen>
        <warning>
          <para>We strongly recommend you remove this line to re-enable the firewall once
                        your networking issues have been resolved.</para>
        </warning>
      </section>
      <section xml:id="packet-loss-from-instances-to-nova-network-server-vlanmanager-mode">
        <title>Packet loss from instances to nova-network server (VLANManager mode)</title>
      </section>
      <section xml:id="id4">
        <title>Problem</title>
        <para>If you can access your instances with <literal>SSH</literal> but the network to your instance
                    is slow, or if you find that running certain operations are slower than they
                    should be (for example, <literal>sudo</literal>), packet loss could be occurring on the
                    connection to the instance.</para>
        <para>Packet loss can be caused by Linux networking configuration settings related to
                    bridges. Certain settings can cause packets to be dropped between the VLAN
                    interface (for example, <literal>vlan100</literal>) and the associated bridge interface (for
                    example, <literal>br100</literal>) on the host running <literal>nova-network</literal>.</para>
      </section>
      <section xml:id="id5">
        <title>Solution</title>
        <para>One way to check whether this is the problem is to open three terminals and run
                    the following commands:</para>
        <procedure>
          <step>
            <para>In the first terminal, on the host running <literal>nova-network</literal>, use <literal>tcpdump</literal>
                            on the VLAN interface to monitor DNS-related traffic (UDP, port 53). As
                            root, run:</para>
            <screen language="console"># tcpdump -K -p -i vlan100 -v -vv udp port 53</screen>
          </step>
          <step>
            <para>In the second terminal, also on the host running <literal>nova-network</literal>, use
                            <literal>tcpdump</literal> to monitor DNS-related traffic on the bridge interface.  As
                            root, run:</para>
            <screen language="console"># tcpdump -K -p -i br100 -v -vv udp port 53</screen>
          </step>
          <step>
            <para>In the third terminal, use <literal>SSH</literal> to access the instance and generate DNS
                            requests by using the <command>nslookup</command> command:</para>
            <screen language="console">$ nslookup www.google.com</screen>
            <para>The symptoms may be intermittent, so try running <command>nslookup</command>
                            multiple times. If the network configuration is correct, the command should
                            return immediately each time. If it is not correct, the command hangs for
                            several seconds before returning.</para>
          </step>
          <step>
            <para>If the <command>nslookup</command> command sometimes hangs, and there are packets
                            that appear in the first terminal but not the second, then the problem may
                            be due to filtering done on the bridges. Try disabling filtering, and
                            running these commands as root:</para>
            <screen language="console"># sysctl -w net.bridge.bridge-nf-call-arptables=0
# sysctl -w net.bridge.bridge-nf-call-iptables=0
# sysctl -w net.bridge.bridge-nf-call-ip6tables=0</screen>
            <para>If this solves your issue, add the following line to <literal>/etc/sysctl.conf</literal> so
                            that the changes are persistent:</para>
            <screen language="ini">net.bridge.bridge-nf-call-arptables=0
net.bridge.bridge-nf-call-iptables=0
net.bridge.bridge-nf-call-ip6tables=0</screen>
          </step>
        </procedure>
      </section>
      <section xml:id="kvm-network-connectivity-works-initially-then-fails">
        <title>KVM: Network connectivity works initially, then fails</title>
      </section>
      <section xml:id="id6">
        <title>Problem</title>
        <para>With KVM hypervisors, instances running Ubuntu 12.04 sometimes lose network
                    connectivity after functioning properly for a period of time.</para>
      </section>
      <section xml:id="id7">
        <title>Solution</title>
        <para>Try loading the <literal>vhost_net</literal> kernel module as a workaround for this issue (see
                    <link xmlns:xl="http://www.w3.org/1999/xlink" xl:href="https://bugs.launchpad.net/ubuntu/+source/libvirt/+bug/997978/">bug #997978</link>) . This
                    kernel module may also <link xmlns:xl="http://www.w3.org/1999/xlink" xl:href="http://www.linux-kvm.org/page/VhostNet">improve network performance</link> on KVM. To load the kernel module:</para>
        <screen language="console"># modprobe vhost_net</screen>
        <note>
          <para>Loading the module has no effect on running instances.</para>
        </note>
      </section>
    </section>
  </chapter>
  <chapter xml:id="recover-from-a-failed-compute-node" xml:base="node-down">
    <title>Recover from a failed compute node</title>
    <para>If you deploy Compute with a shared file system, you can use several methods to
            quickly recover from a node failure. This section discusses manual recovery.</para>
    <section xml:id="evacuate-instances">
      <title>Evacuate instances</title>
      <para>If a hardware malfunction or other error causes the cloud compute node to fail,
                you can use the <command>nova evacuate</command> command to evacuate instances.  See
                <xref linkend="evacuate"/> for more information on using the command.</para>
    </section>
    <section xml:id="nova-compute-node-down-manual-recovery">
      <title>Manual recovery</title>
      <para>To manually recover a failed compute node:</para>
      <procedure>
        <step>
          <para>Identify the VMs on the affected hosts by using a combination of the
                        <command>openstack server list</command> and <command>openstack server show</command>
                        commands or the <command>euca-describe-instances</command> command.</para>
          <para>For example, this command displays information about the i-000015b9 instance
                        that runs on the np-rcc54 node:</para>
          <screen language="console">$ euca-describe-instances
i-000015b9 at3-ui02 running nectarkey (376, np-rcc54) 0 m1.xxlarge 2012-06-19T00:48:11.000Z 115.146.93.60</screen>
        </step>
        <step>
          <para>Query the Compute database for the status of the host. This example converts
                        an EC2 API instance ID to an OpenStack ID. If you use the <command>nova</command>
                        commands, you can substitute the ID directly. This example output is
                        truncated:</para>
          <screen language="none">mysql&gt; SELECT * FROM instances WHERE id = CONV('15b9', 16, 10) \G;
*************************** 1. row ***************************
created_at: 2012-06-19 00:48:11
updated_at: 2012-07-03 00:35:11
deleted_at: NULL
...
id: 5561
...
power_state: 5
vm_state: shutoff
...
hostname: at3-ui02
host: np-rcc54
...
uuid: 3f57699a-e773-4650-a443-b4b37eed5a06
...
task_state: NULL
...</screen>
          <note>
            <para>Find the credentials for your database in <literal>/etc/nova.conf</literal> file.</para>
          </note>
        </step>
        <step>
          <para>Decide to which compute host to move the affected VM. Run this database
                        command to move the VM to that host:</para>
          <screen language="mysql">mysql&gt; UPDATE instances SET host = 'np-rcc46' WHERE uuid = '3f57699a-e773-4650-a443-b4b37eed5a06';</screen>
        </step>
        <step>
          <para>If you use a hypervisor that relies on libvirt, such as KVM, update the
                        <literal>libvirt.xml</literal> file in <literal>/var/lib/nova/instances/[instance ID]</literal> with these
                        changes:</para>
          <itemizedlist>
            <listitem>
              <para>Change the <literal>DHCPSERVER</literal> value to the host IP address of the new compute
                                host.</para>
            </listitem>
            <listitem>
              <para>Update the VNC IP to <literal>0.0.0.0</literal>.</para>
            </listitem>
          </itemizedlist>
        </step>
        <step>
          <para>Reboot the VM:</para>
          <screen language="console">$ openstack server reboot 3f57699a-e773-4650-a443-b4b37eed5a06</screen>
        </step>
      </procedure>
      <para>Typically, the database update and <command>openstack server reboot</command> command
                recover a VM from a failed host. However, if problems persist, try one of these
                actions:</para>
      <itemizedlist>
        <listitem>
          <para>Use <command>virsh</command> to recreate the network filter configuration.</para>
        </listitem>
        <listitem>
          <para>Restart Compute services.</para>
        </listitem>
        <listitem>
          <para>Update the <literal>vm_state</literal> and <literal>power_state</literal> fields in the Compute database.</para>
        </listitem>
      </itemizedlist>
    </section>
    <section xml:id="recover-from-a-uid-gid-mismatch">
      <title>Recover from a UID/GID mismatch</title>
      <para>Sometimes when you run Compute with a shared file system or an automated
                configuration tool, files on your compute node might use the wrong UID or GID.
                This UID or GID mismatch can prevent you from running live migrations or
                starting virtual machines.</para>
      <para>This procedure runs on <literal>nova-compute</literal> hosts, based on the KVM hypervisor:</para>
      <procedure>
        <step>
          <para>Set the nova UID to the same number in <literal>/etc/passwd</literal> on all hosts. For
                        example, set the UID to <literal>112</literal>.</para>
          <note>
            <para>Choose UIDs or GIDs that are not in use for other users or groups.</para>
          </note>
        </step>
        <step>
          <para>Set the <literal>libvirt-qemu</literal> UID to the same number in the <literal>/etc/passwd</literal> file
                        on all hosts. For example, set the UID to <literal>119</literal>.</para>
        </step>
        <step>
          <para>Set the <literal>nova</literal> group to the same number in the <literal>/etc/group</literal> file on all
                        hosts. For example, set the group to <literal>120</literal>.</para>
        </step>
        <step>
          <para>Set the <literal>libvirtd</literal> group to the same number in the <literal>/etc/group</literal> file on
                        all hosts. For example, set the group to <literal>119</literal>.</para>
        </step>
        <step>
          <para>Stop the services on the compute node.</para>
        </step>
        <step>
          <para>Change all files that the nova user or group owns. For example:</para>
          <screen language="console"># find / -uid 108 -exec chown nova {} \;
# note the 108 here is the old nova UID before the change
# find / -gid 120 -exec chgrp nova {} \;</screen>
        </step>
        <step>
          <para>Repeat all steps for the <literal>libvirt-qemu</literal> files, if required.</para>
        </step>
        <step>
          <para>Restart the services.</para>
        </step>
        <step>
          <para>To verify that all files use the correct IDs, run the <command>find</command>
                        command.</para>
        </step>
      </procedure>
    </section>
    <section xml:id="recover-cloud-after-disaster">
      <title>Recover cloud after disaster</title>
      <para>This section describes how to manage your cloud after a disaster and back up
                persistent storage volumes. Backups are mandatory, even outside of disaster
                scenarios.</para>
      <para>For a definition of a disaster recovery plan (DRP), see
                <link xmlns:xl="http://www.w3.org/1999/xlink" xl:href="https://en.wikipedia.org/wiki/Disaster_Recovery_Plan"/>.</para>
      <para>A disk crash, network loss, or power failure can affect several components in
                your cloud architecture. The worst disaster for a cloud is a power loss. A
                power loss affects these components:</para>
      <itemizedlist>
        <listitem>
          <para>A cloud controller (<literal>nova-api</literal>, <literal>nova-objectstore</literal>, <literal>nova-network</literal>)</para>
        </listitem>
        <listitem>
          <para>A compute node (<literal>nova-compute</literal>)</para>
        </listitem>
        <listitem>
          <para>A storage area network (SAN) used by OpenStack Block Storage
                        (<literal>cinder-volumes</literal>)</para>
        </listitem>
      </itemizedlist>
      <para>Before a power loss:</para>
      <itemizedlist>
        <listitem>
          <para>Create an active iSCSI session from the SAN to the cloud controller (used
                        for the <literal>cinder-volumes</literal> LVM's VG).</para>
        </listitem>
        <listitem>
          <para>Create an active iSCSI session from the cloud controller to the compute node
                        (managed by <literal>cinder-volume</literal>).</para>
        </listitem>
        <listitem>
          <para>Create an iSCSI session for every volume (so 14 EBS volumes requires 14
                        iSCSI sessions).</para>
        </listitem>
        <listitem>
          <para>Create <literal>iptables</literal> or <literal>ebtables</literal> rules from the cloud controller to the
                        compute node. This allows access from the cloud controller to the running
                        instance.</para>
        </listitem>
        <listitem>
          <para>Save the current state of the database, the current state of the running
                        instances, and the attached volumes (mount point, volume ID, volume status,
                        etc), at least from the cloud controller to the compute node.</para>
        </listitem>
      </itemizedlist>
      <para>After power resumes and all hardware components restart:</para>
      <itemizedlist>
        <listitem>
          <para>The iSCSI session from the SAN to the cloud no longer exists.</para>
        </listitem>
        <listitem>
          <para>The iSCSI session from the cloud controller to the compute node no longer
                        exists.</para>
        </listitem>
        <listitem>
          <para>nova-network reapplies configurations on boot and, as a result, recreates
                        the iptables and ebtables from the cloud controller to the compute node.</para>
        </listitem>
        <listitem>
          <para>Instances stop running.</para>
          <para>Instances are not lost because neither <literal>destroy</literal> nor <literal>terminate</literal> ran.
                        The files for the instances remain on the compute node.</para>
        </listitem>
        <listitem>
          <para>The database does not update.</para>
        </listitem>
      </itemizedlist>
      <warning>
        <para>Do not add any steps or change the order of steps in this procedure.</para>
      </warning>
      <procedure>
        <step>
          <para>Check the current relationship between the volume and its instance, so that
                        you can recreate the attachment.</para>
          <para>Use the <command>openstack volume list</command> command to get this information.
                        Note that the <command>openstack</command> client can get volume information from
                        OpenStack Block Storage.</para>
        </step>
        <step>
          <para>Update the database to clean the stalled state. Do this for every volume by
                        using these queries:</para>
          <screen language="mysql">mysql&gt; use cinder;
mysql&gt; update volumes set mountpoint=NULL;
mysql&gt; update volumes set status="available" where status &lt;&gt;"error_deleting";
mysql&gt; update volumes set attach_status="detached";
mysql&gt; update volumes set instance_id=0;</screen>
          <para>Use <command>openstack volume list</command> command to list all volumes.</para>
        </step>
        <step>
          <para>Restart the instances by using the <command>openstack server reboot
                            INSTANCE</command> command.</para>
          <important>
            <para>Some instances completely reboot and become reachable, while some might
                            stop at the plymouth stage. This is expected behavior. DO NOT reboot a
                            second time.</para>
            <para>Instance state at this stage depends on whether you added an <literal>/etc/fstab</literal>
                            entry for that volume. Images built with the cloud-init package remain in
                            a <literal>pending</literal> state, while others skip the missing volume and start. You
                            perform this step to ask Compute to reboot every instance so that the
                            stored state is preserved. It does not matter if not all instances come
                            up successfully. For more information about cloud-init, see
                            <link xmlns:xl="http://www.w3.org/1999/xlink" xl:href="https://help.ubuntu.com/community/CloudInit/">help.ubuntu.com/community/CloudInit/</link>.</para>
          </important>
        </step>
        <step>
          <para>If required, run the <command>openstack server add volume</command> command to
                        reattach the volumes to their respective instances. This example uses a file
                        of listed volumes to reattach them:</para>
          <screen language="bash">#!/bin/bash

while read line; do
    volume=`echo $line | $CUT -f 1 -d " "`
    instance=`echo $line | $CUT -f 2 -d " "`
    mount_point=`echo $line | $CUT -f 3 -d " "`
        echo "ATTACHING VOLUME FOR INSTANCE - $instance"
    openstack server add volume $instance $volume $mount_point
    sleep 2
done &lt; $volumes_tmp_file</screen>
          <para>Instances that were stopped at the plymouth stage now automatically continue
                        booting and start normally. Instances that previously started successfully
                        can now see the volume.</para>
        </step>
        <step>
          <para>Log in to the instances with SSH and reboot them.</para>
          <para>If some services depend on the volume or if a volume has an entry in fstab,
                        you can now restart the instance. Restart directly from the instance itself
                        and not through <command>nova</command>:</para>
          <screen language="console"># shutdown -r now</screen>
          <para>When you plan for and complete a disaster recovery, follow these tips:</para>
        </step>
      </procedure>
      <itemizedlist>
        <listitem>
          <para>Use the <literal>errors=remount</literal> option in the <literal>fstab</literal> file to prevent data
                        corruption.</para>
          <para>In the event of an I/O error, this option prevents writes to the disk. Add
                            this configuration option into the cinder-volume server that performs the
                            iSCSI connection to the SAN and into the instances' <literal>fstab</literal> files.</para>
        </listitem>
        <listitem>
          <para>Do not add the entry for the SAN's disks to the cinder-volume's <literal>fstab</literal>
                        file.</para>
          <para>Some systems hang on that step, which means you could lose access to your
                            cloud-controller. To re-run the session manually, run this command before
                            performing the mount:</para>
          <screen language="console"># iscsiadm -m discovery -t st -p $SAN_IP $ iscsiadm -m node --target-name $IQN -p $SAN_IP -l</screen>
        </listitem>
        <listitem>
          <para>On your instances, if you have the whole <literal>/home/</literal> directory on the disk,
                        leave a user's directory with the user's bash files and the
                        <literal>authorized_keys</literal> file instead of emptying the <literal>/home/</literal> directory and
                        mapping the disk on it.</para>
          <para>This action enables you to connect to the instance without the volume
                        attached, if you allow only connections through public keys.</para>
        </listitem>
      </itemizedlist>
      <para>To script the disaster recovery plan (DRP), use the <link xmlns:xl="http://www.w3.org/1999/xlink" xl:href="https://github.com/Razique/BashStuff/blob/master/SYSTEMS/OpenStack/SCR_5006_V00_NUAC-OPENSTACK-DRP-OpenStack.sh">https://github.com/Razique</link>
                bash script.</para>
      <para>This script completes these steps:</para>
      <procedure>
        <step>
          <para>Creates an array for instances and their attached volumes.</para>
        </step>
        <step>
          <para>Updates the MySQL database.</para>
        </step>
        <step>
          <para>Restarts all instances with euca2ools.</para>
        </step>
        <step>
          <para>Reattaches the volumes.</para>
        </step>
        <step>
          <para>Uses Compute credentials to make an SSH connection into every instance.</para>
        </step>
      </procedure>
      <para>The script includes a <literal>test mode</literal>, which enables you to perform the sequence
                for only one instance.</para>
      <para>To reproduce the power loss, connect to the compute node that runs that
                instance and close the iSCSI session. Do not detach the volume by using the
                <command>openstack server remove volume</command> command. You must manually close the
                iSCSI session. This example closes an iSCSI session with the number <literal>15</literal>:</para>
      <screen language="console"># iscsiadm -m session -u -r 15</screen>
      <para>Do not forget the <literal>-r</literal> option. Otherwise, all sessions close.</para>
      <warning>
        <para>There is potential for data loss while running instances during this
                    procedure. If you are using Liberty or earlier, ensure you have the correct
                    patch and set the options appropriately.</para>
      </warning>
    </section>
  </chapter>
  <chapter xml:id="consider-numa-topology-when-booting-instances" xml:base="numa">
    <title>Consider NUMA topology when booting instances</title>
    <para>NUMA topology can exist on both the physical hardware of the host, and the
            virtual hardware of the instance. OpenStack Compute uses libvirt to tune
            instances to take advantage of NUMA topologies. The libvirt driver boot
            process looks at the NUMA topology field of both the instance and the host it
            is being booted on, and uses that information to generate an appropriate
            configuration.</para>
    <para>If the host is NUMA capable, but the instance has not requested a NUMA
            topology, Compute attempts to pack the instance into a single cell.
            If this fails, though, Compute will not continue to try.</para>
    <para>If the host is NUMA capable, and the instance has requested a specific NUMA
            topology, Compute will try to pin the vCPUs of different NUMA cells
            on the instance to the corresponding NUMA cells on the host. It will also
            expose the NUMA topology of the instance to the guest OS.</para>
    <para>If you want Compute to pin a particular vCPU as part of this process,
            set the <literal>vcpu_pin_set</literal> parameter in the <literal>nova.conf</literal> configuration
            file. For more information about the <literal>vcpu_pin_set</literal> parameter, see the
            Configuration Reference Guide.</para>
  </chapter>
  <chapter xml:id="attaching-physical-pci-devices-to-guests" xml:base="pci-passthrough">
    <title>Attaching physical PCI devices to guests</title>
    <para>The PCI passthrough feature in OpenStack allows full access and direct control
            of a physical PCI device in guests. This mechanism is generic for any kind of
            PCI device, and runs with a Network Interface Card (NIC), Graphics Processing
            Unit (GPU), or any other devices that can be attached to a PCI bus. Correct
            driver installation is the only requirement for the guest to properly use the
            devices.</para>
    <para>Some PCI devices provide Single Root I/O Virtualization and Sharing (SR-IOV)
            capabilities. When SR-IOV is used, a physical device is virtualized and appears
            as multiple PCI devices. Virtual PCI devices are assigned to the same or
            different guests. In the case of PCI passthrough, the full physical device is
            assigned to only one guest and cannot be shared.</para>
    <note>
      <para>For information on attaching virtual SR-IOV devices to guests, refer to the
                <link xmlns:xl="http://www.w3.org/1999/xlink" xl:href="https://docs.openstack.org/neutron/latest/admin/config-sriov.html">Networking Guide</link>.</para>
    </note>
    <para>To enable PCI passthrough, follow the steps below:</para>
    <procedure>
      <step>
        <para>Configure nova-scheduler (Controller)</para>
      </step>
      <step>
        <para>Configure nova-api (Controller)**</para>
      </step>
      <step>
        <para>Configure a flavor (Controller)</para>
      </step>
      <step>
        <para>Enable PCI passthrough (Compute)</para>
      </step>
      <step>
        <para>Configure PCI devices in nova-compute (Compute)</para>
      </step>
    </procedure>
    <note>
      <para>The PCI device with address <literal>0000:41:00.0</literal> is used as an example. This
                will differ between environments.</para>
    </note>
    <section xml:id="configure-nova-scheduler-controller">
      <title>Configure nova-scheduler (Controller)</title>
      <procedure>
        <step>
          <para>Configure <literal>nova-scheduler</literal> as specified in <link xmlns:xl="http://www.w3.org/1999/xlink" xl:href="https://docs.openstack.org/neutron/latest/admin/config-sriov.html#configure-nova-scheduler-controller">Configure nova-scheduler</link>.</para>
        </step>
        <step>
          <para>Restart the <literal>nova-scheduler</literal> service.</para>
        </step>
      </procedure>
    </section>
    <section xml:id="configure-nova-api-controller">
      <title>Configure nova-api (Controller)</title>
      <procedure>
        <step>
          <para>Specify the PCI alias for the device.</para>
          <para>Configure a PCI alias <literal>a1</literal> to request a PCI device with a <literal>vendor_id</literal> of
                        <literal>0x8086</literal> and a <literal>product_id</literal> of <literal>0x154d</literal>. The <literal>vendor_id</literal> and
                        <literal>product_id</literal> correspond the PCI device with address <literal>0000:41:00.0</literal>.</para>
          <para>Edit <literal>/etc/nova/nova.conf</literal>:</para>
          <screen language="ini">[pci]
alias = { "vendor_id":"8086", "product_id":"154d", "device_type":"type-PF", "name":"a1" }</screen>
          <para>For more information about the syntax of <literal>alias</literal>, refer to
                        <xref linkend="../configuration/config"/>.</para>
        </step>
        <step>
          <para>Restart the <literal>nova-api</literal> service.</para>
        </step>
      </procedure>
    </section>
    <section xml:id="configure-a-flavor-controller">
      <title>Configure a flavor (Controller)</title>
      <para>Configure a flavor to request two PCI devices, each with <literal>vendor_id</literal> of
                <literal>0x8086</literal> and <literal>product_id</literal> of <literal>0x154d</literal>:</para>
      <screen language="console"># openstack flavor set m1.large --property "pci_passthrough:alias"="a1:2"</screen>
      <para>For more information about the syntax for <literal>pci_passthrough:alias</literal>, refer to
                <xref linkend="flavors"/>.</para>
    </section>
    <section xml:id="enable-pci-passthrough-compute">
      <title>Enable PCI passthrough (Compute)</title>
      <para>Enable VT-d and IOMMU. For more information, refer to steps one and two in
                <link xmlns:xl="http://www.w3.org/1999/xlink" xl:href="https://docs.openstack.org/neutron/latest/admin/config-sriov.html#create-virtual-functions-compute">Create Virtual Functions</link>.</para>
    </section>
    <section xml:id="configure-pci-devices-compute">
      <title>Configure PCI devices (Compute)</title>
      <procedure>
        <step>
          <para>Configure <literal>nova-compute</literal> to allow the PCI device to pass through to
                        VMs. Edit <literal>/etc/nova/nova.conf</literal>:</para>
          <screen language="ini">[pci]
passthrough_whitelist = { "address": "0000:41:00.0" }</screen>
          <para>Alternatively specify multiple PCI devices using whitelisting:</para>
          <screen language="ini">[pci]
passthrough_whitelist = { "vendor_id": "8086", "product_id": "10fb" }</screen>
          <para>All PCI devices matching the <literal>vendor_id</literal> and <literal>product_id</literal> are added to
                        the pool of PCI devices available for passthrough to VMs.</para>
          <para>For more information about the syntax of <literal>passthrough_whitelist</literal>,
                        refer to <xref linkend="../configuration/config"/>.</para>
        </step>
        <step>
          <para>Specify the PCI alias for the device.</para>
          <para>From the Newton release, to resize guest with PCI device, configure the PCI
                        alias on the compute node as well.</para>
          <para>Configure a PCI alias <literal>a1</literal> to request a PCI device with a <literal>vendor_id</literal> of
                        <literal>0x8086</literal> and a <literal>product_id</literal> of <literal>0x154d</literal>. The <literal>vendor_id</literal> and
                        <literal>product_id</literal> correspond the PCI device with address <literal>0000:41:00.0</literal>.</para>
          <para>Edit <literal>/etc/nova/nova.conf</literal>:</para>
          <screen language="ini">[pci]
alias = { "vendor_id":"8086", "product_id":"154d", "device_type":"type-PF", "name":"a1" }</screen>
          <para>For more information about the syntax of <literal>alias</literal>, refer to <xref linkend="../configuration/config"/>.</para>
        </step>
        <step>
          <para>Restart the <literal>nova-compute</literal> service.</para>
        </step>
      </procedure>
    </section>
    <section xml:id="create-instances-with-pci-passthrough-devices">
      <title>Create instances with PCI passthrough devices</title>
      <para>The <literal>nova-scheduler</literal> selects a destination host that has PCI devices
                available with the specified <literal>vendor_id</literal> and <literal>product_id</literal> that matches the
                <literal>alias</literal> from the flavor.</para>
      <screen language="console"># openstack server create --flavor m1.large --image cirros-0.3.5-x86_64-uec --wait test-pci</screen>
    </section>
  </chapter>
  <chapter xml:id="manage-quotas" xml:base="quotas2">
    <title>Manage quotas</title>
    <para>To prevent system capacities from being exhausted without notification, you can
            set up quotas. Quotas are operational limits. For example, the number of
            gigabytes allowed for each project can be controlled so that cloud resources
            are optimized.  Quotas can be enforced at both the project and the project-user
            level.</para>
    <para>Using the command-line interface, you can manage quotas for the OpenStack
            Compute service, the OpenStack Block Storage service, and the OpenStack
            Networking service.</para>
    <para>The cloud operator typically changes default values because a project requires
            more than ten volumes or 1 TB on a compute node.</para>
    <note>
      <para>To view all projects, run:</para>
      <screen language="console">$ openstack project list
+----------------------------------+----------+
| ID                               | Name     |
+----------------------------------+----------+
| e66d97ac1b704897853412fc8450f7b9 | admin    |
| bf4a37b885fe46bd86e999e50adad1d3 | services |
| 21bd1c7c95234fd28f589b60903606fa | tenant01 |
| f599c5cd1cba4125ae3d7caed08e288c | tenant02 |
+----------------------------------+----------+</screen>
      <para>To display all current users for a project, run:</para>
      <screen language="console">$ openstack user list --project PROJECT_NAME
+----------------------------------+--------+
| ID                               | Name   |
+----------------------------------+--------+
| ea30aa434ab24a139b0e85125ec8a217 | demo00 |
| 4f8113c1d838467cad0c2f337b3dfded | demo01 |
+----------------------------------+--------+</screen>
    </note>
    <para>Use <literal>openstack quota show <emphasis>PROJECT_NAME</emphasis></literal> to list all quotas for a
            project.</para>
    <para>Use <literal>openstack quota set <emphasis>PROJECT_NAME</emphasis><emphasis>--parameters</emphasis></literal> to set quota
            values.</para>
  </chapter>
  <chapter xml:id="manage-compute-service-quotas" xml:base="quotas">
    <title>Manage Compute service quotas</title>
    <para>As an administrative user, you can use the <command>nova quota-*</command> commands,
            which are provided by the <literal>python-novaclient</literal> package, to update the Compute
            service quotas for a specific project or project user, as well as update the
            quota defaults for a new project.</para>
    <informaltable>
      <tgroup cols="2">
        <colspec colname="c0" colwidth="10"/>
        <colspec colname="c1" colwidth="40"/>
        <thead>
          <row>
            <entry>
              <para>Quota name</para>
            </entry>
            <entry>
              <para>Description</para>
            </entry>
          </row>
        </thead>
        <tbody>
          <row>
            <entry>
              <para>cores</para>
            </entry>
            <entry>
              <para>Number of instance cores (VCPUs) allowed per project.</para>
            </entry>
          </row>
          <row>
            <entry>
              <para>fixed-ips</para>
            </entry>
            <entry>
              <para>Number of fixed IP addresses allowed per project. This number
                                must be equal to or greater than the number of allowed
                                instances.</para>
            </entry>
          </row>
          <row>
            <entry>
              <para>floating-ips</para>
            </entry>
            <entry>
              <para>Number of floating IP addresses allowed per project.</para>
            </entry>
          </row>
          <row>
            <entry>
              <para>injected-file-content-bytes</para>
            </entry>
            <entry>
              <para>Number of content bytes allowed per injected file.</para>
            </entry>
          </row>
          <row>
            <entry>
              <para>injected-file-path-bytes</para>
            </entry>
            <entry>
              <para>Length of injected file path.</para>
            </entry>
          </row>
          <row>
            <entry>
              <para>injected-files</para>
            </entry>
            <entry>
              <para>Number of injected files allowed per project.</para>
            </entry>
          </row>
          <row>
            <entry>
              <para>instances</para>
            </entry>
            <entry>
              <para>Number of instances allowed per project.</para>
            </entry>
          </row>
          <row>
            <entry>
              <para>key-pairs</para>
            </entry>
            <entry>
              <para>Number of key pairs allowed per user.</para>
            </entry>
          </row>
          <row>
            <entry>
              <para>metadata-items</para>
            </entry>
            <entry>
              <para>Number of metadata items allowed per instance.</para>
            </entry>
          </row>
          <row>
            <entry>
              <para>ram</para>
            </entry>
            <entry>
              <para>Megabytes of instance ram allowed per project.</para>
            </entry>
          </row>
          <row>
            <entry>
              <para>security-groups</para>
            </entry>
            <entry>
              <para>Number of security groups per project.</para>
            </entry>
          </row>
          <row>
            <entry>
              <para>security-group-rules</para>
            </entry>
            <entry>
              <para>Number of security group rules per project.</para>
            </entry>
          </row>
          <row>
            <entry>
              <para>server-groups</para>
            </entry>
            <entry>
              <para>Number of server groups per project.</para>
            </entry>
          </row>
          <row>
            <entry>
              <para>server-group-members</para>
            </entry>
            <entry>
              <para>Number of servers per server group.</para>
            </entry>
          </row>
        </tbody>
      </tgroup>
    </informaltable>
    <section xml:id="view-and-update-compute-quotas-for-a-project">
      <title>View and update Compute quotas for a project</title>
      <section xml:id="to-view-and-update-default-quota-values">
        <title>To view and update default quota values</title>
        <procedure>
          <step>
            <para>List all default quotas for all projects:</para>
            <screen language="console">$ openstack quota show --default

+-----------------------------+-------+
| Quota                       | Limit |
+-----------------------------+-------+
| instances                   | 10    |
| cores                       | 20    |
| ram                         | 51200 |
| floating_ips                | 10    |
| fixed_ips                   | -1    |
| metadata_items              | 128   |
| injected_files              | 5     |
| injected_file_content_bytes | 10240 |
| injected_file_path_bytes    | 255   |
| key_pairs                   | 100   |
| security_groups             | 10    |
| security_group_rules        | 20    |
| server_groups               | 10    |
| server_group_members        | 10    |
+-----------------------------+-------+</screen>
          </step>
          <step>
            <para>Update a default value for a new project, for example:</para>
            <screen language="console">$ openstack quota set --instances 15 default</screen>
          </step>
        </procedure>
      </section>
      <section xml:id="to-view-quota-values-for-an-existing-project">
        <title>To view quota values for an existing project</title>
        <procedure>
          <step>
            <para>List the currently set quota values for a project:</para>
            <screen language="console">$ openstack quota show PROJECT_NAME

+-----------------------------+-------+
| Quota                       | Limit |
+-----------------------------+-------+
| instances                   | 10    |
| cores                       | 20    |
| ram                         | 51200 |
| floating_ips                | 10    |
| fixed_ips                   | -1    |
| metadata_items              | 128   |
| injected_files              | 5     |
| injected_file_content_bytes | 10240 |
| injected_file_path_bytes    | 255   |
| key_pairs                   | 100   |
| security_groups             | 10    |
| security_group_rules        | 20    |
| server_groups               | 10    |
| server_group_members        | 10    |
+-----------------------------+-------+</screen>
          </step>
        </procedure>
      </section>
      <section xml:id="to-update-quota-values-for-an-existing-project">
        <title>To update quota values for an existing project</title>
        <procedure>
          <step>
            <para>Obtain the project ID.</para>
            <screen language="console">$ project=$(openstack project show -f value -c id PROJECT_NAME)</screen>
          </step>
          <step>
            <para>Update a particular quota value.</para>
            <screen language="console">$ openstack quota set --QUOTA_NAME QUOTA_VALUE PROJECT_OR_CLASS</screen>
            <para>For example:</para>
            <screen language="console">$ openstack quota set --floating-ips 20 PROJECT_OR_CLASS
$ openstack quota show PROJECT_NAME
+-----------------------------+-------+
| Quota                       | Limit |
+-----------------------------+-------+
| instances                   | 10    |
| cores                       | 20    |
| ram                         | 51200 |
| floating_ips                | 20    |
| fixed_ips                   | -1    |
| metadata_items              | 128   |
| injected_files              | 5     |
| injected_file_content_bytes | 10240 |
| injected_file_path_bytes    | 255   |
| key_pairs                   | 100   |
| security_groups             | 10    |
| security_group_rules        | 20    |
| server_groups               | 10    |
| server_group_members        | 10    |
+-----------------------------+-------+</screen>
            <note>
              <para>To view a list of options for the <command>openstack quota set</command> command,
                                run:</para>
              <screen language="console">$ openstack help quota set</screen>
            </note>
          </step>
        </procedure>
      </section>
    </section>
    <section xml:id="view-and-update-compute-quotas-for-a-project-user">
      <title>View and update Compute quotas for a project user</title>
      <section xml:id="to-view-quota-values-for-a-project-user">
        <title>To view quota values for a project user</title>
        <procedure>
          <step>
            <para>Place the user ID in a usable variable.</para>
            <screen language="console">$ projectUser=$(openstack user show -f value -c id USER_NAME)</screen>
          </step>
          <step>
            <para>Place the user's project ID in a usable variable, as follows:</para>
            <screen language="console">$ project=$(openstack project show -f value -c id PROJECT_NAME)</screen>
          </step>
          <step>
            <para>List the currently set quota values for a project user.</para>
            <screen language="console">$ nova quota-show --user $projectUser --tenant $project</screen>
            <para>For example:</para>
            <screen language="console">$ nova quota-show --user $projecUser --tenant $project
+-----------------------------+-------+
| Quota                       | Limit |
+-----------------------------+-------+
| instances                   | 10    |
| cores                       | 20    |
| ram                         | 51200 |
| floating_ips                | 20    |
| fixed_ips                   | -1    |
| metadata_items              | 128   |
| injected_files              | 5     |
| injected_file_content_bytes | 10240 |
| injected_file_path_bytes    | 255   |
| key_pairs                   | 100   |
| security_groups             | 10    |
| security_group_rules        | 20    |
| server_groups               | 10    |
| server_group_members        | 10    |
+-----------------------------+-------+</screen>
          </step>
        </procedure>
      </section>
      <section xml:id="to-update-quota-values-for-a-project-user">
        <title>To update quota values for a project user</title>
        <procedure>
          <step>
            <para>Place the user ID in a usable variable.</para>
            <screen language="console">$ projectUser=$(openstack user show -f value -c id USER_NAME)</screen>
          </step>
          <step>
            <para>Place the user's project ID in a usable variable, as follows:</para>
            <screen language="console">$ project=$(openstack project show -f value -c id PROJECT_NAME)</screen>
          </step>
          <step>
            <para>Update a particular quota value, as follows:</para>
            <screen language="console">$ nova quota-update  --user $projectUser --QUOTA_NAME QUOTA_VALUE $project</screen>
            <para>For example:</para>
            <screen language="console">$ nova quota-update --user $projectUser --floating-ips 12 $project
$ nova quota-show --user $projectUser --tenant $project
+-----------------------------+-------+
| Quota                       | Limit |
+-----------------------------+-------+
| instances                   | 10    |
| cores                       | 20    |
| ram                         | 51200 |
| floating_ips                | 12    |
| fixed_ips                   | -1    |
| metadata_items              | 128   |
| injected_files              | 5     |
| injected_file_content_bytes | 10240 |
| injected_file_path_bytes    | 255   |
| key_pairs                   | 100   |
| security_groups             | 10    |
| security_group_rules        | 20    |
| server_groups               | 10    |
| server_group_members        | 10    |
+-----------------------------+-------+</screen>
            <note>
              <para>To view a list of options for the <command>nova quota-update</command> command,
                                run:</para>
              <screen language="console">$ nova help quota-update</screen>
            </note>
          </step>
        </procedure>
      </section>
      <section xml:id="to-display-the-current-quota-usage-for-a-project-user">
        <title>To display the current quota usage for a project user</title>
        <para>Use <command>nova limits</command> to get a list of the
                    current quota values and the current quota usage:</para>
        <screen language="console">$ nova limits --tenant PROJET_NAME

+------+-----+-------+--------+------+----------------+
| Verb | URI | Value | Remain | Unit | Next_Available |
+------+-----+-------+--------+------+----------------+
+------+-----+-------+--------+------+----------------+

+--------------------+------+-------+
| Name               | Used | Max   |
+--------------------+------+-------+
| Cores              | 0    | 20    |
| Instances          | 0    | 10    |
| Keypairs           | -    | 100   |
| Personality        | -    | 5     |
| Personality Size   | -    | 10240 |
| RAM                | 0    | 51200 |
| Server Meta        | -    | 128   |
| ServerGroupMembers | -    | 10    |
| ServerGroups       | 0    | 10    |
+--------------------+------+-------+</screen>
        <note>
          <para>The <command>nova limits</command> command generates an empty
                        table as a result of the Compute API, which prints an
                        empty list for backward compatibility purposes.</para>
        </note>
      </section>
    </section>
  </chapter>
  <chapter xml:id="configure-remote-console-access" xml:base="remote-console-access">
    <title>Configure remote console access</title>
    <para>To provide a remote console or remote desktop access to guest virtual machines,
            use VNC or SPICE HTML5 through either the OpenStack dashboard or the command
            line. Best practice is to select one or the other to run.</para>
    <section xml:id="about-nova-consoleauth">
      <title>About nova-consoleauth</title>
      <para>Both client proxies leverage a shared service to manage token authentication
                called <literal>nova-consoleauth</literal>. This service must be running for either proxy to
                work. Many proxies of either type can be run against a single
                <literal>nova-consoleauth</literal> service in a cluster configuration.</para>
      <para>Do not confuse the <literal>nova-consoleauth</literal> shared service with <literal>nova-console</literal>,
                which is a XenAPI-specific service that most recent VNC proxy architectures do
                not use.</para>
    </section>
    <section xml:id="spice-console">
      <title>SPICE console</title>
      <para>OpenStack Compute supports VNC consoles to guests. The VNC protocol is fairly
                limited, lacking support for multiple monitors, bi-directional audio, reliable
                cut-and-paste, video streaming and more. SPICE is a new protocol that aims to
                address the limitations in VNC and provide good remote desktop support.</para>
      <para>SPICE support in OpenStack Compute shares a similar architecture to the VNC
                implementation. The OpenStack dashboard uses a SPICE-HTML5 widget in its
                console tab that communicates to the <literal>nova-spicehtml5proxy</literal> service by using
                SPICE-over-websockets. The <literal>nova-spicehtml5proxy</literal> service communicates
                directly with the hypervisor process by using SPICE.</para>
      <para>VNC must be explicitly disabled to get access to the SPICE console. Set the
                <literal>vnc_enabled</literal> option to <literal>False</literal> in the <literal>[DEFAULT]</literal> section to disable the
                VNC console.</para>
      <para>Use the following options to configure SPICE as the console for OpenStack
                Compute:</para>
      <screen language="console">[spice]
agent_enabled = False
enabled = True
html5proxy_base_url = http://IP_ADDRESS:6082/spice_auto.html
html5proxy_host = 0.0.0.0
html5proxy_port = 6082
keymap = en-us
server_listen = 127.0.0.1
server_proxyclient_address = 127.0.0.1</screen>
      <para>Replace <literal>IP_ADDRESS</literal> with the management interface IP address of the
                controller or the VIP.</para>
    </section>
    <section xml:id="vnc-console-proxy">
      <title>VNC console proxy</title>
      <para>The VNC proxy is an OpenStack component that enables compute service users to
                access their instances through VNC clients.</para>
      <note>
        <para>The web proxy console URLs do not support the websocket protocol scheme
                    (ws://) on python versions less than 2.7.4.</para>
      </note>
      <para>The VNC console connection works as follows:</para>
      <procedure>
        <step>
          <para>A user connects to the API and gets an <literal>access_url</literal> such as,
                        <literal>http://ip:port/?token=xyz</literal>.</para>
        </step>
        <step>
          <para>The user pastes the URL in a browser or uses it as a client
                        parameter.</para>
        </step>
        <step>
          <para>The browser or client connects to the proxy.</para>
        </step>
        <step>
          <para>The proxy talks to <literal>nova-consoleauth</literal> to authorize the token for the user,
                        and maps the token to the <emphasis>private</emphasis> host and port of the VNC server for an
                        instance.</para>
          <para>The compute host specifies the address that the proxy should use to connect
                        through the <literal>nova.conf</literal> file option, <literal>vncserver_proxyclient_address</literal>. In
                        this way, the VNC proxy works as a bridge between the public network and
                        private host network.</para>
        </step>
        <step>
          <para>The proxy initiates the connection to VNC server and continues to proxy
                        until the session ends.</para>
        </step>
      </procedure>
      <para>The proxy also tunnels the VNC protocol over WebSockets so that the <literal>noVNC</literal>
                client can talk to VNC servers. In general, the VNC proxy:</para>
      <itemizedlist>
        <listitem>
          <para>Bridges between the public network where the clients live and the private
                        network where VNC servers live.</para>
        </listitem>
        <listitem>
          <para>Mediates token authentication.</para>
        </listitem>
        <listitem>
          <para>Transparently deals with hypervisor-specific connection details to provide a
                        uniform client experience.</para>
        </listitem>
      </itemizedlist>
      <mediaobject>
        <imageobject role="fo">
          <imagedata fileref="SCH_5009_V00_NUAC-VNC_OpenStack.png" width="95%"/>
        </imageobject>
        <imageobject role="html">
          <imagedata fileref="SCH_5009_V00_NUAC-VNC_OpenStack.png" width="95%"/>
        </imageobject>
      </mediaobject>
      <section xml:id="vnc-configuration-options">
        <title>VNC configuration options</title>
        <para>To customize the VNC console, use the following configuration options in your
                    <literal>nova.conf</literal> file:</para>
        <note>
          <para>To support <xref linkend="section-configuring-compute-migrations"/>,
                        you cannot specify a specific IP address for <literal>vncserver_listen</literal>, because
                        that IP address does not exist on the destination host.</para>
        </note>
        <table xml:id="id2">
          <title>Description of VNC configuration options</title>
          <tgroup cols="2">
            <colspec colname="c0" colwidth="25"/>
            <colspec colname="c1" colwidth="25"/>
            <thead>
              <row>
                <entry>
                  <para>Configuration option = Default value</para>
                </entry>
                <entry>
                  <para>Description</para>
                </entry>
              </row>
            </thead>
            <tbody>
              <row>
                <entry>
                  <para>
                    <emphasis role="bold">[DEFAULT]</emphasis>
                  </para>
                </entry>
                <entry/>
              </row>
              <row>
                <entry>
                  <para>
                    <literal>daemon = False</literal>
                  </para>
                </entry>
                <entry>
                  <para>(BoolOpt) Become a daemon (background process)</para>
                </entry>
              </row>
              <row>
                <entry>
                  <para>
                    <literal>key = None</literal>
                  </para>
                </entry>
                <entry>
                  <para>(StrOpt) SSL key file (if separate from cert)</para>
                </entry>
              </row>
              <row>
                <entry>
                  <para>
                    <literal>novncproxy_host = 0.0.0.0</literal>
                  </para>
                </entry>
                <entry>
                  <para>(StrOpt) Host on which to listen for incoming requests</para>
                </entry>
              </row>
              <row>
                <entry>
                  <para>
                    <literal>novncproxy_port = 6080</literal>
                  </para>
                </entry>
                <entry>
                  <para>(IntOpt) Port on which to listen for incoming requests</para>
                </entry>
              </row>
              <row>
                <entry>
                  <para>
                    <literal>record = False</literal>
                  </para>
                </entry>
                <entry>
                  <para>(BoolOpt) Record sessions to FILE.[session_number]</para>
                </entry>
              </row>
              <row>
                <entry>
                  <para>
                    <literal>source_is_ipv6 = False</literal>
                  </para>
                </entry>
                <entry>
                  <para>(BoolOpt) Source is ipv6</para>
                </entry>
              </row>
              <row>
                <entry>
                  <para>
                    <literal>ssl_only = False</literal>
                  </para>
                </entry>
                <entry>
                  <para>(BoolOpt) Disallow non-encrypted connections</para>
                </entry>
              </row>
              <row>
                <entry>
                  <para>
                    <literal>web = /usr/share/spice-html5</literal>
                  </para>
                </entry>
                <entry>
                  <para>(StrOpt) Run webserver on same port. Serve files from DIR.</para>
                </entry>
              </row>
              <row>
                <entry>
                  <para>
                    <emphasis role="bold">[vmware]</emphasis>
                  </para>
                </entry>
                <entry/>
              </row>
              <row>
                <entry>
                  <para>
                    <literal>vnc_port = 5900</literal>
                  </para>
                </entry>
                <entry>
                  <para>(IntOpt) VNC starting port</para>
                </entry>
              </row>
              <row>
                <entry>
                  <para>
                    <literal>vnc_port_total = 10000</literal>
                  </para>
                </entry>
                <entry>
                  <para>vnc_port_total = 10000</para>
                </entry>
              </row>
              <row>
                <entry>
                  <para>
                    <emphasis role="bold">[vnc]</emphasis>
                  </para>
                </entry>
                <entry/>
              </row>
              <row>
                <entry>
                  <para>enabled = True</para>
                </entry>
                <entry>
                  <para>(BoolOpt) Enable VNC related features</para>
                </entry>
              </row>
              <row>
                <entry>
                  <para>novncproxy_base_url = <link xmlns:xl="http://www.w3.org/1999/xlink" xl:href="http://127.0.0.1:6080/vnc_auto.html"/></para>
                </entry>
                <entry>
                  <para>(StrOpt) Location of VNC console proxy, in the form
                                        "<link xmlns:xl="http://www.w3.org/1999/xlink" xl:href="http://127.0.0.1:6080/vnc_auto.html"/>"</para>
                </entry>
              </row>
              <row>
                <entry>
                  <para>vncserver_listen = 127.0.0.1</para>
                </entry>
                <entry>
                  <para>(StrOpt) IP address on which instance vncservers should listen</para>
                </entry>
              </row>
              <row>
                <entry>
                  <para>vncserver_proxyclient_address = 127.0.0.1</para>
                </entry>
                <entry>
                  <para>(StrOpt) The address to which proxy clients (like nova-xvpvncproxy)
                                        should connect</para>
                </entry>
              </row>
              <row>
                <entry>
                  <para>xvpvncproxy_base_url = <link xmlns:xl="http://www.w3.org/1999/xlink" xl:href="http://127.0.0.1:6081/console"/></para>
                </entry>
                <entry>
                  <para>(StrOpt) Location of nova xvp VNC console proxy, in the form
                                        "<link xmlns:xl="http://www.w3.org/1999/xlink" xl:href="http://127.0.0.1:6081/console"/>"</para>
                </entry>
              </row>
            </tbody>
          </tgroup>
        </table>
        <note>
          <itemizedlist>
            <listitem>
              <para>The <literal>vncserver_proxyclient_address</literal> defaults to <literal>127.0.0.1</literal>, which is
                                the address of the compute host that Compute instructs proxies to use when
                                connecting to instance servers.</para>
            </listitem>
            <listitem>
              <para>For all-in-one XenServer domU deployments, set this to <literal>169.254.0.1.</literal></para>
            </listitem>
            <listitem>
              <para>For multi-host XenServer domU deployments, set to a <literal>dom0 management IP</literal>
                                on the same network as the proxies.</para>
            </listitem>
            <listitem>
              <para>For multi-host libvirt deployments, set to a host management IP on the
                                same network as the proxies.</para>
            </listitem>
          </itemizedlist>
        </note>
      </section>
      <section xml:id="typical-deployment">
        <title>Typical deployment</title>
        <para>A typical deployment has the following components:</para>
        <itemizedlist>
          <listitem>
            <para>A <literal>nova-consoleauth</literal> process. Typically runs on the controller host.</para>
          </listitem>
          <listitem>
            <para>One or more <literal>nova-novncproxy</literal> services. Supports browser-based noVNC
                            clients. For simple deployments, this service typically runs on the same
                            machine as <literal>nova-api</literal> because it operates as a proxy between the public
                            network and the private compute host network.</para>
          </listitem>
          <listitem>
            <para>One or more <literal>nova-xvpvncproxy</literal> services. Supports the special Java client
                            discussed here. For simple deployments, this service typically runs on the
                            same machine as <literal>nova-api</literal> because it acts as a proxy between the public
                            network and the private compute host network.</para>
          </listitem>
          <listitem>
            <para>One or more compute hosts. These compute hosts must have correctly configured
                            options, as follows.</para>
          </listitem>
        </itemizedlist>
      </section>
      <section xml:id="nova-novncproxy-novnc">
        <title>nova-novncproxy (noVNC)</title>
        <para>You must install the noVNC package, which contains the <literal>nova-novncproxy</literal>
                    service. As root, run the following command:</para>
        <screen language="console"># apt-get install nova-novncproxy</screen>
        <para>The service starts automatically on installation.</para>
        <para>To restart the service, run:</para>
        <screen language="console"># service nova-novncproxy restart</screen>
        <para>The configuration option parameter should point to your <literal>nova.conf</literal> file,
                    which includes the message queue server address and credentials.</para>
        <para>By default, <literal>nova-novncproxy</literal> binds on <literal>0.0.0.0:6080</literal>.</para>
        <para>To connect the service to your Compute deployment, add the following
                    configuration options to your <literal>nova.conf</literal> file:</para>
        <itemizedlist>
          <listitem>
            <para>
              <literal>vncserver_listen=0.0.0.0</literal>
            </para>
            <para>Specifies the address on which the VNC service should bind. Make sure it is
                            assigned one of the compute node interfaces. This address is the one used by
                            your domain file.</para>
            <screen language="console">&lt;graphics type="vnc" autoport="yes" keymap="en-us" listen="0.0.0.0"/&gt;</screen>
            <note>
              <para>To use live migration, use the 0.0.0.0 address.</para>
            </note>
          </listitem>
          <listitem>
            <para>
              <literal>vncserver_proxyclient_address=127.0.0.1</literal>
            </para>
            <para>The address of the compute host that Compute instructs proxies to use when
                            connecting to instance <literal>vncservers</literal>.</para>
          </listitem>
        </itemizedlist>
      </section>
      <section xml:id="frequently-asked-questions-about-vnc-access-to-virtual-machines">
        <title>Frequently asked questions about VNC access to virtual machines</title>
        <itemizedlist>
          <listitem>
            <para>
              <emphasis role="bold">Q: What is the difference between ``nova-xvpvncproxy`` and
                                ``nova-novncproxy``?</emphasis>
            </para>
            <para>A: <literal>nova-xvpvncproxy</literal>, which ships with OpenStack Compute, is a proxy that
                            supports a simple Java client. nova-novncproxy uses noVNC to provide VNC
                            support through a web browser.</para>
          </listitem>
          <listitem>
            <para>
              <emphasis role="bold">Q: I want VNC support in the OpenStack dashboard. What services do I
                                need?</emphasis>
            </para>
            <para>A: You need <literal>nova-novncproxy</literal>, <literal>nova-consoleauth</literal>, and correctly
                            configured compute hosts.</para>
          </listitem>
          <listitem>
            <para>
              <emphasis role="bold">Q: When I use ``nova get-vnc-console`` or click on the VNC tab of the
                                OpenStack dashboard, it hangs. Why?</emphasis>
            </para>
            <para>A: Make sure you are running <literal>nova-consoleauth</literal> (in addition to
                            <literal>nova-novncproxy</literal>). The proxies rely on <literal>nova-consoleauth</literal> to validate
                            tokens, and waits for a reply from them until a timeout is reached.</para>
          </listitem>
          <listitem>
            <para>
              <emphasis role="bold">Q: My VNC proxy worked fine during my all-in-one test, but now it doesn't
                                work on multi host. Why?</emphasis>
            </para>
            <para>A: The default options work for an all-in-one install, but changes must be
                            made on your compute hosts once you start to build a cluster.  As an example,
                            suppose you have two servers:</para>
            <screen language="bash">PROXYSERVER (public_ip=172.24.1.1, management_ip=192.168.1.1)
COMPUTESERVER (management_ip=192.168.1.2)</screen>
            <para>Your <literal>nova-compute</literal> configuration file must set the following values:</para>
            <screen language="console"># These flags help construct a connection data structure
vncserver_proxyclient_address=192.168.1.2
novncproxy_base_url=http://172.24.1.1:6080/vnc_auto.html
xvpvncproxy_base_url=http://172.24.1.1:6081/console

# This is the address where the underlying vncserver (not the proxy)
# will listen for connections.
vncserver_listen=192.168.1.2</screen>
            <note>
              <para><literal>novncproxy_base_url</literal> and <literal>xvpvncproxy_base_url</literal> use a public IP; this
                                is the URL that is ultimately returned to clients, which generally do not
                                have access to your private network. Your PROXYSERVER must be able to
                                reach <literal>vncserver_proxyclient_address</literal>, because that is the address over
                                which the VNC connection is proxied.</para>
            </note>
          </listitem>
          <listitem>
            <para>
              <emphasis role="bold">Q: My noVNC does not work with recent versions of web browsers. Why?</emphasis>
            </para>
            <para>A: Make sure you have installed <literal>python-numpy</literal>, which is required to
                            support a newer version of the WebSocket protocol (HyBi-07+).</para>
          </listitem>
          <listitem>
            <para>
              <emphasis role="bold">Q: How do I adjust the dimensions of the VNC window image in the OpenStack
                                dashboard?</emphasis>
            </para>
            <para>A: These values are hard-coded in a Django HTML template. To alter them, edit
                            the <literal>_detail_vnc.html</literal> template file. The location of this file varies
                            based on Linux distribution. On Ubuntu 14.04, the file is at
                            <literal>/usr/share/pyshared/horizon/dashboards/nova/instances/templates/instances/_detail_vnc.html</literal>.</para>
            <para>Modify the <literal>width</literal> and <literal>height</literal> options, as follows:</para>
            <screen language="console">&lt;iframe src="{{ vnc_url }}" width="720" height="430"&gt;&lt;/iframe&gt;</screen>
          </listitem>
          <listitem>
            <para>
              <emphasis role="bold">Q: My noVNC connections failed with ValidationError: Origin header protocol
                                does not match. Why?</emphasis>
            </para>
            <para>A: Make sure the <literal>base_url</literal> match your TLS setting. If you are using https
                            console connections, make sure that the value of <literal>novncproxy_base_url</literal> is
                            set explicitly where the <literal>nova-novncproxy</literal> service is running.</para>
          </listitem>
        </itemizedlist>
      </section>
    </section>
  </chapter>
  <chapter xml:id="secure-with-rootwrap" xml:base="root-wrap-reference">
    <title>Secure with rootwrap</title>
    <para>Rootwrap allows unprivileged users to safely run Compute actions as the root
            user. Compute previously used <command>sudo</command> for this purpose, but this was
            difficult to maintain, and did not allow advanced filters. The
            <command>rootwrap</command> command replaces <command>sudo</command> for Compute.</para>
    <para>To use rootwrap, prefix the Compute command with <command>nova-rootwrap</command>. For
            example:</para>
    <screen language="console">$ sudo nova-rootwrap /etc/nova/rootwrap.conf command</screen>
    <para>A generic <literal>sudoers</literal> entry lets the Compute user run <command>nova-rootwrap</command>
            as root. The <command>nova-rootwrap</command> code looks for filter definition
            directories in its configuration file, and loads command filters from them. It
            then checks if the command requested by Compute matches one of those filters
            and, if so, executes the command (as root). If no filter matches, it denies the
            request.</para>
    <note>
      <para>Be aware of issues with using NFS and root-owned files. The NFS share must
                be configured with the <literal>no_root_squash</literal> option enabled, in order for
                rootwrap to work correctly.</para>
    </note>
    <para>Rootwrap is fully controlled by the root user. The root user owns the sudoers
            entry which allows Compute to run a specific rootwrap executable as root, and
            only with a specific configuration file (which should also be owned by root).
            The <command>nova-rootwrap</command> command imports the Python modules it needs from a
            cleaned, system-default PYTHONPATH.  The root-owned configuration file points
            to root-owned filter definition directories, which contain root-owned filters
            definition files. This chain ensures that the Compute user itself is not in
            control of the configuration or modules used by the <command>nova-rootwrap</command>
            executable.</para>
    <section xml:id="configure-rootwrap">
      <title>Configure rootwrap</title>
      <para>Configure rootwrap in the <literal>rootwrap.conf</literal> file. Because it is in the trusted
                security path, it must be owned and writable by only the root user. The
                <literal>rootwrap_config=entry</literal> parameter specifies the file's location in the
                sudoers entry and in the <literal>nova.conf</literal> configuration file.</para>
      <para>The <literal>rootwrap.conf</literal> file uses an INI file format with these sections and
                parameters:</para>
      <table xml:id="id1">
        <title>rootwrap.conf configuration options</title>
        <tgroup cols="2">
          <colspec colname="c0" colwidth="64"/>
          <colspec colname="c1" colwidth="31"/>
          <tbody>
            <row>
              <entry>
                <para>Configuration option=Default value</para>
              </entry>
              <entry>
                <para>(Type) Description</para>
              </entry>
            </row>
            <row>
              <entry>
                <para>[DEFAULT]
                                    filters_path=/etc/nova/rootwrap.d,/usr/share/nova/rootwrap</para>
              </entry>
              <entry>
                <para>(ListOpt) Comma-separated list of directories
                                    containing filter definition files.
                                    Defines where rootwrap filters are stored.
                                    Directories defined on this line should all
                                    exist, and be owned and writable only by the
                                    root user.</para>
              </entry>
            </row>
          </tbody>
        </tgroup>
      </table>
      <para>If the root wrapper is not performing correctly, you can add a workaround
                option into the <literal>nova.conf</literal> configuration file. This workaround re-configures
                the root wrapper configuration to fall back to running commands as <literal>sudo</literal>,
                and is a Kilo release feature.</para>
      <para>Including this workaround in your configuration file safeguards your
                environment from issues that can impair root wrapper performance. Tool changes
                that have impacted <link xmlns:xl="http://www.w3.org/1999/xlink" xl:href="https://git.openstack.org/cgit/openstack-dev/pbr/">Python Build Reasonableness (PBR)</link> for example, are a known
                issue that affects root wrapper performance.</para>
      <para>To set up this workaround, configure the <literal>disable_rootwrap</literal> option in the
                <literal>[workaround]</literal> section of the <literal>nova.conf</literal> configuration file.</para>
      <para>The filters definition files contain lists of filters that rootwrap will use to
                allow or deny a specific command. They are generally suffixed by <literal>.filters</literal> .
                Since they are in the trusted security path, they need to be owned and writable
                only by the root user. Their location is specified in the <literal>rootwrap.conf</literal>
                file.</para>
      <para>Filter definition files use an INI file format with a <literal>[Filters]</literal> section and
                several lines, each with a unique parameter name, which should be different for
                each filter you define:</para>
      <table xml:id="id2">
        <title>Filters configuration options</title>
        <tgroup cols="2">
          <colspec colname="c0" colwidth="72"/>
          <colspec colname="c1" colwidth="39"/>
          <tbody>
            <row>
              <entry>
                <para>Configuration option=Default value</para>
              </entry>
              <entry>
                <para>(Type) Description</para>
              </entry>
            </row>
            <row>
              <entry>
                <para>[Filters]
                                    filter_name=kpartx: CommandFilter, /sbin/kpartx, root</para>
              </entry>
              <entry>
                <para>(ListOpt) Comma-separated list containing the filter class to
                                    use, followed by the Filter arguments (which vary depending
                                    on the Filter class selected).</para>
              </entry>
            </row>
          </tbody>
        </tgroup>
      </table>
    </section>
    <section xml:id="configure-the-rootwrap-daemon">
      <title>Configure the rootwrap daemon</title>
      <para>Administrators can use rootwrap daemon support instead of running rootwrap with
                <command>sudo</command>. The rootwrap daemon reduces the overhead and performance loss
                that results from running <literal>oslo.rootwrap</literal> with <command>sudo</command>. Each call
                that needs rootwrap privileges requires a new instance of rootwrap. The daemon
                prevents overhead from the repeated calls. The daemon does not support long
                running processes, however.</para>
      <para>To enable the rootwrap daemon, set <literal>use_rootwrap_daemon</literal> to <literal>True</literal> in the
                Compute service configuration file.</para>
    </section>
  </chapter>
  <chapter xml:id="manage-project-security" xml:base="security-groups">
    <title>Manage project security</title>
    <para>Security groups are sets of IP filter rules that are applied to all project
            instances, which define networking access to the instance. Group rules are
            project specific; project members can edit the default rules for their group
            and add new rule sets.</para>
    <para>All projects have a <literal>default</literal> security group which is applied to any instance
            that has no other defined security group. Unless you change the default, this
            security group denies all incoming traffic and allows only outgoing traffic to
            your instance.</para>
    <para>You can use the <literal>allow_same_net_traffic</literal> option in the
            <literal>/etc/nova/nova.conf</literal> file to globally control whether the rules apply to
            hosts which share a network. There are two possible values:</para>
    <variablelist>
      <varlistentry>
        <term><literal>True</literal> (default)</term>
        <listitem>
          <para>Hosts on the same subnet are not filtered and are allowed to pass all types
                        of traffic between them. On a flat network, this allows all instances from
                        all projects unfiltered communication.  With VLAN networking, this allows
                        access between instances within the same project. You can also simulate this
                        setting by configuring the default security group to allow all traffic from
                        the subnet.</para>
        </listitem>
      </varlistentry>
      <varlistentry>
        <term>
          <literal>False</literal>
        </term>
        <listitem>
          <para>Security groups are enforced for all connections.</para>
        </listitem>
      </varlistentry>
    </variablelist>
    <para>Additionally, the number of maximum rules per security group is controlled by
            the <literal>security_group_rules</literal> and the number of allowed security groups per
            project is controlled by the <literal>security_groups</literal> quota (see
            <xref linkend="manage-quotas"/>).</para>
    <section xml:id="list-and-view-current-security-groups">
      <title>List and view current security groups</title>
      <para>From the command-line you can get a list of security groups for the project,
                using the <command>openstack</command> and <command>nova</command> commands:</para>
      <procedure>
        <step>
          <para>Ensure your system variables are set for the user and project for which you
                        are checking security group rules. For example:</para>
          <screen language="console">export OS_USERNAME=demo00
export OS_TENANT_NAME=tenant01</screen>
        </step>
        <step>
          <para>Output security groups, as follows:</para>
          <screen language="console">$ openstack security group list
+--------------------------------------+---------+-------------+
| Id                                   | Name    | Description |
+--------------------------------------+---------+-------------+
| 73580272-d8fa-4927-bd55-c85e43bc4877 | default | default     |
| 6777138a-deb7-4f10-8236-6400e7aff5b0 | open    | all ports   |
+--------------------------------------+---------+-------------+</screen>
        </step>
        <step>
          <para>View the details of a group, as follows:</para>
          <screen language="console">$ openstack security group rule list GROUPNAME</screen>
          <para>For example:</para>
          <screen language="console">$ openstack security group rule list open
+--------------------------------------+-------------+-----------+-----------------+-----------------------+
| ID                                   | IP Protocol | IP Range  | Port Range      | Remote Security Group |
+--------------------------------------+-------------+-----------+-----------------+-----------------------+
| 353d0611-3f67-4848-8222-a92adbdb5d3a | udp         | 0.0.0.0/0 | 1:65535         | None                  |
| 63536865-e5b6-4df1-bac5-ca6d97d8f54d | tcp         | 0.0.0.0/0 | 1:65535         | None                  |
+--------------------------------------+-------------+-----------+-----------------+-----------------------+</screen>
          <para>These rules are allow type rules as the default is deny. The first column is
                        the IP protocol (one of ICMP, TCP, or UDP). The second and third columns
                        specify the affected port range. The third column specifies the IP range in
                        CIDR format. This example shows the full port range for all protocols
                        allowed from all IPs.</para>
        </step>
      </procedure>
    </section>
    <section xml:id="create-a-security-group">
      <title>Create a security group</title>
      <para>When adding a new security group, you should pick a descriptive but brief name.
                This name shows up in brief descriptions of the instances that use it where the
                longer description field often does not. For example, seeing that an instance
                is using security group "http" is much easier to understand than "bobs_group"
                or "secgrp1".</para>
      <procedure>
        <step>
          <para>Ensure your system variables are set for the user and project for which you
                        are creating security group rules.</para>
        </step>
        <step>
          <para>Add the new security group, as follows:</para>
          <screen language="console">$ openstack security group create GroupName --description Description</screen>
          <para>For example:</para>
          <screen language="console">$ openstack security group create global_http --description "Allows Web traffic anywhere on the Internet."
+-----------------+--------------------------------------------------------------------------------------------------------------------------+
| Field           | Value                                                                                                                    |
+-----------------+--------------------------------------------------------------------------------------------------------------------------+
| created_at      | 2016-11-03T13:50:53Z                                                                                                     |
| description     | Allows Web traffic anywhere on the Internet.                                                                             |
| headers         |                                                                                                                          |
| id              | c0b92b20-4575-432a-b4a9-eaf2ad53f696                                                                                     |
| name            | global_http                                                                                                              |
| project_id      | 5669caad86a04256994cdf755df4d3c1                                                                                         |
| project_id      | 5669caad86a04256994cdf755df4d3c1                                                                                         |
| revision_number | 1                                                                                                                        |
| rules           | created_at='2016-11-03T13:50:53Z', direction='egress', ethertype='IPv4', id='4d8cec94-e0ee-4c20-9f56-8fb67c21e4df',      |
|                 | project_id='5669caad86a04256994cdf755df4d3c1', revision_number='1', updated_at='2016-11-03T13:50:53Z'                    |
|                 | created_at='2016-11-03T13:50:53Z', direction='egress', ethertype='IPv6', id='31be2ad1-be14-4aef-9492-ecebede2cf12',      |
|                 | project_id='5669caad86a04256994cdf755df4d3c1', revision_number='1', updated_at='2016-11-03T13:50:53Z'                    |
| updated_at      | 2016-11-03T13:50:53Z                                                                                                     |
+-----------------+--------------------------------------------------------------------------------------------------------------------------+</screen>
        </step>
        <step>
          <para>Add a new group rule, as follows:</para>
          <screen language="console">$ openstack security group rule create SEC_GROUP_NAME \
    --protocol PROTOCOL --dst-port FROM_PORT:TO_PORT --remote-ip CIDR</screen>
          <para>The arguments are positional, and the <literal>from-port</literal> and <literal>to-port</literal>
                        arguments specify the local port range connections are allowed to access,
                        not the source and destination ports of the connection. For example:</para>
          <screen language="console">$ openstack security group rule create global_http \
    --protocol tcp --dst-port 80:80 --remote-ip 0.0.0.0/0
+-------------------+--------------------------------------+
| Field             | Value                                |
+-------------------+--------------------------------------+
| created_at        | 2016-11-06T14:02:00Z                 |
| description       |                                      |
| direction         | ingress                              |
| ethertype         | IPv4                                 |
| headers           |                                      |
| id                | 2ba06233-d5c8-43eb-93a9-8eaa94bc9eb5 |
| port_range_max    | 80                                   |
| port_range_min    | 80                                   |
| project_id        | 5669caad86a04256994cdf755df4d3c1     |
| project_id        | 5669caad86a04256994cdf755df4d3c1     |
| protocol          | tcp                                  |
| remote_group_id   | None                                 |
| remote_ip_prefix  | 0.0.0.0/0                            |
| revision_number   | 1                                    |
| security_group_id | c0b92b20-4575-432a-b4a9-eaf2ad53f696 |
| updated_at        | 2016-11-06T14:02:00Z                 |
+-------------------+--------------------------------------+</screen>
          <para>You can create complex rule sets by creating additional rules. For example,
                        if you want to pass both HTTP and HTTPS traffic, run:</para>
          <screen language="console">$ openstack security group rule create global_http \
    --protocol tcp --dst-port 443:443 --remote-ip 0.0.0.0/0
+-------------------+--------------------------------------+
| Field             | Value                                |
+-------------------+--------------------------------------+
| created_at        | 2016-11-06T14:09:20Z                 |
| description       |                                      |
| direction         | ingress                              |
| ethertype         | IPv4                                 |
| headers           |                                      |
| id                | 821c3ef6-9b21-426b-be5b-c8a94c2a839c |
| port_range_max    | 443                                  |
| port_range_min    | 443                                  |
| project_id        | 5669caad86a04256994cdf755df4d3c1     |
| project_id        | 5669caad86a04256994cdf755df4d3c1     |
| protocol          | tcp                                  |
| remote_group_id   | None                                 |
| remote_ip_prefix  | 0.0.0.0/0                            |
| revision_number   | 1                                    |
| security_group_id | c0b92b20-4575-432a-b4a9-eaf2ad53f696 |
| updated_at        | 2016-11-06T14:09:20Z                 |
+-------------------+--------------------------------------+</screen>
          <para>Despite only outputting the newly added rule, this operation is additive
                        (both rules are created and enforced).</para>
        </step>
        <step>
          <para>View all rules for the new security group, as follows:</para>
          <screen language="console">$ openstack security group rule list global_http
+--------------------------------------+-------------+-----------+-----------------+-----------------------+
| ID                                   | IP Protocol | IP Range  | Port Range      | Remote Security Group |
+--------------------------------------+-------------+-----------+-----------------+-----------------------+
| 353d0611-3f67-4848-8222-a92adbdb5d3a | tcp         | 0.0.0.0/0 | 80:80           | None                  |
| 63536865-e5b6-4df1-bac5-ca6d97d8f54d | tcp         | 0.0.0.0/0 | 443:443         | None                  |
+--------------------------------------+-------------+-----------+-----------------+-----------------------+</screen>
        </step>
      </procedure>
    </section>
    <section xml:id="delete-a-security-group">
      <title>Delete a security group</title>
      <procedure>
        <step>
          <para>Ensure your system variables are set for the user and project for which you
                        are deleting a security group.</para>
        </step>
        <step>
          <para>Delete the new security group, as follows:</para>
          <screen language="console">$ openstack security group delete GROUPNAME</screen>
          <para>For example:</para>
          <screen language="console">$ openstack security group delete global_http</screen>
        </step>
      </procedure>
    </section>
    <section xml:id="create-security-group-rules-for-a-cluster-of-instances">
      <title>Create security group rules for a cluster of instances</title>
      <para>Source Groups are a special, dynamic way of defining the CIDR of allowed
                sources. The user specifies a Source Group (Security Group name), and all the
                user's other Instances using the specified Source Group are selected
                dynamically. This alleviates the need for individual rules to allow each new
                member of the cluster.</para>
      <procedure>
        <step>
          <para>Make sure to set the system variables for the user and project for which you
                        are creating a security group rule.</para>
        </step>
        <step>
          <para>Add a source group, as follows:</para>
          <screen language="console">$ openstack security group rule create secGroupName \
    --remote-group source-group --protocol ip-protocol \
    --dst-port from-port:to-port</screen>
          <para>For example:</para>
          <screen language="console">$ openstack security group rule create cluster \
    --remote-group global_http --protocol tcp --dst-port 22:22</screen>
          <para>The <literal>cluster</literal> rule allows SSH access from any other instance that uses the
                        <literal>global_http</literal> group.</para>
        </step>
      </procedure>
    </section>
  </chapter>
  <chapter xml:id="security-hardening" xml:base="security">
    <title>Security hardening</title>
    <para>OpenStack Compute can be integrated with various third-party technologies to
            increase security. For more information, see the <link xmlns:xl="http://www.w3.org/1999/xlink" xl:href="https://docs.openstack.org/security-guide/">OpenStack Security Guide</link>.</para>
    <section xml:id="trusted-compute-pools">
      <title>Trusted compute pools</title>
      <para>Administrators can designate a group of compute hosts as trusted using trusted
                compute pools. The trusted hosts use hardware-based security features, such as
                the Intel Trusted Execution Technology (TXT), to provide an additional level of
                security. Combined with an external stand-alone, web-based remote attestation
                server, cloud providers can ensure that the compute node runs only software
                with verified measurements and can ensure a secure cloud stack.</para>
      <para>Trusted compute pools provide the ability for cloud subscribers to request
                services run only on verified compute nodes.</para>
      <para>The remote attestation server performs node verification like this:</para>
      <procedure>
        <step>
          <para>Compute nodes boot with Intel TXT technology enabled.</para>
        </step>
        <step>
          <para>The compute node BIOS, hypervisor, and operating system are measured.</para>
        </step>
        <step>
          <para>When the attestation server challenges the compute node, the measured data
                        is sent to the attestation server.</para>
        </step>
        <step>
          <para>The attestation server verifies the measurements against a known good
                        database to determine node trustworthiness.</para>
        </step>
      </procedure>
      <para>A description of how to set up an attestation service is beyond the scope of
                this document. For an open source project that you can use to implement an
                attestation service, see the <link xmlns:xl="http://www.w3.org/1999/xlink" xl:href="https://github.com/OpenAttestation/OpenAttestation">Open Attestation</link> project.</para>
      <figure>
        <title>Configuring Compute to use trusted compute pools</title>
        <mediaobject>
          <imageobject role="fo">
            <imagedata fileref="OpenStackTrustedComputePool1.png"/>
          </imageobject>
          <imageobject role="html">
            <imagedata fileref="OpenStackTrustedComputePool1.png"/>
          </imageobject>
        </mediaobject>
      </figure>
      <procedure>
        <step>
          <para>Enable scheduling support for trusted compute pools by adding these lines to
                        the <literal>DEFAULT</literal> section of the <literal>/etc/nova/nova.conf</literal> file:</para>
          <screen language="ini">[DEFAULT]
compute_scheduler_driver=nova.scheduler.filter_scheduler.FilterScheduler
scheduler_available_filters=nova.scheduler.filters.all_filters
scheduler_default_filters=AvailabilityZoneFilter,RamFilter,ComputeFilter,TrustedFilter</screen>
        </step>
        <step>
          <para>Specify the connection information for your attestation service by adding
                        these lines to the <literal>trusted_computing</literal> section of the
                        <literal>/etc/nova/nova.conf</literal> file:</para>
          <screen language="ini">[trusted_computing]
attestation_server = 10.1.71.206
attestation_port = 8443
# If using OAT v2.0 after, use this port:
# attestation_port = 8181
attestation_server_ca_file = /etc/nova/ssl.10.1.71.206.crt
# If using OAT v1.5, use this api_url:
attestation_api_url = /AttestationService/resources
# If using OAT pre-v1.5, use this api_url:
# attestation_api_url = /OpenAttestationWebServices/V1.0
attestation_auth_blob = i-am-openstack</screen>
          <para>In this example:</para>
          <variablelist>
            <varlistentry>
              <term>
                <literal>server</literal>
              </term>
              <listitem>
                <para>Host name or IP address of the host that runs the attestation service</para>
              </listitem>
            </varlistentry>
            <varlistentry>
              <term>
                <literal>port</literal>
              </term>
              <listitem>
                <para>HTTPS port for the attestation service</para>
              </listitem>
            </varlistentry>
            <varlistentry>
              <term>
                <literal>server_ca_file</literal>
              </term>
              <listitem>
                <para>Certificate file used to verify the attestation server's identity</para>
              </listitem>
            </varlistentry>
            <varlistentry>
              <term>
                <literal>api_url</literal>
              </term>
              <listitem>
                <para>The attestation service's URL path</para>
              </listitem>
            </varlistentry>
            <varlistentry>
              <term>
                <literal>auth_blob</literal>
              </term>
              <listitem>
                <para>An authentication blob, required by the attestation service.</para>
              </listitem>
            </varlistentry>
          </variablelist>
        </step>
        <step>
          <para>Save the file, and restart the <literal>nova-compute</literal> and <literal>nova-scheduler</literal>
                        service to pick up the changes.</para>
        </step>
      </procedure>
      <para>To customize the trusted compute pools, use these configuration option
                settings:</para>
      <table xml:id="id2">
        <title>Description of trusted computing configuration options</title>
        <tgroup cols="2">
          <colspec colname="c0" colwidth="50"/>
          <colspec colname="c1" colwidth="50"/>
          <thead>
            <row>
              <entry>
                <para>Configuration option = Default value</para>
              </entry>
              <entry>
                <para>Description</para>
              </entry>
            </row>
            <row>
              <entry>
                <para>[trusted_computing]</para>
              </entry>
              <entry/>
            </row>
          </thead>
          <tbody>
            <row>
              <entry>
                <para>attestation_api_url = /OpenAttestationWebServices/V1.0</para>
              </entry>
              <entry>
                <para>(StrOpt) Attestation web API URL</para>
              </entry>
            </row>
            <row>
              <entry>
                <para>attestation_auth_blob = None</para>
              </entry>
              <entry>
                <para>(StrOpt) Attestation authorization blob - must change</para>
              </entry>
            </row>
            <row>
              <entry>
                <para>attestation_auth_timeout = 60</para>
              </entry>
              <entry>
                <para>(IntOpt) Attestation status cache valid period length</para>
              </entry>
            </row>
            <row>
              <entry>
                <para>attestation_insecure_ssl = False</para>
              </entry>
              <entry>
                <para>(BoolOpt) Disable SSL cert verification for Attestation service</para>
              </entry>
            </row>
            <row>
              <entry>
                <para>attestation_port = 8443</para>
              </entry>
              <entry>
                <para>(StrOpt) Attestation server port</para>
              </entry>
            </row>
            <row>
              <entry>
                <para>attestation_server = None</para>
              </entry>
              <entry>
                <para>(StrOpt) Attestation server HTTP</para>
              </entry>
            </row>
            <row>
              <entry>
                <para>attestation_server_ca_file = None</para>
              </entry>
              <entry>
                <para>(StrOpt) Attestation server Cert file for Identity verification</para>
              </entry>
            </row>
          </tbody>
        </tgroup>
      </table>
      <procedure>
        <step>
          <para>Flavors can be designated as trusted using the <command>openstack flavor
                            set</command> command. In this example, the <literal>m1.tiny</literal> flavor is being set as
                        trusted:</para>
          <screen language="console">$ openstack flavor set --property trusted_host=trusted m1.tiny</screen>
        </step>
        <step>
          <para>You can request that your instance is run on a trusted host by specifying a
                        trusted flavor when booting the instance:</para>
          <screen language="console">$ openstack server create --flavor m1.tiny \
  --key-name myKeypairName --image myImageID newInstanceName</screen>
        </step>
      </procedure>
      <mediaobject>
        <imageobject role="fo">
          <imagedata fileref="OpenStackTrustedComputePool2.png"/>
        </imageobject>
        <imageobject role="html">
          <imagedata fileref="OpenStackTrustedComputePool2.png"/>
        </imageobject>
      </mediaobject>
    </section>
    <section xml:id="encrypt-compute-metadata-traffic">
      <title>Encrypt Compute metadata traffic</title>
      <para>
        <emphasis role="bold">Enabling SSL encryption</emphasis>
      </para>
      <para>OpenStack supports encrypting Compute metadata traffic with HTTPS.  Enable SSL
                encryption in the <literal>metadata_agent.ini</literal> file.</para>
      <procedure>
        <step>
          <para>Enable the HTTPS protocol.</para>
          <screen language="ini">nova_metadata_protocol = https</screen>
        </step>
        <step>
          <para>Determine whether insecure SSL connections are accepted for Compute metadata
                        server requests. The default value is <literal>False</literal>.</para>
          <screen language="ini">nova_metadata_insecure = False</screen>
        </step>
        <step>
          <para>Specify the path to the client certificate.</para>
          <screen language="ini">nova_client_cert = PATH_TO_CERT</screen>
        </step>
        <step>
          <para>Specify the path to the private key.</para>
          <screen language="ini">nova_client_priv_key = PATH_TO_KEY</screen>
        </step>
      </procedure>
    </section>
  </chapter>
  <chapter xml:id="configure-compute-service-groups" xml:base="service-groups">
    <title>Configure Compute service groups</title>
    <para>The Compute service must know the status of each compute node to effectively
            manage and use them. This can include events like a user launching a new VM,
            the scheduler sending a request to a live node, or a query to the ServiceGroup
            API to determine if a node is live.</para>
    <para>When a compute worker running the nova-compute daemon starts, it calls the join
            API to join the compute group. Any service (such as the scheduler) can query
            the group's membership and the status of its nodes.  Internally, the
            ServiceGroup client driver automatically updates the compute worker status.</para>
    <section xml:id="database-servicegroup-driver">
      <title>Database ServiceGroup driver</title>
      <para>By default, Compute uses the database driver to track if a node is live.  In a
                compute worker, this driver periodically sends a <literal>db update</literal> command to the
                database, saying I'm OK with a timestamp. Compute uses a pre-defined
                timeout (<literal>service_down_time</literal>) to determine if a node is dead.</para>
      <para>The driver has limitations, which can be problematic depending on your
                environment. If a lot of compute worker nodes need to be checked, the database
                can be put under heavy load, which can cause the timeout to trigger, and a live
                node could incorrectly be considered dead. By default, the timeout is 60
                seconds. Reducing the timeout value can help in this situation, but you must
                also make the database update more frequently, which again increases the
                database workload.</para>
      <para>The database contains data that is both transient (such as whether the node is
                alive) and persistent (such as entries for VM owners). With the ServiceGroup
                abstraction, Compute can treat each type separately.</para>
    </section>
    <section xml:id="memcache-servicegroup-driver">
      <title>Memcache ServiceGroup driver</title>
      <para>The memcache ServiceGroup driver uses memcached, a distributed memory object
                caching system that is used to increase site performance. For more details, see
                <link xmlns:xl="http://www.w3.org/1999/xlink" xl:href="http://memcached.org/">memcached.org</link>.</para>
      <para>To use the memcache driver, you must install memcached. You might already have
                it installed, as the same driver is also used for the OpenStack Object Storage
                and OpenStack dashboard. To install memcached, see the <emphasis>Environment -&gt;
                    Memcached</emphasis> section in the <link xmlns:xl="http://www.w3.org/1999/xlink" xl:href="https://docs.openstack.org/project-install-guide/ocata">Installation Tutorials and Guides</link> depending on your
                distribution.</para>
      <para>These values in the <literal>/etc/nova/nova.conf</literal> file are required on every node for
                the memcache driver:</para>
      <screen language="ini"># Driver for the ServiceGroup service
servicegroup_driver = "mc"

# Memcached servers. Use either a list of memcached servers to use for caching (list value),
# or "&lt;None&gt;" for in-process caching (default).
memcached_servers = &lt;None&gt;

# Timeout; maximum time since last check-in for up service (integer value).
# Helps to define whether a node is dead
service_down_time = 60</screen>
    </section>
  </chapter>
  <chapter xml:id="manage-compute-services" xml:base="services">
    <title>Manage Compute services</title>
    <para>You can enable and disable Compute services. The following examples disable and
            enable the <literal>nova-compute</literal> service.</para>
    <procedure>
      <step>
        <para>List the Compute services:</para>
        <screen language="console">$ openstack compute service list
+----+--------------+------------+----------+---------+-------+--------------+
| ID | Binary       | Host       | Zone     | Status  | State | Updated At   |
+----+--------------+------------+----------+---------+-------+--------------+
|  4 | nova-        | controller | internal | enabled | up    | 2016-12-20T0 |
|    | consoleauth  |            |          |         |       | 0:44:48.0000 |
|    |              |            |          |         |       | 00           |
|  5 | nova-        | controller | internal | enabled | up    | 2016-12-20T0 |
|    | scheduler    |            |          |         |       | 0:44:48.0000 |
|    |              |            |          |         |       | 00           |
|  6 | nova-        | controller | internal | enabled | up    | 2016-12-20T0 |
|    | conductor    |            |          |         |       | 0:44:54.0000 |
|    |              |            |          |         |       | 00           |
|  9 | nova-compute | compute    | nova     | enabled | up    | 2016-10-21T0 |
|    |              |            |          |         |       | 2:35:03.0000 |
|    |              |            |          |         |       | 00           |
+----+--------------+------------+----------+---------+-------+--------------+</screen>
      </step>
      <step>
        <para>Disable a nova service:</para>
        <screen language="console">$ openstack compute service set --disable --disable-reason trial log nova nova-compute
+----------+--------------+----------+-------------------+
| Host     | Binary       | Status   | Disabled Reason   |
+----------+--------------+----------+-------------------+
| compute  | nova-compute | disabled | trial log         |
+----------+--------------+----------+-------------------+</screen>
      </step>
      <step>
        <para>Check the service list:</para>
        <screen language="console">$ openstack compute service list
+----+--------------+------------+----------+---------+-------+--------------+
| ID | Binary       | Host       | Zone     | Status  | State | Updated At   |
+----+--------------+------------+----------+---------+-------+--------------+
|  4 | nova-        | controller | internal | enabled | up    | 2016-12-20T0 |
|    | consoleauth  |            |          |         |       | 0:44:48.0000 |
|    |              |            |          |         |       | 00           |
|  5 | nova-        | controller | internal | enabled | up    | 2016-12-20T0 |
|    | scheduler    |            |          |         |       | 0:44:48.0000 |
|    |              |            |          |         |       | 00           |
|  6 | nova-        | controller | internal | enabled | up    | 2016-12-20T0 |
|    | conductor    |            |          |         |       | 0:44:54.0000 |
|    |              |            |          |         |       | 00           |
|  9 | nova-compute | compute    | nova     | disabled| up    | 2016-10-21T0 |
|    |              |            |          |         |       | 2:35:03.0000 |
|    |              |            |          |         |       | 00           |
+----+--------------+------------+----------+---------+-------+--------------+</screen>
      </step>
      <step>
        <para>Enable the service:</para>
        <screen language="console">$ openstack compute service set --enable nova nova-compute
+----------+--------------+---------+
| Host     | Binary       | Status  |
+----------+--------------+---------+
| compute  | nova-compute | enabled |
+----------+--------------+---------+</screen>
      </step>
    </procedure>
  </chapter>
  <chapter xml:id="cli-os-migrate-cfg-ssh" xml:base="ssh-configuration">
    <title>Configure SSH between compute nodes</title>
    <para>If you are resizing or migrating an instance between hypervisors, you might
            encounter an SSH (Permission denied) error. Ensure that each node is configured
            with SSH key authentication so that the Compute service can use SSH to move
            disks to other nodes.</para>
    <para>To share a key pair between compute nodes, complete the following steps:</para>
    <procedure>
      <step>
        <para>On the first node, obtain a key pair (public key and private key). Use the
                    root key that is in the <literal>/root/.ssh/id_rsa</literal> and <literal>/root/.ssh/id_ras.pub</literal>
                    directories or generate a new key pair.</para>
      </step>
      <step>
        <para>Run <command>setenforce 0</command> to put SELinux into permissive mode.</para>
      </step>
      <step>
        <para>Enable login abilities for the nova user:</para>
        <screen language="console"># usermod -s /bin/bash nova</screen>
        <para>Switch to the nova account.</para>
        <screen language="console"># su nova</screen>
      </step>
      <step>
        <para>As root, create the folder that is needed by SSH and place the private key
                    that you obtained in step 1 into this folder:</para>
        <screen language="console">mkdir -p /var/lib/nova/.ssh
cp &lt;private key&gt;  /var/lib/nova/.ssh/id_rsa
echo 'StrictHostKeyChecking no' &gt;&gt; /var/lib/nova/.ssh/config
chmod 600 /var/lib/nova/.ssh/id_rsa /var/lib/nova/.ssh/authorized_keys</screen>
      </step>
      <step>
        <para>Repeat steps 2-4 on each node.</para>
        <note>
          <para>The nodes must share the same key pair, so do not generate a new key pair
                        for any subsequent nodes.</para>
        </note>
      </step>
      <step>
        <para>From the first node, where you created the SSH key, run:</para>
        <screen language="console">ssh-copy-id -i &lt;pub key&gt; nova@remote-host</screen>
        <para>This command installs your public key in a remote machine's
                    <literal>authorized_keys</literal> folder.</para>
      </step>
      <step>
        <para>Ensure that the nova user can now log in to each node without using a
                    password:</para>
        <screen language="console"># su nova
$ ssh *computeNodeAddress*
$ exit</screen>
      </step>
      <step>
        <para>As root on each node, restart both libvirt and the Compute services:</para>
        <screen language="console"># systemctl restart libvirtd.service
# systemctl restart openstack-nova-compute.service</screen>
      </step>
    </procedure>
  </chapter>
  <chapter xml:id="troubleshoot-compute" xml:base="support-compute">
    <title>Troubleshoot Compute</title>
    <para>Common problems for Compute typically involve misconfigured networking or
            credentials that are not sourced properly in the environment. Also, most flat
            networking configurations do not enable <command>ping</command> or <command>ssh</command> from
            a compute node to the instances that run on that node. Another common problem
            is trying to run 32-bit images on a 64-bit compute node.  This section shows
            you how to troubleshoot Compute.</para>
    <section xml:id="compute-service-logging">
      <title>Compute service logging</title>
      <para>Compute stores a log file for each service in <literal>/var/log/nova</literal>. For example,
                <literal>nova-compute.log</literal> is the log for the <literal>nova-compute</literal> service. You can set
                the following options to format log strings for the <literal>nova.log</literal> module in the
                <literal>nova.conf</literal> file:</para>
      <itemizedlist>
        <listitem>
          <para>
            <literal>logging_context_format_string</literal>
          </para>
        </listitem>
        <listitem>
          <para>
            <literal>logging_default_format_string</literal>
          </para>
        </listitem>
      </itemizedlist>
      <para>If the log level is set to <literal>debug</literal>, you can also specify
                <literal>logging_debug_format_suffix</literal> to append extra formatting.  For information
                about what variables are available for the formatter, see <link xmlns:xl="http://www.w3.org/1999/xlink" xl:href="https://docs.python.org/library/logging.html#formatter-objects">Formatter Objects</link>.</para>
      <para>You have two logging options for OpenStack Compute based on configuration
                settings. In <literal>nova.conf</literal>, include the <literal>logfile</literal> option to enable logging.
                Alternatively you can set <literal>use_syslog = 1</literal> so that the nova daemon logs to
                syslog.</para>
    </section>
    <section xml:id="guru-meditation-reports">
      <title>Guru Meditation reports</title>
      <para>A Guru Meditation report is sent by the Compute service upon receipt of the
                <literal>SIGUSR2</literal> signal (<literal>SIGUSR1</literal> before Mitaka). This report is a
                general-purpose error report that includes details about the current state of
                the service. The error report is sent to <literal>stderr</literal>.</para>
      <para>For example, if you redirect error output to <literal>nova-api-err.log</literal> using
                <command>nova-api 2&gt;/var/log/nova/nova-api-err.log</command>, resulting in the process
                ID 8675, you can then run:</para>
      <screen language="console"># kill -USR2 8675</screen>
      <para>This command triggers the Guru Meditation report to be printed to
                <literal>/var/log/nova/nova-api-err.log</literal>.</para>
      <para>The report has the following sections:</para>
      <itemizedlist>
        <listitem>
          <para>Package: Displays information about the package to which the process belongs,
                        including version information.</para>
        </listitem>
        <listitem>
          <para>Threads: Displays stack traces and thread IDs for each of the threads within
                        the process.</para>
        </listitem>
        <listitem>
          <para>Green Threads: Displays stack traces for each of the green threads within the
                        process (green threads do not have thread IDs).</para>
        </listitem>
        <listitem>
          <para>Configuration: Lists all configuration options currently accessible through
                        the CONF object for the current process.</para>
        </listitem>
      </itemizedlist>
      <para>For more information, see <link xmlns:xl="http://www.w3.org/1999/xlink" xl:href="https://docs.openstack.org/developer/nova/devref/gmr.html">Guru Meditation Reports</link>.</para>
    </section>
    <section xml:id="compute-common-errors-and-fixes">
      <title>Common errors and fixes for Compute</title>
      <para>The <link xmlns:xl="http://www.w3.org/1999/xlink" xl:href="http://ask.openstack.org">ask.openstack.org</link> site offers a place to ask
                and answer questions, and you can also mark questions as frequently asked
                questions. This section describes some errors people have posted previously.
                Bugs are constantly being fixed, so online resources are a great way to get the
                most up-to-date errors and fixes.</para>
    </section>
    <section xml:id="credential-errors-401-and-403-forbidden-errors">
      <title>Credential errors, 401, and 403 forbidden errors</title>
      <section xml:id="problem">
        <title>Problem</title>
        <para>Missing credentials cause a <literal>403 forbidden</literal> error.</para>
      </section>
      <section xml:id="solution">
        <title>Solution</title>
        <para>To resolve this issue, use one of these methods:</para>
        <procedure>
          <step>
            <para>Manual method</para>
            <para>Gets the <literal>novarc</literal> file from the project ZIP file, saves existing
                            credentials in case of override, and manually sources the <literal>novarc</literal> file.</para>
          </step>
          <step>
            <para>Script method</para>
            <para>Generates <literal>novarc</literal> from the project ZIP file and sources it for you.</para>
          </step>
        </procedure>
        <para>When you run <literal>nova-api</literal> the first time, it generates the certificate
                    authority information, including <literal>openssl.cnf</literal>. If you start the CA services
                    before this, you might not be able to create your ZIP file. Restart the
                    services.  When your CA information is available, create your ZIP file.</para>
        <para>Also, check your HTTP proxy settings to see whether they cause problems with
                    <literal>novarc</literal> creation.</para>
      </section>
    </section>
    <section xml:id="instance-errors">
      <title>Instance errors</title>
      <section xml:id="id2">
        <title>Problem</title>
        <para>Sometimes a particular instance shows <literal>pending</literal> or you cannot SSH to it.
                    Sometimes the image itself is the problem. For example, when you use flat
                    manager networking, you do not have a DHCP server and certain images do not
                    support interface injection; you cannot connect to them.</para>
      </section>
      <section xml:id="id3">
        <title>Solution</title>
        <para>To fix instance errors use an image that does support this method, such as
                    Ubuntu, which obtains an IP address correctly with FlatManager network
                    settings.</para>
        <para>To troubleshoot other possible problems with an instance, such as an instance
                    that stays in a spawning state, check the directory for the particular instance
                    under <literal>/var/lib/nova/instances</literal> on the <literal>nova-compute</literal> host and make sure
                    that these files are present:</para>
        <itemizedlist>
          <listitem>
            <para>
              <literal>libvirt.xml</literal>
            </para>
          </listitem>
          <listitem>
            <para>
              <literal>disk</literal>
            </para>
          </listitem>
          <listitem>
            <para>
              <literal>disk-raw</literal>
            </para>
          </listitem>
          <listitem>
            <para>
              <literal>kernel</literal>
            </para>
          </listitem>
          <listitem>
            <para>
              <literal>ramdisk</literal>
            </para>
          </listitem>
          <listitem>
            <para><literal>console.log</literal>, after the instance starts.</para>
          </listitem>
        </itemizedlist>
        <para>If any files are missing, empty, or very small, the <literal>nova-compute</literal> service
                    did not successfully download the images from the Image service.</para>
        <para>Also check <literal>nova-compute.log</literal> for exceptions. Sometimes they do not appear in
                    the console output.</para>
        <para>Next, check the log file for the instance in the <literal>/var/log/libvirt/qemu</literal>
                    directory to see if it exists and has any useful error messages in it.</para>
        <para>Finally, from the <literal>/var/lib/nova/instances</literal> directory for the instance, see
                    if this command returns an error:</para>
        <screen language="console"># virsh create libvirt.xml</screen>
      </section>
    </section>
    <section xml:id="empty-log-output-for-linux-instances">
      <title>Empty log output for Linux instances</title>
      <section xml:id="id4">
        <title>Problem</title>
        <para>You can view the log output of running instances from either the
                    <guimenu>Log</guimenu> tab of the dashboard or the output of <command>nova
                        console-log</command>. In some cases, the log output of a running Linux instance will be
                    empty or only display a single character (for example, the <literal>?</literal> character).</para>
        <para>This occurs when the Compute service attempts to retrieve the log output of the
                    instance via a serial console while the instance itself is not configured to
                    send output to the console.</para>
      </section>
      <section xml:id="id5">
        <title>Solution</title>
        <para>To rectify this, append the following parameters to kernel arguments specified
                    in the instance's boot loader:</para>
        <screen language="ini">console=tty0 console=ttyS0,115200n8</screen>
        <para>Upon rebooting, the instance will be configured to send output to the Compute
                    service.</para>
      </section>
    </section>
    <section xml:id="reset-the-state-of-an-instance">
      <title>Reset the state of an instance</title>
      <section xml:id="id6">
        <title>Problem</title>
        <para>Instances can remain in an intermediate state, such as <literal>deleting</literal>.</para>
      </section>
      <section xml:id="id7">
        <title>Solution</title>
        <para>You can use the <command>nova reset-state</command> command to manually reset the state
                    of an instance to an error state. You can then delete the instance. For
                    example:</para>
        <screen language="console">$ nova reset-state c6bbbf26-b40a-47e7-8d5c-eb17bf65c485
$ openstack server delete c6bbbf26-b40a-47e7-8d5c-eb17bf65c485</screen>
        <para>You can also use the <literal>--active</literal> parameter to force the instance back to an
                    active state instead of an error state. For example:</para>
        <screen language="console">$ nova reset-state --active c6bbbf26-b40a-47e7-8d5c-eb17bf65c485</screen>
      </section>
    </section>
    <section xml:id="injection-problems">
      <title>Injection problems</title>
      <section xml:id="id8">
        <title>Problem</title>
        <para>Instances may boot slowly, or do not boot. File injection can cause this
                    problem.</para>
      </section>
      <section xml:id="id9">
        <title>Solution</title>
        <para>To disable injection in libvirt, set the following in <literal>nova.conf</literal>:</para>
        <screen language="ini">[libvirt]
inject_partition = -2</screen>
        <note>
          <para>If you have not enabled the configuration drive and you want to make
                        user-specified files available from the metadata server for to improve
                        performance and avoid boot failure if injection fails, you must disable
                        injection.</para>
        </note>
      </section>
    </section>
    <section xml:id="disable-live-snapshotting">
      <title>Disable live snapshotting</title>
      <section xml:id="id10">
        <title>Problem</title>
        <para>Administrators using libvirt version <literal>1.2.2</literal> may experience problems with
                    live snapshot creation. Occasionally, libvirt version <literal>1.2.2</literal> fails to create
                    live snapshots under the load of creating concurrent snapshot.</para>
      </section>
      <section xml:id="id11">
        <title>Solution</title>
        <para>To effectively disable the libvirt live snapshotting, until the problem is
                    resolved, configure the <literal>disable_libvirt_livesnapshot</literal> option.  You can turn
                    off the live snapshotting mechanism by setting up its value to <literal>True</literal> in the
                    <literal>[workarounds]</literal> section of the <literal>nova.conf</literal> file:</para>
        <screen language="ini">[workarounds]
disable_libvirt_livesnapshot = True</screen>
      </section>
    </section>
    <section xml:id="cannot-find-suitable-emulator-for-x86-64">
      <title>Cannot find suitable emulator for x86_64</title>
      <section xml:id="id12">
        <title>Problem</title>
        <para>When you attempt to create a VM, the error shows the VM is in the <literal>BUILD</literal>
                    then <literal>ERROR</literal> state.</para>
      </section>
      <section xml:id="id13">
        <title>Solution</title>
        <para>On the KVM host, run <command>cat /proc/cpuinfo</command>. Make sure the <literal>vmx</literal> or
                    <literal>svm</literal> flags are set.</para>
        <para>Follow the instructions in the <xref linkend="enable-kvm"/>
                    section in the Nova Configuration Reference to enable hardware
                    virtualization support in your BIOS.</para>
      </section>
    </section>
    <section xml:id="failed-to-attach-volume-after-detaching">
      <title>Failed to attach volume after detaching</title>
      <section xml:id="id14">
        <title>Problem</title>
        <para>Failed to attach a volume after detaching the same volume.</para>
      </section>
      <section xml:id="id15">
        <title>Solution</title>
        <para>You must change the device name on the <command>nova-attach</command> command. The VM
                    might not clean up after a <command>nova-detach</command> command runs. This example
                    shows how the <command>nova-attach</command> command fails when you use the <literal>vdb</literal>,
                    <literal>vdc</literal>, or <literal>vdd</literal> device names:</para>
        <screen language="console"># ls -al /dev/disk/by-path/
total 0
drwxr-xr-x 2 root root 200 2012-08-29 17:33 .
drwxr-xr-x 5 root root 100 2012-08-29 17:33 ..
lrwxrwxrwx 1 root root 9 2012-08-29 17:33 pci-0000:00:04.0-virtio-pci-virtio0 -&gt; ../../vda
lrwxrwxrwx 1 root root 10 2012-08-29 17:33 pci-0000:00:04.0-virtio-pci-virtio0-part1 -&gt; ../../vda1
lrwxrwxrwx 1 root root 10 2012-08-29 17:33 pci-0000:00:04.0-virtio-pci-virtio0-part2 -&gt; ../../vda2
lrwxrwxrwx 1 root root 10 2012-08-29 17:33 pci-0000:00:04.0-virtio-pci-virtio0-part5 -&gt; ../../vda5
lrwxrwxrwx 1 root root 9 2012-08-29 17:33 pci-0000:00:06.0-virtio-pci-virtio2 -&gt; ../../vdb
lrwxrwxrwx 1 root root 9 2012-08-29 17:33 pci-0000:00:08.0-virtio-pci-virtio3 -&gt; ../../vdc
lrwxrwxrwx 1 root root 9 2012-08-29 17:33 pci-0000:00:09.0-virtio-pci-virtio4 -&gt; ../../vdd
lrwxrwxrwx 1 root root 10 2012-08-29 17:33 pci-0000:00:09.0-virtio-pci-virtio4-part1 -&gt; ../../vdd1</screen>
        <para>You might also have this problem after attaching and detaching the same volume
                    from the same VM with the same mount point multiple times. In this case,
                    restart the KVM host.</para>
      </section>
    </section>
    <section xml:id="failed-to-attach-volume-systool-is-not-installed">
      <title>Failed to attach volume, systool is not installed</title>
      <section xml:id="id16">
        <title>Problem</title>
        <para>This warning and error occurs if you do not have the required <literal>sysfsutils</literal>
                    package installed on the compute node:</para>
        <screen language="console">WARNING nova.virt.libvirt.utils [req-1200f887-c82b-4e7c-a891-fac2e3735dbb\
admin admin|req-1200f887-c82b-4e7c-a891-fac2e3735dbb admin admin] systool\
is not installed
ERROR nova.compute.manager [req-1200f887-c82b-4e7c-a891-fac2e3735dbb admin\
admin|req-1200f887-c82b-4e7c-a891-fac2e3735dbb admin admin]
[instance: df834b5a-8c3f-477a-be9b-47c97626555c|instance: df834b5a-8c3f-47\
7a-be9b-47c97626555c]
Failed to attach volume 13d5c633-903a-4764-a5a0-3336945b1db1 at /dev/vdk.</screen>
      </section>
      <section xml:id="id17">
        <title>Solution</title>
        <para>Install the <literal>sysfsutils</literal> package on the compute node. For example:</para>
        <screen language="console"># apt-get install sysfsutils</screen>
      </section>
    </section>
    <section xml:id="failed-to-connect-volume-in-fc-san">
      <title>Failed to connect volume in FC SAN</title>
      <section xml:id="id18">
        <title>Problem</title>
        <para>The compute node failed to connect to a volume in a Fibre Channel (FC) SAN
                    configuration. The WWN may not be zoned correctly in your FC SAN that links the
                    compute host to the storage array:</para>
        <screen language="console">ERROR nova.compute.manager [req-2ddd5297-e405-44ab-aed3-152cd2cfb8c2 admin\
demo|req-2ddd5297-e405-44ab-aed3-152cd2cfb8c2 admin demo] [instance: 60ebd\
6c7-c1e3-4bf0-8ef0-f07aa4c3d5f3|instance: 60ebd6c7-c1e3-4bf0-8ef0-f07aa4c3\
d5f3]
Failed to connect to volume 6f6a6a9c-dfcf-4c8d-b1a8-4445ff883200 while\
attaching at /dev/vdjTRACE nova.compute.manager [instance: 60ebd6c7-c1e3-4\
bf0-8ef0-f07aa4c3d5f3|instance: 60ebd6c7-c1e3-4bf0-8ef0-f07aa4c3d5f3]
Traceback (most recent call last):f07aa4c3d5f3\] ClientException: The\
server has either erred or is incapable of performing the requested\
operation.(HTTP 500)(Request-ID: req-71e5132b-21aa-46ee-b3cc-19b5b4ab2f00)</screen>
      </section>
      <section xml:id="id19">
        <title>Solution</title>
        <para>The network administrator must configure the FC SAN fabric by correctly zoning
                    the WWN (port names) from your compute node HBAs.</para>
      </section>
    </section>
    <section xml:id="multipath-call-failed-exit">
      <title>Multipath call failed exit</title>
      <section xml:id="id20">
        <title>Problem</title>
        <para>Multipath call failed exit. This warning occurs in the Compute log if you do
                    not have the optional <literal>multipath-tools</literal> package installed on the compute
                    node. This is an optional package and the volume attachment does work without
                    the multipath tools installed.  If the <literal>multipath-tools</literal> package is installed
                    on the compute node, it is used to perform the volume attachment.  The IDs in
                    your message are unique to your system.</para>
        <screen language="console">WARNING nova.storage.linuxscsi [req-cac861e3-8b29-4143-8f1b-705d0084e571 \
admin admin|req-cac861e3-8b29-4143-8f1b-705d0084e571 admin admin] \
Multipath call failed exit (96)</screen>
      </section>
      <section xml:id="id21">
        <title>Solution</title>
        <para>Install the <literal>multipath-tools</literal> package on the compute node. For example:</para>
        <screen language="console"># apt-get install multipath-tools</screen>
      </section>
    </section>
    <section xml:id="failed-to-attach-volume-missing-sg-scan">
      <title>Failed to Attach Volume, Missing sg_scan</title>
      <section xml:id="id22">
        <title>Problem</title>
        <para>Failed to attach volume to an instance, <literal>sg_scan</literal> file not found. This error
                    occurs when the sg3-utils package is not installed on the compute node.  The
                    IDs in your message are unique to your system:</para>
        <screen language="console">ERROR nova.compute.manager [req-cf2679fd-dd9e-4909-807f-48fe9bda3642 admin admin|req-cf2679fd-dd9e-4909-807f-48fe9bda3642 admin admin]
[instance: 7d7c92e0-49fa-4a8e-87c7-73f22a9585d5|instance:  7d7c92e0-49fa-4a8e-87c7-73f22a9585d5]
Failed to attach volume  4cc104c4-ac92-4bd6-9b95-c6686746414a at /dev/vdcTRACE nova.compute.manager
[instance:  7d7c92e0-49fa-4a8e-87c7-73f22a9585d5|instance: 7d7c92e0-49fa-4a8e-87c7-73f22a9585d5]
Stdout: '/usr/local/bin/nova-rootwrap: Executable not found: /usr/bin/sg_scan'</screen>
      </section>
      <section xml:id="id23">
        <title>Solution</title>
        <para>Install the <literal>sg3-utils</literal> package on the compute node. For example:</para>
        <screen language="console"># apt-get install sg3-utils</screen>
      </section>
    </section>
  </chapter>
  <chapter xml:id="compute-trusted-pools-rst" xml:base="system-admin">
    <title>System administration</title>
    <section xml:id="section-manage-compute-users" xml:base="manage-users">
      <title>Manage Compute users</title>
      <para>Access to the Euca2ools (ec2) API is controlled by an access key and a secret
            key. The user's access key needs to be included in the request, and the request
            must be signed with the secret key. Upon receipt of API requests, Compute
            verifies the signature and runs commands on behalf of the user.</para>
      <para>To begin using Compute, you must create a user with the Identity service.</para>
    </section>
    <section xml:id="manage-volumes" xml:base="manage-volumes">
      <title>Manage volumes</title>
      <para>Depending on the setup of your cloud provider, they may give you an endpoint to
            use to manage volumes, or there may be an extension under the covers. In either
            case, you can use the <literal>openstack</literal> CLI to manage volumes.</para>
      <table xml:id="id1">
        <title>openstack volume commands</title>
        <tgroup cols="2">
          <colspec colname="c0" colwidth="50"/>
          <colspec colname="c1" colwidth="50"/>
          <thead>
            <row>
              <entry>
                <para>Command</para>
              </entry>
              <entry>
                <para>Description</para>
              </entry>
            </row>
          </thead>
          <tbody>
            <row>
              <entry>
                <para>server add volume</para>
              </entry>
              <entry>
                <para>Attach a volume to a server.</para>
              </entry>
            </row>
            <row>
              <entry>
                <para>volume create</para>
              </entry>
              <entry>
                <para>Add a new volume.</para>
              </entry>
            </row>
            <row>
              <entry>
                <para>volume delete</para>
              </entry>
              <entry>
                <para>Remove or delete a volume.</para>
              </entry>
            </row>
            <row>
              <entry>
                <para>server remove volume</para>
              </entry>
              <entry>
                <para>Detach or remove a volume from a server.</para>
              </entry>
            </row>
            <row>
              <entry>
                <para>volume list</para>
              </entry>
              <entry>
                <para>List all the volumes.</para>
              </entry>
            </row>
            <row>
              <entry>
                <para>volume show</para>
              </entry>
              <entry>
                <para>Show details about a volume.</para>
              </entry>
            </row>
            <row>
              <entry>
                <para>snapshot create</para>
              </entry>
              <entry>
                <para>Add a new snapshot.</para>
              </entry>
            </row>
            <row>
              <entry>
                <para>snapshot delete</para>
              </entry>
              <entry>
                <para>Remove a snapshot.</para>
              </entry>
            </row>
            <row>
              <entry>
                <para>snapshot list</para>
              </entry>
              <entry>
                <para>List all the snapshots.</para>
              </entry>
            </row>
            <row>
              <entry>
                <para>snapshot show</para>
              </entry>
              <entry>
                <para>Show details about a snapshot.</para>
              </entry>
            </row>
            <row>
              <entry>
                <para>volume type create</para>
              </entry>
              <entry>
                <para>Create a new volume type.</para>
              </entry>
            </row>
            <row>
              <entry>
                <para>volume type delete</para>
              </entry>
              <entry>
                <para>Delete a specific flavor</para>
              </entry>
            </row>
            <row>
              <entry>
                <para>volume type list</para>
              </entry>
              <entry>
                <para>Print a list of available 'volume types'.</para>
              </entry>
            </row>
          </tbody>
        </tgroup>
      </table>
      <para>For example, to list IDs and names of volumes, run:</para>
      <screen language="console">$ openstack volume list
+--------+--------------+-----------+------+-------------+
| ID     | Display Name | Status    | Size | Attached to |
+--------+--------------+-----------+------+-------------+
| 86e6cb | testnfs      | available |    1 |             |
| e389f7 | demo         | available |    1 |             |
+--------+--------------+-----------+------+-------------+</screen>
    </section>
    <section xml:id="flavors" xml:base="flavors">
      <title>Flavors</title>
      <para>Admin users can use the <command>openstack flavor</command> command to customize and
            manage flavors. To see information for this command, run:</para>
      <screen language="console">$ openstack flavor --help
Command "flavor" matches:
  flavor create
  flavor delete
  flavor list
  flavor set
  flavor show
  flavor unset</screen>
      <note>
        <para>Configuration rights can be delegated to additional users by redefining
                the access controls for <literal>os_compute_api:os-flavor-manage</literal> in
                <literal>/etc/nova/policy.json</literal> on the <literal>nova-api</literal> server.</para>
      </note>
      <para>Flavors define these elements:</para>
      <informaltable xml:id="index-0">
        <tgroup cols="2">
          <colspec colname="c0" colwidth="13"/>
          <colspec colname="c1" colwidth="63"/>
          <thead>
            <row>
              <entry>
                <para>Element</para>
              </entry>
              <entry>
                <para>Description</para>
              </entry>
            </row>
          </thead>
          <tbody>
            <row>
              <entry>
                <para>Name</para>
              </entry>
              <entry>
                <para>A descriptive name. XX.SIZE_NAME is typically not required,
                                though some third party tools may rely on it.</para>
              </entry>
            </row>
            <row>
              <entry>
                <para>Memory MB</para>
              </entry>
              <entry>
                <para>Instance memory in megabytes.</para>
              </entry>
            </row>
            <row>
              <entry>
                <para>Disk</para>
              </entry>
              <entry>
                <para>Virtual root disk size in gigabytes. This is an ephemeral disk that the base image is copied into. When booting from a persistent volume it is not used. The "0" size is a special case which uses the native base image size as the size of the
                                ephemeral root volume. However, in this case the filter
                                scheduler cannot select the compute host based on the virtual
                                image size. Therefore 0 should only be used for volume booted
                                instances or for testing purposes.</para>
              </entry>
            </row>
            <row>
              <entry>
                <para>Ephemeral</para>
              </entry>
              <entry>
                <para>Specifies the size of a secondary ephemeral data disk. This
                                is an empty, unformatted disk and exists only for the life of the instance. Default value is <literal>0</literal>.</para>
              </entry>
            </row>
            <row>
              <entry>
                <para>Swap</para>
              </entry>
              <entry>
                <para>Optional swap space allocation for the instance. Default
                                value is <literal>0</literal>.</para>
              </entry>
            </row>
            <row>
              <entry>
                <para>VCPUs</para>
              </entry>
              <entry>
                <para>Number of virtual CPUs presented to the instance.</para>
              </entry>
            </row>
            <row>
              <entry>
                <para>RXTX Factor</para>
              </entry>
              <entry>
                <para>Optional property allows created servers to have a different
                                bandwidth cap than that defined in the network they are attached to. This factor is multiplied by the rxtx_base property of the network. Default value is <literal>1.0</literal>. That is, the same
                                as attached network. This parameter is only available for Xen
                                or NSX based systems.</para>
              </entry>
            </row>
            <row>
              <entry>
                <para>Is Public</para>
              </entry>
              <entry>
                <para>Boolean value, whether flavor is available to all users or private to the project it was created in. Defaults to <literal>True</literal>.</para>
              </entry>
            </row>
            <row>
              <entry>
                <para>Extra Specs</para>
              </entry>
              <entry>
                <para>Key and value pairs that define on which compute nodes a flavor can run. These pairs must match corresponding pairs on the compute nodes. Use to implement special resources, such as flavors that run on only compute nodes with GPU hardware.</para>
              </entry>
            </row>
          </tbody>
        </tgroup>
      </informaltable>
      <note>
        <para>Flavor customization can be limited by the hypervisor in use. For example
                the libvirt driver enables quotas on CPUs available to a VM, disk tuning,
                bandwidth I/O, watchdog behavior, random number generator device control,
                and instance VIF traffic control.</para>
      </note>
      <section xml:id="is-public">
        <title>Is Public</title>
        <para>Flavors can be assigned to particular projects. By default, a flavor is public
                and available to all projects. Private flavors are only accessible to those on
                the access list and are invisible to other projects. To create and assign a
                private flavor to a project, run this command:</para>
        <screen language="console">$ openstack flavor create --private p1.medium --id auto --ram 512 --disk 40 --vcpus 4</screen>
      </section>
      <section xml:id="extra-specs">
        <title>Extra Specs</title>
        <variablelist>
          <varlistentry>
            <term>CPU limits</term>
            <listitem>
              <para>You can configure the CPU limits with control parameters with the <literal>nova</literal>
                            client. For example, to configure the I/O limit, use:</para>
              <screen language="console">$ openstack flavor set FLAVOR-NAME \
    --property quota:read_bytes_sec=10240000 \
    --property quota:write_bytes_sec=10240000</screen>
              <para>Use these optional parameters to control weight shares, enforcement intervals
                            for runtime quotas, and a quota for maximum allowed bandwidth:</para>
              <itemizedlist>
                <listitem>
                  <para><literal>cpu_shares</literal>: Specifies the proportional weighted share for the domain.
                                    If this element is omitted, the service defaults to the OS provided
                                    defaults. There is no unit for the value; it is a relative measure based on
                                    the setting of other VMs. For example, a VM configured with value 2048 gets
                                    twice as much CPU time as a VM configured with value 1024.</para>
                </listitem>
                <listitem>
                  <para><literal>cpu_shares_level</literal>: On VMware, specifies the allocation level. Can be
                                    <literal>custom</literal>, <literal>high</literal>, <literal>normal</literal>, or <literal>low</literal>. If you choose <literal>custom</literal>, set
                                    the number of shares using <literal>cpu_shares_share</literal>.</para>
                </listitem>
                <listitem>
                  <para><literal>cpu_period</literal>: Specifies the enforcement interval (unit: microseconds)
                                    for QEMU and LXC hypervisors. Within a period, each VCPU of the domain is
                                    not allowed to consume more than the quota worth of runtime. The value
                                    should be in range <literal>[1000, 1000000]</literal>.  A period with value 0 means no
                                    value.</para>
                </listitem>
                <listitem>
                  <para><literal>cpu_limit</literal>: Specifies the upper limit for VMware machine CPU allocation
                                    in MHz. This parameter ensures that a machine never uses more than the
                                    defined amount of CPU time. It can be used to enforce a limit on the
                                    machine's CPU performance.</para>
                </listitem>
                <listitem>
                  <para><literal>cpu_reservation</literal>: Specifies the guaranteed minimum CPU reservation in
                                    MHz for VMware. This means that if needed, the machine will definitely get
                                    allocated the reserved amount of CPU cycles.</para>
                </listitem>
                <listitem>
                  <para><literal>cpu_quota</literal>: Specifies the maximum allowed bandwidth (unit:
                                    microseconds). A domain with a negative-value quota indicates that the
                                    domain has infinite bandwidth, which means that it is not bandwidth
                                    controlled. The value should be in range <literal>[1000, 18446744073709551]</literal> or
                                    less than 0. A quota with value 0 means no value. You can use this feature
                                    to ensure that all vCPUs run at the same speed. For example:</para>
                  <screen language="console">$ openstack flavor set FLAVOR-NAME \
    --property quota:cpu_quota=10000 \
    --property quota:cpu_period=20000</screen>
                  <para>In this example, an instance of <literal>FLAVOR-NAME</literal> can only consume a maximum
                                    of 50% CPU of a physical CPU computing capability.</para>
                </listitem>
              </itemizedlist>
            </listitem>
          </varlistentry>
          <varlistentry>
            <term>Memory limits</term>
            <listitem>
              <para>For VMware, you can configure the memory limits with control parameters.</para>
              <para>Use these optional parameters to limit the memory allocation, guarantee
                            minimum memory reservation, and to specify shares used in case of resource
                            contention:</para>
              <itemizedlist>
                <listitem>
                  <para><literal>memory_limit</literal>: Specifies the upper limit for VMware machine memory
                                    allocation in MB. The utilization of a virtual machine will not exceed this
                                    limit, even if there are available resources. This is typically used to
                                    ensure a consistent performance of virtual machines independent of
                                    available resources.</para>
                </listitem>
                <listitem>
                  <para><literal>memory_reservation</literal>: Specifies the guaranteed minimum memory reservation
                                    in MB for VMware. This means the specified amount of memory will definitely
                                    be allocated to the machine.</para>
                </listitem>
                <listitem>
                  <para><literal>memory_shares_level</literal>: On VMware, specifies the allocation level.  This
                                    can be <literal>custom</literal>, <literal>high</literal>, <literal>normal</literal> or <literal>low</literal>. If you choose
                                    <literal>custom</literal>, set the number of shares using <literal>memory_shares_share</literal>.</para>
                </listitem>
                <listitem>
                  <para><literal>memory_shares_share</literal>: Specifies the number of shares allocated in the
                                    event that <literal>custom</literal> is used. There is no unit for this value. It is a
                                    relative measure based on the settings for other VMs.  For example:</para>
                  <screen language="console">$ openstack flavor set FLAVOR-NAME \
    --property quota:memory_shares_level=custom \
    --property quota:memory_shares_share=15</screen>
                </listitem>
              </itemizedlist>
            </listitem>
          </varlistentry>
          <varlistentry>
            <term>Disk I/O limits</term>
            <listitem>
              <para>For VMware, you can configure the resource limits for disk with control
                            parameters.</para>
              <para>Use these optional parameters to limit the disk utilization, guarantee disk
                            allocation, and to specify shares used in case of resource contention. This
                            allows the VMware driver to enable disk allocations for the running instance.</para>
              <itemizedlist>
                <listitem>
                  <para><literal>disk_io_limit</literal>: Specifies the upper limit for disk utilization in I/O
                                    per second. The utilization of a virtual machine will not exceed this
                                    limit, even if there are available resources. The default value is -1 which
                                    indicates unlimited usage.</para>
                </listitem>
                <listitem>
                  <para><literal>disk_io_reservation</literal>: Specifies the guaranteed minimum disk allocation
                                    in terms of Input/output Operations Per Second (IOPS).</para>
                </listitem>
                <listitem>
                  <para><literal>disk_io_shares_level</literal>: Specifies the allocation level. This can be
                                    <literal>custom</literal>, <literal>high</literal>, <literal>normal</literal> or <literal>low</literal>.  If you choose custom, set the
                                    number of shares using <literal>disk_io_shares_share</literal>.</para>
                </listitem>
                <listitem>
                  <para><literal>disk_io_shares_share</literal>: Specifies the number of shares allocated in the
                                    event that <literal>custom</literal> is used.  When there is resource contention, this
                                    value is used to determine the resource allocation.</para>
                  <para>The example below sets the <literal>disk_io_reservation</literal> to 2000 IOPS.</para>
                  <screen language="console">$ openstack flavor set FLAVOR-NAME \
    --property quota:disk_io_reservation=2000</screen>
                </listitem>
              </itemizedlist>
            </listitem>
          </varlistentry>
          <varlistentry>
            <term>Disk tuning</term>
            <listitem>
              <para>Using disk I/O quotas, you can set maximum disk write to 10 MB per second for
                            a VM user. For example:</para>
              <screen language="console">$ openstack flavor set FLAVOR-NAME \
    --property quota:disk_write_bytes_sec=10485760</screen>
              <para>The disk I/O options are:</para>
              <itemizedlist>
                <listitem>
                  <para>
                    <literal>disk_read_bytes_sec</literal>
                  </para>
                </listitem>
                <listitem>
                  <para>
                    <literal>disk_read_iops_sec</literal>
                  </para>
                </listitem>
                <listitem>
                  <para>
                    <literal>disk_write_bytes_sec</literal>
                  </para>
                </listitem>
                <listitem>
                  <para>
                    <literal>disk_write_iops_sec</literal>
                  </para>
                </listitem>
                <listitem>
                  <para>
                    <literal>disk_total_bytes_sec</literal>
                  </para>
                </listitem>
                <listitem>
                  <para>
                    <literal>disk_total_iops_sec</literal>
                  </para>
                </listitem>
              </itemizedlist>
            </listitem>
          </varlistentry>
          <varlistentry>
            <term>Bandwidth I/O</term>
            <listitem>
              <para>The vif I/O options are:</para>
              <itemizedlist>
                <listitem>
                  <para>
                    <literal>vif_inbound_average</literal>
                  </para>
                </listitem>
                <listitem>
                  <para>
                    <literal>vif_inbound_burst</literal>
                  </para>
                </listitem>
                <listitem>
                  <para>
                    <literal>vif_inbound_peak</literal>
                  </para>
                </listitem>
                <listitem>
                  <para>
                    <literal>vif_outbound_average</literal>
                  </para>
                </listitem>
                <listitem>
                  <para>
                    <literal>vif_outbound_burst</literal>
                  </para>
                </listitem>
                <listitem>
                  <para>
                    <literal>vif_outbound_peak</literal>
                  </para>
                </listitem>
              </itemizedlist>
              <para>Incoming and outgoing traffic can be shaped independently. The bandwidth
                            element can have at most, one inbound and at most, one outbound child
                            element. If you leave any of these child elements out, no quality of service
                            (QoS) is applied on that traffic direction. So, if you want to shape only the
                            network's incoming traffic, use inbound only (and vice versa). Each element
                            has one mandatory attribute average, which specifies the average bit rate on
                            the interface being shaped.</para>
              <para>There are also two optional attributes (integer): <literal>peak</literal>, which specifies
                            the maximum rate at which a bridge can send data (kilobytes/second), and
                            <literal>burst</literal>, the amount of bytes that can be burst at peak speed (kilobytes).
                            The rate is shared equally within domains connected to the network.</para>
              <para>The example below sets network traffic bandwidth limits for existing flavor
                            as follows:</para>
              <itemizedlist>
                <listitem>
                  <para>Outbound traffic:</para>
                  <itemizedlist>
                    <listitem>
                      <para>average: 262 Mbps (32768 kilobytes/second)</para>
                    </listitem>
                    <listitem>
                      <para>peak: 524 Mbps (65536 kilobytes/second)</para>
                    </listitem>
                    <listitem>
                      <para>burst: 65536 kilobytes</para>
                    </listitem>
                  </itemizedlist>
                </listitem>
                <listitem>
                  <para>Inbound traffic:</para>
                  <itemizedlist>
                    <listitem>
                      <para>average: 262 Mbps (32768 kilobytes/second)</para>
                    </listitem>
                    <listitem>
                      <para>peak: 524 Mbps (65536 kilobytes/second)</para>
                    </listitem>
                    <listitem>
                      <para>burst: 65536 kilobytes</para>
                    </listitem>
                  </itemizedlist>
                </listitem>
              </itemizedlist>
              <screen language="console">$ openstack flavor set FLAVOR-NAME \
    --property quota:vif_outbound_average=32768 \
    --property quota:vif_outbound_peak=65536 \
    --property quota:vif_outbound_burst=65536 \
    --property quota:vif_inbound_average=32768 \
    --property quota:vif_inbound_peak=65536 \
    --property quota:vif_inbound_burst=65536</screen>
              <note>
                <para>All the speed limit values in above example are specified in
                                kilobytes/second. And burst values are in kilobytes. Values were converted
                                using 'Data rate units on Wikipedia
                                &lt;<link xmlns:xl="http://www.w3.org/1999/xlink" xl:href="https://en.wikipedia.org/wiki/Data_rate_units"/>&gt;`_.</para>
              </note>
            </listitem>
          </varlistentry>
          <varlistentry>
            <term>Watchdog behavior</term>
            <listitem>
              <para>For the libvirt driver, you can enable and set the behavior of a virtual
                            hardware watchdog device for each flavor. Watchdog devices keep an eye on the
                            guest server, and carry out the configured action, if the server hangs. The
                            watchdog uses the i6300esb device (emulating a PCI Intel 6300ESB). If
                            <literal>hw:watchdog_action</literal> is not specified, the watchdog is disabled.</para>
              <para>To set the behavior, use:</para>
              <screen language="console">$ openstack flavor set FLAVOR-NAME --property hw:watchdog_action=ACTION</screen>
              <para>Valid ACTION values are:</para>
              <itemizedlist>
                <listitem>
                  <para><literal>disabled</literal>: (default) The device is not attached.</para>
                </listitem>
                <listitem>
                  <para><literal>reset</literal>: Forcefully reset the guest.</para>
                </listitem>
                <listitem>
                  <para><literal>poweroff</literal>: Forcefully power off the guest.</para>
                </listitem>
                <listitem>
                  <para><literal>pause</literal>: Pause the guest.</para>
                </listitem>
                <listitem>
                  <para><literal>none</literal>: Only enable the watchdog; do nothing if the server hangs.</para>
                </listitem>
              </itemizedlist>
              <note>
                <para>Watchdog behavior set using a specific image's properties will override
                                behavior set using flavors.</para>
              </note>
            </listitem>
          </varlistentry>
          <varlistentry>
            <term>Random-number generator</term>
            <listitem>
              <para>If a random-number generator device has been added to the instance through
                            its image properties, the device can be enabled and configured using:</para>
              <screen language="console">$ openstack flavor set FLAVOR-NAME \
    --property hw_rng:allowed=True \
    --property hw_rng:rate_bytes=RATE-BYTES \
    --property hw_rng:rate_period=RATE-PERIOD</screen>
              <para>Where:</para>
              <itemizedlist>
                <listitem>
                  <para>RATE-BYTES: (integer) Allowed amount of bytes that the guest can read from
                                    the host's entropy per period.</para>
                </listitem>
                <listitem>
                  <para>RATE-PERIOD: (integer) Duration of the read period in seconds.</para>
                </listitem>
              </itemizedlist>
            </listitem>
          </varlistentry>
          <varlistentry>
            <term>CPU topology</term>
            <listitem>
              <para>For the libvirt driver, you can define the topology of the processors in the
                            virtual machine using properties. The properties with <literal>max</literal> limit the
                            number that can be selected by the user with image properties.</para>
              <screen language="console">$ openstack flavor set FLAVOR-NAME \
    --property hw:cpu_sockets=FLAVOR-SOCKETS \
    --property hw:cpu_cores=FLAVOR-CORES \
    --property hw:cpu_threads=FLAVOR-THREADS \
    --property hw:cpu_max_sockets=FLAVOR-SOCKETS \
    --property hw:cpu_max_cores=FLAVOR-CORES \
    --property hw:cpu_max_threads=FLAVOR-THREADS</screen>
              <para>Where:</para>
              <itemizedlist>
                <listitem>
                  <para>FLAVOR-SOCKETS: (integer) The number of sockets for the guest VM. By
                                    default, this is set to the number of vCPUs requested.</para>
                </listitem>
                <listitem>
                  <para>FLAVOR-CORES: (integer) The number of cores per socket for the guest VM. By
                                    default, this is set to <literal>1</literal>.</para>
                </listitem>
                <listitem>
                  <para>FLAVOR-THREADS: (integer) The number of threads per core for the guest VM.
                                    By default, this is set to <literal>1</literal>.</para>
                </listitem>
              </itemizedlist>
            </listitem>
          </varlistentry>
          <varlistentry>
            <term>CPU pinning policy</term>
            <listitem>
              <para>For the libvirt driver, you can pin the virtual CPUs (vCPUs) of instances to
                            the host's physical CPU cores (pCPUs) using properties. You can further
                            refine this by stating how hardware CPU threads in a simultaneous
                            multithreading-based (SMT) architecture be used. These configurations will
                            result in improved per-instance determinism and performance.</para>
              <note>
                <para>SMT-based architectures include Intel processors with Hyper-Threading
                                technology. In these architectures, processor cores share a number of
                                components with one or more other cores. Cores in such architectures are
                                commonly referred to as hardware threads, while the cores that a given
                                core share components with are known as thread siblings.</para>
              </note>
              <note>
                <para>Host aggregates should be used to separate these pinned instances from
                                unpinned instances as the latter will not respect the resourcing
                                requirements of the former.</para>
              </note>
              <screen>$ openstack flavor set FLAVOR-NAME \
    --property hw:cpu_policy=CPU-POLICY \
    --property hw:cpu_thread_policy=CPU-THREAD-POLICY</screen>
              <para>Valid CPU-POLICY values are:</para>
              <itemizedlist>
                <listitem>
                  <para><literal>shared</literal>: (default) The guest vCPUs will be allowed to freely float
                                    across host pCPUs, albeit potentially constrained by NUMA policy.</para>
                </listitem>
                <listitem>
                  <para><literal>dedicated</literal>: The guest vCPUs will be strictly pinned to a set of host
                                    pCPUs. In the absence of an explicit vCPU topology request, the drivers
                                    typically expose all vCPUs as sockets with one core and one thread.  When
                                    strict CPU pinning is in effect the guest CPU topology will be setup to
                                    match the topology of the CPUs to which it is pinned. This option implies
                                    an overcommit ratio of 1.0. For example, if a two vCPU guest is pinned to a
                                    single host core with two threads, then the guest will get a topology of
                                    one socket, one core, two threads.</para>
                </listitem>
              </itemizedlist>
              <para>Valid CPU-THREAD-POLICY values are:</para>
              <itemizedlist>
                <listitem>
                  <para><literal>prefer</literal>: (default) The host may or may not have an SMT architecture.
                                    Where an SMT architecture is present, thread siblings are preferred.</para>
                </listitem>
                <listitem>
                  <para><literal>isolate</literal>: The host must not have an SMT architecture or must emulate a
                                    non-SMT architecture. If the host does not have an SMT architecture, each
                                    vCPU is placed on a different core as expected. If the host does have an
                                    SMT architecture - that is, one or more cores have thread siblings - then
                                    each vCPU is placed on a different physical core. No vCPUs from other
                                    guests are placed on the same core. All but one thread sibling on each
                                    utilized core is therefore guaranteed to be unusable.</para>
                </listitem>
                <listitem>
                  <para><literal>require</literal>: The host must have an SMT architecture. Each vCPU is allocated
                                    on thread siblings. If the host does not have an SMT architecture, then it
                                    is not used. If the host has an SMT architecture, but not enough cores with
                                    free thread siblings are available, then scheduling fails.</para>
                </listitem>
              </itemizedlist>
              <note>
                <para>The <literal>hw:cpu_thread_policy</literal> option is only valid if <literal>hw:cpu_policy</literal> is
                                set to <literal>dedicated</literal>.</para>
              </note>
            </listitem>
          </varlistentry>
        </variablelist>
        <variablelist>
          <varlistentry>
            <term>NUMA topology</term>
            <listitem>
              <para>For the libvirt driver, you can define the host NUMA placement for the
                            instance vCPU threads as well as the allocation of instance vCPUs and memory
                            from the host NUMA nodes. For flavors whose memory and vCPU allocations are
                            larger than the size of NUMA nodes in the compute hosts, the definition of a
                            NUMA topology allows hosts to better utilize NUMA and improve performance of
                            the instance OS.</para>
              <screen language="console">$ openstack flavor set FLAVOR-NAME \
    --property hw:numa_nodes=FLAVOR-NODES \
    --property hw:numa_cpus.N=FLAVOR-CORES \
    --property hw:numa_mem.N=FLAVOR-MEMORY</screen>
              <para>Where:</para>
              <itemizedlist>
                <listitem>
                  <para>FLAVOR-NODES: (integer) The number of host NUMA nodes to restrict execution
                                    of instance vCPU threads to. If not specified, the vCPU threads can run on
                                    any number of the host NUMA nodes available.</para>
                </listitem>
                <listitem>
                  <para>N: (integer) The instance NUMA node to apply a given CPU or memory
                                    configuration to, where N is in the range <literal>0</literal> to <literal>FLAVOR-NODES - 1</literal>.</para>
                </listitem>
                <listitem>
                  <para>FLAVOR-CORES: (comma-separated list of integers) A list of instance vCPUs
                                    to map to instance NUMA node N. If not specified, vCPUs are evenly divided
                                    among available NUMA nodes.</para>
                </listitem>
                <listitem>
                  <para>FLAVOR-MEMORY: (integer) The number of MB of instance memory to map to
                                    instance NUMA node N. If not specified, memory is evenly divided among
                                    available NUMA nodes.</para>
                </listitem>
              </itemizedlist>
              <note>
                <para><literal>hw:numa_cpus.N</literal> and <literal>hw:numa_mem.N</literal> are only valid if
                                <literal>hw:numa_nodes</literal> is set. Additionally, they are only required if the
                                instance's NUMA nodes have an asymmetrical allocation of CPUs and RAM
                                (important for some NFV workloads).</para>
              </note>
              <note>
                <para>The <literal>N</literal> parameter is an index of <emphasis>guest</emphasis> NUMA nodes and may not
                                correspond to <emphasis>host</emphasis> NUMA nodes. For example, on a platform with two NUMA
                                nodes, the scheduler may opt to place guest NUMA node 0, as referenced in
                                <literal>hw:numa_mem.0</literal> on host NUMA node 1 and vice versa.  Similarly, the
                                integers used for <literal>FLAVOR-CORES</literal> are indexes of <emphasis>guest</emphasis> vCPUs and may
                                not correspond to <emphasis>host</emphasis> CPUs. As such, this feature cannot be used to
                                constrain instances to specific host CPUs or NUMA nodes.</para>
              </note>
              <warning>
                <para>If the combined values of <literal>hw:numa_cpus.N</literal> or <literal>hw:numa_mem.N</literal> are
                                greater than the available number of CPUs or memory respectively, an
                                exception is raised.</para>
              </warning>
            </listitem>
          </varlistentry>
          <varlistentry>
            <term>Large pages allocation</term>
            <listitem>
              <para>You can configure the size of large pages used to back the VMs.</para>
              <screen>$ openstack flavor set FLAVOR-NAME \
    --property hw:mem_page_size=PAGE_SIZE</screen>
              <para>Valid <literal>PAGE_SIZE</literal> values are:</para>
              <itemizedlist>
                <listitem>
                  <para><literal>small</literal>: (default) The smallest page size is used. Example: 4 KB on x86.</para>
                </listitem>
                <listitem>
                  <para><literal>large</literal>: Only use larger page sizes for guest RAM. Example: either 2 MB
                                    or 1 GB on x86.</para>
                </listitem>
                <listitem>
                  <para><literal>any</literal>: It is left up to the compute driver to decide. In this case, the
                                    libvirt driver might try to find large pages, but fall back to small pages.
                                    Other drivers may choose alternate policies for <literal>any</literal>.</para>
                </listitem>
                <listitem>
                  <para>pagesize: (string) An explicit page size can be set if the workload has
                                    specific requirements. This value can be an integer value for the page size
                                    in KB, or can use any standard suffix. Example: <literal>4KB</literal>, <literal>2MB</literal>,
                                    <literal>2048</literal>, <literal>1GB</literal>.</para>
                </listitem>
              </itemizedlist>
              <note>
                <para>Large pages can be enabled for guest RAM without any regard to whether the
                                guest OS will use them or not. If the guest OS chooses not to use huge
                                pages, it will merely see small pages as before. Conversely, if a guest OS
                                does intend to use huge pages, it is very important that the guest RAM be
                                backed by huge pages. Otherwise, the guest OS will not be getting the
                                performance benefit it is expecting.</para>
              </note>
            </listitem>
          </varlistentry>
          <varlistentry>
            <term>PCI passthrough</term>
            <listitem>
              <para>You can assign PCI devices to a guest by specifying them in the flavor.</para>
              <screen>$ openstack flavor set FLAVOR-NAME \
    --property pci_passthrough:alias=ALIAS:COUNT</screen>
              <para>Where:</para>
              <itemizedlist>
                <listitem>
                  <para>ALIAS: (string) The alias which correspond to a particular PCI device class
                                    as configured in the nova configuration file (see
                                    <xref linkend="../configuration/config"/>).</para>
                </listitem>
                <listitem>
                  <para>COUNT: (integer) The amount of PCI devices of type ALIAS to be assigned to
                                    a guest.</para>
                </listitem>
              </itemizedlist>
            </listitem>
          </varlistentry>
          <varlistentry>
            <term>Secure Boot</term>
            <listitem>
              <para>When your Compute services use the Hyper-V hypervisor, you can enable secure
                            boot for Windows and Linux instances.</para>
              <screen>$ openstack flavor set FLAVOR-NAME \
    --property os:secure_boot=SECURE_BOOT_OPTION</screen>
              <para>Valid <literal>SECURE_BOOT_OPTION</literal> values are:</para>
              <itemizedlist>
                <listitem>
                  <para><literal>required</literal>: Enable Secure Boot for instances running with this flavor.</para>
                </listitem>
                <listitem>
                  <para><literal>disabled</literal> or <literal>optional</literal>: (default) Disable Secure Boot for instances
                                    running with this flavor.</para>
                </listitem>
              </itemizedlist>
            </listitem>
          </varlistentry>
        </variablelist>
      </section>
    </section>
    <section xml:id="compute-service-node-firewall-requirements" xml:base="default-ports">
      <title>Compute service node firewall requirements</title>
      <para>Console connections for virtual machines, whether direct or through a
            proxy, are received on ports <literal>5900</literal> to <literal>5999</literal>. The firewall on each
            Compute service node must allow network traffic on these ports.</para>
      <para>This procedure modifies the iptables firewall to allow incoming
            connections to the Compute services.</para>
      <para>
        <emphasis role="bold">Configuring the service-node firewall</emphasis>
      </para>
      <procedure>
        <step>
          <para>Log in to the server that hosts the Compute service, as root.</para>
        </step>
        <step>
          <para>Edit the <literal>/etc/sysconfig/iptables</literal> file, to add an INPUT rule that
                    allows TCP traffic on ports from <literal>5900</literal> to <literal>5999</literal>. Make sure the new
                    rule appears before any INPUT rules that REJECT traffic:</para>
          <screen language="console">-A INPUT -p tcp -m multiport --dports 5900:5999 -j ACCEPT</screen>
        </step>
        <step>
          <para>Save the changes to the <literal>/etc/sysconfig/iptables</literal> file, and restart the
                    <literal>iptables</literal> service to pick up the changes:</para>
          <screen language="console">$ service iptables restart</screen>
        </step>
        <step>
          <para>Repeat this process for each Compute service node.</para>
        </step>
      </procedure>
    </section>
    <section xml:id="injecting-the-administrator-password" xml:base="admin-password-injection">
      <title>Injecting the administrator password</title>
      <para>Compute can generate a random administrator (root) password and inject that
            password into an instance. If this feature is enabled, users can run
            <command>ssh</command> to an instance without an <command>ssh</command> keypair.  The random
            password appears in the output of the <command>openstack server create</command>
            command.  You can also view and set the admin password from the dashboard.</para>
      <para>By default, the dashboard will display the <literal>admin</literal> password and allow the
            user to modify it.</para>
      <para>If you do not want to support password injection, disable the password fields
            by editing the dashboard's <literal>local_settings.py</literal> file.</para>
      <screen language="none">OPENSTACK_HYPERVISOR_FEATURES = {
...
    'can_set_password': False,
}</screen>
      <para>For hypervisors that use the libvirt back end (such as KVM, QEMU, and LXC),
            admin password injection is disabled by default. To enable it, set this option
            in <literal>/etc/nova/nova.conf</literal>:</para>
      <screen language="ini">[libvirt]
inject_password=true</screen>
      <para>When enabled, Compute will modify the password of the admin account by editing
            the <literal>/etc/shadow</literal> file inside the virtual machine instance.</para>
      <note>
        <para>Users can only use <command>ssh</command> to access the instance by using the admin
                password if the virtual machine image is a Linux distribution, and it has
                been configured to allow users to use <command>ssh</command> as the root user. This
                is not the case for <link xmlns:xl="http://www.w3.org/1999/xlink" xl:href="http://uec-images.ubuntu.com">Ubuntu cloud images</link>
                which, by default, does not allow users to use <command>ssh</command> to access the
                root account.</para>
      </note>
      <para>When using the XenAPI hypervisor back end, Compute uses the XenAPI agent to
            inject passwords into guests. The virtual machine image must be configured with
            the agent for password injection to work.</para>
      <para>For Windows virtual machines, configure the Windows image to retrieve the admin
            password on boot by installing an agent such as <link xmlns:xl="http://www.w3.org/1999/xlink" xl:href="https://cloudbase.it/cloudbase-init">cloudbase-init</link>.</para>
    </section>
    <section xml:id="section-manage-the-cloud" xml:base="manage-the-cloud">
      <title>Manage the cloud</title>
      <section xml:id="managing-the-cloud-with-euca2ools" xml:base="euca2ools">
        <title>Managing the cloud with euca2ools</title>
        <para>The <literal>euca2ools</literal> command-line tool provides a command line interface to EC2
            API calls. For more information, see the <link xmlns:xl="http://www.w3.org/1999/xlink" xl:href="http://docs.hpcloud.com/eucalyptus/">Official Eucalyptus Documentation</link>.</para>
      </section>
      <section xml:id="show-usage-statistics-for-hosts-and-instances" xml:base="common/nova-show-usage-statistics-for-hosts-instances">
        <title>Show usage statistics for hosts and instances</title>
        <para>You can show basic statistics on resource usage for hosts and instances.</para>
        <note>
          <para>For more sophisticated monitoring, see the
                <link xmlns:xl="http://www.w3.org/1999/xlink" xl:href="https://launchpad.net/ceilometer">ceilometer</link> project. You can
                also use tools, such as <link xmlns:xl="http://www.w3.org/1999/xlink" xl:href="http://ganglia.info/">Ganglia</link> or
                <link xmlns:xl="http://www.w3.org/1999/xlink" xl:href="http://graphite.wikidot.com/">Graphite</link>, to gather more detailed
                data.</para>
        </note>
        <section xml:id="show-host-usage-statistics">
          <title>Show host usage statistics</title>
          <para>The following examples show the host usage statistics for a host called
                <literal>devstack</literal>.</para>
          <itemizedlist>
            <listitem>
              <para>List the hosts and the nova-related services that run on them:</para>
              <screen language="console">$ openstack host list
+-----------+-------------+----------+
| Host Name | Service     | Zone     |
+-----------+-------------+----------+
| devstack  | conductor   | internal |
| devstack  | compute     | nova     |
| devstack  | cert        | internal |
| devstack  | network     | internal |
| devstack  | scheduler   | internal |
| devstack  | consoleauth | internal |
+-----------+-------------+----------+</screen>
            </listitem>
            <listitem>
              <para>Get a summary of resource usage of all of the instances running on the host:</para>
              <screen language="console">$ openstack host show devstack
+----------+----------------------------------+-----+-----------+---------+
| Host     | Project                          | CPU | MEMORY MB | DISK GB |
+----------+----------------------------------+-----+-----------+---------+
| devstack | (total)                          | 2   | 4003      | 157     |
| devstack | (used_now)                       | 3   | 5120      | 40      |
| devstack | (used_max)                       | 3   | 4608      | 40      |
| devstack | b70d90d65e464582b6b2161cf3603ced | 1   | 512       | 0       |
| devstack | 66265572db174a7aa66eba661f58eb9e | 2   | 4096      | 40      |
+----------+----------------------------------+-----+-----------+---------+</screen>
              <para>The <literal>CPU</literal> column shows the sum of the virtual CPUs for instances running on
                        the host.</para>
              <para>The <literal>MEMORY MB</literal> column shows the sum of the memory (in MB) allocated to the
                        instances that run on the host.</para>
              <para>The <literal>DISK GB</literal> column shows the sum of the root and ephemeral disk sizes (in
                        GB) of the instances that run on the host.</para>
              <para>The row that has the value <literal>used_now</literal> in the <literal>PROJECT</literal> column shows the
                        sum of the resources allocated to the instances that run on the host, plus
                        the resources allocated to the virtual machine of the host itself.</para>
              <para>The row that has the value <literal>used_max</literal> in the <literal>PROJECT</literal> column shows the
                        sum of the resources allocated to the instances that run on the host.</para>
              <note>
                <para>These values are computed by using information about the flavors of the
                            instances that run on the hosts. This command does not query the CPU
                            usage, memory usage, or hard disk usage of the physical host.</para>
              </note>
            </listitem>
          </itemizedlist>
        </section>
        <section xml:id="show-instance-usage-statistics">
          <title>Show instance usage statistics</title>
          <itemizedlist>
            <listitem>
              <para>Get CPU, memory, I/O, and network statistics for an instance.</para>
              <procedure>
                <step>
                  <para>List instances:</para>
                  <screen language="console">$ openstack server list
+----------+----------------------+--------+------------+-------------+------------------+------------+
| ID       | Name                 | Status | Task State | Power State | Networks         | Image Name |
+----------+----------------------+--------+------------+-------------+------------------+------------+
| 84c6e... | myCirrosServer       | ACTIVE | None       | Running     | private=10.0.0.3 | cirros     |
| 8a995... | myInstanceFromVolume | ACTIVE | None       | Running     | private=10.0.0.4 | ubuntu     |
+----------+----------------------+--------+------------+-------------+------------------+------------+</screen>
                </step>
                <step>
                  <para>Get diagnostic statistics:</para>
                  <screen language="console">$ nova diagnostics myCirrosServer
+---------------------------+--------+
| Property                  | Value  |
+---------------------------+--------+
| memory                    | 524288 |
| memory-actual             | 524288 |
| memory-rss                | 6444   |
| tap1fec8fb8-7a_rx         | 22137  |
| tap1fec8fb8-7a_rx_drop    | 0      |
| tap1fec8fb8-7a_rx_errors  | 0      |
| tap1fec8fb8-7a_rx_packets | 166    |
| tap1fec8fb8-7a_tx         | 18032  |
| tap1fec8fb8-7a_tx_drop    | 0      |
| tap1fec8fb8-7a_tx_errors  | 0      |
| tap1fec8fb8-7a_tx_packets | 130    |
| vda_errors                | -1     |
| vda_read                  | 2048   |
| vda_read_req              | 2      |
| vda_write                 | 182272 |
| vda_write_req             | 74     |
+---------------------------+--------+</screen>
                </step>
              </procedure>
            </listitem>
            <listitem>
              <para>Get summary statistics for each project:</para>
              <screen language="console">$ openstack usage list
Usage from 2013-06-25 to 2013-07-24:
+---------+---------+--------------+-----------+---------------+
| Project | Servers | RAM MB-Hours | CPU Hours | Disk GB-Hours |
+---------+---------+--------------+-----------+---------------+
| demo    | 1       | 344064.44    | 672.00    | 0.00          |
| stack   | 3       | 671626.76    | 327.94    | 6558.86       |
+---------+---------+--------------+-----------+---------------+</screen>
            </listitem>
          </itemizedlist>
         <para>System administrators can use the <command>openstack</command> and <command>euca2ools</command>
            commands to manage their clouds.</para>
      <para>The <literal>openstack</literal> client and <literal>euca2ools</literal> can be used by all users, though
            specific commands might be restricted by the Identity service.</para>
      <para>
        <emphasis role="bold">Managing the cloud with the openstack client</emphasis>
      </para>
      <procedure>
        <step>
          <para>The <literal>python-openstackclient</literal> package provides an <literal>openstack</literal> shell that
                    enables Compute API interactions from the command line. Install the client,
                    and provide your user name and password (which can be set as environment
                    variables for convenience), for the ability to administer the cloud from the
                    command line.</para>
          <para>To install python-openstackclient, follow the instructions in the <link xmlns:xl="http://www.w3.org/1999/xlink" xl:href="https://docs.openstack.org/user-guide/common/cli-install-openstack-command-line-clients.html">OpenStack
                        User Guide</link>.</para>
        </step>
        <step>
          <para>Confirm the installation was successful:</para>
          <screen language="console">$ openstack help
usage: openstack [--version] [-v | -q] [--log-file LOG_FILE] [-h] [--debug]
           [--os-cloud &lt;cloud-config-name&gt;]
           [--os-region-name &lt;auth-region-name&gt;]
           [--os-cacert &lt;ca-bundle-file&gt;] [--verify | --insecure]
           [--os-default-domain &lt;auth-domain&gt;]
           ...</screen>
          <para>Running <command>openstack help</command> returns a list of <literal>openstack</literal> commands
                    and parameters. To get help for a subcommand, run:</para>
          <screen language="console">$ openstack help SUBCOMMAND</screen>
          <para>For a complete list of <literal>openstack</literal> commands and parameters, see the
                    <link xmlns:xl="http://www.w3.org/1999/xlink" xl:href="https://docs.openstack.org/cli-reference/openstack.html">OpenStack Command-Line Reference</link>.</para>
        </step>
        <step>
          <para>Set the required parameters as environment variables to make running
                    commands easier. For example, you can add <literal>--os-username</literal> as an
                    <literal>openstack</literal> option, or set it as an environment variable. To set the user
                    name, password, and project as environment variables, use:</para>
          <screen language="console">$ export OS_USERNAME=joecool
$ export OS_PASSWORD=coolword
$ export OS_TENANT_NAME=coolu</screen>
        </step>
        <step>
          <para>The Identity service gives you an authentication endpoint, which Compute
                    recognizes as <literal>OS_AUTH_URL</literal>:</para>
          <screen language="console">$ export OS_AUTH_URL=http://hostname:5000/v2.0</screen>
        </step>
      </procedure>
        </section>
      </section>
    </section>
    <section xml:id="logging" xml:base="manage-logs">
      <title>Logging</title>
      <section xml:id="logging-module">
        <title>Logging module</title>
        <para>Logging behavior can be changed by creating a configuration file. To specify
                the configuration file, add this line to the <literal>/etc/nova/nova.conf</literal> file:</para>
        <screen language="ini">log-config=/etc/nova/logging.conf</screen>
        <para>To change the logging level, add <literal>DEBUG</literal>, <literal>INFO</literal>, <literal>WARNING</literal>, or <literal>ERROR</literal>
                as a parameter.</para>
        <para>The logging configuration file is an INI-style configuration file, which must
                contain a section called <literal>logger_nova</literal>. This controls the behavior of the
                logging facility in the <literal>nova-*</literal> services. For example:</para>
        <screen language="ini">[logger_nova]
level = INFO
handlers = stderr
qualname = nova</screen>
        <para>This example sets the debugging level to <literal>INFO</literal> (which is less verbose than
                the default <literal>DEBUG</literal> setting).</para>
        <para>For more about the logging configuration syntax, including the <literal>handlers</literal> and
                <literal>quaname</literal> variables, see the <link xmlns:xl="http://www.w3.org/1999/xlink" xl:href="https://docs.python.org/release/2.7/library/logging.html#configuration-file-format">Python documentation</link>
                on logging configuration files.</para>
        <para>For an example of the <literal>logging.conf</literal> file with various defined handlers, see
                the <link xmlns:xl="http://www.w3.org/1999/xlink" xl:href="https://docs.openstack.org/oslo.log/latest/admin/example_nova.html">Example Configuration File for nova</link>.</para>
      </section>
      <section xml:id="syslog">
        <title>Syslog</title>
        <para>OpenStack Compute services can send logging information to syslog. This is
                useful if you want to use rsyslog to forward logs to a remote machine.
                Separately configure the Compute service (nova), the Identity service
                (keystone), the Image service (glance), and, if you are using it, the Block
                Storage service (cinder) to send log messages to syslog.  Open these
                configuration files:</para>
        <itemizedlist>
          <listitem>
            <para>
              <literal>/etc/nova/nova.conf</literal>
            </para>
          </listitem>
          <listitem>
            <para>
              <literal>/etc/keystone/keystone.conf</literal>
            </para>
          </listitem>
          <listitem>
            <para>
              <literal>/etc/glance/glance-api.conf</literal>
            </para>
          </listitem>
          <listitem>
            <para>
              <literal>/etc/glance/glance-registry.conf</literal>
            </para>
          </listitem>
          <listitem>
            <para>
              <literal>/etc/cinder/cinder.conf</literal>
            </para>
          </listitem>
        </itemizedlist>
        <para>In each configuration file, add these lines:</para>
        <screen language="ini">debug = False
use_syslog = True
syslog_log_facility = LOG_LOCAL0</screen>
        <para>In addition to enabling syslog, these settings also turn off debugging output
                from the log.</para>
        <note>
          <para>Although this example uses the same local facility for each service
                    (<literal>LOG_LOCAL0</literal>, which corresponds to syslog facility <literal>LOCAL0</literal>), we
                    recommend that you configure a separate local facility for each service, as
                    this provides better isolation and more flexibility. For example, you can
                    capture logging information at different severity levels for different
                    services. syslog allows you to define up to eight local facilities,
                    <literal>LOCAL0, LOCAL1, ..., LOCAL7</literal>. For more information, see the syslog
                    documentation.</para>
        </note>
      </section>
      <section xml:id="rsyslog">
        <title>Rsyslog</title>
        <para>rsyslog is useful for setting up a centralized log server across multiple
                machines. This section briefly describe the configuration to set up an rsyslog
                server. A full treatment of rsyslog is beyond the scope of this book. This
                section assumes rsyslog has already been installed on your hosts (it is
                installed by default on most Linux distributions).</para>
        <para>This example provides a minimal configuration for <literal>/etc/rsyslog.conf</literal> on the
                log server host, which receives the log files</para>
        <screen language="console"># provides TCP syslog reception
$ModLoad imtcp
$InputTCPServerRun 1024</screen>
        <para>Add a filter rule to <literal>/etc/rsyslog.conf</literal> which looks for a host name.  This
                example uses COMPUTE_01 as the compute host name:</para>
        <screen language="none">:hostname, isequal, "COMPUTE_01" /mnt/rsyslog/logs/compute-01.log</screen>
        <para>On each compute host, create a file named <literal>/etc/rsyslog.d/60-nova.conf</literal>, with
                the following content:</para>
        <screen language="none"># prevent debug from dnsmasq with the daemon.none parameter
*.*;auth,authpriv.none,daemon.none,local0.none -/var/log/syslog
# Specify a log level of ERROR
local0.error    @@172.20.1.43:1024</screen>
        <para>Once you have created the file, restart the <literal>rsyslog</literal> service. Error-level
                log messages on the compute hosts should now be sent to the log server.</para>
      </section>
      <section xml:id="serial-console">
        <title>Serial console</title>
        <para>The serial console provides a way to examine kernel output and other system
                messages during troubleshooting if the instance lacks network connectivity.</para>
        <para>Read-only access from server serial console is possible using the
                <literal>os-GetSerialOutput</literal> server action. Most cloud images enable this feature by
                default. For more information, see <xref linkend="compute-common-errors-and-fixes"/>.</para>
        <para>OpenStack Juno and later supports read-write access using the serial console
                using the <literal>os-GetSerialConsole</literal> server action. This feature also requires a
                websocket client to access the serial console.</para>
        <procedure>
          <step>
            <para>On a compute node, edit the <literal>/etc/nova/nova.conf</literal> file:</para>
            <para>In the <literal>[serial_console]</literal> section, enable the serial console:</para>
            <screen language="ini">[serial_console]
# ...
enabled = true</screen>
          </step>
          <step>
            <para>In the <literal>[serial_console]</literal> section, configure the serial console proxy
                        similar to graphical console proxies:</para>
            <screen language="ini">[serial_console]
# ...
base_url = ws://controller:6083/
listen = 0.0.0.0
proxyclient_address = MANAGEMENT_INTERFACE_IP_ADDRESS</screen>
            <para>The <literal>base_url</literal> option specifies the base URL that clients receive from the
                        API upon requesting a serial console. Typically, this refers to the host
                        name of the controller node.</para>
            <para>The <literal>listen</literal> option specifies the network interface nova-compute should
                        listen on for virtual console connections. Typically, 0.0.0.0 will enable
                        listening on all interfaces.</para>
            <para>The <literal>proxyclient_address</literal> option specifies which network interface the
                        proxy should connect to. Typically, this refers to the IP address of the
                        management interface.</para>
            <para>When you enable read-write serial console access, Compute will add serial
                        console information to the Libvirt XML file for the instance. For example:</para>
            <screen language="xml">&lt;console type='tcp'&gt;
  &lt;source mode='bind' host='127.0.0.1' service='10000'/&gt;
  &lt;protocol type='raw'/&gt;
  &lt;target type='serial' port='0'/&gt;
  &lt;alias name='serial0'/&gt;
&lt;/console&gt;</screen>
          </step>
        </procedure>
        <procedure>
          <step>
            <para>Use the <command>nova get-serial-proxy</command> command to retrieve the websocket
                        URL for the serial console on the instance:</para>
            <screen language="console">$ nova get-serial-proxy INSTANCE_NAME</screen>
            <informaltable>
              <tgroup cols="2">
                <colspec colname="c0" colwidth="9"/>
                <colspec colname="c1" colwidth="65"/>
                <tbody>
                  <row>
                    <entry>
                      <para>Type</para>
                    </entry>
                    <entry>
                      <para>Url</para>
                    </entry>
                  </row>
                  <row>
                    <entry>
                      <para>serial</para>
                    </entry>
                    <entry>
                      <para>ws://127.0.0.1:6083/?token=18510769-71ad-4e5a-8348-4218b5613b3d</para>
                    </entry>
                  </row>
                </tbody>
              </tgroup>
            </informaltable>
            <para>Alternatively, use the API directly:</para>
            <screen language="console">$ curl -i 'http://&lt;controller&gt;:8774/v2.1/&lt;tenant_uuid&gt;/servers/&lt;instance_uuid&gt;/action' \
  -X POST \
  -H "Accept: application/json" \
  -H "Content-Type: application/json" \
  -H "X-Auth-Project-Id: &lt;project_id&gt;" \
  -H "X-Auth-Token: &lt;auth_token&gt;" \
  -d '{"os-getSerialConsole": {"type": "serial"}}'</screen>
          </step>
          <step>
            <para>Use Python websocket with the URL to generate <literal>.send</literal>, <literal>.recv</literal>, and
                        <literal>.fileno</literal> methods for serial console access. For example:</para>
            <screen language="python">import websocket
ws = websocket.create_connection(
    'ws://127.0.0.1:6083/?token=18510769-71ad-4e5a-8348-4218b5613b3d',
    subprotocols=['binary', 'base64'])</screen>
          </step>
        </procedure>
        <para>Alternatively, use a <link xmlns:xl="http://www.w3.org/1999/xlink" xl:href="https://github.com/larsks/novaconsole/">Python websocket client</link>.</para>
        <note>
          <para>When you enable the serial console, typical instance logging using the
                    <command>nova console-log</command> command is disabled. Kernel output and other
                    system messages will not be visible unless you are actively viewing the
                    serial console.</para>
        </note>
      </section>
    </section>
    <section xml:id="secure-with-rootwrap" xml:base="root-wrap-reference">
      <title>Secure with rootwrap</title>
      <para>Rootwrap allows unprivileged users to safely run Compute actions as the root
            user. Compute previously used <command>sudo</command> for this purpose, but this was
            difficult to maintain, and did not allow advanced filters. The
            <command>rootwrap</command> command replaces <command>sudo</command> for Compute.</para>
      <para>To use rootwrap, prefix the Compute command with <command>nova-rootwrap</command>. For
            example:</para>
      <screen language="console">$ sudo nova-rootwrap /etc/nova/rootwrap.conf command</screen>
      <para>A generic <literal>sudoers</literal> entry lets the Compute user run <command>nova-rootwrap</command>
            as root. The <command>nova-rootwrap</command> code looks for filter definition
            directories in its configuration file, and loads command filters from them. It
            then checks if the command requested by Compute matches one of those filters
            and, if so, executes the command (as root). If no filter matches, it denies the
            request.</para>
      <note>
        <para>Be aware of issues with using NFS and root-owned files. The NFS share must
                be configured with the <literal>no_root_squash</literal> option enabled, in order for
                rootwrap to work correctly.</para>
      </note>
      <para>Rootwrap is fully controlled by the root user. The root user owns the sudoers
            entry which allows Compute to run a specific rootwrap executable as root, and
            only with a specific configuration file (which should also be owned by root).
            The <command>nova-rootwrap</command> command imports the Python modules it needs from a
            cleaned, system-default PYTHONPATH.  The root-owned configuration file points
            to root-owned filter definition directories, which contain root-owned filters
            definition files. This chain ensures that the Compute user itself is not in
            control of the configuration or modules used by the <command>nova-rootwrap</command>
            executable.</para>
      <section xml:id="configure-rootwrap">
        <title>Configure rootwrap</title>
        <para>Configure rootwrap in the <literal>rootwrap.conf</literal> file. Because it is in the trusted
                security path, it must be owned and writable by only the root user. The
                <literal>rootwrap_config=entry</literal> parameter specifies the file's location in the
                sudoers entry and in the <literal>nova.conf</literal> configuration file.</para>
        <para>The <literal>rootwrap.conf</literal> file uses an INI file format with these sections and
                parameters:</para>
        <table xml:id="id1">
          <title>rootwrap.conf configuration options</title>
          <tgroup cols="2">
            <colspec colname="c0" colwidth="64"/>
            <colspec colname="c1" colwidth="31"/>
            <tbody>
              <row>
                <entry>
                  <para>Configuration option=Default value</para>
                </entry>
                <entry>
                  <para>(Type) Description</para>
                </entry>
              </row>
              <row>
                <entry>
                  <para>[DEFAULT]
                                    filters_path=/etc/nova/rootwrap.d,/usr/share/nova/rootwrap</para>
                </entry>
                <entry>
                  <para>(ListOpt) Comma-separated list of directories
                                    containing filter definition files.
                                    Defines where rootwrap filters are stored.
                                    Directories defined on this line should all
                                    exist, and be owned and writable only by the
                                    root user.</para>
                </entry>
              </row>
            </tbody>
          </tgroup>
        </table>
        <para>If the root wrapper is not performing correctly, you can add a workaround
                option into the <literal>nova.conf</literal> configuration file. This workaround re-configures
                the root wrapper configuration to fall back to running commands as <literal>sudo</literal>,
                and is a Kilo release feature.</para>
        <para>Including this workaround in your configuration file safeguards your
                environment from issues that can impair root wrapper performance. Tool changes
                that have impacted <link xmlns:xl="http://www.w3.org/1999/xlink" xl:href="https://git.openstack.org/cgit/openstack-dev/pbr/">Python Build Reasonableness (PBR)</link> for example, are a known
                issue that affects root wrapper performance.</para>
        <para>To set up this workaround, configure the <literal>disable_rootwrap</literal> option in the
                <literal>[workaround]</literal> section of the <literal>nova.conf</literal> configuration file.</para>
        <para>The filters definition files contain lists of filters that rootwrap will use to
                allow or deny a specific command. They are generally suffixed by <literal>.filters</literal> .
                Since they are in the trusted security path, they need to be owned and writable
                only by the root user. Their location is specified in the <literal>rootwrap.conf</literal>
                file.</para>
        <para>Filter definition files use an INI file format with a <literal>[Filters]</literal> section and
                several lines, each with a unique parameter name, which should be different for
                each filter you define:</para>
        <table xml:id="id2">
          <title>Filters configuration options</title>
          <tgroup cols="2">
            <colspec colname="c0" colwidth="72"/>
            <colspec colname="c1" colwidth="39"/>
            <tbody>
              <row>
                <entry>
                  <para>Configuration option=Default value</para>
                </entry>
                <entry>
                  <para>(Type) Description</para>
                </entry>
              </row>
              <row>
                <entry>
                  <para>[Filters]
                                    filter_name=kpartx: CommandFilter, /sbin/kpartx, root</para>
                </entry>
                <entry>
                  <para>(ListOpt) Comma-separated list containing the filter class to
                                    use, followed by the Filter arguments (which vary depending
                                    on the Filter class selected).</para>
                </entry>
              </row>
            </tbody>
          </tgroup>
        </table>
      </section>
      <section xml:id="configure-the-rootwrap-daemon">
        <title>Configure the rootwrap daemon</title>
        <para>Administrators can use rootwrap daemon support instead of running rootwrap with
                <command>sudo</command>. The rootwrap daemon reduces the overhead and performance loss
                that results from running <literal>oslo.rootwrap</literal> with <command>sudo</command>. Each call
                that needs rootwrap privileges requires a new instance of rootwrap. The daemon
                prevents overhead from the repeated calls. The daemon does not support long
                running processes, however.</para>
        <para>To enable the rootwrap daemon, set <literal>use_rootwrap_daemon</literal> to <literal>True</literal> in the
                Compute service configuration file.</para>
      </section>
    </section>
    <section xml:id="section-configuring-compute-migrations" xml:base="configuring-migrations">
      <title>Configure live migrations</title>
      <para>Migration enables an administrator to move a virtual machine instance from one
            compute host to another. A typical scenario is planned maintenance on the
            source host, but migration can also be useful to redistribute the load when
            many VM instances are running on a specific physical machine.</para>
      <para>This document covers live migrations using the
            <link xmlns:xl="http://www.w3.org/1999/xlink" xl:href="#configuring-migrations-kvm-libvirt">KVM-libvirt</link> and
            <link xmlns:xl="http://www.w3.org/1999/xlink" xl:href="#configuring-migrations-xenserver">XenServer</link> hypervisors.</para>
      <note>
        <para>Not all Compute service hypervisor drivers support live-migration, or
                support all live-migration features.</para>
        <para>Consult the <link xmlns:xl="http://www.w3.org/1999/xlink" xl:href="https://docs.openstack.org/developer/nova/support-matrix.html">Hypervisor Support Matrix</link> to
                determine which hypervisors support live-migration.</para>
        <para>See the <link xmlns:xl="http://www.w3.org/1999/xlink" xl:href="https://docs.openstack.org/ocata/config-reference/compute/hypervisors.html">Hypervisor configuration pages</link>
                for details on hypervisor-specific configuration settings.</para>
      </note>
      <para>The migration types are:</para>
      <itemizedlist>
        <listitem>
          <para><emphasis role="bold">Non-live migration</emphasis>, also known as cold migration or simply migration.</para>
          <para>The instance is shut down, then moved to another hypervisor and restarted.
                    The instance recognizes that it was rebooted, and the application running on
                    the instance is disrupted.</para>
          <para>This section does not cover cold migration.</para>
        </listitem>
        <listitem>
          <para>
            <emphasis role="bold">Live migration</emphasis>
          </para>
          <para>The instance keeps running throughout the migration.  This is useful when it
                    is not possible or desirable to stop the application running on the instance.</para>
          <para>Live migrations can be classified further by the way they treat instance
                    storage:</para>
          <itemizedlist>
            <listitem>
              <para><emphasis role="bold">Shared storage-based live migration</emphasis>. The instance has ephemeral disks
                            that are located on storage shared between the source and destination
                            hosts.</para>
            </listitem>
            <listitem>
              <para><emphasis role="bold">Block live migration</emphasis>, or simply block migration.  The instance has
                            ephemeral disks that are not shared between the source and destination
                            hosts.  Block migration is incompatible with read-only devices such as
                            CD-ROMs and Configuration Drive (config_drive).</para>
            </listitem>
            <listitem>
              <para><emphasis role="bold">Volume-backed live migration</emphasis>. Instances use volumes rather than
                            ephemeral disks.</para>
            </listitem>
          </itemizedlist>
          <para>Block live migration requires copying disks from the source to the
                    destination host. It takes more time and puts more load on the network.
                    Shared-storage and volume-backed live migration does not copy disks.</para>
        </listitem>
      </itemizedlist>
      <note>
        <para>In a multi-cell cloud, instances can be live migrated to a
                different host in the same cell, but not across cells.</para>
      </note>
      <para>The following sections describe how to configure your hosts for live migrations
            using the KVM and XenServer hypervisors.</para>
      <section xml:id="configuring-migrations-kvm-libvirt">
        <title>KVM-libvirt</title>
        <section xml:id="configuring-migrations-kvm-general">
          <title>General configuration</title>
          <para>To enable any type of live migration, configure the compute hosts according to
                    the instructions below:</para>
          <procedure>
            <step>
              <para>Set the following parameters in <literal>nova.conf</literal> on all compute hosts:</para>
              <itemizedlist>
                <listitem>
                  <para>
                    <literal>vncserver_listen=0.0.0.0</literal>
                  </para>
                  <para>You must not make the VNC server listen to the IP address of its compute
                                    host, since that addresses changes when the instance is migrated.</para>
                  <important>
                    <para>Since this setting allows VNC clients from any IP address to connect to
                                        instance consoles, you must take additional measures like secure
                                        networks or firewalls to prevent potential attackers from gaining
                                        access to instances.</para>
                  </important>
                </listitem>
                <listitem>
                  <para><literal>instances_path</literal> must have the same value for all compute hosts. In
                                    this guide, the value <literal>/var/lib/nova/instances</literal> is assumed.</para>
                </listitem>
              </itemizedlist>
            </step>
            <step>
              <para>Ensure that name resolution on all compute hosts is identical, so that they
                            can connect each other through their hostnames.</para>
              <para>If you use <literal>/etc/hosts</literal> for name resolution and enable SELinux, ensure
                            that <literal>/etc/hosts</literal> has the correct SELinux context:</para>
              <screen language="console"># restorecon /etc/hosts</screen>
            </step>
            <step>
              <para>Enable password-less SSH so that root on one compute host can log on to any
                            other compute host without providing a password.  The <literal>libvirtd</literal> daemon,
                            which runs as root, uses the SSH protocol to copy the instance to the
                            destination and can't know the passwords of all compute hosts.</para>
              <para>You may, for example, compile root's public SSH keys on all compute hosts
                            into an <literal>authorized_keys</literal> file and deploy that file to the compute hosts.</para>
            </step>
            <step>
              <para>Configure the firewalls to allow libvirt to communicate between compute
                            hosts.</para>
              <para>By default, libvirt uses the TCP port range from 49152 to 49261 for copying
                            memory and disk contents. Compute hosts must accept connections in this
                            range.</para>
              <para>For information about ports used by libvirt, see the <link xmlns:xl="http://www.w3.org/1999/xlink" xl:href="http://libvirt.org/remote.html#Remote_libvirtd_configuration">libvirt documentation</link>.</para>
              <important>
                <para>Be mindful of the security risks introduced by opening ports.</para>
              </important>
            </step>
          </procedure>
        </section>
        <section xml:id="configuring-migrations-kvm-block-and-volume-migration">
          <title>Block migration, volume-based live migration</title>
          <para>No additional configuration is required for block migration and volume-backed
                    live migration.</para>
          <para>Be aware that block migration adds load to the network and storage subsystems.</para>
        </section>
        <section xml:id="configuring-migrations-kvm-shared-storage">
          <title>Shared storage</title>
          <para>Compute hosts have many options for sharing storage, for example NFS, shared
                    disk array LUNs, Ceph or GlusterFS.</para>
          <para>The next steps show how a regular Linux system might be configured as an NFS v4
                    server for live migration.  For detailed information and alternative ways to
                    configure NFS on Linux, see instructions for <link xmlns:xl="http://www.w3.org/1999/xlink" xl:href="https://help.ubuntu.com/community/SettingUpNFSHowTo">Ubuntu</link>, <link xmlns:xl="http://www.w3.org/1999/xlink" xl:href="https://access.redhat.com/documentation/en-US/Red_Hat_Enterprise_Linux/7/html/Storage_Administration_Guide/nfs-serverconfig.html">RHEL and derivatives</link>
                    or <link xmlns:xl="http://www.w3.org/1999/xlink" xl:href="https://www.suse.com/documentation/sles-12/book_sle_admin/data/sec_nfs_configuring-nfs-server.html">SLES and OpenSUSE</link>.</para>
          <procedure>
            <step>
              <para>Ensure that UID and GID of the nova user are identical on the compute hosts
                            and the NFS server.</para>
            </step>
            <step>
              <para>Create a directory with enough disk space for all instances in the cloud,
                            owned by user nova. In this guide, we assume <literal>/var/lib/nova/instances</literal>.</para>
            </step>
            <step>
              <para>Set the execute/search bit on the <literal>instances</literal> directory:</para>
              <screen language="console">$ chmod o+x /var/lib/nova/instances</screen>
              <para>This  allows qemu to access the <literal>instances</literal> directory tree.</para>
            </step>
            <step>
              <para>Export <literal>/var/lib/nova/instances</literal> to the compute hosts. For example, add
                            the following line to <literal>/etc/exports</literal>:</para>
              <screen language="ini">/var/lib/nova/instances *(rw,sync,fsid=0,no_root_squash)</screen>
              <para>The asterisk permits access to any NFS client. The option <literal>fsid=0</literal> exports
                            the instances directory as the NFS root.</para>
            </step>
          </procedure>
          <para>After setting up the NFS server, mount the remote filesystem on all compute
                    hosts.</para>
          <procedure>
            <step>
              <para>Assuming the NFS server's hostname is <literal>nfs-server</literal>, add this line to
                            <literal>/etc/fstab</literal> to mount the NFS root:</para>
              <screen language="console">nfs-server:/ /var/lib/nova/instances nfs4 defaults 0 0</screen>
            </step>
            <step>
              <para>Test NFS by mounting the instances directory and check access permissions
                            for the nova user:</para>
              <screen language="console">$ sudo mount -a -v
$ ls -ld /var/lib/nova/instances/
drwxr-xr-x. 2 nova nova 6 Mar 14 21:30 /var/lib/nova/instances/</screen>
            </step>
          </procedure>
        </section>
        <section xml:id="configuring-migrations-kvm-advanced">
          <title>Advanced configuration for KVM and QEMU</title>
          <para>Live migration copies the instance's memory from the source to the destination
                    compute host. After a memory page has been copied, the instance may write to it
                    again, so that it has to be copied again.  Instances that frequently write to
                    different memory pages can overwhelm the memory copy process and prevent the
                    live migration from completing.</para>
          <para>This section covers configuration settings that can help live migration of
                    memory-intensive instances succeed.</para>
          <procedure>
            <step>
              <para>
                <emphasis role="bold">Live migration completion timeout</emphasis>
              </para>
              <para>The Compute service aborts a migration when it has been running for too
                            long.  The timeout is calculated based on the instance size, which is the
                            instance's memory size in GiB. In the case of block migration, the size of
                            ephemeral storage in GiB is added.</para>
              <para>The timeout in seconds is the instance size multiplied by the configurable
                            parameter <literal>live_migration_completion_timeout</literal>, whose default is 800. For
                            example, shared-storage live migration of an instance with 8GiB memory will
                            time out after 6400 seconds.</para>
            </step>
            <step>
              <para>
                <emphasis role="bold">Live migration progress timeout</emphasis>
              </para>
              <para>The Compute service also aborts a live migration when it detects that memory
                            copy is not making progress for a certain time. You can set this time, in
                            seconds, through the configurable parameter
                            <literal>live_migration_progress_timeout</literal>.</para>
              <para>In Ocata, the default value of <literal>live_migration_progress_timeout</literal> is 0,
                            which disables progress timeouts. You should not change this value, since
                            the algorithm that detects memory copy progress has been determined to be
                            unreliable. It may be re-enabled in future releases.</para>
            </step>
            <step>
              <para>
                <emphasis role="bold">Instance downtime</emphasis>
              </para>
              <para>Near the end of the memory copy, the instance is paused for a short time so
                            that the remaining few pages can be copied without interference from
                            instance memory writes. The Compute service initializes this time to a small
                            value that depends on the instance size, typically around 50 milliseconds.
                            When it notices that the memory copy does not make sufficient progress, it
                            increases the time gradually.</para>
              <para>You can influence the instance downtime algorithm with the help of three
                            configuration variables on the compute hosts:</para>
              <screen language="ini">live_migration_downtime = 500
live_migration_downtime_steps = 10
live_migration_downtime_delay = 75</screen>
              <para><literal>live_migration_downtime</literal> sets the maximum permitted downtime for a live
                            migration, in <emphasis>milliseconds</emphasis>.  The default is 500.</para>
              <para><literal>live_migration_downtime_steps</literal> sets the total number of adjustment steps
                            until <literal>live_migration_downtime</literal> is reached.  The default is 10 steps.</para>
              <para><literal>live_migration_downtime_delay</literal> sets the time interval between two
                            adjustment steps in <emphasis>seconds</emphasis>. The default is 75.</para>
            </step>
            <step>
              <para>
                <emphasis role="bold">Auto-convergence</emphasis>
              </para>
              <para>One strategy for a successful live migration of a memory-intensive instance
                            is slowing the instance down. This is called auto-convergence.  Both libvirt
                            and QEMU implement this feature by automatically throttling the instance's
                            CPU when memory copy delays are detected.</para>
              <para>Auto-convergence is disabled by default.  You can enable it by setting
                            <literal>live_migration_permit_auto_converge=true</literal>.</para>
              <important>
                <para>Before enabling auto-convergence, make sure that the instance's
                                application tolerates a slow-down.</para>
                <para>Be aware that auto-convergence does not guarantee live migration success.</para>
              </important>
            </step>
            <step>
              <para>
                <emphasis role="bold">Post-copy</emphasis>
              </para>
              <para>Live migration of a memory-intensive instance is certain to succeed when you
                            enable post-copy. This feature, implemented by libvirt and QEMU, activates
                            the virtual machine on the destination host before all of its memory has
                            been copied.  When the virtual machine accesses a page that is missing on
                            the destination host, the resulting page fault is resolved by copying the
                            page from the source host.</para>
              <para>Post-copy is disabled by default. You can enable it by setting
                            <literal>live_migration_permit_post_copy=true</literal>.</para>
              <para>When you enable both auto-convergence and post-copy, auto-convergence
                            remains disabled.</para>
              <important>
                <para>The page faults introduced by post-copy can slow the instance down.</para>
                <para>When the network connection between source and destination host is
                                interrupted, page faults cannot be resolved anymore and the instance is
                                rebooted.</para>
              </important>
            </step>
          </procedure>
          <para>The full list of live migration configuration parameters is documented in the
                    <xref linkend="../configuration/config"/></para>
        </section>
      </section>
      <section xml:id="configuring-migrations-xenserver">
        <title>XenServer</title>
        <section xml:id="configuring-migrations-xenserver-shared-storage">
          <title>Shared storage</title>
          <para>
            <emphasis role="bold">Prerequisites</emphasis>
          </para>
          <itemizedlist>
            <listitem>
              <para><emphasis role="bold">Compatible XenServer hypervisors</emphasis>.</para>
              <para>For more information, see the <link xmlns:xl="http://www.w3.org/1999/xlink" xl:href="http://docs.vmd.citrix.com/XenServer/6.0.0/1.0/en_gb/reference.html#pooling_homogeneity_requirements">Requirements for Creating Resource Pools</link>
                            section of the XenServer Administrator's Guide.</para>
            </listitem>
            <listitem>
              <para><emphasis role="bold">Shared storage</emphasis>.</para>
              <para>An NFS export, visible to all XenServer hosts.</para>
              <note>
                <para>For the supported NFS versions, see the <link xmlns:xl="http://www.w3.org/1999/xlink" xl:href="http://docs.vmd.citrix.com/XenServer/6.0.0/1.0/en_gb/reference.html#id1002701">NFS VHD</link>
                                    section of the XenServer Administrator's Guide.</para>
              </note>
            </listitem>
          </itemizedlist>
          <para>To use shared storage live migration with XenServer hypervisors, the hosts must
                    be joined to a XenServer pool. To create that pool, a host aggregate must be
                    created with specific metadata. This metadata is used by the XAPI plug-ins to
                    establish the pool.</para>
          <procedure>
            <step>
              <para>Add an NFS VHD storage to your master XenServer, and set it as the default
                            storage repository. For more information, see NFS VHD in the XenServer
                            Administrator's Guide.</para>
            </step>
            <step>
              <para>Configure all compute nodes to use the default storage repository (<literal>sr</literal>)
                            for pool operations. Add this line to your <literal>nova.conf</literal> configuration files
                            on all compute nodes:</para>
              <screen language="ini">sr_matching_filter=default-sr:true</screen>
            </step>
            <step>
              <para>Create a host aggregate. This command creates the aggregate, and then
                            displays a table that contains the ID of the new aggregate</para>
              <screen language="console">$ openstack aggregate create --zone AVAILABILITY_ZONE POOL_NAME</screen>
              <para>Add metadata to the aggregate, to mark it as a hypervisor pool</para>
              <screen language="console">$ openstack aggregate set --property hypervisor_pool=true AGGREGATE_ID

$ openstack aggregate set --property operational_state=created AGGREGATE_ID</screen>
              <para>Make the first compute node part of that aggregate</para>
              <screen language="console">$ openstack aggregate add host AGGREGATE_ID MASTER_COMPUTE_NAME</screen>
              <para>The host is now part of a XenServer pool.</para>
            </step>
            <step>
              <para>Add hosts to the pool</para>
              <screen language="console">$ openstack aggregate add host AGGREGATE_ID COMPUTE_HOST_NAME</screen>
              <note>
                <para>The added compute node and the host will shut down to join the host to
                                the XenServer pool. The operation will fail if any server other than the
                                compute node is running or suspended on the host.</para>
              </note>
            </step>
          </procedure>
        </section>
        <section xml:id="configuring-migrations-xenserver-block-migration">
          <title>Block migration</title>
          <itemizedlist>
            <listitem>
              <para><emphasis role="bold">Compatible XenServer hypervisors</emphasis>.</para>
              <para>The hypervisors must support the Storage XenMotion feature.  See your
                            XenServer manual to make sure your edition has this feature.</para>
              <note>
                <itemizedlist>
                  <listitem>
                    <para>To use block migration, you must use the <literal>--block-migrate</literal> parameter
                                            with the live migration command.</para>
                  </listitem>
                  <listitem>
                    <para>Block migration works only with EXT local storage storage repositories,
                                            and the server must not have any volumes attached.</para>
                  </listitem>
                </itemizedlist>
              </note>
            </listitem>
          </itemizedlist>
        </section>
      </section>
    </section>
    <section xml:id="live-migrate-instances" xml:base="live-migration-usage">
      <title>Live-migrate instances</title>
      <para>Live-migrating an instance means moving its virtual machine to a different
            OpenStack Compute server while the instance continues running.  Before starting
            a live-migration, review the chapter
            <xref linkend="section-configuring-compute-migrations"/>. It covers the configuration
            settings required to enable live-migration, but also reasons for migrations and
            non-live-migration options.</para>
      <para>The instructions below cover shared-storage and volume-backed migration.  To
            block-migrate instances, add the command-line option
            <literal>-block-migrate</literal> to the <command>nova live-migration</command> command,
            and <literal>--block-migration</literal> to the <command>openstack server migrate</command>
            command.</para>
      <section xml:id="section-manual-selection-of-dest">
        <title>Manual selection of the destination host</title>
        <procedure>
          <step>
            <para>Obtain the ID of the instance you want to migrate:</para>
            <screen language="console">$ openstack server list

+--------------------------------------+------+--------+-----------------+------------+
| ID                                   | Name | Status | Networks        | Image Name |
+--------------------------------------+------+--------+-----------------+------------+
| d1df1b5a-70c4-4fed-98b7-423362f2c47c | vm1  | ACTIVE | private=a.b.c.d | ...        |
| d693db9e-a7cf-45ef-a7c9-b3ecb5f22645 | vm2  | ACTIVE | private=e.f.g.h | ...        |
+--------------------------------------+------+--------+-----------------+------------+</screen>
          </step>
          <step>
            <para>Determine on which host the instance is currently running. In this example,
                        <literal>vm1</literal> is running on <literal>HostB</literal>:</para>
            <screen language="console">$ openstack server show d1df1b5a-70c4-4fed-98b7-423362f2c47c

+----------------------+--------------------------------------+
| Field                | Value                                |
+----------------------+--------------------------------------+
| ...                  | ...                                  |
| OS-EXT-SRV-ATTR:host | HostB                                |
| ...                  | ...                                  |
| addresses            | a.b.c.d                              |
| flavor               | m1.tiny                              |
| id                   | d1df1b5a-70c4-4fed-98b7-423362f2c47c |
| name                 | vm1                                  |
| status               | ACTIVE                               |
| ...                  | ...                                  |
+----------------------+--------------------------------------+</screen>
          </step>
          <step>
            <para>Select the compute node the instance will be migrated to. In this example,
                        we will migrate the instance to <literal>HostC</literal>, because <literal>nova-compute</literal> is
                        running on it:</para>
            <screen language="console">$ openstack compute service list

+----+------------------+-------+----------+---------+-------+----------------------------+
| ID | Binary           | Host  | Zone     | Status  | State | Updated At                 |
+----+------------------+-------+----------+---------+-------+----------------------------+
|  3 | nova-conductor   | HostA | internal | enabled | up    | 2017-02-18T09:42:29.000000 |
|  4 | nova-scheduler   | HostA | internal | enabled | up    | 2017-02-18T09:42:26.000000 |
|  5 | nova-consoleauth | HostA | internal | enabled | up    | 2017-02-18T09:42:29.000000 |
|  6 | nova-compute     | HostB | nova     | enabled | up    | 2017-02-18T09:42:29.000000 |
|  7 | nova-compute     | HostC | nova     | enabled | up    | 2017-02-18T09:42:29.000000 |
+----+------------------+-------+----------+---------+-------+----------------------------+</screen>
          </step>
          <step>
            <para>Check that <literal>HostC</literal> has enough resources for migration:</para>
            <screen language="console">$ openstack host show HostC

+-------+------------+-----+-----------+---------+
| Host  | Project    | CPU | Memory MB | Disk GB |
+-------+------------+-----+-----------+---------+
| HostC | (total)    |  16 |     32232 |     878 |
| HostC | (used_now) |  22 |     21284 |     422 |
| HostC | (used_max) |  22 |     21284 |     422 |
| HostC | p1         |  22 |     21284 |     422 |
| HostC | p2         |  22 |     21284 |     422 |
+-------+------------+-----+-----------+---------+</screen>
            <itemizedlist>
              <listitem>
                <para><literal>cpu</literal>: Number of CPUs</para>
              </listitem>
              <listitem>
                <para><literal>memory_mb</literal>: Total amount of memory, in MB</para>
              </listitem>
              <listitem>
                <para><literal>disk_gb</literal>: Total amount of space for NOVA-INST-DIR/instances, in GB</para>
              </listitem>
            </itemizedlist>
            <para>In this table, the first row shows the total amount of resources available
                        on the physical server. The second line shows the currently used resources.
                        The third line shows the maximum used resources. The fourth line and below
                        shows the resources available for each project.</para>
          </step>
          <step>
            <para>Migrate the instance:</para>
            <screen language="console">$ openstack server migrate d1df1b5a-70c4-4fed-98b7-423362f2c47c --live HostC</screen>
          </step>
          <step>
            <para>Confirm that the instance has been migrated successfully:</para>
            <screen language="console">$ openstack server show d1df1b5a-70c4-4fed-98b7-423362f2c47c

+----------------------+--------------------------------------+
| Field                | Value                                |
+----------------------+--------------------------------------+
| ...                  | ...                                  |
| OS-EXT-SRV-ATTR:host | HostC                                |
| ...                  | ...                                  |
+----------------------+--------------------------------------+</screen>
            <para>If the instance is still running on <literal>HostB</literal>, the migration failed. The
                        <literal>nova-scheduler</literal> and <literal>nova-conductor</literal> log files on the controller and
                        the <literal>nova-compute</literal> log file on the source compute host can help pin-point
                        the problem.</para>
          </step>
        </procedure>
      </section>
      <section xml:id="auto-selection-of-dest">
        <title>Automatic selection of the destination host</title>
        <para>To leave the selection of the destination host to the Compute service, use the
                nova command-line client.</para>
        <procedure>
          <step>
            <para>Obtain the instance ID as shown in step 1 of the section
                        <link xmlns:xl="http://www.w3.org/1999/xlink" xl:href="#section-manual-selection-of-dest">Manual selection of the destination host</link>.</para>
          </step>
          <step>
            <para>Leave out the host selection steps 2, 3, and 4.</para>
          </step>
          <step>
            <para>Migrate the instance:</para>
            <screen language="console">$ nova live-migration d1df1b5a-70c4-4fed-98b7-423362f2c47c</screen>
          </step>
        </procedure>
      </section>
      <section xml:id="monitoring-the-migration">
        <title>Monitoring the migration</title>
        <procedure>
          <step>
            <para>Confirm that the instance is migrating:</para>
            <screen language="console">$ openstack server show d1df1b5a-70c4-4fed-98b7-423362f2c47c

+----------------------+--------------------------------------+
| Field                | Value                                |
+----------------------+--------------------------------------+
| ...                  | ...                                  |
| status               | MIGRATING                            |
| ...                  | ...                                  |
+----------------------+--------------------------------------+</screen>
          </step>
          <step>
            <para>Check progress</para>
            <para>Use the nova command-line client for nova's migration monitoring feature.
                        First, obtain the migration ID:</para>
            <screen language="console">$ nova server-migration-list d1df1b5a-70c4-4fed-98b7-423362f2c47c
+----+-------------+-----------  (...)
| Id | Source Node | Dest Node | (...)
+----+-------------+-----------+ (...)
| 2  | -           | -         | (...)
+----+-------------+-----------+ (...)</screen>
            <para>For readability, most output columns were removed. Only the first column,
                        <emphasis role="bold">Id</emphasis>, is relevant.  In this example, the migration ID is 2. Use this to
                        get the migration status.</para>
            <screen language="console">$ nova server-migration-show d1df1b5a-70c4-4fed-98b7-423362f2c47c 2
+------------------------+--------------------------------------+
| Property               | Value                                |
+------------------------+--------------------------------------+
| created_at             | 2017-03-08T02:53:06.000000           |
| dest_compute           | controller                           |
| dest_host              | -                                    |
| dest_node              | -                                    |
| disk_processed_bytes   | 0                                    |
| disk_remaining_bytes   | 0                                    |
| disk_total_bytes       | 0                                    |
| id                     | 2                                    |
| memory_processed_bytes | 65502513                             |
| memory_remaining_bytes | 786427904                            |
| memory_total_bytes     | 1091379200                           |
| server_uuid            | d1df1b5a-70c4-4fed-98b7-423362f2c47c |
| source_compute         | compute2                             |
| source_node            | -                                    |
| status                 | running                              |
| updated_at             | 2017-03-08T02:53:47.000000           |
+------------------------+--------------------------------------+</screen>
            <para>The output shows that the migration is running. Progress is measured by the
                        number of memory bytes that remain to be copied. If this number is not
                        decreasing over time, the migration may be unable to complete, and it may be
                        aborted by the Compute service.</para>
            <note>
              <para>The command reports that no disk bytes are processed, even in the event
                            of block migration.</para>
            </note>
          </step>
        </procedure>
      </section>
      <section xml:id="what-to-do-when-the-migration-times-out">
        <title>What to do when the migration times out</title>
        <para>During the migration process, the instance may write to a memory page after
                that page has been copied to the destination. When that happens, the same page
                has to be copied again. The instance may write to memory pages faster than they
                can be copied, so that the migration cannot complete.  The Compute service will
                cancel it when the <literal>live_migration_completion_timeout</literal>, a configuration
                parameter, is reached.</para>
        <para>The following remarks assume the KVM/Libvirt hypervisor.</para>
        <section xml:id="how-to-know-that-the-migration-timed-out">
          <title>How to know that the migration timed out</title>
          <para>To determine that the migration timed out, inspect the <literal>nova-compute</literal> log
                    file on the source host. The following log entry shows that the migration timed
                    out:</para>
          <screen language="console"># grep WARNING.*d1df1b5a-70c4-4fed-98b7-423362f2c47c /var/log/nova/nova-compute.log
...
WARNING nova.virt.libvirt.migration [req-...] [instance: ...]
live migration not completed after 1800 sec</screen>
          <para>The Compute service also cancels migrations when the memory copy seems to make
                    no progress. Ocata disables this feature by default, but it can be enabled
                    using the configuration parameter <literal>live_migration_progress_timeout</literal>. Should
                    this be the case, you may find the following message in the log:</para>
          <screen language="console">WARNING nova.virt.libvirt.migration [req-...] [instance: ...]
live migration stuck for 150 sec</screen>
        </section>
        <section xml:id="addressing-migration-timeouts">
          <title>Addressing migration timeouts</title>
          <para>To stop the migration from putting load on infrastructure resources like
                    network and disks, you may opt to cancel it manually.</para>
          <screen language="console">$ nova live-migration-abort INSTANCE_ID MIGRATION_ID</screen>
          <para>To make live-migration succeed, you have several options:</para>
          <itemizedlist>
            <listitem>
              <para>
                <emphasis role="bold">Manually force-complete the migration</emphasis>
              </para>
              <screen language="console">$ nova live-migration-force-complete INSTANCE_ID MIGRATION_ID</screen>
              <para>The instance is paused until memory copy completes.</para>
              <important>
                <para>Since the pause impacts time keeping on the instance and not all
                                applications tolerate incorrect time settings, use this approach with
                                caution.</para>
              </important>
            </listitem>
            <listitem>
              <para>
                <emphasis role="bold">Enable auto-convergence</emphasis>
              </para>
              <para>Auto-convergence is a Libvirt feature. Libvirt detects that the migration is
                            unlikely to complete and slows down its CPU until the memory copy process is
                            faster than the instance's memory writes.</para>
              <para>To enable auto-convergence, set
                            <literal>live_migration_permit_auto_converge=true</literal> in <literal>nova.conf</literal> and restart
                            <literal>nova-compute</literal>. Do this on all compute hosts.</para>
              <important>
                <para>One possible downside of auto-convergence is the slowing down of the
                                instance.</para>
              </important>
            </listitem>
            <listitem>
              <para>
                <emphasis role="bold">Enable post-copy</emphasis>
              </para>
              <para>This is a Libvirt feature. Libvirt detects that the migration does not
                            progress and responds by activating the virtual machine on the destination
                            host before all its memory has been copied. Access to missing memory pages
                            result in page faults that are satisfied from the source host.</para>
              <para>To enable post-copy, set <literal>live_migration_permit_post_copy=true</literal> in
                            <literal>nova.conf</literal> and restart <literal>nova-compute</literal>. Do this on all compute hosts.</para>
              <para>When post-copy is enabled, manual force-completion does not pause the
                            instance but switches to the post-copy process.</para>
              <important>
                <para>Possible downsides:</para>
                <itemizedlist>
                  <listitem>
                    <para>When the network connection between source and destination is
                                        interrupted, page faults cannot be resolved anymore, and the virtual
                                        machine is rebooted.</para>
                  </listitem>
                  <listitem>
                    <para>Post-copy may lead to an increased page fault rate during migration,
                                        which can slow the instance down.</para>
                  </listitem>
                </itemizedlist>
              </important>
            </listitem>
          </itemizedlist>
        </section>
      </section>
    </section>
    <section xml:id="configure-remote-console-access" xml:base="remote-console-access">
      <title>Configure remote console access</title>
      <para>To provide a remote console or remote desktop access to guest virtual machines,
            use VNC or SPICE HTML5 through either the OpenStack dashboard or the command
            line. Best practice is to select one or the other to run.</para>
      <section xml:id="about-nova-consoleauth">
        <title>About nova-consoleauth</title>
        <para>Both client proxies leverage a shared service to manage token authentication
                called <literal>nova-consoleauth</literal>. This service must be running for either proxy to
                work. Many proxies of either type can be run against a single
                <literal>nova-consoleauth</literal> service in a cluster configuration.</para>
        <para>Do not confuse the <literal>nova-consoleauth</literal> shared service with <literal>nova-console</literal>,
                which is a XenAPI-specific service that most recent VNC proxy architectures do
                not use.</para>
      </section>
      <section xml:id="spice-console">
        <title>SPICE console</title>
        <para>OpenStack Compute supports VNC consoles to guests. The VNC protocol is fairly
                limited, lacking support for multiple monitors, bi-directional audio, reliable
                cut-and-paste, video streaming and more. SPICE is a new protocol that aims to
                address the limitations in VNC and provide good remote desktop support.</para>
        <para>SPICE support in OpenStack Compute shares a similar architecture to the VNC
                implementation. The OpenStack dashboard uses a SPICE-HTML5 widget in its
                console tab that communicates to the <literal>nova-spicehtml5proxy</literal> service by using
                SPICE-over-websockets. The <literal>nova-spicehtml5proxy</literal> service communicates
                directly with the hypervisor process by using SPICE.</para>
        <para>VNC must be explicitly disabled to get access to the SPICE console. Set the
                <literal>vnc_enabled</literal> option to <literal>False</literal> in the <literal>[DEFAULT]</literal> section to disable the
                VNC console.</para>
        <para>Use the following options to configure SPICE as the console for OpenStack
                Compute:</para>
        <screen language="console">[spice]
agent_enabled = False
enabled = True
html5proxy_base_url = http://IP_ADDRESS:6082/spice_auto.html
html5proxy_host = 0.0.0.0
html5proxy_port = 6082
keymap = en-us
server_listen = 127.0.0.1
server_proxyclient_address = 127.0.0.1</screen>
        <para>Replace <literal>IP_ADDRESS</literal> with the management interface IP address of the
                controller or the VIP.</para>
      </section>
      <section xml:id="vnc-console-proxy">
        <title>VNC console proxy</title>
        <para>The VNC proxy is an OpenStack component that enables compute service users to
                access their instances through VNC clients.</para>
        <note>
          <para>The web proxy console URLs do not support the websocket protocol scheme
                    (ws://) on python versions less than 2.7.4.</para>
        </note>
        <para>The VNC console connection works as follows:</para>
        <procedure>
          <step>
            <para>A user connects to the API and gets an <literal>access_url</literal> such as,
                        <literal>http://ip:port/?token=xyz</literal>.</para>
          </step>
          <step>
            <para>The user pastes the URL in a browser or uses it as a client
                        parameter.</para>
          </step>
          <step>
            <para>The browser or client connects to the proxy.</para>
          </step>
          <step>
            <para>The proxy talks to <literal>nova-consoleauth</literal> to authorize the token for the user,
                        and maps the token to the <emphasis>private</emphasis> host and port of the VNC server for an
                        instance.</para>
            <para>The compute host specifies the address that the proxy should use to connect
                        through the <literal>nova.conf</literal> file option, <literal>vncserver_proxyclient_address</literal>. In
                        this way, the VNC proxy works as a bridge between the public network and
                        private host network.</para>
          </step>
          <step>
            <para>The proxy initiates the connection to VNC server and continues to proxy
                        until the session ends.</para>
          </step>
        </procedure>
        <para>The proxy also tunnels the VNC protocol over WebSockets so that the <literal>noVNC</literal>
                client can talk to VNC servers. In general, the VNC proxy:</para>
        <itemizedlist>
          <listitem>
            <para>Bridges between the public network where the clients live and the private
                        network where VNC servers live.</para>
          </listitem>
          <listitem>
            <para>Mediates token authentication.</para>
          </listitem>
          <listitem>
            <para>Transparently deals with hypervisor-specific connection details to provide a
                        uniform client experience.</para>
          </listitem>
        </itemizedlist>
        <mediaobject>
          <imageobject role="fo">
            <imagedata fileref="SCH_5009_V00_NUAC-VNC_OpenStack.png" width="95%"/>
          </imageobject>
          <imageobject role="html">
            <imagedata fileref="SCH_5009_V00_NUAC-VNC_OpenStack.png" width="95%"/>
          </imageobject>
        </mediaobject>
        <section xml:id="vnc-configuration-options">
          <title>VNC configuration options</title>
          <para>To customize the VNC console, use the following configuration options in your
                    <literal>nova.conf</literal> file:</para>
          <note>
            <para>To support <xref linkend="section-configuring-compute-migrations"/>,
                        you cannot specify a specific IP address for <literal>vncserver_listen</literal>, because
                        that IP address does not exist on the destination host.</para>
          </note>
          <table xml:id="id2">
            <title>Description of VNC configuration options</title>
            <tgroup cols="2">
              <colspec colname="c0" colwidth="25"/>
              <colspec colname="c1" colwidth="25"/>
              <thead>
                <row>
                  <entry>
                    <para>Configuration option = Default value</para>
                  </entry>
                  <entry>
                    <para>Description</para>
                  </entry>
                </row>
              </thead>
              <tbody>
                <row>
                  <entry>
                    <para>
                      <emphasis role="bold">[DEFAULT]</emphasis>
                    </para>
                  </entry>
                  <entry/>
                </row>
                <row>
                  <entry>
                    <para>
                      <literal>daemon = False</literal>
                    </para>
                  </entry>
                  <entry>
                    <para>(BoolOpt) Become a daemon (background process)</para>
                  </entry>
                </row>
                <row>
                  <entry>
                    <para>
                      <literal>key = None</literal>
                    </para>
                  </entry>
                  <entry>
                    <para>(StrOpt) SSL key file (if separate from cert)</para>
                  </entry>
                </row>
                <row>
                  <entry>
                    <para>
                      <literal>novncproxy_host = 0.0.0.0</literal>
                    </para>
                  </entry>
                  <entry>
                    <para>(StrOpt) Host on which to listen for incoming requests</para>
                  </entry>
                </row>
                <row>
                  <entry>
                    <para>
                      <literal>novncproxy_port = 6080</literal>
                    </para>
                  </entry>
                  <entry>
                    <para>(IntOpt) Port on which to listen for incoming requests</para>
                  </entry>
                </row>
                <row>
                  <entry>
                    <para>
                      <literal>record = False</literal>
                    </para>
                  </entry>
                  <entry>
                    <para>(BoolOpt) Record sessions to FILE.[session_number]</para>
                  </entry>
                </row>
                <row>
                  <entry>
                    <para>
                      <literal>source_is_ipv6 = False</literal>
                    </para>
                  </entry>
                  <entry>
                    <para>(BoolOpt) Source is ipv6</para>
                  </entry>
                </row>
                <row>
                  <entry>
                    <para>
                      <literal>ssl_only = False</literal>
                    </para>
                  </entry>
                  <entry>
                    <para>(BoolOpt) Disallow non-encrypted connections</para>
                  </entry>
                </row>
                <row>
                  <entry>
                    <para>
                      <literal>web = /usr/share/spice-html5</literal>
                    </para>
                  </entry>
                  <entry>
                    <para>(StrOpt) Run webserver on same port. Serve files from DIR.</para>
                  </entry>
                </row>
                <row>
                  <entry>
                    <para>
                      <emphasis role="bold">[vmware]</emphasis>
                    </para>
                  </entry>
                  <entry/>
                </row>
                <row>
                  <entry>
                    <para>
                      <literal>vnc_port = 5900</literal>
                    </para>
                  </entry>
                  <entry>
                    <para>(IntOpt) VNC starting port</para>
                  </entry>
                </row>
                <row>
                  <entry>
                    <para>
                      <literal>vnc_port_total = 10000</literal>
                    </para>
                  </entry>
                  <entry>
                    <para>vnc_port_total = 10000</para>
                  </entry>
                </row>
                <row>
                  <entry>
                    <para>
                      <emphasis role="bold">[vnc]</emphasis>
                    </para>
                  </entry>
                  <entry/>
                </row>
                <row>
                  <entry>
                    <para>enabled = True</para>
                  </entry>
                  <entry>
                    <para>(BoolOpt) Enable VNC related features</para>
                  </entry>
                </row>
                <row>
                  <entry>
                    <para>novncproxy_base_url = <link xmlns:xl="http://www.w3.org/1999/xlink" xl:href="http://127.0.0.1:6080/vnc_auto.html"/></para>
                  </entry>
                  <entry>
                    <para>(StrOpt) Location of VNC console proxy, in the form
                                        "<link xmlns:xl="http://www.w3.org/1999/xlink" xl:href="http://127.0.0.1:6080/vnc_auto.html"/>"</para>
                  </entry>
                </row>
                <row>
                  <entry>
                    <para>vncserver_listen = 127.0.0.1</para>
                  </entry>
                  <entry>
                    <para>(StrOpt) IP address on which instance vncservers should listen</para>
                  </entry>
                </row>
                <row>
                  <entry>
                    <para>vncserver_proxyclient_address = 127.0.0.1</para>
                  </entry>
                  <entry>
                    <para>(StrOpt) The address to which proxy clients (like nova-xvpvncproxy)
                                        should connect</para>
                  </entry>
                </row>
                <row>
                  <entry>
                    <para>xvpvncproxy_base_url = <link xmlns:xl="http://www.w3.org/1999/xlink" xl:href="http://127.0.0.1:6081/console"/></para>
                  </entry>
                  <entry>
                    <para>(StrOpt) Location of nova xvp VNC console proxy, in the form
                                        "<link xmlns:xl="http://www.w3.org/1999/xlink" xl:href="http://127.0.0.1:6081/console"/>"</para>
                  </entry>
                </row>
              </tbody>
            </tgroup>
          </table>
          <note>
            <itemizedlist>
              <listitem>
                <para>The <literal>vncserver_proxyclient_address</literal> defaults to <literal>127.0.0.1</literal>, which is
                                the address of the compute host that Compute instructs proxies to use when
                                connecting to instance servers.</para>
              </listitem>
              <listitem>
                <para>For all-in-one XenServer domU deployments, set this to <literal>169.254.0.1.</literal></para>
              </listitem>
              <listitem>
                <para>For multi-host XenServer domU deployments, set to a <literal>dom0 management IP</literal>
                                on the same network as the proxies.</para>
              </listitem>
              <listitem>
                <para>For multi-host libvirt deployments, set to a host management IP on the
                                same network as the proxies.</para>
              </listitem>
            </itemizedlist>
          </note>
        </section>
        <section xml:id="typical-deployment">
          <title>Typical deployment</title>
          <para>A typical deployment has the following components:</para>
          <itemizedlist>
            <listitem>
              <para>A <literal>nova-consoleauth</literal> process. Typically runs on the controller host.</para>
            </listitem>
            <listitem>
              <para>One or more <literal>nova-novncproxy</literal> services. Supports browser-based noVNC
                            clients. For simple deployments, this service typically runs on the same
                            machine as <literal>nova-api</literal> because it operates as a proxy between the public
                            network and the private compute host network.</para>
            </listitem>
            <listitem>
              <para>One or more <literal>nova-xvpvncproxy</literal> services. Supports the special Java client
                            discussed here. For simple deployments, this service typically runs on the
                            same machine as <literal>nova-api</literal> because it acts as a proxy between the public
                            network and the private compute host network.</para>
            </listitem>
            <listitem>
              <para>One or more compute hosts. These compute hosts must have correctly configured
                            options, as follows.</para>
            </listitem>
          </itemizedlist>
        </section>
        <section xml:id="nova-novncproxy-novnc">
          <title>nova-novncproxy (noVNC)</title>
          <para>You must install the noVNC package, which contains the <literal>nova-novncproxy</literal>
                    service. As root, run the following command:</para>
          <screen language="console"># apt-get install nova-novncproxy</screen>
          <para>The service starts automatically on installation.</para>
          <para>To restart the service, run:</para>
          <screen language="console"># service nova-novncproxy restart</screen>
          <para>The configuration option parameter should point to your <literal>nova.conf</literal> file,
                    which includes the message queue server address and credentials.</para>
          <para>By default, <literal>nova-novncproxy</literal> binds on <literal>0.0.0.0:6080</literal>.</para>
          <para>To connect the service to your Compute deployment, add the following
                    configuration options to your <literal>nova.conf</literal> file:</para>
          <itemizedlist>
            <listitem>
              <para>
                <literal>vncserver_listen=0.0.0.0</literal>
              </para>
              <para>Specifies the address on which the VNC service should bind. Make sure it is
                            assigned one of the compute node interfaces. This address is the one used by
                            your domain file.</para>
              <screen language="console">&lt;graphics type="vnc" autoport="yes" keymap="en-us" listen="0.0.0.0"/&gt;</screen>
              <note>
                <para>To use live migration, use the 0.0.0.0 address.</para>
              </note>
            </listitem>
            <listitem>
              <para>
                <literal>vncserver_proxyclient_address=127.0.0.1</literal>
              </para>
              <para>The address of the compute host that Compute instructs proxies to use when
                            connecting to instance <literal>vncservers</literal>.</para>
            </listitem>
          </itemizedlist>
        </section>
        <section xml:id="frequently-asked-questions-about-vnc-access-to-virtual-machines">
          <title>Frequently asked questions about VNC access to virtual machines</title>
          <itemizedlist>
            <listitem>
              <para>
                <emphasis role="bold">Q: What is the difference between ``nova-xvpvncproxy`` and
                                ``nova-novncproxy``?</emphasis>
              </para>
              <para>A: <literal>nova-xvpvncproxy</literal>, which ships with OpenStack Compute, is a proxy that
                            supports a simple Java client. nova-novncproxy uses noVNC to provide VNC
                            support through a web browser.</para>
            </listitem>
            <listitem>
              <para>
                <emphasis role="bold">Q: I want VNC support in the OpenStack dashboard. What services do I
                                need?</emphasis>
              </para>
              <para>A: You need <literal>nova-novncproxy</literal>, <literal>nova-consoleauth</literal>, and correctly
                            configured compute hosts.</para>
            </listitem>
            <listitem>
              <para>
                <emphasis role="bold">Q: When I use ``nova get-vnc-console`` or click on the VNC tab of the
                                OpenStack dashboard, it hangs. Why?</emphasis>
              </para>
              <para>A: Make sure you are running <literal>nova-consoleauth</literal> (in addition to
                            <literal>nova-novncproxy</literal>). The proxies rely on <literal>nova-consoleauth</literal> to validate
                            tokens, and waits for a reply from them until a timeout is reached.</para>
            </listitem>
            <listitem>
              <para>
                <emphasis role="bold">Q: My VNC proxy worked fine during my all-in-one test, but now it doesn't
                                work on multi host. Why?</emphasis>
              </para>
              <para>A: The default options work for an all-in-one install, but changes must be
                            made on your compute hosts once you start to build a cluster.  As an example,
                            suppose you have two servers:</para>
              <screen language="bash">PROXYSERVER (public_ip=172.24.1.1, management_ip=192.168.1.1)
COMPUTESERVER (management_ip=192.168.1.2)</screen>
              <para>Your <literal>nova-compute</literal> configuration file must set the following values:</para>
              <screen language="console"># These flags help construct a connection data structure
vncserver_proxyclient_address=192.168.1.2
novncproxy_base_url=http://172.24.1.1:6080/vnc_auto.html
xvpvncproxy_base_url=http://172.24.1.1:6081/console

# This is the address where the underlying vncserver (not the proxy)
# will listen for connections.
vncserver_listen=192.168.1.2</screen>
              <note>
                <para><literal>novncproxy_base_url</literal> and <literal>xvpvncproxy_base_url</literal> use a public IP; this
                                is the URL that is ultimately returned to clients, which generally do not
                                have access to your private network. Your PROXYSERVER must be able to
                                reach <literal>vncserver_proxyclient_address</literal>, because that is the address over
                                which the VNC connection is proxied.</para>
              </note>
            </listitem>
            <listitem>
              <para>
                <emphasis role="bold">Q: My noVNC does not work with recent versions of web browsers. Why?</emphasis>
              </para>
              <para>A: Make sure you have installed <literal>python-numpy</literal>, which is required to
                            support a newer version of the WebSocket protocol (HyBi-07+).</para>
            </listitem>
            <listitem>
              <para>
                <emphasis role="bold">Q: How do I adjust the dimensions of the VNC window image in the OpenStack
                                dashboard?</emphasis>
              </para>
              <para>A: These values are hard-coded in a Django HTML template. To alter them, edit
                            the <literal>_detail_vnc.html</literal> template file. The location of this file varies
                            based on Linux distribution. On Ubuntu 14.04, the file is at
                            <literal>/usr/share/pyshared/horizon/dashboards/nova/instances/templates/instances/_detail_vnc.html</literal>.</para>
              <para>Modify the <literal>width</literal> and <literal>height</literal> options, as follows:</para>
              <screen language="console">&lt;iframe src="{{ vnc_url }}" width="720" height="430"&gt;&lt;/iframe&gt;</screen>
            </listitem>
            <listitem>
              <para>
                <emphasis role="bold">Q: My noVNC connections failed with ValidationError: Origin header protocol
                                does not match. Why?</emphasis>
              </para>
              <para>A: Make sure the <literal>base_url</literal> match your TLS setting. If you are using https
                            console connections, make sure that the value of <literal>novncproxy_base_url</literal> is
                            set explicitly where the <literal>nova-novncproxy</literal> service is running.</para>
            </listitem>
          </itemizedlist>
        </section>
      </section>
    </section>
    <section xml:id="configure-compute-service-groups" xml:base="service-groups">
      <title>Configure Compute service groups</title>
      <para>The Compute service must know the status of each compute node to effectively
            manage and use them. This can include events like a user launching a new VM,
            the scheduler sending a request to a live node, or a query to the ServiceGroup
            API to determine if a node is live.</para>
      <para>When a compute worker running the nova-compute daemon starts, it calls the join
            API to join the compute group. Any service (such as the scheduler) can query
            the group's membership and the status of its nodes.  Internally, the
            ServiceGroup client driver automatically updates the compute worker status.</para>
      <section xml:id="database-servicegroup-driver">
        <title>Database ServiceGroup driver</title>
        <para>By default, Compute uses the database driver to track if a node is live.  In a
                compute worker, this driver periodically sends a <literal>db update</literal> command to the
                database, saying I'm OK with a timestamp. Compute uses a pre-defined
                timeout (<literal>service_down_time</literal>) to determine if a node is dead.</para>
        <para>The driver has limitations, which can be problematic depending on your
                environment. If a lot of compute worker nodes need to be checked, the database
                can be put under heavy load, which can cause the timeout to trigger, and a live
                node could incorrectly be considered dead. By default, the timeout is 60
                seconds. Reducing the timeout value can help in this situation, but you must
                also make the database update more frequently, which again increases the
                database workload.</para>
        <para>The database contains data that is both transient (such as whether the node is
                alive) and persistent (such as entries for VM owners). With the ServiceGroup
                abstraction, Compute can treat each type separately.</para>
      </section>
      <section xml:id="memcache-servicegroup-driver">
        <title>Memcache ServiceGroup driver</title>
        <para>The memcache ServiceGroup driver uses memcached, a distributed memory object
                caching system that is used to increase site performance. For more details, see
                <link xmlns:xl="http://www.w3.org/1999/xlink" xl:href="http://memcached.org/">memcached.org</link>.</para>
        <para>To use the memcache driver, you must install memcached. You might already have
                it installed, as the same driver is also used for the OpenStack Object Storage
                and OpenStack dashboard. To install memcached, see the <emphasis>Environment -&gt;
                    Memcached</emphasis> section in the <link xmlns:xl="http://www.w3.org/1999/xlink" xl:href="https://docs.openstack.org/project-install-guide/ocata">Installation Tutorials and Guides</link> depending on your
                distribution.</para>
        <para>These values in the <literal>/etc/nova/nova.conf</literal> file are required on every node for
                the memcache driver:</para>
        <screen language="ini"># Driver for the ServiceGroup service
servicegroup_driver = "mc"

# Memcached servers. Use either a list of memcached servers to use for caching (list value),
# or "&lt;None&gt;" for in-process caching (default).
memcached_servers = &lt;None&gt;

# Timeout; maximum time since last check-in for up service (integer value).
# Helps to define whether a node is dead
service_down_time = 60</screen>
      </section>
    </section>
    <section xml:id="security-hardening" xml:base="security">
      <title>Security hardening</title>
      <para>OpenStack Compute can be integrated with various third-party technologies to
            increase security. For more information, see the <link xmlns:xl="http://www.w3.org/1999/xlink" xl:href="https://docs.openstack.org/security-guide/">OpenStack Security Guide</link>.</para>
      <section xml:id="trusted-compute-pools">
        <title>Trusted compute pools</title>
        <para>Administrators can designate a group of compute hosts as trusted using trusted
                compute pools. The trusted hosts use hardware-based security features, such as
                the Intel Trusted Execution Technology (TXT), to provide an additional level of
                security. Combined with an external stand-alone, web-based remote attestation
                server, cloud providers can ensure that the compute node runs only software
                with verified measurements and can ensure a secure cloud stack.</para>
        <para>Trusted compute pools provide the ability for cloud subscribers to request
                services run only on verified compute nodes.</para>
        <para>The remote attestation server performs node verification like this:</para>
        <procedure>
          <step>
            <para>Compute nodes boot with Intel TXT technology enabled.</para>
          </step>
          <step>
            <para>The compute node BIOS, hypervisor, and operating system are measured.</para>
          </step>
          <step>
            <para>When the attestation server challenges the compute node, the measured data
                        is sent to the attestation server.</para>
          </step>
          <step>
            <para>The attestation server verifies the measurements against a known good
                        database to determine node trustworthiness.</para>
          </step>
        </procedure>
        <para>A description of how to set up an attestation service is beyond the scope of
                this document. For an open source project that you can use to implement an
                attestation service, see the <link xmlns:xl="http://www.w3.org/1999/xlink" xl:href="https://github.com/OpenAttestation/OpenAttestation">Open Attestation</link> project.</para>
        <figure>
          <title>Configuring Compute to use trusted compute pools</title>
          <mediaobject>
            <imageobject role="fo">
              <imagedata fileref="OpenStackTrustedComputePool1.png"/>
            </imageobject>
            <imageobject role="html">
              <imagedata fileref="OpenStackTrustedComputePool1.png"/>
            </imageobject>
          </mediaobject>
        </figure>
        <procedure>
          <step>
            <para>Enable scheduling support for trusted compute pools by adding these lines to
                        the <literal>DEFAULT</literal> section of the <literal>/etc/nova/nova.conf</literal> file:</para>
            <screen language="ini">[DEFAULT]
compute_scheduler_driver=nova.scheduler.filter_scheduler.FilterScheduler
scheduler_available_filters=nova.scheduler.filters.all_filters
scheduler_default_filters=AvailabilityZoneFilter,RamFilter,ComputeFilter,TrustedFilter</screen>
          </step>
          <step>
            <para>Specify the connection information for your attestation service by adding
                        these lines to the <literal>trusted_computing</literal> section of the
                        <literal>/etc/nova/nova.conf</literal> file:</para>
            <screen language="ini">[trusted_computing]
attestation_server = 10.1.71.206
attestation_port = 8443
# If using OAT v2.0 after, use this port:
# attestation_port = 8181
attestation_server_ca_file = /etc/nova/ssl.10.1.71.206.crt
# If using OAT v1.5, use this api_url:
attestation_api_url = /AttestationService/resources
# If using OAT pre-v1.5, use this api_url:
# attestation_api_url = /OpenAttestationWebServices/V1.0
attestation_auth_blob = i-am-openstack</screen>
            <para>In this example:</para>
            <variablelist>
              <varlistentry>
                <term>
                  <literal>server</literal>
                </term>
                <listitem>
                  <para>Host name or IP address of the host that runs the attestation service</para>
                </listitem>
              </varlistentry>
              <varlistentry>
                <term>
                  <literal>port</literal>
                </term>
                <listitem>
                  <para>HTTPS port for the attestation service</para>
                </listitem>
              </varlistentry>
              <varlistentry>
                <term>
                  <literal>server_ca_file</literal>
                </term>
                <listitem>
                  <para>Certificate file used to verify the attestation server's identity</para>
                </listitem>
              </varlistentry>
              <varlistentry>
                <term>
                  <literal>api_url</literal>
                </term>
                <listitem>
                  <para>The attestation service's URL path</para>
                </listitem>
              </varlistentry>
              <varlistentry>
                <term>
                  <literal>auth_blob</literal>
                </term>
                <listitem>
                  <para>An authentication blob, required by the attestation service.</para>
                </listitem>
              </varlistentry>
            </variablelist>
          </step>
          <step>
            <para>Save the file, and restart the <literal>nova-compute</literal> and <literal>nova-scheduler</literal>
                        service to pick up the changes.</para>
          </step>
        </procedure>
        <para>To customize the trusted compute pools, use these configuration option
                settings:</para>
        <table xml:id="id2">
          <title>Description of trusted computing configuration options</title>
          <tgroup cols="2">
            <colspec colname="c0" colwidth="50"/>
            <colspec colname="c1" colwidth="50"/>
            <thead>
              <row>
                <entry>
                  <para>Configuration option = Default value</para>
                </entry>
                <entry>
                  <para>Description</para>
                </entry>
              </row>
              <row>
                <entry>
                  <para>[trusted_computing]</para>
                </entry>
                <entry/>
              </row>
            </thead>
            <tbody>
              <row>
                <entry>
                  <para>attestation_api_url = /OpenAttestationWebServices/V1.0</para>
                </entry>
                <entry>
                  <para>(StrOpt) Attestation web API URL</para>
                </entry>
              </row>
              <row>
                <entry>
                  <para>attestation_auth_blob = None</para>
                </entry>
                <entry>
                  <para>(StrOpt) Attestation authorization blob - must change</para>
                </entry>
              </row>
              <row>
                <entry>
                  <para>attestation_auth_timeout = 60</para>
                </entry>
                <entry>
                  <para>(IntOpt) Attestation status cache valid period length</para>
                </entry>
              </row>
              <row>
                <entry>
                  <para>attestation_insecure_ssl = False</para>
                </entry>
                <entry>
                  <para>(BoolOpt) Disable SSL cert verification for Attestation service</para>
                </entry>
              </row>
              <row>
                <entry>
                  <para>attestation_port = 8443</para>
                </entry>
                <entry>
                  <para>(StrOpt) Attestation server port</para>
                </entry>
              </row>
              <row>
                <entry>
                  <para>attestation_server = None</para>
                </entry>
                <entry>
                  <para>(StrOpt) Attestation server HTTP</para>
                </entry>
              </row>
              <row>
                <entry>
                  <para>attestation_server_ca_file = None</para>
                </entry>
                <entry>
                  <para>(StrOpt) Attestation server Cert file for Identity verification</para>
                </entry>
              </row>
            </tbody>
          </tgroup>
        </table>
        <procedure>
          <step>
            <para>Flavors can be designated as trusted using the <command>openstack flavor
                            set</command> command. In this example, the <literal>m1.tiny</literal> flavor is being set as
                        trusted:</para>
            <screen language="console">$ openstack flavor set --property trusted_host=trusted m1.tiny</screen>
          </step>
          <step>
            <para>You can request that your instance is run on a trusted host by specifying a
                        trusted flavor when booting the instance:</para>
            <screen language="console">$ openstack server create --flavor m1.tiny \
  --key-name myKeypairName --image myImageID newInstanceName</screen>
          </step>
        </procedure>
        <mediaobject>
          <imageobject role="fo">
            <imagedata fileref="OpenStackTrustedComputePool2.png"/>
          </imageobject>
          <imageobject role="html">
            <imagedata fileref="OpenStackTrustedComputePool2.png"/>
          </imageobject>
        </mediaobject>
      </section>
      <section xml:id="encrypt-compute-metadata-traffic">
        <title>Encrypt Compute metadata traffic</title>
        <para>
          <emphasis role="bold">Enabling SSL encryption</emphasis>
        </para>
        <para>OpenStack supports encrypting Compute metadata traffic with HTTPS.  Enable SSL
                encryption in the <literal>metadata_agent.ini</literal> file.</para>
        <procedure>
          <step>
            <para>Enable the HTTPS protocol.</para>
            <screen language="ini">nova_metadata_protocol = https</screen>
          </step>
          <step>
            <para>Determine whether insecure SSL connections are accepted for Compute metadata
                        server requests. The default value is <literal>False</literal>.</para>
            <screen language="ini">nova_metadata_insecure = False</screen>
          </step>
          <step>
            <para>Specify the path to the client certificate.</para>
            <screen language="ini">nova_client_cert = PATH_TO_CERT</screen>
          </step>
          <step>
            <para>Specify the path to the private key.</para>
            <screen language="ini">nova_client_priv_key = PATH_TO_KEY</screen>
          </step>
        </procedure>
      </section>
    </section>
    <section xml:id="recover-from-a-failed-compute-node" xml:base="node-down">
      <title>Recover from a failed compute node</title>
      <para>If you deploy Compute with a shared file system, you can use several methods to
            quickly recover from a node failure. This section discusses manual recovery.</para>
      <section xml:id="evacuate-instances">
        <title>Evacuate instances</title>
        <para>If a hardware malfunction or other error causes the cloud compute node to fail,
                you can use the <command>nova evacuate</command> command to evacuate instances.  See
                <xref linkend="evacuate"/> for more information on using the command.</para>
      </section>
      <section xml:id="nova-compute-node-down-manual-recovery">
        <title>Manual recovery</title>
        <para>To manually recover a failed compute node:</para>
        <procedure>
          <step>
            <para>Identify the VMs on the affected hosts by using a combination of the
                        <command>openstack server list</command> and <command>openstack server show</command>
                        commands or the <command>euca-describe-instances</command> command.</para>
            <para>For example, this command displays information about the i-000015b9 instance
                        that runs on the np-rcc54 node:</para>
            <screen language="console">$ euca-describe-instances
i-000015b9 at3-ui02 running nectarkey (376, np-rcc54) 0 m1.xxlarge 2012-06-19T00:48:11.000Z 115.146.93.60</screen>
          </step>
          <step>
            <para>Query the Compute database for the status of the host. This example converts
                        an EC2 API instance ID to an OpenStack ID. If you use the <command>nova</command>
                        commands, you can substitute the ID directly. This example output is
                        truncated:</para>
            <screen language="none">mysql&gt; SELECT * FROM instances WHERE id = CONV('15b9', 16, 10) \G;
*************************** 1. row ***************************
created_at: 2012-06-19 00:48:11
updated_at: 2012-07-03 00:35:11
deleted_at: NULL
...
id: 5561
...
power_state: 5
vm_state: shutoff
...
hostname: at3-ui02
host: np-rcc54
...
uuid: 3f57699a-e773-4650-a443-b4b37eed5a06
...
task_state: NULL
...</screen>
            <note>
              <para>Find the credentials for your database in <literal>/etc/nova.conf</literal> file.</para>
            </note>
          </step>
          <step>
            <para>Decide to which compute host to move the affected VM. Run this database
                        command to move the VM to that host:</para>
            <screen language="mysql">mysql&gt; UPDATE instances SET host = 'np-rcc46' WHERE uuid = '3f57699a-e773-4650-a443-b4b37eed5a06';</screen>
          </step>
          <step>
            <para>If you use a hypervisor that relies on libvirt, such as KVM, update the
                        <literal>libvirt.xml</literal> file in <literal>/var/lib/nova/instances/[instance ID]</literal> with these
                        changes:</para>
            <itemizedlist>
              <listitem>
                <para>Change the <literal>DHCPSERVER</literal> value to the host IP address of the new compute
                                host.</para>
              </listitem>
              <listitem>
                <para>Update the VNC IP to <literal>0.0.0.0</literal>.</para>
              </listitem>
            </itemizedlist>
          </step>
          <step>
            <para>Reboot the VM:</para>
            <screen language="console">$ openstack server reboot 3f57699a-e773-4650-a443-b4b37eed5a06</screen>
          </step>
        </procedure>
        <para>Typically, the database update and <command>openstack server reboot</command> command
                recover a VM from a failed host. However, if problems persist, try one of these
                actions:</para>
        <itemizedlist>
          <listitem>
            <para>Use <command>virsh</command> to recreate the network filter configuration.</para>
          </listitem>
          <listitem>
            <para>Restart Compute services.</para>
          </listitem>
          <listitem>
            <para>Update the <literal>vm_state</literal> and <literal>power_state</literal> fields in the Compute database.</para>
          </listitem>
        </itemizedlist>
      </section>
      <section xml:id="recover-from-a-uid-gid-mismatch">
        <title>Recover from a UID/GID mismatch</title>
        <para>Sometimes when you run Compute with a shared file system or an automated
                configuration tool, files on your compute node might use the wrong UID or GID.
                This UID or GID mismatch can prevent you from running live migrations or
                starting virtual machines.</para>
        <para>This procedure runs on <literal>nova-compute</literal> hosts, based on the KVM hypervisor:</para>
        <procedure>
          <step>
            <para>Set the nova UID to the same number in <literal>/etc/passwd</literal> on all hosts. For
                        example, set the UID to <literal>112</literal>.</para>
            <note>
              <para>Choose UIDs or GIDs that are not in use for other users or groups.</para>
            </note>
          </step>
          <step>
            <para>Set the <literal>libvirt-qemu</literal> UID to the same number in the <literal>/etc/passwd</literal> file
                        on all hosts. For example, set the UID to <literal>119</literal>.</para>
          </step>
          <step>
            <para>Set the <literal>nova</literal> group to the same number in the <literal>/etc/group</literal> file on all
                        hosts. For example, set the group to <literal>120</literal>.</para>
          </step>
          <step>
            <para>Set the <literal>libvirtd</literal> group to the same number in the <literal>/etc/group</literal> file on
                        all hosts. For example, set the group to <literal>119</literal>.</para>
          </step>
          <step>
            <para>Stop the services on the compute node.</para>
          </step>
          <step>
            <para>Change all files that the nova user or group owns. For example:</para>
            <screen language="console"># find / -uid 108 -exec chown nova {} \;
# note the 108 here is the old nova UID before the change
# find / -gid 120 -exec chgrp nova {} \;</screen>
          </step>
          <step>
            <para>Repeat all steps for the <literal>libvirt-qemu</literal> files, if required.</para>
          </step>
          <step>
            <para>Restart the services.</para>
          </step>
          <step>
            <para>To verify that all files use the correct IDs, run the <command>find</command>
                        command.</para>
          </step>
        </procedure>
      </section>
      <section xml:id="recover-cloud-after-disaster">
        <title>Recover cloud after disaster</title>
        <para>This section describes how to manage your cloud after a disaster and back up
                persistent storage volumes. Backups are mandatory, even outside of disaster
                scenarios.</para>
        <para>For a definition of a disaster recovery plan (DRP), see
                <link xmlns:xl="http://www.w3.org/1999/xlink" xl:href="https://en.wikipedia.org/wiki/Disaster_Recovery_Plan"/>.</para>
        <para>A disk crash, network loss, or power failure can affect several components in
                your cloud architecture. The worst disaster for a cloud is a power loss. A
                power loss affects these components:</para>
        <itemizedlist>
          <listitem>
            <para>A cloud controller (<literal>nova-api</literal>, <literal>nova-objectstore</literal>, <literal>nova-network</literal>)</para>
          </listitem>
          <listitem>
            <para>A compute node (<literal>nova-compute</literal>)</para>
          </listitem>
          <listitem>
            <para>A storage area network (SAN) used by OpenStack Block Storage
                        (<literal>cinder-volumes</literal>)</para>
          </listitem>
        </itemizedlist>
        <para>Before a power loss:</para>
        <itemizedlist>
          <listitem>
            <para>Create an active iSCSI session from the SAN to the cloud controller (used
                        for the <literal>cinder-volumes</literal> LVM's VG).</para>
          </listitem>
          <listitem>
            <para>Create an active iSCSI session from the cloud controller to the compute node
                        (managed by <literal>cinder-volume</literal>).</para>
          </listitem>
          <listitem>
            <para>Create an iSCSI session for every volume (so 14 EBS volumes requires 14
                        iSCSI sessions).</para>
          </listitem>
          <listitem>
            <para>Create <literal>iptables</literal> or <literal>ebtables</literal> rules from the cloud controller to the
                        compute node. This allows access from the cloud controller to the running
                        instance.</para>
          </listitem>
          <listitem>
            <para>Save the current state of the database, the current state of the running
                        instances, and the attached volumes (mount point, volume ID, volume status,
                        etc), at least from the cloud controller to the compute node.</para>
          </listitem>
        </itemizedlist>
        <para>After power resumes and all hardware components restart:</para>
        <itemizedlist>
          <listitem>
            <para>The iSCSI session from the SAN to the cloud no longer exists.</para>
          </listitem>
          <listitem>
            <para>The iSCSI session from the cloud controller to the compute node no longer
                        exists.</para>
          </listitem>
          <listitem>
            <para>nova-network reapplies configurations on boot and, as a result, recreates
                        the iptables and ebtables from the cloud controller to the compute node.</para>
          </listitem>
          <listitem>
            <para>Instances stop running.</para>
            <para>Instances are not lost because neither <literal>destroy</literal> nor <literal>terminate</literal> ran.
                        The files for the instances remain on the compute node.</para>
          </listitem>
          <listitem>
            <para>The database does not update.</para>
          </listitem>
        </itemizedlist>
        <warning>
          <para>Do not add any steps or change the order of steps in this procedure.</para>
        </warning>
        <procedure>
          <step>
            <para>Check the current relationship between the volume and its instance, so that
                        you can recreate the attachment.</para>
            <para>Use the <command>openstack volume list</command> command to get this information.
                        Note that the <command>openstack</command> client can get volume information from
                        OpenStack Block Storage.</para>
          </step>
          <step>
            <para>Update the database to clean the stalled state. Do this for every volume by
                        using these queries:</para>
            <screen language="mysql">mysql&gt; use cinder;
mysql&gt; update volumes set mountpoint=NULL;
mysql&gt; update volumes set status="available" where status &lt;&gt;"error_deleting";
mysql&gt; update volumes set attach_status="detached";
mysql&gt; update volumes set instance_id=0;</screen>
            <para>Use <command>openstack volume list</command> command to list all volumes.</para>
          </step>
          <step>
            <para>Restart the instances by using the <command>openstack server reboot
                            INSTANCE</command> command.</para>
            <important>
              <para>Some instances completely reboot and become reachable, while some might
                            stop at the plymouth stage. This is expected behavior. DO NOT reboot a
                            second time.</para>
              <para>Instance state at this stage depends on whether you added an <literal>/etc/fstab</literal>
                            entry for that volume. Images built with the cloud-init package remain in
                            a <literal>pending</literal> state, while others skip the missing volume and start. You
                            perform this step to ask Compute to reboot every instance so that the
                            stored state is preserved. It does not matter if not all instances come
                            up successfully. For more information about cloud-init, see
                            <link xmlns:xl="http://www.w3.org/1999/xlink" xl:href="https://help.ubuntu.com/community/CloudInit/">help.ubuntu.com/community/CloudInit/</link>.</para>
            </important>
          </step>
          <step>
            <para>If required, run the <command>openstack server add volume</command> command to
                        reattach the volumes to their respective instances. This example uses a file
                        of listed volumes to reattach them:</para>
            <screen language="bash">#!/bin/bash

while read line; do
    volume=`echo $line | $CUT -f 1 -d " "`
    instance=`echo $line | $CUT -f 2 -d " "`
    mount_point=`echo $line | $CUT -f 3 -d " "`
        echo "ATTACHING VOLUME FOR INSTANCE - $instance"
    openstack server add volume $instance $volume $mount_point
    sleep 2
done &lt; $volumes_tmp_file</screen>
            <para>Instances that were stopped at the plymouth stage now automatically continue
                        booting and start normally. Instances that previously started successfully
                        can now see the volume.</para>
          </step>
          <step>
            <para>Log in to the instances with SSH and reboot them.</para>
            <para>If some services depend on the volume or if a volume has an entry in fstab,
                        you can now restart the instance. Restart directly from the instance itself
                        and not through <command>nova</command>:</para>
            <screen language="console"># shutdown -r now</screen>
            <para>When you plan for and complete a disaster recovery, follow these tips:</para>
          </step>
        </procedure>
        <itemizedlist>
          <listitem>
            <para>Use the <literal>errors=remount</literal> option in the <literal>fstab</literal> file to prevent data
                        corruption.</para>
            <para>In the event of an I/O error, this option prevents writes to the disk. Add
                            this configuration option into the cinder-volume server that performs the
                            iSCSI connection to the SAN and into the instances' <literal>fstab</literal> files.</para>
          </listitem>
          <listitem>
            <para>Do not add the entry for the SAN's disks to the cinder-volume's <literal>fstab</literal>
                        file.</para>
            <para>Some systems hang on that step, which means you could lose access to your
                            cloud-controller. To re-run the session manually, run this command before
                            performing the mount:</para>
            <screen language="console"># iscsiadm -m discovery -t st -p $SAN_IP $ iscsiadm -m node --target-name $IQN -p $SAN_IP -l</screen>
          </listitem>
          <listitem>
            <para>On your instances, if you have the whole <literal>/home/</literal> directory on the disk,
                        leave a user's directory with the user's bash files and the
                        <literal>authorized_keys</literal> file instead of emptying the <literal>/home/</literal> directory and
                        mapping the disk on it.</para>
            <para>This action enables you to connect to the instance without the volume
                        attached, if you allow only connections through public keys.</para>
          </listitem>
        </itemizedlist>
        <para>To script the disaster recovery plan (DRP), use the <link xmlns:xl="http://www.w3.org/1999/xlink" xl:href="https://github.com/Razique/BashStuff/blob/master/SYSTEMS/OpenStack/SCR_5006_V00_NUAC-OPENSTACK-DRP-OpenStack.sh">https://github.com/Razique</link>
                bash script.</para>
        <para>This script completes these steps:</para>
        <procedure>
          <step>
            <para>Creates an array for instances and their attached volumes.</para>
          </step>
          <step>
            <para>Updates the MySQL database.</para>
          </step>
          <step>
            <para>Restarts all instances with euca2ools.</para>
          </step>
          <step>
            <para>Reattaches the volumes.</para>
          </step>
          <step>
            <para>Uses Compute credentials to make an SSH connection into every instance.</para>
          </step>
        </procedure>
        <para>The script includes a <literal>test mode</literal>, which enables you to perform the sequence
                for only one instance.</para>
        <para>To reproduce the power loss, connect to the compute node that runs that
                instance and close the iSCSI session. Do not detach the volume by using the
                <command>openstack server remove volume</command> command. You must manually close the
                iSCSI session. This example closes an iSCSI session with the number <literal>15</literal>:</para>
        <screen language="console"># iscsiadm -m session -u -r 15</screen>
        <para>Do not forget the <literal>-r</literal> option. Otherwise, all sessions close.</para>
        <warning>
          <para>There is potential for data loss while running instances during this
                    procedure. If you are using Liberty or earlier, ensure you have the correct
                    patch and set the options appropriately.</para>
        </warning>
      </section>
    </section>
    <section xml:id="advanced-configuration" xml:base="adv-config">
      <title>Advanced configuration</title>
      <para>OpenStack clouds run on platforms that differ greatly in the capabilities that
            they provide. By default, the Compute service seeks to abstract the underlying
            hardware that it runs on, rather than exposing specifics about the underlying
            host platforms. This abstraction manifests itself in many ways. For example,
            rather than exposing the types and topologies of CPUs running on hosts, the
            service exposes a number of generic CPUs (virtual CPUs, or vCPUs) and allows
            for overcommitting of these. In a similar manner, rather than exposing the
            individual types of network devices available on hosts, generic
            software-powered network ports are provided. These features are designed to
            allow high resource utilization and allows the service to provide a generic
            cost-effective and highly scalable cloud upon which to build applications.</para>
      <para>This abstraction is beneficial for most workloads. However, there are some
            workloads where determinism and per-instance performance are important, if not
            vital. In these cases, instances can be expected to deliver near-native
            performance. The Compute service provides features to improve individual
            instance for these kind of workloads.</para>
      <section xml:id="attaching-physical-pci-devices-to-guests" xml:base="pci-passthrough">
        <title>Attaching physical PCI devices to guests</title>
        <para>The PCI passthrough feature in OpenStack allows full access and direct control
            of a physical PCI device in guests. This mechanism is generic for any kind of
            PCI device, and runs with a Network Interface Card (NIC), Graphics Processing
            Unit (GPU), or any other devices that can be attached to a PCI bus. Correct
            driver installation is the only requirement for the guest to properly use the
            devices.</para>
        <para>Some PCI devices provide Single Root I/O Virtualization and Sharing (SR-IOV)
            capabilities. When SR-IOV is used, a physical device is virtualized and appears
            as multiple PCI devices. Virtual PCI devices are assigned to the same or
            different guests. In the case of PCI passthrough, the full physical device is
            assigned to only one guest and cannot be shared.</para>
        <note>
          <para>For information on attaching virtual SR-IOV devices to guests, refer to the
                <link xmlns:xl="http://www.w3.org/1999/xlink" xl:href="https://docs.openstack.org/neutron/latest/admin/config-sriov.html">Networking Guide</link>.</para>
        </note>
        <para>To enable PCI passthrough, follow the steps below:</para>
        <procedure>
          <step>
            <para>Configure nova-scheduler (Controller)</para>
          </step>
          <step>
            <para>Configure nova-api (Controller)**</para>
          </step>
          <step>
            <para>Configure a flavor (Controller)</para>
          </step>
          <step>
            <para>Enable PCI passthrough (Compute)</para>
          </step>
          <step>
            <para>Configure PCI devices in nova-compute (Compute)</para>
          </step>
        </procedure>
        <note>
          <para>The PCI device with address <literal>0000:41:00.0</literal> is used as an example. This
                will differ between environments.</para>
        </note>
        <section xml:id="configure-nova-scheduler-controller">
          <title>Configure nova-scheduler (Controller)</title>
          <procedure>
            <step>
              <para>Configure <literal>nova-scheduler</literal> as specified in <link xmlns:xl="http://www.w3.org/1999/xlink" xl:href="https://docs.openstack.org/neutron/latest/admin/config-sriov.html#configure-nova-scheduler-controller">Configure nova-scheduler</link>.</para>
            </step>
            <step>
              <para>Restart the <literal>nova-scheduler</literal> service.</para>
            </step>
          </procedure>
        </section>
        <section xml:id="configure-nova-api-controller">
          <title>Configure nova-api (Controller)</title>
          <procedure>
            <step>
              <para>Specify the PCI alias for the device.</para>
              <para>Configure a PCI alias <literal>a1</literal> to request a PCI device with a <literal>vendor_id</literal> of
                        <literal>0x8086</literal> and a <literal>product_id</literal> of <literal>0x154d</literal>. The <literal>vendor_id</literal> and
                        <literal>product_id</literal> correspond the PCI device with address <literal>0000:41:00.0</literal>.</para>
              <para>Edit <literal>/etc/nova/nova.conf</literal>:</para>
              <screen language="ini">[pci]
alias = { "vendor_id":"8086", "product_id":"154d", "device_type":"type-PF", "name":"a1" }</screen>
              <para>For more information about the syntax of <literal>alias</literal>, refer to
                        <xref linkend="../configuration/config"/>.</para>
            </step>
            <step>
              <para>Restart the <literal>nova-api</literal> service.</para>
            </step>
          </procedure>
        </section>
        <section xml:id="configure-a-flavor-controller">
          <title>Configure a flavor (Controller)</title>
          <para>Configure a flavor to request two PCI devices, each with <literal>vendor_id</literal> of
                <literal>0x8086</literal> and <literal>product_id</literal> of <literal>0x154d</literal>:</para>
          <screen language="console"># openstack flavor set m1.large --property "pci_passthrough:alias"="a1:2"</screen>
          <para>For more information about the syntax for <literal>pci_passthrough:alias</literal>, refer to
                <xref linkend="flavors"/>.</para>
        </section>
        <section xml:id="enable-pci-passthrough-compute">
          <title>Enable PCI passthrough (Compute)</title>
          <para>Enable VT-d and IOMMU. For more information, refer to steps one and two in
                <link xmlns:xl="http://www.w3.org/1999/xlink" xl:href="https://docs.openstack.org/neutron/latest/admin/config-sriov.html#create-virtual-functions-compute">Create Virtual Functions</link>.</para>
        </section>
        <section xml:id="configure-pci-devices-compute">
          <title>Configure PCI devices (Compute)</title>
          <procedure>
            <step>
              <para>Configure <literal>nova-compute</literal> to allow the PCI device to pass through to
                        VMs. Edit <literal>/etc/nova/nova.conf</literal>:</para>
              <screen language="ini">[pci]
passthrough_whitelist = { "address": "0000:41:00.0" }</screen>
              <para>Alternatively specify multiple PCI devices using whitelisting:</para>
              <screen language="ini">[pci]
passthrough_whitelist = { "vendor_id": "8086", "product_id": "10fb" }</screen>
              <para>All PCI devices matching the <literal>vendor_id</literal> and <literal>product_id</literal> are added to
                        the pool of PCI devices available for passthrough to VMs.</para>
              <para>For more information about the syntax of <literal>passthrough_whitelist</literal>,
                        refer to <xref linkend="../configuration/config"/>.</para>
            </step>
            <step>
              <para>Specify the PCI alias for the device.</para>
              <para>From the Newton release, to resize guest with PCI device, configure the PCI
                        alias on the compute node as well.</para>
              <para>Configure a PCI alias <literal>a1</literal> to request a PCI device with a <literal>vendor_id</literal> of
                        <literal>0x8086</literal> and a <literal>product_id</literal> of <literal>0x154d</literal>. The <literal>vendor_id</literal> and
                        <literal>product_id</literal> correspond the PCI device with address <literal>0000:41:00.0</literal>.</para>
              <para>Edit <literal>/etc/nova/nova.conf</literal>:</para>
              <screen language="ini">[pci]
alias = { "vendor_id":"8086", "product_id":"154d", "device_type":"type-PF", "name":"a1" }</screen>
              <para>For more information about the syntax of <literal>alias</literal>, refer to <xref linkend="../configuration/config"/>.</para>
            </step>
            <step>
              <para>Restart the <literal>nova-compute</literal> service.</para>
            </step>
          </procedure>
        </section>
        <section xml:id="create-instances-with-pci-passthrough-devices">
          <title>Create instances with PCI passthrough devices</title>
          <para>The <literal>nova-scheduler</literal> selects a destination host that has PCI devices
                available with the specified <literal>vendor_id</literal> and <literal>product_id</literal> that matches the
                <literal>alias</literal> from the flavor.</para>
          <screen language="console"># openstack server create --flavor m1.large --image cirros-0.3.5-x86_64-uec --wait test-pci</screen>
        </section>
      </section>
      <section xml:id="cpu-topologies" xml:base="cpu-topologies">
        <title>CPU topologies</title>
        <para>The NUMA topology and CPU pinning features in OpenStack provide high-level
            control over how instances run on hypervisor CPUs and the topology of virtual
            CPUs available to instances. These features help minimize latency and maximize
            performance.</para>
        <section xml:id="smp-numa-and-smt">
          <title>SMP, NUMA, and SMT</title>
          <variablelist>
            <varlistentry>
              <term>Symmetric multiprocessing (SMP)</term>
              <listitem>
                <para>SMP is a design found in many modern multi-core systems. In an SMP system,
                            there are two or more CPUs and these CPUs are connected by some interconnect.
                            This provides CPUs with equal access to system resources like memory and
                            input/output ports.</para>
              </listitem>
            </varlistentry>
            <varlistentry>
              <term>Non-uniform memory access (NUMA)</term>
              <listitem>
                <para>NUMA is a derivative of the SMP design that is found in many multi-socket
                            systems. In a NUMA system, system memory is divided into cells or nodes that
                            are associated with particular CPUs. Requests for memory on other nodes are
                            possible through an interconnect bus. However, bandwidth across this shared
                            bus is limited. As a result, competition for this resource can incur
                            performance penalties.</para>
              </listitem>
            </varlistentry>
            <varlistentry>
              <term>Simultaneous Multi-Threading (SMT)</term>
              <listitem>
                <para>SMT is a design complementary to SMP. Whereas CPUs in SMP systems share a bus
                            and some memory, CPUs in SMT systems share many more components. CPUs that
                            share components are known as thread siblings.  All CPUs appear as usable
                            CPUs on the system and can execute workloads in parallel. However, as with
                            NUMA, threads compete for shared resources.</para>
              </listitem>
            </varlistentry>
          </variablelist>
          <para>In OpenStack, SMP CPUs are known as <emphasis>cores</emphasis>, NUMA cells or nodes are known as
                <emphasis>sockets</emphasis>, and SMT CPUs are known as <emphasis>threads</emphasis>. For example, a quad-socket,
                eight core system with Hyper-Threading would have four sockets, eight cores per
                socket and two threads per core, for a total of 64 CPUs.</para>
        </section>
        <section xml:id="configuring-compute-nodes-for-instances-with-numa-placement-policies">
          <title>Configuring compute nodes for instances with NUMA placement policies</title>
          <para>Hyper-V is configured by default to allow instances to span multiple NUMA
                nodes, regardless if the instances have been configured to only span N NUMA
                nodes. This behaviour allows Hyper-V instances to have up to 64 vCPUs and 1 TB
                of memory.</para>
          <para>Checking NUMA spanning can easily be done by running this following powershell
                command:</para>
          <screen language="console">(Get-VMHost).NumaSpanningEnabled</screen>
          <para>In order to disable this behaviour, the host will have to be configured to
                disable NUMA spanning. This can be done by executing these following
                powershell commands:</para>
          <screen language="console">Set-VMHost -NumaSpanningEnabled $false
Restart-Service vmms</screen>
          <para>In order to restore this behaviour, execute these powershell commands:</para>
          <screen language="console">Set-VMHost -NumaSpanningEnabled $true
Restart-Service vmms</screen>
          <para>The <literal>vmms</literal> service (Virtual Machine Management Service) is responsible for
                managing the Hyper-V VMs. The VMs will still run while the service is down
                or restarting, but they will not be manageable by the <literal>nova-compute</literal>
                service. In order for the effects of the Host NUMA spanning configuration
                to take effect, the VMs will have to be restarted.</para>
          <para>Hyper-V does not allow instances with a NUMA topology to have dynamic
                memory allocation turned on. The Hyper-V driver will ignore the configured
                <literal>dynamic_memory_ratio</literal> from the given <literal>nova.conf</literal> file when spawning
                instances with a NUMA topology.</para>
        </section>
        <section xml:id="customizing-instance-numa-placement-policies">
          <title>Customizing instance NUMA placement policies</title>
          <important>
            <para>The functionality described below is currently only supported by the
                    libvirt/KVM and Hyper-V driver.</para>
          </important>
          <para>When running workloads on NUMA hosts, it is important that the vCPUs executing
                processes are on the same NUMA node as the memory used by these processes.
                This ensures all memory accesses are local to the node and thus do not consume
                the limited cross-node memory bandwidth, adding latency to memory accesses.
                Similarly, large pages are assigned from memory and benefit from the same
                performance improvements as memory allocated using standard pages. Thus, they
                also should be local. Finally, PCI devices are directly associated with
                specific NUMA nodes for the purposes of DMA. Instances that use PCI or SR-IOV
                devices should be placed on the NUMA node associated with these devices.</para>
          <para>By default, an instance floats across all NUMA nodes on a host. NUMA awareness
                can be enabled implicitly through the use of huge pages or pinned CPUs or
                explicitly through the use of flavor extra specs or image metadata.  In all
                cases, the <literal>NUMATopologyFilter</literal> filter must be enabled. Details on this
                filter are provided in <xref linkend="configuration/schedulers"/> in Nova
                configuration guide.</para>
          <important>
            <para>The NUMA node(s) used are normally chosen at random. However, if a PCI
                    passthrough or SR-IOV device is attached to the instance, then the NUMA
                    node that the device is associated with will be used. This can provide
                    important performance improvements. However, booting a large number of
                    similar instances can result in unbalanced NUMA node usage. Care should
                    be taken to mitigate this issue. See this <link xmlns:xl="http://www.w3.org/1999/xlink" xl:href="http://lists.openstack.org/pipermail/openstack-dev/2016-March/090367.html">discussion</link> for more details.</para>
          </important>
          <important>
            <para>Inadequate per-node resources will result in scheduling failures. Resources
                    that are specific to a node include not only CPUs and memory, but also PCI
                    and SR-IOV resources. It is not possible to use multiple resources from
                    different nodes without requesting a multi-node layout. As such, it may be
                    necessary to ensure PCI or SR-IOV resources are associated with the same
                    NUMA node or force a multi-node layout.</para>
          </important>
          <para>When used, NUMA awareness allows the operating system of the instance to
                intelligently schedule the workloads that it runs and minimize cross-node
                memory bandwidth. To restrict an instance's vCPUs to a single host NUMA node,
                run:</para>
          <screen language="console">$ openstack flavor set m1.large --property hw:numa_nodes=1</screen>
          <para>Some workloads have very demanding requirements for memory access latency or
                bandwidth that exceed the memory bandwidth available from a single NUMA node.
                For such workloads, it is beneficial to spread the instance across multiple
                host NUMA nodes, even if the instance's RAM/vCPUs could theoretically fit on a
                single NUMA node. To force an instance's vCPUs to spread across two host NUMA
                nodes, run:</para>
          <screen language="console">$ openstack flavor set m1.large --property hw:numa_nodes=2</screen>
          <para>The allocation of instances vCPUs and memory from different host NUMA nodes can
                be configured. This allows for asymmetric allocation of vCPUs and memory, which
                can be important for some workloads. To spread the 6 vCPUs and 6 GB of memory
                of an instance across two NUMA nodes and create an asymmetric 1:2 vCPU and
                memory mapping between the two nodes, run:</para>
          <screen language="console">$ openstack flavor set m1.large --property hw:numa_nodes=2
$ openstack flavor set m1.large \  # configure guest node 0
  --property hw:numa_cpus.0=0,1 \
  --property hw:numa_mem.0=2048
$ openstack flavor set m1.large \  # configure guest node 1
  --property hw:numa_cpus.1=2,3,4,5 \
  --property hw:numa_mem.1=4096</screen>
          <note>
            <para>Hyper-V does not support asymmetric NUMA topologies, and the Hyper-V
                    driver will not spawn instances with such topologies.</para>
          </note>
          <para>For more information about the syntax for <literal>hw:numa_nodes</literal>, <literal>hw:numa_cpus.N</literal>
                and <literal>hw:num_mem.N</literal>, refer to the <xref linkend="extra-specs-numa-topology"/> guide.</para>
        </section>
        <section xml:id="customizing-instance-cpu-pinning-policies">
          <title>Customizing instance CPU pinning policies</title>
          <important>
            <para>The functionality described below is currently only supported by the
                    libvirt/KVM driver. Hyper-V does not support CPU pinning.</para>
          </important>
          <para>By default, instance vCPU processes are not assigned to any particular host
                CPU, instead, they float across host CPUs like any other process. This allows
                for features like overcommitting of CPUs. In heavily contended systems, this
                provides optimal system performance at the expense of performance and latency
                for individual instances.</para>
          <para>Some workloads require real-time or near real-time behavior, which is not
                possible with the latency introduced by the default CPU policy. For such
                workloads, it is beneficial to control which host CPUs are bound to an
                instance's vCPUs. This process is known as pinning. No instance with pinned
                CPUs can use the CPUs of another pinned instance, thus preventing resource
                contention between instances. To configure a flavor to use pinned vCPUs, a
                use a dedicated CPU policy. To force this, run:</para>
          <screen language="console">$ openstack flavor set m1.large --property hw:cpu_policy=dedicated</screen>
          <important>
            <para>Host aggregates should be used to separate pinned instances from unpinned
                    instances as the latter will not respect the resourcing requirements of
                    the former.</para>
          </important>
          <para>When running workloads on SMT hosts, it is important to be aware of the impact
                that thread siblings can have. Thread siblings share a number of components
                and contention on these components can impact performance. To configure how
                to use threads, a CPU thread policy should be specified. For workloads where
                sharing benefits performance, use thread siblings. To force this, run:</para>
          <screen language="console">$ openstack flavor set m1.large \
  --property hw:cpu_policy=dedicated \
  --property hw:cpu_thread_policy=require</screen>
          <para>For other workloads where performance is impacted by contention for resources,
                use non-thread siblings or non-SMT hosts. To force this, run:</para>
          <screen language="console">$ openstack flavor set m1.large \
  --property hw:cpu_policy=dedicated \
  --property hw:cpu_thread_policy=isolate</screen>
          <para>Finally, for workloads where performance is minimally impacted, use thread
                siblings if available. This is the default, but it can be set explicitly:</para>
          <screen language="console">$ openstack flavor set m1.large \
  --property hw:cpu_policy=dedicated \
  --property hw:cpu_thread_policy=prefer</screen>
          <para>For more information about the syntax for <literal>hw:cpu_policy</literal> and
                <literal>hw:cpu_thread_policy</literal>, refer to the <xref linkend="flavors"/> guide.</para>
          <para>Applications are frequently packaged as images. For applications that require
                real-time or near real-time behavior, configure image metadata to ensure
                created instances are always pinned regardless of flavor. To configure an
                image to use pinned vCPUs and avoid thread siblings, run:</para>
          <screen language="console">$ openstack image set [IMAGE_ID] \
  --property hw_cpu_policy=dedicated \
  --property hw_cpu_thread_policy=isolate</screen>
          <para>If the flavor specifies a CPU policy of <literal>dedicated</literal> then that policy will be
                used. If the flavor explicitly specifies a CPU policy of <literal>shared</literal> and the
                image specifies no policy or a policy of <literal>shared</literal> then the <literal>shared</literal> policy
                will be used, but if the image specifies a policy of <literal>dedicated</literal> an exception
                will be raised. By setting a <literal>shared</literal> policy through flavor extra-specs,
                administrators can prevent users configuring CPU policies in images and
                impacting resource utilization. To configure this policy, run:</para>
          <screen language="console">$ openstack flavor set m1.large --property hw:cpu_policy=shared</screen>
          <para>If the flavor does not specify a CPU thread policy then the CPU thread policy
                specified by the image (if any) will be used. If both the flavor and image
                specify a CPU thread policy then they must specify the same policy, otherwise
                an exception will be raised.</para>
          <note>
            <para>There is no correlation required between the NUMA topology exposed in the
                    instance and how the instance is actually pinned on the host. This is by
                    design. See this <link xmlns:xl="http://www.w3.org/1999/xlink" xl:href="https://bugs.launchpad.net/nova/+bug/1466780">invalid bug</link> for more information.</para>
          </note>
          <para>For more information about image metadata, refer to the <link xmlns:xl="http://www.w3.org/1999/xlink" xl:href="https://docs.openstack.org/image-guide/image-metadata.html">Image metadata</link>
                guide.</para>
        </section>
        <section xml:id="customizing-instance-cpu-topologies">
          <title>Customizing instance CPU topologies</title>
          <important>
            <para>The functionality described below is currently only supported by the
                    libvirt/KVM driver.</para>
          </important>
          <para>In addition to configuring how an instance is scheduled on host CPUs, it is
                possible to configure how CPUs are represented in the instance itself. By
                default, when instance NUMA placement is not specified, a topology of N
                sockets, each with one core and one thread, is used for an instance, where N
                corresponds to the number of instance vCPUs requested. When instance NUMA
                placement is specified, the number of sockets is fixed to the number of host
                NUMA nodes to use and the total number of instance CPUs is split over these
                sockets.</para>
          <para>Some workloads benefit from a custom topology. For example, in some operating
                systems, a different license may be needed depending on the number of CPU
                sockets. To configure a flavor to use a maximum of two sockets, run:</para>
          <screen language="console">$ openstack flavor set m1.large --property hw:cpu_sockets=2</screen>
          <para>Similarly, to configure a flavor to use one core and one thread, run:</para>
          <screen language="console">$ openstack flavor set m1.large \
  --property hw:cpu_cores=1 \
  --property hw:cpu_threads=1</screen>
          <important>
            <para>If specifying all values, the product of sockets multiplied by cores
                    multiplied by threads must equal the number of instance vCPUs. If specifying
                    any one of these values or the multiple of two values, the values must be a
                    factor of the number of instance vCPUs to prevent an exception. For example,
                    specifying <literal>hw:cpu_sockets=2</literal> on a host with an odd number of cores fails.
                    Similarly, specifying <literal>hw:cpu_cores=2</literal> and <literal>hw:cpu_threads=4</literal> on a host
                    with ten cores fails.</para>
          </important>
          <para>For more information about the syntax for <literal>hw:cpu_sockets</literal>, <literal>hw:cpu_cores</literal>
                and <literal>hw:cpu_threads</literal>, refer to the <xref linkend="flavors"/> guide.</para>
          <para>It is also possible to set upper limits on the number of sockets, cores, and
                threads used. Unlike the hard values above, it is not necessary for this exact
                number to used because it only provides a limit. This can be used to provide
                some flexibility in scheduling, while ensuring certains limits are not
                exceeded. For example, to ensure no more than two sockets are defined in the
                instance topology, run:</para>
          <screen language="console">$ openstack flavor set m1.large --property=hw:cpu_max_sockets=2</screen>
          <para>For more information about the syntax for <literal>hw:cpu_max_sockets</literal>,
                <literal>hw:cpu_max_cores</literal>, and <literal>hw:cpu_max_threads</literal>, refer to the
                <xref linkend="flavors"/> guide.</para>
          <para>Applications are frequently packaged as images. For applications that prefer
                certain CPU topologies, configure image metadata to hint that created instances
                should have a given topology regardless of flavor. To configure an image to
                request a two-socket, four-core per socket topology, run:</para>
          <screen language="console">$ openstack image set [IMAGE_ID] \
  --property hw_cpu_sockets=2 \
  --property hw_cpu_cores=4</screen>
          <para>To constrain instances to a given limit of sockets, cores or threads, use the
                <literal>max_</literal> variants. To configure an image to have a maximum of two sockets and a
                maximum of one thread, run:</para>
          <screen language="console">$ openstack image set [IMAGE_ID] \
  --property hw_cpu_max_sockets=2 \
  --property hw_cpu_max_threads=1</screen>
          <para>The value specified in the flavor is treated as the abolute limit.  The image
                limits are not permitted to exceed the flavor limits, they can only be equal
                to or lower than what the flavor defines. By setting a <literal>max</literal> value for
                sockets, cores, or threads, administrators can prevent users configuring
                topologies that might, for example, incur an additional licensing fees.</para>
          <para>For more information about image metadata, refer to the <link xmlns:xl="http://www.w3.org/1999/xlink" xl:href="https://docs.openstack.org/image-guide/image-metadata.html">Image metadata</link>
                guide.</para>
        </section>
      </section>
      <section xml:id="huge-pages" xml:base="huge-pages">
        <title>Huge pages</title>
        <para>The huge page feature in OpenStack provides important performance improvements
            for applications that are highly memory IO-bound.</para>
        <note>
          <para>Huge pages may also be referred to hugepages or large pages, depending on
                the source. These terms are synonyms.</para>
        </note>
        <section xml:id="pages-the-tlb-and-huge-pages">
          <title>Pages, the TLB and huge pages</title>
          <variablelist>
            <varlistentry>
              <term>Pages</term>
              <listitem>
                <para>Physical memory is segmented into a series of contiguous regions called
                            pages. Each page contains a number of bytes, referred to as the page size.
                            The system retrieves memory by accessing entire pages, rather than byte by
                            byte.</para>
              </listitem>
            </varlistentry>
            <varlistentry>
              <term>Translation Lookaside Buffer (TLB)</term>
              <listitem>
                <para>A TLB is used to map the virtual addresses of pages to the physical addresses
                            in actual memory. The TLB is a cache and is not limitless, storing only the
                            most recent or frequently accessed pages. During normal operation, processes
                            will sometimes attempt to retrieve pages that are not stored in the cache.
                            This is known as a TLB miss and results in a delay as the processor iterates
                            through the pages themselves to find the missing address mapping.</para>
              </listitem>
            </varlistentry>
            <varlistentry>
              <term>Huge Pages</term>
              <listitem>
                <para>The standard page size in x86 systems is 4 kB. This is optimal for general
                            purpose computing but larger page sizes - 2 MB and 1 GB - are also available.
                            These larger page sizes are known as huge pages. Huge pages result in less
                            efficient memory usage as a process will not generally use all memory
                            available in each page. However, use of huge pages will result in fewer
                            overall pages and a reduced risk of TLB misses. For processes that have
                            significant memory requirements or are memory intensive, the benefits of huge
                            pages frequently outweigh the drawbacks.</para>
              </listitem>
            </varlistentry>
            <varlistentry>
              <term>Persistent Huge Pages</term>
              <listitem>
                <para>On Linux hosts, persistent huge pages are huge pages that are reserved
                            upfront. The HugeTLB provides for the mechanism for this upfront
                            configuration of huge pages. The HugeTLB allows for the allocation of varying
                            quantities of different huge page sizes. Allocation can be made at boot time
                            or run time. Refer to the <link xmlns:xl="http://www.w3.org/1999/xlink" xl:href="https://www.kernel.org/doc/Documentation/vm/hugetlbpage.txt">Linux hugetlbfs guide</link> for more information.</para>
              </listitem>
            </varlistentry>
            <varlistentry>
              <term>Transparent Huge Pages (THP)</term>
              <listitem>
                <para>On Linux hosts, transparent huge pages are huge pages that are automatically
                            provisioned based on process requests. Transparent huge pages are provisioned
                            on a best effort basis, attempting to provision 2 MB huge pages if available
                            but falling back to 4 kB small pages if not. However, no upfront
                            configuration is necessary. Refer to the <link xmlns:xl="http://www.w3.org/1999/xlink" xl:href="https://www.kernel.org/doc/Documentation/vm/transhuge.txt">Linux THP guide</link> for more
                            information.</para>
              </listitem>
            </varlistentry>
          </variablelist>
        </section>
        <section xml:id="enabling-huge-pages-on-the-host">
          <title>Enabling huge pages on the host</title>
          <para>Persistent huge pages are required owing to their guaranteed availability.
                However, persistent huge pages are not enabled by default in most environments.
                The steps for enabling huge pages differ from platform to platform and only the
                steps for Linux hosts are described here. On Linux hosts, the number of
                persistent huge pages on the host can be queried by checking <literal>/proc/meminfo</literal>:</para>
          <screen language="console">$ grep Huge /proc/meminfo
AnonHugePages:         0 kB
ShmemHugePages:        0 kB
HugePages_Total:       0
HugePages_Free:        0
HugePages_Rsvd:        0
HugePages_Surp:        0
Hugepagesize:       2048 kB</screen>
          <para>In this instance, there are 0 persistent huge pages (<literal>HugePages_Total</literal>) and 0
                transparent huge pages (<literal>AnonHugePages</literal>) allocated. Huge pages can be
                allocated at boot time or run time. Huge pages require a contiguous area of
                memory - memory that gets increasingly fragmented the long a host is running.
                Identifying contiguous areas of memory is a issue for all huge page sizes, but
                it is particularly problematic for larger huge page sizes such as 1 GB huge
                pages. Allocating huge pages at boot time will ensure the correct number of huge
                pages is always available, while allocating them at run time can fail if memory
                has become too fragmented.</para>
          <para>To allocate huge pages at run time, the kernel boot parameters must be extended
                to include some huge page-specific parameters. This can be achieved by
                modifying <literal>/etc/default/grub</literal> and appending the <literal>hugepagesz</literal>,
                <literal>hugepages</literal>, and <literal>transparent_hugepages=never</literal> arguments to
                <literal>GRUB_CMDLINE_LINUX</literal>. To allocate, for example, 2048 persistent 2 MB huge
                pages at boot time, run:</para>
          <screen language="console"># echo 'GRUB_CMDLINE_LINUX="$GRUB_CMDLINE_LINUX hugepagesz=2M hugepages=2048 transparent_hugepage=never"' &gt; /etc/default/grub
$ grep GRUB_CMDLINE_LINUX /etc/default/grub
GRUB_CMDLINE_LINUX="..."
GRUB_CMDLINE_LINUX="$GRUB_CMDLINE_LINUX hugepagesz=2M hugepages=2048 transparent_hugepage=never"</screen>
          <important>
            <para>Persistent huge pages are not usable by standard host OS processes. Ensure
                    enough free, non-huge page memory is reserved for these processes.</para>
          </important>
          <para>Reboot the host, then validate that huge pages are now available:</para>
          <screen language="console">$ grep "Huge" /proc/meminfo
AnonHugePages:         0 kB
ShmemHugePages:        0 kB
HugePages_Total:    2048
HugePages_Free:     2048
HugePages_Rsvd:        0
HugePages_Surp:        0
Hugepagesize:       2048 kB</screen>
          <para>There are now 2048 2 MB huge pages totalling 4 GB of huge pages. These huge
                pages must be mounted. On most platforms, this happens automatically. To verify
                that the huge pages are mounted, run:</para>
          <screen language="console"># mount | grep huge
hugetlbfs on /dev/hugepages type hugetlbfs (rw)</screen>
          <para>In this instance, the huge pages are mounted at <literal>/dev/hugepages</literal>. This mount
                point varies from platform to platform. If the above command did not return
                anything, the hugepages must be mounted manually. To mount the huge pages at
                <literal>/dev/hugepages</literal>, run:</para>
          <screen language="console"># mkdir -p /dev/hugepages
# mount -t hugetlbfs hugetlbfs /dev/hugepages</screen>
          <para>There are many more ways to configure huge pages, including allocating huge
                pages at run time, specifying varying allocations for different huge page
                sizes, or allocating huge pages from memory affinitized to different NUMA
                nodes. For more information on configuring huge pages on Linux hosts, refer to
                the <link xmlns:xl="http://www.w3.org/1999/xlink" xl:href="https://www.kernel.org/doc/Documentation/vm/hugetlbpage.txt">Linux hugetlbfs guide</link>.</para>
        </section>
        <section xml:id="customizing-instance-huge-pages-allocations">
          <title>Customizing instance huge pages allocations</title>
          <important>
            <para>The functionality described below is currently only supported by the
                    libvirt/KVM driver.</para>
          </important>
          <important>
            <para>For performance reasons, configuring huge pages for an instance will
                    implicitly result in a NUMA topology being configured for the instance.
                    Configuring a NUMA topology for an instance requires enablement of
                    <literal>NUMATopologyFilter</literal>. Refer to <xref linkend="cpu-topologies"/> for more
                    information.</para>
          </important>
          <para>By default, an instance does not use huge pages for its underlying memory.
                However, huge pages can bring important or required performance improvements
                for some workloads. Huge pages must be requested explicitly through the use of
                flavor extra specs or image metadata. To request an instance use huge pages,
                run:</para>
          <screen language="console">$ openstack flavor set m1.large --property hw:mem_page_size=large</screen>
          <para>Different platforms offer different huge page sizes. For example: x86-based
                platforms offer 2 MB and 1 GB huge page sizes. Specific huge page sizes can be
                also be requested, with or without a unit suffix. The unit suffix must be one
                of: Kb(it), Kib(it), Mb(it), Mib(it), Gb(it), Gib(it), Tb(it), Tib(it), KB,
                KiB, MB, MiB, GB, GiB, TB, TiB. Where a unit suffix is not provided, Kilobytes
                are assumed. To request an instance to use 2 MB huge pages, run one of:</para>
          <screen language="console">$ openstack flavor set m1.large --property hw:mem_page_size=2Mb</screen>
          <screen language="console">$ openstack flavor set m1.large --property hw:mem_page_size=2048</screen>
          <para>Enabling huge pages for an instance can have negative consequences for other
                instances by consuming limited huge pages resources. To explicitly request
                an instance use small pages, run:</para>
          <screen language="console">$ openstack flavor set m1.large --property hw:mem_page_size=small</screen>
          <note>
            <para>Explicitly requesting any page size will still result in a NUMA topology
                    being applied to the instance, as described earlier in this document.</para>
          </note>
          <para>Finally, to leave the decision of huge or small pages to the compute driver,
                run:</para>
          <screen language="console">$ openstack flavor set m1.large --property hw:mem_page_size=any</screen>
          <para>For more information about the syntax for <literal>hw:mem_page_size</literal>, refer to the
                <link xmlns:xl="http://www.w3.org/1999/xlink" xl:href="https://docs.openstack.org/admin-guide/compute-flavors.html">Flavors</link> guide.</para>
          <para>Applications are frequently packaged as images. For applications that require
                the IO performance improvements that huge pages provides, configure image
                metadata to ensure instances always request the specific page size regardless
                of flavor. To configure an image to use 1 GB huge pages, run:</para>
          <screen language="console">$ openstack image set [IMAGE_ID]  --property hw_mem_page_size=1GB</screen>
          <para>If the flavor specifies a numerical page size or a page size of "small" the
                image is not allowed to specify a page size and if it does an exception will
                be raised. If the flavor specifies a page size of <literal>any</literal> or <literal>large</literal> then
                any page size specified in the image will be used. By setting a <literal>small</literal>
                page size in the flavor, administrators can prevent users requesting huge
                pages in flavors and impacting resource utilization. To configure this page
                size, run:</para>
          <screen language="console">$ openstack flavor set m1.large --property hw:mem_page_size=small</screen>
          <note>
            <para>Explicitly requesting any page size will still result in a NUMA topology
                    being applied to the instance, as described earlier in this document.</para>
          </note>
          <para>For more information about image metadata, refer to the <link xmlns:xl="http://www.w3.org/1999/xlink" xl:href="https://docs.openstack.org/image-guide/image-metadata.html">Image metadata</link>
                guide.</para>
         <para>To effectively administer compute, you must understand how the different
            installed nodes interact with each other. Compute can be installed in many
            different ways using multiple servers, but generally multiple compute nodes
            control the virtual servers and a cloud controller node contains the remaining
            Compute services.</para>
    <para>The Compute cloud works using a series of daemon processes named <literal>nova-*</literal>
            that exist persistently on the host machine. These binaries can all run on the
            same machine or be spread out on multiple boxes in a large deployment. The
            responsibilities of services and drivers are:</para>
    <para>
      <emphasis role="bold">Services</emphasis>
    </para>
    <variablelist>
      <varlistentry>
        <term>
          <literal>nova-api</literal>
        </term>
        <listitem>
          <para>Receives XML requests and sends them to the rest of the system. A WSGI app
                        routes and authenticates requests. Supports the EC2 and OpenStack APIs. A
                        <literal>nova.conf</literal> configuration file is created when Compute is installed.</para>
        </listitem>
      </varlistentry>
      <varlistentry>
        <term>
          <literal>nova-cert</literal>
        </term>
        <listitem>
          <para>Manages certificates.</para>
        </listitem>
      </varlistentry>
      <varlistentry>
        <term>
          <literal>nova-compute</literal>
        </term>
        <listitem>
          <para>Manages virtual machines. Loads a Service object, and exposes the public
                        methods on ComputeManager through a Remote Procedure Call (RPC).</para>
        </listitem>
      </varlistentry>
      <varlistentry>
        <term>
          <literal>nova-conductor</literal>
        </term>
        <listitem>
          <para>Provides database-access support for compute nodes (thereby reducing security
                        risks).</para>
        </listitem>
      </varlistentry>
      <varlistentry>
        <term>
          <literal>nova-consoleauth</literal>
        </term>
        <listitem>
          <para>Manages console authentication.</para>
        </listitem>
      </varlistentry>
      <varlistentry>
        <term>
          <literal>nova-objectstore</literal>
        </term>
        <listitem>
          <para>A simple file-based storage system for images that replicates most of the S3
                        API. It can be replaced with OpenStack Image service and either a simple
                        image manager or OpenStack Object Storage as the virtual machine image
                        storage facility. It must exist on the same node as <literal>nova-compute</literal>.</para>
        </listitem>
      </varlistentry>
      <varlistentry>
        <term>
          <literal>nova-network</literal>
        </term>
        <listitem>
          <para>Manages floating and fixed IPs, DHCP, bridging and VLANs. Loads a Service
                        object which exposes the public methods on one of the subclasses of
                        NetworkManager. Different networking strategies are available by changing the
                        <literal>network_manager</literal> configuration option to <literal>FlatManager</literal>,
                        <literal>FlatDHCPManager</literal>, or <literal>VLANManager</literal> (defaults to <literal>VLANManager</literal> if
                        nothing is specified).</para>
        </listitem>
      </varlistentry>
      <varlistentry>
        <term>
          <literal>nova-scheduler</literal>
        </term>
        <listitem>
          <para>Dispatches requests for new virtual machines to the correct node.</para>
        </listitem>
      </varlistentry>
      <varlistentry>
        <term>
          <literal>nova-novncproxy</literal>
        </term>
        <listitem>
          <para>Provides a VNC proxy for browsers, allowing VNC consoles to access virtual
                        machines.</para>
        </listitem>
      </varlistentry>
    </variablelist>
    <note>
      <para>Some services have drivers that change how the service implements its core
                functionality. For example, the <literal>nova-compute</literal> service supports drivers
                that let you choose which hypervisor type it can use. <literal>nova-network</literal> and
                <literal>nova-scheduler</literal> also have drivers.</para>
    </note>
        </section>
      </section>
    </section>
  </chapter>
</book>
