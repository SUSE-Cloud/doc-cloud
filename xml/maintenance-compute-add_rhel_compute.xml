<?xml version="1.0"?>
<?xml-stylesheet href="urn:x-suse:xslt:profiling:docbook51-profile.xsl"
 type="text/xml"
 title="Profiling step"?>
<!DOCTYPE chapter [
 <!ENTITY % entities SYSTEM "entities.ent"> %entities;
]>
<chapter xmlns="http://docbook.org/ns/docbook" xmlns:xi="http://www.w3.org/2001/XInclude" xmlns:xlink="http://www.w3.org/1999/xlink" xml:id="add_rhel_compute">
 <title>Adding a &rhla; Compute Node</title>
 <para>
  Adding a &rhla; compute node allows you to increase cloud capacity for more
  virtual machines. These steps will help you add new &rhla; compute hosts for
  this purpose.
 </para>
 <section xml:id="idg-all-operations-maintenance-compute-add_rhel_compute-xml-1">
  <title>Prerequisites</title>
  <para>
   You need to ensure your input model files are properly setup for &rhla;
   compute host clusters. This must be done during the installation process of
   your cloud and is discussed further at <xref linkend="install_rhel_compute_node"/>.
  </para>
 </section>
 <section xml:id="idg-all-operations-maintenance-compute-add_rhel_compute-xml-2">
  <title>Adding a &rhla; compute node</title>
  <para>
   You must have &rhla; 7.5 pre-installed on the baremetal host prior to
   beginning these steps.
  </para>
  <procedure>
   <step>
    <para>
     Ensure you have &rhla; 7.5 pre-installed on your baremetal host.
    </para>
   </step>
   <step>
    <para>
     Log in to the &lcm;.
    </para>
   </step>
   <step>
    <para>
     Edit your <literal>~/openstack/my_cloud/definition/data/servers.yml</literal>
     file to include the details about your new compute host.
    </para>
    <para>
     For example, if you already had a cluster of three &rhla; compute hosts
     using the <literal>RHEL-COMPUTE-ROLE</literal> role and need to add a
     fourth one, you would add your details to the bottom of the file.
     format. Note IPMI details are not necessary since &rhla; has already
     been preinstalled.
    </para>
<screen>- id: compute4
  ip-addr: 192.168.102.70
  role: RHEL-COMPUTE-ROLE
  server-group: RACK1</screen>
    <para>
     The <literal>role</literal> used for new &rhla; hosts must match the one
     used for all existing &rhla; hosts.
    </para>
    <important>
     <para>
      Verify that the <literal>ip-addr</literal> value you
      set for this host does not conflict with any other IP address in your
      cloud environment. You can confirm this by checking the
      <filename>~/openstack/my_cloud/info/address_info.yml</filename> file on your
      &lcm;.
     </para>
    </important>
   </step>
   <step>
    <para>
     In your
     <literal>~/openstack/my_cloud/definition/data/control_plane.yml</literal>
     file you will need to check the values for
     <literal>member-count</literal>, <literal>min-count</literal>, and
     <literal>max-count</literal>. If you specified them, ensure that they
     match up with your new total node count. For example, if you had
     previously specified <literal>member-count: 3</literal> and are adding a
     fourth compute node, you will need to change that value to
     <literal>member-count: 4</literal>.
    </para>
   </step>
   <step>
    <para>
     Commit the changes to git:
    </para>
<screen>&prompt.ardana;git commit -a -m "Add node &lt;name&gt;"</screen>
   </step>
   <step>
    <para>
     Run the configuration processor and resolve any errors that are indicated:
    </para>
<screen>&prompt.ardana;cd ~/openstack/ardana/ansible
&prompt.ardana;ansible-playbook -i hosts/localhost config-processor-run.yml</screen>
   </step>
   <step>
    <para>
     Update your deployment directory:
    </para>
<screen>&prompt.ardana;cd ~/openstack/ardana/ansible
&prompt.ardana;ansible-playbook -i hosts/localhost ready-deployment.yml</screen>
    <para>
     Before proceeding, review <emphasis
     role="bold">info/server_info.yml</emphasis> to see if the assignment of
     the node you have added is what you expect. It may not be, as nodes will
     not be numbered consecutively if any have previously been removed. This is
     to prevent loss of data; the config processor retains data about removed
     nodes and keeps their ID numbers from being reallocated.
    </para>
   </step>
   <step>
    <para>
     Look up the value for the new compute node's <literal>hostname</literal> in
     <filename>~/scratch/ansible/next/ardana/ansible/hosts/verb_hosts</filename>.
    </para>
    <para>
     Then, complete the compute host deployment with this playbook:
    </para>
<screen>&prompt.ardana;cd ~/scratch/ansible/next/ardana/ansible/
&prompt.ardana;ansible-playbook -i hosts/verb_hosts site.yml --tag "generate_hosts_file"
&prompt.ardana;ansible-playbook -i hosts/verb_hosts site.yml --limit &lt;hostname&gt;</screen>
   </step>
  </procedure>
 </section>
 <section xml:id="idg-all-operations-maintenance-compute-add_rhel_compute-xml-3">
  <title>Adding a new &rhla; compute node to monitoring</title>
  <para>
   If you want to add a new compute node to the monitoring service checks,
   run this additional playbook to install the necessary components:
  </para>
<screen>&prompt.ardana;cd ~/scratch/ansible/next/ardana/ansible
&prompt.ardana;ansible-playbook -i hosts/verb_hosts monasca-deploy.yml --tags "active_ping_checks"</screen>
 </section>
</chapter>
