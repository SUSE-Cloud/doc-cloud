<?xml version="1.0"?>
<!DOCTYPE section [
 <!ENTITY % entities SYSTEM "entity-decl.ent"> %entities;
]>
<section xmlns="http://docbook.org/ns/docbook" xmlns:xi="http://www.w3.org/2001/XInclude"
    xmlns:xlink="http://www.w3.org/1999/xlink" xml:id="mid-scale-kvm" version="5.1">
 <title>Single-Region Mid-Size Model</title>
 <para>
  The mid-size model is intended as a template for a moderate sized cloud. The
  Control plane is made up of multiple server clusters to provide sufficient
  computational, network and IOPS capacity for a mid-size production style
  cloud.
 </para>
 <variablelist>
  <varlistentry>
   <term>Control Plane</term>
   <listitem>
    <itemizedlist>
     <listitem>
      <para>
       <emphasis role="bold">Core Cluster</emphasis> runs core OpenStack
       Services, such as &o_ident;, &o_comp; API, &o_img; API, &o_netw; API,
       &o_dash;, and &o_orch; API. Default configuration is two nodes of role
       type <literal>CORE-ROLE</literal>.
      </para>
     </listitem>
     <listitem>
      <para>
       <emphasis role="bold">Metering and Monitoring Cluster</emphasis> runs
       the OpenStack Services for metering and monitoring (for example,
       &o_meter;, &o_monitor; and logging). Default configuration is three
       nodes of role type <literal>MTRMON-ROLE</literal>.
      </para>
     </listitem>
     <listitem>
      <para>
       <emphasis role="bold">Database and Message Queue Cluster</emphasis> runs
       clustered &mariadb; and RabbitMQ services to support the Ardana cloud
       infrastructure. Default configuration is three nodes of role type
       <literal>DBMQ-ROLE</literal>. Three nodes are required for high
       availability.
      </para>
     </listitem>
     <listitem>
      <para>
       <emphasis role="bold">Swift PAC Cluster</emphasis> runs the &swift;
       Proxy, Account and Container services. Default configuration is three
       nodes of role type <literal>SWPAC-ROLE</literal>.
      </para>
     </listitem>
     <listitem>
      <para>
       <emphasis role="bold">&o_netw; Agent Cluster</emphasis> Runs &o_netw;
       VPN (L3), DHCP, Metadata and OpenVswitch agents. Default configuration
       is two nodes of role type <literal>NEUTRON-ROLE</literal>.
      </para>
     </listitem>
    </itemizedlist>
   </listitem>
  </varlistentry>
  <varlistentry>
   <term>&lcm;</term>
   <listitem>
    <para>
     The &lcm; runs on one of the control-plane nodes of type
     <literal>CONTROLLER-ROLE</literal>. The IP address of the node that will
     run the &lcm; needs to be included in the
     <filename>data/servers.yml</filename> file.
    </para>
   </listitem>
  </varlistentry>
  <varlistentry>
   <term>Resource Nodes</term>
   <listitem>
    <itemizedlist>
     <listitem>
      <para>
       <emphasis role="bold">Compute</emphasis> runs &o_comp; Compute and
       associated services. Runs on nodes of role type
       <literal>COMPUTE-ROLE</literal>. This model lists 3 nodes. 1 node is the
       minimum requirement.
      </para>
     </listitem>
     <listitem>
      <para>
       <emphasis role="bold">Object Storage</emphasis> 3 nodes of type
       <literal>SOWBJ-ROLE</literal> run the &swift; Object service. The
       minimum node count should match your &swift; replica count.
      </para>
     </listitem>
    </itemizedlist>
    <para>
     The minimum node count required to run this model unmodified is 19 nodes.
     This can be reduced by consolidating services on the control plane
     clusters.
    </para>
   </listitem>
  </varlistentry>
  <varlistentry>
   <term>Networking</term>
   <listitem>
    <para>
     This example requires the following networks:
    </para>
    <itemizedlist>
     <listitem>
      <para>
       <emphasis role="bold">IPMI</emphasis> network connected to the
       lifecycle-manager and the IPMI ports of all servers.
      </para>
     </listitem>
    </itemizedlist>
    <para>
     Nodes require a pair of bonded NICs which are used by the following
     networks:
    </para>
    <itemizedlist>
     <listitem>
      <para>
       <emphasis role="bold">External API</emphasis> The network for making
       requests to the cloud.
      </para>
     </listitem>
     <listitem>
      <para>
       <emphasis role="bold">Internal API</emphasis> This network is used
       within the cloud for API access between services.
      </para>
     </listitem>
     <listitem>
      <para>
       <emphasis role="bold">External VM</emphasis> This network provides
       access to VMs via floating IP addresses.
      </para>
     </listitem>
     <listitem>
      <para>
       <emphasis role="bold">Cloud Management</emphasis> This network is used
       for all internal traffic between the cloud services. It is also used to
       install and configure the nodes. The network needs to be on an untagged
       VLAN.
      </para>
     </listitem>
     <listitem>
      <para>
       <emphasis role="bold">Guest</emphasis> The network that carries traffic
       between VMs on private networks within the cloud.
      </para>
     </listitem>
     <listitem>
      <para>
       <emphasis role="bold">SWIFT</emphasis> This network is used for internal
       &swift; communications between the &swift; nodes.
      </para>
     </listitem>
    </itemizedlist>
    <para>
     The <literal>EXTERNAL API</literal> network must be reachable from the
     <literal>EXTERNAL VM</literal> network for VMs to be able to make API
     calls to the cloud.
    </para>
    <para>
     An example set of networks is defined in
     <filename>data/networks.yml</filename>. The file needs to be modified to
     reflect your environment.
    </para>
    <para>
     The example uses the devices <filename>hed3</filename> and
     <filename>hed4</filename> as a bonded network interface for all services.
     The name given to a network interface by the system is configured in the
     file <filename>data/net_interfaces.yml</filename>. That file needs to be
     edited to match your system.
    </para>
   </listitem>
  </varlistentry>
 </variablelist>
 <section>
  <title>Adapting the Mid-Size Model to Fit Your Environment</title>
  <para>
   The minimum set of changes you need to make to adapt the model for your
   environment are:
  </para>
  <itemizedlist>
   <listitem>
    <para>
     Update <filename>servers.yml</filename> to list the details of your
     baremetal servers.
    </para>
   </listitem>
   <listitem>
    <para>
     Update the <filename>networks.yml</filename> file to replace network CIDRs
     and VLANs with site specific values.
    </para>
   </listitem>
   <listitem>
    <para>
     Update the <filename>nic_mappings.yml</filename> file to ensure that
     network devices are mapped to the correct physical port(s).
    </para>
   </listitem>
   <listitem>
    <para>
     Review the disk models (<filename>disks_*.yml</filename>) and confirm that
     the associated servers have the number of disks required by the disk
     model. The device names in the disk models might need to be adjusted to
     match the probe order of your servers. The default number of disks for the
     &swift; nodes (3 disks) is set low on purpose to facilitate deployment on
     generic hardware. For production scale &swift; the servers should have
     more disks. For example, 6 on SWPAC nodes and 12 on SWOBJ nodes. If you
     allocate more &swift; disks then you should review the ring power in the
     &swift; ring configuration. This is documented in the &swift; section.
     Disk models are provided as follows:
    </para>
    <itemizedlist>
     <listitem>
      <para>
       DISK SET CONTROLLER: Minimum 1 disk
      </para>
     </listitem>
     <listitem>
      <para>
       DISK SET DBMQ: Minimum 3 disks
      </para>
     </listitem>
     <listitem>
      <para>
       DISK SET COMPUTE: Minimum 2 disks
      </para>
     </listitem>
     <listitem>
      <para>
       DISK SET SWPAC: Minimum 3 disks
      </para>
     </listitem>
     <listitem>
      <para>
       DISK SET SWOBJ: Minimum 3 disks
      </para>
     </listitem>
    </itemizedlist>
   </listitem>
   <listitem>
    <para>
     Update the <filename>netinterfaces.yml</filename> file to match the server
     NICs used in your configuration. This file has a separate interface model
     definition for each of the following:
    </para>
    <itemizedlist>
     <listitem>
      <para>
       INTERFACE SET CONTROLLER
      </para>
     </listitem>
     <listitem>
      <para>
       INTERFACE SET DBMQ
      </para>
     </listitem>
     <listitem>
      <para>
       INTERFACE SET SWPAC
      </para>
     </listitem>
     <listitem>
      <para>
       INTERFACE SET SWOBJ
      </para>
     </listitem>
     <listitem>
      <para>
       INTERFACE SET COMPUTE
      </para>
     </listitem>
    </itemizedlist>
   </listitem>
  </itemizedlist>
 </section>
</section>
