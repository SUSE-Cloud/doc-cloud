<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE appendix
[
  <!ENTITY % entities SYSTEM "entity-decl.ent">
  %entities;
]>
<!-- Converted by suse-upgrade version 1.1 -->
<appendix xmlns="http://docbook.org/ns/docbook"
 xmlns:xi="http://www.w3.org/2001/XInclude"
 xmlns:xlink="http://www.w3.org/1999/xlink" version="5.0"
 xml:id="app.deploy.ha_recovery">
 <title>Recovering Clusters to a Healthy State</title>
 <info>
  <dm:docmanager xmlns:dm="urn:x-suse:ns:docmanager">
   <dm:maintainer>taroth</dm:maintainer>
   <dm:status>editing</dm:status>
   <dm:deadline/>
   <dm:priority/>
   <dm:translation>no</dm:translation>
   <dm:languages/>
  </dm:docmanager>
 </info>
 <!--taroth 2016-02-22: depending on the differences between recovery for
  controll node clusters (bsc#966643) vs. compute node clusters (bsc#966641), an additional
 structure level may be needed in the future -->
 <para>If one node in your cluster refuses to rejoin the cluster, it is most
  likely that the node has not been shut down cleanly. This can either happen
  because of manual intervention or because the node has been fenced (shut down) by the
  &stonith; mechanism of the cluster. Fencing is used to protect the integrity of data in
  case of a split-brain scenario.</para>
 <para>The following sections refer to problems with the &contrnode;s
  cluster and show how to recover your degraded cluster to full strength. This
  takes the following basic steps:</para>
 <procedure>
  <step>
   <!--XREF: Recovering the Pacemaker Cluster-->
   <para>
    <xref linkend="sec.deploy.ha_recovery.contr.node.add"
     xrefstyle="select:title"/>
   </para>
  </step>
  <step>
   <!--XREF: Recovering Crowbar and Chef-->
   <para>
    <xref linkend="sec.deploy.ha_recovery.contr.crow.chef"
     xrefstyle="select:title"/>
   </para>
  </step>
  <step>
   <!--XREF: Cleaning Up Resources-->
   <para>In addition, you may need to reset resource failcounts to
    allow resources to start on the node you have re-added to the cluster. See
     <xref linkend="sec.deploy.ha_recovery.cleanup"/>.</para>
  </step>
  <step>
   <!--XREF: Removing Maintenance Mode from the Node-->
   <para>In addition, you may need to manually remove the maintenance mode flag
    from a node. See <xref linkend="sec.deploy.ha_recovery.maint"/>.</para>
  </step>
 </procedure>
 <para>
   For a list of possible symptoms that help you to diagnose a
   degraded cluster, see <xref
   linkend="sec.deploy.ha_recovery.contr.symptoms"/>.</para>

 <sect1 xml:id="sec.deploy.ha_recovery.contr.symptoms">
  <title>Symptoms of a Degraded &contrnode; Cluster</title>
  <para>The following incidents may occur if a &contrnode; in your cluster
   has been shut down in an unclean state:</para>
  <itemizedlist>
   <listitem>
    <para>A VM reboots although the &cloud; administrator did not trigger
     this action.</para>
   </listitem>
   <listitem>
    <para>One of the &contrnode; in the &crow; &wi; is in status
      <literal>Problem</literal>, signified by a red dot next to the
     node.</para>
   </listitem>
   <listitem>
    <para>The &hawk; &wi; stops responding on one of the
     &contrnode;s, while it is still responding on the others.</para>
   </listitem>
   <listitem>
    <para>The SSH connection to one of the &contrnode;s freezes.</para>
   </listitem>
   <listitem>
    <para>The &ostack; components stop responding for a short while.</para>
   </listitem>
  </itemizedlist>
 </sect1>

 <sect1 xml:id="sec.deploy.ha_recovery.contr.node.add">
  <title>Re-adding the Node to the Cluster</title>
  <procedure>
   <step>
    <para>Reboot the node.</para>
   </step>
   <step>
    <para>Connect to the node via SSH from the &admserv;.</para>
   </step>
   <step>
    <para>
     If you have a 2-node cluster with the <xref
     linkend="var.depl.ostack.pacemaker.corosync_fencing"/> option set
     to <guimenu>Automatic</guimenu>, remove the block file that is created on a
     node during start of the cluster service:
    </para>
    <screen>&prompt.root;rm /var/spool/corosync/block_automatic_start</screen>
    <para>The block file avoids &stonith; deathmatches for 2-node clusters
     (where each node kills the other one, resulting in both nodes rebooting all
     the time). When &corosync; shuts down cleanly, the block file is automatically removed.
     Otherwise the block file is still present and prevents the cluster service
     from (re-)starting on that node.</para>
    <para>
     Alternatively, bypass the block file by starting the cluster service on the
     cluster node before reconnecting the node to Crowbar in the next section
     (see <xref
     linkend="sec.deploy.ha_recovery.contr.crow.chef"/>):
    </para>
    <screen>&prompt.root;<command>systemctl</command> start pacemaker</screen>
    </step>
   </procedure>
 </sect1>

 <sect1 xml:id="sec.deploy.ha_recovery.contr.crow.chef">
  <title>Recovering &crow; and &chef;</title>
  <para>Making the Pacemaker node rejoin the cluster is not enough. All nodes in
   the cloud (including the &admserv;) need to be aware that this node is
   back online. This requires the following steps for &crow; and
   &chef;:</para>
  <procedure>
   <step>
    <para>Log in to the node you have re-added to the cluster.</para>
   </step>
   <step>
    <para>Re-register the node with &crow; by executing:</para>
    <screen>&prompt.root;<command>systemctl</command> start crowbar_join</screen>
   </step>
   <step>
    <para>Log in to each of the <emphasis>other</emphasis>
     &contrnode;s.</para>
   </step>
   <step>
    <para>Trigger a &chef; run:</para>
    <screen>&prompt.root;<command>chef-client</command></screen>
   </step>
  </procedure>
 </sect1>

 <sect1 xml:id="sec.deploy.ha_recovery.cleanup">
  <title>Cleaning Up Resources</title>
  <para>
   A resource will be automatically restarted if it fails, but each
   failure increases the resource's failcount. If a
   <literal>migration-threshold</literal> has been set for the resource,
   the node will no longer run the resource when the number of failures
   reaches the migration threshold.
   <remark>aspiers 2016-03-18: todo: actually, this is
   not true until https://trello.com/c/0wmqMP52 is completed</remark>
   To allow the resource to start again on the node, reset
   the resource's failcount by cleaning up the resource manually. You can clean
   up individual resources by using the &hawk; &wi; or all in one go as
   described below:
  </para>
  <procedure>
   <step>
     <para>
      Log in to any one of the cluster nodes which is currently online
      in the cluster.  (You can check this via <command>crm_mon</command>.)
     </para>
   </step>
   <step>
    <para>Clean-up all stopped resources with the following command:</para>
    <screen>&prompt.root;crm_resource -o | \
  awk '/\tStopped |Timed Out/ { print $1 }' | \
  xargs -r -n1 crm resource cleanup</screen>
   </step>
  </procedure>
 </sect1>

 <sect1 xml:id="sec.deploy.ha_recovery.maint">
  <title>Removing the Maintenance Mode Flag from a Node</title>
  <para>During normal operation, chef-client sometimes needs to place a node
   into maintenance mode. The node is kept in maintenance mode until the
   chef-client run finishes. However, if the chef-client run fails, the node may
   be left in maintenance mode. In that case, the cluster management tools like
   crmsh or &hawk; will show all resources on that node as
    <literal>unmanaged</literal>. To remove the maintenance flag:</para>
  <procedure>
   <step>
    <para>Log in the cluster node.</para>
   </step>
   <step>
    <para>Disable the maintenance mode with:</para>
    <screen>&prompt.root;crm node ready</screen>
   </step>
  </procedure>
 </sect1>
 <sect1 xml:id="sec.deploy.ha_recovery.drbd">
  <title>
   Recovering From an Unresolvable DRBD Split Brain Situation
  </title>
  <para>
   Although policies to automatically resolve a DRBD split brain situations
   exist, there are situations which require to be resolved manually. Such a
   situation is indicated by a Kernel message like:
  </para>
  <screen>kernel: block drbd0: Split-Brain detected, dropping connection!</screen>
  <para>
   To resolve the split brain you need to choose a node which data
   modifications will be discarded. These modifications will be replaced by
   the data from the <quote>healthy</quote> node and will not be recoverable,
   so make sure to choose the right node. If in doubt, make a backup of the
   node before starting the recovery process. Proceed as follows:
  </para>
  <procedure>
   <step>
    <para>
     Put the cluster in maintenance mode:
    </para>
    <screen>&prompt.root;crm configure property maintenance-mode=true</screen>
   </step>
   <step>
    <para>
     Check if the chose node is in primary role:</para>
    <screen>&prompt.root;systemctl status drbd</screen>
   </step>
   <step>
    <para>
     If it is in primary role, stop all services using this
     resource and switch it to secondary role. If the node
     already is in secondary role, skip this step.
    </para>
    <screen>&prompt.root;drbdadm secondary <replaceable>RESOURCE</replaceable></screen>
   </step>
   <step>
    <para>
     Check if a node is in state <literal>WFConnection</literal> by looking at
     the output of <command>systemctl status drbd</command>.
    </para>
   </step>
   <step>
    <para>
     If the node is in state <literal>WFConnection</literal> disconnect
     the resource:
    </para>
    <screen>&prompt.root;drbdadm disconnect <replaceable>RESOURCE</replaceable></screen>
   </step>
   <step>
    <para>
     Discard all modifications on the chosen node. This step is
     irreversible, the modifications on the chosen node will be lost!
    </para>
    <screen>&prompt.root;drbdadm -- --discard-my-data connect <replaceable>RESOURCE</replaceable></screen>
   </step>
   <step>
    <para>
     If the other (healthy) node is in state
     <literal>WFConnection</literal>, synchronization to the chosen node
     will start automatically. If not, reconnect the healthy node to start
     the synchronization:
    </para>
    <screen>&prompt.root;drbdadm connect <replaceable>RESOURCE</replaceable></screen>
    <para>
     During the synchronization all data modifications on the chosen node
     will be overwritten with the data from the healthy node.
    </para>
   </step>
   <step>
    <para>
     When the synchronization has finished, reset the cluster to normal mode:
    </para>
    <screen>&prompt.root;crm configure property maintenance-mode=false</screen>
   </step>
  </procedure>
 </sect1>
</appendix>
