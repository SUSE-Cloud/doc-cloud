<?xml version="1.0"?>
<?xml-stylesheet href="urn:x-suse:xslt:profiling:docbook51-profile.xsl"
 type="text/xml"
 title="Profiling step"?>
<!DOCTYPE section [
 <!ENTITY % entities SYSTEM "entity-decl.ent"> %entities;
]>
<section xmlns="http://docbook.org/ns/docbook" xmlns:xi="http://www.w3.org/2001/XInclude" xmlns:xlink="http://www.w3.org/1999/xlink" xml:id="remove_compute_node">
 <title>Removing a Compute Node</title>
 <para>
  Removing a Compute node allows you to remove capacity.
 </para>
 <para>
  You may have a need to remove a Compute node and these steps will help you
  achieve this.
 </para>
 <section xml:id="disable_provisioning">
  <title>Disable Provisioning on the Compute Host</title>
  <procedure>
   <step>
    <para>
     Get a list of the Nova services running which will provide us with the
     details we need to disable the provisioning on the Compute host you are
     wanting to remove:
    </para>
<screen>nova service-list</screen>
    <para>
     Here is an example below. I've highlighted the Compute node we are going
     to remove in the examples:
    </para>
<screen>$ nova service-list
+----+------------------+--------------------------+----------+---------+-------+----------------------------+-----------------+
| Id | Binary           | Host                     | Zone     | Status  | State | Updated_at                 | Disabled Reason |
+----+------------------+--------------------------+----------+---------+-------+----------------------------+-----------------+
| 1  | nova-conductor   | ardana-cp1-c1-m1-mgmt    | internal | enabled | up    | 2015-11-22T22:50:43.000000 | -               |
| 10 | nova-scheduler   | ardana-cp1-c1-m1-mgmt    | internal | enabled | up    | 2015-11-22T22:50:34.000000 | -               |
| 13 | nova-conductor   | ardana-cp1-c1-m3-mgmt    | internal | enabled | up    | 2015-11-22T22:50:43.000000 | -               |
| 16 | nova-conductor   | ardana-cp1-c1-m2-mgmt    | internal | enabled | up    | 2015-11-22T22:50:43.000000 | -               |
| 25 | nova-consoleauth | ardana-cp1-c1-m1-mgmt    | internal | enabled | up    | 2015-11-22T22:50:38.000000 | -               |
| 28 | nova-scheduler   | ardana-cp1-c1-m2-mgmt    | internal | enabled | up    | 2015-11-22T22:50:38.000000 | -               |
| 31 | nova-scheduler   | ardana-cp1-c1-m3-mgmt    | internal | enabled | up    | 2015-11-22T22:50:42.000000 | -               |
| 34 | nova-compute     | ardana-cp1-comp0001-mgmt | AZ1      | enabled | up    | 2015-11-22T22:50:35.000000 | -               |
<emphasis role="bold">| 37 | nova-compute     | ardana-cp1-comp0002-mgmt | AZ2      | enabled | up    | 2015-11-22T22:50:44.000000 | -               |</emphasis>
+----+------------------+--------------------------+----------+---------+-------+----------------------------+-----------------+</screen>
   </step>
   <step>
    <para>
     Disable the Nova service on the Compute node you are wanting to remove
     which will ensure it is taken out of the scheduling rotation:
    </para>
<screen>nova service-disable --reason "&lt;enter reason here&gt;" &lt;node hostname&gt; nova-compute</screen>
    <para>
     Here is an example if I wanted to remove the
     <literal>ardana-cp1-comp0002-mgmt</literal> in the output above:
    </para>
<screen>$ nova service-disable --reason "hardware reallocation" ardana-cp1-comp0002-mgmt nova-compute
+--------------------------+--------------+----------+-----------------------+
| Host                     | Binary       | Status   | Disabled Reason       |
+--------------------------+--------------+----------+-----------------------+
| ardana-cp1-comp0002-mgmt | nova-compute | disabled | hardware reallocation |
+--------------------------+--------------+----------+-----------------------+</screen>
   </step>
  </procedure>
 </section>
 <section xml:id="remove_az">
  <title>Remove the Compute Host from its Availability Zone</title>
  <para>
   If you configured the Compute host to be part of an availability zone, these
   steps will show you how to remove it.
  </para>
  <procedure>
   <step>
    <para>
     Get a list of the Nova services running which will provide us with the
     details we need to remove a Compute node:
    </para>
<screen>nova service-list</screen>
    <para>
     Here is an example below. I've highlighted the Compute node we are going
     to remove in the examples:
    </para>
<screen>$ nova service-list
+----+------------------+--------------------------+----------+---------+-------+----------------------------+-----------------------+
| Id | Binary           | Host                     | Zone     | Status  | State | Updated_at                 | Disabled Reason       |
+----+------------------+--------------------------+----------+---------+-------+----------------------------+-----------------------+
| 1  | nova-conductor   | ardana-cp1-c1-m1-mgmt    | internal | enabled | up    | 2015-11-22T22:50:43.000000 | -                     |
| 10 | nova-scheduler   | ardana-cp1-c1-m1-mgmt    | internal | enabled | up    | 2015-11-22T22:50:34.000000 | -                     |
| 13 | nova-conductor   | ardana-cp1-c1-m3-mgmt    | internal | enabled | up    | 2015-11-22T22:50:43.000000 | -                     |
| 16 | nova-conductor   | ardana-cp1-c1-m2-mgmt    | internal | enabled | up    | 2015-11-22T22:50:43.000000 | -                     |
| 25 | nova-consoleauth | ardana-cp1-c1-m1-mgmt    | internal | enabled | up    | 2015-11-22T22:50:38.000000 | -                     |
| 28 | nova-scheduler   | ardana-cp1-c1-m2-mgmt    | internal | enabled | up    | 2015-11-22T22:50:38.000000 | -                     |
| 31 | nova-scheduler   | ardana-cp1-c1-m3-mgmt    | internal | enabled | up    | 2015-11-22T22:50:42.000000 | -                     |
| 34 | nova-compute     | ardana-cp1-comp0001-mgmt | AZ1      | enabled | up    | 2015-11-22T22:50:35.000000 | -                     |
<emphasis role="bold">| 37 | nova-compute     | ardana-cp1-comp0002-mgmt | AZ2      | enabled | up    | 2015-11-22T22:50:44.000000 | hardware reallocation |</emphasis>
+----+------------------+--------------------------+----------+---------+-------+----------------------------+-----------------------+</screen>
   </step>
   <step>
    <para>
     You can remove the Compute host from the availability zone it was a part
     of with this command:
    </para>
<screen>nova aggregate-remove-host &lt;availability zone&gt; &lt;nova hostname&gt;</screen>
    <para>
     So for the same example as the previous step, the
     <literal>ardana-cp1-comp0002-mgmt</literal> host was in the
     <literal>AZ2</literal> availability zone so I would use this command to
     remove it:
    </para>
<screen>$ nova aggregate-remove-host AZ2 ardana-cp1-comp0002-mgmt
Host ardana-cp1-comp0002-mgmt has been successfully removed from aggregate 4
+----+------+-------------------+-------+-------------------------+
| Id | Name | Availability Zone | Hosts | Metadata                |
+----+------+-------------------+-------+-------------------------+
| 4  | AZ2  | AZ2               |       | 'availability_zone=AZ2' |
+----+------+-------------------+-------+-------------------------+</screen>
   </step>
   <step>
    <para>
     You can confirm the last two steps completed successfully by running
     another <literal>nova service-list</literal>.
    </para>
    <para>
     Here is an example which confirms that the node has been disabled and that
     it has been removed from the availability zone. I have highlighted these:
    </para>
<screen>$ nova service-list
+----+------------------+--------------------------+----------+----------+-------+----------------------------+-----------------------+
| Id | Binary           | Host                     | Zone     | Status   | State | Updated_at                 | Disabled Reason       |
+----+------------------+--------------------------+----------+----------+-------+----------------------------+-----------------------+
| 1  | nova-conductor   | ardana-cp1-c1-m1-mgmt    | internal | enabled  | up    | 2015-11-22T23:04:33.000000 | -                     |
| 10 | nova-scheduler   | ardana-cp1-c1-m1-mgmt    | internal | enabled  | up    | 2015-11-22T23:04:34.000000 | -                     |
| 13 | nova-conductor   | ardana-cp1-c1-m3-mgmt    | internal | enabled  | up    | 2015-11-22T23:04:33.000000 | -                     |
| 16 | nova-conductor   | ardana-cp1-c1-m2-mgmt    | internal | enabled  | up    | 2015-11-22T23:04:33.000000 | -                     |
| 25 | nova-consoleauth | ardana-cp1-c1-m1-mgmt    | internal | enabled  | up    | 2015-11-22T23:04:28.000000 | -                     |
| 28 | nova-scheduler   | ardana-cp1-c1-m2-mgmt    | internal | enabled  | up    | 2015-11-22T23:04:28.000000 | -                     |
| 31 | nova-scheduler   | ardana-cp1-c1-m3-mgmt    | internal | enabled  | up    | 2015-11-22T23:04:32.000000 | -                     |
| 34 | nova-compute     | ardana-cp1-comp0001-mgmt | AZ1      | enabled  | up    | 2015-11-22T23:04:25.000000 | -                     |
<emphasis role="bold">| 37 | nova-compute     | ardana-cp1-comp0002-mgmt | nova     | disabled | up    | 2015-11-22T23:04:34.000000 | hardware reallocation |</emphasis>
+----+------------------+--------------------------+----------+----------+-------+----------------------------+-----------------------+</screen>
   </step>
  </procedure>
 </section>
 <section xml:id="live_migration">
  <title>Use Live Migration to Move Any Instances on this Host to Other Hosts</title>
  <procedure>
   <step>
    <para>
     You will need to verify if the Compute node is currently hosting any
     instances on it. You can do this with the command below:
    </para>
<screen>nova list --host=&lt;nova hostname&gt; --all_tenants=1</screen>
    <para>
     Here is an example below which shows that we have a single running
     instance on this node currently:
    </para>
<screen>$ nova list --host=ardana-cp1-comp0002-mgmt --all_tenants=1
+--------------------------------------+--------+----------------------------------+--------+------------+-------------+-----------------+
| ID                                   | Name   | Tenant ID                        | Status | Task State | Power State | Networks        |
+--------------------------------------+--------+----------------------------------+--------+------------+-------------+-----------------+
| 78fdb938-a89c-4a0c-a0d4-b88f1555c3b9 | paul4d | 5e9998f1b1824ea9a3b06ad142f09ca5 | ACTIVE | -          | Running     | paul=10.10.10.7 |
+--------------------------------------+--------+----------------------------------+--------+------------+-------------+-----------------+</screen>
   </step>
   <step>
    <para>
     You will likely want to migrate this instance off of this node before
     removing it. You can do this with the live migration functionality within
     Nova. The command will look like this:
    </para>
<screen>nova live-migration --block-migrate &lt;nova instance ID&gt;</screen>
    <para>
     Here is an example using the instance in the previous step:
    </para>
<screen>$ nova live-migration --block-migrate 78fdb938-a89c-4a0c-a0d4-b88f1555c3b9</screen>
    <para>
     You can check the status of the migration using the same command from the
     previous step:
    </para>
<screen>$ nova list --host=ardana-cp1-comp0002-mgmt --all_tenants=1
+--------------------------------------+--------+----------------------------------+-----------+------------+-------------+-----------------+
| ID                                   | Name   | Tenant ID                        | Status    | Task State | Power State | Networks        |
+--------------------------------------+--------+----------------------------------+-----------+------------+-------------+-----------------+
| 78fdb938-a89c-4a0c-a0d4-b88f1555c3b9 | paul4d | 5e9998f1b1824ea9a3b06ad142f09ca5 | MIGRATING | migrating  | Running     | paul=10.10.10.7 |
+--------------------------------------+--------+----------------------------------+-----------+------------+-------------+-----------------+</screen>
   </step>
   <step>
    <para>
     Run nova list again
    </para>
<screen>$ nova list --host=ardana-cp1-comp0002-mgmt --all_tenants=1</screen>
    <para>
     to see that the running instance has been migrated:
    </para>
<screen>+----+------+-----------+--------+------------+-------------+----------+
| ID | Name | Tenant ID | Status | Task State | Power State | Networks |
+----+------+-----------+--------+------------+-------------+----------+
+----+------+-----------+--------+------------+-------------+----------+</screen>
   </step>
  </procedure>
 </section>
 <section>
  <title>Disable Neutron Agents on Node to be Removed</title>
  <para>
   You should also locate and disable or remove neutron agents. To see the
   neutron agents running:
  </para>
<screen>$ neutron agent-list | grep NODE_NAME
+--------------------------------------+----------------------+--------------------------+-------+----------------+---------------------------+
| id                                   | agent_type           | host                     | alive | admin_state_up | binary                    |
+--------------------------------------+----------------------+--------------------------+-------+----------------+---------------------------+
| 08f16dbc-4ba2-4c1d-a4a3-a2ff2526ebe4 | L3 agent             | ardana-cp1-comp0002-mgmt | :-)   | True           | neutron-l3-agent          |
| dbe4fe11-8f08-4306-8244-cc68e98bb770 | Metadata agent       | ardana-cp1-comp0002-mgmt | :-)   | True           | neutron-metadata-agent    |
| f0d262d1-7139-40c7-bdc2-f227c6dee5c8 | Open vSwitch agent   | ardana-cp1-comp0002-mgmt | :-)   | True           | neutron-openvswitch-agent |
+--------------------------------------+----------------------+--------------------------+-------+----------------+---------------------------+

$ neutron agent-update --admin-state-down 08f16dbc-4ba2-4c1d-a4a3-a2ff2526ebe4
$ neutron agent-update --admin-state-down dbe4fe11-8f08-4306-8244-cc68e98bb770
$ neutron agent-update --admin-state-down f0d262d1-7139-40c7-bdc2-f227c6dee5c8</screen>
 </section>
 <section xml:id="shutdown_node">
  <title>Shut down or Stop the Nova and Neutron Services on the Compute Host</title>
  <para>
   To perform this step you have a few options. You can SSH into the Compute
   host and run the following commands:
  </para>
<screen>sudo systemctl stop nova-compute</screen>
<screen >sudo systemctl stop neutron-*</screen>
  <para>
   Because the Neutron agent self-registers against Neutron server, you may
   want to prevent the following services from coming back online. Here is how
   you can get the list:
  </para>
<screen>sudo systemctl list-units neutron-* --all</screen>
  <para>
   Here are the results:
  </para>
<screen>UNIT                                  LOAD        ACTIVE     SUB      DESCRIPTION
neutron-common-rundir.service         loaded      inactive   dead     Create /var/run/neutron
•neutron-dhcp-agent.service         not-found     inactive   dead     neutron-dhcp-agent.service
neutron-l3-agent.service              loaded      inactive   dead     neutron-l3-agent Service
neutron-lbaasv2-agent.service         loaded      inactive   dead     neutron-lbaasv2-agent Service
neutron-metadata-agent.service        loaded      inactive   dead     neutron-metadata-agent Service
•neutron-openvswitch-agent.service    loaded      failed     failed   neutron-openvswitch-agent Service
neutron-ovs-cleanup.service           loaded      inactive   dead     Neutron OVS Cleanup Service

        LOAD   = Reflects whether the unit definition was properly loaded.
        ACTIVE = The high-level unit activation state, i.e. generalization of SUB.
        SUB    = The low-level unit activation state, values depend on unit type.

        7 loaded units listed.
        To show all installed unit files use 'systemctl list-unit-files'.</screen>
  <para>
   For each loaded service issue the command
  </para>
<screen>sudo systemctl disable &lt;service-name&gt;</screen>
  <para>
   In the above example that would be each service, <emphasis>except neutron-dhcp-agent.service
   </emphasis>
  </para>
  <para>
   For example:
  </para>
<screen>sudo systemctl disable neutron-common-rundir neutron-l3-agent neutron-lbaasv2-agent neutron-metadata-agent neutron-openvswitch-agent</screen>
  <para>
   Now you can shut down the node:
  </para>
<screen>sudo shutdown now</screen>
  <para>
   OR
  </para>
  <para>
   From the &lcm; you can use the
   <literal>bm-power-down.yml</literal> playbook to shut down the node:
  </para>
<screen>cd ~/openstack/ardana/ansible
ansible-playbook -i hosts/localhost bm-power-down.yml -e nodelist=&lt;node name&gt;</screen>
 <note>
  <para>
   The <literal>&lt;node name&gt;</literal> value will be the value
   corresponding to this node in Cobbler. You can run
   <command>sudo cobbler system list</command> to retrieve these names.
  </para>
 </note>
 </section>
 <section xml:id="delete_node">
  <title>Delete the Compute Host from Nova</title>
  <para>
   Retrieve the list of Nova services:
  </para>
<screen>nova service-list</screen>
  <para>
   Here is an example highlighting the Compute host we're going to remove:
  </para>
<screen>$ nova service-list
+----+------------------+--------------------------+----------+----------+-------+----------------------------+-----------------------+
| Id | Binary           | Host                     | Zone     | Status   | State | Updated_at                 | Disabled Reason       |
+----+------------------+--------------------------+----------+----------+-------+----------------------------+-----------------------+
| 1  | nova-conductor   | ardana-cp1-c1-m1-mgmt    | internal | enabled  | up    | 2015-11-22T23:04:33.000000 | -                     |
| 10 | nova-scheduler   | ardana-cp1-c1-m1-mgmt    | internal | enabled  | up    | 2015-11-22T23:04:34.000000 | -                     |
| 13 | nova-conductor   | ardana-cp1-c1-m3-mgmt    | internal | enabled  | up    | 2015-11-22T23:04:33.000000 | -                     |
| 16 | nova-conductor   | ardana-cp1-c1-m2-mgmt    | internal | enabled  | up    | 2015-11-22T23:04:33.000000 | -                     |
| 25 | nova-consoleauth | ardana-cp1-c1-m1-mgmt    | internal | enabled  | up    | 2015-11-22T23:04:28.000000 | -                     |
| 28 | nova-scheduler   | ardana-cp1-c1-m2-mgmt    | internal | enabled  | up    | 2015-11-22T23:04:28.000000 | -                     |
| 31 | nova-scheduler   | ardana-cp1-c1-m3-mgmt    | internal | enabled  | up    | 2015-11-22T23:04:32.000000 | -                     |
| 34 | nova-compute     | ardana-cp1-comp0001-mgmt | AZ1      | enabled  | up    | 2015-11-22T23:04:25.000000 | -                     |
<emphasis role="bold">| 37 | nova-compute     | ardana-cp1-comp0002-mgmt | nova     | disabled | up    | 2015-11-22T23:04:34.000000 | hardware reallocation |</emphasis>
+----+------------------+--------------------------+----------+----------+-------+----------------------------+-----------------------+</screen>
  <para>
   Delete the host from Nova using the command below:
  </para>
<screen>nova service-delete &lt;service ID&gt;</screen>
  <para>
   Following our example above, you would use:
  </para>
<screen>nova service-delete 37</screen>
  <para>
   Use the command below to confirm that the Compute host has been completely
   removed from Nova:
  </para>
<screen>nova hypervisor-list</screen>
 </section>
 <section xml:id="deletefromneutron">
  <title>Delete the Compute Host from Neutron</title>
  <para>
   Multiple Neutron agents are running on the compute node. You have to remove
   all of the agents running on the node using the "neutron agent-delete"
   command. In the example below, the l3-agent, openvswitch-agent and
   metadata-agent are running:
  </para>
<screen>$ neutron agent-list | grep NODE_NAME
+--------------------------------------+----------------------+--------------------------+-------+----------------+---------------------------+
| id                                   | agent_type           | host                     | alive | admin_state_up | binary                    |
+--------------------------------------+----------------------+--------------------------+-------+----------------+---------------------------+
| 08f16dbc-4ba2-4c1d-a4a3-a2ff2526ebe4 | L3 agent             | ardana-cp1-comp0002-mgmt | :-)   | False          | neutron-l3-agent          |
| dbe4fe11-8f08-4306-8244-cc68e98bb770 | Metadata agent       | ardana-cp1-comp0002-mgmt | :-)   | False          | neutron-metadata-agent    |
| f0d262d1-7139-40c7-bdc2-f227c6dee5c8 | Open vSwitch agent   | ardana-cp1-comp0002-mgmt | :-)   | False          | neutron-openvswitch-agent |
+--------------------------------------+----------------------+--------------------------+-------+----------------+---------------------------+

$ neutron agent-delete AGENT_ID

$ neutron agent-delete 08f16dbc-4ba2-4c1d-a4a3-a2ff2526ebe4
$ neutron agent-delete dbe4fe11-8f08-4306-8244-cc68e98bb770
$ neutron agent-delete f0d262d1-7139-40c7-bdc2-f227c6dee5c8</screen>
 </section>
 <section xml:id="remove_node">
  <title>Remove the Compute Host from the servers.yml File and Run the Configuration Processor</title>
  <para>
   Complete these steps from the &lcm; to remove the Compute node:
  </para>
  <procedure>
   <step>
    <para>
     Log in to the &lcm;
    </para>
   </step>
   <step>
    <para>
     Edit your <literal>servers.yml</literal> file in the location below to
     remove references to the Compute node(s) you want to remove:
    </para>
<screen>~/openstack/my_cloud/definition/data/servers.yml</screen>
   </step>
   <step>
    <para>
     You may also need to edit your <literal>control_plane.yml</literal> file
     to update the values for <literal>member-count</literal>,
     <literal>min-count</literal>, and <literal>max-count</literal> if you used
     those to ensure they reflect the proper number of nodes you are using.
    </para>
    <para>
     See <xref linkend="configobj_controlplane"/> for more details.
    </para>
   </step>
   <step>
    <para>
     Commit the changes to git:
    </para>
<screen>git commit -a -m "Remove node &lt;name&gt;"</screen>
   </step>
   <step>
    <para>
     Run the configuration processor:
    </para>
<screen>cd ~/openstack/ardana/ansible
ansible-playbook -i hosts/localhost config-processor-run.yml</screen>
    <para>
     To free up the resources when running the configuration processor, use
     the switches <literal>remove_deleted_servers</literal> and
     <literal>free_unused_addresses</literal>. For more information, see
     <xref linkend="persisteddata"/>.
    </para>
<screen>cd ~/openstack/ardana/ansible
ansible-playbook -i hosts/localhost config-processor-run.yml -e remove_deleted_servers="y" -e free_unused_addresses="y"</screen>
   </step>
   <step>
    <para>
     Update your deployment directory:
    </para>
<screen>cd ~/openstack/ardana/ansible
ansible-playbook -i hosts/localhost ready-deployment.yml</screen>
   </step>
  </procedure>
 </section>
 <section xml:id="remove_cobbler">
  <title>Remove the Compute Host from Cobbler</title>
  <para>
   Complete these steps to remove the node from Cobbler:
  </para>
  <procedure>
   <step>
    <para>
     Confirm the system name in Cobbler with this command:
    </para>
<screen>sudo cobbler system list</screen>
   </step>
   <step>
    <para>
     Remove the system from Cobbler using this command:
    </para>
<screen>sudo cobbler system remove --name=&lt;node&gt;</screen>
   </step>
   <step>
    <para>
     Run the <literal>cobbler-deploy.yml</literal> playbook to complete the
     process:
    </para>
<screen>cd ~/openstack/ardana/ansible
ansible-playbook -i hosts/localhost cobbler-deploy.yml</screen>
   </step>
  </procedure>
 </section>
 <section xml:id="idg-all-operations-maintenance-compute-remove_compute_node-xml-14">
  <title>Remove the Compute Host from Monitoring</title>
  <para>
   Once you have removed the Compute nodes, the alarms against them will
   trigger so there are additional steps to take to resolve this issue.
  </para>
  <para>
    To find all Monasca API servers 
  </para>
<screen>&prompt.sudo;cat /etc/haproxy/haproxy.cfg | grep MON
listen ardana-cp1-vip-public-MON-API-extapi-8070
    bind ardana-cp1-vip-public-MON-API-extapi:8070  ssl crt /etc/ssl/private//my-public-cert-entry-scale                                          
    server ardana-cp1-c1-m1-mgmt-MON_API-8070 ardana-cp1-c1-m1-mgmt:8070 check inter 5000 rise 2 fall 5                                          
    server ardana-cp1-c1-m2-mgmt-MON_API-8070 ardana-cp1-c1-m2-mgmt:8070 check inter 5000 rise 2 fall 5                                          
    server ardana-cp1-c1-m3-mgmt-MON_API-8070 ardana-cp1-c1-m3-mgmt:8070 check inter 5000 rise 2 fall 5        
listen ardana-cp1-vip-MON-API-mgmt-8070
    bind ardana-cp1-vip-MON-API-mgmt:8070  ssl crt /etc/ssl/private//ardana-internal-cert                                          
    server ardana-cp1-c1-m1-mgmt-MON_API-8070 ardana-cp1-c1-m1-mgmt:8070 check inter 5000 rise 2 fall 5                                          
    server ardana-cp1-c1-m2-mgmt-MON_API-8070 ardana-cp1-c1-m2-mgmt:8070 check inter 5000 rise 2 fall 5                                          
    server ardana-cp1-c1-m3-mgmt-MON_API-8070 ardana-cp1-c1-m3-mgmt:8070 check inter 5000 rise 2 fall 5</screen>
  <para>In above example <literal>ardana-cp1-c1-m1-mgmt</literal>,<literal>ardana-cp1-c1-m2-mgmt</literal>,
  <literal>ardana-cp1-c1-m3-mgmt</literal> are Monasa API servers</para>
  <para>
   You will want to SSH to each of the Monasca API servers and edit the
   <literal>/etc/monasca/agent/conf.d/host_alive.yaml</literal> file to remove
   references to the Compute node you removed. This will require
   <literal>sudo</literal> access. The entries will look similar to the one
   below:
  </para>
<screen>- alive_test: ping
  built_by: HostAlive
  host_name: ardana-cp1-comp0001-mgmt
  name: ardana-cp1-comp0001-mgmt ping</screen>
  <para>
   Once you have removed the references on each of your Monasca API servers you
   then need to restart the monasca-agent on each of those servers with this
   command:
  </para>
<screen>&prompt.sudo;service openstack-monasca-agent restart</screen>
  <para>
   With the Compute node references removed and the monasca-agent restarted,
   you can then delete the corresponding alarm to finish this process. To do so
   we recommend using the Monasca CLI which should be installed on each of your
   Monasca API servers by default:
  </para>
<screen>monasca alarm-list --metric-dimensions hostname=&lt;compute node deleted&gt;</screen>
  <para>
   For example, if your Compute node looked like the example above then you
   would use this command to get the alarm ID:
  </para>
<screen>monasca alarm-list --metric-dimensions hostname=ardana-cp1-comp0001-mgmt</screen>
  <para>
   You can then delete the alarm with this command:
  </para>
<screen>monasca alarm-delete &lt;alarm ID&gt;</screen>
 </section>
</section>
