<?xml version="1.0"?>
<!DOCTYPE section [
 <!ENTITY % entities SYSTEM "entities.ent"> %entities;
]>
<!---->
<section xmlns="http://docbook.org/ns/docbook" xmlns:xi="http://www.w3.org/2001/XInclude" xmlns:xlink="http://www.w3.org/1999/xlink" xml:id="topic_lfq_3wf_rt"><title>&kw-hos-tm; &kw-hos-version-50;: Removing a Cluster from the Compute Resource
    Pool</title>
    <!---->


    <sidebar xml:id="idg-all-esx-remove_existing_cluster_compute_resource_pool-xml-6"><para><emphasis role="bold">Prerequisites</emphasis></para>
<para>Write down the Hostname and ESXi configuration IP addresses of OVSvAPP VMs of that ESX
        cluster before deleting the VMs. These IP address and Hostname will be used to cleanup
        Monasca alarm definitions.</para>
<para> Perform the following steps:</para>
<orderedlist xml:id="ol_j5m_txk_xx">
        <listitem><para>Login to vSphere client.</para>
</listitem>
        <listitem><para>Select the ovsvapp node running on each ESXi host and click <emphasis role="bold">Summary</emphasis> tab as shown
          in the following example. </para>
<mediaobject xml:id="idg-all-esx-remove_existing_cluster_compute_resource_pool-xml-8"><imageobject role="fo"><imagedata fileref="media-esx-esx_hostname.png" width="75%" format="PNG"/></imageobject><imageobject role="html"><imagedata fileref="media-esx-esx_hostname.png"/></imageobject></mediaobject>
<para>Similarly you can retrieve the compute-proxy node
            information.</para>
<mediaobject xml:id="image_wgh_f1f_xx"><imageobject role="fo"><imagedata fileref="media-esx-esx_cluster2.png" width="75%" format="PNG"/></imageobject><imageobject role="html"><imagedata fileref="media-esx-esx_cluster2.png"/></imageobject></mediaobject>
</listitem>
      </orderedlist></sidebar>

    <sidebar><title>Removing an existing cluster from the compute resource pool</title><para>Perform the following steps to remove an existing cluster from the compute resource pool.</para>
<orderedlist xml:id="ol_dq1_pwf_rt">
        <listitem><para>Run the following command to check for the instances launched in that
              cluster:</para>
<screen># nova list --host &lt;hostname&gt;
+--------------------------------------+---------+--------+------------+-------------+------------------+
| ID                                   | Name    | Status | Task State | Power State | Networks         |
+--------------------------------------+---------+--------+------------+-------------+------------------+
| 80e54965-758b-425e-901b-9ea756576331 | HLM-VM1 | ACTIVE | -          | Running     | private=10.0.0.2 |
+--------------------------------------+---------+--------+------------+-------------+------------------+</screen><para>where:</para>
<itemizedlist xml:id="ul_zr4_tqh_rt">
              <listitem><para><emphasis role="bold">hostname</emphasis>: Specifies hostname of the compute proxy present in that cluster.
                  <!----></para>
</listitem>
            </itemizedlist>
</listitem>
        <listitem><para>Delete all instances spawned in that
              cluster:</para>
<screen># nova delete &lt;server&gt; [&lt;server ...&gt;]</screen><para>where:</para>
<itemizedlist xml:id="ul_ckw_crh_rt">
              <listitem><para><emphasis role="bold">server</emphasis>: Specifies the name or ID of server (s)</para>
</listitem>
            </itemizedlist>
<para>OR</para>
<para>Migrate all instances spawned in that
            cluster.</para>
<screen># nova migrate &lt;server&gt;</screen>
</listitem>
        <listitem><para>Run the following playbooks for stop the Compute (Nova) and Networking (Neutron)
            services:</para>
<screen>ansible-playbook -i hosts/verb_hosts nova-stop --limit &lt;hostname&gt;;
ansible-playbook -i hosts/verb_hosts neutron-stop --limit &lt;hostname&gt;;</screen><para>where:
              </para>
<itemizedlist xml:id="ul_hll_qrh_rt">
              <listitem><para><emphasis role="bold">hostname</emphasis>: Specifies hostname of the compute proxy present in that
                  cluster.</para>
</listitem>
            </itemizedlist>
</listitem>
        <listitem><para>Deactivate the ESX Cluster
            </para>
<screen>eon resource-deactivate &lt;RESOURCE_ID&gt;</screen><para>This command deletes
            the ESX compute proxy, OVSvApp VMs in the cluster, and triggers the ansible
            playbooks.</para>
<para><emphasis role="bold">Sample
            Output</emphasis></para>
<screen>eon resource-deactivate 9ab2196d-37d2-4006-a2c7-08946b5194ea
+-----------------+--------------------------------------+
| Property        | Value                                |
+-----------------+--------------------------------------+
| id              | 9ab2196d-37d2-4006-a2c7-08946b5194ea |
| ip_address      | UNSET                                |
| name            | compute                              |
| password        | UNSET                                |
| port            | UNSET                                |
| resource_mgr_id | 9FDCFA66-6677-42A1-83FF-16DC32448021 |
| state           | cleanup-initiated                    |
| type            | esxcluster                           |
| username        | UNSET                                |
+-----------------+--------------------------------------+ </screen>
<para>Once
            the deactivation is successful, the state of the cluster is set to
              <literal>imported</literal>.</para>
<screen>eon resource-list
+--------------------------------------+-----------+-------------+--------------------------------------+------------+-------+------------+------------+
| ID                                   | Name      | Moid        | Resource Manager ID                  | IP Address | Port  | Type       | State      |
+--------------------------------------+-----------+-------------+--------------------------------------+------------+-------+------------+------------+
| 9ab2196d-37d2-4006-a2c7-08946b5194ea | compute   | domain-c104 | 9FDCFA66-6677-42A1-83FF-16DC32448021 | UNSET      | UNSET | esxcluster | imported   |
+--------------------------------------+-----------+-------------+--------------------------------------+------------+-------+------------+------------+</screen>
</listitem>
      </orderedlist>
</sidebar>
    <sidebar><title>Cleanup Monasca Agent for OVSvAPP Service</title><para>Perform the following procedure to cleanup Monasca agents for ovsvapp-agent service.</para>
<orderedlist xml:id="idg-all-esx-remove_existing_cluster_compute_resource_pool-xml-14">
          <listitem><para>If Monasca-API is installed on different node, copy the <literal>service.orsc</literal>
            from lifecycle manager to Monasca API
            server.</para>
<screen>scp service.orsc $USER@helion-cp1-mtrmon-m1-mgmt:</screen></listitem>
          <listitem><para>SSH to Monasca API server. You must SSH to each Monasca API server for cleanup. </para>
<para>For
              example:</para>
<screen>ssh helion-cp1-mtrmon-m1-mgmt</screen>
</listitem>
          <listitem><para>Edit <literal>/etc/monasca/agent/conf.d/host_alive.yaml</literal> file to remove the
            reference to the OVSvAPP you removed. This requires <computeroutput>sudo</computeroutput>
              access.</para>
<screen>sudo vi /etc/monasca/agent/conf.d/host_alive.yaml</screen><para>A
              sample of
              <literal>host_alive.yaml</literal>:</para>
<screen>- alive_test: ping
  built_by: HostAlive
  host_name: esx-cp1-esx-ovsvapp0001-mgmt
  name: esx-cp1-esx-ovsvapp0001-mgmt ping
  target_hostname: esx-cp1-esx-ovsvapp0001-mgmt </screen>
<para>where
              &lt;host_name and target_hostname&gt; is mentioned at the DNS name field at the vSphere
              client. (Refer <link xlink:href="#topic_lfq_3wf_rt/preq">prerequisite</link>).</para>
</listitem>
          <listitem><para>After removing the reference on each of the Monasca API servers, restart the
            monasca-agent on each of those servers by executing the following
            command.</para>
<screen>sudo service monasca-agent restart</screen></listitem>
          <listitem><para>With the OVSvAPP references removed and the monasca-agent restarted, you can delete
            the corresponding alarm to complete the cleanup process. We recommend using the Monasca
            CLI which is installed on each of your Monasca API servers by default.
            Execute
            the following command from the Monasca API server (for example:
              <literal>helion-cp1-mtrmon-mX-mgmt</literal>).</para>
<screen>monasca alarm-list --metric-name host_alive_status --metric-dimensions hostname=&lt;ovsvapp deleted&gt;</screen><para>For
              example: You can execute the following command to get the alarm ID, if the OVSvAPP
              appears as a preceding example.</para>
<screen>monasca alarm-list --metric-name host_alive_status --metric-dimensions hostname=MCP-VCP-cpesx-esx-ovsvapp0001-mgmt
+--------------------------------------+--------------------------------------+-----------------------+-------------------+-------------------------------------------+----------+-------+-----------------+------+--------------------------+--------------------------+--------------------------+
| id                                   | alarm_definition_id                  | alarm_definition_name | metric_name       | metric_dimensions                         | severity | state | lifecycle_state | link | state_updated_timestamp  | updated_timestamp        | created_timestamp        |
+--------------------------------------+--------------------------------------+-----------------------+-------------------+-------------------------------------------+----------+-------+-----------------+------+--------------------------+--------------------------+--------------------------+
| cfc6bfa4-2485-4319-b1e5-0107886f4270 | cca96c53-a927-4b0a-9bf3-cb21d28216f3 | Host Status           | host_alive_status | service: system                           | HIGH     | OK    | None            | None | 2016-10-27T06:33:04.256Z | 2016-10-27T06:33:04.256Z | 2016-10-23T13:41:57.258Z |
|                                      |                                      |                       |                   | cloud_name: entry-scale-kvm-esx-vsa-mml   |          |       |                 |      |                          |                          |                          |
|                                      |                                      |                       |                   | test_type: ping                           |          |       |                 |      |                          |                          |                          |
|                                      |                                      |                       |                   | hostname: helion-cp1-esx-ovsvapp0001-mgmt |          |       |                 |      |                          |                          |                          |
|                                      |                                      |                       |                   | control_plane: control-plane-1            |          |       |                 |      |                          |                          |                          |
|                                      |                                      |                       |                   | cluster: mtrmon                           |          |       |                 |      |                          |                          |                          |
|                                      |                                      |                       |                   | observer_host: helion-cp1-mtrmon-m1-mgmt  |          |       |                 |      |                          |                          |                          |
|                                      |                                      |                       | host_alive_status | service: system                           |          |       |                 |      |                          |                          |                          |
|                                      |                                      |                       |                   | cloud_name: entry-scale-kvm-esx-vsa-mml   |          |       |                 |      |                          |                          |                          |
|                                      |                                      |                       |                   | test_type: ping                           |          |       |                 |      |                          |                          |                          |
|                                      |                                      |                       |                   | hostname: helion-cp1-esx-ovsvapp0001-mgmt |          |       |                 |      |                          |                          |                          |
|                                      |                                      |                       |                   | control_plane: control-plane-1            |          |       |                 |      |                          |                          |                          |
|                                      |                                      |                       |                   | cluster: mtrmon                           |          |       |                 |      |                          |                          |                          |
|                                      |                                      |                       |                   | observer_host: helion-cp1-mtrmon-m3-mgmt  |          |       |                 |      |                          |                          |                          |
|                                      |                                      |                       | host_alive_status | service: system                           |          |       |                 |      |                          |                          |                          |
|                                      |                                      |                       |                   | cloud_name: entry-scale-kvm-esx-vsa-mml   |          |       |                 |      |                          |                          |                          |
|                                      |                                      |                       |                   | test_type: ping                           |          |       |                 |      |                          |                          |                          |
|                                      |                                      |                       |                   | hostname: helion-cp1-esx-ovsvapp0001-mgmt |          |       |                 |      |                          |                          |                          |
|                                      |                                      |                       |                   | control_plane: control-plane-1            |          |       |                 |      |                          |                          |                          |
|                                      |                                      |                       |                   | cluster: mtrmon                           |          |       |                 |      |                          |                          |                          |
|                                      |                                      |                       |                   | observer_host: helion-cp1-mtrmon-m2-mgmt  |          |       |                 |      |                          |                          |                          |
+--------------------------------------+--------------------------------------+-----------------------+-------------------+-------------------------------------------+----------+-------+-----------------+------+--------------------------+--------------------------+--------------------------+</screen>
</listitem>
          <listitem><para>Delete the Monasca
              alaram.</para>
<screen>monasca alarm-delete &lt;alarm ID&gt;</screen><para>For
              example:</para>
<screen>monasca alarm-delete cfc6bfa4-2485-4319-b1e5-0107886f4270Successfully deleted alarm </screen>
<para>After
              deleting the alarms and updating the monasca-agent configuration, those alarms will be
              removed from the Opsconsole UI. You can login to Opconsole and view the
            status.</para>
</listitem>
        </orderedlist>
</sidebar>
    <sidebar><title>Remove the Compute proxy from Monitoring</title><para>Once you have removed the Compute proxy, the alarms against them will still trigger.
        Therefore to resolve this, you must perform the following steps.</para>
<orderedlist xml:id="ol_c54_3fk_xx">
          <listitem><para>SSH to Monasca API server. You must SSH to each Monasca API server for cleanup. </para>
<para>For
              example:</para>
<screen>ssh helion-cp1-mtrmon-m1-mgmt</screen>
</listitem>
          <listitem><para>Edit <literal>/etc/monasca/agent/conf.d/host_alive.yaml</literal> file to remove the
            reference to the Compute proxy you removed. This requires
              <computeroutput>sudo</computeroutput>
              access.</para>
<screen>sudo vi /etc/monasca/agent/conf.d/host_alive.yaml</screen><para>A sample of <literal>host_alive.yaml</literal>
              file.</para>
<screen>- alive_test: ping
  built_by: HostAlive
  host_name: MCP-VCP-cpesx-esx-comp0001-mgmt
  name: MCP-VCP-cpesx-esx-comp0001-mgmt ping</screen>
</listitem>
          <listitem><para>Once you have removed the references on each of your Monasca API servers, execute the
            following command to restart the monasca-agent on each of those
            servers.</para>
<screen>sudo service monasca-agent restart</screen></listitem>
          <listitem><para>With the Compute proxy references removed and the monasca-agent restarted, delete the
            corresponding alarm to complete this process. complete the cleanup process. We recommend
            using the Monasca CLI which is installed on each of your Monasca API servers by default.
              </para>
<screen>monasca alarm-list --metric-dimensions hostname= &lt;compute node deleted&gt;</screen><para>For
              example: You can execute the following command to get the alarm ID, if the Compute
              proxy appears as a preceding example.</para>
<screen>monasca alarm-list --metric-dimensions hostname=helion-cp1-comp0001-mgmt</screen>
</listitem>
          <listitem><para>Delete the Monasca alarm</para>
<screen>monasca alarm-delete &lt;alarm ID&gt;</screen></listitem>
        </orderedlist>
</sidebar>
    <sidebar><title><emphasis role="bold">Cleaning the monasca alarms related to ESX proxy and vCenter cluster</emphasis></title><para>Perform the following procedure:</para>
<orderedlist xml:id="ol_e2x_zbk_xx">
          <listitem xml:id="idg-all-esx-remove_existing_cluster_compute_resource_pool-xml-17"><para>Using the ESX proxy hostname, execute the following command to list all
            alarms.
              </para>
<screen>monasca alarm-list --metric-dimensions hostname=&lt;compute node deleted&gt;</screen><para>where
              &lt;compute node deleted&gt; - hostname is taken from the vSphere client (refer <link xlink:href="#topic_lfq_3wf_rt/preq">prerequisite</link>).</para>
<para>For example, the compute proxy hostname is
                <literal>MCP-VCP-cpesx-esx-comp0001-mgmt</literal>.</para>
<screen>monasca alarm-list --metric-dimensions hostname=MCP-VCP-cpesx-esx-comp0001-mgmt
stack@R28N6340-701-cp1-c1-m1-mgmt:~$ monasca alarm-list --metric-dimensions hostname=R28N6340-701-cp1-esx-comp0001-mgmt
+--------------------------------------+--------------------------------------+------------------------+------------------------+--------------------------------------------------+----------+-------+-----------------+------+--------------------------+--------------------------+--------------------------+
| id                                   | alarm_definition_id                  | alarm_definition_name  | metric_name            | metric_dimensions                                | severity | state | lifecycle_state | link | state_updated_timestamp  | updated_timestamp        | created_timestamp        |
+--------------------------------------+--------------------------------------+------------------------+------------------------+--------------------------------------------------+----------+-------+-----------------+------+--------------------------+--------------------------+--------------------------+
| 02342bcb-da81-40db-a262-09539523c482 | 3e302297-0a36-4f0e-a1bd-03402b937a4e | HTTP Status            | http_status            | service: compute                                 | HIGH     | OK    | None            | None | 2016-11-11T06:58:11.717Z | 2016-11-11T06:58:11.717Z | 2016-11-10T08:55:45.136Z |
|                                      |                                      |                        |                        | cloud_name: entry-scale-esx-kvm-vsa              |          |       |                 |      |                          |                          |                          |
|                                      |                                      |                        |                        | url: https://10.244.209.9:8774                   |          |       |                 |      |                          |                          |                          |
|                                      |                                      |                        |                        | hostname: R28N6340-701-cp1-esx-comp0001-mgmt     |          |       |                 |      |                          |                          |                          |
|                                      |                                      |                        |                        | component: nova-api                              |          |       |                 |      |                          |                          |                          |
|                                      |                                      |                        |                        | control_plane: control-plane-1                   |          |       |                 |      |                          |                          |                          |
|                                      |                                      |                        |                        | cluster: esx-compute                             |          |       |                 |      |                          |                          |                          |
| 04cb36ce-0c7c-4b4c-9ebc-c4011e2f6c0a | 15c593de-fa54-4803-bd71-afab95b980a4 | Disk Usage             | disk.space_used_perc   | mount_point: /proc/sys/fs/binfmt_misc            | HIGH     | OK    | None            | None | 2016-11-10T08:52:52.886Z | 2016-11-10T08:52:52.886Z | 2016-11-10T08:51:29.197Z |
|                                      |                                      |                        |                        | service: system                                  |          |       |                 |      |                          |                          |                          |
|                                      |                                      |                        |                        | cloud_name: entry-scale-esx-kvm-vsa              |          |       |                 |      |                          |                          |                          |
|                                      |                                      |                        |                        | hostname: R28N6340-701-cp1-esx-comp0001-mgmt     |          |       |                 |      |                          |                          |                          |
|                                      |                                      |                        |                        | control_plane: control-plane-1                   |          |       |                 |      |                          |                          |                          |
|                                      |                                      |                        |                        | cluster: esx-compute                             |          |       |                 |      |                          |                          |                          |
|                                      |                                      |                        |                        | device: systemd-1                                |          |       |                 |      |                          |                          |                          |
+--------------------------------------+--------------------------------------+------------------------+------------------------+--------------------------------------------------+----------+-------+-----------------+------+--------------------------+--------------------------+--------------------------+</screen>
</listitem>
          <listitem><para>Delete the alarm using the alarm
              IDs.</para>
<screen>monasca alarm-delete &lt;alarm ID&gt;</screen><para>This step has to be
              performed for all alarm IDs listed from the preceding step (<link xlink:href="#topic_lfq_3wf_rt/one">1</link>).</para>
<para>For
              Example:</para>
<screen>monasca alarm-delete 1cc219b1-ce4d-476b-80c2-0cafa53e1a12</screen>
</listitem>
        </orderedlist>
</sidebar>
    <sidebar><title>[Optional]  Force Deactivate (if there is any irreversible error during
        deactivation)</title><para>Cleans-up the OVSvApp and Compute Proxy VM's and updates
        database.</para>
<screen>eon resource-deactivate --force true &lt;RESOURCE_ID&gt;</screen>
<important><para>If there is any error while cleaning the OVSvApp and Compute Proxy
          VM's, the exceptions are ignored and cluster are set to the imported state. If the cluster
          is not moved back to the imported state, the cluster cannot be activated again. In this
          case, you need to manually clean-up the VM's.</para>
<para>Executing force deactivation of resource
            command does not clean-up the input model. You must perform the following steps to
            clean-up the input model:</para>
<orderedlist xml:id="ol_gv4_2kd_xw">
              <listitem><para>Login to the lifecycle manager.</para>
</listitem>
              <listitem><para>Remove the entries manually from
                  <literal>~/helion/my_cloud/definition/data/servers.yml</literal> and
                  <literal>~/helion/my_cloud/definition/data/passthrough.yml</literal> files.</para>
</listitem>
              <listitem><para>Commit your
                changes</para>
<screen>cd /home/stack/helion/hos/ansible
git add -A
git commit -m "commit message&gt;"</screen></listitem>
              <listitem><para>Run the configuration processor
                </para>
<screen>cd ~/helion/hos/ansible
ansible-playbook -i hosts/localhost config-processor-run.yml -e remove_deleted_servers="y" -e free_unused_addresses="y"</screen></listitem>
              <listitem><para>Run ready
                deployment</para>
<screen>ansible-playbook -i hosts/localhost ready-deployment.yml</screen></listitem>
            </orderedlist>
</important>
</sidebar>
  </section>
