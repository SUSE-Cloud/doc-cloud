<?xml version="1.0" encoding="UTF-8" standalone="no"?>
<chapter xml:id="ironic-user-guide" xmlns="http://docbook.org/ns/docbook" xmlns:xlink="http://www.w3.org/1999/xlink" xmlns:xi="http://www.w3.org/2001/XInclude" version="5.1">
    <title>User Guide</title>
    <section xml:id="user-guide" xml:base="user/index">
      <title>Bare Metal Service User Guide</title>
      <para>Ironic is an OpenStack project which provisions bare metal (as opposed to
            virtual) machines. It may be used independently or as part of an OpenStack
            Cloud, and integrates with the OpenStack Identity (keystone), Compute (nova),
            Network (neutron), Image (glance) and Object (swift) services.</para>
      <para>When the Bare Metal service is appropriately configured with the Compute and
            Network services, it is possible to provision both virtual and physical
            machines through the Compute service’s API. However, the set of instance
            actions is limited, arising from the different characteristics of physical
            servers and switch hardware. For example, live migration can not be performed
            on a bare metal instance.</para>
      <para>The community maintains reference drivers that leverage open-source
            technologies (eg. PXE and IPMI) to cover a wide range of hardware. Ironic’s
            pluggable driver architecture also allows hardware vendors to write and
            contribute drivers that may improve performance or add functionality not
            provided by the community drivers.</para>
      <section xml:id="why-provision-bare-metal">
        <title>Why Provision Bare Metal</title>
        <para>Here are a few use-cases for bare metal (physical server) provisioning in
                cloud; there are doubtless many more interesting ones:</para>
        <itemizedlist>
          <listitem>
            <para>High-performance computing clusters</para>
          </listitem>
          <listitem>
            <para>Computing tasks that require access to hardware devices which can’t be
                        virtualized</para>
          </listitem>
          <listitem>
            <para>Database hosting (some databases run poorly in a hypervisor)</para>
          </listitem>
          <listitem>
            <para>Single tenant, dedicated hardware for performance, security, dependability
                        and other regulatory requirements</para>
          </listitem>
          <listitem>
            <para>Or, rapidly deploying a cloud infrastructure</para>
          </listitem>
        </itemizedlist>
      </section>
      <section xml:id="conceptual-architecture">
        <title>Conceptual Architecture</title>
        <para>The following diagram shows the relationships and how all services come into
                play during the provisioning of a physical server. (Note that Ceilometer and
                Swift can be used with Ironic, but are missing from this diagram.)</para>
        <informalfigure><mediaobject>
          <imageobject role="fo">
            <imagedata fileref="conceptual_architecture.png"/>
          </imageobject>
          <imageobject role="html">
            <imagedata fileref="conceptual_architecture.png"/>
          </imageobject>
        </mediaobject></informalfigure>
      </section>
      <section xml:id="logical-architecture">
        <title>Logical Architecture</title>
        <para>The diagram below shows the logical architecture. It shows the basic
                components that form the Ironic service, the relation of Ironic service with
                other OpenStack services and the logical flow of a boot instance request
                resulting in the provisioning of a physical server.</para>
        <informalfigure><mediaobject>
          <imageobject role="fo">
            <imagedata fileref="logical_architecture.png"/>
          </imageobject>
          <imageobject role="html">
            <imagedata fileref="logical_architecture.png"/>
          </imageobject>
        </mediaobject></informalfigure>
        <para>The Ironic service is composed of the following components:</para>
        <procedure>
          <step>
            <para>a RESTful API service, by which operators and other services may interact
                        with the managed bare metal servers.</para>
          </step>
          <step>
            <para>a Conductor service, which does the bulk of the work. Functionality is
                        exposed via the API service. The Conductor and API services communicate
                        via RPC.</para>
          </step>
          <step>
            <para>various Drivers that support heterogeneous hardware</para>
          </step>
          <step>
            <para>a Message Queue</para>
          </step>
          <step>
            <para>a Database for storing information about the resources. Among other things,
                        this includes the state of the conductors, nodes (physical servers), and
                        drivers.</para>
          </step>
        </procedure>
        <para>As in Figure 1.2. Logical Architecture, a user request to boot an instance is
                passed to the Nova Compute service via Nova API and Nova Scheduler. The Compute
                service hands over this request to the Ironic service, where the request passes
                from the Ironic API, to the Conductor, to a Driver to successfully provision a
                physical server for the user.</para>
        <para>Just as the Nova Compute service talks to various OpenStack services like
                Glance, Neutron, Swift etc to provision a virtual machine instance, here the
                Ironic service talks to the same OpenStack services for image, network and
                other resource needs to provision a bare metal instance.</para>
      </section>
      <section xml:id="key-technologies-for-bare-metal-hosting">
        <title>Key Technologies for Bare Metal Hosting</title>
        <section xml:id="preboot-execution-environment-pxe">
          <title>Preboot Execution Environment (PXE)</title>
          <para>PXE is part of the Wired for Management (WfM) specification developed by Intel
                    and Microsoft. The PXE enables system’s BIOS and network interface card (NIC)
                    to bootstrap a computer from the network in place of a disk. Bootstrapping is
                    the process by which a system loads the OS into local memory so that it can be
                    executed by the processor. This capability of allowing a system to boot over a
                    network simplifies server deployment and server management for administrators.</para>
        </section>
        <section xml:id="dynamic-host-configuration-protocol-dhcp">
          <title>Dynamic Host Configuration Protocol (DHCP)</title>
          <para>DHCP is a standardized networking protocol used on Internet Protocol (IP)
                    networks for dynamically distributing network configuration parameters, such
                    as IP addresses for interfaces and services. Using PXE, the BIOS uses DHCP to
                    obtain an IP address for the network interface and to locate the server that
                    stores the network bootstrap program (NBP).</para>
        </section>
        <section xml:id="network-bootstrap-program-nbp">
          <title>Network Bootstrap Program (NBP)</title>
          <para>NBP is equivalent to GRUB (GRand Unified Bootloader) or LILO (LInux LOader) -
                    loaders which are traditionally used in local booting. Like the boot program
                    in a hard drive environment, the NBP is responsible for loading the OS kernel
                    into memory so that the OS can be bootstrapped over a network.</para>
        </section>
        <section xml:id="trivial-file-transfer-protocol-tftp">
          <title>Trivial File Transfer Protocol (TFTP)</title>
          <para>TFTP is a simple file transfer protocol that is generally used for automated
                    transfer of configuration or boot files between machines in a local
                    environment.  In a PXE environment, TFTP is used to download NBP over the
                    network using information from the DHCP server.</para>
        </section>
        <section xml:id="intelligent-platform-management-interface-ipmi">
          <title>Intelligent Platform Management Interface (IPMI)</title>
          <para>IPMI is a standardized computer system interface used by system administrators
                    for out-of-band management of computer systems and monitoring of their
                    operation. It is a method to manage systems that may be unresponsive or powered
                    off by using only a network connection to the hardware rather than to an
                    operating system.</para>
        </section>
      </section>
      <section xml:id="ironic-deployment-architecture">
        <title>Ironic Deployment Architecture</title>
        <para>The Ironic RESTful API service is used to enroll hardware that Ironic will
                manage. A cloud administrator usually registers the hardware, specifying their
                attributes such as MAC addresses and IPMI credentials. There can be multiple
                instances of the API service.</para>
        <para>The Ironic conductor service does the bulk of the work.
                For security reasons, it is advisable to place the conductor service on
                an isolated host, since it is the only service that requires access to both
                the data plane and IPMI control plane.</para>
        <para>There can be multiple instances of the conductor service to support
                various class of drivers and also to manage fail over. Instances of the
                conductor service should be on separate nodes. Each conductor can itself run
                many drivers to operate heterogeneous hardware. This is depicted in the
                following figure.</para>
        <para>The API exposes a list of supported drivers and the names of conductor hosts
                servicing them.</para>
        <informalfigure><mediaobject>
          <imageobject role="fo">
            <imagedata fileref="deployment_architecture_2.png"/>
          </imageobject>
          <imageobject role="html">
            <imagedata fileref="deployment_architecture_2.png"/>
          </imageobject>
        </mediaobject></informalfigure>
      </section>
      <section xml:id="understanding-bare-metal-deployment">
        <title>Understanding Bare Metal Deployment</title>
        <para>What happens when a boot instance request comes in? The below diagram walks
                through the steps involved during the provisioning of a bare metal instance.</para>
        <para>These pre-requisites must be met before the deployment process:</para>
        <itemizedlist>
          <listitem>
            <para>Dependent packages to be configured on the Bare Metal service node(s)
                        where ironic-conductor is running like tftp-server, ipmi, syslinux etc for
                        bare metal provisioning.</para>
          </listitem>
          <listitem>
            <para>Nova must be configured to make use of the bare metal service endpoint
                        and compute driver should be configured to use ironic driver on the Nova
                        compute node(s).</para>
          </listitem>
          <listitem>
            <para>Flavors to be created for the available hardware. Nova must know the flavor
                        to boot from.</para>
          </listitem>
          <listitem>
            <para>Images to be made available in Glance. Listed below are some image types
                        required for successful bare metal deployment:</para>
            <itemizedlist>
              <listitem>
                <para>bm-deploy-kernel</para>
              </listitem>
              <listitem>
                <para>bm-deploy-ramdisk</para>
              </listitem>
              <listitem>
                <para>user-image</para>
              </listitem>
              <listitem>
                <para>user-image-vmlinuz</para>
              </listitem>
              <listitem>
                <para>user-image-initrd</para>
              </listitem>
            </itemizedlist>
          </listitem>
          <listitem>
            <para>Hardware to be enrolled via Ironic RESTful API service.</para>
          </listitem>
        </itemizedlist>
<informalfigure>        <mediaobject>
          <imageobject role="fo">
            <imagedata fileref="deployment_steps.png"/>
          </imageobject>
          <imageobject role="html">
            <imagedata fileref="deployment_steps.png"/>
          </imageobject>
        </mediaobject></informalfigure>
        <section xml:id="deploy-process">
          <title>Deploy Process</title>
          <procedure>
            <step>
              <para>A boot instance request comes in via the Nova API, through the message
                            queue to the Nova scheduler.</para>
            </step>
            <step>
              <para>Nova scheduler applies filter and finds the eligible compute node. Nova
                            scheduler uses flavor extra_specs detail such as ‘cpu_arch’,
                            ‘baremetal:deploy_kernel_id’, ‘baremetal:deploy_ramdisk_id’ etc to match
                            the target physical node.</para>
            </step>
            <step>
              <para>A spawn task is placed by the driver which contains all information such
                            as which image to boot from etc. It invokes the driver.spawn from the
                            virt layer of Nova compute.</para>
            </step>
            <step>
              <para>Information about the bare metal node is retrieved from the bare metal
                            database and the node is reserved.</para>
            </step>
            <step>
              <para>Images from Glance are pulled down to the local disk of the Ironic
                            conductor servicing the bare metal node.</para>
              <substeps>
                <step>
                  <para>For pxe_* drivers these include all images: both the deploy ramdisk and
                                    user instance images.</para>
                </step>
                <step>
                  <para>For agent_* drivers only the deploy ramdisk is stored locally. Temporary
                                    URLs in OpenStack’s Object Storage service are created for user instance
                                    images.</para>
                </step>
              </substeps>
            </step>
            <step>
              <para>Virtual interfaces are plugged in and Neutron API updates DHCP port to
                            support PXE/TFTP options.</para>
            </step>
            <step>
              <para>Nova’s ironic driver issues a deploy request via the Ironic API to the
                            Ironic conductor servicing the bare metal node.</para>
            </step>
            <step>
              <para>PXE driver prepares tftp bootloader.</para>
            </step>
            <step>
              <para>The IPMI driver issues command to enable network boot of a node and power
                            it on.</para>
            </step>
            <step>
              <para>The DHCP boots the deploy ramdisk. Next, depending on the exact driver
                            used, either the conductor copies the image over iSCSI to the physical node
                            (pxe_* group of drivers) or the deploy ramdisk downloads the image from
                            a temporary URL (agent_* group of drivers), which can be generated by
                            a variety of object stores, e.g. <emphasis>swift</emphasis>, <emphasis>radosgw</emphasis>, etc, and uploaded
                            to OpenStack’s Object Storage service. In the former case, the conductor
                            connects to the iSCSI end point, partitions volume, “dd” the image and
                            closes the iSCSI connection.</para>
              <para>The deployment is done. The Ironic conductor will switch pxe config to service
                            mode and notify ramdisk agent on the successful deployment.</para>
            </step>
            <step>
              <para>The IPMI driver reboots the bare metal node. Note that there are 2 power
                            cycles during bare metal deployment; the first time when powered-on, the
                            images get deployed as mentioned in step 9. The second time as in this case,
                            after the images are deployed, the node is powered up.</para>
            </step>
            <step>
              <para>The bare metal node status is updated and the node instance is made
                            available.</para>
            </step>
          </procedure>
        </section>
        <section xml:id="example-1-pxe-boot-and-iscsi-deploy-process">
          <title>Example 1: PXE Boot and iSCSI Deploy Process</title>
          <para>This process is used with pxe_* family of drivers.</para>
          <informalfigure>
            <mediaobject>
              <imageobject role="fo">
                <imagedata fileref="seqdiag-e30ed1fcd13b15eeee05c69651c2169ae8754f24.png"/>
              </imageobject>
              <imageobject role="html">
                <imagedata fileref="seqdiag-e30ed1fcd13b15eeee05c69651c2169ae8754f24.png"/>
              </imageobject>
            </mediaobject>
          </informalfigure>
          <para>(From a <link xmlns:xl="http://www.w3.org/1999/xlink" xl:href="https://www.openstack.org/summit/vancouver-2015/summit-videos/presentation/isn-and-039t-it-ironic-the-bare-metal-cloud">talk</link>  and <link xmlns:xl="http://www.w3.org/1999/xlink" xl:href="http://www.slideshare.net/devananda1/isnt-it-ironic-managing-a-bare-metal-cloud-osl-tes-2015">slides</link>)</para>
        </section>
        <section xml:id="example-2-pxe-boot-and-direct-deploy-process">
          <title>Example 2: PXE Boot and Direct Deploy Process</title>
          <para>This process is used with agent_* family of drivers.</para>
          <informalfigure>
            <mediaobject>
              <imageobject role="fo">
                <imagedata fileref="seqdiag-1de5120a379728d9975fd02eab7102fd1256e91b.png"/>
              </imageobject>
              <imageobject role="html">
                <imagedata fileref="seqdiag-1de5120a379728d9975fd02eab7102fd1256e91b.png"/>
              </imageobject>
            </mediaobject>
          </informalfigure>
          <para>(From a <link xmlns:xl="http://www.w3.org/1999/xlink" xl:href="https://www.openstack.org/summit/vancouver-2015/summit-videos/presentation/isn-and-039t-it-ironic-the-bare-metal-cloud">talk</link>  and <link xmlns:xl="http://www.w3.org/1999/xlink" xl:href="http://www.slideshare.net/devananda1/isnt-it-ironic-managing-a-bare-metal-cloud-osl-tes-2015">slides</link>)</para>
        </section>
      </section>
    </section>
  </chapter>
