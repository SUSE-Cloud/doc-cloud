<?xml version="1.0" encoding="UTF-8" standalone="no"?>
<chapter xml:id="magnum-user-guide" xmlns="http://docbook.org/ns/docbook" xmlns:xlink="http://www.w3.org/1999/xlink" xmlns:xi="http://www.w3.org/2001/XInclude" version="5.1">
      <title>Magnum User Documentation</title>
      <section xml:base="user/index">
        <title>Introduction</title>
        <para>This guide is intended for users who use Magnum to deploy and manage clusters
            of hosts for a Container Orchestration Engine.  It describes the infrastructure
            that Magnum creates and how to work with them.</para>
        <para>Section 1-3 describe Magnum itself, including an overview, the CLI and
            Horizon interface.  Section 4-9 describe the Container Orchestration
            Engine (COE) supported along with a guide on how to select one that
            best meets your needs and how to develop a driver for a new COE.
            Section 10-15 describe the low level OpenStack infrastructure that is
            created and managed by Magnum to support the COE’s.</para>
      </section>
      <section xml:base="user/index">
        <title>Terminology</title>
        <variablelist>
          <varlistentry>
            <term>Cluster (previously Bay)</term>
            <listitem>
              <para>A cluster is the construct in which Magnum launches container orchestration
                        engines. After a cluster has been created the user is able to add containers
                        to it either directly, or in the case of the Kubernetes container
                        orchestration engine within pods - a logical construct specific to that
                        implementation. A cluster is created based on a ClusterTemplate.</para>
            </listitem>
          </varlistentry>
          <varlistentry>
            <term>ClusterTemplate (previously BayModel)</term>
            <listitem>
              <para>A ClusterTemplate in Magnum is roughly equivalent to a flavor in Nova. It
                        acts as a template that defines options such as the container orchestration
                        engine, keypair and image for use when Magnum is creating clusters using
                        the given ClusterTemplate.</para>
            </listitem>
          </varlistentry>
          <varlistentry>
            <term>Container Orchestration Engine (COE)</term>
            <listitem>
              <para>A container orchestration engine manages the lifecycle of one or more
                        containers, logically represented in Magnum as a cluster. Magnum supports a
                        number of container orchestration engines, each with their own pros and cons,
                        including Docker Swarm, Kubernetes, and Mesos.</para>
            </listitem>
          </varlistentry>
        </variablelist>
      </section>
      <section xml:base="user/index">
        <title>Overview</title>
        <para>Magnum is an OpenStack API service developed by the OpenStack Containers Team
            making container orchestration engines (COE) such as Docker Swarm, Kubernetes
            and Apache Mesos available as the first class resources in OpenStack.</para>
        <para>Magnum uses Heat to orchestrate an OS image which contains Docker and COE
            and runs that image in either virtual machines or bare metal in a cluster
            configuration.</para>
        <para>Magnum offers complete life-cycle management of COEs in an
            OpenStack environment, integrated with other OpenStack services for a seamless
            experience for OpenStack users who wish to run containers in an OpenStack
            environment.</para>
        <para>Following are few salient features of Magnum:</para>
        <itemizedlist>
          <listitem>
            <para>Standard API based complete life-cycle management for Container Clusters</para>
          </listitem>
          <listitem>
            <para>Multi-tenancy for container clusters</para>
          </listitem>
          <listitem>
            <para>Choice of COE: Kubernetes, Swarm, Mesos, DC/OS</para>
          </listitem>
          <listitem>
            <para>Choice of container cluster deployment model: VM or Bare-metal</para>
          </listitem>
          <listitem>
            <para>Keystone-based multi-tenant security and auth management</para>
          </listitem>
          <listitem>
            <para>Neutron based multi-tenant network control and isolation</para>
          </listitem>
          <listitem>
            <para>Cinder based volume service for containers</para>
          </listitem>
          <listitem>
            <para>Integrated with OpenStack: SSO experience for cloud users</para>
          </listitem>
          <listitem>
            <para>Secure container cluster access (TLS enabled)</para>
          </listitem>
        </itemizedlist>
        <para>More details: <link xlink:href="https://wiki.openstack.org/wiki/Magnum">Magnum Project Wiki</link></para>
      </section>
      <section xml:base="user/index">
        <title>ClusterTemplate</title>
        <para>A ClusterTemplate (previously known as BayModel) is a collection of parameters
            to describe how a cluster can be constructed.  Some parameters are relevant to
            the infrastructure of the cluster, while others are for the particular COE.  In
            a typical workflow, a user would create a ClusterTemplate, then create one or
            more clusters using the ClusterTemplate.  A cloud provider can also define a
            number of ClusterTemplates and provide them to the users.  A ClusterTemplate
            cannot be updated or deleted if a cluster using this ClusterTemplate still
            exists.</para>
        <para>The definition and usage of the parameters of a ClusterTemplate are as follows.
            They are loosely grouped as: mandatory, infrastructure, COE specific.</para>
        <variablelist>
          <varlistentry>
            <term>&lt;name&gt;</term>
            <listitem>
              <para>Name of the ClusterTemplate to create.  The name does not have to be
                        unique.  If multiple ClusterTemplates have the same name, you will need to
                        use the UUID to select the ClusterTemplate when creating a cluster or
                        updating, deleting a ClusterTemplate.  If a name is not specified, a random
                        name will be generated using a string and a number, for example
                        “pi-13-model”.</para>
            </listitem>
          </varlistentry>
          <varlistentry>
            <term>–coe &lt;coe&gt;</term>
            <listitem>
              <para>Specify the Container Orchestration Engine to use.  Supported
                        COE’s include ‘kubernetes’, ‘swarm’, ‘mesos’.  If your environment
                        has additional cluster drivers installed, refer to the cluster driver
                        documentation for the new COE names.  This is a mandatory parameter
                        and there is no default value.</para>
            </listitem>
          </varlistentry>
          <varlistentry>
            <term>–image &lt;image&gt;</term>
            <listitem>
              <para>The name or UUID of the base image in Glance to boot the servers for
                        the cluster.  The image must have the attribute ‘os_distro’ defined
                        as appropriate for the cluster driver.  For the currently supported
                        images, the os_distro names are:</para>
              <informaltable>
                <tgroup cols="2">
                  <colspec colname="c1" colwidth="10"/>
                  <colspec colname="c2" colwidth="21"/>
                  <thead>
                    <row>
                      <entry>
                        <para>COE</para>
                      </entry>
                      <entry>
                        <para>os-distro</para>
                      </entry>
                    </row>
                  </thead>
                  <tbody>
                    <row>
                      <entry>
                        <para>Kubernetes</para>
                      </entry>
                      <entry>
                        <para>Fedora-atomic, CoreOS</para>
                      </entry>
                    </row>
                    <row>
                      <entry>
                        <para>Swarm</para>
                      </entry>
                      <entry>
                        <para>Fedora-atomic</para>
                      </entry>
                    </row>
                    <row>
                      <entry>
                        <para>Mesos</para>
                      </entry>
                      <entry>
                        <para>Ubuntu</para>
                      </entry>
                    </row>
                  </tbody>
                </tgroup>
              </informaltable>
              <para>This is a mandatory parameter and there is no default value.</para>
            </listitem>
          </varlistentry>
          <varlistentry>
            <term>–keypair &lt;keypair&gt;</term>
            <listitem>
              <para>The name of the SSH keypair to configure in the cluster servers
                        for ssh access.  You will need the key to be able to ssh to the
                        servers in the cluster.  The login name is specific to the cluster
                        driver. If keypair is not provided in template it will be required at
                        Cluster create. This value will be overridden by any keypair value that
                        is provided during Cluster create.</para>
            </listitem>
          </varlistentry>
          <varlistentry>
            <term>–external-network &lt;external-network&gt;</term>
            <listitem>
              <para>The name or network ID of a Neutron network to provide connectivity
                        to the external internet for the cluster.  This network must be an
                        external network, i.e. its attribute ‘router:external’ must be
                        ‘True’.  The servers in the cluster will be connected to a private
                        network and Magnum will create a router between this private network
                        and the external network.  This will allow the servers to download
                        images, access discovery service, etc, and the containers to install
                        packages, etc.  In the opposite direction, floating IP’s will be
                        allocated from the external network to provide access from the
                        external internet to servers and the container services hosted in
                        the cluster.  This is a mandatory parameter and there is no default
                        value.</para>
            </listitem>
          </varlistentry>
        </variablelist>
        <variablelist>
          <varlistentry>
            <term>
              <option>--public</option>
            </term>
            <listitem>
              <para>Access to a ClusterTemplate is normally limited to the admin, owner or users
                        within the same tenant as the owners.  Setting this flag
                        makes the ClusterTemplate public and accessible by other users.  The default
                        is not public.</para>
            </listitem>
          </varlistentry>
        </variablelist>
        <variablelist>
          <varlistentry>
            <term>–server-type &lt;server-type&gt;</term>
            <listitem>
              <para>The servers in the cluster can be VM or baremetal.  This parameter selects
                        the type of server to create for the cluster.  The default is ‘vm’. Possible
                        values are ‘vm’, ‘bm’.</para>
            </listitem>
          </varlistentry>
          <varlistentry>
            <term>–network-driver &lt;network-driver&gt;</term>
            <listitem>
              <para>The name of a network driver for providing the networks for the
                        containers.  Note that this is different and separate from the Neutron
                        network for the cluster.  The operation and networking model are specific
                        to the particular driver.<!-- ; refer to the <xref linkend="networking"/> section for more
                        details. --> Supported network drivers and the default driver are:</para>
              <informaltable>
                <tgroup cols="3">
                  <colspec colname="c1" colwidth="11"/>
                  <colspec colname="c2" colwidth="17"/>
                  <colspec colname="c3" colwidth="8"/>
                  <thead>
                    <row>
                      <entry>
                        <para>COE</para>
                      </entry>
                      <entry>
                        <para>Network-Driver</para>
                      </entry>
                      <entry>
                        <para>Default</para>
                      </entry>
                    </row>
                  </thead>
                  <tbody>
                    <row>
                      <entry>
                        <para>Kubernetes</para>
                      </entry>
                      <entry>
                        <para>Flannel</para>
                      </entry>
                      <entry>
                        <para>Flannel</para>
                      </entry>
                    </row>
                    <row>
                      <entry>
                        <para>Swarm</para>
                      </entry>
                      <entry>
                        <para>Docker, Flannel</para>
                      </entry>
                      <entry>
                        <para>Flannel</para>
                      </entry>
                    </row>
                    <row>
                      <entry>
                        <para>Mesos</para>
                      </entry>
                      <entry>
                        <para>Docker</para>
                      </entry>
                      <entry>
                        <para>Docker</para>
                      </entry>
                    </row>
                  </tbody>
                </tgroup>
              </informaltable>
            </listitem>
          </varlistentry>
          <varlistentry>
            <term>–volume-driver &lt;volume-driver&gt;</term>
            <listitem>
              <para>The name of a volume driver for managing the persistent storage for
                        the containers.  The functionality supported are specific to the
                        driver.  Supported volume drivers and the default driver are:</para>
              <informaltable>
                <tgroup cols="3">
                  <colspec colname="c1" colwidth="13"/>
                  <colspec colname="c2" colwidth="13"/>
                  <colspec colname="c3" colwidth="11"/>
                  <thead>
                    <row>
                      <entry>
                        <para>COE</para>
                      </entry>
                      <entry>
                        <para>Volume-Driver</para>
                      </entry>
                      <entry>
                        <para>Default</para>
                      </entry>
                    </row>
                  </thead>
                  <tbody>
                    <row>
                      <entry>
                        <para>Kubernetes</para>
                      </entry>
                      <entry>
                        <para>Cinder</para>
                      </entry>
                      <entry>
                        <para>No Driver</para>
                      </entry>
                    </row>
                    <row>
                      <entry>
                        <para>Swarm</para>
                      </entry>
                      <entry>
                        <para>Rexray</para>
                      </entry>
                      <entry>
                        <para>No Driver</para>
                      </entry>
                    </row>
                    <row>
                      <entry>
                        <para>Mesos</para>
                      </entry>
                      <entry>
                        <para>Rexray</para>
                      </entry>
                      <entry>
                        <para>No Driver</para>
                      </entry>
                    </row>
                  </tbody>
                </tgroup>
              </informaltable>
            </listitem>
          </varlistentry>
          <varlistentry>
            <term>–dns-nameserver &lt;dns-nameserver&gt;</term>
            <listitem>
              <para>The DNS nameserver for the servers and containers in the cluster to use.
                        This is configured in the private Neutron network for the cluster.  The
                        default is ‘8.8.8.8’.</para>
            </listitem>
          </varlistentry>
          <varlistentry>
            <term>–flavor &lt;flavor&gt;</term>
            <listitem>
              <para>The nova flavor id for booting the node servers.  The default
                        is ‘m1.small’.</para>
            </listitem>
          </varlistentry>
          <varlistentry>
            <term>–master-flavor &lt;master-flavor&gt;</term>
            <listitem>
              <para>The nova flavor id for booting the master or manager servers.  The
                        default is ‘m1.small’.</para>
            </listitem>
          </varlistentry>
          <varlistentry>
            <term>–http-proxy &lt;http-proxy&gt;</term>
            <listitem>
              <para>The IP address for a proxy to use when direct http access from the
                        servers to sites on the external internet is blocked.  This may
                        happen in certain countries or enterprises, and the proxy allows the
                        servers and containers to access these sites.  The format is a URL
                        including a port number.  The default is ‘None’.</para>
            </listitem>
          </varlistentry>
          <varlistentry>
            <term>–https-proxy &lt;https-proxy&gt;</term>
            <listitem>
              <para>The IP address for a proxy to use when direct https access from the
                        servers to sites on the external internet is blocked.  This may
                        happen in certain countries or enterprises, and the proxy allows the
                        servers and containers to access these sites.  The format is a URL
                        including a port number.  The default is ‘None’.</para>
            </listitem>
          </varlistentry>
          <varlistentry>
            <term>–no-proxy &lt;no-proxy&gt;</term>
            <listitem>
              <para>When a proxy server is used, some sites should not go through the
                        proxy and should be accessed normally.  In this case, you can
                        specify these sites as a comma separated list of IP’s.  The default
                        is ‘None’.</para>
            </listitem>
          </varlistentry>
          <varlistentry>
            <term>–docker-volume-size &lt;docker-volume-size&gt;</term>
            <listitem>
              <para>If specified, container images will be stored in a cinder volume of the
                        specified size in GB. Each cluster node will have a volume attached of
                        the above size. If not specified, images will be stored in the compute
                        instance’s local disk. For the ‘devicemapper’ storage driver, the minimum
                        value is 3GB. For the ‘overlay’ storage driver, the minimum value is 1GB.
                        This value can be overridden at cluster creation.</para>
            </listitem>
          </varlistentry>
          <varlistentry>
            <term>–docker-storage-driver &lt;docker-storage-driver&gt;</term>
            <listitem>
              <para>The name of a driver to manage the storage for the images and the
                        container’s writable layer.  The supported drivers are ‘devicemapper’
                        and ‘overlay’.  The default is ‘devicemapper’.</para>
            </listitem>
          </varlistentry>
          <varlistentry>
            <term>–labels &lt;KEY1=VALUE1,KEY2=VALUE2;KEY3=VALUE3…&gt;</term>
            <listitem>
              <para>Arbitrary labels in the form of key=value pairs.  The accepted keys
                        and valid values are defined in the cluster drivers.  They are used as a
                        way to pass additional parameters that are specific to a cluster driver.
                        Refer to the subsection on labels for a list of the supported
                        key/value pairs and their usage.</para>
            </listitem>
          </varlistentry>
        </variablelist>
        <variablelist>
          <varlistentry>
            <term>
              <option>--tls-disabled</option>
            </term>
            <listitem>
              <para>Transport Layer Security (TLS) is normally enabled to secure the
                        cluster.  In some cases, users may want to disable TLS in the cluster,
                        for instance during development or to troubleshoot certain problems.
                        Specifying this parameter will disable TLS so that users can access
                        the COE endpoints without a certificate.  The default is TLS
                        enabled.</para>
            </listitem>
          </varlistentry>
          <varlistentry>
            <term>
              <option>--registry-enabled</option>
            </term>
            <listitem>
              <para>Docker images by default are pulled from the public Docker registry,
                        but in some cases, users may want to use a private registry.  This
                        option provides an alternative registry based on the Registry V2:
                        Magnum will create a local registry in the cluster backed by swift to
                        host the images.  Refer to
                        <link xlink:href="https://github.com/docker/distribution">Docker Registry 2.0</link>
                        for more details.  The default is to use the public registry.</para>
            </listitem>
          </varlistentry>
          <varlistentry>
            <term>
              <option>--master-lb-enabled</option>
            </term>
            <listitem>
              <para>Since multiple masters may exist in a cluster, a load balancer is
                        created to provide the API endpoint for the cluster and to direct
                        requests to the masters.  In some cases, such as when the LBaaS
                        service is not available, this option can be set to ‘false’ to
                        create a cluster without the load balancer.  In this case, one of the
                        masters will serve as the API endpoint.  The default is ‘true’,
                        i.e. to create the load balancer for the cluster.</para>
            </listitem>
          </varlistentry>
        </variablelist>
        <section>
          <title>Labels</title>
          <para>Labels is a general method to specify supplemental parameters that are
                specific to certain COE or associated with certain options.  Their
                format is key/value pair and their meaning is interpreted by the
                drivers that uses them.  The drivers do validate the key/value pairs.
                Their usage is explained in details in the appropriate sections,
                however, since there are many possible labels, the following table
                provides a summary to help give a clearer picture.  The label keys in
                the table are linked to more details elsewhere in the user guide.</para>
          <informaltable>
            <tgroup cols="3">
              <colspec colname="c1" colwidth="39"/>
              <colspec colname="c2" colwidth="20"/>
              <colspec colname="c3" colwidth="15"/>
              <thead>
                <row>
                  <entry>
                    <para>label key</para>
                  </entry>
                  <entry>
                    <para>label value</para>
                  </entry>
                  <entry>
                    <para>default</para>
                  </entry>
                </row>
              </thead>
              <tbody>
                <row>
                  <entry>
                    <para>
                      <literal>flannel-network-cidr</literal>
                    </para>
                  </entry>
                  <entry>
                    <para>IPv4 CIDR</para>
                  </entry>
                  <entry>
                    <para>10.100.0.0/16</para>
                  </entry>
                </row>
                <row>
                  <entry>
                    <para>
                      <literal>flannel-backend</literal>
                    </para>
                  </entry>
                  <entry>
                    <itemizedlist>
                      <listitem>
                        <para>udp</para>
                      </listitem>
                      <listitem>
                        <para>vxlan</para>
                      </listitem>
                      <listitem>
                        <para>host-gw</para>
                      </listitem>
                    </itemizedlist>
                  </entry>
                  <entry>
                    <para>udp</para>
                  </entry>
                </row>
                <row>
                  <entry>
                    <para>
                      <literal>flannel-network-subnetlen</literal>
                    </para>
                  </entry>
                  <entry>
                    <para>size of subnet to
                                    assign to node</para>
                  </entry>
                  <entry>
                    <para>24</para>
                  </entry>
                </row>
                <row>
                  <entry>
                    <para>
                      <literal>rexray-preempt</literal>
                    </para>
                  </entry>
                  <entry>
                    <itemizedlist>
                      <listitem>
                        <para>true</para>
                      </listitem>
                      <listitem>
                        <para>false</para>
                      </listitem>
                    </itemizedlist>
                  </entry>
                  <entry>
                    <para>false</para>
                  </entry>
                </row>
                <row>
                  <entry>
                    <para>
                      <literal>mesos-slave-isolation</literal>
                    </para>
                  </entry>
                  <entry>
                    <itemizedlist>
                      <listitem>
                        <para>filesystem/posix</para>
                      </listitem>
                      <listitem>
                        <para>filesystem/linux</para>
                      </listitem>
                      <listitem>
                        <para>filesystem/shared</para>
                      </listitem>
                      <listitem>
                        <para>posix/cpu</para>
                      </listitem>
                      <listitem>
                        <para>posix/mem</para>
                      </listitem>
                      <listitem>
                        <para>posix/disk</para>
                      </listitem>
                      <listitem>
                        <para>cgroups/cpu</para>
                      </listitem>
                      <listitem>
                        <para>cgroups/mem</para>
                      </listitem>
                      <listitem>
                        <para>docker/runtime</para>
                      </listitem>
                      <listitem>
                        <para>namespaces/pid</para>
                      </listitem>
                    </itemizedlist>
                  </entry>
                  <entry>
                    <para>“”</para>
                  </entry>
                </row>
                <row>
                  <entry>
                    <para>
                      <literal>mesos-slave-image-providers</literal>
                    </para>
                  </entry>
                  <entry>
                    <itemizedlist>
                      <listitem>
                        <para>appc</para>
                      </listitem>
                      <listitem>
                        <para>docker</para>
                      </listitem>
                      <listitem>
                        <para>appc,docker</para>
                      </listitem>
                    </itemizedlist>
                  </entry>
                  <entry>
                    <para>“”</para>
                  </entry>
                </row>
                <row>
                  <entry>
                    <para>
                      <literal>mesos-slave-work-dir</literal>
                    </para>
                  </entry>
                  <entry>
                    <para>(directory name)</para>
                  </entry>
                  <entry>
                    <para>“”</para>
                  </entry>
                </row>
                <row>
                  <entry>
                    <para>
                      <literal>mesos-slave-executor-env-variables</literal>
                    </para>
                  </entry>
                  <entry>
                    <para>(file name)</para>
                  </entry>
                  <entry>
                    <para>“”</para>
                  </entry>
                </row>
                <row>
                  <entry>
                    <para>
                      <literal>swarm-strategy</literal>
                    </para>
                  </entry>
                  <entry>
                    <itemizedlist>
                      <listitem>
                        <para>spread</para>
                      </listitem>
                      <listitem>
                        <para>binpack</para>
                      </listitem>
                      <listitem>
                        <para>random</para>
                      </listitem>
                    </itemizedlist>
                  </entry>
                  <entry>
                    <para>spread</para>
                  </entry>
                </row>
                <row>
                  <entry>
                    <para>
                      <literal>admission-control-list</literal>
                    </para>
                  </entry>
                  <entry>
                    <para>see below</para>
                  </entry>
                  <entry>
                    <para>see below</para>
                  </entry>
                </row>
                <row>
                  <entry>
                    <para>
                      <literal>prometheus-monitoring</literal>
                    </para>
                  </entry>
                  <entry>
                    <itemizedlist>
                      <listitem>
                        <para>true</para>
                      </listitem>
                      <listitem>
                        <para>false</para>
                      </listitem>
                    </itemizedlist>
                  </entry>
                  <entry>
                    <para>false</para>
                  </entry>
                </row>
                <row>
                  <entry>
                    <para>
                      <literal>grafana-admin-passwd</literal>
                    </para>
                  </entry>
                  <entry>
                    <para>(any string)</para>
                  </entry>
                  <entry>
                    <para>“admin”</para>
                  </entry>
                </row>
                <row>
                  <entry>
                    <para>
                      <literal>kube-tag</literal>
                    </para>
                  </entry>
                  <entry>
                    <para>see below</para>
                  </entry>
                  <entry>
                    <para>see below</para>
                  </entry>
                </row>
                <row>
                  <entry>
                    <para>
                      <literal>kube-dashboard-enabled</literal>
                    </para>
                  </entry>
                  <entry>
                    <itemizedlist>
                      <listitem>
                        <para>true</para>
                      </listitem>
                      <listitem>
                        <para>false</para>
                      </listitem>
                    </itemizedlist>
                  </entry>
                  <entry>
                    <para>true</para>
                  </entry>
                </row>
                <row>
                  <entry>
                    <para>
                      <literal>"docker-volume-type</literal>
                    </para>
                  </entry>
                  <entry>
                    <para>see below</para>
                  </entry>
                  <entry>
                    <para>see below</para>
                  </entry>
                </row>
                <row>
                  <entry>
                    <para>
                      <literal>etcd-volume-size</literal>
                    </para>
                  </entry>
                  <entry>
                    <para>etcd storage
                                    volume size</para>
                  </entry>
                  <entry>
                    <para>0</para>
                  </entry>
                </row>
              </tbody>
            </tgroup>
          </informaltable>
        </section>
      </section>
      <section xml:base="user/index">
        <title>Cluster</title>
        <para>A cluster (previously known as bay) is an instance of the ClusterTemplate
            of a COE.  Magnum deploys a cluster by referring to the attributes
            defined in the particular ClusterTemplate as well as a few additional
            parameters for the cluster.  Magnum deploys the orchestration templates
            provided by the cluster driver to create and configure all the necessary
            infrastructure.  When ready, the cluster is a fully operational COE that
            can host containers.</para>
        <section>
          <title>Infrastructure</title>
          <para>The infrastructure of the cluster consists of the resources provided by
                the various OpenStack services.  Existing infrastructure, including
                infrastructure external to OpenStack, can also be used by the cluster,
                such as DNS, public network, public discovery service, Docker registry.
                The actual resources created depends on the COE type and the options
                specified; therefore you need to refer to the cluster driver documentation
                of the COE for specific details.  For instance, the option
                ‘–master-lb-enabled’ in the ClusterTemplate will cause a load balancer pool
                along with the health monitor and floating IP to be created.  It is
                important to distinguish resources in the IaaS level from resources in
                the PaaS level.  For instance, the infrastructure networking in
                OpenStack IaaS is different and separate from the container networking
                in Kubernetes or Swarm PaaS.</para>
          <para>Typical infrastructure includes the following.</para>
          <variablelist>
            <varlistentry>
              <term>Servers</term>
              <listitem>
                <para>The servers host the containers in the cluster and these servers can be
                            VM or bare metal.  VM’s are provided by Nova.  Since multiple VM’s
                            are hosted on a physical server, the VM’s provide the isolation
                            needed for containers between different tenants running on the same
                            physical server.  Bare metal servers are provided by Ironic and are
                            used when peak performance with virtually no overhead is needed for
                            the containers.</para>
              </listitem>
            </varlistentry>
            <varlistentry>
              <term>Identity</term>
              <listitem>
                <para>Keystone provides the authentication and authorization for managing
                            the cluster infrastructure.</para>
              </listitem>
            </varlistentry>
            <varlistentry>
              <term>Network</term>
              <listitem>
                <para>Networking among the servers is provided by Neutron.  Since COE
                            currently are not multi-tenant, isolation for multi-tenancy on the
                            networking level is done by using a private network for each cluster.
                            As a result, containers belonging to one tenant will not be
                            accessible to containers or servers of another tenant.  Other
                            networking resources may also be used, such as load balancer and
                            routers.  Networking among containers can be provided by Kuryr if
                            needed.</para>
              </listitem>
            </varlistentry>
            <varlistentry>
              <term>Storage</term>
              <listitem>
                <para>Cinder provides the block storage that can be used to host the
                            containers and as persistent storage for the containers.</para>
              </listitem>
            </varlistentry>
            <varlistentry>
              <term>Security</term>
              <listitem>
                <para>Barbican provides the storage of secrets such as certificates used
                            for Transport Layer Security (TLS) within the cluster.</para>
              </listitem>
            </varlistentry>
          </variablelist>
        </section>
        <section>
          <title>Life cycle</title>
          <para>The set of life cycle operations on the cluster is one of the key value
                that Magnum provides, enabling clusters to be managed painlessly on
                OpenStack.  The current operations are the basic CRUD operations, but
                more advanced operations are under discussion in the community and
                will be implemented as needed.</para>
          <para><emphasis role="bold">NOTE</emphasis> The OpenStack resources created for a cluster are fully
                accessible to the cluster owner.  Care should be taken when modifying or
                reusing these resources to avoid impacting Magnum operations in
                unexpected manners.  For instance, if you launch your own Nova
                instance on the cluster private network, Magnum would not be aware of this
                instance.  Therefore, the cluster-delete operation will fail because
                Magnum would not delete the extra Nova instance and the private Neutron
                network cannot be removed while a Nova instance is still attached.</para>
          <para><emphasis role="bold">NOTE</emphasis> Currently Heat nested templates are used to create the
                resources; therefore if an error occurs, you can troubleshoot through
                Heat.  For more help on Heat stack troubleshooting, refer to the
            <link xlink:href="https://github.com/openstack/magnum/blob/master/doc/source/troubleshooting-guide.rst#heat-stacks">Troubleshooting Guide</link>.</para>
          <section>
            <title>Create</title>
            <para><emphasis role="bold">NOTE</emphasis> bay-&lt;command&gt; are the deprecated versions of these commands and are
                    still support in current release. They will be removed in a future version.
                    Any references to the term bay will be replaced in the parameters when using
                    the ‘bay’ versions of the commands. For example, in ‘bay-create’ –baymodel
                    is used as the baymodel parameter for this command instead of
                    –cluster-template.</para>
            <para>The ‘cluster-create’ command deploys a cluster, for example:</para>
            <screen>magnum cluster-create mycluster \
                  --cluster-template mytemplate \
                  --node-count 8 \
                  --master-count 3</screen>
            <para>The ‘cluster-create’ operation is asynchronous; therefore you can initiate
                    another ‘cluster-create’ operation while the current cluster is being created.
                    If the cluster fails to be created, the infrastructure created so far may
                    be retained or deleted depending on the particular orchestration
                    engine.  As a common practice, a failed cluster is retained during
                    development for troubleshooting, but they are automatically deleted in
                    production.  The current cluster drivers use Heat templates and the
                    resources of a failed ‘cluster-create’ are retained.</para>
            <para>The definition and usage of the parameters for ‘cluster-create’ are as
                    follows:</para>
            <variablelist>
              <varlistentry>
                <term>&lt;name&gt;</term>
                <listitem>
                  <para>Name of the cluster to create.  If a name is not specified, a random
                                name will be generated using a string and a number, for example
                                “gamma-7-cluster”.</para>
                </listitem>
              </varlistentry>
              <varlistentry>
                <term>–cluster-template &lt;cluster-template&gt;</term>
                <listitem>
                  <para>The ID or name of the ClusterTemplate to use.  This is a mandatory
                                parameter.  Once a ClusterTemplate is used to create a cluster, it cannot
                                be deleted or modified until all clusters that use the ClusterTemplate have
                                been deleted.</para>
                </listitem>
              </varlistentry>
              <varlistentry>
                <term>–keypair &lt;keypair&gt;</term>
                <listitem>
                  <para>The name of the SSH keypair to configure in the cluster servers
                                for ssh access.  You will need the key to be able to ssh to the
                                servers in the cluster.  The login name is specific to the cluster
                                driver. If keypair is not provided it will attempt to use the value in
                                the ClusterTemplate. If the ClusterTemplate is also missing a keypair value
                                then an error will be returned.  The keypair value provided here will
                                override the keypair value from the ClusterTemplate.</para>
                </listitem>
              </varlistentry>
              <varlistentry>
                <term>–node-count &lt;node-count&gt;</term>
                <listitem>
                  <para>The number of servers that will serve as node in the cluster.
                                The default is 1.</para>
                </listitem>
              </varlistentry>
              <varlistentry>
                <term>–master-count &lt;master-count&gt;</term>
                <listitem>
                  <para>The number of servers that will serve as master for the cluster.
                                The default is 1.  Set to more than 1 master to enable High
                                Availability.  If the option ‘–master-lb-enabled’ is specified in
                                the ClusterTemplate, the master servers will be placed in a load balancer
                                pool.</para>
                </listitem>
              </varlistentry>
              <varlistentry>
                <term>–discovery-url &lt;discovery-url&gt;</term>
                <listitem>
                  <para>The custom discovery url for node discovery.  This is used by the
                                COE to discover the servers that have been created to host the
                                containers.  The actual discovery mechanism varies with the COE.  In
                                some cases, Magnum fills in the server info in the discovery
                                service.  In other cases, if the discovery-url is not specified,
                                Magnum will use the public discovery service at:</para>
                  <screen>https://discovery.etcd.io</screen>
                  <para>In this case, Magnum will generate a unique url here for each cluster
                                and store the info for the servers.</para>
                </listitem>
              </varlistentry>
              <varlistentry>
                <term>–timeout &lt;timeout&gt;</term>
                <listitem>
                  <para>The timeout for cluster creation in minutes. The value expected is a
                                positive integer and the default is 60 minutes.  If the timeout is
                                reached during cluster-create, the operation will be aborted and the
                                cluster status will be set to ‘CREATE_FAILED’.</para>
                </listitem>
              </varlistentry>
            </variablelist>
          </section>
          <section>
            <title>List</title>
            <para>The ‘cluster-list’ command lists all the clusters that belong to the tenant,
                    for example:</para>
            <screen>magnum cluster-list</screen>
          </section>
          <section>
            <title>Show</title>
            <para>The ‘cluster-show’ command prints all the details of a cluster, for
                    example:</para>
            <screen>magnum cluster-show mycluster</screen>
            <para>The properties include those not specified by users that have been
                    assigned default values and properties from new resources that
                    have been created for the cluster.</para>
          </section>
          <section>
            <title>Update</title>
            <para>A cluster can be modified using the ‘cluster-update’ command, for example:</para>
            <screen>magnum cluster-update mycluster replace node_count=8</screen>
            <para>The parameters are positional and their definition and usage are as
                    follows.</para>
            <variablelist>
              <varlistentry>
                <term>&lt;cluster&gt;</term>
                <listitem>
                  <para>This is the first parameter, specifying the UUID or name of the cluster
                                to update.</para>
                </listitem>
              </varlistentry>
              <varlistentry>
                <term>&lt;op&gt;</term>
                <listitem>
                  <para>This is the second parameter, specifying the desired change to be
                                made to the cluster attributes.  The allowed changes are ‘add’,
                                ‘replace’ and ‘remove’.</para>
                </listitem>
              </varlistentry>
              <varlistentry>
                <term>&lt;attribute=value&gt;</term>
                <listitem>
                  <para>This is the third parameter, specifying the targeted attributes in
                                the cluster as a list separated by blank space.  To add or replace an
                                attribute, you need to specify the value for the attribute.  To
                                remove an attribute, you only need to specify the name of the
                                attribute.  Currently the only attribute that can be replaced or
                                removed is ‘node_count’.  The attributes ‘name’, ‘master_count’ and
                                ‘discovery_url’ cannot be replaced or delete.  The table below
                                summarizes the possible change to a cluster.</para>
                  <informaltable>
                    <tgroup cols="4">
                      <colspec colname="c1" colwidth="15"/>
                      <colspec colname="c2" colwidth="5"/>
                      <colspec colname="c3" colwidth="18"/>
                      <colspec colname="c4" colwidth="23"/>
                      <thead>
                        <row>
                          <entry>
                            <para>Attribute</para>
                          </entry>
                          <entry>
                            <para>add</para>
                          </entry>
                          <entry>
                            <para>replace</para>
                          </entry>
                          <entry>
                            <para>remove</para>
                          </entry>
                        </row>
                      </thead>
                      <tbody>
                        <row>
                          <entry>
                            <para>node_count</para>
                          </entry>
                          <entry>
                            <para>no</para>
                          </entry>
                          <entry>
                            <para>add/remove nodes</para>
                          </entry>
                          <entry>
                            <para>reset to default of 1</para>
                          </entry>
                        </row>
                        <row>
                          <entry>
                            <para>master_count</para>
                          </entry>
                          <entry>
                            <para>no</para>
                          </entry>
                          <entry>
                            <para>no</para>
                          </entry>
                          <entry>
                            <para>no</para>
                          </entry>
                        </row>
                        <row>
                          <entry>
                            <para>name</para>
                          </entry>
                          <entry>
                            <para>no</para>
                          </entry>
                          <entry>
                            <para>no</para>
                          </entry>
                          <entry>
                            <para>no</para>
                          </entry>
                        </row>
                        <row>
                          <entry>
                            <para>discovery_url</para>
                          </entry>
                          <entry>
                            <para>no</para>
                          </entry>
                          <entry>
                            <para>no</para>
                          </entry>
                          <entry>
                            <para>no</para>
                          </entry>
                        </row>
                      </tbody>
                    </tgroup>
                  </informaltable>
                </listitem>
              </varlistentry>
            </variablelist>
            <para>The ‘cluster-update’ operation cannot be initiated when another operation
                    is in progress.</para>
            <para><emphasis role="bold">NOTE:</emphasis> The attribute names in cluster-update are slightly different
                    from the corresponding names in the cluster-create command: the dash ‘-‘
                    is replaced by an underscore ‘_’.  For instance, ‘node-count’ in
                    cluster-create is ‘node_count’ in cluster-update.</para>
          </section>
          <section>
            <title>Scale</title>
            <para>Scaling a cluster means adding servers to or removing servers from the cluster.
                    Currently, this is done through the ‘cluster-update’ operation by modifying
                    the node-count attribute, for example:</para>
            <screen>magnum cluster-update mycluster replace node_count=2</screen>
            <para>When some nodes are removed, Magnum will attempt to find nodes with no
                    containers to remove.  If some nodes with containers must be removed,
                    Magnum will log a warning message.</para>
          </section>
          <section>
            <title>Delete</title>
            <para>The ‘cluster-delete’ operation removes the cluster by deleting all resources
                    such as servers, network, storage;  for example:</para>
            <screen>magnum cluster-delete mycluster</screen>
            <para>The only parameter for the cluster-delete command is the ID or name of the
                    cluster to delete.  Multiple clusters can be specified, separated by a blank
                    space.</para>
            <para>If the operation fails, there may be some remaining resources that
                    have not been deleted yet.  In this case, you can troubleshoot through
                    Heat.  If the templates are deleted manually in Heat, you can delete
                    the cluster in Magnum to clean up the cluster from Magnum database.</para>
            <para>The ‘cluster-delete’ operation can be initiated when another operation is
                    still in progress.</para>
          </section>
        </section>
      </section>
      <section xml:base="user/index">
        <title>Python Client</title>
        <section>
          <title>Installation</title>
          <para>Follow the instructions in the OpenStack Installation Guide to enable the
                repositories for your distribution:</para>
          <itemizedlist>
            <listitem>
              <para>
                <link xlink:href="http://docs.openstack.org/liberty/install-guide-rdo/">RHEL/CentOS/Fedora</link>
              </para>
            </listitem>
            <listitem>
              <para>
                <link xlink:href="http://docs.openstack.org/liberty/install-guide-ubuntu/">Ubuntu/Debian</link>
              </para>
            </listitem>
            <listitem>
              <para>
                <link xlink:href="http://docs.openstack.org/liberty/install-guide-obs/">openSUSE/SUSE Linux Enterprise</link>
              </para>
            </listitem>
          </itemizedlist>
          <para>Install using distribution packages for RHEL/CentOS/Fedora:</para>
          <screen>$ sudo yum install python-magnumclient</screen>
          <para>Install using distribution packages for Ubuntu/Debian:</para>
          <screen>$ sudo apt-get install python-magnumclient</screen>
          <para>Install using distribution packages for OpenSuSE and SuSE Enterprise Linux:</para>
          <screen>$ sudo zypper install python-magnumclient</screen>
        </section>
        <section>
          <title>Verifying installation</title>
          <para>Execute the <literal>magnum</literal> command with the <literal>–version</literal> argument to confirm that the
                client is installed and in the system path:</para>
          <screen>$ magnum --version
1.1.0</screen>
          <para>Note that the version returned may differ from the above, 1.1.0 was the latest
                available version at the time of writing.</para>
        </section>
        <section>
          <title>Using the command-line client</title>
          <para>Refer to the <link xlink:href="http://docs.openstack.org/cli-reference/magnum.html">OpenStack Command-Line Interface Reference</link> for a full list of the
                commands supported by the <literal>magnum</literal> command-line client.</para>
        </section>
      </section>
      <section xml:base="user/index">
        <title>Horizon Interface</title>
        <para>Magnum provides a Horizon plugin so that users can access the Container
            Infrastructure Management service through the OpenStack browser-based
            graphical UI.  The plugin is available from
          <link xlink:href="https://github.com/openstack/magnum-ui">magnum-ui</link>.  It is not
            installed by default in the standard Horizon service, but you can
            follow the instruction for <link xlink:href="http://docs.openstack.org/developer/horizon/tutorials/plugin.html#installing-your-plugin">installing a Horizon plugin</link>.</para>
        <para>In Horizon, the container infrastructure panel is part of the
            ‘Project’ view and it currently supports the following operations:</para>
        <itemizedlist>
          <listitem>
            <para>View list of cluster templates</para>
          </listitem>
          <listitem>
            <para>View details of a cluster template</para>
          </listitem>
          <listitem>
            <para>Create a cluster template</para>
          </listitem>
          <listitem>
            <para>Delete a cluster template</para>
          </listitem>
          <listitem>
            <para>View list of clusters</para>
          </listitem>
          <listitem>
            <para>View details of a cluster</para>
          </listitem>
          <listitem>
            <para>Create a cluster</para>
          </listitem>
          <listitem>
            <para>Delete a cluster</para>
          </listitem>
          <listitem>
            <para>Get the Certificate Authority for a cluster</para>
          </listitem>
          <listitem>
            <para>Sign a user key and obtain a signed certificate for accessing the secured
                    COE API endpoint in a cluster.</para>
          </listitem>
        </itemizedlist>
        <para>Other operations are not yet supported and the CLI should be used for these.</para>
        <!--<para>Following is the screenshot of the Horizon view showing the list of cluster
            templates.</para>
        <informalfigure>
          <mediaobject>
            <imageobject role="fo">
              <imagedata fileref="cluster-template.png"/>
            </imageobject>
            <imageobject role="html">
              <imagedata fileref="cluster-template.png"/>
            </imageobject>
          </mediaobject>
        </informalfigure>
        <para>Following is the screenshot of the Horizon view showing the details of a
            cluster template.</para>
        <informalfigure>
          <mediaobject>
            <imageobject role="fo">
              <imagedata fileref="cluster-template-details.png"/>
            </imageobject>
            <imageobject role="html">
              <imagedata fileref="cluster-template-details.png"/>
            </imageobject>
          </mediaobject>
        </informalfigure>
        <para>Following is the screenshot of the dialog to create a new cluster.</para>
        <informalfigure>
          <mediaobject>
            <imageobject role="fo">
              <imagedata fileref="cluster-create.png"/>
            </imageobject>
            <imageobject role="html">
              <imagedata fileref="cluster-create.png"/>
            </imageobject>
          </mediaobject>
        </informalfigure> -->
      </section>
      <section xml:base="user/index">
        <title>Cluster Drivers</title>
        <para>A cluster driver is a collection of python code, heat templates, scripts,
            images, and documents for a particular COE on a particular
            distro.  Magnum presents the concept of ClusterTemplates and clusters.  The
            implementation for a particular cluster type is provided by the cluster driver.
            In other words, the cluster driver provisions and manages the infrastructure
            for the COE.  Magnum includes default drivers for the following
            COE and distro pairs:</para>
        <informaltable>
          <tgroup cols="2">
            <colspec colname="c1" colwidth="12"/>
            <colspec colname="c2" colwidth="15"/>
            <thead>
              <row>
                <entry>
                  <para>COE</para>
                </entry>
                <entry>
                  <para>distro</para>
                </entry>
              </row>
            </thead>
            <tbody>
              <row>
                <entry>
                  <para>Kubernetes</para>
                </entry>
                <entry>
                  <para>Fedora Atomic</para>
                </entry>
              </row>
              <row>
                <entry>
                  <para>Kubernetes</para>
                </entry>
                <entry>
                  <para>CoreOS</para>
                </entry>
              </row>
              <row>
                <entry>
                  <para>Swarm</para>
                </entry>
                <entry>
                  <para>Fedora Atomic</para>
                </entry>
              </row>
              <row>
                <entry>
                  <para>Mesos</para>
                </entry>
                <entry>
                  <para>Ubuntu</para>
                </entry>
              </row>
            </tbody>
          </tgroup>
        </informaltable>
        <para>Magnum is designed to accommodate new cluster drivers to support custom
            COE’s and this section describes how a new cluster driver can be
            constructed and enabled in Magnum.</para>
        <section>
          <title>Directory structure</title>
          <para>Magnum expects the components to be organized in the following
                directory structure under the directory ‘drivers’:</para>
          <screen>COE_Distro/
   image/
   templates/
   api.py
   driver.py
   monitor.py
   scale.py
   template_def.py
   version.py</screen>
          <para>The minimum required components are:</para>
          <variablelist>
            <varlistentry>
              <term>driver.py</term>
              <listitem>
                <para>Python code that implements the controller operations for
                            the particular COE.  The driver must implement:
                            Currently supported:
                            <literal>cluster_create</literal>, <literal>cluster_update</literal>, <literal>cluster_delete</literal>.</para>
              </listitem>
            </varlistentry>
            <varlistentry>
              <term>templates</term>
              <listitem>
                <para>A directory of orchestration templates for managing the lifecycle
                            of clusters, including creation, configuration, update, and deletion.
                            Currently only Heat templates are supported, but in the future
                            other orchestration mechanism such as Ansible may be supported.</para>
              </listitem>
            </varlistentry>
            <varlistentry>
              <term>template_def.py</term>
              <listitem>
                <para>Python code that maps the parameters from the ClusterTemplate to the
                            input parameters for the orchestration and invokes
                            the orchestration in the templates directory.</para>
              </listitem>
            </varlistentry>
            <varlistentry>
              <term>version.py</term>
              <listitem>
                <para>Tracks the latest version of the driver in this directory.
                            This is defined by a <literal>version</literal> attribute and is represented in the
                            form of <literal>1.0.0</literal>. It should also include a <literal>Driver</literal> attribute with
                            descriptive name such as <literal>fedora_swarm_atomic</literal>.</para>
              </listitem>
            </varlistentry>
          </variablelist>
          <para>The remaining components are optional:</para>
          <variablelist>
            <varlistentry>
              <term>image</term>
              <listitem>
                <para>Instructions for obtaining or building an image suitable for the COE.</para>
              </listitem>
            </varlistentry>
            <varlistentry>
              <term>api.py</term>
              <listitem>
                <para>Python code to interface with the COE.</para>
              </listitem>
            </varlistentry>
            <varlistentry>
              <term>monitor.py</term>
              <listitem>
                <para>Python code to monitor the resource utilization of the cluster.</para>
              </listitem>
            </varlistentry>
            <varlistentry>
              <term>scale.py</term>
              <listitem>
                <para>Python code to scale the cluster by adding or removing nodes.</para>
              </listitem>
            </varlistentry>
          </variablelist>
        </section>
        <section>
          <title>Sample cluster driver</title>
          <para>To help developers in creating new COE drivers, a minimal cluster driver
                is provided as an example.  The ‘docker’ cluster driver will simply deploy
                a single VM running Ubuntu with the latest Docker version installed.
                It is not a true cluster, but the simplicity will help to illustrate
                the key concepts.</para>
          <para>
            <emphasis>To be filled in</emphasis>
          </para>
        </section>
        <section>
          <title>Installing a cluster driver</title>
          <para>
            <emphasis>To be filled in</emphasis>
          </para>
        </section>
      </section>
      <section xml:base="user/index">
        <title>Cluster Type Definition</title>
        <para>There are three key pieces to a Cluster Type Definition:</para>
        <procedure>
          <step>
            <para>Heat Stack template - The HOT file that Magnum will use to generate a
                    cluster using a Heat Stack.</para>
          </step>
          <step>
            <para>Template definition - Magnum’s interface for interacting with the Heat
                    template.</para>
          </step>
          <step>
            <para>Definition Entry Point - Used to advertise the available Cluster Types.</para>
          </step>
        </procedure>
        <section>
          <title>The Heat Stack Template</title>
          <para>The Heat Stack Template is where most of the real work happens. The result of
                the Heat Stack Template should be a full Container Orchestration Environment.</para>
        </section>
        <section>
          <title>The Template Definition</title>
          <para>Template definitions are a mapping of Magnum object attributes and Heat
                template parameters, along with Magnum consumable template outputs. A
                Cluster Type Definition indicates which Cluster Types it can provide.
                Cluster Types are how Magnum determines which of the enabled Cluster
                Type Definitions it will use for a given cluster.</para>
        </section>
        <section>
          <title>The Definition Entry Point</title>
          <para>Entry points are a standard discovery and import mechanism for Python objects.
                Each Template Definition should have an Entry Point in the
                <literal>magnum.template_definitions</literal> group. This example exposes it’s Template
                Definition as <literal>example_template = example_template:ExampleTemplate</literal> in the
                <literal>magnum.template_definitions</literal> group.</para>
        </section>
        <section>
          <title>Installing Cluster Templates</title>
          <para>Because Cluster Type Definitions are basically Python projects, they can be
                worked with like any other Python project. They can be cloned from version
                control and installed or uploaded to a package index and installed via
                utilities such as pip.</para>
          <para>Enabling a Cluster Type is as simple as adding it’s Entry Point to the
                <literal>enabled_definitions</literal> config option in magnum.conf.:</para>
          <screen># Setup python environment and install Magnum

$ virtualenv .venv
$ source .venv/bin/active
(.venv)$ git clone https://github.com/openstack/magnum.git
(.venv)$ cd magnum
(.venv)$ python setup.py install

# List installed templates, notice default templates are enabled

(.venv)$ magnum-template-manage list-templates
Enabled Templates
  magnum_vm_atomic_k8s: /home/example/.venv/local/lib/python2.7/site-packages/magnum/templates/kubernetes/kubecluster.yaml
  magnum_vm_coreos_k8s: /home/example/.venv/local/lib/python2.7/site-packages/magnum/templates/kubernetes/kubecluster-coreos.yaml
Disabled Templates

# Install example template

(.venv)$ cd contrib/templates/example
(.venv)$ python setup.py install

# List installed templates, notice example template is disabled

(.venv)$ magnum-template-manage list-templates
Enabled Templates
  magnum_vm_atomic_k8s: /home/example/.venv/local/lib/python2.7/site-packages/magnum/templates/kubernetes/kubecluster.yaml
  magnum_vm_coreos_k8s: /home/example/.venv/local/lib/python2.7/site-packages/magnum/templates/kubernetes/kubecluster-coreos.yaml
Disabled Templates
  example_template: /home/example/.venv/local/lib/python2.7/site-packages/ExampleTemplate-0.1-py2.7.egg/example_template/example.yaml

# Enable example template by setting enabled_definitions in magnum.conf

(.venv)$ sudo mkdir /etc/magnum
(.venv)$ sudo bash -c "cat &gt; /etc/magnum/magnum.conf &lt;&lt; END_CONF
[bay]
enabled_definitions=magnum_vm_atomic_k8s,magnum_vm_coreos_k8s,example_template
END_CONF"

# List installed templates, notice example template is now enabled

(.venv)$ magnum-template-manage list-templates
Enabled Templates
  example_template: /home/example/.venv/local/lib/python2.7/site-packages/ExampleTemplate-0.1-py2.7.egg/example_template/example.yaml
  magnum_vm_atomic_k8s: /home/example/.venv/local/lib/python2.7/site-packages/magnum/templates/kubernetes/kubecluster.yaml
  magnum_vm_coreos_k8s: /home/example/.venv/local/lib/python2.7/site-packages/magnum/templates/kubernetes/kubecluster-coreos.yaml
Disabled Templates

# Use --details argument to get more details about each template

(.venv)$ magnum-template-manage list-templates --details
Enabled Templates
  example_template: /home/example/.venv/local/lib/python2.7/site-packages/ExampleTemplate-0.1-py2.7.egg/example_template/example.yaml
     Server_Type  OS       CoE
     vm         example  example_coe
  magnum_vm_atomic_k8s: /home/example/.venv/local/lib/python2.7/site-packages/magnum/templates/kubernetes/kubecluster.yaml
     Server_Type   OS             CoE
     vm        fedora-atomic  kubernetes
  magnum_vm_coreos_k8s: /home/example/.venv/local/lib/python2.7/site-packages/magnum/templates/kubernetes/kubecluster-coreos.yaml
     Server_Type  OS      CoE
     vm         coreos  kubernetes
Disabled Templates</screen>
        </section>
      </section>
      <section xml:base="user/index">
        <title>Heat Stack Templates</title>
        <para>Heat Stack Templates are what Magnum passes to Heat to generate a cluster. For
            each ClusterTemplate resource in Magnum, a Heat stack is created to arrange all
            of the cloud resources needed to support the container orchestration
            environment. These Heat stack templates provide a mapping of Magnum object
            attributes to Heat template parameters, along with Magnum consumable stack
            outputs. Magnum passes the Heat Stack Template to the Heat service to create a
            Heat stack. The result is a full Container Orchestration Environment.</para>
      </section>
      <section xml:base="user/index">
        <title>Choosing a COE</title>
        <para>Magnum supports a variety of COE options, and allows more to be added over time
            as they gain popularity. As an operator, you may choose to support the full
            variety of options, or you may want to offer a subset of the available choices.
            Given multiple choices, your users can run one or more clusters, and each may
            use a different COE. For example, I might have multiple clusters that use
            Kubernetes, and just one cluster that uses Swarm. All of these clusters can
            run concurrently, even though they use different COE software.</para>
        <para>Choosing which COE to use depends on what tools you want to use to manage your
            containers once you start your app. If you want to use the Docker tools, you
            may want to use the Swarm cluster type. Swarm will spread your containers
            across the various nodes in your cluster automatically. It does not monitor
            the health of your containers, so it can’t restart them for you if they stop.
            It will not automatically scale your app for you (as of Swarm version 1.2.2).
            You may view this as a plus. If you prefer to manage your application yourself,
            you might prefer swarm over the other COE options.</para>
        <para>Kubernetes (as of v1.2) is more sophisticated than Swarm (as of v1.2.2). It
            offers an attractive YAML file description of a pod, which is a grouping of
            containers that run together as part of a distributed application. This file
            format allows you to model your application deployment using a declarative
            style. It has support for auto scaling and fault recovery, as well as features
            that allow for sophisticated software deployments, including canary deploys
            and blue/green deploys. Kubernetes is very popular, especially for web
            applications.</para>
        <para>Apache Mesos is a COE that has been around longer than Kubernetes or Swarm. It
            allows for a variety of different frameworks to be used along with it,
            including Marathon, Aurora, Chronos, Hadoop, and <link xlink:href="http://mesos.apache.org/documentation/latest/frameworks/">a number of others.</link></para>
        <para>The Apache Mesos framework design can be used to run alternate COE software
            directly on Mesos. Although this approach is not widely used yet, it may soon
            be possible to run Mesos with Kubernetes and Swarm as frameworks, allowing
            you to share the resources of a cluster between multiple different COEs. Until
            this option matures, we encourage Magnum users to create multiple clusters, and
            use the COE in each cluster that best fits the anticipated workload.</para>
        <para>Finding the right COE for your workload is up to you, but Magnum offers you a
            choice to select among the prevailing leading options. Once you decide, see
            the next sections for examples of how to create a cluster with your desired
            COE.</para>
      </section>
      <section xml:base="user/index">
        <title>Native Clients</title>
        <para>Magnum preserves the native user experience with a COE and does not
            provide a separate API or client.  This means you will need to use the
            native client for the particular cluster type to interface with the
            clusters.  In the typical case, there are two clients to consider:</para>
        <variablelist>
          <varlistentry>
            <term>COE level</term>
            <listitem>
              <para>This is the orchestration or management level such as Kubernetes,
                        Swarm, Mesos and its frameworks.</para>
            </listitem>
          </varlistentry>
          <varlistentry>
            <term>Container level</term>
            <listitem>
              <para>This is the low level container operation.  Currently it is
                        Docker for all clusters.</para>
            </listitem>
          </varlistentry>
        </variablelist>
        <para>The clients can be CLI and/or browser-based.  You will need to refer
            to the documentation for the specific native client and appropriate
            version for details, but following are some pointers for reference.</para>
        <para>Kubernetes CLI is the tool ‘kubectl’, which can be simply copied from
            a node in the cluster or downloaded from the Kubernetes release.  For
            instance, if the cluster is running Kubernetes release 1.2.0, the
            binary for ‘kubectl’ can be downloaded as and set up locally as
            follows:</para>
        <screen>curl -O https://storage.googleapis.com/kubernetes-release/release/v1.2.0/bin/linux/amd64/kubectl
chmod +x kubectl
sudo mv kubectl /usr/local/bin/kubectl</screen>
        <para>Kubernetes also provides a browser UI. If the cluster has the
            Kubernetes Dashboard running; it can be accessed using:</para>
        <screen>eval $(magnum cluster-config &lt;cluster-name&gt;)
kubectl proxy

The browser can be accessed at http://localhost:8001/ui</screen>
        <para>For Swarm, the main CLI is ‘docker’, along with associated tools
            such as ‘docker-compose’, etc.  Specific version of the binaries can
            be obtained from the <link xlink:href="https://docs.docker.com/engine/installation/binaries/">Docker Engine installation</link>.</para>
        <para>Mesos cluster uses the Marathon framework<!--and details on the Marathon
            UI can be found in the section <xref linkend="using-marathon"/>-->.</para>
        <para>Depending on the client requirement, you may need to use a version of
            the client that matches the version in the cluster.  To determine the
            version of the COE and container, use the command ‘cluster-show’ and
            look for the attribute <emphasis>coe_version</emphasis> and <emphasis>container_version</emphasis>:</para>
        <screen>magnum cluster-show k8s-cluster
+--------------------+------------------------------------------------------------+
| Property           | Value                                                      |
+--------------------+------------------------------------------------------------+
| status             | CREATE_COMPLETE                                            |
| uuid               | 04952c60-a338-437f-a7e7-d016d1d00e65                       |
| stack_id           | b7bf72ce-b08e-4768-8201-e63a99346898                       |
| status_reason      | Stack CREATE completed successfully                        |
| created_at         | 2016-07-25T23:14:06+00:00                                  |
| updated_at         | 2016-07-25T23:14:10+00:00                                  |
| create_timeout     | 60                                                         |
| coe_version        | v1.2.0                                                     |
| api_address        | https://192.168.19.86:6443                                 |
| cluster_template_id| da2825a0-6d09-4208-b39e-b2db666f1118                       |
| master_addresses   | ['192.168.19.87']                                          |
| node_count         | 1                                                          |
| node_addresses     | ['192.168.19.88']                                          |
| master_count       | 1                                                          |
| container_version  | 1.9.1                                                      |
| discovery_url      | https://discovery.etcd.io/3b7fb09733429d16679484673ba3bfd5 |
| name               | k8s-cluster                                                |
+--------------------+------------------------------------------------------------+</screen>
      </section>
      <section xml:base="user/index">
        <title>Kubernetes</title>
        <para>Kubernetes uses a range of terminology that we refer to in this guide. We
            define these common terms for your reference:</para>
        <variablelist>
          <varlistentry>
            <term>Pod</term>
            <listitem>
              <para>When using the Kubernetes container orchestration engine, a pod is the
                        smallest deployable unit that can be created and managed. A pod is a
                        co-located group of application containers that run with a shared context.
                        When using Magnum, pods are created and managed within clusters. Refer to the
                <link xlink:href="http://kubernetes.io/v1.0/docs/user-guide/pods.html">pods section</link> in the <link xlink:href="http://kubernetes.io/v1.0/docs/user-guide/">Kubernetes
                            User Guide</link> for more information.</para>
            </listitem>
          </varlistentry>
          <varlistentry>
            <term>Replication controller</term>
            <listitem>
              <para>A replication controller is used to ensure that at any given time a certain
                        number of replicas of a pod are running. Pods are automatically created and
                        deleted by the replication controller as necessary based on a template to
                        ensure that the defined number of replicas exist. Refer to the <link xlink:href="http://kubernetes.io/v1.0/docs/user-guide/replication-controller.html">replication
                            controller section</link> in
                        the <link xlink:href="http://kubernetes.io/v1.0/docs/user-guide/">Kubernetes User Guide</link> for more information.</para>
            </listitem>
          </varlistentry>
          <varlistentry>
            <term>Service</term>
            <listitem>
              <para>A service is an additional layer of abstraction provided by the Kubernetes
                        container orchestration engine which defines a logical set of pods and a
                        policy for accessing them. This is useful because pods are created and
                        deleted by a replication controller, for example, other pods needing to
                        discover them can do so via the service abstraction. Refer to the
                        <link xlink:href="http://kubernetes.io/v1.0/docs/user-guide/services.html">services section</link> in the
                        <link xlink:href="http://kubernetes.io/v1.0/docs/user-guide/">Kubernetes User Guide</link> for more information.</para>
            </listitem>
          </varlistentry>
        </variablelist>
        <para>When Magnum deploys a Kubernetes cluster, it uses parameters defined in the
            ClusterTemplate and specified on the cluster-create command, for example:</para>
        <screen>magnum cluster-template-create k8s-cluster-template \
                           --image fedora-atomic-latest \
                           --keypair testkey \
                           --external-network public \
                           --dns-nameserver 8.8.8.8 \
                           --flavor m1.small \
                           --docker-volume-size 5 \
                           --network-driver flannel \
                           --coe kubernetes

magnum cluster-create k8s-cluster \
                      --cluster-template k8s-cluster-template \
                      --master-count 3 \
                      --node-count 8</screen>
        <para><!-- Refer to the <xref linkend="clustertemplate"/> and <xref linkend="cluster"/> sections for the full list of
            parameters. -->Following are further details relevant to a Kubernetes cluster:</para>
        <variablelist>
          <varlistentry>
            <term>Number of masters (master-count)</term>
            <listitem>
              <para>Specified in the cluster-create command to indicate how many servers will
                        run as master in the cluster.  Having more than one will provide high
                        availability.  The masters will be in a load balancer pool and the
                        virtual IP address (VIP) of the load balancer will serve as the
                        Kubernetes API endpoint.  For external access, a floating IP
                        associated with this VIP is available and this is the endpoint
                        shown for Kubernetes in the ‘cluster-show’ command.</para>
            </listitem>
          </varlistentry>
          <varlistentry>
            <term>Number of nodes (node-count)</term>
            <listitem>
              <para>Specified in the cluster-create command to indicate how many servers will
                        run as node in the cluster to host the users’ pods.  The nodes are registered
                        in Kubernetes using the Nova instance name.</para>
            </listitem>
          </varlistentry>
          <varlistentry>
            <term>Network driver (network-driver)</term>
            <listitem>
              <para>Specified in the ClusterTemplate to select the network driver.
                        The supported and default network driver is ‘flannel’, an overlay
                        network providing a flat network for all pods.<!--  Refer to the
                        <xref linkend="networking"/> section for more details.--></para>
            </listitem>
          </varlistentry>
          <varlistentry>
            <term>Volume driver (volume-driver)</term>
            <listitem>
              <para>Specified in the ClusterTemplate to select the volume driver.  The supported
                        volume driver is ‘cinder’, allowing Cinder volumes to be mounted in
                        containers for use as persistent storage.  Data written to these volumes
                        will persist after the container exits and can be accessed again from other
                        containers, while data written to the union file system hosting the container
                        will be deleted.<!--  Refer to the <xref linkend="storage"/> section for more details.--></para>
            </listitem>
          </varlistentry>
          <varlistentry>
            <term>Storage driver (docker-storage-driver)</term>
            <listitem>
              <para>Specified in the ClusterTemplate to select the Docker storage driver.  The
                        supported storage drivers are ‘devicemapper’ and ‘overlay’, with
                        ‘devicemapper’ being the default.<!-- Refer to the <xref linkend="storage"/> section for more
                        details.--></para>
            </listitem>
          </varlistentry>
          <varlistentry>
            <term>Image (image)</term>
            <listitem>
              <para>Specified in the ClusterTemplate to indicate the image to boot the servers.
                        The image binary is loaded in Glance with the attribute
                        ‘os_distro = fedora-atomic’.
                        Current supported images are Fedora Atomic (download from <link xlink:href="https://alt.fedoraproject.org/pub/alt/atomic/stable/Cloud-Images/x86_64/Images">Fedora</link> )
                        and CoreOS (download from <link xlink:href="http://beta.release.core-os.net/amd64-usr/current/coreos_production_openstack_image.img.bz2">CoreOS</link> )</para>
            </listitem>
          </varlistentry>
          <varlistentry>
            <term>TLS (tls-disabled)</term>
            <listitem>
              <para>Transport Layer Security is enabled by default, so you need a key and
                        signed certificate to access the Kubernetes API and CLI.  Magnum
                        handles its own key and certificate when interfacing with the
                        Kubernetes cluster.  In development mode, TLS can be disabled.  Refer to
                        the ‘Transport Layer Security’_ section for more details.</para>
            </listitem>
          </varlistentry>
          <varlistentry>
            <term>What runs on the servers</term>
            <listitem>
              <para>The servers for Kubernetes master host containers in the ‘kube-system’
                        name space to run the Kubernetes proxy, scheduler and controller manager.
                        The masters will not host users’ pods.  Kubernetes API server, docker
                        daemon, etcd and flannel run as systemd services.  The servers for
                        Kubernetes node also host a container in the ‘kube-system’ name space
                        to run the Kubernetes proxy, while Kubernetes kubelet, docker daemon
                        and flannel run as systemd services.</para>
            </listitem>
          </varlistentry>
          <varlistentry>
            <term>Log into the servers</term>
            <listitem>
              <para>You can log into the master servers using the login ‘fedora’ and the
                        keypair specified in the ClusterTemplate.</para>
            </listitem>
          </varlistentry>
        </variablelist>
        <para>In addition to the common attributes in the ClusterTemplate, you can specify
            the following attributes that are specific to Kubernetes by using the
            labels attribute.</para>
        <variablelist>
          <varlistentry>
            <term/>
            <listitem>
              <para>This label corresponds to Kubernetes parameter for the API server ‘–admission-control’.
                        For more details, refer to the <link xlink:href="https://kubernetes.io/docs/admin/admission-controllers//">Admission Controllers</link>.
                        The default value corresponds to the one recommended in this doc
                        for our current Kubernetes version.</para>
            </listitem>
          </varlistentry>
          <varlistentry>
            <term/>
            <listitem>
              <para>This label sets the size of a volume holding the etcd storage data.
                        The default value is 0, meaning the etcd data is not persisted (no volume).</para>
            </listitem>
          </varlistentry>
          <varlistentry>
            <term/>
            <listitem>
              <para>This label allows users to select <link xlink:href="https://hub.docker.com/r/openstackmagnum/kubernetes-apiserver/tags/">a specific Kubernetes release,
                            based on its container tag</link>.
                        If unset, the current Magnum version’s default Kubernetes release is
                        installed.</para>
            </listitem>
          </varlistentry>
          <varlistentry>
            <term/>
            <listitem>
              <para>This label triggers the deployment of the kubernetes dashboard.
                        The default value is 1, meaning it will be enabled.</para>
            </listitem>
          </varlistentry>
        </variablelist>
        <section>
          <title>External load balancer for services</title>
          <para>All Kubernetes pods and services created in the cluster are assigned IP
                addresses on a private container network so they can access each other
                and the external internet.  However, these IP addresses are not
                accessible from an external network.</para>
          <para>To publish a service endpoint externally so that the service can be
                accessed from the external network, Kubernetes provides the external
                load balancer feature.  This is done by simply specifying in the
                service manifest the attribute “type: LoadBalancer”.  Magnum enables
                and configures the Kubernetes plugin for OpenStack so that it can
                interface with Neutron and manage the necessary networking resources.</para>
          <para>When the service is created, Kubernetes will add an external load
                balancer in front of the service so that the service will have an
                external IP address in addition to the internal IP address on the
                container network.  The service endpoint can then be accessed with
                this external IP address.  Kubernetes handles all the life cycle
                operations when pods are modified behind the service and when the
                service is deleted.</para>
          <para>Refer to the document <link xlink:href="https://github.com/openstack/magnum/blob/master/doc/source/dev/kubernetes-load-balancer.rst">Kubernetes external load balancer</link>
                for more details.</para>
        </section>
      </section>
      <section xml:base="user/index">
        <title>Swarm</title>
        <para>A Swarm cluster is a pool of servers running Docker daemon that is
            managed as a single Docker host.  One or more Swarm managers accepts
            the standard Docker API and manage this pool of servers.
            Magnum deploys a Swarm cluster using parameters defined in
            the ClusterTemplate and specified on the ‘cluster-create’ command, for
            example:</para>
        <screen>magnum cluster-template-create swarm-cluster-template \
                           --image fedora-atomic-latest \
                           --keypair testkey \
                           --external-network public \
                           --dns-nameserver 8.8.8.8 \
                           --flavor m1.small \
                           --docker-volume-size 5 \
                           --coe swarm

magnum cluster-create swarm-cluster \
                  --cluster-template swarm-cluster-template \
                  --master-count 3 \
                  --node-count 8</screen>
        <para><!-- Refer to the <xref linkend="clustertemplate"/> and <xref linkend="cluster"/> sections for the full list of
            parameters. -->Following are further details relevant to Swarm:</para>
        <variablelist>
          <varlistentry>
            <term>What runs on the servers</term>
            <listitem>
              <para>There are two types of servers in the Swarm cluster: managers and nodes.
                        The Docker daemon runs on all servers.  On the servers for manager,
                        the Swarm manager is run as a Docker container on port 2376 and this
                        is initiated by the systemd service swarm-manager.  Etcd is also run
                        on the manager servers for discovery of the node servers in the cluster.
                        On the servers for node, the Swarm agent is run as a Docker
                        container on port 2375 and this is initiated by the systemd service
                        swarm-agent.  On start up, the agents will register themselves in
                        etcd and the managers will discover the new node to manage.</para>
            </listitem>
          </varlistentry>
          <varlistentry>
            <term>Number of managers (master-count)</term>
            <listitem>
              <para>Specified in the cluster-create command to indicate how many servers will
                        run as managers in the cluster.  Having more than one will provide high
                        availability.  The managers will be in a load balancer pool and the
                        load balancer virtual IP address (VIP) will serve as the Swarm API
                        endpoint.  A floating IP associated with the load balancer VIP will
                        serve as the external Swarm API endpoint.  The managers accept
                        the standard Docker API and perform the corresponding operation on the
                        servers in the pool.  For instance, when a new container is created,
                        the managers will select one of the servers based on some strategy
                        and schedule the containers there.</para>
            </listitem>
          </varlistentry>
          <varlistentry>
            <term>Number of nodes (node-count)</term>
            <listitem>
              <para>Specified in the cluster-create command to indicate how many servers will
                        run as nodes in the cluster to host your Docker containers.  These servers
                        will register themselves in etcd for discovery by the managers, and
                        interact with the managers.  Docker daemon is run locally to host
                        containers from users.</para>
            </listitem>
          </varlistentry>
          <varlistentry>
            <term>Network driver (network-driver)</term>
            <listitem>
              <para>Specified in the ClusterTemplate to select the network driver.  The supported
                        drivers are ‘docker’ and ‘flannel’, with ‘docker’ as the default.
                        With the ‘docker’ driver, containers are connected to the ‘docker0’
                        bridge on each node and are assigned local IP address.  With the
                        ‘flannel’ driver, containers are connected to a flat overlay network
                        and are assigned IP address by Flannel.<!--  Refer to the <xref linkend="networking"/>
                        section for more details. --></para>
            </listitem>
          </varlistentry>
          <varlistentry>
            <term>Volume driver (volume-driver)</term>
            <listitem>
              <para>Specified in the ClusterTemplate to select the volume driver to provide
                        persistent storage for containers.  The supported volume driver is
                        ‘rexray’.  The default is no volume driver.  When ‘rexray’ or other
                        volume driver is deployed, you can use the Docker ‘volume’ command to
                        create, mount, unmount, delete volumes in containers.  Cinder block
                        storage is used as the backend to support this feature.<!--
                        Refer to the <xref linkend="storage"/> section for more
			details. --></para>
            </listitem>
          </varlistentry>
          <varlistentry>
            <term>Storage driver (docker-storage-driver)</term>
            <listitem>
              <para>Specified in the ClusterTemplate to select the Docker storage driver.  The
                        supported storage driver are ‘devicemapper’ and ‘overlay’, with
                        ‘devicemapper’ being the default.<!-- Refer to the <xref linkend="storage"/> section for more
                        details. --></para>
            </listitem>
          </varlistentry>
          <varlistentry>
            <term>Image (image)</term>
            <listitem>
              <para>Specified in the ClusterTemplate to indicate the image to boot the servers
                        for the Swarm manager and node.
                        The image binary is loaded in Glance with the attribute
                        ‘os_distro = fedora-atomic’.
                        Current supported image is Fedora Atomic (download from <link xlink:href="https://alt.fedoraproject.org/pub/alt/atomic/stable/Cloud-Images/x86_64/Images">Fedora</link> )</para>
            </listitem>
          </varlistentry>
          <varlistentry>
            <term>TLS (tls-disabled)</term>
            <listitem>
              <para>Transport Layer Security is enabled by default to secure the Swarm API for
                        access by both the users and Magnum.  You will need a key and a
                        signed certificate to access the Swarm API and CLI.  Magnum
                        handles its own key and certificate when interfacing with the
                        Swarm cluster.  In development mode, TLS can be disabled.  Refer to
                        the ‘Transport Layer Security’_ section for details on how to create your
                        key and have Magnum sign your certificate.</para>
            </listitem>
          </varlistentry>
          <varlistentry>
            <term>Log into the servers</term>
            <listitem>
              <para>You can log into the manager and node servers with the account ‘fedora’ and
                        the keypair specified in the ClusterTemplate.</para>
            </listitem>
          </varlistentry>
        </variablelist>
        <para>In addition to the common attributes in the ClusterTemplate, you can specify
            the following attributes that are specific to Swarm by using the
            labels attribute.</para>
        <variablelist>
          <varlistentry>
            <term/>
            <listitem>
              <para>This label corresponds to Swarm parameter for master ‘–strategy’.
                        For more details, refer to the <link xlink:href="https://docs.docker.com/swarm/scheduler/strategy/">Swarm Strategy</link>.
                        Valid values for this label are:</para>
              <itemizedlist>
                <listitem>
                  <para>spread</para>
                </listitem>
                <listitem>
                  <para>binpack</para>
                </listitem>
                <listitem>
                  <para>random</para>
                </listitem>
              </itemizedlist>
            </listitem>
          </varlistentry>
        </variablelist>
      </section>
      <section xml:base="user/index">
        <title>Mesos</title>
        <para>A Mesos cluster consists of a pool of servers running as Mesos slaves,
            managed by a set of servers running as Mesos masters.  Mesos manages
            the resources from the slaves but does not itself deploy containers.
            Instead, one of more Mesos frameworks running on the Mesos cluster would
            accept user requests on their own endpoint, using their particular
            API.  These frameworks would then negotiate the resources with Mesos
            and the containers are deployed on the servers where the resources are
            offered.</para>
        <para>Magnum deploys a Mesos cluster using parameters defined in the ClusterTemplate
            and specified on the ‘cluster-create’ command, for example:</para>
        <screen>magnum cluster-template-create mesos-cluster-template \
                       --image ubuntu-mesos \
                       --keypair testkey \
                       --external-network public \
                       --dns-nameserver 8.8.8.8 \
                       --flavor m1.small \
                       --coe mesos

magnum cluster-create mesos-cluster \
                  --cluster-template mesos-cluster-template \
                  --master-count 3 \
                  --node-count 8</screen>
        <para><!-- Refer to the <xref linkend="clustertemplate"/> and <xref linkend="cluster"/> sections for the full list of
            parameters.  -->Following are further details relevant to Mesos:</para>
        <variablelist>
          <varlistentry>
            <term>What runs on the servers</term>
            <listitem>
              <para>There are two types of servers in the Mesos cluster: masters and slaves.
                        The Docker daemon runs on all servers.  On the servers for master,
                        the Mesos master is run as a process on port 5050 and this is
                        initiated by the upstart service ‘mesos-master’.  Zookeeper is also
                        run on the master servers, initiated by the upstart service
                        ‘zookeeper’.  Zookeeper is used by the master servers for electing
                        the leader among the masters, and by the slave servers and
                        frameworks to determine the current leader.  The framework Marathon
                        is run as a process on port 8080 on the master servers, initiated by
                        the upstart service ‘marathon’.  On the servers for slave, the Mesos
                        slave is run as a process initiated by the upstart service
                        ‘mesos-slave’.</para>
            </listitem>
          </varlistentry>
          <varlistentry>
            <term>Number of master (master-count)</term>
            <listitem>
              <para>Specified in the cluster-create command to indicate how many servers
                        will run as masters in the cluster.  Having more than one will provide
                        high availability.  If the load balancer option is specified, the
                        masters will be in a load balancer pool and the load balancer
                        virtual IP address (VIP) will serve as the Mesos API endpoint.  A
                        floating IP associated with the load balancer VIP will serve as the
                        external Mesos API endpoint.</para>
            </listitem>
          </varlistentry>
          <varlistentry>
            <term>Number of agents (node-count)</term>
            <listitem>
              <para>Specified in the cluster-create command to indicate how many servers
                        will run as Mesos slave in the cluster.  Docker daemon is run locally to
                        host containers from users.  The slaves report their available
                        resources to the master and accept request from the master to deploy
                        tasks from the frameworks.  In this case, the tasks will be to
                        run Docker containers.</para>
            </listitem>
          </varlistentry>
          <varlistentry>
            <term>Network driver (network-driver)</term>
            <listitem>
              <para>Specified in the ClusterTemplate to select the network driver.  Currently
                        ‘docker’ is the only supported driver: containers are connected to
                        the ‘docker0’ bridge on each node and are assigned local IP address.<!--
                        Refer to the <xref linkend="networking"/> section for
			more details. --></para>
            </listitem>
          </varlistentry>
          <varlistentry>
            <term>Volume driver (volume-driver)</term>
            <listitem>
              <para>Specified in the ClusterTemplate to select the volume driver to provide
                        persistent storage for containers.  The supported volume driver is
                        ‘rexray’.  The default is no volume driver.  When ‘rexray’ or other
                        volume driver is deployed, you can use the Docker ‘volume’ command to
                        create, mount, unmount, delete volumes in containers.  Cinder block
                        storage is used as the backend to support this feature.<!--
                        Refer to the <xref linkend="storage"/> section for more details.--></para>
            </listitem>
          </varlistentry>
          <varlistentry>
            <term>Storage driver (docker-storage-driver)</term>
            <listitem>
              <para>This is currently not supported for Mesos.</para>
            </listitem>
          </varlistentry>
        </variablelist>
        <para>Image (image)</para>
        <para>Specified in the ClusterTemplate to indicate the image to boot the servers
                for the Mesos master and slave.  The image binary is loaded in
                Glance with the attribute ‘os_distro = ubuntu’.  You can download
                the <link
		xlink:href="https://fedorapeople.org/groups/magnum/ubuntu-mesos-latest.qcow2">ready-built
		image</link><!-- FIXME ,
                or you can create the image as described below in the <xref linkend="building-mesos-image"/> section-->.</para>
        <variablelist>
          <varlistentry>
            <term>TLS (tls-disabled)</term>
            <listitem>
              <para>Transport Layer Security is currently not implemented yet for Mesos.</para>
            </listitem>
          </varlistentry>
          <varlistentry>
            <term>Log into the servers</term>
            <listitem>
              <para>You can log into the manager and node servers with the account
                        ‘ubuntu’ and the keypair specified in the ClusterTemplate.</para>
            </listitem>
          </varlistentry>
        </variablelist>
        <para>In addition to the common attributes in the baymodel, you can specify
            the following attributes that are specific to Mesos by using the
            labels attribute.</para>
        <variablelist>
          <varlistentry>
            <term/>
            <listitem>
              <para>When the volume driver ‘rexray’ is used, you can mount a data volume
                        backed by Cinder to a host to be accessed by a container.  In this
                        case, the label ‘rexray_preempt’ can optionally be set to True or
                        False to enable any host to take control of the volume regardless of
                        whether other hosts are using the volume.  This will in effect
                        unmount the volume from the current host and remount it on the new
                        host.  If this label is set to false, then rexray will ensure data
                        safety for locking the volume before remounting.  The default value
                        is False.</para>
            </listitem>
          </varlistentry>
          <varlistentry>
            <term/>
            <listitem>
              <para>This label corresponds to the Mesos parameter for slave
                        ‘–isolation’.  The isolators are needed to provide proper isolation
                        according to the runtime configurations specified in the container
                        image.  For more details, refer to the <link xlink:href="http://mesos.apache.org/documentation/latest/configuration/">Mesos configuration</link>
                        and the <link xlink:href="http://mesos.apache.org/documentation/latest/container-image/">Mesos container image support</link>.
                        Valid values for this label are:</para>
              <itemizedlist>
                <listitem>
                  <para>filesystem/posix</para>
                </listitem>
                <listitem>
                  <para>filesystem/linux</para>
                </listitem>
                <listitem>
                  <para>filesystem/shared</para>
                </listitem>
                <listitem>
                  <para>posix/cpu</para>
                </listitem>
                <listitem>
                  <para>posix/mem</para>
                </listitem>
                <listitem>
                  <para>posix/disk</para>
                </listitem>
                <listitem>
                  <para>cgroups/cpu</para>
                </listitem>
                <listitem>
                  <para>cgroups/mem</para>
                </listitem>
                <listitem>
                  <para>docker/runtime</para>
                </listitem>
                <listitem>
                  <para>namespaces/pid</para>
                </listitem>
              </itemizedlist>
            </listitem>
          </varlistentry>
          <varlistentry>
            <term/>
            <listitem>
              <para>This label corresponds to the Mesos parameter for agent
                        ‘–image_providers’, which tells Mesos containerizer what
                        types of container images are allowed.
                        For more details, refer to the <link xlink:href="http://mesos.apache.org/documentation/latest/configuration/">Mesos configuration</link> and
                        the <link xlink:href="http://mesos.apache.org/documentation/latest/container-image/">Mesos container image support</link>.
                        Valid values are:</para>
              <itemizedlist>
                <listitem>
                  <para>appc</para>
                </listitem>
                <listitem>
                  <para>docker</para>
                </listitem>
                <listitem>
                  <para>appc,docker</para>
                </listitem>
              </itemizedlist>
            </listitem>
          </varlistentry>
          <varlistentry>
            <term/>
            <listitem>
              <para>This label corresponds to the Mesos parameter ‘–work_dir’ for slave.
                        For more details, refer to the <link xlink:href="http://mesos.apache.org/documentation/latest/configuration/">Mesos configuration</link>.
                        Valid value is a directory path to use as the work directory for
                        the framework, for example:</para>
              <screen>mesos_slave_work_dir=/tmp/mesos</screen>
            </listitem>
          </varlistentry>
          <varlistentry>
            <term/>
            <listitem>
              <para>This label corresponds to the Mesos parameter for slave
                        ‘–executor_environment_variables’, which passes additional
                        environment variables to the executor and subsequent tasks.
                        For more details, refer to the <link xlink:href="http://mesos.apache.org/documentation/latest/configuration/">Mesos configuration</link>.
                        Valid value is the name of a JSON file, for example:</para>
              <screen>mesos_slave_executor_env_variables=/home/ubuntu/test.json</screen>
              <para>The JSON file should contain environment variables, for example:</para>
              <screen>{
   "PATH": "/bin:/usr/bin",
   "LD_LIBRARY_PATH": "/usr/local/lib"
}</screen>
              <para>By default the executor will inherit the slave’s environment
                        variables.</para>
            </listitem>
          </varlistentry>
        </variablelist>
        <section>
          <title>Building Mesos image</title>
          <para>The boot image for Mesos cluster is an Ubuntu 14.04 base image with the
                following middleware pre-installed:</para>
          <itemizedlist>
            <listitem>
              <para>
                <literal>docker</literal>
              </para>
            </listitem>
            <listitem>
              <para>
                <literal>zookeeper</literal>
              </para>
            </listitem>
            <listitem>
              <para>
                <literal>mesos</literal>
              </para>
            </listitem>
            <listitem>
              <para>
                <literal>marathon</literal>
              </para>
            </listitem>
          </itemizedlist>
          <para>The cluster driver provides two ways to create this image, as follows.</para>
          <section>
            <title>Diskimage-builder</title>
            <para>To run the <link xlink:href="http://docs.openstack.org/developer/diskimage-builder">diskimage-builder</link> tool
                    manually, use the provided <link xlink:href="http://git.openstack.org/cgit/openstack/magnum/tree/magnum/drivers/mesos_ubuntu_v1/image/mesos/">elements</link>.
                    Following are the typical steps to use the diskimage-builder tool on
                    an Ubuntu server:</para>
            <screen>$ sudo apt-get update
$ sudo apt-get install git qemu-utils python-pip
$ sudo pip install diskimage-builder

$ git clone https://git.openstack.org/openstack/magnum
$ git clone https://git.openstack.org/openstack/dib-utils.git
$ git clone https://git.openstack.org/openstack/tripleo-image-elements.git
$ git clone https://git.openstack.org/openstack/heat-templates.git
$ export PATH="${PWD}/dib-utils/bin:$PATH"
$ export ELEMENTS_PATH=tripleo-image-elements/elements:heat-templates/hot/software-config/elements:magnum/magnum/drivers/mesos_ubuntu_v1/image/mesos
$ export DIB_RELEASE=trusty

$ disk-image-create ubuntu vm docker mesos \
    os-collect-config os-refresh-config os-apply-config \
    heat-config heat-config-script \
    -o ubuntu-mesos.qcow2</screen>
          </section>
          <section>
            <title>Dockerfile</title>
            <para>To build the image as above but within a Docker container, use the
                    provided <link xlink:href="http://git.openstack.org/cgit/openstack/magnum/tree/magnum/drivers/mesos_ubuntu_v1/image/Dockerfile">Dockerfile</link>.
                    The output image will be saved as ‘/tmp/ubuntu-mesos.qcow2’.
                    Following are the typical steps to run a Docker container to build the image:</para>
            <screen>$ git clone https://git.openstack.org/openstack/magnum
$ cd magnum/magnum/drivers/mesos_ubuntu_v1/image
$ sudo docker build -t magnum/mesos-builder .
$ sudo docker run -v /tmp:/output --rm -ti --privileged magnum/mesos-builder
...
Image file /output/ubuntu-mesos.qcow2 created...</screen>
          </section>
        </section>
        <section>
          <title>Using Marathon</title>
          <para>Marathon is a Mesos framework for long running applications.  Docker
                containers can be deployed via Marathon’s REST API.  To get the
                endpoint for Marathon, run the cluster-show command and look for the
                property ‘api_address’.  Marathon’s endpoint is port 8080 on this IP
                address, so the web console can be accessed at:</para>
          <screen>http://&lt;api_address&gt;:8080/</screen>
          <para>Refer to Marathon documentation for details on running applications.
                For example, you can ‘post’ a JSON app description to
                <literal>http://&lt;api_address&gt;:8080/apps</literal> to deploy a Docker container:</para>
          <screen>$ cat &gt; app.json &lt;&lt; END
{
  "container": {
    "type": "DOCKER",
    "docker": {
      "image": "libmesos/ubuntu"
    }
  },
  "id": "ubuntu",
  "instances": 1,
  "cpus": 0.5,
  "mem": 512,
  "uris": [],
  "cmd": "while sleep 10; do date -u +%T; done"
}
END
$ API_ADDRESS=$(magnum cluster-show mesos-cluster | awk '/ api_address /{print $4}')
$ curl -X POST -H "Content-Type: application/json" \
    http://${API_ADDRESS}:8080/v2/apps -d@app.json</screen>
        </section>
      </section>
      <section xml:base="user/index">
        <title>Transport Layer Security</title>
        <para>Magnum uses TLS to secure communication between a cluster’s services and
            the outside world.  TLS is a complex subject, and many guides on it
            exist already.  This guide will not attempt to fully describe TLS, but
            instead will only cover the necessary steps to get a client set up to
            talk to a cluster with TLS. A more in-depth guide on TLS can be found in
            the <link xlink:href="https://www.feistyduck.com/books/openssl-cookbook/">OpenSSL Cookbook</link> by Ivan Ristić.</para>
        <para>TLS is employed at 3 points in a cluster:</para>
        <procedure>
          <step>
            <para>By Magnum to communicate with the cluster API endpoint</para>
          </step>
          <step>
            <para>By the cluster worker nodes to communicate with the master nodes</para>
          </step>
          <step>
            <para>By the end-user when they use the native client libraries to
                    interact with the cluster.  This applies to both a CLI or a program
                    that uses a client for the particular cluster.  Each client needs a
                    valid certificate to authenticate and communicate with a cluster.</para>
          </step>
        </procedure>
        <para>The first two cases are implemented internally by Magnum and are not
            exposed to the users, while the last case involves the users and is
            described in more details below.</para>
        <section>
          <title>Deploying a secure cluster</title>
          <para>Current TLS support is summarized below:</para>
          <informaltable>
            <tgroup cols="2">
              <colspec colname="c1" colwidth="12"/>
              <colspec colname="c2" colwidth="13"/>
              <thead>
                <row>
                  <entry>
                    <para>COE</para>
                  </entry>
                  <entry>
                    <para>TLS support</para>
                  </entry>
                </row>
              </thead>
              <tbody>
                <row>
                  <entry>
                    <para>Kubernetes</para>
                  </entry>
                  <entry>
                    <para>yes</para>
                  </entry>
                </row>
                <row>
                  <entry>
                    <para>Swarm</para>
                  </entry>
                  <entry>
                    <para>yes</para>
                  </entry>
                </row>
                <row>
                  <entry>
                    <para>Mesos</para>
                  </entry>
                  <entry>
                    <para>no</para>
                  </entry>
                </row>
              </tbody>
            </tgroup>
          </informaltable>
          <para>For cluster type with TLS support, e.g. Kubernetes and Swarm, TLS is
                enabled by default.  To disable TLS in Magnum, you can specify the
                parameter ‘–tls-disabled’ in the ClusterTemplate.  Please note it is not
                recommended to disable TLS due to security reasons.</para>
          <para>In the following example, Kubernetes is used to illustrate a secure
                cluster, but the steps are similar for other cluster types that have TLS
                support.</para>
          <para>First, create a ClusterTemplate; by default TLS is enabled in
                Magnum, therefore it does not need to be specified via a parameter:</para>
          <screen>magnum cluster-template-create secure-kubernetes \
                           --keypair default \
                           --external-network public \
                           --image fedora-atomic-latest \
                           --dns-nameserver 8.8.8.8 \
                           --flavor m1.small \
                           --docker-volume-size 3 \
                           --coe kubernetes \
                           --network-driver flannel

+-----------------------+--------------------------------------+
| Property              | Value                                |
+-----------------------+--------------------------------------+
| insecure_registry     | None                                 |
| http_proxy            | None                                 |
| updated_at            | None                                 |
| master_flavor_id      | None                                 |
| uuid                  | 5519b24a-621c-413c-832f-c30424528b31 |
| no_proxy              | None                                 |
| https_proxy           | None                                 |
| tls_disabled          | False                                |
| keypair_id            | time4funkey                          |
| public                | False                                |
| labels                | {}                                   |
| docker_volume_size    | 5                                    |
| server_type           | vm                                   |
| external_network_id   | public                               |
| cluster_distro        | fedora-atomic                        |
| image_id              | fedora-atomic-latest                 |
| volume_driver         | None                                 |
| registry_enabled      | False                                |
| docker_storage_driver | devicemapper                         |
| apiserver_port        | None                                 |
| name                  | secure-kubernetes                    |
| created_at            | 2016-07-25T23:09:50+00:00            |
| network_driver        | flannel                              |
| fixed_network         | None                                 |
| coe                   | kubernetes                           |
| flavor_id             | m1.small                             |
| dns_nameserver        | 8.8.8.8                              |
+-----------------------+--------------------------------------+</screen>
          <para>Now create a cluster. Use the ClusterTemplate name as a template for cluster
                creation:</para>
          <screen>magnum cluster-create secure-k8s-cluster \
                      --cluster-template secure-kubernetes \
                      --node-count 1

+--------------------+------------------------------------------------------------+
| Property           | Value                                                      |
+--------------------+------------------------------------------------------------+
| status             | CREATE_IN_PROGRESS                                         |
| uuid               | 3968ffd5-678d-4555-9737-35f191340fda                       |
| stack_id           | c96b66dd-2109-4ae2-b510-b3428f1e8761                       |
| status_reason      | None                                                       |
| created_at         | 2016-07-25T23:14:06+00:00                                  |
| updated_at         | None                                                       |
| create_timeout     | 0                                                          |
| api_address        | None                                                       |
| coe_version        | -                                                          |
| cluster_template_id| 5519b24a-621c-413c-832f-c30424528b31                       |
| master_addresses   | None                                                       |
| node_count         | 1                                                          |
| node_addresses     | None                                                       |
| master_count       | 1                                                          |
| container_version  | -                                                          |
| discovery_url      | https://discovery.etcd.io/ba52a8178e7364d43a323ee4387cf28e |
| name               | secure-k8s-cluster                                          |
+--------------------+------------------------------------------------------------+</screen>
          <para>Now run cluster-show command to get the details of the cluster and verify that
                the api_address is ‘https’:</para>
          <screen>magnum cluster-show secure-k8scluster
+--------------------+------------------------------------------------------------+
| Property           | Value                                                      |
+--------------------+------------------------------------------------------------+
| status             | CREATE_COMPLETE                                            |
| uuid               | 04952c60-a338-437f-a7e7-d016d1d00e65                       |
| stack_id           | b7bf72ce-b08e-4768-8201-e63a99346898                       |
| status_reason      | Stack CREATE completed successfully                        |
| created_at         | 2016-07-25T23:14:06+00:00                                  |
| updated_at         | 2016-07-25T23:14:10+00:00                                  |
| create_timeout     | 60                                                         |
| coe_version        | v1.2.0                                                     |
| api_address        | https://192.168.19.86:6443                                 |
| cluster_template_id| da2825a0-6d09-4208-b39e-b2db666f1118                       |
| master_addresses   | ['192.168.19.87']                                          |
| node_count         | 1                                                          |
| node_addresses     | ['192.168.19.88']                                          |
| master_count       | 1                                                          |
| container_version  | 1.9.1                                                      |
| discovery_url      | https://discovery.etcd.io/3b7fb09733429d16679484673ba3bfd5 |
| name               | secure-k8s-cluster                                          |
+--------------------+------------------------------------------------------------+</screen>
          <para>You can see the api_address contains https in the URL, showing that
                the Kubernetes services are configured securely with SSL certificates
                and now any communication to kube-apiserver will be over https.</para>
        </section>
        <section>
          <title>Interfacing with a secure cluster</title>
          <para>To communicate with the API endpoint of a secure cluster, you will need so
                supply 3 SSL artifacts:</para>
          <procedure>
            <step>
              <para>Your client key</para>
            </step>
            <step>
              <para>A certificate for your client key that has been signed by a
                        Certificate Authority (CA)</para>
            </step>
            <step>
              <para>The certificate of the CA</para>
            </step>
          </procedure>
          <para>There are two ways to obtain these 3 artifacts.</para>
          <section>
            <title>Automated</title>
            <para>Magnum provides the command ‘cluster-config’ to help the user in setting
                    up the environment and artifacts for TLS, for example:</para>
            <screen>magnum cluster-config swarm-cluster --dir myclusterconfig</screen>
            <para>This will display the necessary environment variables, which you
                    can add to your environment:</para>
            <screen>export DOCKER_HOST=tcp://172.24.4.5:2376
export DOCKER_CERT_PATH=myclusterconfig
export DOCKER_TLS_VERIFY=True</screen>
            <para>And the artifacts are placed in the directory specified:</para>
            <screen>ca.pem
cert.pem
key.pem</screen>
            <para>You can now use the native client to interact with the COE.
                    The variables and artifacts are unique to the cluster.</para>
            <para>The parameters for ‘bay-config’ are as follows:</para>
            <variablelist>
              <varlistentry>
                <term>–dir &lt;dirname&gt;</term>
                <listitem>
                  <para>Directory to save the certificate and config files.</para>
                </listitem>
              </varlistentry>
            </variablelist>
            <variablelist>
              <varlistentry>
                <term>
                  <option>--force</option>
                </term>
                <listitem>
                  <para>Overwrite existing files in the directory specified.</para>
                </listitem>
              </varlistentry>
            </variablelist>
          </section>
          <section>
            <title>Manual</title>
            <para>You can create the key and certificates manually using the following steps.</para>
            <variablelist>
              <varlistentry>
                <term>Client Key</term>
                <listitem>
                  <para>Your personal private key is essentially a cryptographically generated
                                string of bytes. It should be protected in the same manner as a
                                password. To generate an RSA key, you can use the ‘genrsa’ command of
                                the ‘openssl’ tool:</para>
                  <screen>openssl genrsa -out key.pem 4096</screen>
                  <para>This command generates a 4096 byte RSA key at key.pem.</para>
                </listitem>
              </varlistentry>
              <varlistentry>
                <term>Signed Certificate</term>
                <listitem>
                  <para>To authenticate your key, you need to have it signed by a CA.  First
                                generate the Certificate Signing Request (CSR).  The CSR will be
                                used by Magnum to generate a signed certificate that you will use to
                                communicate with the cluster.  To generate a CSR, openssl requires a
                                config file that specifies a few values.  Using the example template
                                below, you can fill in the ‘CN’ value with your name and save it as
                                client.conf:</para>
                  <screen>$ cat &gt; client.conf &lt;&lt; END
[req]
distinguished_name = req_distinguished_name
req_extensions     = req_ext
prompt = no
[req_distinguished_name]
CN = Your Name
[req_ext]
extendedKeyUsage = clientAuth
END</screen>
                  <para>Once you have client.conf, you can run the openssl ‘req’ command to
                                generate the CSR:</para>
                  <screen>openssl req -new -days 365 \
    -config client.conf \
    -key key.pem \
    -out client.csr</screen>
                  <para>Now that you have your client CSR, you can use the Magnum CLI to
                                send it off to Magnum to get it signed:</para>
                  <screen>magnum ca-sign --cluster secure-k8s-cluster --csr client.csr &gt; cert.pem</screen>
                </listitem>
              </varlistentry>
              <varlistentry>
                <term>Certificate Authority</term>
                <listitem>
                  <para>The final artifact you need to retrieve is the CA certificate for
                                the cluster. This is used by your native client to ensure you are only
                                communicating with hosts that Magnum set up:</para>
                  <screen>magnum ca-show --cluster secure-k8s-cluster &gt; ca.pem</screen>
                </listitem>
              </varlistentry>
              <varlistentry>
                <term>Rotate Certificate</term>
                <listitem>
                  <para>To rotate the CA certificate for a cluster and invalidate all user
                                certificates, you can use the following command:</para>
                  <screen>magnum ca-rotate --cluster secure-k8s-cluster</screen>
                </listitem>
              </varlistentry>
            </variablelist>
          </section>
        </section>
        <section>
          <title>User Examples</title>
          <para>Here are some examples for using the CLI on a secure Kubernetes and
                Swarm cluster.  You can perform all the TLS set up automatically by:</para>
          <screen>eval $(magnum cluster-config &lt;cluster-name&gt;)</screen>
          <para>Or you can perform the manual steps as described above and specify
                the TLS options on the CLI.  The SSL artifacts are assumed to be
                saved in local files as follows:</para>
          <screen>- key.pem: your SSL key
- cert.pem: signed certificate
- ca.pem: certificate for cluster CA</screen>
          <para>For Kubernetes, you need to get ‘kubectl’, a kubernetes CLI tool, to
                communicate with the cluster:</para>
          <screen>curl -O https://storage.googleapis.com/kubernetes-release/release/v1.2.0/bin/linux/amd64/kubectl
chmod +x kubectl
sudo mv kubectl /usr/local/bin/kubectl</screen>
          <para>Now let’s run some ‘kubectl’ commands to check the secure communication.
                If you used ‘cluster-config’, then you can simply run the ‘kubectl’ command
                without having to specify the TLS options since they have been defined
                in the environment:</para>
          <screen>kubectl version
Client Version: version.Info{Major:"1", Minor:"0", GitVersion:"v1.2.0", GitCommit:"cffae0523cfa80ddf917aba69f08508b91f603d5", GitTreeState:"clean"}
Server Version: version.Info{Major:"1", Minor:"0", GitVersion:"v1.2.0", GitCommit:"cffae0523cfa80ddf917aba69f08508b91f603d5", GitTreeState:"clean"}</screen>
          <para>You can specify the TLS options manually as follows:</para>
          <screen>KUBERNETES_URL=$(magnum cluster-show secure-k8s-cluster |
                 awk '/ api_address /{print $4}')
kubectl version --certificate-authority=ca.pem \
                --client-key=key.pem \
                --client-certificate=cert.pem -s $KUBERNETES_URL

kubectl create -f redis-master.yaml --certificate-authority=ca.pem \
                                    --client-key=key.pem \
                                    --client-certificate=cert.pem -s $KUBERNETES_URL

pods/test2

kubectl get pods --certificate-authority=ca.pem \
                 --client-key=key.pem \
                 --client-certificate=cert.pem -s $KUBERNETES_URL
NAME           READY     STATUS    RESTARTS   AGE
redis-master   2/2       Running   0          1m</screen>
          <para>Beside using the environment variables, you can also configure ‘kubectl’
                to remember the TLS options:</para>
          <screen>kubectl config set-cluster secure-k8s-cluster --server=${KUBERNETES_URL} \
    --certificate-authority=${PWD}/ca.pem
kubectl config set-credentials client --certificate-authority=${PWD}/ca.pem \
    --client-key=${PWD}/key.pem --client-certificate=${PWD}/cert.pem
kubectl config set-context secure-k8scluster --cluster=secure-k8scluster --user=client
kubectl config use-context secure-k8scluster</screen>
          <para>Then you can use ‘kubectl’ commands without the certificates:</para>
          <screen>kubectl get pods
NAME           READY     STATUS    RESTARTS   AGE
redis-master   2/2       Running   0          1m</screen>
          <para>Access to Kubernetes User Interface:</para>
          <screen>curl -L ${KUBERNETES_URL}/ui --cacert ca.pem --key key.pem \
    --cert cert.pem</screen>
          <para>You may also set up ‘kubectl’ proxy which will use your client
                certificates to allow you to browse to a local address to use the UI
                without installing a certificate in your browser:</para>
          <screen>kubectl proxy --api-prefix=/ --certificate-authority=ca.pem --client-key=key.pem \
              --client-certificate=cert.pem -s $KUBERNETES_URL</screen>
          <para>You can then open <link xlink:href="http://localhost:8001/ui"/> in your browser.</para>
          <para>The examples for Docker are similar.  With ‘cluster-config’ set up,
                you can just run docker commands without TLS options.  To specify the
                TLS options manually:</para>
          <screen>docker -H tcp://192.168.19.86:2376 --tlsverify \
       --tlscacert ca.pem \
       --tlskey key.pem \
       --tlscert cert.pem \
       info</screen>
        </section>
        <section>
          <title>Storing the certificates</title>
          <para>Magnum generates and maintains a certificate for each cluster so that it
                can also communicate securely with the cluster.  As a result, it is
                necessary to store the certificates in a secure manner.  Magnum
                provides the following methods for storing the certificates and this
                is configured in /etc/magnum/magnum.conf in the section [certificates]
                with the parameter ‘cert_manager_type’.</para>
          <procedure>
            <step>
              <para>Barbican:
                        Barbican is a service in OpenStack for storing secrets.  It is used
                        by Magnum to store the certificates when cert_manager_type is
                        configured as:</para>
              <screen>cert_manager_type = barbican</screen>
              <para>This is the recommended configuration for a production environment.
                        Magnum will interface with Barbican to store and retrieve
                        certificates, delegating the task of securing the certificates to
                        Barbican.</para>
            </step>
            <step>
              <para>Magnum database:
                        In some cases, a user may want an alternative to storing the
                        certificates that does not require Barbican.  This can be a
                        development environment, or a private cloud that has been secured
                        by other means.  Magnum can store the certificates in its own
                        database; this is done with the configuration:</para>
              <screen>cert_manager_type = x509keypair</screen>
              <para>This storage mode is only as secure as the controller server that
                        hosts the database for the OpenStack services.</para>
            </step>
            <step>
              <para>Local store:
                        As another alternative that does not require Barbican, Magnum can
                        simply store the certificates on the local host filesystem where the
                        conductor is running, using the configuration:</para>
              <screen>cert_manager_type = local</screen>
              <para>Note that this mode is only supported when there is a single Magnum
                        conductor running since the certificates are stored locally.  The
                        ‘local’ mode is not recommended for a production environment.</para>
            </step>
          </procedure>
          <para>For the nodes, the certificates for communicating with the masters are
                stored locally and the nodes are assumed to be secured.</para>
        </section>
      </section>
      <section xml:base="user/index">
        <title>Networking</title>
        <para>There are two components that make up the networking in a cluster.</para>
        <procedure>
          <step>
            <para>The Neutron infrastructure for the cluster: this includes the
                    private network, subnet, ports, routers, load balancers, etc.</para>
          </step>
          <step>
            <para>The networking model presented to the containers: this is what the
                    containers see in communicating with each other and to the external
                    world. Typically this consists of a driver deployed on each node.</para>
          </step>
        </procedure>
        <para>The two components are deployed and managed separately.  The Neutron
            infrastructure is the integration with OpenStack; therefore, it
            is stable and more or less similar across different COE
            types.  The networking model, on the other hand, is specific to the
            COE type and is still under active development in the various
            COE communities, for example,
            <link xlink:href="https://github.com/docker/libnetwork">Docker libnetwork</link> and
            <link xlink:href="https://github.com/kubernetes/kubernetes/blob/release-1.1/docs/design/networking.md">Kubernetes Container Networking</link>.
            As a result, the implementation for the networking models is evolving and
            new models are likely to be introduced in the future.</para>
        <para>For the Neutron infrastructure, the following configuration can
            be set in the ClusterTemplate:</para>
        <variablelist>
          <varlistentry>
            <term>external-network</term>
            <listitem>
              <para>The external Neutron network ID to connect to this cluster. This
                        is used to connect the cluster to the external internet, allowing
                        the nodes in the cluster to access external URL for discovery, image
                        download, etc.  If not specified, the default value is “public” and this
                        is valid for a typical devstack.</para>
            </listitem>
          </varlistentry>
          <varlistentry>
            <term>fixed-network</term>
            <listitem>
              <para>The Neutron network to use as the private network for the cluster nodes.
                        If not specified, a new Neutron private network will be created.</para>
            </listitem>
          </varlistentry>
          <varlistentry>
            <term>dns-nameserver</term>
            <listitem>
              <para>The DNS nameserver to use for this cluster.  This is an IP address for
                        the server and it is used to configure the Neutron subnet of the
                        cluster (dns_nameservers).  If not specified, the default DNS is
                        8.8.8.8, the publicly available DNS.</para>
            </listitem>
          </varlistentry>
          <varlistentry>
            <term>http-proxy, https-proxy, no-proxy</term>
            <listitem>
              <para>The proxy for the nodes in the cluster, to be used when the cluster is
                        behind a firewall and containers cannot access URL’s on the external
                        internet directly.  For the parameter http-proxy and https-proxy, the
                        value to provide is a URL and it will be set in the environment
                        variable HTTP_PROXY and HTTPS_PROXY respectively in the nodes.  For
                        the parameter no-proxy, the value to provide is an IP or list of IP’s
                        separated by comma.  Likewise, the value will be set in the
                        environment variable NO_PROXY in the nodes.</para>
            </listitem>
          </varlistentry>
        </variablelist>
        <para>For the networking model to the container, the following configuration
            can be set in the ClusterTemplate:</para>
        <variablelist>
          <varlistentry>
            <term>network-driver</term>
            <listitem>
              <para>The network driver name for instantiating container networks.
                        Currently, the following network drivers are supported:</para>
              <informaltable>
                <tgroup cols="4">
                  <colspec colname="c1" colwidth="8"/>
                  <colspec colname="c2" colwidth="13"/>
                  <colspec colname="c3" colwidth="11"/>
                  <colspec colname="c4" colwidth="13"/>
                  <thead>
                    <row>
                      <entry>
                        <para>Driver</para>
                      </entry>
                      <entry>
                        <para>Kubernetes</para>
                      </entry>
                      <entry>
                        <para>Swarm</para>
                      </entry>
                      <entry>
                        <para>Mesos</para>
                      </entry>
                    </row>
                  </thead>
                  <tbody>
                    <row>
                      <entry>
                        <para>Flannel</para>
                      </entry>
                      <entry>
                        <para>supported</para>
                      </entry>
                      <entry>
                        <para>supported</para>
                      </entry>
                      <entry>
                        <para>unsupported</para>
                      </entry>
                    </row>
                    <row>
                      <entry>
                        <para>Docker</para>
                      </entry>
                      <entry>
                        <para>unsupported</para>
                      </entry>
                      <entry>
                        <para>supported</para>
                      </entry>
                      <entry>
                        <para>supported</para>
                      </entry>
                    </row>
                  </tbody>
                </tgroup>
              </informaltable>
              <para>If not specified, the default driver is Flannel for Kubernetes, and
                        Docker for Swarm and Mesos.</para>
            </listitem>
          </varlistentry>
        </variablelist>
        <para>Particular network driver may require its own set of parameters for
            configuration, and these parameters are specified through the labels
            in the ClusterTemplate.  Labels are arbitrary key=value pairs.</para>
        <para>When Flannel is specified as the network driver, the following
            optional labels can be added:</para>
        <variablelist>
          <varlistentry>
            <term/>
            <listitem>
              <para>IPv4 network in CIDR format to use for the entire Flannel network.
                        If not specified, the default is 10.100.0.0/16.</para>
            </listitem>
          </varlistentry>
          <varlistentry>
            <term/>
            <listitem>
              <para>The size of the subnet allocated to each host. If not specified, the
                        default is 24.</para>
            </listitem>
          </varlistentry>
          <varlistentry>
            <term/>
            <listitem>
              <para>The type of backend for Flannel.  Possible values are <emphasis>udp, vxlan,
                            host-gw</emphasis>.  If not specified, the default is <emphasis>udp</emphasis>.  Selecting the
                        best backend depends on your networking.  Generally, <emphasis>udp</emphasis> is
                        the most generally supported backend since there is little
                        requirement on the network, but it typically offers the lowest
                        performance.  The <emphasis>vxlan</emphasis> backend performs better, but requires
                        vxlan support in the kernel so the image used to provision the
                        nodes needs to include this support.  The <emphasis>host-gw</emphasis> backend offers
                        the best performance since it does not actually encapsulate
                        messages, but it requires all the nodes to be on the same L2
                        network.  The private Neutron network that Magnum creates does
                        meet this requirement;  therefore if the parameter <emphasis>fixed_network</emphasis>
                        is not specified in the ClusterTemplate, <emphasis>host-gw</emphasis> is the best choice for
                        the Flannel backend.</para>
            </listitem>
          </varlistentry>
        </variablelist>
      </section>
      <section xml:base="user/index">
        <title>High Availability</title>
        <para>
          <emphasis>To be filled in</emphasis>
        </para>
      </section>
      <section xml:base="user/index">
        <title>Scaling</title>
        <section>
          <title>Performance tuning for periodic task</title>
          <para>Magnum’s periodic task performs a <literal>stack-get</literal> operation on the Heat stack
                underlying each of its clusters. If you have a large amount of clusters this
                can create considerable load on the Heat API. To reduce that load you can
                configure Magnum to perform one global <literal>stack-list</literal> per periodic task instead
                of one per cluster. This is disabled by default, both from the Heat and Magnum
                side since it causes a security issue, though: any user in any tenant holding
                the <literal>admin</literal> role can perform a global <literal>stack-list</literal> operation if Heat is
                configured to allow it for Magnum. If you want to enable it nonetheless,
                proceed as follows:</para>
          <procedure>
            <step>
              <para>Set <literal>periodic_global_stack_list</literal> in magnum.conf to <literal>True</literal>
                        (<literal>False</literal> by default).</para>
            </step>
            <step>
              <para>Update heat policy to allow magnum list stacks. To this end, edit your heat
                        policy file, usually etc/heat/policy.json``:</para>
              <screen language="ini">...
stacks:global_index: "rule:context_is_admin",</screen>
              <para>Now restart heat.</para>
            </step>
          </procedure>
        </section>
        <section>
          <title>Containers and nodes</title>
          <para>Scaling containers and nodes refers to increasing or decreasing
                allocated system resources.  Scaling is a broad topic and involves
                many dimensions.  In the context of Magnum in this guide, we consider
                the following issues:</para>
          <itemizedlist>
            <listitem>
              <para>Scaling containers and scaling cluster nodes (infrastructure)</para>
            </listitem>
            <listitem>
              <para>Manual and automatic scaling</para>
            </listitem>
          </itemizedlist>
          <para>Since this is an active area of development, a complete solution
                covering all issues does not exist yet, but partial solutions are
                emerging.</para>
          <para>Scaling containers involves managing the number of instances of the
                container by replicating or deleting instances.  This can be used to
                respond to change in the workload being supported by the application;
                in this case, it is typically driven by certain metrics relevant to the
                application such as response time, etc.  Other use cases include
                rolling upgrade, where a new version of a service can gradually be
                scaled up while the older version is gradually scaled down.  Scaling
                containers is supported at the COE level and is specific to each COE
                as well as the version of the COE.  You will need to refer to the
                documentation for the proper COE version for full details, but
                following are some pointers for reference.</para>
          <para>For Kubernetes, pods are scaled manually by setting the count in the
                replication controller.  Kubernetes version 1.3 and later also
                supports <link xlink:href="http://blog.kubernetes.io/2016/07/autoscaling-in-kubernetes.html">autoscaling</link>.
                For Docker, the tool ‘Docker Compose’ provides the command
                <link xlink:href="https://docs.docker.com/compose/reference/scale/">docker-compose scale</link> which lets you
                manually set the number of instances of a container.  For Swarm
                version 1.12 and later, services can also be scaled manually through
                the command <link xlink:href="https://docs.docker.com/engine/swarm/swarm-tutorial/scale-service/">docker service scale</link>.
                Automatic scaling for Swarm is not yet available.  Mesos manages the
                resources and does not support scaling directly; instead, this is
                provided by frameworks running within Mesos.  With the Marathon
                framework currently supported in the Mesos cluster, you can use the
                <link xlink:href="https://mesosphere.github.io/marathon/docs/application-basics.html">scale operation</link>
                on the Marathon UI or through a REST API call to manually set the
                attribute ‘instance’ for a container.</para>
          <para>Scaling the cluster nodes involves managing the number of nodes in the
                cluster by adding more nodes or removing nodes.  There is no direct
                correlation between the number of nodes and the number of containers
                that can be hosted since the resources consumed (memory, CPU, etc)
                depend on the containers.  However, if a certain resource is exhausted
                in the cluster, adding more nodes would add more resources for hosting
                more containers.  As part of the infrastructure management, Magnum
                supports manual scaling through the attribute ‘node_count’ in the
                cluster, so you can scale the cluster simply by changing this
                attribute:</para>
          <screen>magnum cluster-update mycluster replace node_count=2</screen>
          <para><!--Refer to the section <xref linkend="scale"/> lifecycle operation for more details.--></para>
          <para>Adding nodes to a cluster is straightforward: Magnum deploys
                additional VMs or baremetal servers through the heat templates and
                invokes the COE-specific mechanism for registering the new nodes to
                update the available resources in the cluster.  Afterward, it is up to
                the COE or user to re-balance the workload by launching new container
                instances or re-launching dead instances on the new nodes.</para>
          <para>Removing nodes from a cluster requires some more care to ensure
                continuous operation of the containers since the nodes being removed
                may be actively hosting some containers.  Magnum performs a simple
                heuristic that is specific to the COE to find the best node candidates
                for removal, as follows:</para>
          <variablelist>
            <varlistentry>
              <term>Kubernetes</term>
              <listitem>
                <para>Magnum scans the pods in the namespace ‘Default’ to determine the
                            nodes that are <emphasis>not</emphasis> hosting any (empty nodes).  If the number of
                            nodes to be removed is equal or less than the number of these empty
                            nodes, these nodes will be removed from the cluster.  If the number
                            of nodes to be removed is larger than the number of empty nodes, a
                            warning message will be sent to the Magnum log and the empty nodes
                            along with additional nodes will be removed from the cluster.  The
                            additional nodes are selected randomly and the pods running on them
                            will be deleted without warning.  For this reason, a good practice
                            is to manage the pods through the replication controller so that the
                            deleted pods will be relaunched elsewhere in the cluster.  Note also
                            that even when only the empty nodes are removed, there is no
                            guarantee that no pod will be deleted because there is no locking to
                            ensure that Kubernetes will not launch new pods on these nodes after
                            Magnum has scanned the pods.</para>
              </listitem>
            </varlistentry>
            <varlistentry>
              <term>Swarm</term>
              <listitem>
                <para>No node selection heuristic is currently supported.  If you decrease
                            the node_count, a node will be chosen by magnum without
                            consideration of what containers are running on the selected node.</para>
              </listitem>
            </varlistentry>
            <varlistentry>
              <term>Mesos</term>
              <listitem>
                <para>Magnum scans the running tasks on Marathon server to determine the
                            nodes on which there is <emphasis>no</emphasis> task running (empty nodes). If the
                            number of nodes to be removed is equal or less than the number of
                            these empty nodes, these nodes will be removed from the cluster.
                            If the number of nodes to be removed is larger than the number of
                            empty nodes, a warning message will be sent to the Magnum log and
                            the empty nodes along with additional nodes will be removed from the
                            cluster. The additional nodes are selected randomly and the containers
                            running on them will be deleted without warning. Note that even when
                            only the empty nodes are removed, there is no guarantee that no
                            container will be deleted because there is no locking to ensure that
                            Mesos will not launch new containers on these nodes after Magnum
                            has scanned the tasks.</para>
              </listitem>
            </varlistentry>
          </variablelist>
          <para>Currently, scaling containers and scaling cluster nodes are handled
                separately, but in many use cases, there are interactions between the
                two operations.  For instance, scaling up the containers may exhaust
                the available resources in the cluster, thereby requiring scaling up
                the cluster nodes as well.  Many complex issues are involved in
                managing this interaction.  A presentation at the OpenStack Tokyo
                Summit 2015 covered some of these issues along with some early
                proposals, <link xlink:href="https://www.openstack.org/summit/tokyo-2015/videos/presentation/exploring-magnum-and-senlin-integration-for-autoscaling-containers">Exploring Magnum and Senlin integration for autoscaling
                    containers</link>.
                This remains an active area of discussion and research.</para>
        </section>
      </section>
      <section xml:base="user/index">
        <title>Storage</title>
        <para>Currently Cinder provides the block storage to the containers, and the
            storage is made available in two ways: as ephemeral storage and as
            persistent storage.</para>
        <section>
          <title>Ephemeral storage</title>
          <para>The filesystem for the container consists of multiple layers from the
                image and a top layer that holds the modification made by the
                container.  This top layer requires storage space and the storage is
                configured in the Docker daemon through a number of storage options.
                When the container is removed, the storage allocated to the particular
                container is also deleted.</para>
          <para>Magnum can manage the containers’ filesystem in two ways, storing them
                on the local disk of the compute instances or in a separate Cinder block
                volume for each node in the cluster, mounts it to the node and
                configures it to be used as ephemeral storage.  Users can specify the
                size of the Cinder volume with the ClusterTemplate attribute
                ‘docker-volume-size’. Currently the block size is fixed at cluster
                creation time, but future lifecycle operations may allow modifying the
                block size during the life of the cluster.</para>
          <variablelist>
            <varlistentry>
              <term/>
              <listitem>
                <para>For drivers that support additional volumes for container storage, a
                            label named ‘docker_volume_type’ is exposed so that users can select
                            different cinder volume types for their volumes. The default volume
                            <emphasis>must</emphasis> be set in ‘default_docker_volume_type’ in the ‘cinder’ section
                            of magnum.conf, an obvious value is the default volume type set in
                            cinder.conf of your cinder deployment . Please note, that
                            docker_volume_type refers to a cinder volume type and it is unrelated
                            to docker or kubernetes volumes.</para>
              </listitem>
            </varlistentry>
          </variablelist>
          <para>Both local disk and the Cinder block storage can be used with a number
                of Docker storage drivers available.</para>
          <itemizedlist>
            <listitem>
              <para>‘devicemapper’: When used with a dedicated Cinder volume it is
                        configured using direct-lvm and offers very good performance. If it’s
                        used with the compute instance’s local disk uses a loopback device
                        offering poor performance and it’s not recommended for production
                        environments. Using the ‘devicemapper’ driver does allow the use of
                        SELinux.</para>
            </listitem>
            <listitem>
              <para>‘overlay’ When used with a dedicated Cinder volume offers as good
                        or better performance than devicemapper. If used on the local disk of
                        the compute instance (especially with high IOPS drives) you can get
                        significant performance gains. However, for kernel versions less than
                        4.9, SELinux must be disabled inside the containers resulting in worse
                        container isolation, although it still runs in enforcing mode on the
                        cluster compute instances.</para>
            </listitem>
          </itemizedlist>
        </section>
        <section>
          <title>Persistent storage</title>
          <para>In some use cases, data read/written by a container needs to persist
                so that it can be accessed later.  To persist the data, a Cinder
                volume with a filesystem on it can be mounted on a host and be made
                available to the container, then be unmounted when the container exits.</para>
          <para>Docker provides the ‘volume’ feature for this purpose: the user
                invokes the ‘volume create’ command, specifying a particular volume
                driver to perform the actual work.  Then this volume can be mounted
                when a container is created.  A number of third-party volume drivers
                support OpenStack Cinder as the backend, for example Rexray and
                Flocker.  Magnum currently supports Rexray as the volume driver for
                Swarm and Mesos.  Other drivers are being considered.</para>
          <para>Kubernetes allows a previously created Cinder block to be mounted to
                a pod and this is done by specifying the block ID in the pod YAML file.
                When the pod is scheduled on a node, Kubernetes will interface with
                Cinder to request the volume to be mounted on this node, then
                Kubernetes will launch the Docker container with the proper options to
                make the filesystem on the Cinder volume accessible to the container
                in the pod.  When the pod exits, Kubernetes will again send a request
                to Cinder to unmount the volume’s filesystem, making it available to be
                mounted on other nodes.</para>
          <para>Magnum supports these features to use Cinder as persistent storage
                using the ClusterTemplate attribute ‘volume-driver’ and the support matrix
                for the COE types is summarized as follows:</para>
          <informaltable>
            <tgroup cols="4">
              <colspec colname="c1" colwidth="8"/>
              <colspec colname="c2" colwidth="13"/>
              <colspec colname="c3" colwidth="13"/>
              <colspec colname="c4" colwidth="13"/>
              <thead>
                <row>
                  <entry>
                    <para>Driver</para>
                  </entry>
                  <entry>
                    <para>Kubernetes</para>
                  </entry>
                  <entry>
                    <para>Swarm</para>
                  </entry>
                  <entry>
                    <para>Mesos</para>
                  </entry>
                </row>
              </thead>
              <tbody>
                <row>
                  <entry>
                    <para>cinder</para>
                  </entry>
                  <entry>
                    <para>supported</para>
                  </entry>
                  <entry>
                    <para>unsupported</para>
                  </entry>
                  <entry>
                    <para>unsupported</para>
                  </entry>
                </row>
                <row>
                  <entry>
                    <para>rexray</para>
                  </entry>
                  <entry>
                    <para>unsupported</para>
                  </entry>
                  <entry>
                    <para>supported</para>
                  </entry>
                  <entry>
                    <para>supported</para>
                  </entry>
                </row>
              </tbody>
            </tgroup>
          </informaltable>
          <para>Following are some examples for using Cinder as persistent storage.</para>
          <section>
            <title>Using Cinder in Kubernetes</title>
            <para><emphasis role="bold">NOTE:</emphasis> This feature requires Kubernetes version 1.5.0 or above.
                    The public Fedora image from Atomic currently meets this requirement.</para>
            <procedure>
              <step>
                <para>Create the ClusterTemplate.</para>
                <para>Specify ‘cinder’ as the volume-driver for Kubernetes:</para>
                <screen>magnum cluster-template-create k8s-cluster-template \
                           --image fedora-23-atomic-7 \
                           --keypair testkey \
                           --external-network public \
                           --dns-nameserver 8.8.8.8 \
                           --flavor m1.small \
                           --docker-volume-size 5 \
                           --network-driver flannel \
                           --coe kubernetes \
                           --volume-driver cinder</screen>
              </step>
              <step>
                <para>Create the cluster:</para>
                <screen>magnum cluster-create k8s-cluster \
                      --cluster-template k8s-cluster-template \
                      --node-count 1</screen>
              </step>
            </procedure>
            <para>Kubernetes is now ready to use Cinder for persistent storage.
                    Following is an example illustrating how Cinder is used in a pod.</para>
            <procedure>
              <step>
                <para>Create the cinder volume:</para>
                <screen>cinder create --display-name=test-repo 1

XML:ID=$(cinder create --display-name=test-repo 1 | awk -F'|' '$2~/^[[:space:]]*id/ {print $3}')</screen>
                <para>The command will generate the volume with a ID. The volume ID will be
                            specified in Step 2.</para>
              </step>
              <step>
                <para>Create a pod in this cluster and mount this cinder volume to the pod.
                            Create a file (e.g nginx-cinder.yaml) describing the pod:</para>
                <screen>cat &gt; nginx-cinder.yaml &lt;&lt; END
apiVersion: v1
kind: Pod
metadata:
  name: aws-web
spec:
  containers:
    - name: web
      image: nginx
      ports:
        - name: web
          containerPort: 80
          hostPort: 8081
          protocol: TCP
      volumeMounts:
        - name: html-volume
          mountPath: "/usr/share/nginx/html"
  volumes:
    - name: html-volume
      cinder:
        # Enter the volume ID below
        volumeID: $ID
        fsType: ext4
END</screen>
              </step>
            </procedure>
            <para><emphasis role="bold">NOTE:</emphasis> The Cinder volume ID needs to be configured in the YAML file
                    so the existing Cinder volume can be mounted in a pod by specifying
                    the volume ID in the pod manifest as follows:</para>
            <screen>volumes:
- name: html-volume
  cinder:
    volumeID: $ID
    fsType: ext4</screen>
            <procedure>
              <step>
                <para>Create the pod by the normal Kubernetes interface:</para>
                <screen>kubectl create -f nginx-cinder.yaml</screen>
              </step>
            </procedure>
            <para>You can start a shell in the container to check that the mountPath exists,
                    and on an OpenStack client you can run the command ‘cinder list’ to verify
                    that the cinder volume status is ‘in-use’.</para>
          </section>
          <section>
            <title>Using Cinder in Swarm</title>
            <para>
              <emphasis>To be filled in</emphasis>
            </para>
          </section>
          <section>
            <title>Using Cinder in Mesos</title>
            <procedure>
              <step>
                <para>Create the ClusterTemplate.</para>
                <para>Specify ‘rexray’ as the volume-driver for Mesos.  As an option, you
                            can specify in a label the attributes ‘rexray_preempt’ to enable
                            any host to take control of a volume regardless if other
                            hosts are using the volume. If this is set to false, the driver
                            will ensure data safety by locking the volume:</para>
                <screen>magnum cluster-template-create mesos-cluster-template \
                           --image ubuntu-mesos \
                           --keypair testkey \
                           --external-network public \
                           --dns-nameserver 8.8.8.8 \
                           --master-flavor m1.magnum \
                           --docker-volume-size 4 \
                           --tls-disabled \
                           --flavor m1.magnum \
                           --coe mesos \
                           --volume-driver rexray \
                           --labels rexray-preempt=true</screen>
              </step>
              <step>
                <para>Create the Mesos cluster:</para>
                <screen>magnum cluster-create mesos-cluster \
                      --cluster-template mesos-cluster-template \
                      --node-count 1</screen>
              </step>
              <step>
                <para>Create the cinder volume and configure this cluster:</para>
                <screen>cinder create --display-name=redisdata 1</screen>
                <para>Create the following file</para>
                <screen>cat &gt; mesos.json &lt;&lt; END
{
  "id": "redis",
  "container": {
    "docker": {
    "image": "redis",
    "network": "BRIDGE",
    "portMappings": [
      { "containerPort": 80, "hostPort": 0, "protocol": "tcp"}
    ],
    "parameters": [
       { "key": "volume-driver", "value": "rexray" },
       { "key": "volume", "value": "redisdata:/data" }
    ]
    }
 },
 "cpus": 0.2,
 "mem": 32.0,
 "instances": 1
}
END</screen>
              </step>
            </procedure>
            <para><emphasis role="bold">NOTE:</emphasis> When the Mesos cluster is created using this ClusterTemplate, the
                    Mesos cluster will be configured so that a filesystem on an existing cinder
                    volume can be mounted in a container by configuring the parameters to mount
                    the cinder volume in the JSON file</para>
            <screen>"parameters": [
   { "key": "volume-driver", "value": "rexray" },
   { "key": "volume", "value": "redisdata:/data" }
]</screen>
            <procedure>
              <step>
                <para>Create the container using Marathon REST API</para>
                <screen>MASTER_IP=$(magnum cluster-show mesos-cluster | awk '/ api_address /{print $4}')
curl -X POST -H "Content-Type: application/json" \
http://${MASTER_IP}:8080/v2/apps -d@mesos.json</screen>
              </step>
            </procedure>
            <para>You can log into the container to check that the mountPath exists, and
                    you can run the command ‘cinder list’ to verify that your cinder
                    volume status is ‘in-use’.</para>
          </section>
        </section>
      </section>
      <section xml:base="user/index">
        <title>Image Management</title>
        <para>When a COE is deployed, an image from Glance is used to boot the nodes
            in the cluster and then the software will be configured and started on
            the nodes to bring up the full cluster.  An image is based on a
            particular distro such as Fedora, Ubuntu, etc, and is prebuilt with
            the software specific to the COE such as Kubernetes, Swarm, Mesos.
            The image is tightly coupled with the following in Magnum:</para>
        <procedure>
          <step>
            <para>Heat templates to orchestrate the configuration.</para>
          </step>
          <step>
            <para>Template definition to map ClusterTemplate parameters to Heat
                    template parameters.</para>
          </step>
          <step>
            <para>Set of scripts to configure software.</para>
          </step>
        </procedure>
        <para>Collectively, they constitute the driver for a particular COE and a
            particular distro; therefore, developing a new image needs to be done
            in conjunction with developing these other components.  Image can be
            built by various methods such as diskimagebuilder, or in some case, a
            distro image can be used directly.  A number of drivers and the
            associated images is supported in Magnum as reference implementation.
            In this section, we focus mainly on the supported images.</para>
        <para>All images must include support for cloud-init and the heat software
            configuration utility:</para>
        <itemizedlist>
          <listitem>
            <para>os-collect-config</para>
          </listitem>
          <listitem>
            <para>os-refresh-config</para>
          </listitem>
          <listitem>
            <para>os-apply-config</para>
          </listitem>
          <listitem>
            <para>heat-config</para>
          </listitem>
          <listitem>
            <para>heat-config-script</para>
          </listitem>
        </itemizedlist>
        <para>Additional software are described as follows.</para>
        <section>
          <title>Kubernetes on Fedora Atomic</title>
          <para>This image can be downloaded from the <link xlink:href="https://alt.fedoraproject.org/pub/alt/atomic/stable/Cloud-Images/x86_64/Images/">public Atomic site</link>
                or can be built locally using diskimagebuilder.  Details can be found in the
                <link xlink:href="https://github.com/openstack/magnum/tree/master/magnum/elements/fedora-atomic">fedora-atomic element</link>
                The image currently has the following OS/software:</para>
          <informaltable>
            <tgroup cols="2">
              <colspec colname="c1" colwidth="13"/>
              <colspec colname="c2" colwidth="11"/>
              <thead>
                <row>
                  <entry>
                    <para>OS/software</para>
                  </entry>
                  <entry>
                    <para>version</para>
                  </entry>
                </row>
              </thead>
              <tbody>
                <row>
                  <entry>
                    <para>Fedora</para>
                  </entry>
                  <entry>
                    <para>26</para>
                  </entry>
                </row>
                <row>
                  <entry>
                    <para>Docker</para>
                  </entry>
                  <entry>
                    <para>1.13.1</para>
                  </entry>
                </row>
                <row>
                  <entry>
                    <para>Kubernetes</para>
                  </entry>
                  <entry>
                    <para>1.7.4</para>
                  </entry>
                </row>
                <row>
                  <entry>
                    <para>etcd</para>
                  </entry>
                  <entry>
                    <para>3.1.3</para>
                  </entry>
                </row>
                <row>
                  <entry>
                    <para>Flannel</para>
                  </entry>
                  <entry>
                    <para>0.7.0</para>
                  </entry>
                </row>
              </tbody>
            </tgroup>
          </informaltable>
          <para>The following software are managed as systemd services:</para>
          <itemizedlist>
            <listitem>
              <para>kube-apiserver</para>
            </listitem>
            <listitem>
              <para>kubelet</para>
            </listitem>
            <listitem>
              <para>etcd</para>
            </listitem>
            <listitem>
              <para>flannel (if specified as network driver)</para>
            </listitem>
            <listitem>
              <para>docker</para>
            </listitem>
          </itemizedlist>
          <para>The following software are managed as Docker containers:</para>
          <itemizedlist>
            <listitem>
              <para>kube-controller-manager</para>
            </listitem>
            <listitem>
              <para>kube-scheduler</para>
            </listitem>
            <listitem>
              <para>kube-proxy</para>
            </listitem>
          </itemizedlist>
          <para>The login for this image is <emphasis>fedora</emphasis>.</para>
        </section>
        <section>
          <title>Kubernetes on CoreOS</title>
          <para>CoreOS publishes a <link xlink:href="http://beta.release.core-os.net/amd64-usr/current/coreos_production_openstack_image.img.bz2">stock image</link>
                that is being used to deploy Kubernetes.
                This image has the following OS/software:</para>
          <informaltable>
            <tgroup cols="2">
              <colspec colname="c1" colwidth="13"/>
              <colspec colname="c2" colwidth="11"/>
              <thead>
                <row>
                  <entry>
                    <para>OS/software</para>
                  </entry>
                  <entry>
                    <para>version</para>
                  </entry>
                </row>
              </thead>
              <tbody>
                <row>
                  <entry>
                    <para>CoreOS</para>
                  </entry>
                  <entry>
                    <para>4.3.6</para>
                  </entry>
                </row>
                <row>
                  <entry>
                    <para>Docker</para>
                  </entry>
                  <entry>
                    <para>1.9.1</para>
                  </entry>
                </row>
                <row>
                  <entry>
                    <para>Kubernetes</para>
                  </entry>
                  <entry>
                    <para>1.0.6</para>
                  </entry>
                </row>
                <row>
                  <entry>
                    <para>etcd</para>
                  </entry>
                  <entry>
                    <para>2.2.3</para>
                  </entry>
                </row>
                <row>
                  <entry>
                    <para>Flannel</para>
                  </entry>
                  <entry>
                    <para>0.5.5</para>
                  </entry>
                </row>
              </tbody>
            </tgroup>
          </informaltable>
          <para>The following software are managed as systemd services:</para>
          <itemizedlist>
            <listitem>
              <para>kubelet</para>
            </listitem>
            <listitem>
              <para>flannel (if specified as network driver)</para>
            </listitem>
            <listitem>
              <para>docker</para>
            </listitem>
            <listitem>
              <para>etcd</para>
            </listitem>
          </itemizedlist>
          <para>The following software are managed as Docker containers:</para>
          <itemizedlist>
            <listitem>
              <para>kube-apiserver</para>
            </listitem>
            <listitem>
              <para>kube-controller-manager</para>
            </listitem>
            <listitem>
              <para>kube-scheduler</para>
            </listitem>
            <listitem>
              <para>kube-proxy</para>
            </listitem>
          </itemizedlist>
          <para>The login for this image is <emphasis>core</emphasis>.</para>
        </section>
        <section>
          <title>Kubernetes on Ironic</title>
          <para>This image is built manually using diskimagebuilder.  The scripts and
                instructions are included in <link xlink:href="https://github.com/openstack/magnum/tree/master/magnum/templates/kubernetes/elements">Magnum code repo</link>.
                Currently Ironic is not fully supported yet, therefore more details will be
                provided when this driver has been fully tested.</para>
        </section>
        <section>
          <title>Swarm on Fedora Atomic</title>
          <para>This image is the same as the image for Kubernetes on Fedora
	  Atomic described above.  The login for this image is <emphasis>fedora</emphasis>.</para>
        </section>
        <section>
          <title>Mesos on Ubuntu</title>
          <para>This image is built manually using diskimagebuilder.<!--  The instructions are
                provided in the section <xref linkend="diskimage-builder"/>.-->
                The Fedora site hosts the current image <link xlink:href="https://fedorapeople.org/groups/magnum/ubuntu-mesos-latest.qcow2">ubuntu-mesos-latest.qcow2</link>.</para>
          <informaltable>
            <tgroup cols="2">
              <colspec colname="c1" colwidth="13"/>
              <colspec colname="c2" colwidth="11"/>
              <thead>
                <row>
                  <entry>
                    <para>OS/software</para>
                  </entry>
                  <entry>
                    <para>version</para>
                  </entry>
                </row>
              </thead>
              <tbody>
                <row>
                  <entry>
                    <para>Ubuntu</para>
                  </entry>
                  <entry>
                    <para>14.04</para>
                  </entry>
                </row>
                <row>
                  <entry>
                    <para>Docker</para>
                  </entry>
                  <entry>
                    <para>1.8.1</para>
                  </entry>
                </row>
                <row>
                  <entry>
                    <para>Mesos</para>
                  </entry>
                  <entry>
                    <para>0.25.0</para>
                  </entry>
                </row>
                <row>
                  <entry>
                    <para>Marathon</para>
                  </entry>
                  <entry>
                    <para>0.11.1</para>
                  </entry>
                </row>
              </tbody>
            </tgroup>
          </informaltable>
        </section>
      </section>
      <section xml:base="user/index">
        <title>Notification</title>
        <para>Magnum provides notifications about usage data so that 3rd party applications
            can use the data for auditing, billing, monitoring, or quota purposes. This
            document describes the current inclusions and exclusions for Magnum
            notifications.</para>
        <para>Magnum uses Cloud Auditing Data Federation (<link xlink:href="http://www.dmtf.org/standards/cadf">CADF</link>) Notification as its
            notification format for better support of auditing, details about CADF are
            documented below.</para>
        <section>
          <title>Auditing with CADF</title>
          <para>Magnum uses the <link xlink:href="http://docs.openstack.org/developer/pycadf">PyCADF</link> library to emit CADF notifications, these events
                adhere to the DMTF <link xlink:href="http://www.dmtf.org/standards/cadf">CADF</link> specification. This standard provides auditing
                capabilities for compliance with security, operational, and business processes
                and supports normalized and categorized event data for federation and
                aggregation.</para>
          <para>Below table describes the event model components and semantics for
                each component:</para>
          <informaltable>
            <tgroup cols="2">
              <colspec colname="c1" colwidth="17"/>
              <colspec colname="c2" colwidth="58"/>
              <thead>
                <row>
                  <entry>
                    <para>model component</para>
                  </entry>
                  <entry>
                    <para>CADF Definition</para>
                  </entry>
                </row>
              </thead>
              <tbody>
                <row>
                  <entry>
                    <para>OBSERVER</para>
                  </entry>
                  <entry>
                    <para>The RESOURCE that generates the CADF Event Record based
                                    on its observation (directly or indirectly) of the
                                    Actual Event.</para>
                  </entry>
                </row>
                <row>
                  <entry>
                    <para>INITIATOR</para>
                  </entry>
                  <entry>
                    <para>The RESOURCE that initiated, originated, or instigated
                                    the event’s ACTION, according to the OBSERVER.</para>
                  </entry>
                </row>
                <row>
                  <entry>
                    <para>ACTION</para>
                  </entry>
                  <entry>
                    <para>The operation or activity the INITIATOR has performed,
                                    has attempted to perform or has pending against the
                                    event’s TARGET, according to the OBSERVER.</para>
                  </entry>
                </row>
                <row>
                  <entry>
                    <para>TARGET</para>
                  </entry>
                  <entry>
                    <para>The RESOURCE against which the ACTION of a CADF Event
                                    Record was performed, attempted, or is pending,
                                    according to the OBSERVER.</para>
                  </entry>
                </row>
                <row>
                  <entry>
                    <para>OUTCOME</para>
                  </entry>
                  <entry>
                    <para>The result or status of the ACTION against the TARGET,
                                    according to the OBSERVER.</para>
                  </entry>
                </row>
              </tbody>
            </tgroup>
          </informaltable>
          <para>The <literal>payload</literal> portion of a CADF Notification is a CADF <literal>event</literal>, which
                is represented as a JSON dictionary. For example:</para>
          <screen language="javascript">{
    "typeURI": "http://schemas.dmtf.org/cloud/audit/1.0/event",
    "initiator": {
        "typeURI": "service/security/account/user",
        "host": {
            "agent": "curl/7.22.0(x86_64-pc-linux-gnu)",
            "address": "127.0.0.1"
        },
        "id": "&lt;initiator_id&gt;"
    },
    "target": {
        "typeURI": "&lt;target_uri&gt;",
        "id": "openstack:1c2fc591-facb-4479-a327-520dade1ea15"
    },
    "observer": {
        "typeURI": "service/security",
        "id": "openstack:3d4a50a9-2b59-438b-bf19-c231f9c7625a"
    },
    "eventType": "activity",
    "eventTime": "2014-02-14T01:20:47.932842+00:00",
    "action": "&lt;action&gt;",
    "outcome": "success",
    "id": "openstack:f5352d7b-bee6-4c22-8213-450e7b646e9f",
}</screen>
          <para>Where the following are defined:</para>
          <itemizedlist>
            <listitem>
              <para><literal>&lt;initiator_id&gt;</literal>: ID of the user that performed the operation</para>
            </listitem>
            <listitem>
              <para><literal>&lt;target_uri&gt;</literal>: CADF specific target URI, (i.e.:  data/security/project)</para>
            </listitem>
            <listitem>
              <para><literal>&lt;action&gt;</literal>: The action being performed, typically:
                        <literal>&lt;operation&gt;</literal>. <literal>&lt;resource_type&gt;</literal></para>
            </listitem>
          </itemizedlist>
          <para>Additionally there may be extra keys present depending on the operation being
                performed, these will be discussed below.</para>
          <para>Note, the <literal>eventType</literal> property of the CADF payload is different from the
                <literal>event_type</literal> property of a notifications. The former (<literal>eventType</literal>) is a
                CADF keyword which designates the type of event that is being measured, this
                can be: <literal>activity</literal>, <literal>monitor</literal> or <literal>control</literal>. Whereas the latter
                (<literal>event_type</literal>) is described in previous sections as:
                <literal>magnum.&lt;resource_type&gt;.&lt;operation&gt;</literal></para>
        </section>
        <section>
          <title>Supported Events</title>
          <para>The following table displays the corresponding relationship between resource
                types and operations. The bay type is deprecated and will be removed in a
                future version. Cluster is the new equivalent term.</para>
          <informaltable>
            <tgroup cols="3">
              <colspec colname="c1" colwidth="15"/>
              <colspec colname="c2" colwidth="28"/>
              <colspec colname="c3" colwidth="25"/>
              <thead>
                <row>
                  <entry>
                    <para>resource type</para>
                  </entry>
                  <entry>
                    <para>supported operations</para>
                  </entry>
                  <entry>
                    <para>typeURI</para>
                  </entry>
                </row>
              </thead>
              <tbody>
                <row>
                  <entry>
                    <para>bay</para>
                  </entry>
                  <entry>
                    <para>create, update, delete</para>
                  </entry>
                  <entry>
                    <para>service/magnum/bay</para>
                  </entry>
                </row>
                <row>
                  <entry>
                    <para>cluster</para>
                  </entry>
                  <entry>
                    <para>create, update, delete</para>
                  </entry>
                  <entry>
                    <para>service/magnum/cluster</para>
                  </entry>
                </row>
              </tbody>
            </tgroup>
          </informaltable>
        </section>
        <section>
          <title>Example Notification - Cluster Create</title>
          <para>The following is an example of a notification that is sent when a cluster is
                created. This example can be applied for any <literal>create</literal>, <literal>update</literal> or
                <literal>delete</literal> event that is seen in the table above. The <literal>&lt;action&gt;</literal> and
                <literal>typeURI</literal> fields will be change.</para>
          <screen language="javascript">{
    "event_type": "magnum.cluster.created",
    "message_id": "0156ee79-b35f-4cef-ac37-d4a85f231c69",
    "payload": {
        "typeURI": "http://schemas.dmtf.org/cloud/audit/1.0/event",
        "initiator": {
            "typeURI": "service/security/account/user",
            "id": "c9f76d3c31e142af9291de2935bde98a",
            "user_id": "0156ee79-b35f-4cef-ac37-d4a85f231c69",
            "project_id": "3d4a50a9-2b59-438b-bf19-c231f9c7625a"
        },
        "target": {
            "typeURI": "service/magnum/cluster",
            "id": "openstack:1c2fc591-facb-4479-a327-520dade1ea15"
        },
        "observer": {
            "typeURI": "service/magnum/cluster",
            "id": "openstack:3d4a50a9-2b59-438b-bf19-c231f9c7625a"
        },
        "eventType": "activity",
        "eventTime": "2015-05-20T01:20:47.932842+00:00",
        "action": "create",
        "outcome": "success",
        "id": "openstack:f5352d7b-bee6-4c22-8213-450e7b646e9f",
        "resource_info": "671da331c47d4e29bb6ea1d270154ec3"
    }
    "priority": "INFO",
    "publisher_id": "magnum.host1234",
    "timestamp": "2016-05-20 15:03:45.960280"
}</screen>
        </section>
      </section>
      <section xml:base="user/index">
        <title>Container Monitoring</title>
        <para>The offered monitoring stack relies on the following set of containers and
            services:</para>
        <itemizedlist>
          <listitem>
            <para>cAdvisor</para>
          </listitem>
          <listitem>
            <para>Node Exporter</para>
          </listitem>
          <listitem>
            <para>Prometheus</para>
          </listitem>
          <listitem>
            <para>Grafana</para>
          </listitem>
        </itemizedlist>
        <para>To setup this monitoring stack, users are given two configurable labels in
            the Magnum cluster template’s definition:</para>
        <variablelist>
          <varlistentry>
            <term/>
            <listitem>
              <para>This label accepts a boolean value. If <emphasis>True</emphasis>, the monitoring stack will be
                        setup. By default <emphasis>prometheus_monitoring = False</emphasis>.</para>
            </listitem>
          </varlistentry>
          <varlistentry>
            <term/>
            <listitem>
              <para>This label lets users create their own <emphasis>admin</emphasis> user password for the Grafana
                        interface. It expects a string value. By default it is set to <emphasis>admin</emphasis>.</para>
            </listitem>
          </varlistentry>
        </variablelist>
        <section>
          <title>Container Monitoring in Kubernetes</title>
          <para>By default, all Kubernetes clusters already contain <emphasis>cAdvisor</emphasis> integrated
                with the <emphasis>Kubelet</emphasis> binary. Its container monitoring data can be accessed on
                a node level basis through <emphasis>http://NODE_IP:4194</emphasis>.</para>
          <para>Node Exporter is part of the above mentioned monitoring stack as it can be
                used to export machine metrics. Such functionality also work on a node level
                which means that when <literal>prometheus-monitoring</literal> is <emphasis>True</emphasis>, the Kubernetes nodes
                will be populated with an additional manifest under
                <emphasis>/etc/kubernetes/manifests</emphasis>. Node Exporter is then automatically picked up
                and launched as a regular Kubernetes POD.</para>
          <para>To aggregate and complement all the existing monitoring metrics and add a
                built-in visualization layer, Prometheus is used. It is launched by the
                Kubernetes master node(s) as a <emphasis>Service</emphasis> within a <emphasis>Deployment</emphasis> with one
                replica and it relies on a <emphasis>ConfigMap</emphasis> where the Prometheus configuration
                (prometheus.yml) is defined. This configuration uses Prometheus native
                support for service discovery in Kubernetes clusters,
                <emphasis>kubernetes_sd_configs</emphasis>. The respective manifests can be found in
                <emphasis>/srv/kubernetes/monitoring/</emphasis> on the master nodes and once the service is
                up and running, Prometheus UI can be accessed through port 9090.</para>
          <para>Finally, for custom plotting and enhanced metric aggregation and
                visualization, Prometheus can be integrated with Grafana as it provides
                native compliance for Prometheus data sources. Also Grafana is deployed as
                a <emphasis>Service</emphasis> within a <emphasis>Deployment</emphasis> with one replica. The default user is
                <emphasis>admin</emphasis> and the password is setup according to <literal>grafana-admin-passwd</literal>.
                There is also a default Grafana dashboard provided with this installation,
                from the official <link xlink:href="https://grafana.net/dashboards">Grafana dashboards’ repository</link>. The Prometheus data
                source is automatically added to Grafana once it is up and running, pointing
                to <emphasis>http://prometheus:9090</emphasis> through <emphasis>Proxy</emphasis>. The respective manifests can
                also be found in <emphasis>/srv/kubernetes/monitoring/</emphasis> on the master nodes and once
                the service is running, the Grafana dashboards can be accessed through port
                3000.</para>
          <para>For both Prometheus and Grafana, there is an assigned <emphasis>systemd</emphasis> service
                called <emphasis>kube-enable-monitoring</emphasis>.</para>
        </section>
      </section>
      <section xml:base="user/index">
        <title>Kubernetes External Load Balancer</title>
        <para>In a Kubernetes cluster, all masters and minions are connected to a private
            Neutron subnet, which in turn is connected by a router to the public network.
            This allows the nodes to access each other and the external internet.</para>
        <para>All Kubernetes pods and services created in the cluster are connected to a
            private container network which by default is Flannel, an overlay network that
            runs on top of the Neutron private subnet.  The pods and services are assigned
            IP addresses from this container network and they can access each other and
            the external internet.  However, these IP addresses are not accessible from an
            external network.</para>
        <para>To publish a service endpoint externally so that the service can be accessed
            from the external network, Kubernetes provides the external load balancer
            feature.  This is done by simply specifying the attribute “type: LoadBalancer”
            in the service manifest.  When the service is created, Kubernetes will add an
            external load balancer in front of the service so that the service will have
            an external IP address in addition to the internal IP address on the container
            network.  The service endpoint can then be accessed with this external IP
            address.  Refer to the <link xlink:href="https://kubernetes.io/docs/concepts/services-networking/service/#type-loadbalancer">Kubernetes service document</link> for more details.</para>
        <para>A Kubernetes cluster deployed by Magnum will have all the necessary
            configuration required for the external load balancer.  This document describes
            how to use this feature.</para>
        <section>
          <title>Steps for the cluster administrator</title>
          <para>Because the Kubernetes master needs to interface with OpenStack to create and
                manage the Neutron load balancer, we need to provide a credential for
                Kubernetes to use.</para>
          <para>In the current implementation, the cluster administrator needs to manually
                perform this step.  We are looking into several ways to let Magnum automate
                this step in a secure manner.  This means that after the Kubernetes cluster is
                initially deployed, the load balancer support is disabled.  If the
                administrator does not want to enable this feature, no further action is
                required.  All the services will be created normally; services that specify the
                load balancer will also be created successfully, but a load balancer will not
                be created.</para>
          <para>Note that different versions of Kubernetes require different versions of
                Neutron LBaaS plugin running on the OpenStack instance:</para>
          <screen>============================  ==============================
Kubernetes Version on Master  Neutron LBaaS Version Required
============================  ==============================
1.2                           LBaaS v1
1.3 or later                  LBaaS v2
============================  ==============================</screen>
          <para>Before enabling the Kubernetes load balancer feature, confirm that the
                OpenStack instance is running the required version of Neutron LBaaS plugin.
                To determine if your OpenStack instance is running LBaaS v1, try running
                the following command from your OpenStack control node:</para>
          <screen>neutron lb-pool-list</screen>
          <para>Or look for the following configuration in neutron.conf or
                neutron_lbaas.conf:</para>
          <screen>service_provider = LOADBALANCER:Haproxy:neutron_lbaas.services.loadbalancer.drivers.haproxy.plugin_driver.HaproxyOnHostPluginDriver:default</screen>
          <para>To determine if your OpenStack instance is running LBaaS v2, try running
                the following command from your OpenStack control node:</para>
          <screen>neutron lbaas-pool-list</screen>
          <para>Or look for the following configuration in neutron.conf or
                neutron_lbaas.conf:</para>
          <screen>service_plugins = neutron.plugins.services.agent_loadbalancer.plugin.LoadBalancerPluginv2</screen>
          <para>To configure LBaaS v1 or v2, refer to the Neutron documentation.</para>
          <para>Before deleting the Kubernetes cluster, make sure to
                delete all the services that created load balancers. Because the Neutron
                objects created by Kubernetes are not managed by Heat, they will not be
                deleted by Heat and this will cause the cluster-delete operation to fail. If
                this occurs, delete the neutron objects manually (lb-pool, lb-vip, lb-member,
                lb-healthmonitor) and then run cluster-delete again.</para>
        </section>
        <section>
          <title>Steps for the users</title>
          <para>This feature requires the OpenStack cloud provider to be enabled.
                To do so, enable the cinder support (–volume-driver cinder).</para>
          <para>For the user, publishing the service endpoint externally involves the following
                2 steps:</para>
          <procedure>
            <step>
              <para>Specify “type: LoadBalancer” in the service manifest</para>
            </step>
            <step>
              <para>After the service is created, associate a floating IP with the VIP of the
                        load balancer pool.</para>
            </step>
          </procedure>
          <para>The following example illustrates how to create an external endpoint for
                a pod running nginx.</para>
          <para>Create a file (e.g nginx.yaml) describing a pod running nginx:</para>
          <screen>apiVersion: v1
kind: Pod
metadata:
  name: nginx
  labels:
   app: nginx
spec:
  containers:
  - name: nginx
    image: nginx
    ports:
    - containerPort: 80</screen>
          <para>Create a file (e.g nginx-service.yaml) describing a service for the nginx pod:</para>
          <screen>apiVersion: v1
kind: Service
metadata:
  name: nginxservice
  labels:
    app: nginx
spec:
  ports:
  - port: 80
    targetPort: 80
    protocol: TCP
  selector:
    app: nginx
  type: LoadBalancer</screen>
          <para>Please refer to the <link xlink:href="https://docs.openstack.org/developer/magnum/userguide.html">quickstart</link> guide on how to connect to Kubernetes running on the launched
                cluster. Assuming a Kubernetes cluster named k8sclusterv1 has been created,
                deploy the pod and service using following commands:</para>
          <screen>kubectl create -f nginx.yaml

kubectl create -f nginx-service.yaml</screen>
          <para>For more details on verifying the load balancer in OpenStack, refer to the
                following section on how it works.</para>
          <para>Next, associate a floating IP to the load balancer.  This can be done easily
                on Horizon by navigating to:</para>
          <screen>Compute -&gt; Access &amp; Security -&gt; Floating IPs</screen>
          <para>Click on “Allocate IP To Project” and then on “Associate” for the new floating
                IP.</para>
          <para>Alternatively, associating a floating IP can be done on the command line by
                allocating a floating IP, finding the port of the VIP, and associating the
                floating IP to the port.
                The commands shown below are for illustration purpose and assume
                that there is only one service with load balancer running in the cluster and
                no other load balancers exist except for those created for the cluster.</para>
          <para>First create a floating IP on the public network:</para>
          <screen>neutron floatingip-create public

Created a new floatingip:

+---------------------+--------------------------------------+
| Field               | Value                                |
+---------------------+--------------------------------------+
| fixed_ip_address    |                                      |
| floating_ip_address | 172.24.4.78                          |
| floating_network_id | 4808eacb-e1a0-40aa-97b6-ecb745af2a4d |
| id                  | b170eb7a-41d0-4c00-9207-18ad1c30fecf |
| port_id             |                                      |
| router_id           |                                      |
| status              | DOWN                                 |
| tenant_id           | 012722667dc64de6bf161556f49b8a62     |
+---------------------+--------------------------------------+</screen>
          <para>Note the floating IP 172.24.4.78 that has been allocated.  The ID for this
                floating IP is shown above, but it can also be queried by:</para>
          <screen>FLOATING_XML:ID=$(neutron floatingip-list | grep "172.24.4.78" | awk '{print $2}')</screen>
          <para>Next find the VIP for the load balancer:</para>
          <screen>VIP_XML:ID=$(neutron lb-vip-list | grep TCP | grep -v pool | awk '{print $2}')</screen>
          <para>Find the port for this VIP:</para>
          <screen>PORT_XML:ID=$(neutron lb-vip-show $VIP_ID | grep port_id | awk '{print $4}')</screen>
          <para>Finally associate the floating IP with the port of the VIP:</para>
          <screen>neutron floatingip-associate $FLOATING_ID $PORT_ID</screen>
          <para>The endpoint for nginx can now be accessed on a browser at this floating IP:</para>
          <screen>http://172.24.4.78:80</screen>
          <para>Alternatively, you can check for the nginx ‘welcome’ message by:</para>
          <screen>curl http://172.24.4.78:80</screen>
          <para>NOTE: it is not necessary to indicate port :80 here but it is shown to
                correlate with the port that was specified in the service manifest.</para>
        </section>
        <section>
          <title>How it works</title>
          <para>Kubernetes is designed to work with different Clouds such as Google Compute
                Engine (GCE), Amazon Web Services (AWS), and OpenStack;  therefore, different
                load balancers need to be created on the particular Cloud for the services.
                This is done through a plugin for each Cloud and the OpenStack plugin was
                developed by Angus Lees:</para>
          <screen>https://github.com/kubernetes/kubernetes/blob/release-1.0/pkg/cloudprovider/openstack/openstack.go</screen>
          <para>When the Kubernetes components kube-apiserver and kube-controller-manager start
                up, they will use the credential provided to authenticate a client
                to interface with OpenStack.</para>
          <para>When a service with load balancer is created, the plugin code will interface
                with Neutron in this sequence:</para>
          <procedure>
            <step>
              <para>Create lb-pool for the Kubernetes service</para>
            </step>
            <step>
              <para>Create lb-member for the minions</para>
            </step>
            <step>
              <para>Create lb-healthmonitor</para>
            </step>
            <step>
              <para>Create lb-vip on the private network of the Kubernetes cluster</para>
            </step>
          </procedure>
          <para>These Neutron objects can be verified as follows.  For the load balancer pool:</para>
          <screen>neutron lb-pool-list
+--------------------------------------+--------------------------------------------------+----------+-------------+----------+----------------+--------+
| id                                   | name                                             | provider | lb_method   | protocol | admin_state_up | status |
+--------------------------------------+--------------------------------------------------+----------+-------------+----------+----------------+--------+
| 241357b3-2a8f-442e-b534-bde7cd6ba7e4 | a1f03e40f634011e59c9efa163eae8ab                 | haproxy  | ROUND_ROBIN | TCP      | True           | ACTIVE |
| 82b39251-1455-4eb6-a81e-802b54c2df29 | k8sclusterv1-iypacicrskib-api_pool-fydshw7uvr7h  | haproxy  | ROUND_ROBIN | HTTP     | True           | ACTIVE |
| e59ea983-c6e8-4cec-975d-89ade6b59e50 | k8sclusterv1-iypacicrskib-etcd_pool-qbpo43ew2m3x | haproxy  | ROUND_ROBIN | HTTP     | True           | ACTIVE |
+--------------------------------------+--------------------------------------------------+----------+-------------+----------+----------------+--------+</screen>
          <para>Note that 2 load balancers already exist to implement high availability for the
                cluster (api and ectd). The new load balancer for the Kubernetes service uses
                the TCP protocol and has a name assigned by Kubernetes.</para>
          <para>For the members of the pool:</para>
          <screen>neutron lb-member-list
+--------------------------------------+----------+---------------+--------+----------------+--------+
| id                                   | address  | protocol_port | weight | admin_state_up | status |
+--------------------------------------+----------+---------------+--------+----------------+--------+
| 9ab7dcd7-6e10-4d9f-ba66-861f4d4d627c | 10.0.0.5 |          8080 |      1 | True           | ACTIVE |
| b179c1ad-456d-44b2-bf83-9cdc127c2b27 | 10.0.0.5 |          2379 |      1 | True           | ACTIVE |
| f222b60e-e4a9-4767-bc44-ffa66ec22afe | 10.0.0.6 |         31157 |      1 | True           | ACTIVE |
+--------------------------------------+----------+---------------+--------+----------------+--------+</screen>
          <para>Again, 2 members already exist for high availability and they serve the master
                node at 10.0.0.5. The new member serves the minion at 10.0.0.6, which hosts the
                Kubernetes service.</para>
          <para>For the monitor of the pool:</para>
          <screen>neutron lb-healthmonitor-list
+--------------------------------------+------+----------------+
| id                                   | type | admin_state_up |
+--------------------------------------+------+----------------+
| 381d3d35-7912-40da-9dc9-b2322d5dda47 | TCP  | True           |
| 67f2ae8f-ffc6-4f86-ba5f-1a135f4af85c | TCP  | True           |
| d55ff0f3-9149-44e7-9b52-2e055c27d1d3 | TCP  | True           |
+--------------------------------------+------+----------------+</screen>
          <para>For the VIP of the pool:</para>
          <screen>neutron lb-vip-list
+--------------------------------------+----------------------------------+----------+----------+----------------+--------+
| id                                   | name                             | address  | protocol | admin_state_up | status |
+--------------------------------------+----------------------------------+----------+----------+----------------+--------+
| 9ae2ebfb-b409-4167-9583-4a3588d2ff42 | api_pool.vip                     | 10.0.0.3 | HTTP     | True           | ACTIVE |
| c318aec6-8b7b-485c-a419-1285a7561152 | a1f03e40f634011e59c9efa163eae8ab | 10.0.0.7 | TCP      | True           | ACTIVE |
| fc62cf40-46ad-47bd-aa1e-48339b95b011 | etcd_pool.vip                    | 10.0.0.4 | HTTP     | True           | ACTIVE |
+--------------------------------------+----------------------------------+----------+----------+----------------+--------+</screen>
          <para>Note that the VIP is created on the private network of the cluster;  therefore
                it has an internal IP address of 10.0.0.7.  This address is also associated as
                the “external address” of the Kubernetes service.  You can verify this in
                Kubernetes by running following command:</para>
          <screen>kubectl get services
NAME           LABELS                                    SELECTOR    IP(S)            PORT(S)
kubernetes     component=apiserver,provider=kubernetes   &lt;none&gt;      10.254.0.1       443/TCP
nginxservice   app=nginx                                 app=nginx   10.254.122.191   80/TCP
                                                                     10.0.0.7</screen>
          <para>On GCE, the networking implementation gives the load balancer an external
                address automatically. On OpenStack, we need to take the additional step of
                associating a floating IP to the load balancer.</para>
        </section>
      </section>
      <section xml:base="user/index#terminology">
        <title>Terminology</title>
        <variablelist>
          <varlistentry>
            <term>Cluster (previously Bay)</term>
            <listitem>
              <para>A cluster is the construct in which Magnum launches container orchestration
                        engines. After a cluster has been created the user is able to add containers
                        to it either directly, or in the case of the Kubernetes container
                        orchestration engine within pods - a logical construct specific to that
                        implementation. A cluster is created based on a ClusterTemplate.</para>
            </listitem>
          </varlistentry>
          <varlistentry>
            <term>ClusterTemplate (previously BayModel)</term>
            <listitem>
              <para>A ClusterTemplate in Magnum is roughly equivalent to a flavor in Nova. It
                        acts as a template that defines options such as the container orchestration
                        engine, keypair and image for use when Magnum is creating clusters using
                        the given ClusterTemplate.</para>
            </listitem>
          </varlistentry>
          <varlistentry>
            <term>Container Orchestration Engine (COE)</term>
            <listitem>
              <para>A container orchestration engine manages the lifecycle of one or more
                        containers, logically represented in Magnum as a cluster. Magnum supports a
                        number of container orchestration engines, each with their own pros and cons,
                        including Docker Swarm, Kubernetes, and Mesos.</para>
            </listitem>
          </varlistentry>
        </variablelist>
      </section>
      <section xml:base="user/index#overview">
        <title>Overview</title>
        <para>Magnum is an OpenStack API service developed by the OpenStack Containers Team
            making container orchestration engines (COE) such as Docker Swarm, Kubernetes
            and Apache Mesos available as the first class resources in OpenStack.</para>
        <para>Magnum uses Heat to orchestrate an OS image which contains Docker and COE
            and runs that image in either virtual machines or bare metal in a cluster
            configuration.</para>
        <para>Magnum offers complete life-cycle management of COEs in an
            OpenStack environment, integrated with other OpenStack services for a seamless
            experience for OpenStack users who wish to run containers in an OpenStack
            environment.</para>
        <para>Following are few salient features of Magnum:</para>
        <itemizedlist>
          <listitem>
            <para>Standard API based complete life-cycle management for Container Clusters</para>
          </listitem>
          <listitem>
            <para>Multi-tenancy for container clusters</para>
          </listitem>
          <listitem>
            <para>Choice of COE: Kubernetes, Swarm, Mesos, DC/OS</para>
          </listitem>
          <listitem>
            <para>Choice of container cluster deployment model: VM or Bare-metal</para>
          </listitem>
          <listitem>
            <para>Keystone-based multi-tenant security and auth management</para>
          </listitem>
          <listitem>
            <para>Neutron based multi-tenant network control and isolation</para>
          </listitem>
          <listitem>
            <para>Cinder based volume service for containers</para>
          </listitem>
          <listitem>
            <para>Integrated with OpenStack: SSO experience for cloud users</para>
          </listitem>
          <listitem>
            <para>Secure container cluster access (TLS enabled)</para>
          </listitem>
        </itemizedlist>
        <para>More details: <link xlink:href="https://wiki.openstack.org/wiki/Magnum">Magnum Project Wiki</link></para>
      </section>
      <section xml:base="user/index#clustertemplate">
        <title>ClusterTemplate</title>
        <para>A ClusterTemplate (previously known as BayModel) is a collection of parameters
            to describe how a cluster can be constructed.  Some parameters are relevant to
            the infrastructure of the cluster, while others are for the particular COE.  In
            a typical workflow, a user would create a ClusterTemplate, then create one or
            more clusters using the ClusterTemplate.  A cloud provider can also define a
            number of ClusterTemplates and provide them to the users.  A ClusterTemplate
            cannot be updated or deleted if a cluster using this ClusterTemplate still
            exists.</para>
        <para>The definition and usage of the parameters of a ClusterTemplate are as follows.
            They are loosely grouped as: mandatory, infrastructure, COE specific.</para>
        <variablelist>
          <varlistentry>
            <term>&lt;name&gt;</term>
            <listitem>
              <para>Name of the ClusterTemplate to create.  The name does not have to be
                        unique.  If multiple ClusterTemplates have the same name, you will need to
                        use the UUID to select the ClusterTemplate when creating a cluster or
                        updating, deleting a ClusterTemplate.  If a name is not specified, a random
                        name will be generated using a string and a number, for example
                        “pi-13-model”.</para>
            </listitem>
          </varlistentry>
          <varlistentry>
            <term>–coe &lt;coe&gt;</term>
            <listitem>
              <para>Specify the Container Orchestration Engine to use.  Supported
                        COE’s include ‘kubernetes’, ‘swarm’, ‘mesos’.  If your environment
                        has additional cluster drivers installed, refer to the cluster driver
                        documentation for the new COE names.  This is a mandatory parameter
                        and there is no default value.</para>
            </listitem>
          </varlistentry>
          <varlistentry>
            <term>–image &lt;image&gt;</term>
            <listitem>
              <para>The name or UUID of the base image in Glance to boot the servers for
                        the cluster.  The image must have the attribute ‘os_distro’ defined
                        as appropriate for the cluster driver.  For the currently supported
                        images, the os_distro names are:</para>
              <informaltable>
                <tgroup cols="2">
                  <colspec colname="c1" colwidth="10"/>
                  <colspec colname="c2" colwidth="21"/>
                  <thead>
                    <row>
                      <entry>
                        <para>COE</para>
                      </entry>
                      <entry>
                        <para>os-distro</para>
                      </entry>
                    </row>
                  </thead>
                  <tbody>
                    <row>
                      <entry>
                        <para>Kubernetes</para>
                      </entry>
                      <entry>
                        <para>Fedora-atomic, CoreOS</para>
                      </entry>
                    </row>
                    <row>
                      <entry>
                        <para>Swarm</para>
                      </entry>
                      <entry>
                        <para>Fedora-atomic</para>
                      </entry>
                    </row>
                    <row>
                      <entry>
                        <para>Mesos</para>
                      </entry>
                      <entry>
                        <para>Ubuntu</para>
                      </entry>
                    </row>
                  </tbody>
                </tgroup>
              </informaltable>
              <para>This is a mandatory parameter and there is no default value.</para>
            </listitem>
          </varlistentry>
          <varlistentry>
            <term>–keypair &lt;keypair&gt;</term>
            <listitem>
              <para>The name of the SSH keypair to configure in the cluster servers
                        for ssh access.  You will need the key to be able to ssh to the
                        servers in the cluster.  The login name is specific to the cluster
                        driver. If keypair is not provided in template it will be required at
                        Cluster create. This value will be overridden by any keypair value that
                        is provided during Cluster create.</para>
            </listitem>
          </varlistentry>
          <varlistentry>
            <term>–external-network &lt;external-network&gt;</term>
            <listitem>
              <para>The name or network ID of a Neutron network to provide connectivity
                        to the external internet for the cluster.  This network must be an
                        external network, i.e. its attribute ‘router:external’ must be
                        ‘True’.  The servers in the cluster will be connected to a private
                        network and Magnum will create a router between this private network
                        and the external network.  This will allow the servers to download
                        images, access discovery service, etc, and the containers to install
                        packages, etc.  In the opposite direction, floating IP’s will be
                        allocated from the external network to provide access from the
                        external internet to servers and the container services hosted in
                        the cluster.  This is a mandatory parameter and there is no default
                        value.</para>
            </listitem>
          </varlistentry>
        </variablelist>
        <variablelist>
          <varlistentry>
            <term>
              <option>--public</option>
            </term>
            <listitem>
              <para>Access to a ClusterTemplate is normally limited to the admin, owner or users
                        within the same tenant as the owners.  Setting this flag
                        makes the ClusterTemplate public and accessible by other users.  The default
                        is not public.</para>
            </listitem>
          </varlistentry>
        </variablelist>
        <variablelist>
          <varlistentry>
            <term>–server-type &lt;server-type&gt;</term>
            <listitem>
              <para>The servers in the cluster can be VM or baremetal.  This parameter selects
                        the type of server to create for the cluster.  The default is ‘vm’. Possible
                        values are ‘vm’, ‘bm’.</para>
            </listitem>
          </varlistentry>
          <varlistentry>
            <term>–network-driver &lt;network-driver&gt;</term>
            <listitem>
              <para>The name of a network driver for providing the networks for the
                        containers.  Note that this is different and separate from the Neutron
                        network for the cluster.  The operation and networking model are specific
                        to the particular driver<!--; refer to the <xref linkend="networking"/> section for more
                        details-->.  Supported network drivers and the default driver are:</para>
              <informaltable>
                <tgroup cols="3">
                  <colspec colname="c1" colwidth="11"/>
                  <colspec colname="c2" colwidth="17"/>
                  <colspec colname="c3" colwidth="8"/>
                  <thead>
                    <row>
                      <entry>
                        <para>COE</para>
                      </entry>
                      <entry>
                        <para>Network-Driver</para>
                      </entry>
                      <entry>
                        <para>Default</para>
                      </entry>
                    </row>
                  </thead>
                  <tbody>
                    <row>
                      <entry>
                        <para>Kubernetes</para>
                      </entry>
                      <entry>
                        <para>Flannel</para>
                      </entry>
                      <entry>
                        <para>Flannel</para>
                      </entry>
                    </row>
                    <row>
                      <entry>
                        <para>Swarm</para>
                      </entry>
                      <entry>
                        <para>Docker, Flannel</para>
                      </entry>
                      <entry>
                        <para>Flannel</para>
                      </entry>
                    </row>
                    <row>
                      <entry>
                        <para>Mesos</para>
                      </entry>
                      <entry>
                        <para>Docker</para>
                      </entry>
                      <entry>
                        <para>Docker</para>
                      </entry>
                    </row>
                  </tbody>
                </tgroup>
              </informaltable>
            </listitem>
          </varlistentry>
          <varlistentry>
            <term>–volume-driver &lt;volume-driver&gt;</term>
            <listitem>
              <para>The name of a volume driver for managing the persistent storage for
                        the containers.  The functionality supported are specific to the
                        driver.  Supported volume drivers and the default driver are:</para>
              <informaltable>
                <tgroup cols="3">
                  <colspec colname="c1" colwidth="13"/>
                  <colspec colname="c2" colwidth="13"/>
                  <colspec colname="c3" colwidth="11"/>
                  <thead>
                    <row>
                      <entry>
                        <para>COE</para>
                      </entry>
                      <entry>
                        <para>Volume-Driver</para>
                      </entry>
                      <entry>
                        <para>Default</para>
                      </entry>
                    </row>
                  </thead>
                  <tbody>
                    <row>
                      <entry>
                        <para>Kubernetes</para>
                      </entry>
                      <entry>
                        <para>Cinder</para>
                      </entry>
                      <entry>
                        <para>No Driver</para>
                      </entry>
                    </row>
                    <row>
                      <entry>
                        <para>Swarm</para>
                      </entry>
                      <entry>
                        <para>Rexray</para>
                      </entry>
                      <entry>
                        <para>No Driver</para>
                      </entry>
                    </row>
                    <row>
                      <entry>
                        <para>Mesos</para>
                      </entry>
                      <entry>
                        <para>Rexray</para>
                      </entry>
                      <entry>
                        <para>No Driver</para>
                      </entry>
                    </row>
                  </tbody>
                </tgroup>
              </informaltable>
            </listitem>
          </varlistentry>
          <varlistentry>
            <term>–dns-nameserver &lt;dns-nameserver&gt;</term>
            <listitem>
              <para>The DNS nameserver for the servers and containers in the cluster to use.
                        This is configured in the private Neutron network for the cluster.  The
                        default is ‘8.8.8.8’.</para>
            </listitem>
          </varlistentry>
          <varlistentry>
            <term>–flavor &lt;flavor&gt;</term>
            <listitem>
              <para>The nova flavor id for booting the node servers.  The default
                        is ‘m1.small’.</para>
            </listitem>
          </varlistentry>
          <varlistentry>
            <term>–master-flavor &lt;master-flavor&gt;</term>
            <listitem>
              <para>The nova flavor id for booting the master or manager servers.  The
                        default is ‘m1.small’.</para>
            </listitem>
          </varlistentry>
          <varlistentry>
            <term>–http-proxy &lt;http-proxy&gt;</term>
            <listitem>
              <para>The IP address for a proxy to use when direct http access from the
                        servers to sites on the external internet is blocked.  This may
                        happen in certain countries or enterprises, and the proxy allows the
                        servers and containers to access these sites.  The format is a URL
                        including a port number.  The default is ‘None’.</para>
            </listitem>
          </varlistentry>
          <varlistentry>
            <term>–https-proxy &lt;https-proxy&gt;</term>
            <listitem>
              <para>The IP address for a proxy to use when direct https access from the
                        servers to sites on the external internet is blocked.  This may
                        happen in certain countries or enterprises, and the proxy allows the
                        servers and containers to access these sites.  The format is a URL
                        including a port number.  The default is ‘None’.</para>
            </listitem>
          </varlistentry>
          <varlistentry>
            <term>–no-proxy &lt;no-proxy&gt;</term>
            <listitem>
              <para>When a proxy server is used, some sites should not go through the
                        proxy and should be accessed normally.  In this case, you can
                        specify these sites as a comma separated list of IP’s.  The default
                        is ‘None’.</para>
            </listitem>
          </varlistentry>
          <varlistentry>
            <term>–docker-volume-size &lt;docker-volume-size&gt;</term>
            <listitem>
              <para>If specified, container images will be stored in a cinder volume of the
                        specified size in GB. Each cluster node will have a volume attached of
                        the above size. If not specified, images will be stored in the compute
                        instance’s local disk. For the ‘devicemapper’ storage driver, the minimum
                        value is 3GB. For the ‘overlay’ storage driver, the minimum value is 1GB.
                        This value can be overridden at cluster creation.</para>
            </listitem>
          </varlistentry>
          <varlistentry>
            <term>–docker-storage-driver &lt;docker-storage-driver&gt;</term>
            <listitem>
              <para>The name of a driver to manage the storage for the images and the
                        container’s writable layer.  The supported drivers are ‘devicemapper’
                        and ‘overlay’.  The default is ‘devicemapper’.</para>
            </listitem>
          </varlistentry>
          <varlistentry>
            <term>–labels &lt;KEY1=VALUE1,KEY2=VALUE2;KEY3=VALUE3…&gt;</term>
            <listitem>
              <para>Arbitrary labels in the form of key=value pairs.  The accepted keys
                        and valid values are defined in the cluster drivers.  They are used as a
                        way to pass additional parameters that are specific to a cluster driver.
                        Refer to the subsection on labels for a list of the supported
                        key/value pairs and their usage.</para>
            </listitem>
          </varlistentry>
        </variablelist>
        <variablelist>
          <varlistentry>
            <term>
              <option>--tls-disabled</option>
            </term>
            <listitem>
              <para>Transport Layer Security (TLS) is normally enabled to secure the
                        cluster.  In some cases, users may want to disable TLS in the cluster,
                        for instance during development or to troubleshoot certain problems.
                        Specifying this parameter will disable TLS so that users can access
                        the COE endpoints without a certificate.  The default is TLS
                        enabled.</para>
            </listitem>
          </varlistentry>
          <varlistentry>
            <term>
              <option>--registry-enabled</option>
            </term>
            <listitem>
              <para>Docker images by default are pulled from the public Docker registry,
                        but in some cases, users may want to use a private registry.  This
                        option provides an alternative registry based on the Registry V2:
                        Magnum will create a local registry in the cluster backed by swift to
                        host the images.  Refer to
                        <link xlink:href="https://github.com/docker/distribution">Docker Registry 2.0</link>
                        for more details.  The default is to use the public registry.</para>
            </listitem>
          </varlistentry>
          <varlistentry>
            <term>
              <option>--master-lb-enabled</option>
            </term>
            <listitem>
              <para>Since multiple masters may exist in a cluster, a load balancer is
                        created to provide the API endpoint for the cluster and to direct
                        requests to the masters.  In some cases, such as when the LBaaS
                        service is not available, this option can be set to ‘false’ to
                        create a cluster without the load balancer.  In this case, one of the
                        masters will serve as the API endpoint.  The default is ‘true’,
                        i.e. to create the load balancer for the cluster.</para>
            </listitem>
          </varlistentry>
        </variablelist>
        <section>
          <title>Labels</title>
          <para>Labels is a general method to specify supplemental parameters that are
                specific to certain COE or associated with certain options.  Their
                format is key/value pair and their meaning is interpreted by the
                drivers that uses them.  The drivers do validate the key/value pairs.
                Their usage is explained in details in the appropriate sections,
                however, since there are many possible labels, the following table
                provides a summary to help give a clearer picture.  The label keys in
                the table are linked to more details elsewhere in the user guide.</para>
          <informaltable>
            <tgroup cols="3">
              <colspec colname="c1" colwidth="39"/>
              <colspec colname="c2" colwidth="20"/>
              <colspec colname="c3" colwidth="15"/>
              <thead>
                <row>
                  <entry>
                    <para>label key</para>
                  </entry>
                  <entry>
                    <para>label value</para>
                  </entry>
                  <entry>
                    <para>default</para>
                  </entry>
                </row>
              </thead>
              <tbody>
                <row>
                  <entry>
                    <para>
                      <literal>flannel-network-cidr</literal>
                    </para>
                  </entry>
                  <entry>
                    <para>IPv4 CIDR</para>
                  </entry>
                  <entry>
                    <para>10.100.0.0/16</para>
                  </entry>
                </row>
                <row>
                  <entry>
                    <para>
                      <literal>flannel-backend</literal>
                    </para>
                  </entry>
                  <entry>
                    <itemizedlist>
                      <listitem>
                        <para>udp</para>
                      </listitem>
                      <listitem>
                        <para>vxlan</para>
                      </listitem>
                      <listitem>
                        <para>host-gw</para>
                      </listitem>
                    </itemizedlist>
                  </entry>
                  <entry>
                    <para>udp</para>
                  </entry>
                </row>
                <row>
                  <entry>
                    <para>
                      <literal>flannel-network-subnetlen</literal>
                    </para>
                  </entry>
                  <entry>
                    <para>size of subnet to
                                    assign to node</para>
                  </entry>
                  <entry>
                    <para>24</para>
                  </entry>
                </row>
                <row>
                  <entry>
                    <para>
                      <literal>rexray-preempt</literal>
                    </para>
                  </entry>
                  <entry>
                    <itemizedlist>
                      <listitem>
                        <para>true</para>
                      </listitem>
                      <listitem>
                        <para>false</para>
                      </listitem>
                    </itemizedlist>
                  </entry>
                  <entry>
                    <para>false</para>
                  </entry>
                </row>
                <row>
                  <entry>
                    <para>
                      <literal>mesos-slave-isolation</literal>
                    </para>
                  </entry>
                  <entry>
                    <itemizedlist>
                      <listitem>
                        <para>filesystem/posix</para>
                      </listitem>
                      <listitem>
                        <para>filesystem/linux</para>
                      </listitem>
                      <listitem>
                        <para>filesystem/shared</para>
                      </listitem>
                      <listitem>
                        <para>posix/cpu</para>
                      </listitem>
                      <listitem>
                        <para>posix/mem</para>
                      </listitem>
                      <listitem>
                        <para>posix/disk</para>
                      </listitem>
                      <listitem>
                        <para>cgroups/cpu</para>
                      </listitem>
                      <listitem>
                        <para>cgroups/mem</para>
                      </listitem>
                      <listitem>
                        <para>docker/runtime</para>
                      </listitem>
                      <listitem>
                        <para>namespaces/pid</para>
                      </listitem>
                    </itemizedlist>
                  </entry>
                  <entry>
                    <para>“”</para>
                  </entry>
                </row>
                <row>
                  <entry>
                    <para>
                      <literal>mesos-slave-image-providers</literal>
                    </para>
                  </entry>
                  <entry>
                    <itemizedlist>
                      <listitem>
                        <para>appc</para>
                      </listitem>
                      <listitem>
                        <para>docker</para>
                      </listitem>
                      <listitem>
                        <para>appc,docker</para>
                      </listitem>
                    </itemizedlist>
                  </entry>
                  <entry>
                    <para>“”</para>
                  </entry>
                </row>
                <row>
                  <entry>
                    <para>
                      <literal>mesos-slave-work-dir</literal>
                    </para>
                  </entry>
                  <entry>
                    <para>(directory name)</para>
                  </entry>
                  <entry>
                    <para>“”</para>
                  </entry>
                </row>
                <row>
                  <entry>
                    <para>
                      <literal>mesos-slave-executor-env-variables</literal>
                    </para>
                  </entry>
                  <entry>
                    <para>(file name)</para>
                  </entry>
                  <entry>
                    <para>“”</para>
                  </entry>
                </row>
                <row>
                  <entry>
                    <para>
                      <literal>swarm-strategy</literal>
                    </para>
                  </entry>
                  <entry>
                    <itemizedlist>
                      <listitem>
                        <para>spread</para>
                      </listitem>
                      <listitem>
                        <para>binpack</para>
                      </listitem>
                      <listitem>
                        <para>random</para>
                      </listitem>
                    </itemizedlist>
                  </entry>
                  <entry>
                    <para>spread</para>
                  </entry>
                </row>
                <row>
                  <entry>
                    <para>
                      <literal>admission-control-list</literal>
                    </para>
                  </entry>
                  <entry>
                    <para>see below</para>
                  </entry>
                  <entry>
                    <para>see below</para>
                  </entry>
                </row>
                <row>
                  <entry>
                    <para>
                      <literal>prometheus-monitoring</literal>
                    </para>
                  </entry>
                  <entry>
                    <itemizedlist>
                      <listitem>
                        <para>true</para>
                      </listitem>
                      <listitem>
                        <para>false</para>
                      </listitem>
                    </itemizedlist>
                  </entry>
                  <entry>
                    <para>false</para>
                  </entry>
                </row>
                <row>
                  <entry>
                    <para>
                      <literal>grafana-admin-passwd</literal>
                    </para>
                  </entry>
                  <entry>
                    <para>(any string)</para>
                  </entry>
                  <entry>
                    <para>“admin”</para>
                  </entry>
                </row>
                <row>
                  <entry>
                    <para>
                      <literal>kube-tag</literal>
                    </para>
                  </entry>
                  <entry>
                    <para>see below</para>
                  </entry>
                  <entry>
                    <para>see below</para>
                  </entry>
                </row>
                <row>
                  <entry>
                    <para>
                      <literal>kube-dashboard-enabled</literal>
                    </para>
                  </entry>
                  <entry>
                    <itemizedlist>
                      <listitem>
                        <para>true</para>
                      </listitem>
                      <listitem>
                        <para>false</para>
                      </listitem>
                    </itemizedlist>
                  </entry>
                  <entry>
                    <para>true</para>
                  </entry>
                </row>
                <row>
                  <entry>
                    <para>
                      <literal>docker-volume-type</literal>
                    </para>
                  </entry>
                  <entry>
                    <para>see below</para>
                  </entry>
                  <entry>
                    <para>see below</para>
                  </entry>
                </row>
                <row>
                  <entry>
                    <para>
                      <literal>etcd-volume-size</literal>
                    </para>
                  </entry>
                  <entry>
                    <para>etcd storage
                                    volume size</para>
                  </entry>
                  <entry>
                    <para>0</para>
                  </entry>
                </row>
              </tbody>
            </tgroup>
          </informaltable>
        </section>
      </section>
      <section xml:base="user/index#cluster">
        <title>Cluster</title>
        <para>A cluster (previously known as bay) is an instance of the ClusterTemplate
            of a COE.  Magnum deploys a cluster by referring to the attributes
            defined in the particular ClusterTemplate as well as a few additional
            parameters for the cluster.  Magnum deploys the orchestration templates
            provided by the cluster driver to create and configure all the necessary
            infrastructure.  When ready, the cluster is a fully operational COE that
            can host containers.</para>
        <section>
          <title>Infrastructure</title>
          <para>The infrastructure of the cluster consists of the resources provided by
                the various OpenStack services.  Existing infrastructure, including
                infrastructure external to OpenStack, can also be used by the cluster,
                such as DNS, public network, public discovery service, Docker registry.
                The actual resources created depends on the COE type and the options
                specified; therefore you need to refer to the cluster driver documentation
                of the COE for specific details.  For instance, the option
                ‘–master-lb-enabled’ in the ClusterTemplate will cause a load balancer pool
                along with the health monitor and floating IP to be created.  It is
                important to distinguish resources in the IaaS level from resources in
                the PaaS level.  For instance, the infrastructure networking in
                OpenStack IaaS is different and separate from the container networking
                in Kubernetes or Swarm PaaS.</para>
          <para>Typical infrastructure includes the following.</para>
          <variablelist>
            <varlistentry>
              <term>Servers</term>
              <listitem>
                <para>The servers host the containers in the cluster and these servers can be
                            VM or bare metal.  VM’s are provided by Nova.  Since multiple VM’s
                            are hosted on a physical server, the VM’s provide the isolation
                            needed for containers between different tenants running on the same
                            physical server.  Bare metal servers are provided by Ironic and are
                            used when peak performance with virtually no overhead is needed for
                            the containers.</para>
              </listitem>
            </varlistentry>
            <varlistentry>
              <term>Identity</term>
              <listitem>
                <para>Keystone provides the authentication and authorization for managing
                            the cluster infrastructure.</para>
              </listitem>
            </varlistentry>
            <varlistentry>
              <term>Network</term>
              <listitem>
                <para>Networking among the servers is provided by Neutron.  Since COE
                            currently are not multi-tenant, isolation for multi-tenancy on the
                            networking level is done by using a private network for each cluster.
                            As a result, containers belonging to one tenant will not be
                            accessible to containers or servers of another tenant.  Other
                            networking resources may also be used, such as load balancer and
                            routers.  Networking among containers can be provided by Kuryr if
                            needed.</para>
              </listitem>
            </varlistentry>
            <varlistentry>
              <term>Storage</term>
              <listitem>
                <para>Cinder provides the block storage that can be used to host the
                            containers and as persistent storage for the containers.</para>
              </listitem>
            </varlistentry>
            <varlistentry>
              <term>Security</term>
              <listitem>
                <para>Barbican provides the storage of secrets such as certificates used
                            for Transport Layer Security (TLS) within the cluster.</para>
              </listitem>
            </varlistentry>
          </variablelist>
        </section>
        <section>
          <title>Life cycle</title>
          <para>The set of life cycle operations on the cluster is one of the key value
                that Magnum provides, enabling clusters to be managed painlessly on
                OpenStack.  The current operations are the basic CRUD operations, but
                more advanced operations are under discussion in the community and
                will be implemented as needed.</para>
          <para><emphasis role="bold">NOTE</emphasis> The OpenStack resources created for a cluster are fully
                accessible to the cluster owner.  Care should be taken when modifying or
                reusing these resources to avoid impacting Magnum operations in
                unexpected manners.  For instance, if you launch your own Nova
                instance on the cluster private network, Magnum would not be aware of this
                instance.  Therefore, the cluster-delete operation will fail because
                Magnum would not delete the extra Nova instance and the private Neutron
                network cannot be removed while a Nova instance is still attached.</para>
          <para><emphasis role="bold">NOTE</emphasis> Currently Heat nested templates are used to create the
                resources; therefore if an error occurs, you can troubleshoot through
                Heat.  For more help on Heat stack troubleshooting, refer to the
                <link xlink:href="https://github.com/openstack/magnum/blob/master/doc/source/troubleshooting-guide.rst#heat-stacks">Troubleshooting Guide</link>.</para>
          <section>
            <title>Create</title>
            <para><emphasis role="bold">NOTE</emphasis> bay-&lt;command&gt; are the deprecated versions of these commands and are
                    still support in current release. They will be removed in a future version.
                    Any references to the term bay will be replaced in the parameters when using
                    the ‘bay’ versions of the commands. For example, in ‘bay-create’ –baymodel
                    is used as the baymodel parameter for this command instead of
                    –cluster-template.</para>
            <para>The ‘cluster-create’ command deploys a cluster, for example:</para>
            <screen>magnum cluster-create mycluster \
                  --cluster-template mytemplate \
                  --node-count 8 \
                  --master-count 3</screen>
            <para>The ‘cluster-create’ operation is asynchronous; therefore you can initiate
                    another ‘cluster-create’ operation while the current cluster is being created.
                    If the cluster fails to be created, the infrastructure created so far may
                    be retained or deleted depending on the particular orchestration
                    engine.  As a common practice, a failed cluster is retained during
                    development for troubleshooting, but they are automatically deleted in
                    production.  The current cluster drivers use Heat templates and the
                    resources of a failed ‘cluster-create’ are retained.</para>
            <para>The definition and usage of the parameters for ‘cluster-create’ are as
                    follows:</para>
            <variablelist>
              <varlistentry>
                <term>&lt;name&gt;</term>
                <listitem>
                  <para>Name of the cluster to create.  If a name is not specified, a random
                                name will be generated using a string and a number, for example
                                “gamma-7-cluster”.</para>
                </listitem>
              </varlistentry>
              <varlistentry>
                <term>–cluster-template &lt;cluster-template&gt;</term>
                <listitem>
                  <para>The ID or name of the ClusterTemplate to use.  This is a mandatory
                                parameter.  Once a ClusterTemplate is used to create a cluster, it cannot
                                be deleted or modified until all clusters that use the ClusterTemplate have
                                been deleted.</para>
                </listitem>
              </varlistentry>
              <varlistentry>
                <term>–keypair &lt;keypair&gt;</term>
                <listitem>
                  <para>The name of the SSH keypair to configure in the cluster servers
                                for ssh access.  You will need the key to be able to ssh to the
                                servers in the cluster.  The login name is specific to the cluster
                                driver. If keypair is not provided it will attempt to use the value in
                                the ClusterTemplate. If the ClusterTemplate is also missing a keypair value
                                then an error will be returned.  The keypair value provided here will
                                override the keypair value from the ClusterTemplate.</para>
                </listitem>
              </varlistentry>
              <varlistentry>
                <term>–node-count &lt;node-count&gt;</term>
                <listitem>
                  <para>The number of servers that will serve as node in the cluster.
                                The default is 1.</para>
                </listitem>
              </varlistentry>
              <varlistentry>
                <term>–master-count &lt;master-count&gt;</term>
                <listitem>
                  <para>The number of servers that will serve as master for the cluster.
                                The default is 1.  Set to more than 1 master to enable High
                                Availability.  If the option ‘–master-lb-enabled’ is specified in
                                the ClusterTemplate, the master servers will be placed in a load balancer
                                pool.</para>
                </listitem>
              </varlistentry>
              <varlistentry>
                <term>–discovery-url &lt;discovery-url&gt;</term>
                <listitem>
                  <para>The custom discovery url for node discovery.  This is used by the
                                COE to discover the servers that have been created to host the
                                containers.  The actual discovery mechanism varies with the COE.  In
                                some cases, Magnum fills in the server info in the discovery
                                service.  In other cases, if the discovery-url is not specified,
                                Magnum will use the public discovery service at:</para>
                  <screen>https://discovery.etcd.io</screen>
                  <para>In this case, Magnum will generate a unique url here for each cluster
                                and store the info for the servers.</para>
                </listitem>
              </varlistentry>
              <varlistentry>
                <term>–timeout &lt;timeout&gt;</term>
                <listitem>
                  <para>The timeout for cluster creation in minutes. The value expected is a
                                positive integer and the default is 60 minutes.  If the timeout is
                                reached during cluster-create, the operation will be aborted and the
                                cluster status will be set to ‘CREATE_FAILED’.</para>
                </listitem>
              </varlistentry>
            </variablelist>
          </section>
          <section>
            <title>List</title>
            <para>The ‘cluster-list’ command lists all the clusters that belong to the tenant,
                    for example:</para>
            <screen>magnum cluster-list</screen>
          </section>
          <section>
            <title>Show</title>
            <para>The ‘cluster-show’ command prints all the details of a cluster, for
                    example:</para>
            <screen>magnum cluster-show mycluster</screen>
            <para>The properties include those not specified by users that have been
                    assigned default values and properties from new resources that
                    have been created for the cluster.</para>
          </section>
          <section>
            <title>Update</title>
            <para>A cluster can be modified using the ‘cluster-update’ command, for example:</para>
            <screen>magnum cluster-update mycluster replace node_count=8</screen>
            <para>The parameters are positional and their definition and usage are as
                    follows.</para>
            <variablelist>
              <varlistentry>
                <term>&lt;cluster&gt;</term>
                <listitem>
                  <para>This is the first parameter, specifying the UUID or name of the cluster
                                to update.</para>
                </listitem>
              </varlistentry>
              <varlistentry>
                <term>&lt;op&gt;</term>
                <listitem>
                  <para>This is the second parameter, specifying the desired change to be
                                made to the cluster attributes.  The allowed changes are ‘add’,
                                ‘replace’ and ‘remove’.</para>
                </listitem>
              </varlistentry>
              <varlistentry>
                <term>&lt;attribute=value&gt;</term>
                <listitem>
                  <para>This is the third parameter, specifying the targeted attributes in
                                the cluster as a list separated by blank space.  To add or replace an
                                attribute, you need to specify the value for the attribute.  To
                                remove an attribute, you only need to specify the name of the
                                attribute.  Currently the only attribute that can be replaced or
                                removed is ‘node_count’.  The attributes ‘name’, ‘master_count’ and
                                ‘discovery_url’ cannot be replaced or delete.  The table below
                                summarizes the possible change to a cluster.</para>
                  <informaltable>
                    <tgroup cols="4">
                      <colspec colname="c1" colwidth="15"/>
                      <colspec colname="c2" colwidth="5"/>
                      <colspec colname="c3" colwidth="18"/>
                      <colspec colname="c4" colwidth="23"/>
                      <thead>
                        <row>
                          <entry>
                            <para>Attribute</para>
                          </entry>
                          <entry>
                            <para>add</para>
                          </entry>
                          <entry>
                            <para>replace</para>
                          </entry>
                          <entry>
                            <para>remove</para>
                          </entry>
                        </row>
                      </thead>
                      <tbody>
                        <row>
                          <entry>
                            <para>node_count</para>
                          </entry>
                          <entry>
                            <para>no</para>
                          </entry>
                          <entry>
                            <para>add/remove nodes</para>
                          </entry>
                          <entry>
                            <para>reset to default of 1</para>
                          </entry>
                        </row>
                        <row>
                          <entry>
                            <para>master_count</para>
                          </entry>
                          <entry>
                            <para>no</para>
                          </entry>
                          <entry>
                            <para>no</para>
                          </entry>
                          <entry>
                            <para>no</para>
                          </entry>
                        </row>
                        <row>
                          <entry>
                            <para>name</para>
                          </entry>
                          <entry>
                            <para>no</para>
                          </entry>
                          <entry>
                            <para>no</para>
                          </entry>
                          <entry>
                            <para>no</para>
                          </entry>
                        </row>
                        <row>
                          <entry>
                            <para>discovery_url</para>
                          </entry>
                          <entry>
                            <para>no</para>
                          </entry>
                          <entry>
                            <para>no</para>
                          </entry>
                          <entry>
                            <para>no</para>
                          </entry>
                        </row>
                      </tbody>
                    </tgroup>
                  </informaltable>
                </listitem>
              </varlistentry>
            </variablelist>
            <para>The ‘cluster-update’ operation cannot be initiated when another operation
                    is in progress.</para>
            <para><emphasis role="bold">NOTE:</emphasis> The attribute names in cluster-update are slightly different
                    from the corresponding names in the cluster-create command: the dash ‘-‘
                    is replaced by an underscore ‘_’.  For instance, ‘node-count’ in
                    cluster-create is ‘node_count’ in cluster-update.</para>
          </section>
          <section>
            <title>Scale</title>
            <para>Scaling a cluster means adding servers to or removing servers from the cluster.
                    Currently, this is done through the ‘cluster-update’ operation by modifying
                    the node-count attribute, for example:</para>
            <screen>magnum cluster-update mycluster replace node_count=2</screen>
            <para>When some nodes are removed, Magnum will attempt to find nodes with no
                    containers to remove.  If some nodes with containers must be removed,
                    Magnum will log a warning message.</para>
          </section>
          <section>
            <title>Delete</title>
            <para>The ‘cluster-delete’ operation removes the cluster by deleting all resources
                    such as servers, network, storage;  for example:</para>
            <screen>magnum cluster-delete mycluster</screen>
            <para>The only parameter for the cluster-delete command is the ID or name of the
                    cluster to delete.  Multiple clusters can be specified, separated by a blank
                    space.</para>
            <para>If the operation fails, there may be some remaining resources that
                    have not been deleted yet.  In this case, you can troubleshoot through
                    Heat.  If the templates are deleted manually in Heat, you can delete
                    the cluster in Magnum to clean up the cluster from Magnum database.</para>
            <para>The ‘cluster-delete’ operation can be initiated when another operation is
                    still in progress.</para>
          </section>
        </section>
      </section>
      <section xml:base="user/index#python-client">
        <title>Python Client</title>
        <section>
          <title>Installation</title>
          <para>Follow the instructions in the OpenStack Installation Guide to enable the
                repositories for your distribution:</para>
          <itemizedlist>
            <listitem>
              <para>
                <link xlink:href="http://docs.openstack.org/liberty/install-guide-rdo/">RHEL/CentOS/Fedora</link>
              </para>
            </listitem>
            <listitem>
              <para>
                <link xlink:href="http://docs.openstack.org/liberty/install-guide-ubuntu/">Ubuntu/Debian</link>
              </para>
            </listitem>
            <listitem>
              <para>
                <link xlink:href="http://docs.openstack.org/liberty/install-guide-obs/">openSUSE/SUSE Linux Enterprise</link>
              </para>
            </listitem>
          </itemizedlist>
          <para>Install using distribution packages for RHEL/CentOS/Fedora:</para>
          <screen>$ sudo yum install python-magnumclient</screen>
          <para>Install using distribution packages for Ubuntu/Debian:</para>
          <screen>$ sudo apt-get install python-magnumclient</screen>
          <para>Install using distribution packages for OpenSuSE and SuSE Enterprise Linux:</para>
          <screen>$ sudo zypper install python-magnumclient</screen>
        </section>
        <section>
          <title>Verifying installation</title>
          <para>Execute the <literal>magnum</literal> command with the <literal>–version</literal> argument to confirm that the
                client is installed and in the system path:</para>
          <screen>$ magnum --version
1.1.0</screen>
          <para>Note that the version returned may differ from the above, 1.1.0 was the latest
                available version at the time of writing.</para>
        </section>
        <section>
          <title>Using the command-line client</title>
          <para>Refer to the <link xlink:href="http://docs.openstack.org/cli-reference/magnum.html">OpenStack Command-Line Interface Reference</link> for a full list of the
                commands supported by the <literal>magnum</literal> command-line client.</para>
        </section>
      </section>
      <section xml:base="user/index#horizon-interface">
        <title>Horizon Interface</title>
        <para>Magnum provides a Horizon plugin so that users can access the Container
            Infrastructure Management service through the OpenStack browser-based
            graphical UI.  The plugin is available from
            <link xlink:href="https://github.com/openstack/magnum-ui">magnum-ui</link>.  It is not
            installed by default in the standard Horizon service, but you can
            follow the instruction for <link xlink:href="http://docs.openstack.org/developer/horizon/tutorials/plugin.html#installing-your-plugin">installing a Horizon plugin</link>.</para>
        <para>In Horizon, the container infrastructure panel is part of the
            ‘Project’ view and it currently supports the following operations:</para>
        <itemizedlist>
          <listitem>
            <para>View list of cluster templates</para>
          </listitem>
          <listitem>
            <para>View details of a cluster template</para>
          </listitem>
          <listitem>
            <para>Create a cluster template</para>
          </listitem>
          <listitem>
            <para>Delete a cluster template</para>
          </listitem>
          <listitem>
            <para>View list of clusters</para>
          </listitem>
          <listitem>
            <para>View details of a cluster</para>
          </listitem>
          <listitem>
            <para>Create a cluster</para>
          </listitem>
          <listitem>
            <para>Delete a cluster</para>
          </listitem>
          <listitem>
            <para>Get the Certificate Authority for a cluster</para>
          </listitem>
          <listitem>
            <para>Sign a user key and obtain a signed certificate for accessing the secured
                    COE API endpoint in a cluster.</para>
          </listitem>
        </itemizedlist>
        <para>Other operations are not yet supported and the CLI should be used for these.</para>
        <!-- <para>Following is the screenshot of the Horizon view showing the list of cluster
            templates.</para>
        <informalfigure>
          <mediaobject>
            <imageobject role="fo">
              <imagedata fileref="cluster-template.png"/>
            </imageobject>
            <imageobject role="html">
              <imagedata fileref="cluster-template.png"/>
            </imageobject>
          </mediaobject>
        </informalfigure>
        <para>Following is the screenshot of the Horizon view showing the details of a
            cluster template.</para>
        <informalfigure>
          <mediaobject>
            <imageobject role="fo">
              <imagedata fileref="cluster-template-details.png"/>
            </imageobject>
            <imageobject role="html">
              <imagedata fileref="cluster-template-details.png"/>
            </imageobject>
          </mediaobject>
        </informalfigure>
        <para>Following is the screenshot of the dialog to create a new cluster.</para>
        <informalfigure>
          <mediaobject>
            <imageobject role="fo">
              <imagedata fileref="cluster-create.png"/>
            </imageobject>
            <imageobject role="html">
              <imagedata fileref="cluster-create.png"/>
            </imageobject>
          </mediaobject>
        </informalfigure> -->
      </section>
      <section xml:base="user/index#cluster-drivers">
        <title>Cluster Drivers</title>
        <para>A cluster driver is a collection of python code, heat templates, scripts,
            images, and documents for a particular COE on a particular
            distro.  Magnum presents the concept of ClusterTemplates and clusters.  The
            implementation for a particular cluster type is provided by the cluster driver.
            In other words, the cluster driver provisions and manages the infrastructure
            for the COE.  Magnum includes default drivers for the following
            COE and distro pairs:</para>
        <informaltable>
          <tgroup cols="2">
            <colspec colname="c1" colwidth="12"/>
            <colspec colname="c2" colwidth="15"/>
            <thead>
              <row>
                <entry>
                  <para>COE</para>
                </entry>
                <entry>
                  <para>distro</para>
                </entry>
              </row>
            </thead>
            <tbody>
              <row>
                <entry>
                  <para>Kubernetes</para>
                </entry>
                <entry>
                  <para>Fedora Atomic</para>
                </entry>
              </row>
              <row>
                <entry>
                  <para>Kubernetes</para>
                </entry>
                <entry>
                  <para>CoreOS</para>
                </entry>
              </row>
              <row>
                <entry>
                  <para>Swarm</para>
                </entry>
                <entry>
                  <para>Fedora Atomic</para>
                </entry>
              </row>
              <row>
                <entry>
                  <para>Mesos</para>
                </entry>
                <entry>
                  <para>Ubuntu</para>
                </entry>
              </row>
            </tbody>
          </tgroup>
        </informaltable>
        <para>Magnum is designed to accommodate new cluster drivers to support custom
            COE’s and this section describes how a new cluster driver can be
            constructed and enabled in Magnum.</para>
        <section>
          <title>Directory structure</title>
          <para>Magnum expects the components to be organized in the following
                directory structure under the directory ‘drivers’:</para>
          <screen>COE_Distro/
   image/
   templates/
   api.py
   driver.py
   monitor.py
   scale.py
   template_def.py
   version.py</screen>
          <para>The minimum required components are:</para>
          <variablelist>
            <varlistentry>
              <term>driver.py</term>
              <listitem>
                <para>Python code that implements the controller operations for
                            the particular COE.  The driver must implement:
                            Currently supported:
                            <literal>cluster_create</literal>, <literal>cluster_update</literal>, <literal>cluster_delete</literal>.</para>
              </listitem>
            </varlistentry>
            <varlistentry>
              <term>templates</term>
              <listitem>
                <para>A directory of orchestration templates for managing the lifecycle
                            of clusters, including creation, configuration, update, and deletion.
                            Currently only Heat templates are supported, but in the future
                            other orchestration mechanism such as Ansible may be supported.</para>
              </listitem>
            </varlistentry>
            <varlistentry>
              <term>template_def.py</term>
              <listitem>
                <para>Python code that maps the parameters from the ClusterTemplate to the
                            input parameters for the orchestration and invokes
                            the orchestration in the templates directory.</para>
              </listitem>
            </varlistentry>
            <varlistentry>
              <term>version.py</term>
              <listitem>
                <para>Tracks the latest version of the driver in this directory.
                            This is defined by a <literal>version</literal> attribute and is represented in the
                            form of <literal>1.0.0</literal>. It should also include a <literal>Driver</literal> attribute with
                            descriptive name such as <literal>fedora_swarm_atomic</literal>.</para>
              </listitem>
            </varlistentry>
          </variablelist>
          <para>The remaining components are optional:</para>
          <variablelist>
            <varlistentry>
              <term>image</term>
              <listitem>
                <para>Instructions for obtaining or building an image suitable for the COE.</para>
              </listitem>
            </varlistentry>
            <varlistentry>
              <term>api.py</term>
              <listitem>
                <para>Python code to interface with the COE.</para>
              </listitem>
            </varlistentry>
            <varlistentry>
              <term>monitor.py</term>
              <listitem>
                <para>Python code to monitor the resource utilization of the cluster.</para>
              </listitem>
            </varlistentry>
            <varlistentry>
              <term>scale.py</term>
              <listitem>
                <para>Python code to scale the cluster by adding or removing nodes.</para>
              </listitem>
            </varlistentry>
          </variablelist>
        </section>
        <section>
          <title>Sample cluster driver</title>
          <para>To help developers in creating new COE drivers, a minimal cluster driver
                is provided as an example.  The ‘docker’ cluster driver will simply deploy
                a single VM running Ubuntu with the latest Docker version installed.
                It is not a true cluster, but the simplicity will help to illustrate
                the key concepts.</para>
          <para>
            <emphasis>To be filled in</emphasis>
          </para>
        </section>
        <section>
          <title>Installing a cluster driver</title>
          <para>
            <emphasis>To be filled in</emphasis>
          </para>
        </section>
      </section>
      <section xml:base="user/index#cluster-type-definition">
        <title>Cluster Type Definition</title>
        <para>There are three key pieces to a Cluster Type Definition:</para>
        <procedure>
          <step>
            <para>Heat Stack template - The HOT file that Magnum will use to generate a
                    cluster using a Heat Stack.</para>
          </step>
          <step>
            <para>Template definition - Magnum’s interface for interacting with the Heat
                    template.</para>
          </step>
          <step>
            <para>Definition Entry Point - Used to advertise the available Cluster Types.</para>
          </step>
        </procedure>
        <section>
          <title>The Heat Stack Template</title>
          <para>The Heat Stack Template is where most of the real work happens. The result of
                the Heat Stack Template should be a full Container Orchestration Environment.</para>
        </section>
        <section>
          <title>The Template Definition</title>
          <para>Template definitions are a mapping of Magnum object attributes and Heat
                template parameters, along with Magnum consumable template outputs. A
                Cluster Type Definition indicates which Cluster Types it can provide.
                Cluster Types are how Magnum determines which of the enabled Cluster
                Type Definitions it will use for a given cluster.</para>
        </section>
        <section>
          <title>The Definition Entry Point</title>
          <para>Entry points are a standard discovery and import mechanism for Python objects.
                Each Template Definition should have an Entry Point in the
                <literal>magnum.template_definitions</literal> group. This example exposes it’s Template
                Definition as <literal>example_template = example_template:ExampleTemplate</literal> in the
                <literal>magnum.template_definitions</literal> group.</para>
        </section>
        <section>
          <title>Installing Cluster Templates</title>
          <para>Because Cluster Type Definitions are basically Python projects, they can be
                worked with like any other Python project. They can be cloned from version
                control and installed or uploaded to a package index and installed via
                utilities such as pip.</para>
          <para>Enabling a Cluster Type is as simple as adding it’s Entry Point to the
                <literal>enabled_definitions</literal> config option in magnum.conf.:</para>
          <screen># Setup python environment and install Magnum

$ virtualenv .venv
$ source .venv/bin/active
(.venv)$ git clone https://github.com/openstack/magnum.git
(.venv)$ cd magnum
(.venv)$ python setup.py install

# List installed templates, notice default templates are enabled

(.venv)$ magnum-template-manage list-templates
Enabled Templates
  magnum_vm_atomic_k8s: /home/example/.venv/local/lib/python2.7/site-packages/magnum/templates/kubernetes/kubecluster.yaml
  magnum_vm_coreos_k8s: /home/example/.venv/local/lib/python2.7/site-packages/magnum/templates/kubernetes/kubecluster-coreos.yaml
Disabled Templates

# Install example template

(.venv)$ cd contrib/templates/example
(.venv)$ python setup.py install

# List installed templates, notice example template is disabled

(.venv)$ magnum-template-manage list-templates
Enabled Templates
  magnum_vm_atomic_k8s: /home/example/.venv/local/lib/python2.7/site-packages/magnum/templates/kubernetes/kubecluster.yaml
  magnum_vm_coreos_k8s: /home/example/.venv/local/lib/python2.7/site-packages/magnum/templates/kubernetes/kubecluster-coreos.yaml
Disabled Templates
  example_template: /home/example/.venv/local/lib/python2.7/site-packages/ExampleTemplate-0.1-py2.7.egg/example_template/example.yaml

# Enable example template by setting enabled_definitions in magnum.conf

(.venv)$ sudo mkdir /etc/magnum
(.venv)$ sudo bash -c "cat &gt; /etc/magnum/magnum.conf &lt;&lt; END_CONF
[bay]
enabled_definitions=magnum_vm_atomic_k8s,magnum_vm_coreos_k8s,example_template
END_CONF"

# List installed templates, notice example template is now enabled

(.venv)$ magnum-template-manage list-templates
Enabled Templates
  example_template: /home/example/.venv/local/lib/python2.7/site-packages/ExampleTemplate-0.1-py2.7.egg/example_template/example.yaml
  magnum_vm_atomic_k8s: /home/example/.venv/local/lib/python2.7/site-packages/magnum/templates/kubernetes/kubecluster.yaml
  magnum_vm_coreos_k8s: /home/example/.venv/local/lib/python2.7/site-packages/magnum/templates/kubernetes/kubecluster-coreos.yaml
Disabled Templates

# Use --details argument to get more details about each template

(.venv)$ magnum-template-manage list-templates --details
Enabled Templates
  example_template: /home/example/.venv/local/lib/python2.7/site-packages/ExampleTemplate-0.1-py2.7.egg/example_template/example.yaml
     Server_Type  OS       CoE
     vm         example  example_coe
  magnum_vm_atomic_k8s: /home/example/.venv/local/lib/python2.7/site-packages/magnum/templates/kubernetes/kubecluster.yaml
     Server_Type   OS             CoE
     vm        fedora-atomic  kubernetes
  magnum_vm_coreos_k8s: /home/example/.venv/local/lib/python2.7/site-packages/magnum/templates/kubernetes/kubecluster-coreos.yaml
     Server_Type  OS      CoE
     vm         coreos  kubernetes
Disabled Templates</screen>
        </section>
      </section>
      <section xml:base="user/index#heat-stack-templates">
        <title>Heat Stack Templates</title>
        <para>Heat Stack Templates are what Magnum passes to Heat to generate a cluster. For
            each ClusterTemplate resource in Magnum, a Heat stack is created to arrange all
            of the cloud resources needed to support the container orchestration
            environment. These Heat stack templates provide a mapping of Magnum object
            attributes to Heat template parameters, along with Magnum consumable stack
            outputs. Magnum passes the Heat Stack Template to the Heat service to create a
            Heat stack. The result is a full Container Orchestration Environment.</para>
      </section>
      <section xml:base="user/index#choosing-a-coe">
        <title>Choosing a COE</title>
        <para>Magnum supports a variety of COE options, and allows more to be added over time
            as they gain popularity. As an operator, you may choose to support the full
            variety of options, or you may want to offer a subset of the available choices.
            Given multiple choices, your users can run one or more clusters, and each may
            use a different COE. For example, I might have multiple clusters that use
            Kubernetes, and just one cluster that uses Swarm. All of these clusters can
            run concurrently, even though they use different COE software.</para>
        <para>Choosing which COE to use depends on what tools you want to use to manage your
            containers once you start your app. If you want to use the Docker tools, you
            may want to use the Swarm cluster type. Swarm will spread your containers
            across the various nodes in your cluster automatically. It does not monitor
            the health of your containers, so it can’t restart them for you if they stop.
            It will not automatically scale your app for you (as of Swarm version 1.2.2).
            You may view this as a plus. If you prefer to manage your application yourself,
            you might prefer swarm over the other COE options.</para>
        <para>Kubernetes (as of v1.2) is more sophisticated than Swarm (as of v1.2.2). It
            offers an attractive YAML file description of a pod, which is a grouping of
            containers that run together as part of a distributed application. This file
            format allows you to model your application deployment using a declarative
            style. It has support for auto scaling and fault recovery, as well as features
            that allow for sophisticated software deployments, including canary deploys
            and blue/green deploys. Kubernetes is very popular, especially for web
            applications.</para>
        <para>Apache Mesos is a COE that has been around longer than Kubernetes or Swarm. It
            allows for a variety of different frameworks to be used along with it,
            including Marathon, Aurora, Chronos, Hadoop, and <link xlink:href="http://mesos.apache.org/documentation/latest/frameworks/">a number of others.</link></para>
        <para>The Apache Mesos framework design can be used to run alternate COE software
            directly on Mesos. Although this approach is not widely used yet, it may soon
            be possible to run Mesos with Kubernetes and Swarm as frameworks, allowing
            you to share the resources of a cluster between multiple different COEs. Until
            this option matures, we encourage Magnum users to create multiple clusters, and
            use the COE in each cluster that best fits the anticipated workload.</para>
        <para>Finding the right COE for your workload is up to you, but Magnum offers you a
            choice to select among the prevailing leading options. Once you decide, see
            the next sections for examples of how to create a cluster with your desired
            COE.</para>
      </section>
      <section xml:base="user/index#native-clients">
        <title>Native Clients</title>
        <para>Magnum preserves the native user experience with a COE and does not
            provide a separate API or client.  This means you will need to use the
            native client for the particular cluster type to interface with the
            clusters.  In the typical case, there are two clients to consider:</para>
        <variablelist>
          <varlistentry>
            <term>COE level</term>
            <listitem>
              <para>This is the orchestration or management level such as Kubernetes,
                        Swarm, Mesos and its frameworks.</para>
            </listitem>
          </varlistentry>
          <varlistentry>
            <term>Container level</term>
            <listitem>
              <para>This is the low level container operation.  Currently it is
                        Docker for all clusters.</para>
            </listitem>
          </varlistentry>
        </variablelist>
        <para>The clients can be CLI and/or browser-based.  You will need to refer
            to the documentation for the specific native client and appropriate
            version for details, but following are some pointers for reference.</para>
        <para>Kubernetes CLI is the tool ‘kubectl’, which can be simply copied from
            a node in the cluster or downloaded from the Kubernetes release.  For
            instance, if the cluster is running Kubernetes release 1.2.0, the
            binary for ‘kubectl’ can be downloaded as and set up locally as
            follows:</para>
        <screen>curl -O https://storage.googleapis.com/kubernetes-release/release/v1.2.0/bin/linux/amd64/kubectl
chmod +x kubectl
sudo mv kubectl /usr/local/bin/kubectl</screen>
        <para>Kubernetes also provides a browser UI. If the cluster has the
            Kubernetes Dashboard running; it can be accessed using:</para>
        <screen>eval $(magnum cluster-config &lt;cluster-name&gt;)
kubectl proxy

The browser can be accessed at http://localhost:8001/ui</screen>
        <para>For Swarm, the main CLI is ‘docker’, along with associated tools
            such as ‘docker-compose’, etc.  Specific version of the binaries can
            be obtained from the <link xlink:href="https://docs.docker.com/engine/installation/binaries/">Docker Engine installation</link>.</para>
        <para>Mesos cluster uses the Marathon framework<!-- and details on the Marathon
            UI can be found in the section <xref linkend="using-marathon"/>-->.</para>
        <para>Depending on the client requirement, you may need to use a version of
            the client that matches the version in the cluster.  To determine the
            version of the COE and container, use the command ‘cluster-show’ and
            look for the attribute <emphasis>coe_version</emphasis> and <emphasis>container_version</emphasis>:</para>
        <screen>magnum cluster-show k8s-cluster
+--------------------+------------------------------------------------------------+
| Property           | Value                                                      |
+--------------------+------------------------------------------------------------+
| status             | CREATE_COMPLETE                                            |
| uuid               | 04952c60-a338-437f-a7e7-d016d1d00e65                       |
| stack_id           | b7bf72ce-b08e-4768-8201-e63a99346898                       |
| status_reason      | Stack CREATE completed successfully                        |
| created_at         | 2016-07-25T23:14:06+00:00                                  |
| updated_at         | 2016-07-25T23:14:10+00:00                                  |
| create_timeout     | 60                                                         |
| coe_version        | v1.2.0                                                     |
| api_address        | https://192.168.19.86:6443                                 |
| cluster_template_id| da2825a0-6d09-4208-b39e-b2db666f1118                       |
| master_addresses   | ['192.168.19.87']                                          |
| node_count         | 1                                                          |
| node_addresses     | ['192.168.19.88']                                          |
| master_count       | 1                                                          |
| container_version  | 1.9.1                                                      |
| discovery_url      | https://discovery.etcd.io/3b7fb09733429d16679484673ba3bfd5 |
| name               | k8s-cluster                                                |
+--------------------+------------------------------------------------------------+</screen>
      </section>
      <section xml:base="user/index#kubernetes">
        <title>Kubernetes</title>
        <para>Kubernetes uses a range of terminology that we refer to in this guide. We
            define these common terms for your reference:</para>
        <variablelist>
          <varlistentry>
            <term>Pod</term>
            <listitem>
              <para>When using the Kubernetes container orchestration engine, a pod is the
                        smallest deployable unit that can be created and managed. A pod is a
                        co-located group of application containers that run with a shared context.
                        When using Magnum, pods are created and managed within clusters. Refer to the
                        <link xlink:href="http://kubernetes.io/v1.0/docs/user-guide/pods.html">pods section</link> in the <link xlink:href="http://kubernetes.io/v1.0/docs/user-guide/">Kubernetes
                            User Guide</link> for more information.</para>
            </listitem>
          </varlistentry>
          <varlistentry>
            <term>Replication controller</term>
            <listitem>
              <para>A replication controller is used to ensure that at any given time a certain
                        number of replicas of a pod are running. Pods are automatically created and
                        deleted by the replication controller as necessary based on a template to
                        ensure that the defined number of replicas exist. Refer to the <link xlink:href="http://kubernetes.io/v1.0/docs/user-guide/replication-controller.html">replication
                            controller section</link> in
                        the <link xlink:href="http://kubernetes.io/v1.0/docs/user-guide/">Kubernetes User Guide</link> for more information.</para>
            </listitem>
          </varlistentry>
          <varlistentry>
            <term>Service</term>
            <listitem>
              <para>A service is an additional layer of abstraction provided by the Kubernetes
                        container orchestration engine which defines a logical set of pods and a
                        policy for accessing them. This is useful because pods are created and
                        deleted by a replication controller, for example, other pods needing to
                        discover them can do so via the service abstraction. Refer to the
                        <link xlink:href="http://kubernetes.io/v1.0/docs/user-guide/services.html">services section</link> in the
                        <link xlink:href="http://kubernetes.io/v1.0/docs/user-guide/">Kubernetes User Guide</link> for more information.</para>
            </listitem>
          </varlistentry>
        </variablelist>
        <para>When Magnum deploys a Kubernetes cluster, it uses parameters defined in the
            ClusterTemplate and specified on the cluster-create command, for example:</para>
        <screen>magnum cluster-template-create k8s-cluster-template \
                           --image fedora-atomic-latest \
                           --keypair testkey \
                           --external-network public \
                           --dns-nameserver 8.8.8.8 \
                           --flavor m1.small \
                           --docker-volume-size 5 \
                           --network-driver flannel \
                           --coe kubernetes

magnum cluster-create k8s-cluster \
                      --cluster-template k8s-cluster-template \
                      --master-count 3 \
                      --node-count 8</screen>
        <para><!--Refer to the <xref linkend="clustertemplate"/> and <xref linkend="cluster"/> sections for the full list of
            parameters. -->Following are further details relevant to a Kubernetes cluster:</para>
        <variablelist>
          <varlistentry>
            <term>Number of masters (master-count)</term>
            <listitem>
              <para>Specified in the cluster-create command to indicate how many servers will
                        run as master in the cluster.  Having more than one will provide high
                        availability.  The masters will be in a load balancer pool and the
                        virtual IP address (VIP) of the load balancer will serve as the
                        Kubernetes API endpoint.  For external access, a floating IP
                        associated with this VIP is available and this is the endpoint
                        shown for Kubernetes in the ‘cluster-show’ command.</para>
            </listitem>
          </varlistentry>
          <varlistentry>
            <term>Number of nodes (node-count)</term>
            <listitem>
              <para>Specified in the cluster-create command to indicate how many servers will
                        run as node in the cluster to host the users’ pods.  The nodes are registered
                        in Kubernetes using the Nova instance name.</para>
            </listitem>
          </varlistentry>
          <varlistentry>
            <term>Network driver (network-driver)</term>
            <listitem>
              <para>Specified in the ClusterTemplate to select the network driver.
                        The supported and default network driver is ‘flannel’, an overlay
                        network providing a flat network for all pods.<!--  Refer to the
                        <xref linkend="networking"/> section for more details.--></para>
            </listitem>
          </varlistentry>
          <varlistentry>
            <term>Volume driver (volume-driver)</term>
            <listitem>
              <para>Specified in the ClusterTemplate to select the volume driver.  The supported
                        volume driver is ‘cinder’, allowing Cinder volumes to be mounted in
                        containers for use as persistent storage.  Data written to these volumes
                        will persist after the container exits and can be accessed again from other
                        containers, while data written to the union file system hosting the container
                        will be deleted.<!--  Refer to the <xref linkend="storage"/> section for more details.--></para>
            </listitem>
          </varlistentry>
          <varlistentry>
            <term>Storage driver (docker-storage-driver)</term>
            <listitem>
              <para>Specified in the ClusterTemplate to select the Docker storage driver.  The
                        supported storage drivers are ‘devicemapper’ and ‘overlay’, with
                        ‘devicemapper’ being the default.<!-- Refer to the <xref linkend="storage"/> section for more
                        details.--></para>
            </listitem>
          </varlistentry>
          <varlistentry>
            <term>Image (image)</term>
            <listitem>
              <para>Specified in the ClusterTemplate to indicate the image to boot the servers.
                        The image binary is loaded in Glance with the attribute
                        ‘os_distro = fedora-atomic’.
                        Current supported images are Fedora Atomic (download from <link xlink:href="https://alt.fedoraproject.org/pub/alt/atomic/stable/Cloud-Images/x86_64/Images">Fedora</link> )
                        and CoreOS (download from <link xlink:href="http://beta.release.core-os.net/amd64-usr/current/coreos_production_openstack_image.img.bz2">CoreOS</link> )</para>
            </listitem>
          </varlistentry>
          <varlistentry>
            <term>TLS (tls-disabled)</term>
            <listitem>
              <para>Transport Layer Security is enabled by default, so you need a key and
                        signed certificate to access the Kubernetes API and CLI.  Magnum
                        handles its own key and certificate when interfacing with the
                        Kubernetes cluster.  In development mode, TLS can be disabled.  Refer to
                        the ‘Transport Layer Security’_ section for more details.</para>
            </listitem>
          </varlistentry>
          <varlistentry>
            <term>What runs on the servers</term>
            <listitem>
              <para>The servers for Kubernetes master host containers in the ‘kube-system’
                        name space to run the Kubernetes proxy, scheduler and controller manager.
                        The masters will not host users’ pods.  Kubernetes API server, docker
                        daemon, etcd and flannel run as systemd services.  The servers for
                        Kubernetes node also host a container in the ‘kube-system’ name space
                        to run the Kubernetes proxy, while Kubernetes kubelet, docker daemon
                        and flannel run as systemd services.</para>
            </listitem>
          </varlistentry>
          <varlistentry>
            <term>Log into the servers</term>
            <listitem>
              <para>You can log into the master servers using the login ‘fedora’ and the
                        keypair specified in the ClusterTemplate.</para>
            </listitem>
          </varlistentry>
        </variablelist>
        <para>In addition to the common attributes in the ClusterTemplate, you can specify
            the following attributes that are specific to Kubernetes by using the
            labels attribute.</para>
        <variablelist>
          <varlistentry>
            <term/>
            <listitem>
              <para>This label corresponds to Kubernetes parameter for the API server ‘–admission-control’.
                        For more details, refer to the <link xlink:href="https://kubernetes.io/docs/admin/admission-controllers//">Admission Controllers</link>.
                        The default value corresponds to the one recommended in this doc
                        for our current Kubernetes version.</para>
            </listitem>
          </varlistentry>
          <varlistentry>
            <term/>
            <listitem>
              <para>This label sets the size of a volume holding the etcd storage data.
                        The default value is 0, meaning the etcd data is not persisted (no volume).</para>
            </listitem>
          </varlistentry>
          <varlistentry>
            <term/>
            <listitem>
              <para>This label allows users to select <link xlink:href="https://hub.docker.com/r/openstackmagnum/kubernetes-apiserver/tags/">a specific Kubernetes release,
                            based on its container tag</link>.
                        If unset, the current Magnum version’s default Kubernetes release is
                        installed.</para>
            </listitem>
          </varlistentry>
          <varlistentry>
            <term/>
            <listitem>
              <para>This label triggers the deployment of the kubernetes dashboard.
                        The default value is 1, meaning it will be enabled.</para>
            </listitem>
          </varlistentry>
        </variablelist>
        <section>
          <title>External load balancer for services</title>
          <para>All Kubernetes pods and services created in the cluster are assigned IP
                addresses on a private container network so they can access each other
                and the external internet.  However, these IP addresses are not
                accessible from an external network.</para>
          <para>To publish a service endpoint externally so that the service can be
                accessed from the external network, Kubernetes provides the external
                load balancer feature.  This is done by simply specifying in the
                service manifest the attribute “type: LoadBalancer”.  Magnum enables
                and configures the Kubernetes plugin for OpenStack so that it can
                interface with Neutron and manage the necessary networking resources.</para>
          <para>When the service is created, Kubernetes will add an external load
                balancer in front of the service so that the service will have an
                external IP address in addition to the internal IP address on the
                container network.  The service endpoint can then be accessed with
                this external IP address.  Kubernetes handles all the life cycle
                operations when pods are modified behind the service and when the
                service is deleted.</para>
          <para>Refer to the document <link xlink:href="https://github.com/openstack/magnum/blob/master/doc/source/dev/kubernetes-load-balancer.rst">Kubernetes external load balancer</link>
                for more details.</para>
        </section>
      </section>
      <section xml:base="user/index#swarm">
        <title>Swarm</title>
        <para>A Swarm cluster is a pool of servers running Docker daemon that is
            managed as a single Docker host.  One or more Swarm managers accepts
            the standard Docker API and manage this pool of servers.
            Magnum deploys a Swarm cluster using parameters defined in
            the ClusterTemplate and specified on the ‘cluster-create’ command, for
            example:</para>
        <screen>magnum cluster-template-create swarm-cluster-template \
                           --image fedora-atomic-latest \
                           --keypair testkey \
                           --external-network public \
                           --dns-nameserver 8.8.8.8 \
                           --flavor m1.small \
                           --docker-volume-size 5 \
                           --coe swarm

magnum cluster-create swarm-cluster \
                  --cluster-template swarm-cluster-template \
                  --master-count 3 \
                  --node-count 8</screen>
        <para><!--Refer to the <xref linkend="clustertemplate"/> and <xref linkend="cluster"/> sections for the full list of
            parameters. -->Following are further details relevant to Swarm:</para>
        <variablelist>
          <varlistentry>
            <term>What runs on the servers</term>
            <listitem>
              <para>There are two types of servers in the Swarm cluster: managers and nodes.
                        The Docker daemon runs on all servers.  On the servers for manager,
                        the Swarm manager is run as a Docker container on port 2376 and this
                        is initiated by the systemd service swarm-manager.  Etcd is also run
                        on the manager servers for discovery of the node servers in the cluster.
                        On the servers for node, the Swarm agent is run as a Docker
                        container on port 2375 and this is initiated by the systemd service
                        swarm-agent.  On start up, the agents will register themselves in
                        etcd and the managers will discover the new node to manage.</para>
            </listitem>
          </varlistentry>
          <varlistentry>
            <term>Number of managers (master-count)</term>
            <listitem>
              <para>Specified in the cluster-create command to indicate how many servers will
                        run as managers in the cluster.  Having more than one will provide high
                        availability.  The managers will be in a load balancer pool and the
                        load balancer virtual IP address (VIP) will serve as the Swarm API
                        endpoint.  A floating IP associated with the load balancer VIP will
                        serve as the external Swarm API endpoint.  The managers accept
                        the standard Docker API and perform the corresponding operation on the
                        servers in the pool.  For instance, when a new container is created,
                        the managers will select one of the servers based on some strategy
                        and schedule the containers there.</para>
            </listitem>
          </varlistentry>
          <varlistentry>
            <term>Number of nodes (node-count)</term>
            <listitem>
              <para>Specified in the cluster-create command to indicate how many servers will
                        run as nodes in the cluster to host your Docker containers.  These servers
                        will register themselves in etcd for discovery by the managers, and
                        interact with the managers.  Docker daemon is run locally to host
                        containers from users.</para>
            </listitem>
          </varlistentry>
          <varlistentry>
            <term>Network driver (network-driver)</term>
            <listitem>
              <para>Specified in the ClusterTemplate to select the network driver.  The supported
                        drivers are ‘docker’ and ‘flannel’, with ‘docker’ as the default.
                        With the ‘docker’ driver, containers are connected to the ‘docker0’
                        bridge on each node and are assigned local IP address.  With the
                        ‘flannel’ driver, containers are connected to a flat overlay network
                        and are assigned IP address by Flannel.<!--  Refer to the <xref linkend="networking"/>
                        section for more details.--></para>
            </listitem>
          </varlistentry>
          <varlistentry>
            <term>Volume driver (volume-driver)</term>
            <listitem>
              <para>Specified in the ClusterTemplate to select the volume driver to provide
                        persistent storage for containers.  The supported volume driver is
                        ‘rexray’.  The default is no volume driver.  When ‘rexray’ or other
                        volume driver is deployed, you can use the Docker ‘volume’ command to
                        create, mount, unmount, delete volumes in containers.  Cinder block
                        storage is used as the backend to support this feature.<!--
                        Refer to the <xref linkend="storage"/> section for more details.--></para>
            </listitem>
          </varlistentry>
          <varlistentry>
            <term>Storage driver (docker-storage-driver)</term>
            <listitem>
              <para>Specified in the ClusterTemplate to select the Docker storage driver.  The
                        supported storage driver are ‘devicemapper’ and ‘overlay’, with
                        ‘devicemapper’ being the default.<!-- Refer to the <xref linkend="storage"/> section for more
                        details.--></para>
            </listitem>
          </varlistentry>
          <varlistentry>
            <term>Image (image)</term>
            <listitem>
              <para>Specified in the ClusterTemplate to indicate the image to boot the servers
                        for the Swarm manager and node.
                        The image binary is loaded in Glance with the attribute
                        ‘os_distro = fedora-atomic’.
                        Current supported image is Fedora Atomic (download from <link xlink:href="https://alt.fedoraproject.org/pub/alt/atomic/stable/Cloud-Images/x86_64/Images">Fedora</link> )</para>
            </listitem>
          </varlistentry>
          <varlistentry>
            <term>TLS (tls-disabled)</term>
            <listitem>
              <para>Transport Layer Security is enabled by default to secure the Swarm API for
                        access by both the users and Magnum.  You will need a key and a
                        signed certificate to access the Swarm API and CLI.  Magnum
                        handles its own key and certificate when interfacing with the
                        Swarm cluster.  In development mode, TLS can be disabled.  Refer to
                        the ‘Transport Layer Security’_ section for details on how to create your
                        key and have Magnum sign your certificate.</para>
            </listitem>
          </varlistentry>
          <varlistentry>
            <term>Log into the servers</term>
            <listitem>
              <para>You can log into the manager and node servers with the account ‘fedora’ and
                        the keypair specified in the ClusterTemplate.</para>
            </listitem>
          </varlistentry>
        </variablelist>
        <para>In addition to the common attributes in the ClusterTemplate, you can specify
            the following attributes that are specific to Swarm by using the
            labels attribute.</para>
        <variablelist>
          <varlistentry>
            <term/>
            <listitem>
              <para>This label corresponds to Swarm parameter for master ‘–strategy’.
                        For more details, refer to the <link xlink:href="https://docs.docker.com/swarm/scheduler/strategy/">Swarm Strategy</link>.
                        Valid values for this label are:</para>
              <itemizedlist>
                <listitem>
                  <para>spread</para>
                </listitem>
                <listitem>
                  <para>binpack</para>
                </listitem>
                <listitem>
                  <para>random</para>
                </listitem>
              </itemizedlist>
            </listitem>
          </varlistentry>
        </variablelist>
      </section>
      <section xml:base="user/index#mesos">
        <title>Mesos</title>
        <para>A Mesos cluster consists of a pool of servers running as Mesos slaves,
            managed by a set of servers running as Mesos masters.  Mesos manages
            the resources from the slaves but does not itself deploy containers.
            Instead, one of more Mesos frameworks running on the Mesos cluster would
            accept user requests on their own endpoint, using their particular
            API.  These frameworks would then negotiate the resources with Mesos
            and the containers are deployed on the servers where the resources are
            offered.</para>
        <para>Magnum deploys a Mesos cluster using parameters defined in the ClusterTemplate
            and specified on the ‘cluster-create’ command, for example:</para>
        <screen>magnum cluster-template-create mesos-cluster-template \
                       --image ubuntu-mesos \
                       --keypair testkey \
                       --external-network public \
                       --dns-nameserver 8.8.8.8 \
                       --flavor m1.small \
                       --coe mesos

magnum cluster-create mesos-cluster \
                  --cluster-template mesos-cluster-template \
                  --master-count 3 \
                  --node-count 8</screen>
        <para><!--Refer to the <xref linkend="clustertemplate"/> and <xref linkend="cluster"/> sections for the full list of
            parameters.  -->Following are further details relevant to Mesos:</para>
        <variablelist>
          <varlistentry>
            <term>What runs on the servers</term>
            <listitem>
              <para>There are two types of servers in the Mesos cluster: masters and slaves.
                        The Docker daemon runs on all servers.  On the servers for master,
                        the Mesos master is run as a process on port 5050 and this is
                        initiated by the upstart service ‘mesos-master’.  Zookeeper is also
                        run on the master servers, initiated by the upstart service
                        ‘zookeeper’.  Zookeeper is used by the master servers for electing
                        the leader among the masters, and by the slave servers and
                        frameworks to determine the current leader.  The framework Marathon
                        is run as a process on port 8080 on the master servers, initiated by
                        the upstart service ‘marathon’.  On the servers for slave, the Mesos
                        slave is run as a process initiated by the upstart service
                        ‘mesos-slave’.</para>
            </listitem>
          </varlistentry>
          <varlistentry>
            <term>Number of master (master-count)</term>
            <listitem>
              <para>Specified in the cluster-create command to indicate how many servers
                        will run as masters in the cluster.  Having more than one will provide
                        high availability.  If the load balancer option is specified, the
                        masters will be in a load balancer pool and the load balancer
                        virtual IP address (VIP) will serve as the Mesos API endpoint.  A
                        floating IP associated with the load balancer VIP will serve as the
                        external Mesos API endpoint.</para>
            </listitem>
          </varlistentry>
          <varlistentry>
            <term>Number of agents (node-count)</term>
            <listitem>
              <para>Specified in the cluster-create command to indicate how many servers
                        will run as Mesos slave in the cluster.  Docker daemon is run locally to
                        host containers from users.  The slaves report their available
                        resources to the master and accept request from the master to deploy
                        tasks from the frameworks.  In this case, the tasks will be to
                        run Docker containers.</para>
            </listitem>
          </varlistentry>
          <varlistentry>
            <term>Network driver (network-driver)</term>
            <listitem>
              <para>Specified in the ClusterTemplate to select the network driver.  Currently
                        ‘docker’ is the only supported driver: containers are connected to
                        the ‘docker0’ bridge on each node and are assigned local IP address.<!--
                        Refer to the <xref linkend="networking"/> section for more details.--></para>
            </listitem>
          </varlistentry>
          <varlistentry>
            <term>Volume driver (volume-driver)</term>
            <listitem>
              <para>Specified in the ClusterTemplate to select the volume driver to provide
                        persistent storage for containers.  The supported volume driver is
                        ‘rexray’.  The default is no volume driver.  When ‘rexray’ or other
                        volume driver is deployed, you can use the Docker ‘volume’ command to
                        create, mount, unmount, delete volumes in containers.  Cinder block
                        storage is used as the backend to support this feature.<!--
                        Refer to the <xref linkend="storage"/> section for more details.--></para>
            </listitem>
          </varlistentry>
          <varlistentry>
            <term>Storage driver (docker-storage-driver)</term>
            <listitem>
              <para>This is currently not supported for Mesos.</para>
            </listitem>
          </varlistentry>
        </variablelist>
        <para>Image (image)</para>
        <para>Specified in the ClusterTemplate to indicate the image to boot the servers
                for the Mesos master and slave.  The image binary is loaded in
                Glance with the attribute ‘os_distro = ubuntu’.  You can download
                the <link
		xlink:href="https://fedorapeople.org/groups/magnum/ubuntu-mesos-latest.qcow2">ready-built
		image</link><!-- FIXME ,
                or you can create the image as described below in the <xref linkend="building-mesos-image"/>-->.</para>
        <variablelist>
          <varlistentry>
            <term>TLS (tls-disabled)</term>
            <listitem>
              <para>Transport Layer Security is currently not implemented yet for Mesos.</para>
            </listitem>
          </varlistentry>
          <varlistentry>
            <term>Log into the servers</term>
            <listitem>
              <para>You can log into the manager and node servers with the account
                        ‘ubuntu’ and the keypair specified in the ClusterTemplate.</para>
            </listitem>
          </varlistentry>
        </variablelist>
        <para>In addition to the common attributes in the baymodel, you can specify
            the following attributes that are specific to Mesos by using the
            labels attribute.</para>
        <variablelist>
          <varlistentry>
            <term/>
            <listitem>
              <para>When the volume driver ‘rexray’ is used, you can mount a data volume
                        backed by Cinder to a host to be accessed by a container.  In this
                        case, the label ‘rexray_preempt’ can optionally be set to True or
                        False to enable any host to take control of the volume regardless of
                        whether other hosts are using the volume.  This will in effect
                        unmount the volume from the current host and remount it on the new
                        host.  If this label is set to false, then rexray will ensure data
                        safety for locking the volume before remounting.  The default value
                        is False.</para>
            </listitem>
          </varlistentry>
          <varlistentry>
            <term/>
            <listitem>
              <para>This label corresponds to the Mesos parameter for slave
                        ‘–isolation’.  The isolators are needed to provide proper isolation
                        according to the runtime configurations specified in the container
                        image.  For more details, refer to the <link xlink:href="http://mesos.apache.org/documentation/latest/configuration/">Mesos configuration</link>
                        and the <link xlink:href="http://mesos.apache.org/documentation/latest/container-image/">Mesos container image support</link>.
                        Valid values for this label are:</para>
              <itemizedlist>
                <listitem>
                  <para>filesystem/posix</para>
                </listitem>
                <listitem>
                  <para>filesystem/linux</para>
                </listitem>
                <listitem>
                  <para>filesystem/shared</para>
                </listitem>
                <listitem>
                  <para>posix/cpu</para>
                </listitem>
                <listitem>
                  <para>posix/mem</para>
                </listitem>
                <listitem>
                  <para>posix/disk</para>
                </listitem>
                <listitem>
                  <para>cgroups/cpu</para>
                </listitem>
                <listitem>
                  <para>cgroups/mem</para>
                </listitem>
                <listitem>
                  <para>docker/runtime</para>
                </listitem>
                <listitem>
                  <para>namespaces/pid</para>
                </listitem>
              </itemizedlist>
            </listitem>
          </varlistentry>
          <varlistentry>
            <term/>
            <listitem>
              <para>This label corresponds to the Mesos parameter for agent
                        ‘–image_providers’, which tells Mesos containerizer what
                        types of container images are allowed.
                        For more details, refer to the <link xlink:href="http://mesos.apache.org/documentation/latest/configuration/">Mesos configuration</link> and
                        the <link xlink:href="http://mesos.apache.org/documentation/latest/container-image/">Mesos container image support</link>.
                        Valid values are:</para>
              <itemizedlist>
                <listitem>
                  <para>appc</para>
                </listitem>
                <listitem>
                  <para>docker</para>
                </listitem>
                <listitem>
                  <para>appc,docker</para>
                </listitem>
              </itemizedlist>
            </listitem>
          </varlistentry>
          <varlistentry>
            <term/>
            <listitem>
              <para>This label corresponds to the Mesos parameter ‘–work_dir’ for slave.
                        For more details, refer to the <link xlink:href="http://mesos.apache.org/documentation/latest/configuration/">Mesos configuration</link>.
                        Valid value is a directory path to use as the work directory for
                        the framework, for example:</para>
              <screen>mesos_slave_work_dir=/tmp/mesos</screen>
            </listitem>
          </varlistentry>
          <varlistentry>
            <term/>
            <listitem>
              <para>This label corresponds to the Mesos parameter for slave
                        ‘–executor_environment_variables’, which passes additional
                        environment variables to the executor and subsequent tasks.
                        For more details, refer to the <link xlink:href="http://mesos.apache.org/documentation/latest/configuration/">Mesos configuration</link>.
                        Valid value is the name of a JSON file, for example:</para>
              <screen>mesos_slave_executor_env_variables=/home/ubuntu/test.json</screen>
              <para>The JSON file should contain environment variables, for example:</para>
              <screen>{
   "PATH": "/bin:/usr/bin",
   "LD_LIBRARY_PATH": "/usr/local/lib"
}</screen>
              <para>By default the executor will inherit the slave’s environment
                        variables.</para>
            </listitem>
          </varlistentry>
        </variablelist>
        <section>
          <title>Building Mesos image</title>
          <para>The boot image for Mesos cluster is an Ubuntu 14.04 base image with the
                following middleware pre-installed:</para>
          <itemizedlist>
            <listitem>
              <para>
                <literal>docker</literal>
              </para>
            </listitem>
            <listitem>
              <para>
                <literal>zookeeper</literal>
              </para>
            </listitem>
            <listitem>
              <para>
                <literal>mesos</literal>
              </para>
            </listitem>
            <listitem>
              <para>
                <literal>marathon</literal>
              </para>
            </listitem>
          </itemizedlist>
          <para>The cluster driver provides two ways to create this image, as follows.</para>
          <section>
            <title>Diskimage-builder</title>
            <para>To run the <link xlink:href="http://docs.openstack.org/developer/diskimage-builder">diskimage-builder</link> tool
                    manually, use the provided <link xlink:href="http://git.openstack.org/cgit/openstack/magnum/tree/magnum/drivers/mesos_ubuntu_v1/image/mesos/">elements</link>.
                    Following are the typical steps to use the diskimage-builder tool on
                    an Ubuntu server:</para>
            <screen>$ sudo apt-get update
$ sudo apt-get install git qemu-utils python-pip
$ sudo pip install diskimage-builder

$ git clone https://git.openstack.org/openstack/magnum
$ git clone https://git.openstack.org/openstack/dib-utils.git
$ git clone https://git.openstack.org/openstack/tripleo-image-elements.git
$ git clone https://git.openstack.org/openstack/heat-templates.git
$ export PATH="${PWD}/dib-utils/bin:$PATH"
$ export ELEMENTS_PATH=tripleo-image-elements/elements:heat-templates/hot/software-config/elements:magnum/magnum/drivers/mesos_ubuntu_v1/image/mesos
$ export DIB_RELEASE=trusty

$ disk-image-create ubuntu vm docker mesos \
    os-collect-config os-refresh-config os-apply-config \
    heat-config heat-config-script \
    -o ubuntu-mesos.qcow2</screen>
          </section>
          <section>
            <title>Dockerfile</title>
            <para>To build the image as above but within a Docker container, use the
                    provided <link xlink:href="http://git.openstack.org/cgit/openstack/magnum/tree/magnum/drivers/mesos_ubuntu_v1/image/Dockerfile">Dockerfile</link>.
                    The output image will be saved as ‘/tmp/ubuntu-mesos.qcow2’.
                    Following are the typical steps to run a Docker container to build the image:</para>
            <screen>$ git clone https://git.openstack.org/openstack/magnum
$ cd magnum/magnum/drivers/mesos_ubuntu_v1/image
$ sudo docker build -t magnum/mesos-builder .
$ sudo docker run -v /tmp:/output --rm -ti --privileged magnum/mesos-builder
...
Image file /output/ubuntu-mesos.qcow2 created...</screen>
          </section>
        </section>
        <section>
          <title>Using Marathon</title>
          <para>Marathon is a Mesos framework for long running applications.  Docker
                containers can be deployed via Marathon’s REST API.  To get the
                endpoint for Marathon, run the cluster-show command and look for the
                property ‘api_address’.  Marathon’s endpoint is port 8080 on this IP
                address, so the web console can be accessed at:</para>
          <screen>http://&lt;api_address&gt;:8080/</screen>
          <para>Refer to Marathon documentation for details on running applications.
                For example, you can ‘post’ a JSON app description to
                <literal>http://&lt;api_address&gt;:8080/apps</literal> to deploy a Docker container:</para>
          <screen>$ cat &gt; app.json &lt;&lt; END
{
  "container": {
    "type": "DOCKER",
    "docker": {
      "image": "libmesos/ubuntu"
    }
  },
  "id": "ubuntu",
  "instances": 1,
  "cpus": 0.5,
  "mem": 512,
  "uris": [],
  "cmd": "while sleep 10; do date -u +%T; done"
}
END
$ API_ADDRESS=$(magnum cluster-show mesos-cluster | awk '/ api_address /{print $4}')
$ curl -X POST -H "Content-Type: application/json" \
    http://${API_ADDRESS}:8080/v2/apps -d@app.json</screen>
        </section>
      </section>
      <section xml:base="user/index#transport-layer-security">
        <title>Transport Layer Security</title>
        <para>Magnum uses TLS to secure communication between a cluster’s services and
            the outside world.  TLS is a complex subject, and many guides on it
            exist already.  This guide will not attempt to fully describe TLS, but
            instead will only cover the necessary steps to get a client set up to
            talk to a cluster with TLS. A more in-depth guide on TLS can be found in
            the <link xlink:href="https://www.feistyduck.com/books/openssl-cookbook/">OpenSSL Cookbook</link> by Ivan Ristić.</para>
        <para>TLS is employed at 3 points in a cluster:</para>
        <procedure>
          <step>
            <para>By Magnum to communicate with the cluster API endpoint</para>
          </step>
          <step>
            <para>By the cluster worker nodes to communicate with the master nodes</para>
          </step>
          <step>
            <para>By the end-user when they use the native client libraries to
                    interact with the cluster.  This applies to both a CLI or a program
                    that uses a client for the particular cluster.  Each client needs a
                    valid certificate to authenticate and communicate with a cluster.</para>
          </step>
        </procedure>
        <para>The first two cases are implemented internally by Magnum and are not
            exposed to the users, while the last case involves the users and is
            described in more details below.</para>
        <section>
          <title>Deploying a secure cluster</title>
          <para>Current TLS support is summarized below:</para>
          <informaltable>
            <tgroup cols="2">
              <colspec colname="c1" colwidth="12"/>
              <colspec colname="c2" colwidth="13"/>
              <thead>
                <row>
                  <entry>
                    <para>COE</para>
                  </entry>
                  <entry>
                    <para>TLS support</para>
                  </entry>
                </row>
              </thead>
              <tbody>
                <row>
                  <entry>
                    <para>Kubernetes</para>
                  </entry>
                  <entry>
                    <para>yes</para>
                  </entry>
                </row>
                <row>
                  <entry>
                    <para>Swarm</para>
                  </entry>
                  <entry>
                    <para>yes</para>
                  </entry>
                </row>
                <row>
                  <entry>
                    <para>Mesos</para>
                  </entry>
                  <entry>
                    <para>no</para>
                  </entry>
                </row>
              </tbody>
            </tgroup>
          </informaltable>
          <para>For cluster type with TLS support, e.g. Kubernetes and Swarm, TLS is
                enabled by default.  To disable TLS in Magnum, you can specify the
                parameter ‘–tls-disabled’ in the ClusterTemplate.  Please note it is not
                recommended to disable TLS due to security reasons.</para>
          <para>In the following example, Kubernetes is used to illustrate a secure
                cluster, but the steps are similar for other cluster types that have TLS
                support.</para>
          <para>First, create a ClusterTemplate; by default TLS is enabled in
                Magnum, therefore it does not need to be specified via a parameter:</para>
          <screen>magnum cluster-template-create secure-kubernetes \
                           --keypair default \
                           --external-network public \
                           --image fedora-atomic-latest \
                           --dns-nameserver 8.8.8.8 \
                           --flavor m1.small \
                           --docker-volume-size 3 \
                           --coe kubernetes \
                           --network-driver flannel

+-----------------------+--------------------------------------+
| Property              | Value                                |
+-----------------------+--------------------------------------+
| insecure_registry     | None                                 |
| http_proxy            | None                                 |
| updated_at            | None                                 |
| master_flavor_id      | None                                 |
| uuid                  | 5519b24a-621c-413c-832f-c30424528b31 |
| no_proxy              | None                                 |
| https_proxy           | None                                 |
| tls_disabled          | False                                |
| keypair_id            | time4funkey                          |
| public                | False                                |
| labels                | {}                                   |
| docker_volume_size    | 5                                    |
| server_type           | vm                                   |
| external_network_id   | public                               |
| cluster_distro        | fedora-atomic                        |
| image_id              | fedora-atomic-latest                 |
| volume_driver         | None                                 |
| registry_enabled      | False                                |
| docker_storage_driver | devicemapper                         |
| apiserver_port        | None                                 |
| name                  | secure-kubernetes                    |
| created_at            | 2016-07-25T23:09:50+00:00            |
| network_driver        | flannel                              |
| fixed_network         | None                                 |
| coe                   | kubernetes                           |
| flavor_id             | m1.small                             |
| dns_nameserver        | 8.8.8.8                              |
+-----------------------+--------------------------------------+</screen>
          <para>Now create a cluster. Use the ClusterTemplate name as a template for cluster
                creation:</para>
          <screen>magnum cluster-create secure-k8s-cluster \
                      --cluster-template secure-kubernetes \
                      --node-count 1

+--------------------+------------------------------------------------------------+
| Property           | Value                                                      |
+--------------------+------------------------------------------------------------+
| status             | CREATE_IN_PROGRESS                                         |
| uuid               | 3968ffd5-678d-4555-9737-35f191340fda                       |
| stack_id           | c96b66dd-2109-4ae2-b510-b3428f1e8761                       |
| status_reason      | None                                                       |
| created_at         | 2016-07-25T23:14:06+00:00                                  |
| updated_at         | None                                                       |
| create_timeout     | 0                                                          |
| api_address        | None                                                       |
| coe_version        | -                                                          |
| cluster_template_id| 5519b24a-621c-413c-832f-c30424528b31                       |
| master_addresses   | None                                                       |
| node_count         | 1                                                          |
| node_addresses     | None                                                       |
| master_count       | 1                                                          |
| container_version  | -                                                          |
| discovery_url      | https://discovery.etcd.io/ba52a8178e7364d43a323ee4387cf28e |
| name               | secure-k8s-cluster                                          |
+--------------------+------------------------------------------------------------+</screen>
          <para>Now run cluster-show command to get the details of the cluster and verify that
                the api_address is ‘https’:</para>
          <screen>magnum cluster-show secure-k8scluster
+--------------------+------------------------------------------------------------+
| Property           | Value                                                      |
+--------------------+------------------------------------------------------------+
| status             | CREATE_COMPLETE                                            |
| uuid               | 04952c60-a338-437f-a7e7-d016d1d00e65                       |
| stack_id           | b7bf72ce-b08e-4768-8201-e63a99346898                       |
| status_reason      | Stack CREATE completed successfully                        |
| created_at         | 2016-07-25T23:14:06+00:00                                  |
| updated_at         | 2016-07-25T23:14:10+00:00                                  |
| create_timeout     | 60                                                         |
| coe_version        | v1.2.0                                                     |
| api_address        | https://192.168.19.86:6443                                 |
| cluster_template_id| da2825a0-6d09-4208-b39e-b2db666f1118                       |
| master_addresses   | ['192.168.19.87']                                          |
| node_count         | 1                                                          |
| node_addresses     | ['192.168.19.88']                                          |
| master_count       | 1                                                          |
| container_version  | 1.9.1                                                      |
| discovery_url      | https://discovery.etcd.io/3b7fb09733429d16679484673ba3bfd5 |
| name               | secure-k8s-cluster                                          |
+--------------------+------------------------------------------------------------+</screen>
          <para>You can see the api_address contains https in the URL, showing that
                the Kubernetes services are configured securely with SSL certificates
                and now any communication to kube-apiserver will be over https.</para>
        </section>
        <section>
          <title>Interfacing with a secure cluster</title>
          <para>To communicate with the API endpoint of a secure cluster, you will need so
                supply 3 SSL artifacts:</para>
          <procedure>
            <step>
              <para>Your client key</para>
            </step>
            <step>
              <para>A certificate for your client key that has been signed by a
                        Certificate Authority (CA)</para>
            </step>
            <step>
              <para>The certificate of the CA</para>
            </step>
          </procedure>
          <para>There are two ways to obtain these 3 artifacts.</para>
          <section>
            <title>Automated</title>
            <para>Magnum provides the command ‘cluster-config’ to help the user in setting
                    up the environment and artifacts for TLS, for example:</para>
            <screen>magnum cluster-config swarm-cluster --dir myclusterconfig</screen>
            <para>This will display the necessary environment variables, which you
                    can add to your environment:</para>
            <screen>export DOCKER_HOST=tcp://172.24.4.5:2376
export DOCKER_CERT_PATH=myclusterconfig
export DOCKER_TLS_VERIFY=True</screen>
            <para>And the artifacts are placed in the directory specified:</para>
            <screen>ca.pem
cert.pem
key.pem</screen>
            <para>You can now use the native client to interact with the COE.
                    The variables and artifacts are unique to the cluster.</para>
            <para>The parameters for ‘bay-config’ are as follows:</para>
            <variablelist>
              <varlistentry>
                <term>–dir &lt;dirname&gt;</term>
                <listitem>
                  <para>Directory to save the certificate and config files.</para>
                </listitem>
              </varlistentry>
            </variablelist>
            <variablelist>
              <varlistentry>
                <term>
                  <option>--force</option>
                </term>
                <listitem>
                  <para>Overwrite existing files in the directory specified.</para>
                </listitem>
              </varlistentry>
            </variablelist>
          </section>
          <section>
            <title>Manual</title>
            <para>You can create the key and certificates manually using the following steps.</para>
            <variablelist>
              <varlistentry>
                <term>Client Key</term>
                <listitem>
                  <para>Your personal private key is essentially a cryptographically generated
                                string of bytes. It should be protected in the same manner as a
                                password. To generate an RSA key, you can use the ‘genrsa’ command of
                                the ‘openssl’ tool:</para>
                  <screen>openssl genrsa -out key.pem 4096</screen>
                  <para>This command generates a 4096 byte RSA key at key.pem.</para>
                </listitem>
              </varlistentry>
              <varlistentry>
                <term>Signed Certificate</term>
                <listitem>
                  <para>To authenticate your key, you need to have it signed by a CA.  First
                                generate the Certificate Signing Request (CSR).  The CSR will be
                                used by Magnum to generate a signed certificate that you will use to
                                communicate with the cluster.  To generate a CSR, openssl requires a
                                config file that specifies a few values.  Using the example template
                                below, you can fill in the ‘CN’ value with your name and save it as
                                client.conf:</para>
                  <screen>$ cat &gt; client.conf &lt;&lt; END
[req]
distinguished_name = req_distinguished_name
req_extensions     = req_ext
prompt = no
[req_distinguished_name]
CN = Your Name
[req_ext]
extendedKeyUsage = clientAuth
END</screen>
                  <para>Once you have client.conf, you can run the openssl ‘req’ command to
                                generate the CSR:</para>
                  <screen>openssl req -new -days 365 \
    -config client.conf \
    -key key.pem \
    -out client.csr</screen>
                  <para>Now that you have your client CSR, you can use the Magnum CLI to
                                send it off to Magnum to get it signed:</para>
                  <screen>magnum ca-sign --cluster secure-k8s-cluster --csr client.csr &gt; cert.pem</screen>
                </listitem>
              </varlistentry>
              <varlistentry>
                <term>Certificate Authority</term>
                <listitem>
                  <para>The final artifact you need to retrieve is the CA certificate for
                                the cluster. This is used by your native client to ensure you are only
                                communicating with hosts that Magnum set up:</para>
                  <screen>magnum ca-show --cluster secure-k8s-cluster &gt; ca.pem</screen>
                </listitem>
              </varlistentry>
              <varlistentry>
                <term>Rotate Certificate</term>
                <listitem>
                  <para>To rotate the CA certificate for a cluster and invalidate all user
                                certificates, you can use the following command:</para>
                  <screen>magnum ca-rotate --cluster secure-k8s-cluster</screen>
                </listitem>
              </varlistentry>
            </variablelist>
          </section>
        </section>
        <section>
          <title>User Examples</title>
          <para>Here are some examples for using the CLI on a secure Kubernetes and
                Swarm cluster.  You can perform all the TLS set up automatically by:</para>
          <screen>eval $(magnum cluster-config &lt;cluster-name&gt;)</screen>
          <para>Or you can perform the manual steps as described above and specify
                the TLS options on the CLI.  The SSL artifacts are assumed to be
                saved in local files as follows:</para>
          <screen>- key.pem: your SSL key
- cert.pem: signed certificate
- ca.pem: certificate for cluster CA</screen>
          <para>For Kubernetes, you need to get ‘kubectl’, a kubernetes CLI tool, to
                communicate with the cluster:</para>
          <screen>curl -O https://storage.googleapis.com/kubernetes-release/release/v1.2.0/bin/linux/amd64/kubectl
chmod +x kubectl
sudo mv kubectl /usr/local/bin/kubectl</screen>
          <para>Now let’s run some ‘kubectl’ commands to check the secure communication.
                If you used ‘cluster-config’, then you can simply run the ‘kubectl’ command
                without having to specify the TLS options since they have been defined
                in the environment:</para>
          <screen>kubectl version
Client Version: version.Info{Major:"1", Minor:"0", GitVersion:"v1.2.0", GitCommit:"cffae0523cfa80ddf917aba69f08508b91f603d5", GitTreeState:"clean"}
Server Version: version.Info{Major:"1", Minor:"0", GitVersion:"v1.2.0", GitCommit:"cffae0523cfa80ddf917aba69f08508b91f603d5", GitTreeState:"clean"}</screen>
          <para>You can specify the TLS options manually as follows:</para>
          <screen>KUBERNETES_URL=$(magnum cluster-show secure-k8s-cluster |
                 awk '/ api_address /{print $4}')
kubectl version --certificate-authority=ca.pem \
                --client-key=key.pem \
                --client-certificate=cert.pem -s $KUBERNETES_URL

kubectl create -f redis-master.yaml --certificate-authority=ca.pem \
                                    --client-key=key.pem \
                                    --client-certificate=cert.pem -s $KUBERNETES_URL

pods/test2

kubectl get pods --certificate-authority=ca.pem \
                 --client-key=key.pem \
                 --client-certificate=cert.pem -s $KUBERNETES_URL
NAME           READY     STATUS    RESTARTS   AGE
redis-master   2/2       Running   0          1m</screen>
          <para>Beside using the environment variables, you can also configure ‘kubectl’
                to remember the TLS options:</para>
          <screen>kubectl config set-cluster secure-k8s-cluster --server=${KUBERNETES_URL} \
    --certificate-authority=${PWD}/ca.pem
kubectl config set-credentials client --certificate-authority=${PWD}/ca.pem \
    --client-key=${PWD}/key.pem --client-certificate=${PWD}/cert.pem
kubectl config set-context secure-k8scluster --cluster=secure-k8scluster --user=client
kubectl config use-context secure-k8scluster</screen>
          <para>Then you can use ‘kubectl’ commands without the certificates:</para>
          <screen>kubectl get pods
NAME           READY     STATUS    RESTARTS   AGE
redis-master   2/2       Running   0          1m</screen>
          <para>Access to Kubernetes User Interface:</para>
          <screen>curl -L ${KUBERNETES_URL}/ui --cacert ca.pem --key key.pem \
    --cert cert.pem</screen>
          <para>You may also set up ‘kubectl’ proxy which will use your client
                certificates to allow you to browse to a local address to use the UI
                without installing a certificate in your browser:</para>
          <screen>kubectl proxy --api-prefix=/ --certificate-authority=ca.pem --client-key=key.pem \
              --client-certificate=cert.pem -s $KUBERNETES_URL</screen>
          <para>You can then open <link xlink:href="http://localhost:8001/ui"/> in your browser.</para>
          <para>The examples for Docker are similar.  With ‘cluster-config’ set up,
                you can just run docker commands without TLS options.  To specify the
                TLS options manually:</para>
          <screen>docker -H tcp://192.168.19.86:2376 --tlsverify \
       --tlscacert ca.pem \
       --tlskey key.pem \
       --tlscert cert.pem \
       info</screen>
        </section>
        <section>
          <title>Storing the certificates</title>
          <para>Magnum generates and maintains a certificate for each cluster so that it
                can also communicate securely with the cluster.  As a result, it is
                necessary to store the certificates in a secure manner.  Magnum
                provides the following methods for storing the certificates and this
                is configured in /etc/magnum/magnum.conf in the section [certificates]
                with the parameter ‘cert_manager_type’.</para>
          <procedure>
            <step>
              <para>Barbican:
                        Barbican is a service in OpenStack for storing secrets.  It is used
                        by Magnum to store the certificates when cert_manager_type is
                        configured as:</para>
              <screen>cert_manager_type = barbican</screen>
              <para>This is the recommended configuration for a production environment.
                        Magnum will interface with Barbican to store and retrieve
                        certificates, delegating the task of securing the certificates to
                        Barbican.</para>
            </step>
            <step>
              <para>Magnum database:
                        In some cases, a user may want an alternative to storing the
                        certificates that does not require Barbican.  This can be a
                        development environment, or a private cloud that has been secured
                        by other means.  Magnum can store the certificates in its own
                        database; this is done with the configuration:</para>
              <screen>cert_manager_type = x509keypair</screen>
              <para>This storage mode is only as secure as the controller server that
                        hosts the database for the OpenStack services.</para>
            </step>
            <step>
              <para>Local store:
                        As another alternative that does not require Barbican, Magnum can
                        simply store the certificates on the local host filesystem where the
                        conductor is running, using the configuration:</para>
              <screen>cert_manager_type = local</screen>
              <para>Note that this mode is only supported when there is a single Magnum
                        conductor running since the certificates are stored locally.  The
                        ‘local’ mode is not recommended for a production environment.</para>
            </step>
          </procedure>
          <para>For the nodes, the certificates for communicating with the masters are
                stored locally and the nodes are assumed to be secured.</para>
        </section>
      </section>
      <section xml:base="user/index#networking">
        <title>Networking</title>
        <para>There are two components that make up the networking in a cluster.</para>
        <procedure>
          <step>
            <para>The Neutron infrastructure for the cluster: this includes the
                    private network, subnet, ports, routers, load balancers, etc.</para>
          </step>
          <step>
            <para>The networking model presented to the containers: this is what the
                    containers see in communicating with each other and to the external
                    world. Typically this consists of a driver deployed on each node.</para>
          </step>
        </procedure>
        <para>The two components are deployed and managed separately.  The Neutron
            infrastructure is the integration with OpenStack; therefore, it
            is stable and more or less similar across different COE
            types.  The networking model, on the other hand, is specific to the
            COE type and is still under active development in the various
            COE communities, for example,
            <link xlink:href="https://github.com/docker/libnetwork">Docker libnetwork</link> and
            <link xlink:href="https://github.com/kubernetes/kubernetes/blob/release-1.1/docs/design/networking.md">Kubernetes Container Networking</link>.
            As a result, the implementation for the networking models is evolving and
            new models are likely to be introduced in the future.</para>
        <para>For the Neutron infrastructure, the following configuration can
            be set in the ClusterTemplate:</para>
        <variablelist>
          <varlistentry>
            <term>external-network</term>
            <listitem>
              <para>The external Neutron network ID to connect to this cluster. This
                        is used to connect the cluster to the external internet, allowing
                        the nodes in the cluster to access external URL for discovery, image
                        download, etc.  If not specified, the default value is “public” and this
                        is valid for a typical devstack.</para>
            </listitem>
          </varlistentry>
          <varlistentry>
            <term>fixed-network</term>
            <listitem>
              <para>The Neutron network to use as the private network for the cluster nodes.
                        If not specified, a new Neutron private network will be created.</para>
            </listitem>
          </varlistentry>
          <varlistentry>
            <term>dns-nameserver</term>
            <listitem>
              <para>The DNS nameserver to use for this cluster.  This is an IP address for
                        the server and it is used to configure the Neutron subnet of the
                        cluster (dns_nameservers).  If not specified, the default DNS is
                        8.8.8.8, the publicly available DNS.</para>
            </listitem>
          </varlistentry>
          <varlistentry>
            <term>http-proxy, https-proxy, no-proxy</term>
            <listitem>
              <para>The proxy for the nodes in the cluster, to be used when the cluster is
                        behind a firewall and containers cannot access URL’s on the external
                        internet directly.  For the parameter http-proxy and https-proxy, the
                        value to provide is a URL and it will be set in the environment
                        variable HTTP_PROXY and HTTPS_PROXY respectively in the nodes.  For
                        the parameter no-proxy, the value to provide is an IP or list of IP’s
                        separated by comma.  Likewise, the value will be set in the
                        environment variable NO_PROXY in the nodes.</para>
            </listitem>
          </varlistentry>
        </variablelist>
        <para>For the networking model to the container, the following configuration
            can be set in the ClusterTemplate:</para>
        <variablelist>
          <varlistentry>
            <term>network-driver</term>
            <listitem>
              <para>The network driver name for instantiating container networks.
                        Currently, the following network drivers are supported:</para>
              <informaltable>
                <tgroup cols="4">
                  <colspec colname="c1" colwidth="8"/>
                  <colspec colname="c2" colwidth="13"/>
                  <colspec colname="c3" colwidth="11"/>
                  <colspec colname="c4" colwidth="13"/>
                  <thead>
                    <row>
                      <entry>
                        <para>Driver</para>
                      </entry>
                      <entry>
                        <para>Kubernetes</para>
                      </entry>
                      <entry>
                        <para>Swarm</para>
                      </entry>
                      <entry>
                        <para>Mesos</para>
                      </entry>
                    </row>
                  </thead>
                  <tbody>
                    <row>
                      <entry>
                        <para>Flannel</para>
                      </entry>
                      <entry>
                        <para>supported</para>
                      </entry>
                      <entry>
                        <para>supported</para>
                      </entry>
                      <entry>
                        <para>unsupported</para>
                      </entry>
                    </row>
                    <row>
                      <entry>
                        <para>Docker</para>
                      </entry>
                      <entry>
                        <para>unsupported</para>
                      </entry>
                      <entry>
                        <para>supported</para>
                      </entry>
                      <entry>
                        <para>supported</para>
                      </entry>
                    </row>
                  </tbody>
                </tgroup>
              </informaltable>
              <para>If not specified, the default driver is Flannel for Kubernetes, and
                        Docker for Swarm and Mesos.</para>
            </listitem>
          </varlistentry>
        </variablelist>
        <para>Particular network driver may require its own set of parameters for
            configuration, and these parameters are specified through the labels
            in the ClusterTemplate.  Labels are arbitrary key=value pairs.</para>
        <para>When Flannel is specified as the network driver, the following
            optional labels can be added:</para>
        <variablelist>
          <varlistentry>
            <term/>
            <listitem>
              <para>IPv4 network in CIDR format to use for the entire Flannel network.
                        If not specified, the default is 10.100.0.0/16.</para>
            </listitem>
          </varlistentry>
          <varlistentry>
            <term/>
            <listitem>
              <para>The size of the subnet allocated to each host. If not specified, the
                        default is 24.</para>
            </listitem>
          </varlistentry>
          <varlistentry>
            <term/>
            <listitem>
              <para>The type of backend for Flannel.  Possible values are <emphasis>udp, vxlan,
                            host-gw</emphasis>.  If not specified, the default is <emphasis>udp</emphasis>.  Selecting the
                        best backend depends on your networking.  Generally, <emphasis>udp</emphasis> is
                        the most generally supported backend since there is little
                        requirement on the network, but it typically offers the lowest
                        performance.  The <emphasis>vxlan</emphasis> backend performs better, but requires
                        vxlan support in the kernel so the image used to provision the
                        nodes needs to include this support.  The <emphasis>host-gw</emphasis> backend offers
                        the best performance since it does not actually encapsulate
                        messages, but it requires all the nodes to be on the same L2
                        network.  The private Neutron network that Magnum creates does
                        meet this requirement;  therefore if the parameter <emphasis>fixed_network</emphasis>
                        is not specified in the ClusterTemplate, <emphasis>host-gw</emphasis> is the best choice for
                        the Flannel backend.</para>
            </listitem>
          </varlistentry>
        </variablelist>
      </section>
      <section xml:base="user/index#high-availability">
        <title>High Availability</title>
        <para>
          <emphasis>To be filled in</emphasis>
        </para>
      </section>
      <section xml:base="user/index#scaling">
        <title>Scaling</title>
        <section>
          <title>Performance tuning for periodic task</title>
          <para>Magnum’s periodic task performs a <literal>stack-get</literal> operation on the Heat stack
                underlying each of its clusters. If you have a large amount of clusters this
                can create considerable load on the Heat API. To reduce that load you can
                configure Magnum to perform one global <literal>stack-list</literal> per periodic task instead
                of one per cluster. This is disabled by default, both from the Heat and Magnum
                side since it causes a security issue, though: any user in any tenant holding
                the <literal>admin</literal> role can perform a global <literal>stack-list</literal> operation if Heat is
                configured to allow it for Magnum. If you want to enable it nonetheless,
                proceed as follows:</para>
          <procedure>
            <step>
              <para>Set <literal>periodic_global_stack_list</literal> in magnum.conf to <literal>True</literal>
                        (<literal>False</literal> by default).</para>
            </step>
            <step>
              <para>Update heat policy to allow magnum list stacks. To this end, edit your heat
                        policy file, usually etc/heat/policy.json``:</para>
              <screen language="ini">...
stacks:global_index: "rule:context_is_admin",</screen>
              <para>Now restart heat.</para>
            </step>
          </procedure>
        </section>
        <section>
          <title>Containers and nodes</title>
          <para>Scaling containers and nodes refers to increasing or decreasing
                allocated system resources.  Scaling is a broad topic and involves
                many dimensions.  In the context of Magnum in this guide, we consider
                the following issues:</para>
          <itemizedlist>
            <listitem>
              <para>Scaling containers and scaling cluster nodes (infrastructure)</para>
            </listitem>
            <listitem>
              <para>Manual and automatic scaling</para>
            </listitem>
          </itemizedlist>
          <para>Since this is an active area of development, a complete solution
                covering all issues does not exist yet, but partial solutions are
                emerging.</para>
          <para>Scaling containers involves managing the number of instances of the
                container by replicating or deleting instances.  This can be used to
                respond to change in the workload being supported by the application;
                in this case, it is typically driven by certain metrics relevant to the
                application such as response time, etc.  Other use cases include
                rolling upgrade, where a new version of a service can gradually be
                scaled up while the older version is gradually scaled down.  Scaling
                containers is supported at the COE level and is specific to each COE
                as well as the version of the COE.  You will need to refer to the
                documentation for the proper COE version for full details, but
                following are some pointers for reference.</para>
          <para>For Kubernetes, pods are scaled manually by setting the count in the
                replication controller.  Kubernetes version 1.3 and later also
                supports <link xlink:href="http://blog.kubernetes.io/2016/07/autoscaling-in-kubernetes.html">autoscaling</link>.
                For Docker, the tool ‘Docker Compose’ provides the command
                <link xlink:href="https://docs.docker.com/compose/reference/scale/">docker-compose scale</link> which lets you
                manually set the number of instances of a container.  For Swarm
                version 1.12 and later, services can also be scaled manually through
                the command <link xlink:href="https://docs.docker.com/engine/swarm/swarm-tutorial/scale-service/">docker service scale</link>.
                Automatic scaling for Swarm is not yet available.  Mesos manages the
                resources and does not support scaling directly; instead, this is
                provided by frameworks running within Mesos.  With the Marathon
                framework currently supported in the Mesos cluster, you can use the
                <link xlink:href="https://mesosphere.github.io/marathon/docs/application-basics.html">scale operation</link>
                on the Marathon UI or through a REST API call to manually set the
                attribute ‘instance’ for a container.</para>
          <para>Scaling the cluster nodes involves managing the number of nodes in the
                cluster by adding more nodes or removing nodes.  There is no direct
                correlation between the number of nodes and the number of containers
                that can be hosted since the resources consumed (memory, CPU, etc)
                depend on the containers.  However, if a certain resource is exhausted
                in the cluster, adding more nodes would add more resources for hosting
                more containers.  As part of the infrastructure management, Magnum
                supports manual scaling through the attribute ‘node_count’ in the
                cluster, so you can scale the cluster simply by changing this
                attribute:</para>
          <screen>magnum cluster-update mycluster replace node_count=2</screen>
          <para><!-- Refer to the section <xref linkend="scale"/> lifecycle operation for more details.--></para>
          <para>Adding nodes to a cluster is straightforward: Magnum deploys
                additional VMs or baremetal servers through the heat templates and
                invokes the COE-specific mechanism for registering the new nodes to
                update the available resources in the cluster.  Afterward, it is up to
                the COE or user to re-balance the workload by launching new container
                instances or re-launching dead instances on the new nodes.</para>
          <para>Removing nodes from a cluster requires some more care to ensure
                continuous operation of the containers since the nodes being removed
                may be actively hosting some containers.  Magnum performs a simple
                heuristic that is specific to the COE to find the best node candidates
                for removal, as follows:</para>
          <variablelist>
            <varlistentry>
              <term>Kubernetes</term>
              <listitem>
                <para>Magnum scans the pods in the namespace ‘Default’ to determine the
                            nodes that are <emphasis>not</emphasis> hosting any (empty nodes).  If the number of
                            nodes to be removed is equal or less than the number of these empty
                            nodes, these nodes will be removed from the cluster.  If the number
                            of nodes to be removed is larger than the number of empty nodes, a
                            warning message will be sent to the Magnum log and the empty nodes
                            along with additional nodes will be removed from the cluster.  The
                            additional nodes are selected randomly and the pods running on them
                            will be deleted without warning.  For this reason, a good practice
                            is to manage the pods through the replication controller so that the
                            deleted pods will be relaunched elsewhere in the cluster.  Note also
                            that even when only the empty nodes are removed, there is no
                            guarantee that no pod will be deleted because there is no locking to
                            ensure that Kubernetes will not launch new pods on these nodes after
                            Magnum has scanned the pods.</para>
              </listitem>
            </varlistentry>
            <varlistentry>
              <term>Swarm</term>
              <listitem>
                <para>No node selection heuristic is currently supported.  If you decrease
                            the node_count, a node will be chosen by magnum without
                            consideration of what containers are running on the selected node.</para>
              </listitem>
            </varlistentry>
            <varlistentry>
              <term>Mesos</term>
              <listitem>
                <para>Magnum scans the running tasks on Marathon server to determine the
                            nodes on which there is <emphasis>no</emphasis> task running (empty nodes). If the
                            number of nodes to be removed is equal or less than the number of
                            these empty nodes, these nodes will be removed from the cluster.
                            If the number of nodes to be removed is larger than the number of
                            empty nodes, a warning message will be sent to the Magnum log and
                            the empty nodes along with additional nodes will be removed from the
                            cluster. The additional nodes are selected randomly and the containers
                            running on them will be deleted without warning. Note that even when
                            only the empty nodes are removed, there is no guarantee that no
                            container will be deleted because there is no locking to ensure that
                            Mesos will not launch new containers on these nodes after Magnum
                            has scanned the tasks.</para>
              </listitem>
            </varlistentry>
          </variablelist>
          <para>Currently, scaling containers and scaling cluster nodes are handled
                separately, but in many use cases, there are interactions between the
                two operations.  For instance, scaling up the containers may exhaust
                the available resources in the cluster, thereby requiring scaling up
                the cluster nodes as well.  Many complex issues are involved in
                managing this interaction.  A presentation at the OpenStack Tokyo
                Summit 2015 covered some of these issues along with some early
                proposals, <link xlink:href="https://www.openstack.org/summit/tokyo-2015/videos/presentation/exploring-magnum-and-senlin-integration-for-autoscaling-containers">Exploring Magnum and Senlin integration for autoscaling
                    containers</link>.
                This remains an active area of discussion and research.</para>
        </section>
      </section>
      <section xml:base="user/index#storage">
        <title>Storage</title>
        <para>Currently Cinder provides the block storage to the containers, and the
            storage is made available in two ways: as ephemeral storage and as
            persistent storage.</para>
        <section>
          <title>Ephemeral storage</title>
          <para>The filesystem for the container consists of multiple layers from the
                image and a top layer that holds the modification made by the
                container.  This top layer requires storage space and the storage is
                configured in the Docker daemon through a number of storage options.
                When the container is removed, the storage allocated to the particular
                container is also deleted.</para>
          <para>Magnum can manage the containers’ filesystem in two ways, storing them
                on the local disk of the compute instances or in a separate Cinder block
                volume for each node in the cluster, mounts it to the node and
                configures it to be used as ephemeral storage.  Users can specify the
                size of the Cinder volume with the ClusterTemplate attribute
                ‘docker-volume-size’. Currently the block size is fixed at cluster
                creation time, but future lifecycle operations may allow modifying the
                block size during the life of the cluster.</para>
          <variablelist>
            <varlistentry>
              <term/>
              <listitem>
                <para>For drivers that support additional volumes for container storage, a
                            label named ‘docker_volume_type’ is exposed so that users can select
                            different cinder volume types for their volumes. The default volume
                            <emphasis>must</emphasis> be set in ‘default_docker_volume_type’ in the ‘cinder’ section
                            of magnum.conf, an obvious value is the default volume type set in
                            cinder.conf of your cinder deployment . Please note, that
                            docker_volume_type refers to a cinder volume type and it is unrelated
                            to docker or kubernetes volumes.</para>
              </listitem>
            </varlistentry>
          </variablelist>
          <para>Both local disk and the Cinder block storage can be used with a number
                of Docker storage drivers available.</para>
          <itemizedlist>
            <listitem>
              <para>‘devicemapper’: When used with a dedicated Cinder volume it is
                        configured using direct-lvm and offers very good performance. If it’s
                        used with the compute instance’s local disk uses a loopback device
                        offering poor performance and it’s not recommended for production
                        environments. Using the ‘devicemapper’ driver does allow the use of
                        SELinux.</para>
            </listitem>
            <listitem>
              <para>‘overlay’ When used with a dedicated Cinder volume offers as good
                        or better performance than devicemapper. If used on the local disk of
                        the compute instance (especially with high IOPS drives) you can get
                        significant performance gains. However, for kernel versions less than
                        4.9, SELinux must be disabled inside the containers resulting in worse
                        container isolation, although it still runs in enforcing mode on the
                        cluster compute instances.</para>
            </listitem>
          </itemizedlist>
        </section>
        <section>
          <title>Persistent storage</title>
          <para>In some use cases, data read/written by a container needs to persist
                so that it can be accessed later.  To persist the data, a Cinder
                volume with a filesystem on it can be mounted on a host and be made
                available to the container, then be unmounted when the container exits.</para>
          <para>Docker provides the ‘volume’ feature for this purpose: the user
                invokes the ‘volume create’ command, specifying a particular volume
                driver to perform the actual work.  Then this volume can be mounted
                when a container is created.  A number of third-party volume drivers
                support OpenStack Cinder as the backend, for example Rexray and
                Flocker.  Magnum currently supports Rexray as the volume driver for
                Swarm and Mesos.  Other drivers are being considered.</para>
          <para>Kubernetes allows a previously created Cinder block to be mounted to
                a pod and this is done by specifying the block ID in the pod YAML file.
                When the pod is scheduled on a node, Kubernetes will interface with
                Cinder to request the volume to be mounted on this node, then
                Kubernetes will launch the Docker container with the proper options to
                make the filesystem on the Cinder volume accessible to the container
                in the pod.  When the pod exits, Kubernetes will again send a request
                to Cinder to unmount the volume’s filesystem, making it available to be
                mounted on other nodes.</para>
          <para>Magnum supports these features to use Cinder as persistent storage
                using the ClusterTemplate attribute ‘volume-driver’ and the support matrix
                for the COE types is summarized as follows:</para>
          <informaltable>
            <tgroup cols="4">
              <colspec colname="c1" colwidth="8"/>
              <colspec colname="c2" colwidth="13"/>
              <colspec colname="c3" colwidth="13"/>
              <colspec colname="c4" colwidth="13"/>
              <thead>
                <row>
                  <entry>
                    <para>Driver</para>
                  </entry>
                  <entry>
                    <para>Kubernetes</para>
                  </entry>
                  <entry>
                    <para>Swarm</para>
                  </entry>
                  <entry>
                    <para>Mesos</para>
                  </entry>
                </row>
              </thead>
              <tbody>
                <row>
                  <entry>
                    <para>cinder</para>
                  </entry>
                  <entry>
                    <para>supported</para>
                  </entry>
                  <entry>
                    <para>unsupported</para>
                  </entry>
                  <entry>
                    <para>unsupported</para>
                  </entry>
                </row>
                <row>
                  <entry>
                    <para>rexray</para>
                  </entry>
                  <entry>
                    <para>unsupported</para>
                  </entry>
                  <entry>
                    <para>supported</para>
                  </entry>
                  <entry>
                    <para>supported</para>
                  </entry>
                </row>
              </tbody>
            </tgroup>
          </informaltable>
          <para>Following are some examples for using Cinder as persistent storage.</para>
          <section>
            <title>Using Cinder in Kubernetes</title>
            <para><emphasis role="bold">NOTE:</emphasis> This feature requires Kubernetes version 1.5.0 or above.
                    The public Fedora image from Atomic currently meets this requirement.</para>
            <procedure>
              <step>
                <para>Create the ClusterTemplate.</para>
                <para>Specify ‘cinder’ as the volume-driver for Kubernetes:</para>
                <screen>magnum cluster-template-create k8s-cluster-template \
                           --image fedora-23-atomic-7 \
                           --keypair testkey \
                           --external-network public \
                           --dns-nameserver 8.8.8.8 \
                           --flavor m1.small \
                           --docker-volume-size 5 \
                           --network-driver flannel \
                           --coe kubernetes \
                           --volume-driver cinder</screen>
              </step>
              <step>
                <para>Create the cluster:</para>
                <screen>magnum cluster-create k8s-cluster \
                      --cluster-template k8s-cluster-template \
                      --node-count 1</screen>
              </step>
            </procedure>
            <para>Kubernetes is now ready to use Cinder for persistent storage.
                    Following is an example illustrating how Cinder is used in a pod.</para>
            <procedure>
              <step>
                <para>Create the cinder volume:</para>
                <screen>cinder create --display-name=test-repo 1

XML:ID=$(cinder create --display-name=test-repo 1 | awk -F'|' '$2~/^[[:space:]]*id/ {print $3}')</screen>
                <para>The command will generate the volume with a ID. The volume ID will be
                            specified in Step 2.</para>
              </step>
              <step>
                <para>Create a pod in this cluster and mount this cinder volume to the pod.
                            Create a file (e.g nginx-cinder.yaml) describing the pod:</para>
                <screen>cat &gt; nginx-cinder.yaml &lt;&lt; END
apiVersion: v1
kind: Pod
metadata:
  name: aws-web
spec:
  containers:
    - name: web
      image: nginx
      ports:
        - name: web
          containerPort: 80
          hostPort: 8081
          protocol: TCP
      volumeMounts:
        - name: html-volume
          mountPath: "/usr/share/nginx/html"
  volumes:
    - name: html-volume
      cinder:
        # Enter the volume ID below
        volumeID: $ID
        fsType: ext4
END</screen>
              </step>
            </procedure>
            <para><emphasis role="bold">NOTE:</emphasis> The Cinder volume ID needs to be configured in the YAML file
                    so the existing Cinder volume can be mounted in a pod by specifying
                    the volume ID in the pod manifest as follows:</para>
            <screen>volumes:
- name: html-volume
  cinder:
    volumeID: $ID
    fsType: ext4</screen>
            <procedure>
              <step>
                <para>Create the pod by the normal Kubernetes interface:</para>
                <screen>kubectl create -f nginx-cinder.yaml</screen>
              </step>
            </procedure>
            <para>You can start a shell in the container to check that the mountPath exists,
                    and on an OpenStack client you can run the command ‘cinder list’ to verify
                    that the cinder volume status is ‘in-use’.</para>
          </section>
          <section>
            <title>Using Cinder in Swarm</title>
            <para>
              <emphasis>To be filled in</emphasis>
            </para>
          </section>
          <section>
            <title>Using Cinder in Mesos</title>
            <procedure>
              <step>
                <para>Create the ClusterTemplate.</para>
                <para>Specify ‘rexray’ as the volume-driver for Mesos.  As an option, you
                            can specify in a label the attributes ‘rexray_preempt’ to enable
                            any host to take control of a volume regardless if other
                            hosts are using the volume. If this is set to false, the driver
                            will ensure data safety by locking the volume:</para>
                <screen>magnum cluster-template-create mesos-cluster-template \
                           --image ubuntu-mesos \
                           --keypair testkey \
                           --external-network public \
                           --dns-nameserver 8.8.8.8 \
                           --master-flavor m1.magnum \
                           --docker-volume-size 4 \
                           --tls-disabled \
                           --flavor m1.magnum \
                           --coe mesos \
                           --volume-driver rexray \
                           --labels rexray-preempt=true</screen>
              </step>
              <step>
                <para>Create the Mesos cluster:</para>
                <screen>magnum cluster-create mesos-cluster \
                      --cluster-template mesos-cluster-template \
                      --node-count 1</screen>
              </step>
              <step>
                <para>Create the cinder volume and configure this cluster:</para>
                <screen>cinder create --display-name=redisdata 1</screen>
                <para>Create the following file</para>
                <screen>cat &gt; mesos.json &lt;&lt; END
{
  "id": "redis",
  "container": {
    "docker": {
    "image": "redis",
    "network": "BRIDGE",
    "portMappings": [
      { "containerPort": 80, "hostPort": 0, "protocol": "tcp"}
    ],
    "parameters": [
       { "key": "volume-driver", "value": "rexray" },
       { "key": "volume", "value": "redisdata:/data" }
    ]
    }
 },
 "cpus": 0.2,
 "mem": 32.0,
 "instances": 1
}
END</screen>
              </step>
            </procedure>
            <para><emphasis role="bold">NOTE:</emphasis> When the Mesos cluster is created using this ClusterTemplate, the
                    Mesos cluster will be configured so that a filesystem on an existing cinder
                    volume can be mounted in a container by configuring the parameters to mount
                    the cinder volume in the JSON file</para>
            <screen>"parameters": [
   { "key": "volume-driver", "value": "rexray" },
   { "key": "volume", "value": "redisdata:/data" }
]</screen>
            <procedure>
              <step>
                <para>Create the container using Marathon REST API</para>
                <screen>MASTER_IP=$(magnum cluster-show mesos-cluster | awk '/ api_address /{print $4}')
curl -X POST -H "Content-Type: application/json" \
http://${MASTER_IP}:8080/v2/apps -d@mesos.json</screen>
              </step>
            </procedure>
            <para>You can log into the container to check that the mountPath exists, and
                    you can run the command ‘cinder list’ to verify that your cinder
                    volume status is ‘in-use’.</para>
          </section>
        </section>
      </section>
      <section xml:base="user/index#image-management">
        <title>Image Management</title>
        <para>When a COE is deployed, an image from Glance is used to boot the nodes
            in the cluster and then the software will be configured and started on
            the nodes to bring up the full cluster.  An image is based on a
            particular distro such as Fedora, Ubuntu, etc, and is prebuilt with
            the software specific to the COE such as Kubernetes, Swarm, Mesos.
            The image is tightly coupled with the following in Magnum:</para>
        <procedure>
          <step>
            <para>Heat templates to orchestrate the configuration.</para>
          </step>
          <step>
            <para>Template definition to map ClusterTemplate parameters to Heat
                    template parameters.</para>
          </step>
          <step>
            <para>Set of scripts to configure software.</para>
          </step>
        </procedure>
        <para>Collectively, they constitute the driver for a particular COE and a
            particular distro; therefore, developing a new image needs to be done
            in conjunction with developing these other components.  Image can be
            built by various methods such as diskimagebuilder, or in some case, a
            distro image can be used directly.  A number of drivers and the
            associated images is supported in Magnum as reference implementation.
            In this section, we focus mainly on the supported images.</para>
        <para>All images must include support for cloud-init and the heat software
            configuration utility:</para>
        <itemizedlist>
          <listitem>
            <para>os-collect-config</para>
          </listitem>
          <listitem>
            <para>os-refresh-config</para>
          </listitem>
          <listitem>
            <para>os-apply-config</para>
          </listitem>
          <listitem>
            <para>heat-config</para>
          </listitem>
          <listitem>
            <para>heat-config-script</para>
          </listitem>
        </itemizedlist>
        <para>Additional software are described as follows.</para>
        <section>
          <title>Kubernetes on Fedora Atomic</title>
          <para>This image can be downloaded from the <link xlink:href="https://alt.fedoraproject.org/pub/alt/atomic/stable/Cloud-Images/x86_64/Images/">public Atomic site</link>
                or can be built locally using diskimagebuilder.  Details can be found in the
                <link xlink:href="https://github.com/openstack/magnum/tree/master/magnum/elements/fedora-atomic">fedora-atomic element</link>
                The image currently has the following OS/software:</para>
          <informaltable>
            <tgroup cols="2">
              <colspec colname="c1" colwidth="13"/>
              <colspec colname="c2" colwidth="11"/>
              <thead>
                <row>
                  <entry>
                    <para>OS/software</para>
                  </entry>
                  <entry>
                    <para>version</para>
                  </entry>
                </row>
              </thead>
              <tbody>
                <row>
                  <entry>
                    <para>Fedora</para>
                  </entry>
                  <entry>
                    <para>26</para>
                  </entry>
                </row>
                <row>
                  <entry>
                    <para>Docker</para>
                  </entry>
                  <entry>
                    <para>1.13.1</para>
                  </entry>
                </row>
                <row>
                  <entry>
                    <para>Kubernetes</para>
                  </entry>
                  <entry>
                    <para>1.7.4</para>
                  </entry>
                </row>
                <row>
                  <entry>
                    <para>etcd</para>
                  </entry>
                  <entry>
                    <para>3.1.3</para>
                  </entry>
                </row>
                <row>
                  <entry>
                    <para>Flannel</para>
                  </entry>
                  <entry>
                    <para>0.7.0</para>
                  </entry>
                </row>
              </tbody>
            </tgroup>
          </informaltable>
          <para>The following software are managed as systemd services:</para>
          <itemizedlist>
            <listitem>
              <para>kube-apiserver</para>
            </listitem>
            <listitem>
              <para>kubelet</para>
            </listitem>
            <listitem>
              <para>etcd</para>
            </listitem>
            <listitem>
              <para>flannel (if specified as network driver)</para>
            </listitem>
            <listitem>
              <para>docker</para>
            </listitem>
          </itemizedlist>
          <para>The following software are managed as Docker containers:</para>
          <itemizedlist>
            <listitem>
              <para>kube-controller-manager</para>
            </listitem>
            <listitem>
              <para>kube-scheduler</para>
            </listitem>
            <listitem>
              <para>kube-proxy</para>
            </listitem>
          </itemizedlist>
          <para>The login for this image is <emphasis>fedora</emphasis>.</para>
        </section>
        <section>
          <title>Kubernetes on CoreOS</title>
          <para>CoreOS publishes a <link xlink:href="http://beta.release.core-os.net/amd64-usr/current/coreos_production_openstack_image.img.bz2">stock image</link>
                that is being used to deploy Kubernetes.
                This image has the following OS/software:</para>
          <informaltable>
            <tgroup cols="2">
              <colspec colname="c1" colwidth="13"/>
              <colspec colname="c2" colwidth="11"/>
              <thead>
                <row>
                  <entry>
                    <para>OS/software</para>
                  </entry>
                  <entry>
                    <para>version</para>
                  </entry>
                </row>
              </thead>
              <tbody>
                <row>
                  <entry>
                    <para>CoreOS</para>
                  </entry>
                  <entry>
                    <para>4.3.6</para>
                  </entry>
                </row>
                <row>
                  <entry>
                    <para>Docker</para>
                  </entry>
                  <entry>
                    <para>1.9.1</para>
                  </entry>
                </row>
                <row>
                  <entry>
                    <para>Kubernetes</para>
                  </entry>
                  <entry>
                    <para>1.0.6</para>
                  </entry>
                </row>
                <row>
                  <entry>
                    <para>etcd</para>
                  </entry>
                  <entry>
                    <para>2.2.3</para>
                  </entry>
                </row>
                <row>
                  <entry>
                    <para>Flannel</para>
                  </entry>
                  <entry>
                    <para>0.5.5</para>
                  </entry>
                </row>
              </tbody>
            </tgroup>
          </informaltable>
          <para>The following software are managed as systemd services:</para>
          <itemizedlist>
            <listitem>
              <para>kubelet</para>
            </listitem>
            <listitem>
              <para>flannel (if specified as network driver)</para>
            </listitem>
            <listitem>
              <para>docker</para>
            </listitem>
            <listitem>
              <para>etcd</para>
            </listitem>
          </itemizedlist>
          <para>The following software are managed as Docker containers:</para>
          <itemizedlist>
            <listitem>
              <para>kube-apiserver</para>
            </listitem>
            <listitem>
              <para>kube-controller-manager</para>
            </listitem>
            <listitem>
              <para>kube-scheduler</para>
            </listitem>
            <listitem>
              <para>kube-proxy</para>
            </listitem>
          </itemizedlist>
          <para>The login for this image is <emphasis>core</emphasis>.</para>
        </section>
        <section>
          <title>Kubernetes on Ironic</title>
          <para>This image is built manually using diskimagebuilder.  The scripts and
                instructions are included in <link xlink:href="https://github.com/openstack/magnum/tree/master/magnum/templates/kubernetes/elements">Magnum code repo</link>.
                Currently Ironic is not fully supported yet, therefore more details will be
                provided when this driver has been fully tested.</para>
        </section>
        <section>
          <title>Swarm on Fedora Atomic</title>
          <para>This image is the same as the image for Kubernetes on Fedora
	  Atomic described above.  The login for this image is <emphasis>fedora</emphasis>.</para>
        </section>
        <section>
          <title>Mesos on Ubuntu</title>
          <para>This image is built manually using diskimagebuilder.<!--  The instructions are
                provided in the section <xref linkend="diskimage-builder"/>.-->
                The Fedora site hosts the current image <link xlink:href="https://fedorapeople.org/groups/magnum/ubuntu-mesos-latest.qcow2">ubuntu-mesos-latest.qcow2</link>.</para>
          <informaltable>
            <tgroup cols="2">
              <colspec colname="c1" colwidth="13"/>
              <colspec colname="c2" colwidth="11"/>
              <thead>
                <row>
                  <entry>
                    <para>OS/software</para>
                  </entry>
                  <entry>
                    <para>version</para>
                  </entry>
                </row>
              </thead>
              <tbody>
                <row>
                  <entry>
                    <para>Ubuntu</para>
                  </entry>
                  <entry>
                    <para>14.04</para>
                  </entry>
                </row>
                <row>
                  <entry>
                    <para>Docker</para>
                  </entry>
                  <entry>
                    <para>1.8.1</para>
                  </entry>
                </row>
                <row>
                  <entry>
                    <para>Mesos</para>
                  </entry>
                  <entry>
                    <para>0.25.0</para>
                  </entry>
                </row>
                <row>
                  <entry>
                    <para>Marathon</para>
                  </entry>
                  <entry>
                    <para>0.11.1</para>
                  </entry>
                </row>
              </tbody>
            </tgroup>
          </informaltable>
        </section>
      </section>
      <section xml:base="user/index#notification">
        <title>Notification</title>
        <para>Magnum provides notifications about usage data so that 3rd party applications
            can use the data for auditing, billing, monitoring, or quota purposes. This
            document describes the current inclusions and exclusions for Magnum
            notifications.</para>
        <para>Magnum uses Cloud Auditing Data Federation (<link xlink:href="http://www.dmtf.org/standards/cadf">CADF</link>) Notification as its
            notification format for better support of auditing, details about CADF are
            documented below.</para>
        <section>
          <title>Auditing with CADF</title>
          <para>Magnum uses the <link xlink:href="http://docs.openstack.org/developer/pycadf">PyCADF</link> library to emit CADF notifications, these events
                adhere to the DMTF <link xlink:href="http://www.dmtf.org/standards/cadf">CADF</link> specification. This standard provides auditing
                capabilities for compliance with security, operational, and business processes
                and supports normalized and categorized event data for federation and
                aggregation.</para>
          <para>Below table describes the event model components and semantics for
                each component:</para>
          <informaltable>
            <tgroup cols="2">
              <colspec colname="c1" colwidth="17"/>
              <colspec colname="c2" colwidth="58"/>
              <thead>
                <row>
                  <entry>
                    <para>model component</para>
                  </entry>
                  <entry>
                    <para>CADF Definition</para>
                  </entry>
                </row>
              </thead>
              <tbody>
                <row>
                  <entry>
                    <para>OBSERVER</para>
                  </entry>
                  <entry>
                    <para>The RESOURCE that generates the CADF Event Record based
                                    on its observation (directly or indirectly) of the
                                    Actual Event.</para>
                  </entry>
                </row>
                <row>
                  <entry>
                    <para>INITIATOR</para>
                  </entry>
                  <entry>
                    <para>The RESOURCE that initiated, originated, or instigated
                                    the event’s ACTION, according to the OBSERVER.</para>
                  </entry>
                </row>
                <row>
                  <entry>
                    <para>ACTION</para>
                  </entry>
                  <entry>
                    <para>The operation or activity the INITIATOR has performed,
                                    has attempted to perform or has pending against the
                                    event’s TARGET, according to the OBSERVER.</para>
                  </entry>
                </row>
                <row>
                  <entry>
                    <para>TARGET</para>
                  </entry>
                  <entry>
                    <para>The RESOURCE against which the ACTION of a CADF Event
                                    Record was performed, attempted, or is pending,
                                    according to the OBSERVER.</para>
                  </entry>
                </row>
                <row>
                  <entry>
                    <para>OUTCOME</para>
                  </entry>
                  <entry>
                    <para>The result or status of the ACTION against the TARGET,
                                    according to the OBSERVER.</para>
                  </entry>
                </row>
              </tbody>
            </tgroup>
          </informaltable>
          <para>The <literal>payload</literal> portion of a CADF Notification is a CADF <literal>event</literal>, which
                is represented as a JSON dictionary. For example:</para>
          <screen language="javascript">{
    "typeURI": "http://schemas.dmtf.org/cloud/audit/1.0/event",
    "initiator": {
        "typeURI": "service/security/account/user",
        "host": {
            "agent": "curl/7.22.0(x86_64-pc-linux-gnu)",
            "address": "127.0.0.1"
        },
        "id": "&lt;initiator_id&gt;"
    },
    "target": {
        "typeURI": "&lt;target_uri&gt;",
        "id": "openstack:1c2fc591-facb-4479-a327-520dade1ea15"
    },
    "observer": {
        "typeURI": "service/security",
        "id": "openstack:3d4a50a9-2b59-438b-bf19-c231f9c7625a"
    },
    "eventType": "activity",
    "eventTime": "2014-02-14T01:20:47.932842+00:00",
    "action": "&lt;action&gt;",
    "outcome": "success",
    "id": "openstack:f5352d7b-bee6-4c22-8213-450e7b646e9f",
}</screen>
          <para>Where the following are defined:</para>
          <itemizedlist>
            <listitem>
              <para><literal>&lt;initiator_id&gt;</literal>: ID of the user that performed the operation</para>
            </listitem>
            <listitem>
              <para><literal>&lt;target_uri&gt;</literal>: CADF specific target URI, (i.e.:  data/security/project)</para>
            </listitem>
            <listitem>
              <para><literal>&lt;action&gt;</literal>: The action being performed, typically:
                        <literal>&lt;operation&gt;</literal>. <literal>&lt;resource_type&gt;</literal></para>
            </listitem>
          </itemizedlist>
          <para>Additionally there may be extra keys present depending on the operation being
                performed, these will be discussed below.</para>
          <para>Note, the <literal>eventType</literal> property of the CADF payload is different from the
                <literal>event_type</literal> property of a notifications. The former (<literal>eventType</literal>) is a
                CADF keyword which designates the type of event that is being measured, this
                can be: <literal>activity</literal>, <literal>monitor</literal> or <literal>control</literal>. Whereas the latter
                (<literal>event_type</literal>) is described in previous sections as:
                <literal>magnum.&lt;resource_type&gt;.&lt;operation&gt;</literal></para>
        </section>
        <section>
          <title>Supported Events</title>
          <para>The following table displays the corresponding relationship between resource
                types and operations. The bay type is deprecated and will be removed in a
                future version. Cluster is the new equivalent term.</para>
          <informaltable>
            <tgroup cols="3">
              <colspec colname="c1" colwidth="15"/>
              <colspec colname="c2" colwidth="28"/>
              <colspec colname="c3" colwidth="25"/>
              <thead>
                <row>
                  <entry>
                    <para>resource type</para>
                  </entry>
                  <entry>
                    <para>supported operations</para>
                  </entry>
                  <entry>
                    <para>typeURI</para>
                  </entry>
                </row>
              </thead>
              <tbody>
                <row>
                  <entry>
                    <para>bay</para>
                  </entry>
                  <entry>
                    <para>create, update, delete</para>
                  </entry>
                  <entry>
                    <para>service/magnum/bay</para>
                  </entry>
                </row>
                <row>
                  <entry>
                    <para>cluster</para>
                  </entry>
                  <entry>
                    <para>create, update, delete</para>
                  </entry>
                  <entry>
                    <para>service/magnum/cluster</para>
                  </entry>
                </row>
              </tbody>
            </tgroup>
          </informaltable>
        </section>
        <section>
          <title>Example Notification - Cluster Create</title>
          <para>The following is an example of a notification that is sent when a cluster is
                created. This example can be applied for any <literal>create</literal>, <literal>update</literal> or
                <literal>delete</literal> event that is seen in the table above. The <literal>&lt;action&gt;</literal> and
                <literal>typeURI</literal> fields will be change.</para>
          <screen language="javascript">{
    "event_type": "magnum.cluster.created",
    "message_id": "0156ee79-b35f-4cef-ac37-d4a85f231c69",
    "payload": {
        "typeURI": "http://schemas.dmtf.org/cloud/audit/1.0/event",
        "initiator": {
            "typeURI": "service/security/account/user",
            "id": "c9f76d3c31e142af9291de2935bde98a",
            "user_id": "0156ee79-b35f-4cef-ac37-d4a85f231c69",
            "project_id": "3d4a50a9-2b59-438b-bf19-c231f9c7625a"
        },
        "target": {
            "typeURI": "service/magnum/cluster",
            "id": "openstack:1c2fc591-facb-4479-a327-520dade1ea15"
        },
        "observer": {
            "typeURI": "service/magnum/cluster",
            "id": "openstack:3d4a50a9-2b59-438b-bf19-c231f9c7625a"
        },
        "eventType": "activity",
        "eventTime": "2015-05-20T01:20:47.932842+00:00",
        "action": "create",
        "outcome": "success",
        "id": "openstack:f5352d7b-bee6-4c22-8213-450e7b646e9f",
        "resource_info": "671da331c47d4e29bb6ea1d270154ec3"
    }
    "priority": "INFO",
    "publisher_id": "magnum.host1234",
    "timestamp": "2016-05-20 15:03:45.960280"
}</screen>
        </section>
      </section>
      <section xml:base="user/index#container-monitoring">
        <title>Container Monitoring</title>
        <para>The offered monitoring stack relies on the following set of containers and
            services:</para>
        <itemizedlist>
          <listitem>
            <para>cAdvisor</para>
          </listitem>
          <listitem>
            <para>Node Exporter</para>
          </listitem>
          <listitem>
            <para>Prometheus</para>
          </listitem>
          <listitem>
            <para>Grafana</para>
          </listitem>
        </itemizedlist>
        <para>To setup this monitoring stack, users are given two configurable labels in
            the Magnum cluster template’s definition:</para>
        <variablelist>
          <varlistentry>
            <term/>
            <listitem>
              <para>This label accepts a boolean value. If <emphasis>True</emphasis>, the monitoring stack will be
                        setup. By default <emphasis>prometheus_monitoring = False</emphasis>.</para>
            </listitem>
          </varlistentry>
          <varlistentry>
            <term/>
            <listitem>
              <para>This label lets users create their own <emphasis>admin</emphasis> user password for the Grafana
                        interface. It expects a string value. By default it is set to <emphasis>admin</emphasis>.</para>
            </listitem>
          </varlistentry>
        </variablelist>
        <section>
          <title>Container Monitoring in Kubernetes</title>
          <para>By default, all Kubernetes clusters already contain <emphasis>cAdvisor</emphasis> integrated
                with the <emphasis>Kubelet</emphasis> binary. Its container monitoring data can be accessed on
                a node level basis through <emphasis>http://NODE_IP:4194</emphasis>.</para>
          <para>Node Exporter is part of the above mentioned monitoring stack as it can be
                used to export machine metrics. Such functionality also work on a node level
                which means that when <literal>prometheus-monitoring</literal> is <emphasis>True</emphasis>, the Kubernetes nodes
                will be populated with an additional manifest under
                <emphasis>/etc/kubernetes/manifests</emphasis>. Node Exporter is then automatically picked up
                and launched as a regular Kubernetes POD.</para>
          <para>To aggregate and complement all the existing monitoring metrics and add a
                built-in visualization layer, Prometheus is used. It is launched by the
                Kubernetes master node(s) as a <emphasis>Service</emphasis> within a <emphasis>Deployment</emphasis> with one
                replica and it relies on a <emphasis>ConfigMap</emphasis> where the Prometheus configuration
                (prometheus.yml) is defined. This configuration uses Prometheus native
                support for service discovery in Kubernetes clusters,
                <emphasis>kubernetes_sd_configs</emphasis>. The respective manifests can be found in
                <emphasis>/srv/kubernetes/monitoring/</emphasis> on the master nodes and once the service is
                up and running, Prometheus UI can be accessed through port 9090.</para>
          <para>Finally, for custom plotting and enhanced metric aggregation and
                visualization, Prometheus can be integrated with Grafana as it provides
                native compliance for Prometheus data sources. Also Grafana is deployed as
                a <emphasis>Service</emphasis> within a <emphasis>Deployment</emphasis> with one replica. The default user is
                <emphasis>admin</emphasis> and the password is setup according to <literal>grafana-admin-passwd</literal>.
                There is also a default Grafana dashboard provided with this installation,
                from the official <link xlink:href="https://grafana.net/dashboards">Grafana dashboards’ repository</link>. The Prometheus data
                source is automatically added to Grafana once it is up and running, pointing
                to <emphasis>http://prometheus:9090</emphasis> through <emphasis>Proxy</emphasis>. The respective manifests can
                also be found in <emphasis>/srv/kubernetes/monitoring/</emphasis> on the master nodes and once
                the service is running, the Grafana dashboards can be accessed through port
                3000.</para>
          <para>For both Prometheus and Grafana, there is an assigned <emphasis>systemd</emphasis> service
                called <emphasis>kube-enable-monitoring</emphasis>.</para>
        </section>
      </section>
      <section xml:base="user/index#id7">
        <title>Kubernetes External Load Balancer</title>
        <para>In a Kubernetes cluster, all masters and minions are connected to a private
            Neutron subnet, which in turn is connected by a router to the public network.
            This allows the nodes to access each other and the external internet.</para>
        <para>All Kubernetes pods and services created in the cluster are connected to a
            private container network which by default is Flannel, an overlay network that
            runs on top of the Neutron private subnet.  The pods and services are assigned
            IP addresses from this container network and they can access each other and
            the external internet.  However, these IP addresses are not accessible from an
            external network.</para>
        <para>To publish a service endpoint externally so that the service can be accessed
            from the external network, Kubernetes provides the external load balancer
            feature.  This is done by simply specifying the attribute “type: LoadBalancer”
            in the service manifest.  When the service is created, Kubernetes will add an
            external load balancer in front of the service so that the service will have
            an external IP address in addition to the internal IP address on the container
            network.  The service endpoint can then be accessed with this external IP
            address.  Refer to the <link xlink:href="https://kubernetes.io/docs/concepts/services-networking/service/#type-loadbalancer">Kubernetes service document</link> for more details.</para>
        <para>A Kubernetes cluster deployed by Magnum will have all the necessary
            configuration required for the external load balancer.  This document describes
            how to use this feature.</para>
        <section>
          <title>Steps for the cluster administrator</title>
          <para>Because the Kubernetes master needs to interface with OpenStack to create and
                manage the Neutron load balancer, we need to provide a credential for
                Kubernetes to use.</para>
          <para>In the current implementation, the cluster administrator needs to manually
                perform this step.  We are looking into several ways to let Magnum automate
                this step in a secure manner.  This means that after the Kubernetes cluster is
                initially deployed, the load balancer support is disabled.  If the
                administrator does not want to enable this feature, no further action is
                required.  All the services will be created normally; services that specify the
                load balancer will also be created successfully, but a load balancer will not
                be created.</para>
          <para>Note that different versions of Kubernetes require different versions of
                Neutron LBaaS plugin running on the OpenStack instance:</para>
          <screen>============================  ==============================
Kubernetes Version on Master  Neutron LBaaS Version Required
============================  ==============================
1.2                           LBaaS v1
1.3 or later                  LBaaS v2
============================  ==============================</screen>
          <para>Before enabling the Kubernetes load balancer feature, confirm that the
                OpenStack instance is running the required version of Neutron LBaaS plugin.
                To determine if your OpenStack instance is running LBaaS v1, try running
                the following command from your OpenStack control node:</para>
          <screen>neutron lb-pool-list</screen>
          <para>Or look for the following configuration in neutron.conf or
                neutron_lbaas.conf:</para>
          <screen>service_provider = LOADBALANCER:Haproxy:neutron_lbaas.services.loadbalancer.drivers.haproxy.plugin_driver.HaproxyOnHostPluginDriver:default</screen>
          <para>To determine if your OpenStack instance is running LBaaS v2, try running
                the following command from your OpenStack control node:</para>
          <screen>neutron lbaas-pool-list</screen>
          <para>Or look for the following configuration in neutron.conf or
                neutron_lbaas.conf:</para>
          <screen>service_plugins = neutron.plugins.services.agent_loadbalancer.plugin.LoadBalancerPluginv2</screen>
          <para>To configure LBaaS v1 or v2, refer to the Neutron documentation.</para>
          <para>Before deleting the Kubernetes cluster, make sure to
                delete all the services that created load balancers. Because the Neutron
                objects created by Kubernetes are not managed by Heat, they will not be
                deleted by Heat and this will cause the cluster-delete operation to fail. If
                this occurs, delete the neutron objects manually (lb-pool, lb-vip, lb-member,
                lb-healthmonitor) and then run cluster-delete again.</para>
        </section>
        <section>
          <title>Steps for the users</title>
          <para>This feature requires the OpenStack cloud provider to be enabled.
                To do so, enable the cinder support (–volume-driver cinder).</para>
          <para>For the user, publishing the service endpoint externally involves the following
                2 steps:</para>
          <procedure>
            <step>
              <para>Specify “type: LoadBalancer” in the service manifest</para>
            </step>
            <step>
              <para>After the service is created, associate a floating IP with the VIP of the
                        load balancer pool.</para>
            </step>
          </procedure>
          <para>The following example illustrates how to create an external endpoint for
                a pod running nginx.</para>
          <para>Create a file (e.g nginx.yaml) describing a pod running nginx:</para>
          <screen>apiVersion: v1
kind: Pod
metadata:
  name: nginx
  labels:
   app: nginx
spec:
  containers:
  - name: nginx
    image: nginx
    ports:
    - containerPort: 80</screen>
          <para>Create a file (e.g nginx-service.yaml) describing a service for the nginx pod:</para>
          <screen>apiVersion: v1
kind: Service
metadata:
  name: nginxservice
  labels:
    app: nginx
spec:
  ports:
  - port: 80
    targetPort: 80
    protocol: TCP
  selector:
    app: nginx
  type: LoadBalancer</screen>
          <para>Please refer to the <link xlink:href="https://docs.openstack.org/developer/magnum/userguide.html">quickstart</link> guide on how to connect to Kubernetes running on the launched
                cluster. Assuming a Kubernetes cluster named k8sclusterv1 has been created,
                deploy the pod and service using following commands:</para>
          <screen>kubectl create -f nginx.yaml

kubectl create -f nginx-service.yaml</screen>
          <para>For more details on verifying the load balancer in OpenStack, refer to the
                following section on how it works.</para>
          <para>Next, associate a floating IP to the load balancer.  This can be done easily
                on Horizon by navigating to:</para>
          <screen>Compute -&gt; Access &amp; Security -&gt; Floating IPs</screen>
          <para>Click on “Allocate IP To Project” and then on “Associate” for the new floating
                IP.</para>
          <para>Alternatively, associating a floating IP can be done on the command line by
                allocating a floating IP, finding the port of the VIP, and associating the
                floating IP to the port.
                The commands shown below are for illustration purpose and assume
                that there is only one service with load balancer running in the cluster and
                no other load balancers exist except for those created for the cluster.</para>
          <para>First create a floating IP on the public network:</para>
          <screen>neutron floatingip-create public

Created a new floatingip:

+---------------------+--------------------------------------+
| Field               | Value                                |
+---------------------+--------------------------------------+
| fixed_ip_address    |                                      |
| floating_ip_address | 172.24.4.78                          |
| floating_network_id | 4808eacb-e1a0-40aa-97b6-ecb745af2a4d |
| id                  | b170eb7a-41d0-4c00-9207-18ad1c30fecf |
| port_id             |                                      |
| router_id           |                                      |
| status              | DOWN                                 |
| tenant_id           | 012722667dc64de6bf161556f49b8a62     |
+---------------------+--------------------------------------+</screen>
          <para>Note the floating IP 172.24.4.78 that has been allocated.  The ID for this
                floating IP is shown above, but it can also be queried by:</para>
          <screen>FLOATING_XML:ID=$(neutron floatingip-list | grep "172.24.4.78" | awk '{print $2}')</screen>
          <para>Next find the VIP for the load balancer:</para>
          <screen>VIP_XML:ID=$(neutron lb-vip-list | grep TCP | grep -v pool | awk '{print $2}')</screen>
          <para>Find the port for this VIP:</para>
          <screen>PORT_XML:ID=$(neutron lb-vip-show $VIP_ID | grep port_id | awk '{print $4}')</screen>
          <para>Finally associate the floating IP with the port of the VIP:</para>
          <screen>neutron floatingip-associate $FLOATING_ID $PORT_ID</screen>
          <para>The endpoint for nginx can now be accessed on a browser at this floating IP:</para>
          <screen>http://172.24.4.78:80</screen>
          <para>Alternatively, you can check for the nginx ‘welcome’ message by:</para>
          <screen>curl http://172.24.4.78:80</screen>
          <para>NOTE: it is not necessary to indicate port :80 here but it is shown to
                correlate with the port that was specified in the service manifest.</para>
        </section>
        <section>
          <title>How it works</title>
          <para>Kubernetes is designed to work with different Clouds such as Google Compute
                Engine (GCE), Amazon Web Services (AWS), and OpenStack;  therefore, different
                load balancers need to be created on the particular Cloud for the services.
                This is done through a plugin for each Cloud and the OpenStack plugin was
                developed by Angus Lees:</para>
          <screen>https://github.com/kubernetes/kubernetes/blob/release-1.0/pkg/cloudprovider/openstack/openstack.go</screen>
          <para>When the Kubernetes components kube-apiserver and kube-controller-manager start
                up, they will use the credential provided to authenticate a client
                to interface with OpenStack.</para>
          <para>When a service with load balancer is created, the plugin code will interface
                with Neutron in this sequence:</para>
          <procedure>
            <step>
              <para>Create lb-pool for the Kubernetes service</para>
            </step>
            <step>
              <para>Create lb-member for the minions</para>
            </step>
            <step>
              <para>Create lb-healthmonitor</para>
            </step>
            <step>
              <para>Create lb-vip on the private network of the Kubernetes cluster</para>
            </step>
          </procedure>
          <para>These Neutron objects can be verified as follows.  For the load balancer pool:</para>
          <screen>neutron lb-pool-list
+--------------------------------------+--------------------------------------------------+----------+-------------+----------+----------------+--------+
| id                                   | name                                             | provider | lb_method   | protocol | admin_state_up | status |
+--------------------------------------+--------------------------------------------------+----------+-------------+----------+----------------+--------+
| 241357b3-2a8f-442e-b534-bde7cd6ba7e4 | a1f03e40f634011e59c9efa163eae8ab                 | haproxy  | ROUND_ROBIN | TCP      | True           | ACTIVE |
| 82b39251-1455-4eb6-a81e-802b54c2df29 | k8sclusterv1-iypacicrskib-api_pool-fydshw7uvr7h  | haproxy  | ROUND_ROBIN | HTTP     | True           | ACTIVE |
| e59ea983-c6e8-4cec-975d-89ade6b59e50 | k8sclusterv1-iypacicrskib-etcd_pool-qbpo43ew2m3x | haproxy  | ROUND_ROBIN | HTTP     | True           | ACTIVE |
+--------------------------------------+--------------------------------------------------+----------+-------------+----------+----------------+--------+</screen>
          <para>Note that 2 load balancers already exist to implement high availability for the
                cluster (api and ectd). The new load balancer for the Kubernetes service uses
                the TCP protocol and has a name assigned by Kubernetes.</para>
          <para>For the members of the pool:</para>
          <screen>neutron lb-member-list
+--------------------------------------+----------+---------------+--------+----------------+--------+
| id                                   | address  | protocol_port | weight | admin_state_up | status |
+--------------------------------------+----------+---------------+--------+----------------+--------+
| 9ab7dcd7-6e10-4d9f-ba66-861f4d4d627c | 10.0.0.5 |          8080 |      1 | True           | ACTIVE |
| b179c1ad-456d-44b2-bf83-9cdc127c2b27 | 10.0.0.5 |          2379 |      1 | True           | ACTIVE |
| f222b60e-e4a9-4767-bc44-ffa66ec22afe | 10.0.0.6 |         31157 |      1 | True           | ACTIVE |
+--------------------------------------+----------+---------------+--------+----------------+--------+</screen>
          <para>Again, 2 members already exist for high availability and they serve the master
                node at 10.0.0.5. The new member serves the minion at 10.0.0.6, which hosts the
                Kubernetes service.</para>
          <para>For the monitor of the pool:</para>
          <screen>neutron lb-healthmonitor-list
+--------------------------------------+------+----------------+
| id                                   | type | admin_state_up |
+--------------------------------------+------+----------------+
| 381d3d35-7912-40da-9dc9-b2322d5dda47 | TCP  | True           |
| 67f2ae8f-ffc6-4f86-ba5f-1a135f4af85c | TCP  | True           |
| d55ff0f3-9149-44e7-9b52-2e055c27d1d3 | TCP  | True           |
+--------------------------------------+------+----------------+</screen>
          <para>For the VIP of the pool:</para>
          <screen>neutron lb-vip-list
+--------------------------------------+----------------------------------+----------+----------+----------------+--------+
| id                                   | name                             | address  | protocol | admin_state_up | status |
+--------------------------------------+----------------------------------+----------+----------+----------------+--------+
| 9ae2ebfb-b409-4167-9583-4a3588d2ff42 | api_pool.vip                     | 10.0.0.3 | HTTP     | True           | ACTIVE |
| c318aec6-8b7b-485c-a419-1285a7561152 | a1f03e40f634011e59c9efa163eae8ab | 10.0.0.7 | TCP      | True           | ACTIVE |
| fc62cf40-46ad-47bd-aa1e-48339b95b011 | etcd_pool.vip                    | 10.0.0.4 | HTTP     | True           | ACTIVE |
+--------------------------------------+----------------------------------+----------+----------+----------------+--------+</screen>
          <para>Note that the VIP is created on the private network of the cluster;  therefore
                it has an internal IP address of 10.0.0.7.  This address is also associated as
                the “external address” of the Kubernetes service.  You can verify this in
                Kubernetes by running following command:</para>
          <screen>kubectl get services
NAME           LABELS                                    SELECTOR    IP(S)            PORT(S)
kubernetes     component=apiserver,provider=kubernetes   &lt;none&gt;      10.254.0.1       443/TCP
nginxservice   app=nginx                                 app=nginx   10.254.122.191   80/TCP
                                                                     10.0.0.7</screen>
          <para>On GCE, the networking implementation gives the load balancer an external
                address automatically. On OpenStack, we need to take the additional step of
                associating a floating IP to the load balancer.</para>
        </section>
      </section>
    </chapter>
