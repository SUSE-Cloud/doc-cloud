<?xml version="1.0"?>
<!DOCTYPE section [
 <!ENTITY % entities SYSTEM "entities.ent"> %entities;
]>
<!---->
<section xmlns="http://docbook.org/ns/docbook" xmlns:xi="http://www.w3.org/2001/XInclude" xmlns:xlink="http://www.w3.org/1999/xlink" xml:id="replacing_osd_journal_disks"><title>&kw-hos-tm; &kw-hos-version-50;: Replacing the Journal Disk in a Ceph OSD
    Node</title><abstract><para><para>Maintenance steps for replacing the journal disk in a
      Ceph OSD node.</para></para>
</abstract>
    <sidebar xml:id="idg-all-operations-maintenance-ceph-replace_osd_journaldisk-xml-4"><para>&kw-hos-phrase; supports a Ceph environment with the following OSD node
        configurations:</para>
<itemizedlist>
        <listitem><para>OSD nodes with no journal disk</para>
</listitem>
        <listitem><para>OSD nodes with a dedicated journal disk</para>
</listitem>
        <listitem><para>OSD nodes with a shared journal disk (partition)</para>
</listitem>
      </itemizedlist><para>For the first scenario where the journal information is stored on a partition on the OSD
        data disk, the steps to replace the data disk should be followed. For more information, see
          <xref linkend="replacing_os_disks"/>.</para>
<para>For the other two scenarios where there is a separate drive used for storing journal
        information you can use the steps below to replace the journal disk on a Ceph OSD node.</para>
</sidebar>
    <sidebar xml:id="idg-all-operations-maintenance-ceph-replace_osd_journaldisk-xml-5"><title>Steps for replacing a journal disk on a Ceph OSD node</title><orderedlist>
        <listitem><para>SSH to the Ceph OSD node with the journal disk you want to replace.</para>
</listitem>
        <listitem><para>Use the command below to determine the details for the affected OSD nodes: </para>
<screen>sudo ceph-disk list | grep &lt;disk&gt; | grep osd</screen><para>Here is an example output:</para>
<screen>$ sudo ceph-disk list | grep /dev/sdg | grep osd
 /dev/sdc1 ceph data, active, cluster ceph, osd.8, journal /dev/sdg2
 /dev/sde1 ceph data, active, cluster ceph, osd.5, journal /dev/sdg3
 /dev/sdf1 ceph data, active, cluster ceph, osd.0, journal /dev/sdg1</screen><para>In this example, if the shared journal disk <literal>/dev/sdg</literal> is to be replaced
            then 0, 5, and 8 are the affected OSDs.</para>
</listitem>
        <listitem><para>Use the following command to avoid the cluster rebalancing itself as the journal disk is
          being replaced: </para>
<screen>ceph osd set noout --cluster &lt;ceph-cluster&gt;</screen></listitem>
        <listitem><para>Stop the affected OSDs and flush their journal with these commands: </para>
<screen>sudo systemctl stop ceph-osd@&lt;osd-number&gt;
sudo ceph-osd -i &lt;osd-number&gt; --flush-journal --cluster &lt;ceph-cluster&gt;</screen><para>Here is an example output, using the same information from the earlier example:</para>
<screen>$ sudo systemctl stop ceph-osd@0
$ sudo ceph-osd -i 0 --flush-journal --cluster ceph
$ sudo systemctl stop ceph-osd@5
$ sudo ceph-osd -i 5 --flush-journal --cluster ceph
$ sudo systemctl stop ceph-osd@8
$ sudo ceph-osd -i 8 --flush-journal --cluster ceph</screen></listitem>
        <listitem><para>Replace the faulty journal disk with a new physical disk. Verify that new disk gets the
          same device name (for example, <literal>/dev/sdg</literal> where the failed drive was also
            <literal>/dev/sdg</literal>) and then proceed with the next steps.</para>
</listitem>
        <listitem><para>Use the following command which generates a script file, which will be used later when
          creating the required journal partitions:
          </para>
<screen>printf '#!/bin/bash
# script to create a journal partition for Ceph OSD.

# DO NOT CHANGE it is the journal disk identifier expected by ceph-disk utility
ptype=45b0969e-9b03-4f30-b4c6-b4b80ceff106
# name of the Ceph cluster
ceph_cluster=$1
# disk to be used
new_journal=$2
# journal partition size in MB
partition_size_mb=$3
# the Ceph OSD ID
osd_id=$4
# partition number to be created on the journal disk
part_num=$5

if [ $# -ne 5 ]; then
    echo "Error: Expected input parameters not received!"
    echo "Usage: /bin/bash create_journal_partition.sh ceph_cluster journal_disk partition_size_mb osd_id partition_number"
    exit 1
fi

journal_uuid=$(sudo cat /var/lib/ceph/osd/ceph-$osd_id/journal_uuid)
part_details=$part_num:0:+$partition_size_mb
part_details+=M
sudo sgdisk --new=$part_details --change-name=$part_num:"ceph journal" --partition-guid=$part_num:$journal_uuid --typecode=$part_num:$ptype --mbrtogpt -- $new_journal
sudo partprobe
sudo ceph-osd --cluster $ceph_cluster --mkjournal -i $osd_id' &gt;&gt; create_journal_partition.sh</screen></listitem>
        <listitem><para>For each OSD node that requires a journal partition on the new disk, execute the
            <literal>create_journal_partition.sh</literal> script with the appropriate inputs as
          below: </para>
<screen>/bin/bash create_journal_partition.sh &lt;ceph_cluster&gt; &lt;journal_disk&gt; &lt;partition_size_mb&gt; &lt;osd_id&gt; &lt;partition_number&gt;</screen><para>Here are some example outputs, using the same partitions as our earlier examples:</para>
<screen># for OSD 0
$ /bin/bash create_journal_partition.sh ceph /dev/sdk 5120 0 1

# for OSD 5
$ /bin/bash create_journal_partition.sh ceph /dev/sdk 5120 5 2

# for OSD 8
$ /bin/bash create_journal_partition.sh ceph /dev/sdk 5120 8 3</screen></listitem>
        <listitem><para>Bring the OSDs back online with this command: </para>
<screen>sudo systemctl start ceph-osd@&lt;osd_id&gt;</screen><para>Here is an example output:</para>
<screen>$ sudo systemctl start ceph-osd@0
$ sudo systemctl start ceph-osd@5
$ sudo systemctl start ceph-osd@8</screen></listitem>
        <listitem><para>Unset the cluster from noout:
          </para>
<screen>ceph osd unset noout --cluster &lt;ceph-cluster&gt;</screen></listitem>
        <listitem><para>Use the following command and verify that the status of the affected OSDs are now
          appearing as <literal>up 1</literal>:
          </para>
<screen>ceph osd tree --cluster &lt;cluster-name&gt;</screen></listitem>
      </orderedlist></sidebar>
  </section>
