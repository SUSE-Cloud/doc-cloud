<?xml version="1.0"?>
<!DOCTYPE section [
 <!ENTITY % entities SYSTEM "entities.ent"> %entities;
]>
<section xmlns="http://docbook.org/ns/docbook" xmlns:xi="http://www.w3.org/2001/XInclude"
  xmlns:xlink="http://www.w3.org/1999/xlink" xml:id="ring-specification" version="5.1">
 <title>Understanding Swift Ring Specifications</title>
 <para>
  In Swift, the ring is responsible for mapping data on particular disks. There
  is a separate ring for account databases, container databases, and each
  object storage policy, but each ring works similarly. The
  <literal>swift-ring-builder</literal> utility is used to build and manage
  rings. This utility uses a builder file to contain ring information and
  additional data required to build future rings. In &kw-hos-phrase;, you will
  use the cloud model to specify how the rings are configured and used. This
  model is used to automatically invoke the
  <literal>swift-ring-builder</literal> utility as part of the deploy process.
  (Normally, you will not run the <literal>swift-ring-builder</literal> utility
  directly.)
 </para>
 <para>
  The rings are specified in the input model using the
  <emphasis role="bold"
      >configuration-data</emphasis> key. The
  <literal>configuration-data</literal> in the
  <literal>control-planes</literal> definition is given a name that you will
  then use in the <literal>swift_config.yml</literal> file. If you have several
  control planes hosting Swift services, the ring specifications can use a
  shared <literal>configuration-data</literal> object, however it is considered
  best practice to give each Swift instance its own
  <literal>configuration-data</literal> object.
 </para>
 <section xml:id="input-model">
  <title>Ring Specifications in the Input Model</title>
  <para>
   In most models, the ring-specification is mentioned in the
   <filename>~/openstack/my_cloud/definition/data/swift/swift_config.yml</filename>
   file. For example:
  </para>
<screen>configuration-data:
  - name: SWIFT-CONFIG-CP1
    services:
      - swift
    data:
      control_plane_rings:
        swift-zones:
          - id: 1
            server-groups:
              - AZ1
          - id: 2
            server-groups:
              - AZ2
          - id: 3
            server-groups:
              - AZ3
        rings:
          - name: account
            display-name: Account Ring
            min-part-hours: 16
            partition-power: 12
            replication-policy:
              replica-count: 3

          - name: container
            display-name: Container Ring
            min-part-hours: 16
            partition-power: 12
            replication-policy:
              replica-count: 3

          - name: object-0
            display-name: General
            default: yes
            min-part-hours: 16
            partition-power: 12
            replication-policy:
              replica-count: 3</screen>
  <para>
   The above sample file shows that the rings are specified using the
   <literal>configuration-data</literal> object
   <emphasis role="bold"
        >SWIFT-CONFIG-CP1</emphasis> and has three
   rings as follows:
  </para>
  <itemizedlist xml:id="ul-hl3-q55-dt">
   <listitem>
    <para>
     <emphasis role="bold">Account ring</emphasis>: You must always specify a
     ring called <emphasis role="bold">account</emphasis>. The account ring is
     used by Swift to store metadata about the projects in your system. In
     Swift, a Keystone project maps to a Swift account. The
     <literal>display-name</literal> is informational and not used.
    </para>
   </listitem>
   <listitem>
    <para>
     <emphasis role="bold">Container ring</emphasis>:You must always specify a
     ring called <emphasis role="bold">container</emphasis>. The
     <literal>display-name</literal> is informational and not used.
    </para>
   </listitem>
   <listitem>
    <para>
     <emphasis role="bold">Object ring</emphasis>: This ring is also known as a
     storage policy. You must always specify a ring called
     <emphasis role="bold">object-0</emphasis>. It is possible to have multiple
     object rings, which is known as <emphasis>storage policies</emphasis>. The
     <literal>display-name</literal> is the name of the storage policy and can
     be used by users of the Swift system when they create containers. It
     allows them to specify the storage policy that the container uses. In the
     example, the storage policy is called
     <emphasis role="bold">General</emphasis>. There are also two aliases for
     the storage policy name: <literal>GeneralPolicy</literal> and
     <literal>AnotherAliasForGeneral</literal>. In this example, you can use
     <literal>General</literal>, <literal>GeneralPolicy</literal>, or
     <literal>AnotherAliasForGeneral</literal> to refer to this storage policy.
     The aliases item is optional. The <literal>display-name</literal> is
     required.
    </para>
   </listitem>
   <listitem>
    <para>
     <emphasis role="bold">Min-part-hours, partition-power,
     replication-policy</emphasis> and
     <emphasis role="bold">replica-count</emphasis> are described in the
     following section.
    </para>
   </listitem>
  </itemizedlist>
 </section>
 <section xml:id="ring-parameters">
  <title>Replication Ring Parameters</title>
  <para>
   The ring parameters for traditional replication rings are defined as
   follows:
  </para>
  <informaltable>
   <tgroup cols="2">
    <colspec colname="c1" colnum="1"/>
    <colspec colname="c2" colnum="2"/>
    <thead>
     <row>
      <entry>Parameter</entry>
      <entry>Description</entry>
     </row>
    </thead>
    <tbody>
     <row>
      <entry><literal>replica-count</literal>
      </entry>
      <entry>
       <para>
        Defines the number of copies of object created.
       </para>
       <para>
        Use this to control the degree of resiliency or availability. The
        <literal>replica-count</literal> is normally set to
        <literal>3</literal> (that means Swift will
        keep three copies of accounts, containers, or objects). As a best
        practice, do not set the value below <literal>3</literal>. To achieve
        higher resiliency, increase the value.
       </para>
      </entry>
     </row>
     <row>
      <entry><literal>min-part-hours</literal>
      </entry>
      <entry>
       <para>
        Changes the value used to decide when a given partition can be moved.
        This is the number of hours that the
        <command>swift-ring-builder</command> tool will enforce between ring
        rebuilds. On a small system, this can be as low as
        <literal>1</literal> (one hour). The
        value can be different for each ring.
       </para>
       <para>
        In the example above, the <literal>swift-ring-builder</literal> will
        enforce a minimum of 16 hours between ring rebuilds. However, this time
        is system-dependent so you will be unable to determine the appropriate
        value for <literal>min-part-hours</literal> until you have more
        experience with your system.
       </para>
       <para>
        A value of <literal>0</literal> (zero) is not allowed.
       </para>
       <para>
        In prior releases, this parameter was called
        <literal>min-part-time</literal>. The older name is still supported,
        however do not specify both <literal>min-part-hours</literal> and
        <literal>min-part-time</literal> in the same files.
       </para>
      </entry>
     </row>
     <row>
      <entry><literal>partition-power</literal>
      </entry>
      <entry>The optimal value for this parameter is related to the number of disk drives that
              you allocate to Swift storage. As a best practice, you should use the same drives for
              both the account and container rings. In this case, the
                <literal>partition-power</literal> value should be the same. For more information,
              see <xref linkend="selecting-partition-power"/>.</entry>
     </row>
     <row>
      <entry><literal>replication-policy</literal>
      </entry>
      <entry>Specifies that a ring uses replicated storage. The duplicate copies of the object
              are created and stored on different disk drives. All replicas are identical. If one is
              lost or corrupted, the system automatically copies one of the remaining replicas to
              restore the missing replica.</entry>
     </row>
     <row>
      <entry><literal>default</literal>
      </entry>
      <entry>The default value in the above sample file of ring-specification is set to
                <emphasis role="bold">yes</emphasis>, which means that the storage policy is enabled
              to store objects. For more information, see
              <xref linkend="swift-storage-policies"/>.</entry>
     </row>
    </tbody>
   </tgroup>
  </informaltable>
 </section>
 <section xml:id="erasure-coded">
  <title>Erasure Coded Rings</title>
  <para>
   In the cloud model, a <literal>ring-specification</literal> is mentioned in
   the <filename>~/openstack/my_cloud/definition/data/swift/rings.yml</filename>
   file. A typical erasure coded ring in this file looks like this:
  </para>
<screen>- name: object-1
  display-name: EC_ring
  default: no
  min-part-hours: 16
  partition-power: 12
  erasure-coding-policy:
    ec-type: jerasure_rs_vand
    ec-num-data-fragments: 10
    ec-num-parity-fragments: 4
    ec-object-segment-size: 1048576</screen>
  <para>
   The additional parameters are defined as follows:
  </para>
  <informaltable>
   <tgroup cols="2">
    <colspec colname="c1" colnum="1"/>
    <colspec colname="c2" colnum="2"/>
    <thead>
     <row>
      <entry>Parameter</entry>
      <entry>Description</entry>
     </row>
    </thead>
    <tbody>
     <row>
      <entry>ec-type</entry>
      <entry>
       <para>
        This is the particular erasure policy scheme that is being used. The
        supported ec_types in &kw-hos-phrase; are:
       </para>
       <itemizedlist>
        <listitem>
         <para>
          <literal>jerasure_rs_vand</literal> =&gt; Vandermonde Reed-Solomon
          encoding, based on Jerasure
         </para>
        </listitem>
       </itemizedlist>
      </entry>
     </row>
     <row>
      <entry>erasure-coding-policy</entry>
      <entry>This line indicates that the object ring will be of type "erasure coding"</entry>
     </row>
     <row>
      <entry>ec-num-data-fragments</entry>
      <entry>This indicated the number of data fragments for an object in the ring.</entry>
     </row>
     <row>
      <entry>ec-num-parity-fragments</entry>
      <entry>This indicated the number of parity fragments for an object in the ring.</entry>
     </row>
     <row>
      <entry>ec-object-segment-size</entry>
      <entry>The amount of data that will be buffered up before feeding a segment into the
              encoder/decoder. The default value is 1048576.</entry>
     </row>
    </tbody>
   </tgroup>
  </informaltable>
  <para>
   When using an erasure coded ring, the number of devices in the ring must be
   greater than or equal to the total number of fragments of an object. For
   example, if you define an erasure coded ring with 10 data fragments and 4
   parity fragments, there must be at least 14 (10+4) devices added to the
   ring.
  </para>
  <para>
   When using erasure codes, for a PUT object to be successful it must store
   <literal>ec_ndata + 1</literal> fragment to achieve quorum. Where the number
   of data fragments (<literal>ec_ndata</literal>) is 10 then at least 11
   fragments must be saved for the object PUT to be successful. The 11
   fragments must be saved to different drives. To tolerate a single object
   server going down, say in a system with 3 object servers, each object server
   must have at least 6 drives assigned to the erasure coded storage policy. So
   with a single object server down, 12 drives are available between the
   remaining object servers. This allows an object PUT to save 12 fragments,
   one more than the minimum to achieve quorum.
  </para>
  <para>
   Unlike replication rings, none of the erasure coded parameters may be edited
   after the initial creation. Otherwise there is potential for permanent loss
   of access to the data.
  </para>
  <para>
   On the face of it, you would expect that an erasure coded configuration that
   uses a data to parity ratio of 10:4, that the data consumed storing the
   object is 1.4 times the size of the object just like the x3 replication
   takes x3 times the size of the data when storing the object. However, for
   erasure coding, this 10:4 ratio is not correct. The efficiency (that is how
   much storage is needed to store the object) is very poor for small objects
   and improves as the object size grows. However, the improvement is not
   linear. If all of your files are less than 32K in size, erasure coding will
   take more space to store than the x3 replication.
  </para>
 </section>
 <section xml:id="selecting-partition-power">
  <title>Selecting a Partition Power</title>
  <para>
   When storing an object, the object storage system hashes the name. This hash
   results in a hit on a partition (so a number of different object names
   result in the same partition number). Generally, the partition is mapped to
   available disk drives. With a replica count of 3, each partition is mapped
   to three different disk drives. The hashing algorithm used hashes over a
   fixed number of partitions. The partition-power attribute determines the
   number of partitions you have.
  </para>
  <para>
   Partition power is used to distribute the data uniformly across drives in a
   Swift nodes. It also defines the storage cluster capacity. You must set the
   partition power value based on the total amount of storage you expect your
   entire ring to use.
  </para>
  <para>
   You should select a partition power for a given ring that is appropriate to
   the number of disk drives you allocate to the ring for the following
   reasons:
  </para>
  <itemizedlist xml:id="ul-i4b-gv5-dt">
   <listitem>
    <para>
     If you use a high partition power and have a few disk drives, each disk
     drive will have thousands of partitions. With too many partitions, audit
     and other processes in the Object Storage system cannot walk the
     partitions in a reasonable time and updates will not occur in a timely
     manner.
    </para>
   </listitem>
   <listitem>
    <para>
     If you use a low partition power and have many disk drives, you will have
     tens (or maybe only one) partition on a drive. The Object Storage system
     does not use size when hashing to a partition - it hashes the name.
    </para>
    <para>
     With many partitions on a drive, a large partition is cancelled out by a
     smaller partition so the overall drive usage is similar. However, with
     very small numbers of partitions, the uneven distribution of sizes can be
     reflected in uneven disk drive usage (so one drive becomes full while a
     neighboring drive is empty).
    </para>
   </listitem>
  </itemizedlist>
  <para>
   An ideal number of partitions per drive is 100. If you know the number of
   drives, select a partition power that will give you approximately 100
   partitions per drive. Usually, you install a system with a specific number
   of drives and add drives as needed. However, you cannot change the value of
   the partition power. Hence you must select a value that is a compromise
   between current and planned capacity.
  </para>
  <important>
   <para>
    If you are installing a small capacity system and you need to grow to a
    very large capacity but you cannot fit within any of the ranges in the
    table, please seek help from &serviceteam; to plan your system.
   </para>
  </important>
  <para>
   There are additional factors that can help mitigate the fixed nature of the
   partition power:
  </para>
  <itemizedlist>
   <listitem>
    <para>
     Account and container storage represents a small fraction (typically 1
     percent) of your object storage needs. Hence, you can select a smaller
     partition power (relative to object ring partition power) for the account
     and container rings.
    </para>
   </listitem>
  </itemizedlist>
  <itemizedlist>
   <listitem>
    <para>
     For object storage, you can add additional storage policies (that is, another
     object ring). When you have reached capacity in an existing storage
     policy, you can add a new storage policy with a higher partition power
     (because you now have more disk drives in your system). This means that
     you can install your system using a small partition power appropriate to a
     small number of initial disk drives. Later, when you have many disk
     drives, the new storage policy can have a higher value appropriate to the
     larger number of drives.
    </para>
   </listitem>
  </itemizedlist>
  <para>
   However, when you continue to add storage capacity, existing containers will
   continue to use their original storage policy. Hence, the additional objects
   must be added to new containers to take advantage of the new storage policy.
  </para>
  <para>
   Use the following table to select an appropriate partition power for each
   ring. The partition power of a ring cannot be changed, so it is important to
   select an appropriate value. This table is based on a replica count of 3. If
   your replica count is different, or you are unable to find your system in
   the table, then see <xref linkend="selecting-partition-power"/> for
   information of selecting a partition power.
  </para>
  <para>
   The table assumes that when you first deploy Swift, you have a small number
   of drives (the minimum column in the table), and later you add drives.
  </para>
  <note>
   <itemizedlist>
    <listitem>
     <para>
      Use the total number of drives. For example, if you have three servers,
      each with two drives, the total number of drives is six.
     </para>
    </listitem>
    <listitem>
     <para>
      The lookup should be done separately for each of the account, container
      and object rings. Since account and containers represent approximately 1
      to 2 percent of object storage, you will probably use fewer drives for
      the account and container rings (that is, you will have fewer proxy,
      account, and container (PAC) servers) so that your object rings may have
      a higher partition power.
     </para>
    </listitem>
    <listitem>
     <para>
      The largest anticipated number of drives imposes a limit in the minimum
      drives you can have. (For more information, see
      <xref linkend="selecting-partition-power"/>.) This means that, if you
      anticipate significant growth, your initial system can be small, but
      under a certain limit. For example, if you determine that the maximum
      number of drives the system will grow to is 40,000, then use a partition
      power of 17 as listed in the table below. In addition, a minimum of 36
      drives is required to build the smallest system with this partition
      power.
     </para>
    </listitem>
    <listitem>
     <para>
      The table assumes that disk drives are the same size. The actual size of
      a drive is not significant.
     </para>
    </listitem>
   </itemizedlist>
  </note>
 </section>
</section>
