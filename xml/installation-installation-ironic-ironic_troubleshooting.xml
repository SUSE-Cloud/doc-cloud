<?xml version="1.0"?>
<!DOCTYPE section [
 <!ENTITY % entities SYSTEM "entities.ent"> %entities;
]>
<section xml:id="ironic-toubleshooting"
 xmlns="http://docbook.org/ns/docbook" version="5.1"
 xmlns:xi="http://www.w3.org/2001/XInclude"
 xmlns:xlink="http://www.w3.org/1999/xlink">
 <title>Troubleshooting Ironic Installation</title>
 <para>
  Sometimes the <literal>nova boot</literal> command does not succeed and when
  you do a <literal>nova list</literal>, you will see output like the
  following:
 </para>
<screen>
<?dbsuse-fo font-size="0.70em"?>
nova list

+------------------+--------+--------+------------+-------------+----------+
| ID               | Name   | Status | Task State | Power State | Networks |
+------------------+--------+--------+------------+-------------+----------+
| ee08f82...624e5f | ubuntu | ERROR  | -          | NOSTATE     |          |
+------------------+--------+--------+------------+-------------+----------+
</screen>
 <para>
  You should execute the <literal>nova show &lt;nova-node-id&gt;</literal> and
  <literal>ironic node-show &lt;ironic-node-id&gt;</literal> commands to get
  more information about the error.
 </para>
 <section>
  <title>No valid host was found. There are not enough hosts available.</title>
  <para>
   This error is typically seen when performing the <literal>nova
   boot</literal> where there is a mismatch between the properties set on the
   node and the flavor used. For example, the output from a <literal>nova
   show</literal> command may look like this:
  </para>
<screen>
<?dbsuse-fo font-size="0.65em"?>
nova show  ee08f82e-8920-4360-be51-a3f995624e5f

+------------------------+------------------------------------------------------------------------------+
| Property               | Value                                                                        |
+------------------------+------------------------------------------------------------------------------+
| OS-EXT-AZ:             |                                                                              |
|   availability_zone    |                                                                              |
| OS-EXT-SRV-ATTR:host   | -                                                                            |
| OS-EXT-SRV-ATTR:       |                                                                              |
|   hypervisor_hostname  | -                                                                            |
| OS-EXT-SRV-ATTR:       |                                                                              |
|   instance_name        | instance-00000001                                                            |
| OS-EXT-STS:power_state | 0                                                                            |
| OS-EXT-STS:task_state  | -                                                                            |
| OS-EXT-STS:vm_state    | error                                                                        |
| OS-SRV-USG:launched_at | -                                                                            |
| OS-SRV-USG:            |                                                                              |
|    terminated_at       | -                                                                            |
| accessIPv4             |                                                                              |
| accessIPv6             |                                                                              |
| config_drive           |                                                                              |
| created                | 2016-03-11T11:00:28Z                                                         |
| fault                  | {"message": "<emphasis role="bold">No valid host was found. There are not enough hosts             |
|                        |  available.</emphasis>", "code": 500, "details": "  File \<emphasis role="bold">"/opt/stack/                  |
|                        |  venv/nova-20160308T002421Z/lib/python2.7/site-packages/nova/                |
|                        |  conductor/manager.py\"</emphasis>, line 739, in build_instances                        |
|                        |     request_spec, filter_properties)                                         |
|                        |   File \<emphasis role="bold">"/opt/stack/venv/nova-20160308T002421Z/lib/python2.7/                |
|                        |   site-packages/nova/scheduler/utils.py\"</emphasis>, line 343, in wrapped              |
|                        |     return func(*args, **kwargs)                                             |
|                        |   File \<emphasis role="bold">"/opt/stack/venv/nova-20160308T002421Z/lib/python2.7/                |
|                        |   site-packages/nova/scheduler/client/__init__.py\"</emphasis>, line 52,                |
|                        |     in select_destinations context, request_spec, filter_properties)         |
|                        |   File \"/opt/stack/venv/nova-20160308T002421Z/lib/python2.7/                |
|                        |   site-packages/nova/scheduler/client/__init__.py\",line 37,in __run_method  |
|                        |     return getattr(self.instance, __name)(*args, **kwargs)                   |
|                        |   File \"/opt/stack/venv/nova-20160308T002421Z/lib/python2.7/                |
|                        |   site-packages/nova/scheduler/client/query.py\", line 34,                   |
|                        |     in select_destinations context, request_spec, filter_properties)         |
|                        |   File \"/opt/stack/venv/nova-20160308T002421Z/lib/python2.7/                |
|                        |   site-packages/nova/scheduler/rpcapi.py\", line 120, in select_destinations |
|                        |     request_spec=request_spec, filter_properties=filter_properties)          |
|                        |   File \"/opt/stack/venv/nova-20160308T002421Z/lib/python2.7/                |
|                        |   site-packages/oslo_messaging/rpc/client.py\", line 158, in call            |
|                        |     retry=self.retry)                                                        |
|                        |   File \"/opt/stack/venv/nova-20160308T002421Z/lib/python2.7/                |
|                        |   site-packages/oslo_messaging/transport.py\", line 90, in _send             |
|                        |     timeout=timeout, retry=retry)                                            |
|                        |   File \"/opt/stack/venv/nova-20160308T002421Z/lib/python2.7/                |
|                        |   site-packages/oslo_messaging/_drivers/amqpdriver.py\", line 462, in send   |
|                        |     retry=retry)                                                             |
|                        |   File \"/opt/stack/venv/nova-20160308T002421Z/lib/python2.7/                |
|                        |   site-packages/oslo_messaging/_drivers/amqpdriver.py\", line 453, in _send  |
|                        |     raise result                                                             |
|                        | ", "created": "2016-03-11T11:00:29Z"}                                        |
| flavor                 | bmtest (645de08d-2bc6-43f1-8a5f-2315a75b1348)                                |
| hostId                 |                                                                              |
| id                     | ee08f82e-8920-4360-be51-a3f995624e5f                                         |
| image                  | ubuntu (17e4915a-ada0-4b95-bacf-ba67133f39a7)                                |
| key_name               | ironic_kp                                                                    |
| metadata               | {}                                                                           |
| name                   | ubuntu                                                                       |
| os-extended-volumes:   |                                                                              |
|    volumes_attached    | []                                                                           |
| status                 | ERROR                                                                        |
| tenant_id              | d53bcaf15afb4cb5aea3adaedbaa60dd                                             |
| updated                | 2016-03-11T11:00:28Z                                                         |
| user_id                | e580c645bfec4faeadef7dbd24aaf990                                             |
+------------------------+------------------------------------------------------------------------------+
</screen>
  <para>
   You can find more information about the error by inspecting the log file at
   <literal>/var/log/nova/nova-scheduler.log</literal> or alternatively by
   viewing the error location in the source files listed in the stack-trace (in
   bold above).
  </para>
  <para>
   To find the mismatch, compare the properties of the ironic node:
  </para>
<screen>
<?dbsuse-fo font-size="0.70em"?>
+------------------------+---------------------------------------------------------------------+
| Property               | Value                                                               |
+------------------------+---------------------------------------------------------------------+
| target_power_state     | None                                                                |
| extra                  | {}                                                                  |
| last_error             | None                                                                |
| updated_at             | None                                                                |
| maintenance_reason     | None                                                                |
| provision_state        | available                                                           |
| clean_step             | {}                                                                  |
| uuid                   | ea7246fd-e1d6-4637-9699-0b7c59c22e67                                |
| console_enabled        | False                                                               |
| target_provision_state | None                                                                |
| provision_updated_at   | None                                                                |
| maintenance            | False                                                               |
| inspection_started_at  | None                                                                |
| inspection_finished_at | None                                                                |
| power_state            | None                                                                |
| driver                 | agent_ilo                                                           |
| reservation            | None                                                                |
| properties             | <emphasis role="bold">{u'memory_mb': 64000, u'local_gb': 99, u'cpus': 2, u'capabilities':</emphasis> |
|                        | <emphasis role="bold">u'boot_mode:bios,boot_option:local'} </emphasis>                               |
| instance_uuid          | None                                                                |
| name                   | None                                                                |
| driver_info            | {u'ilo_address': u'10.1.196.117', u'ilo_password': u'******',       |
|                        | u'ilo_deploy_iso': u'b9499494-7db3-4448-b67f-233b86489c1f',         |
|                        | u'ilo_username': u'Administrator'}                                  |
| created_at             | 2016-03-11T10:17:10+00:00                                           |
| driver_internal_info   | {}                                                                  |
| chassis_uuid           |                                                                     |
| instance_info          | {}                                                                  |
+------------------------+---------------------------------------------------------------------+
</screen>
  <para>
   with the flavor characteristics:
  </para>
<screen>
<?dbsuse-fo font-size="0.70em"?>
nova flavor-show

+----------------------------+-------------------------------------------------------------------+
| Property                   | Value                                                             |
+----------------------------+-------------------------------------------------------------------+
| OS-FLV-DISABLED:disabled   | False                                                             |
| OS-FLV-EXT-DATA:ephemeral  | 0                                                                 |
| disk                       | <emphasis role="bold">99 </emphasis>                                                               |
| extra_specs                | <emphasis role="bold">{"capabilities:boot_option": "local", "cpu_arch": "x86_64",       |
|                            | "capabilities:boot_mode": "bios"}</emphasis>                                 |
| id                         | 645de08d-2bc6-43f1-8a5f-2315a75b1348                              |
| name                       | bmtest                                                            |
| os-flavor-access:is_public | True                                                              |
| ram                        | <emphasis role="bold">64000</emphasis>                                                             |
| rxtx_factor                | 1.0                                                               |
| swap                       |                                                                   |
| vcpus                      | <emphasis role="bold">2</emphasis>                                                                 |
+----------------------------+-------------------------------------------------------------------+
</screen>
  <para>
   In this instance, the problem is caused by the absence of the
   <emphasis role="bold">"cpu_arch": "x86_64"</emphasis> property on the ironic
   node. This can be resolved by updating the ironic node, adding the missing
   property:
  </para>
<screen>
<?dbsuse-fo font-size="0.70em"?>
ironic node-update ea7246fd-e1d6-4637-9699-0b7c59c22e67 <emphasis role="bold">add properties/cpu_arch=x86_64</emphasis>
</screen>
  <para>
   and then re-running the <literal>nova boot</literal> command.
  </para>
 </section>
 <section>
  <title>Deployment to a node fails and in "ironic node-list" command, the power_state column for the node is shown as "None"</title>
  <para>
   <emphasis role="bold">Possible cause: </emphasis> The IPMI commands to the
   node take longer to change the power state of the server.
  </para>
  <para>
   <emphasis role="bold">Resolution: </emphasis> Check if the node power state
   can be changed using the following command
  </para>
<screen>ironic node-set-power-state $NODEUUID on</screen>
  <para>
   If the above command succeeds and the power_state column is updated
   correctly, then the following steps are required to increase the power sync
   interval time.
  </para>
  <para>
   On the first controller, reconfigure Ironic to increase the power sync
   interval time. In the example below, it is set to 120 seconds. This value
   may have to be tuned based on the setup.
  </para>
  <procedure>
   <step>
    <para>
     Go to the <literal>~/helion/my_cloud/config/ironic/</literal> directory
     and edit <literal>ironic-conductor.conf.j2</literal> to set the
     <literal>sync_power_state_interval</literal> value:
    </para>
<screen>[conductor]
sync_power_state_interval = 120</screen>
   </step>
   <step>
    <para>
     Save the file and then run the following playbooks:
    </para>
<screen>
cd ~/helion/hos/ansible
ansible-playbook -i hosts/localhost config-processor-run.yml
ansible-playbook -i hosts/localhost ready-deployment.yml
cd ~/scratch/ansible/next/hos/ansible
ansible-playbook -i hosts/verb_hosts ironic-reconfigure.yml
 </screen>
   </step>
  </procedure>
 </section>
 <section>
  <title>Error Downloading Image</title>
  <para>
   If you encounter the error below during the deployment:
  </para>
<screen>
<?dbsuse-fo font-size="0.70em"?>
"u'message': u'Error downloading image: Download of image id 77700...96551 failed:
Image download failed for all URLs.',
u'code': 500,
u'type': u'ImageDownloadError',
u'details': u'Download of image id 77700b53-9e15-406c-b2d5-13e7d9b96551 failed:
Image download failed for all URLs.'"
</screen>
  <para>
   you should visit the Single Sign-On Settings in the Security page of iLO and
   change the Single Sign-On Trust Mode setting from the default of "Trust None
   (SSO disabled)" to "Trust by Certificate".
  </para>
 </section>
 <section>
  <title>Using <literal>node-inspection</literal> can cause temporary claim of IP addresses</title>
  <para>
   <emphasis role="bold">Possible cause: </emphasis> Running
   <literal>node-inspection</literal> on a node discovers all the NIC ports
   including the NICs that don’t have any connectivity. This causes a
   temporary consumption of the network IPs and increased usage of the
   allocated quota. As a result, other nodes are deprived of IP addresses and
   deployments can fail.
  </para>
  <para>
   <emphasis role="bold">Resolution:</emphasis>You can add node properties
   manually added instead of using the inspection tool.
  </para>
  <para>
   Note: Upgrade <literal>ipmitool</literal> to a version &gt;= 1.8.15 or it
   may not return detailed information about the NIC interface for
   <literal>node-inspection</literal>.
  </para>
 </section>
 <section>
  <title>Node permanently stuck in deploying state</title>
  <para>
   <emphasis role="bold">Possible causes:</emphasis>
  </para>
  <itemizedlist>
   <listitem>
    <para>
     Ironic conductor service associated with the node could go down.
    </para>
   </listitem>
   <listitem>
    <para>
     There might be a properties mismatch. MAC address registered for the node
     could be incorrect.
    </para>
   </listitem>
  </itemizedlist>
  <para>
   <emphasis role="bold">Resolution:</emphasis> To recover from this
   condition, set the provision state of the node to <literal>Error</literal>
   and maintenance to <literal>True</literal>. Delete the node and re-register
   again.
  </para>
 </section>
 <section>
  <title>The NICs in the baremetal node should come first in boot order</title>
  <para>
   <emphasis role="bold">Possible causes:</emphasis> By default, the boot
   order of baremetal node is set as NIC1, HDD and NIC2. If NIC1 fails, the
   nodes starts booting from HDD and the provisioning fails.
  </para>
  <para>
   <emphasis role="bold">Resolution:</emphasis> Set boot order so that all the
   NICs appear before the hard disk of the baremetal as NIC1, NIC2…, HDD.
  </para>
 </section>
 <section>
  <title>Increase in the number of nodes can cause power commands to fail</title>
  <para>
   <emphasis role="bold">Possible causes:</emphasis>Ironic periodically
   performs a power state sync with all the baremetal nodes. When the number of
   nodes increase, ironic does not get sufficient time to perform power
   operations.
  </para>
  <para>
   <emphasis role="bold">Resolution:</emphasis> The following procedure gives a
   way to increase <literal>sync_power_state_interval</literal>:
  </para>
  <procedure>
   <step>
    <para>
     Edit the file
     <literal>~/helion/my_cloud/config/ironic/ironic-conductor.conf.j2</literal>
     and navigate to the section for <literal>[conductor]</literal>
    </para>
   </step>
   <step>
    <para>
     Increase the <literal>sync_power_state_interval</literal>. For example,
     for 100 nodes, set <literal>sync_power_state_interval = 90</literal> and
     save the file.
    </para>
   </step>
   <step>
    <para>
     Execute the following set of commands to reconfigure Ironic:
    </para>
<screen>
cd ~/helion/hos/ansible
ansible-playbook -i hosts/localhost config-processor-run.yml
ansible-playbook -i hosts/localhost ready-deployment.yml
cd ~/scratch/ansible/next/hos/ansible
ansible-playbook -i hosts/verb_hosts ironic-reconfigure.yml
 </screen>
   </step>
  </procedure>
 </section>
 <section>
  <title>DHCP succeeds with PXE but times out with iPXE</title>
  <para>
   If you see DHCP error "No configuration methods succeeded" in iPXE, right
   after successful DHCP performed by embedded NIC firmware (please refer to
   screenshot below), there may be an issue with Spanning Tree Protocol on the
   switch.
  </para>
  <informalfigure>
   <mediaobject>
    <imageobject role="fo">
     <imagedata fileref="media-ironic-DHCP-pxe-ipxe.png" width="75%" format="PNG"/>
    </imageobject>
    <imageobject role="html">
     <imagedata fileref="media-ironic-DHCP-pxe-ipxe.png"/>
    </imageobject>
   </mediaobject>
  </informalfigure>
  <para>
   To avoid this error, Rapid Spanning Tree Protocol needs to be enabled on the
   switch. If this is not an option due to conservative loop detection
   strategies, use the steps outlined below to install the iPXE binary with
   increased DHCP timeouts.
  </para>



  <procedure>
   <step>
    <para>
     Clone iPXE source code
    </para>
<screen>$ git clone git://git.ipxe.org/ipxe.git
$ cd ipxe/src</screen>
   </step>
   <step>
    <para>
     Modify lines 22-25 in file <literal>config/dhcp.h</literal>, which declare
     reduced DHCP timeouts (1-10 secs). Comment out lines with reduced timeouts
     and uncomment normal PXE timeouts (4-32)
    </para>
<screen>//#define DHCP_DISC_START_TIMEOUT_SEC     1
//#define DHCP_DISC_END_TIMEOUT_SEC       10
#define DHCP_DISC_START_TIMEOUT_SEC   4       /* as per PXE spec */
#define DHCP_DISC_END_TIMEOUT_SEC     32      /* as per PXE spec */</screen>
   </step>
   <step>
    <para>
     Make <literal>undionly.kpxe</literal> (BIOS) and
     <literal>ipxe.efi</literal> (UEFI) images
    </para>
<screen>$ make bin/undionly.kpxe
$ make bin-x86_64-efi/ipxe.efi</screen>
   </step>
   <step>
    <para>
     Copy iPXE images to &lcm;
    </para>
<screen>$ scp bin/undionly.kpxe bin-x86_64-efi/ipxe.efi stack@10.0.0.4:
stack@10.0.0.4's password:
undionly.kpxe                                    100%   66KB  65.6KB/s   00:00
ipxe.efi                                         100%  918KB 918.2KB/s   00:00</screen>
   </step>
   <step>
    <para>
     From deployer, distribute image files onto all 3 controllers
    </para>
<screen>
<?dbsuse-fo font-size="0.65em"?>
stack@helion-cp1-c1-m1-mgmt:~$ cd ~/scratch/ansible/next/hos/ansible/

stack@helion-cp1-c1-m1-mgmt:~/scratch/ansible/next/hos/ansible$ ansible -i hosts/verb_hosts \
IRN-CND -m copy -b -a 'src=/home/stack/ipxe.efi dest=/tftpboot'
...
stack@helion-cp1-c1-m1-mgmt:~/scratch/ansible/next/hos/ansible$ ansible -i hosts/verb_hosts \
IRN-CND -m copy -b -a 'src=/home/stack/undionly.kpxe dest=/tftpboot'
...</screen>
   </step>
  </procedure>
  <para>
   There is no need to restart services. With next PXE boot attempt, iPXE
   binary with the increased timeout will be downloaded to the target node via
   TFTP.
  </para>
 </section>
</section>
