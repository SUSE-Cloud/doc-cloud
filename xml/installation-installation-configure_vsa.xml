<?xml version="1.0"?>
<!DOCTYPE section [
 <!ENTITY % entities SYSTEM "entities.ent"> %entities;
]>
<section xml:id="config_vsa"
 xmlns="http://docbook.org/ns/docbook" version="5.1"
 xmlns:xi="http://www.w3.org/2001/XInclude"
 xmlns:xlink="http://www.w3.org/1999/xlink">
 <title>Configuring for VSA Block Storage Backend</title>
 <info>
  <abstract>
   <para>
    This page describes how to configure your VSA backend for the &productname;
    Entry-scale with KVM cloud model. This procedure can be performed during
    or after installation.
   </para>
  </abstract>
 </info>
 <section>
  <title>Prerequisites</title>
  <itemizedlist>
   <listitem>
    <para>
     Your &kw-hos; cloud must be fully deployed using the KVM cloud model
     with VSA added. The Entry-Scale KVM with VSA cloud model is an example of
     such a model. For more details on the installation refer to
     <xref linkend="install_kvm"/>.
    </para>
   </listitem>
   <listitem>
    <para>
     It's important that all of your systems have the correct date/time because
     the &storevirtual; VSA license has a start date. If the start date is
     later than the system time then the installation will fail.
    </para>
   </listitem>
  </itemizedlist>
 </section>
 <section xml:id="ul_kg4_y3k_5v">
  <title>Notes</title>
  <itemizedlist>
   <listitem>
    <para>
     The license for the &storevirtual; VSA license is bundled with &kw-hos;
     and comes with a free trial which allows a maximum limit of 50 TB per
     node. Hence the total amount of the configured storage on an individual
     VSA node should not exceed 50 TB. To extend the 50 TB per node limit, you
     can add nodes. A VSA cluster can support up to 16 nodes, which means
     configured storage on a VSA cluster can be as much as 800 TB.
    </para>
    <para>
     You can deploy VSA with Adaptive Optimization (AO) or without. The
     deployment process for each of these options is similar, you just need to
     make a change in the disk input model. For more detailed information,
     refer to <emphasis role="bold">VSA with or without Adaptive Optimization
     (AO)</emphasis> in <xref linkend="modify_entryscale_kvm_vsa"/>.
    </para>
    <!-- FIXME: the following was commented in the DITA original. -->
    <!-- this para for CG -->
    <!--<p product="CG">
     You can deploy VSA with Adaptive Optimization (AO) or without. The
     deployment process for each of these options is similar, you just need
     to make a change in the disk input model. For more detailed information,
     refer to <b>VSA with or without Adaptive Optimization (AO)</b> in
     <xref href="configure_vsa_ao.xml"/>.
    </p>-->
   </listitem>
   <listitem>
    <para>
     A single VSA node can have a maximum of seven raw disks (excluding the
     operating system disks) attached to it, which is defined in the disk input
     model for your VSA nodes. It is expected that no more than seven disks are
     specified (including Adaptive Optimization disks) per VSA node. For
     example, if you want to deploy VSA with two disks for Adaptive
     Optimization then your disk input model should not specify more than five
     raw disks for data and two raw disks for Adaptive Optimization. Exceeding
     the disk limit causes VSA deployment failure.
    </para>
   </listitem>
   <listitem>
    <para>
     You can create more than one VSA cluster of same or different type by
     specifying the configuration in cloud model. For more details, refer to
     <!-- FIXME: <xref href="../operations/blockstorage/vsa/create_multiple_vsa_clusters.xml"/> -->.
    </para>
   </listitem>
   <listitem>
    <para>
     Usernames and passwords designated during VSA configuration or repair must
     be alphabetic only.
    </para>
   </listitem>
  </itemizedlist>
  <para>
   <emphasis role="bold">Concerning using multiple backends:</emphasis> If you
   are using multiple backend options, ensure that you specify each of the
   backends you are using when configuring your
   <literal>cinder.conf.j2</literal> file using a comma delimited list. An
   example would be <literal>enabled_backends=vsa-1,3par_iSCSI,ceph1</literal>
   and is included in the steps below. You will also want to create multiple
   volume types so you can specify which backend you want to utilize when
   creating volumes. These instructions are included below as well. In addition
   to our documentation, you can also read the OpenStack documentation at
   <link xlink:href="http://docs.openstack.org/admin-guide/blockstorage_multi_backend.html">Configure
   multiple storage backends</link> as well.
  </para>
  <para>
   <emphasis role="bold">VSA driver has updated name:</emphasis> In the
   OpenStack Mitaka release, the VSA driver used for &productname; integration had
   its name updated from <literal>hplefthand_</literal> to
   <literal>hpelefthand_</literal>. To prevent issues when upgrading from
   previous &kw-hos; releases, we left the names as-is in the release and
   provided a mapping so that the integration would continue to work. This will
   produce a warning in the <literal>cinder-volume.log</literal> file advising
   you of the deprecated name. The warning will look similar to this:
  </para>
<screen>Option "hplefthand_username" from group "&lt;your section&gt;" is deprecated. Use option "hpelefthand_username" from group "&lt;your section&gt;"
should be ignored.</screen>
  <para>
   These are just warnings and can be ignored.
  </para>
 </section>
 <section>
  <title>Configure &storevirtual; VSA</title>
  <para>
   The process for configuring &storevirtual; VSA for &kw-hos; involves two
   major steps:
  </para>
  <itemizedlist xml:id="ul_kbm_n3k_5v">
   <listitem>
    <para>
     Creating a cluster
    </para>
   </listitem>
   <listitem>
    <para>
     Configuring VSA as the block storage backend
    </para>
   </listitem>
  </itemizedlist>
  <para>
   You can perform these tasks in an automated process using the provided
   Ansible playbooks or manually using the &storevirtual; Centralized
   Management Console (CMC) GUI.
  </para>
  <note>
   <para>
    Using Ansible playbooks to create clusters is strongly recommended. Cluster
    creation using &storevirtual; Centralized Management Console (CMC) GUI is
    deprecated in &kw-hos-phrase;. Clusters created using CMC manually cannot
    be reconfigured using the Ansible playbooks. If you used CMC to create the
    clusters, you must continue to use CMC to administer those clusters.
   </para>
  </note>
 </section>
 <section>
  <title>Using Ansible</title>
  <para>
   Perform the following steps to create the cluster using the provided Ansible
   playbooks.
  </para>

  <procedure>
   <step>
    <para>
     Log in to the &lcm;.
    </para>
   </step>
   <step>
    <para>
     Run the following playbook, which will both create your management group
     and your cluster:
    </para>
<screen>cd ~/scratch/ansible/next/hos/ansible/
ansible-playbook -i hosts/verb_hosts vsalm-configure-cluster.yml</screen>
   </step>
   <step>
    <para>
     When prompted, enter an administrative user name and password you will use
     to administer the CMC utility.
    </para>
    <para>
     Administrative user naming requirements:
    </para>
    <itemizedlist>
     <listitem>
      <para>
       3 to 30 characters
      </para>
     </listitem>
     <listitem>
      <para>
       Must begin with a letter
      </para>
     </listitem>
     <listitem>
      <para>
       Allowed characters: 0-9, a-z, A-Z, hyphen (-), underline (_)
      </para>
     </listitem>
     <listitem>
      <para>
       Disallowed characters: Multibyte character set
      </para>
     </listitem>
    </itemizedlist>
    <para>
     Administrative user password requirements:
    </para>
    <itemizedlist>
     <listitem>
      <para>
       5 to 40 characters
      </para>
     </listitem>
     <listitem>
      <para>
       Must begin with a letter
      </para>
     </listitem>
     <listitem>
      <para>
       Many ASCII and extended ASCII characters, Multibyte character set
      </para>
     </listitem>
     <listitem>
      <para>
       Disallowed characters: dot (.), colon (:), forward slash (/), comma (,),
       backward slash (\), semi-colon (;), single-quote (â€˜), space ( ),
       equals (=)
      </para>
     </listitem>
    </itemizedlist>
    <important>
     <para>
      You will need to remember these values to access the cluster using the
      CMC utility or to re-run this playbook later if further management of
      your VSA system is needed. Do not change the credentials for one or more
      clusters using CMC as it will make the cluster automation playbook ignore
      those clusters for reconfiguration.
     </para>
    </important>
   </step>
   <step>
    <para>
     In the output of the Ansible playbook you will see different steps where
     you can locate the values for your VSA cluster name, the virtual IP
     address, and the IP addresses of each of the VSA hosts in the cluster. You
     will use these values later when configuring your backend.
    </para>
    <para>
     The following is an example, with the important information bolded.
    </para>
    <para>
     This step displays the VSA cluster name and the virtual IP address of the
     cluster:
    </para>
<screen>TASK: [vsalm | _create_cluster | Display the status of cluster creation performed in above step] ***
ok: [helion-cp1-vsa0001-mgmt] =&gt; {
    "msg": "Created cluster - Name=<emphasis role="bold">cluster-vsa</emphasis>, IP=<emphasis role="bold">10.13.111.142</emphasis>"
}</screen>
   </step>
   <step>
    <para>
     You can view the added node to the cluster in CMC GUI as follows:
    </para>
     <substeps>
		<step>
      <para>
       Launch the CMC utility. For more information, see
       <xref linkend="cmc"/>.
      </para>
     </step>
     <step>
      <para>
       Find the new VSA system.
      </para>
     </step>
     <step>
      <para>
       View the added node in the cluster.
      </para>
	</step>
     </substeps>
    </step>
   <!-- FIXME: the following was commented in the DITA original. -->
   <!--
   <li>
    Run the following command from your first controller node which will open
    the &storevirtual; Centralized Management Console (CMC) GUI on your
    local machine:
    <codeblock>/opt/HP/StoreVirtual/UI/jre/bin/java -jar /opt/HP/StoreVirtual/UI/UI.jar</codeblock>
   </li>
   <li>
    In the CMC GUI, click the Find menu and then select the Find Systems
    options.Once systems are found, new created clusters can been observed in
    CMC.
   </li>
   <li>
    In the CMC GUI, click the <b>Find</b> menu and then select the
    <b>Find Systems</b> options. <p>After the systems are found, the newly
    created clusters are displayed in CMC.</p><p>The management group name is
    automatically creating using the following pattern:
    <codeph>mg-&lt;resource-prefix></codeph></p><p>The cluster name is
    automatically creating using the following pattern:
    <codeph>cluster-&lt;resource-prefix></codeph></p>
   </li>
    -->
  </procedure>
  </section>
 <section xml:id="cmc">
  <title>Using the CMC Utility</title>
  <para>
   In &kw-hos;, cluster creation using &storevirtual; Centralized Management
   Console (CMC) GUI is deprecated. The clusters created by CMC manually cannot
   be configured using Ansible playbooks.
  </para>
  <para>
   Perform the following steps to create the cluster using CMC.
  </para>
  <section xml:id="launching-cmc-utility">
   <title>Launching the CMC Utility GUI</title>
  <para>
   The &storevirtual; Centralized Management Console (CMC) GUI is an
   interface where you can configure VSA.
  </para>
  <para>
   The CMC utility requires a GUI to access it. You can use either of the
   following methods to launch the CMC GUI.
  </para>
  <itemizedlist>
   <listitem>
    <para>
     <xref linkend="vnc"/>
    </para>
   </listitem>
   <listitem>
    <para>
     <xref linkend="xming"/>
    </para>
   </listitem>
  </itemizedlist>
  </section>
  <section xml:id="vnc">
   <title>RDP/VNC Connection Method</title>
  <para>
   Set up VNC connect on the Controller Node
  </para>
  <para>
   The following steps will allow you to setup a VNC connect to your controller
   node so you can view the GUI.
  </para>
  <procedure>
   <step>
    <para>
     Log in to your first controller node.
    </para>
   </step>
   <step>
    <para>
     Run the following command to install the package that is required to
     launch CMC:
    </para>
    <screen>sudo apt-get install -y xrdp</screen>
   </step>
   <step xml:id="st.vnc.start-vnc4server">
    <para>
     Start <literal>vnc4server</literal> using the instructions below. You will
     be prompted for a password (minimum 6 characters). Enter a password and
     proceed. A sample output is shown below:
    </para>
<screen>stack@helion-cp1-c1-m1-mgmt:~$ vnc4server

You will require a password to access your desktops.

Password:
Verify:
xauth:  file /home/stack/.Xauthority does not exist

New 'helion-cp1-c1-m1-mgmt:3 (stack)' desktop is helion-cp1-c1-m1-mgmt:3

Creating default startup script /home/stack/.vnc/xstartup
Starting applications specified in /home/stack/.vnc/xstartup
Log file is /home/stack/.vnc/helion-cp1-c1-m1-mgmt:3.log</screen>
    <note>
     <para>
      If you use <command>xrdp</command> directly to connect to the first
      controller node without using the VNC server, then a remote session is
      created whenever you login. To avoid this, a dedicated VNC server
      instance is launched and connected to that instance by
      <command>xrdp</command>. This helps to maintain the session.
     </para>
    </note>
   </step>
   <step>
    <para>
     Run <command>netstat -anp | grep vnc</command> to determine the public
     port that VNC is using. In the example below, the port is 5903:
    </para>
<screen>stack@helion-cp1-c1-m1-mgmt:~$ netstat -anp | grep vnc
(Not all processes could be identified, non-owned process info
will not be shown, you would have to be root to see it all.)
tcp        0      0 0.0.0.0:6003            0.0.0.0:*               LISTEN      1413/Xvnc4
tcp6       0      0 :::5903                 :::*                    LISTEN      1413/Xvnc4</screen>
    <note>
     <para>
      If you reboot the controller node, repeat the instructions starting from
      <xref linkend="st.vnc.start-vnc4server"/>.
     </para>
    </note>
   </step>
   <step>
    <para>
     Connect to your controller node through any remote desktop or VNC client.
     We will show the <command>xrdp</command> method first and the VNC method
     is below it:
    </para>
    <substeps>
     <step>
      <para>
       Connecting through remote desktop client
      </para>
      <substeps>
       <step>
        <para>
         Login to your remote desktop. You will be prompted with xrdp login
         screen.
        </para>
        <informalfigure>
         <mediaobject>
          <imageobject role="html">
           <imagedata fileref="media-vsa-xrdp1.png"/>
          </imageobject>
          <imageobject role="fo">
           <imagedata fileref="media-vsa-xrdp1.png" width="75%"/>
          </imageobject>
          <textobject role="description"><phrase></phrase>
          </textobject>
         </mediaobject>
        </informalfigure>
       </step>
       <step>
        <para>
         Click the <guimenu>Module</guimenu> drop-down list and
         select <guimenu>vnc-any</guimenu>.
        </para>
        <informalfigure>
         <mediaobject>
          <imageobject role="html">
           <imagedata fileref="media-vsa-xrdp2.png"/>
          </imageobject>
          <imageobject role="fo">
           <imagedata fileref="media-vsa-xrdp2.png" width="75%"/>
          </imageobject>
          <textobject role="description"><phrase></phrase>
          </textobject>
         </mediaobject>
        </informalfigure>
       </step>
       <step>
        <para>
         Enter the IP address, port and password in the respective fields.
        </para>
        <informalfigure>
         <mediaobject>
          <imageobject role="html">
           <imagedata fileref="media-vsa-xrdp3.png"/>
          </imageobject>
          <imageobject role="fo">
           <imagedata fileref="media-vsa-xrdp3.png" width="75%"/>
          </imageobject>
         </mediaobject>
        </informalfigure>
       </step>
       <step>
        <para>
         Click <emphasis role="bold">Ok</emphasis>.
        </para>
       </step>
      </substeps>
     </step>
     <step>
      <para>
       Connecting through a VNC client, such as
       <link xlink:href="https://www.realvnc.com/download/viewer/">VNC
       Viewer</link>:
      </para>
      <substeps>
       <step>
        <para>
         Enter the IP address and port and click
         <emphasis role="bold">Connect</emphasis>. You will be prompted for
         your password once the connection is established.
        </para>
        <informalfigure>
         <mediaobject>
          <imageobject role="html">
           <imagedata fileref="media-vsa-vncview1.png" format="PNG"/>
          </imageobject>
          <imageobject role="fo">
           <imagedata fileref="media-vsa-vncview1.png" width="75%" format="PNG"/>
          </imageobject>
         </mediaobject>
        </informalfigure>
       </step>
      </substeps>
     </step>
    </substeps>
    <para>
     A terminal emulator will be displayed where you can enter the CMC launch
     command. Note that the CMC launch with this method will have the following
     limitations: by default, CMC-xterm disables all the title bars and
     borders. This is an expected behavior.
    </para>
   </step>
  </procedure>
  </section>
  <section xml:id="xming">
   <title>Install (any) X Display Tool</title>
  <note>
   <para>
    You can use SSH to an X server but the performance may be poor.
   </para>
  </note>
  <para>
   You must configure an X display tool to launch CMC. You can the choose
   any X display tool. In this section we are
   using the tool Xming as an example for launching
   CMC. The following example provides the steps to installing Xming and launching
   CMC in a Windows environment. Another alternative (not shown in the
   documentation) is
   <link xlink:href="http://mobaxterm.mobatek.net/">MobaXterm</link>.
  </para>
  <procedure>
   <step>
    <para>
     Download and install Xming on a Windows
     machine that can access the &lcm;. You can download Xming from
     <link xlink:href="http://sourceforge.net/projects/xming/"/>.
    </para>
   </step>
   <step>
    <para>
     Select <guimenu>Enable X11 forwarding</guimenu> checkbox on
     the PuTTY session for &lcm;. You can do this in PuTTY by:
    </para>
    <substeps>
     <step>
      <para>
       Navigate to the
       <menuchoice><guimenu>Connection</guimenu><guimenu>SSH</guimenu><guimenu>X11</guimenu></menuchoice>
       option in PuTTY.
      </para>
     </step>
     <step>
      <para>
       Make sure <guimenu>Enable X11 forwarding</guimenu> is activated.
      </para>
     </step>
    </substeps>
    <informalfigure>
     <mediaobject>
      <imageobject role="html">
       <imagedata fileref="media-vsa-xming1.png"/>
      </imageobject>
      <imageobject role="fo">
       <imagedata fileref="media-vsa-xming1.png" width="75%"/>
      </imageobject>
     </mediaobject>
    </informalfigure>
   </step>
   <step>
    <para>
     SSH to first control plane node.
    </para>
    <screen>ssh -X</screen>
    <para>
     and enter the CMC command (as mentioned below) to launch CMC.
    </para>
   </step>
  </procedure>
  <procedure xml:id="ol_yln_fhl_jt">
   <step>
    <para>
     Run the following command from your first controller node which will open
     the CMC GUI on your local machine:
    </para>
    <screen>/opt/HP/StoreVirtual/UI/jre/bin/java -jar /opt/HP/StoreVirtual/UI/UI.jar</screen>
    <para>
     By default, the CMC GUI is configured to discover the VSA nodes in the
     subnet in which it is installed. This discovery functionality of VSA nodes
     using the CMC controller node is not supported in &kw-hos;. Instead, you
     must manually add each VSA node, as shown below.
    </para>
   </step>
   <step>
    <para>
     In the CMC GUI, click the <guimenu>Find</guimenu> menu and
     then select the <guimenu>Find Systems</guimenu> options.
    </para>
    <informalfigure>
     <mediaobject>
      <imageobject role="html">
       <imagedata fileref="media-vsa-cmc1.png"/>
      </imageobject>
      <imageobject role="fo">
       <imagedata fileref="media-vsa-cmc1.png" width="75%"/>
      </imageobject>
     </mediaobject>
    </informalfigure>
   </step>
   <step>
    <para>
     Click the <guimenu>Add</guimenu> button which will open the
     <guimenu>Enter IP Address</guimenu> dialogue box where you
     can enter the IP address of your VSA nodes which you noted earlier from
     your
     <filename>~/scratch/ansible/next/my_cloud/stage/info/net_info.yml</filename>
     file.
    </para>
    <informalfigure>
     <mediaobject>
      <imageobject role="html">
       <imagedata fileref="media-vsa-cmc2.png"/>
      </imageobject>
      <imageobject role="fo">
       <imagedata fileref="media-vsa-cmc2.png" width="75%"/>
      </imageobject>
     </mediaobject>
    </informalfigure>
   </step>
   <step>
    <para>
     Once you have all of your VSA nodes entered, click the
     <emphasis role="bold">Close</emphasis> button.
    </para>
    <informalfigure>
     <mediaobject>
      <imageobject role="html">
       <imagedata fileref="media-vsa-cmc3.png"/>
      </imageobject>
      <imageobject role="fo">
       <imagedata fileref="media-vsa-cmc3.png" width="75%"/>
      </imageobject>
      <textobject role="description"><phrase></phrase>
      </textobject>
     </mediaobject>
    </informalfigure>
   </step>
   <step>
    <para>
     Next, click the <guimenu>Tasks</guimenu> menu and then
     navigate to the <guimenu>Management Group</guimenu> submenu
     and select the <guimenu>New Management Group</guimenu>
     option.
    </para>
    <informalfigure>
     <mediaobject>
      <imageobject role="html">
       <imagedata fileref="media-vsa-cmc4.png"/>
      </imageobject>
      <imageobject role="fo">
       <imagedata fileref="media-vsa-cmc4.png" width="75%"/>
      </imageobject>
     </mediaobject>
    </informalfigure>
   </step>
   <step>
    <para>
     In the Management Group wizard, click
     <guimenu>Next</guimenu> and then select
     <guimenu>New Management Group</guimenu> and then
     <guimenu>Next</guimenu> again to continue.
    </para>
    <informalfigure>
     <mediaobject>
      <imageobject role="html">
       <imagedata fileref="media-vsa-cmc5.png"/>
      </imageobject>
      <imageobject role="fo">
       <imagedata fileref="media-vsa-cmc5.png" width="75%"/>
      </imageobject>
     </mediaobject>
    </informalfigure>
   </step>
   <step>
    <para>
     Enter a name in the <guimenu>New Management Group Name</guimenu> field
     and then click <guimenu>Next</guimenu>.
    </para>
    <informalfigure>
     <mediaobject>
      <imageobject role="html">
       <imagedata fileref="media-vsa-cmc6.png"/>
      </imageobject>
      <imageobject role="fo">
       <imagedata fileref="media-vsa-cmc6.png" width="75%"/>
      </imageobject>
     </mediaobject>
    </informalfigure>
   </step>
   <step>
    <para>
     On the <guimenu>Add Administrative User</guimenu>,
     enter a username and password you will use to administer the CMC utility.
    </para>
    <important>
     <para>
      You will need to remember these values as you will input them into your
      <filename>cinder.conf.j2</filename> file later.
     </para>
    </important>
    <informalfigure>
     <mediaobject>
      <imageobject role="html">
       <imagedata fileref="media-vsa-cmc7.png"/>
      </imageobject>
      <imageobject role="fo">
       <imagedata fileref="media-vsa-cmc7.png" width="75%"/>
      </imageobject>
     </mediaobject>
    </informalfigure>
   </step>
   <step>
    <para>
     Click <guimenu>Next</guimenu>to display the
     <guimenu>Management Group Time</guimenu> page.
    </para>
   </step>
   <step>
    <para>
     Add your NTP server information and click
     <emphasis role="bold">Next</emphasis>
    </para>
    <informalfigure>
     <mediaobject>
      <imageobject role="html">
       <imagedata fileref="media-vsa-cmc8.png"/>
      </imageobject>
      <imageobject role="fo">
       <imagedata fileref="media-vsa-cmc8.png" width="75%"/>
      </imageobject>
     </mediaobject>
    </informalfigure>
   </step>
   <step>
    <para>
     Skip the DNS and SMTP sections. To do so, click
     <guimenu>Next</guimenu> and a popup will display where you
     can choose the <guimenu>Accept Incomplete</guimenu> option.
     Repeat this to skip SMTP section as well.
    </para>
    <informalfigure>
     <mediaobject>
      <imageobject role="html">
       <imagedata fileref="media-vsa-cmc9.png"/>
      </imageobject>
      <imageobject role="fo">
       <imagedata fileref="media-vsa-cmc9.png" width="75%"/>
      </imageobject>
      <textobject role="description"><phrase></phrase>
      </textobject>
     </mediaobject>
    </informalfigure>
   </step>
   <step>
    <para>
     On the <guimenu>Create a Cluster</guimenu> options, select
     <guimenu>Standard Cluster</guimenu> from the displayed
     options and click <guimenu>Next</guimenu>.
    </para>
    <informalfigure>
     <mediaobject>
      <imageobject role="html">
       <imagedata fileref="media-vsa-cmc10.png"/>
      </imageobject>
      <imageobject role="fo">
       <imagedata fileref="media-vsa-cmc10.png" width="75%"/>
      </imageobject>
     </mediaobject>
    </informalfigure>
   </step>
   <step>
    <para>
     In the <guimenu>Cluster Name</guimenu> field, enter a name
     for the cluster and click <guimenu>Next</guimenu>.
    </para>
    <informalfigure>
     <mediaobject>
      <imageobject role="html">
       <imagedata fileref="media-vsa-cmc11.png"/>
      </imageobject>
      <imageobject role="fo">
       <imagedata fileref="media-vsa-cmc11.png" width="75%"/>
      </imageobject>
     </mediaobject>
    </informalfigure>
   </step>
   <step>
    <para>
     On the <guimenu>Assign Virtual IPs and Subnet Masks</guimenu> page,
     click <guimenu>Add</guimenu> and
     enter the virtual IP address and subnet mask of the cluster in the
     respective boxes and click <guimenu>OK</guimenu>.
    </para>
    <note>
     <para>
      The virtual IP address will be found as the <option>cluster_ip</option>
      value in your
      <filename>~/scratch/ansible/next/my_cloud/stage/info/net_info.yml</filename>
      file and your subnet mask will be the subnet address from the network
      your VSA nodes are attached to, usually your
      <literal>MANAGEMENT</literal> network.
     </para>
    </note>
    <informalfigure>
     <mediaobject>
      <imageobject role="html">
       <imagedata fileref="media-vsa-cmc12.png"/>
      </imageobject>
      <imageobject role="fo">
       <imagedata fileref="media-vsa-cmc12.png" width="75%"/>
      </imageobject>
     </mediaobject>
    </informalfigure>
   </step>
   <step>
    <para>
     The CMC utility will verify the virtual IP address information and then
     you can click the <guimenu>Next</guimenu> button.
    </para>
   </step>
   <step>
    <para>
     Select the checkbox for <guimenu>Skip Volume Creation</guimenu> and click
     the <guimenu>Finish</guimenu>
     button which will display your VSA management cluster.
    </para>
    <informalfigure>
     <mediaobject>
      <imageobject role="html">
       <imagedata fileref="media-vsa-cmc13.png"/>
      </imageobject>
      <imageobject role="fo">
       <imagedata fileref="media-vsa-cmc13.png" width="75%"/>
      </imageobject>
     </mediaobject>
    </informalfigure>
    <para>
     Cluster creation using CMC takes approximately 10 minutes or longer.
    </para>
    <tip>
     <para>
      You may get a pop-up notice telling you that your hostnames are not
      unique. This can be ignored by clicking the OK button.
     </para>
    </tip>
   </step>
   <step>
    <para>
     If this process is successful you will see a summary page at the end which
     outlines what you have completed.
    </para>
    <important>
     <para>
      You can create more than one VSA cluster of same or different type by
      specifying the configuration in cloud model. For more details, refer to
      Modifying Cloud Model to Create Multiple Clusters
     </para>
    </important>
   </step>
  </procedure>
  </section>
 </section>
 <section>
  <title>Configure VSA as the Backend</title>
  <para>
   You will use the information you input to the CMC utility to configure your
   Cinder backend to use your VSA environment.
  </para>
  <para>
   To update your Cinder configuration to add VSA storage you must modify the
   <filename>~/helion/my_cloud/config/cinder/cinder.conf.j2</filename> file on
   your &lcm; as follows:
  </para>
  <procedure>
   <step>
    <para>
     Log in to the &lcm;.
    </para>
   </step>
   <step>
    <para>
     Make the following changes to the
     <filename>~/helion/my_cloud/config/cinder/cinder.conf.j2</filename> file:
    </para>
    <substeps>
     <step>
      <para>
       Add your VSA backend to the <literal>enabled_backends</literal> section:
      </para>
<screen># Configure the enabled backends
enabled_backends=vsa-1</screen>
      <important>
       <para>
        If you are using multiple backend types, you can use a comma delimited
        list here. For example, if you are going to use both VSA and Ceph
        backends, you would specify something like this:
        <literal>enabled_backends=vsa-1,ceph1</literal>.
       </para>
      </important>
     </step>
     <step>
      <para>
       [OPTIONAL] If you want your volumes to use a default volume type, then
       enter the name of the volume type in the <literal>[DEFAULT]</literal>
       section with the syntax below.
       <emphasis role="bold">Remember this value when you create your volume
       type in the next section.</emphasis>
      </para>
      <important>
       <para>
        If you do not specify a default type then your volumes will default to
        a non-redundant RAID configuration. It is recommended that you create a
        volume type and specify it here that meets your environments needs.
       </para>
      </important>
<screen>[DEFAULT]
# Set the default volume type
default_volume_type = &lt;your new volume type&gt;</screen>
     </step>
     <step>
      <para>
       Uncomment the <literal>StoreVirtual (VSA) cluster</literal> section and
       fill the values as per your cluster information. If you have more than
       one cluster, you will need to add another similar section with its
       respective values. In the following example only one cluster is added.
      </para>
<screen>[vsa-1]
hplefthand_password: &lt;vsa-cluster-password&gt;
hplefthand_clustername: &lt;vsa-cluster-name&gt;
hplefthand_api_url: https://&lt;vsa-cluster-vip&gt;:8081/lhos
hplefthand_username: &lt;vsa-cluster-username&gt;
hplefthand_iscsi_chap_enabled: false
volume_backend_name: &lt;vsa-backend-name&gt;
volume_driver: cinder.volume.drivers.san.hp.hp_lefthand_iscsi.HPLeftHandISCSIDriver
hplefthand_debug: false</screen>
      <para>
       where:
      </para>
      <informaltable>
       <tgroup cols="2">
        <colspec colname="c1" colnum="1"/>
        <colspec colname="c2" colnum="2"/>
        <thead>
         <row>
          <entry>Value</entry>
          <entry>Description</entry>
         </row>
        </thead>
        <tbody>
         <row>
          <entry>
           <para>
            hplefthand_password
           </para>
          </entry>
          <entry>
           <para>
            Password entered during cluster creation in the CMC utility. If you
            have chosen to encrypt this password, enter the value in this
            format:
           </para>
<screen>hplefthand_password: {{ '&lt;encrypted vsa-cluster-password&gt;' | hos_user_password_decrypt }}</screen>
           <para>
            See
            <!-- FIXME:
            <xref href="../security/encrypted_storage.xml">Encryption of
            Passwords and Sensitive Data</xref> -->
            for more details.
           </para>
          </entry>
         </row>
         <row>
          <entry>
           <para>
            hplefthand_clustername
           </para>
          </entry>
          <entry>
           <para>
            Name of the VSA cluster provided while creating a cluster in the
            CMC utility.
           </para>
          </entry>
         </row>
         <row>
          <entry>
           <para>
            hplefthand_api_url
           </para>
          </entry>
          <entry>
           <para>
            Virtual IP address of your VSA cluster, found in your
            <literal>~/scratch/ansible/next/my_cloud/stage/info/net_info.yml</literal>
            file.
           </para>
          </entry>
         </row>
         <row>
          <entry>
           <para>
            hplefthand_username
           </para>
          </entry>
          <entry>
           <para>
            Username given during cluster creation in the CMC utility.
           </para>
          </entry>
         </row>
         <row>
          <entry>
           <para>
            hplefthand_iscsi_chap_enabled
           </para>
          </entry>
          <entry>
           <para>
            If you set this option as <literal>true</literal>
            then the hosts will not be able to access the storage without the
            generated secrets. And if you set this option as
            <literal>false</literal> then no CHAP authentication
            is required for the ISCSI connection.
           </para>
          </entry>
         </row>
         <row>
          <entry>
           <para>
            volume_backend_name
           </para>
          </entry>
          <entry>
           <para>
            Name given to the VSA backend. You will specify this value later in
            the Associate the Volume Type to a Backend steps.
           </para>
          </entry>
         </row>
         <row>
          <entry>
           <para>
            volume_driver
           </para>
          </entry>
          <entry>
           <para>
            Cinder volume driver. Leave this as the default value for VSA.
           </para>
          </entry>
         </row>
         <row>
          <entry>
           <para>
            hplefthand_debug
           </para>
          </entry>
          <entry>
           <para>
            If you set this option as true then the Cinder driver for the VSA
            will generate logging in debug mode; these logging entries can be
            found in <filename>cinder-volume.log</filename>.
           </para>
          </entry>
         </row>
        </tbody>
       </tgroup>
      </informaltable>
      <para>
       [OPTIONAL] &kw-hos-phrase; supports VSA deployment for KVM hypervisor
       only but it can be used as pre-deployed (or out of the band deployed)
       Lefthand storage boxes or VSA appliances (running on ESX/hyper-v/KVM
       hypervisor). It also supports Cinder configuration of physical Lefthand
       storage device and VSA appliances. Depending upon your setup, you will
       have to edit the below section if your storage array is running LeftHand
       OS lower than version 11:
      </para>
<screen>[&lt;unique-section-name&gt;]
volume_driver=cinder.volume.drivers.san.hp.hp_lefthand_iscsi.HPLeftHandISCSIDriver
volume_backend_name=lefthand-cliq
san_ip=&lt;san-ip&gt;
san_login=&lt;san_username&gt;
If adding a password here, then the password can be encrypted using the
mechanism specified in the documentation. If the password has been encrypted
add the value and the hos_user_password_decrypt filter like so:
san_password= {{ '&lt;encrypted san_password&gt;' | hos_user_password_decrypt }}
Note that the encrypted value has to be enclosed in quotes
If you choose not to encrypt the password then the unencrypted password
must be set as follows:
san_password=&lt;san_password&gt;
san_ssh_port=16022
san_clustername=&lt;vsa-cluster-name&gt;
volume_backend_name=&lt;vsa-backend-name&gt;</screen>
      <important>
       <para>
        Similar to your <literal>hplefthand_password</literal> in the previous
        example, encryption for your <literal>san_password</literal> is
        supported. If you chose to use encryption you would use the syntax
        below to express that:
       </para>
<screen>san_password= {{ '&lt;encrypted san_password&gt;' | hos_user_password_decrypt }}</screen>
       <para>
        See
        <!-- FIXME: <xref href="../security/encrypted_storage.xml">Encryption of Passwords and Sensitive Data</xref> -->
        for more details.
       </para>
      </important>
     </step>
    </substeps>
    <important>
     <para>
      Do not use <literal>backend_host</literal> variable in
      <filename>cinder.conf</filename> file. If <literal>backend_host</literal>
      is set, it will override the [DEFAULT]/host value which &kw-hos-phrase;
      is dependent on.
     </para>
    </important>
   </step>
   <step>
    <para>
     Commit your configuration to a local repository (<xref linkend="using_git"/>):
    </para>
<screen>cd ~/helion/hos/ansible
git add -A
git commit -m "configured VSA backend"</screen>
    <note>
     <para>
      Before you run any playbooks, remember that you need to export the
      encryption key in the following environment variable:
     </para>
     <screen>export HOS_USER_PASSWORD_ENCRYPT_KEY=<replaceable>ENCRYPTION_KEY</replaceable></screen>
     <para>
      For more information, see <xref linkend="install_kvm"/>.
     </para>
    </note>
   </step>
   <step>
    <para>
     Run the configuration processor:
    </para>
<screen>cd ~/helion/hos/ansible
ansible-playbook -i hosts/localhost config-processor-run.yml</screen>
   </step>
   <step>
    <para>
     Run the following command to create a deployment directory:
    </para>
<screen>cd ~/helion/hos/ansible
ansible-playbook -i hosts/localhost ready-deployment.yml</screen>
   </step>
   <step>
    <para>
     Run the Cinder Reconfigure Playbook:
    </para>
<screen>cd ~/scratch/ansible/next/hos/ansible
ansible-playbook -i hosts/verb_hosts cinder-reconfigure.yml</screen>
   </step>
  </procedure>
 </section>
 <section>
  <title>Post-Installation Tasks</title>
  <para>
   After you have configured VSA as your Block Storage backend, here are some
   tasks you will want to complete:
  </para>
  <itemizedlist>
   <listitem>
    <para>
     <!-- FIXME: <xref href="../operations/blockstorage/creating_voltype.xml">Create a Volume Type for your Volumes</xref> -->
    </para>
   </listitem>
   <listitem>
    <para>
     <xref linkend="sec.verify_block_storage.volume"/>
    </para>
   </listitem>
  </itemizedlist>
 </section>
</section>
