<?xml version="1.0"?>
<!DOCTYPE section [
 <!ENTITY % entities SYSTEM "entities.ent"> %entities;
]>
<section xml:id="config_vsa"
 xmlns="http://docbook.org/ns/docbook" version="5.1"
 xmlns:xi="http://www.w3.org/2001/XInclude"
 xmlns:xlink="http://www.w3.org/1999/xlink">
 <title>Configuring for VSA Block Storage Backend</title>
 <bridgehead renderas="sect4">Prerequisites</bridgehead>
 <itemizedlist>
  <listitem>
   <para>
    Your &kw-hos; cloud must be fully deployed using the a KVM cloud model with
    VSA added. The Entry-Scale KVM with VSA cloud model is an example of such a
    model. For more details on the installation refer to
    <xref linkend="install_kvm"/>.
   </para>
  </listitem>
  <listitem>
   <para>
    It's important that all of your systems have the correct date/time because
    the HPE StoreVirtual VSA license has a start date. If the start date is
    later than the system time then the installation will fail.
   </para>
  </listitem>
 </itemizedlist>
 <bridgehead renderas="sect4">Notes</bridgehead>
 <itemizedlist xml:id="ul_kg4_y3k_5v">
  <listitem>
   <para>
    The license for the HPE StoreVirtual VSA license is bundled with &kw-hos;
    and comes with a free trial which allows a maximum limit of 50 TB per node.
    Hence the total amount of the configured storage on an individual VSA node
    should not exceed 50 TB. To extend the 50 TB per node limit, you can add
    nodes. A VSA cluster can support up to 16 nodes, which means configured
    storage on a VSA cluster can be as much as 800 TB.
   </para>
   <para>
    You can deploy VSA with Adaptive Optimization (AO) or without. The
    deployment process for each of these options is similar, you just need to
    make a change in the disk input model. For more detailed information, refer
    to <emphasis role="bold">VSA with or without Adaptive Optimization
    (AO)</emphasis> in <xref linkend="modify_entryscale_kvm_vsa"/>.
   </para>
  </listitem>
  <listitem>
   <para>
    A single VSA node can have a maximum of seven raw disks (excluding the
    operating system disks) attached to it, which is defined in the disk input
    model for your VSA nodes. It is expected that no more than seven disks are
    specified (including Adaptive Optimization disks) per VSA node. For
    example, if you want to deploy VSA with two disks for Adaptive Optimization
    then your disk input model should not specify more than five raw disks for
    data and two raw disks for Adaptive Optimization. Exceeding the disk limit
    causes VSA deployment failure.
   </para>
  </listitem>
  <listitem>
   <para>
    You can create more than one VSA cluster of same or different type by
    specifying the configuration in cloud model. For more details, refer to
    <citetitle>FIXME: broken external xref</citetitle>.
   </para>
  </listitem>
  <listitem>
   <para>
    Usernames and passwords designated during VSA configuration or repair must
    be alphabetic only.
   </para>
  </listitem>
 </itemizedlist>
 <para>
  <emphasis role="bold">Concerning using multiple backends:</emphasis> If you
  are using multiple backend options, ensure that you specify each of the
  backends you are using when configuring your
  <literal>cinder.conf.j2</literal> file using a comma delimited list. An
  example would be <literal>enabled_backends=vsa-1,3par_iSCSI,ceph1</literal>
  and is included in the steps below. You will also want to create multiple
  volume types so you can specify which backend you want to utilize when
  creating volumes. These instructions are included below as well. In addition
  to our documentation, you can also read the OpenStack documentation at
  <link xlink:href="http://docs.openstack.org/admin-guide/blockstorage_multi_backend.html">Configure
  multiple storage backends</link> as well.
 </para>
 <para>
  <emphasis role="bold">VSA driver has updated name:</emphasis> In the
  OpenStack Mitaka release, the VSA driver used for HPE Helion integration had
  its name updated from <literal>hplefthand_</literal> to
  <literal>hpelefthand_</literal>. To prevent issues when upgrading from
  previous &kw-hos; releases, we left the names as-is in the release and
  provided a mapping so that the integration would continue to work. This will
  produce a warning in the <literal>cinder-volume.log</literal> file advising
  you of the deprecated name. The warning will look similar to this:
 </para>
<screen>Option "hplefthand_username" from group "&lt;your section&gt;" is deprecated. Use option "hpelefthand_username" from group "&lt;your section&gt;"
should be ignored.</screen>
 <para>
  These are just warnings and can be ignored.
 </para>
 <bridgehead renderas="sect4">Configure HPE StoreVirtual VSA</bridgehead>
 <para>
  The process for configuring HPE StoreVirtual VSA for &kw-hos; involves two
  major steps:
 </para>
 <itemizedlist xml:id="ul_kbm_n3k_5v">
  <listitem>
   <para>
    Creating a cluster
   </para>
  </listitem>
  <listitem>
   <para>
    Configuring VSA as the block storage backend
   </para>
  </listitem>
 </itemizedlist>
 <para>
  You can perform these tasks in an automated process using the provided
  Ansible playbooks or manually using the HPE StoreVirtual Centralized
  Management Console (CMC) GUI.
 </para>
 <note>
  <para>
   Using Ansible playbooks to create clusters is strongly recommended. Cluster
   creation using HPE StoreVirtual Centralized Management Console (CMC) GUI is
   deprecated in &kw-hos-phrase;. Clusters created using CMC manually cannot be
   reconfigured using the Ansible playbooks. If you used CMC to create the
   clusters, you must continue to use CMC to administer those clusters.
  </para>
 </note>
 <bridgehead renderas="sect4">Using Ansible</bridgehead>
 <para>
  Perform the following steps to create the cluster using the provided Ansible
  playbooks.
 </para>
 <orderedlist>
  <listitem>
   <para>
    Log in to the lifecycle manager.
   </para>
  </listitem>
  <listitem>
   <para>
    Run the following playbook, which will both create your management group
    and your cluster:
   </para>
<screen>cd ~/scratch/ansible/next/hos/ansible/
ansible-playbook -i hosts/verb_hosts vsalm-configure-cluster.yml</screen>
  </listitem>
  <listitem>
   <para>
    When prompted, enter an administrative user name and password you will use
    to administer the CMC utility.
   </para>
   <para>
    Administrative user naming requirements:
   </para>
   <itemizedlist>
    <listitem>
     <para>
      3 to 30 characters
     </para>
    </listitem>
    <listitem>
     <para>
      Must begin with a letter
     </para>
    </listitem>
    <listitem>
     <para>
      Allowed characters: 0-9, a-z, A-Z, hyphen (-), underline (_)
     </para>
    </listitem>
    <listitem>
     <para>
      Disallowed characters: Multibyte character set
     </para>
    </listitem>
   </itemizedlist>
   <para>
    Administrative user password requirements:
   </para>
   <itemizedlist>
    <listitem>
     <para>
      5 to 40 characters
     </para>
    </listitem>
    <listitem>
     <para>
      Must begin with a letter
     </para>
    </listitem>
    <listitem>
     <para>
      Many ASCII and extended ASCII characters, Multibyte character set
     </para>
    </listitem>
    <listitem>
     <para>
      Disallowed characters: dot (.), colon (:), forward slash (/), comma (,),
      backward slash (\), semi-colon (;), single-quote (â€˜), space ( ), equals
      (=)
     </para>
    </listitem>
   </itemizedlist>
   <important>
    <para>
     You will need to remember these values to access the cluster using the CMC
     utility or to re-run this playbook later if further management of your VSA
     system is needed. Do not change the credentials for one or more clusters
     using CMC as it will make the cluster automation playbook ignore those
     clusters for reconfiguration.
    </para>
   </important>
  </listitem>
  <listitem>
   <para>
    In the output of the Ansible playbook you will see different steps where
    you can locate the values for your VSA cluster name, the virtual IP
    address, and the IP addresses of each of the VSA hosts in the cluster. You
    will use these values later when configuring your backend.
   </para>
   <para>
    The following is an example, with the important information bolded.
   </para>
   <para>
    This step displays the VSA cluster name and the virtual IP address of the
    cluster:
   </para>
<screen>TASK: [vsalm | _create_cluster | Display the status of cluster creation performed in above step] ***
ok: [helion-cp1-vsa0001-mgmt] =&gt; {
    "msg": "Created cluster - Name=<emphasis role="bold">cluster-vsa</emphasis>, IP=<emphasis role="bold">10.13.111.142</emphasis>"
}</screen>
  </listitem>
  <listitem>
   <para>
    You can view the added node to the cluster in CMC GUI as follows:
   </para>
   <orderedlist xml:id="ol_r5f_ll3_wv">
    <listitem>
     <para>
      Launch the CMC utility. See <link xlink:href="#config_vsa/cmc">Launching
      the CMC utility GUI</link> for more details.
     </para>
    </listitem>
    <listitem>
     <para>
      Find the new VSA system.
     </para>
    </listitem>
    <listitem>
     <para>
      View the added node in the cluster.
     </para>
    </listitem>
   </orderedlist>
  </listitem>
<!--<li>Run the following command from your first controller node which will open the HPE
          StoreVirtual Centralized Management Console (CMC) GUI on your local machine:
          <codeblock>/opt/HP/StoreVirtual/UI/jre/bin/java -jar /opt/HP/StoreVirtual/UI/UI.jar</codeblock></li>
        <li>In the CMC GUI, click the Find menu and then select the Find Systems options.Once
          systems are found, new created clusters can been observed in CMC.</li>
        <li>In the CMC GUI, click the <b>Find</b> menu and then select the <b>Find Systems</b>
          options. <p>After the systems are found, the newly created clusters are displayed in
            CMC.</p><p>The management group name is automatically creating using the following
            pattern: <codeph>mg-&lt;resource-prefix></codeph></p><p>The cluster name is
            automatically creating using the following pattern:
              <codeph>cluster-&lt;resource-prefix></codeph></p></li>-->
 </orderedlist>
 <formalpara xml:id="cmc">
  <title>Using the CMC Utility</title>
 </formalpara>
 <bridgehead renderas="sect4">Configure VSA as the Backend</bridgehead>
 <para>
  You will use the information you input to the CMC utility to configure your
  Cinder backend to use your VSA environment.
 </para>
 <para>
  To update your Cinder configuration to add VSA storage you must modify the
  <literal>~/helion/my_cloud/config/cinder/cinder.conf.j2</literal> file on
  your lifecycle manager as follows:
 </para>
 <orderedlist>
  <listitem>
   <para>
    Log in to the lifecycle manager.
   </para>
  </listitem>
  <listitem>
   <para>
    Make the following changes to the
    <literal>~/helion/my_cloud/config/cinder/cinder.conf.j2</literal> file:
   </para>
   <orderedlist>
    <listitem>
     <para>
      Add your VSA backend to the <literal>enabled_backends</literal> section:
     </para>
<screen># Configure the enabled backends
enabled_backends=vsa-1</screen>
     <important>
      <para>
       If you are using multiple backend types, you can use a comma delimited
       list here. For example, if you are going to use both VSA and Ceph
       backends, you would specify something like this:
       <literal>enabled_backends=vsa-1,ceph1</literal>.
      </para>
     </important>
    </listitem>
    <listitem>
     <para>
      [OPTIONAL] If you want your volumes to use a default volume type, then
      enter the name of the volume type in the <literal>[DEFAULT]</literal>
      section with the syntax below. <emphasis role="bold">You will want to
      remember this value when you create your volume type in the next
      section.</emphasis>
     </para>
     <important>
      <para>
       If you do not specify a default type then your volumes will default to a
       non-redundant RAID configuration. It is recommended that you create a
       volume type and specify it here that meets your environments needs.
      </para>
     </important>
<screen>[DEFAULT]
# Set the default volume type
default_volume_type = &lt;your new volume type&gt;</screen>
    </listitem>
    <listitem>
     <para>
      Uncomment the <literal>StoreVirtual (VSA) cluster</literal> section and
      fill the values as per your cluster information. If you have more than
      one cluster, you will need to add another similar section with its
      respective values. In the following example only one cluster is added.
     </para>
<screen>[vsa-1]
hplefthand_password: &lt;vsa-cluster-password&gt;
hplefthand_clustername: &lt;vsa-cluster-name&gt;
hplefthand_api_url: https://&lt;vsa-cluster-vip&gt;:8081/lhos
hplefthand_username: &lt;vsa-cluster-username&gt;
hplefthand_iscsi_chap_enabled: false
volume_backend_name: &lt;vsa-backend-name&gt;
volume_driver: cinder.volume.drivers.san.hp.hp_lefthand_iscsi.HPLeftHandISCSIDriver
hplefthand_debug: false</screen>
     <para>
      where:
     </para>
     <informaltable xml:id="idg-installation-installation-configure_vsa-xml-33" colsep="1" rowsep="1">
      <tgroup cols="2">
       <colspec colname="c1" colnum="1"/>
       <colspec colname="c2" colnum="2"/>
       <thead>
        <row>
         <entry>Value</entry>
         <entry>Description</entry>
        </row>
       </thead>
       <tbody>
        <row>
         <entry>hplefthand_password</entry>
         <entry>Password entered during cluster creation in the CMC utility. If you
                        have chosen to encrypt this password, enter the value in this format: <screen>hplefthand_password: {{ '&lt;encrypted vsa-cluster-password&gt;' | hos_user_password_decrypt }}</screen>
          <para>
           See <citetitle>FIXME: broken external xref</citetitle> for more
           details.
          </para>
         </entry>
        </row>
        <row>
         <entry>hplefthand_clustername</entry>
         <entry>Name of the VSA cluster provided while creating a cluster in the CMC
                        utility.</entry>
        </row>
        <row>
         <entry>hplefthand_api_url</entry>
         <entry>Virtual IP address of your VSA cluster, found in your
                          <literal>~/scratch/ansible/next/my_cloud/stage/info/net_info.yml</literal>
                        file.</entry>
        </row>
        <row>
         <entry>hplefthand_username</entry>
         <entry>Username given during cluster creation in the CMC utility.</entry>
        </row>
        <row>
         <entry>hplefthand_iscsi_chap_enabled</entry>
         <entry>If you set this option as <emphasis role="bold">true</emphasis> then the hosts will not be able
                        to access the storage without the generated secrets. And if you set this
                        option as <emphasis role="bold">false</emphasis> then no CHAP authentication is required for the ISCSI
                        connection.</entry>
        </row>
        <row>
         <entry>volume_backend_name</entry>
         <entry>Name given to the VSA backend. You will specify this value later in the
                        Associate the Volume Type to a Backend steps.</entry>
        </row>
        <row>
         <entry>volume_driver</entry>
         <entry>Cinder volume driver. Leave this as the default value for VSA.</entry>
        </row>
        <row>
         <entry>hplefthand_debug</entry>
         <entry>If you set this option as true then the Cinder driver for the VSA will
                        generate logging in debug mode; these logging entries can be found in
                          <emphasis role="bold">cinder-volume.log</emphasis>.</entry>
        </row>
       </tbody>
      </tgroup>
     </informaltable>
     <para>
      [OPTIONAL] &kw-hos-phrase; supports VSA deployment for KVM hypervisor
      only but it can be used as pre-deployed (or out of the band deployed)
      Lefthand storage boxes or VSA appliances (running on ESX/hyper-v/KVM
      hypervisor). It also supports Cinder configuration of physical Lefthand
      storage device and VSA appliances. Depending upon your setup, you will
      have to edit the below section if your storage array is running LeftHand
      OS lower than version 11:
     </para>
<screen>[&lt;unique-section-name&gt;]
volume_driver=cinder.volume.drivers.san.hp.hp_lefthand_iscsi.HPLeftHandISCSIDriver
volume_backend_name=lefthand-cliq
san_ip=&lt;san-ip&gt;
san_login=&lt;san_username&gt;
If adding a password here, then the password can be encrypted using the
mechanism specified in the documentation. If the password has been encrypted
add the value and the hos_user_password_decrypt filter like so:
san_password= {{ '&lt;encrypted san_password&gt;' | hos_user_password_decrypt }}
Note that the encrypted value has to be enclosed in quotes
If you choose not to encrypt the password then the unencrypted password
must be set as follows:
san_password=&lt;san_password&gt;
san_ssh_port=16022
san_clustername=&lt;vsa-cluster-name&gt;
volume_backend_name=&lt;vsa-backend-name&gt;</screen>
     <important>
      <para>
       Similar to your <literal>hplefthand_password</literal> in the previous
       example, encryption for your <literal>san_password</literal> is
       supported. If you chose to use encryption you would use the syntax below
       to express that:
      </para>
<screen>san_password= {{ '&lt;encrypted san_password&gt;' | hos_user_password_decrypt }}</screen>
      <para>
       See <citetitle>FIXME: broken external xref</citetitle> for more details.
      </para>
     </important>
    </listitem>
   </orderedlist>
   <important>
    <para>
     Do not use <literal>backend_host</literal> variable in
     <literal>cinder.conf</literal> file. If <literal>backend_host</literal> is
     set, it will override the [DEFAULT]/host value which &kw-hos-phrase; is
     dependent on.
    </para>
   </important>
  </listitem>
  <listitem>
   <para>
    Commit your configuration to a <xref linkend="using_git"/>:
   </para>
<screen>cd ~/helion/hos/ansible
git add -A
git commit -m "configured VSA backend"</screen>
   <note>
    <para>
     Before you run any playbooks, remember that you need to export the
     encryption key in the following environment variable:<literal> export
     HOS_USER_PASSWORD_ENCRYPT_KEY=&lt;encryption key&gt;</literal> See
     <xref linkend="install_kvm"/> for reference.
    </para>
   </note>
  </listitem>
  <listitem>
   <para>
    Run the configuration processor:
   </para>
<screen>cd ~/helion/hos/ansible
ansible-playbook -i hosts/localhost config-processor-run.yml</screen>
  </listitem>
  <listitem>
   <para>
    Run the following command to create a deployment directory:
   </para>
<screen>cd ~/helion/hos/ansible
ansible-playbook -i hosts/localhost ready-deployment.yml</screen>
  </listitem>
  <listitem>
   <para>
    Run the Cinder Reconfigure Playbook:
   </para>
<screen>cd ~/scratch/ansible/next/hos/ansible
ansible-playbook -i hosts/verb_hosts cinder-reconfigure.yml</screen>
  </listitem>
 </orderedlist>
 <bridgehead renderas="sect4">Post-Installation Tasks</bridgehead>
 <para>
  After you have configured VSA as your Block Storage backend, here are some
  tasks you will want to complete:
 </para>
 <itemizedlist>
  <listitem/>
  <listitem/>
 </itemizedlist>
</section>
