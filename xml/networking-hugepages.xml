<?xml version="1.0"?>
<?xml-stylesheet href="urn:x-suse:xslt:profiling:docbook51-profile.xsl"
 type="text/xml"
 title="Profiling step"?>
<!DOCTYPE section [
 <!ENTITY % entities SYSTEM "entity-decl.ent"> %entities;
]>
<section xmlns="http://docbook.org/ns/docbook" xmlns:xi="http://www.w3.org/2001/XInclude" xmlns:xlink="http://www.w3.org/1999/xlink" xml:id="hugepages">
 <title>Configuring Hugepages for DPDK in Neutron Networks</title>
 <para>
  To take advantage of DPDK and its network
  performance enhancements, enable hugepages first.
 </para>
 <para>
  With hugepages, physical RAM is reserved at boot time and dedicated to a
  virtual machine. Only that virtual machine and Open vSwitch can use this
  specifically allocated RAM. The host OS cannot access it. This memory is
  contiguous, and because of its larger size, reduces the number of entries in
  the memory map and number of times it must be read.
 </para>
 <para>
  The hugepage reservation is made in <literal>/etc/default/grub</literal>,
  but this is handled by the &lcm;.
 </para>
 <para>
  In addition to hugepages, to use DPDK, CPU isolation is required. This is
  achieved with the 'isolcups' command in
  <literal>/etc/default/grub</literal>, but this is also managed by the
  &lcm; using a new input model file.
 </para>
 <para>
  The two new input model files introduced with this release to help you
  configure the necessary settings and persist them are:
 </para>
 <itemizedlist>
  <listitem>
   <para>
    memory_models.yml (for hugepages)
   </para>
  </listitem>
  <listitem>
   <para>
    cpu_models.yml (for CPU isolation)
   </para>
  </listitem>
 </itemizedlist>
 <section>
  <title>memory_models.yml</title>
  <para>
   In this file you set your huge page size along with the number of such
   huge-page allocations.
  </para>
<screen> ---
  product:
    version: 2

  memory-models:
    - name: COMPUTE-MEMORY-NUMA
      default-huge-page-size: 1G
      huge-pages:
        - size: 1G
          count: 24
          numa-node: 0
        - size: 1G
          count: 24
          numa-node: 1
        - size: 1G
          count: 48</screen>
 </section>
 <section>
  <title>cpu_models.yml</title>
<screen>---
  product:
    version: 2

  cpu-models:

    - name: COMPUTE-CPU
      assignments:
       - components:
           - nova-compute-kvm
         cpu:
           - processor-ids: 3-5,12-17
             role: vm

       - components:
           - openvswitch
         cpu:
           - processor-ids: 0
             role: eal
           - processor-ids: 1-2
             role: pmd</screen>
 </section>
 <section>
  <title>NUMA memory allocation</title>
  <para>
   As mentioned above, the memory used for hugepages is locked down at boot
   time by an entry in <literal>/etc/default/grub</literal>. As an admin, you
   can specify in the input model how to arrange this memory on NUMA nodes. It
   can be spread across NUMA nodes or you can specify where you want it. For
   example, if you have only one NIC, you would probably want all the hugepages
   memory to be on the NUMA node closest to that NIC.
<!-- Commented in DITA original: -->
<!-- Input model looks something like:
product:       version: 2              memory-models:              - name:
COMPUTE-MEMORY-NUMA       default-huge-page-size: 1G       huge-pages:       - size: 1G
count: 24       numa-node: 0       - size: 1G       count: 24       numa-node: 1       - size:
1G       count: 48-->
  </para>
  <para>
   If you do not specify the <literal>numa-node</literal> settings in the
   <literal>memory_models.yml</literal> input model file and use only the last
   entry indicating "size: 1G" and "count: 48" then this memory is spread
   evenly across all NUMA nodes.
  </para>
  <para>
   Also note that the hugepage service runs once at boot time and then goes to
   an inactive state so you should not expect to see it running. If you decide
   to make changes to the NUMA memory allocation, you will need to reboot the
   compute node for the changes to take effect.
  </para>
 </section>
</section>
