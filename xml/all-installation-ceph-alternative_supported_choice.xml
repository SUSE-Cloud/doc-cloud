<?xml version="1.0"?>
<!DOCTYPE section [
 <!ENTITY % entities SYSTEM "entities.ent"> %entities;
]>
<section xmlns="http://docbook.org/ns/docbook" xmlns:xi="http://www.w3.org/2001/XInclude" xmlns:xlink="http://www.w3.org/1999/xlink" xml:id="idg-all-installation-ceph-alternative_supported_choice-xml-1"><title>&kw-hos-tm; &kw-hos-version-50;: Alternative Supported Choices</title><abstract><para><para>This section provides insight on how to alter the
                <literal>entry-scale-kvm-ceph</literal> input model to deploy Ceph with various
            supported options. We recommend that you deploye your supported choice only after
            evaluating all pros and cons.</para></para>
</abstract>
        <!---->


        <para>This section provides insight on how to alter the <literal>entry-scale-kvm-ceph</literal>
            input model to deploy Ceph with various supported options. We recommend that you deploye
            your supported choice only after evaluating all pros and cons. For technical details,
            please consult with the technical support team. The choices available can impact the
            performance and scaling of clusters. Choices are illustrated for reference purposes and
            you can combine one or more of them as needed. The content is categorized as
            follows:</para>


        <orderedlist xml:id="ol_sqb_mrg_kw">
                <listitem><para>Core Ceph</para>
<itemizedlist xml:id="ul_ejq_yrg_kw">
                        <listitem><para><link xlink:href="#config_ceph/deploying-monitor-on-standalone-node">Installing the Monitor Service on Standalone
                                Nodes</link></para>
</listitem>
                        <listitem><para><link xlink:href="#config_ceph/single-vlan-for-all-ceph-traffic">Using a Single VLAN for All Ceph Traffic (Management, Client, and
                                Internal OSD)</link></para>
</listitem>
                        <listitem><para><link xlink:href="#config_ceph/using-two-vlan">Using Two VLANs:
                                For Management and Client Traffic and for Internal OSD`
                                Traffic</link></para>
</listitem>
                    </itemizedlist></listitem>
                <listitem><para>RADOS Gateway</para>
<itemizedlist xml:id="ul_ptr_frt_mx">
                        <listitem><para><link xlink:href="#config_ceph/install-rados-gateway-on-cluster-node-that-host-ceph-monitor">Installing RADOS Gateway on Dedicated Cluster Nodes
                                that Host the Ceph Monitor Service</link></para>
</listitem>
                        <listitem><para><link xlink:href="#config_ceph/install-rados-gateway-on-controller-nodes">Installing RADOS Gateway on Controller
                            Nodes</link></para>
</listitem>
                        <listitem><para><link xlink:href="#config_ceph/install-more-two-rados-gateway-servers">Installing More than Two RADOS Gateway
                            Servers</link></para>
</listitem>
                    </itemizedlist></listitem>
                <listitem userlevel="INTERNAL"><para><link xlink:href="#config_ceph/ceph-deployment-vcp">Ceph Deployment with
                        Virtual Control Plane </link></para>
</listitem>
            </orderedlist>

        <formalpara><title>Core Ceph</title></formalpara>
        <sidebar xml:id="deploying-monitor-on-standalone-node"><para><emphasis role="bold">Installing the Monitor Service on
                Standalone Nodes</emphasis></para>
<para>The following section provides the procedure for installing the monitor service on
                standalone nodes instead of installing on controller nodes, as mentioned in
                    <literal>entry-scale-kvm-ceph</literal>.</para>
<important><para>If you want to install the monitor service as a dedicated
                    resource node, you must decide before deploying Ceph. &kw-hos-phrase; does not support deployment transition. After Ceph
                    is deployed, you cannot migrate the monitor service from controller nodes to
                    dedicated resource nodes. </para>
</important>
<para><emphasis role="bold">Prerequisite</emphasis></para>
<para>Perform the following steps to install the Ceph monitor
                on a dedicated node. Note that the Ceph requires at least three monitoring servers
                to form a cluster in case of a node failure.</para>
<orderedlist xml:id="ol_sys_n3l_lw">
                    <listitem><para>Log in to the lifecycle manager.</para>
</listitem>
                    <listitem><para>Copy the <literal>entry-scale-kvm-ceph</literal> input model to the
                            <literal>~/helion/my_cloud/definition</literal> directory before you begin
                        the editing
                        process:</para>
<screen>cp -r ~/helion/examples/entry-scale-kvm-ceph/* ~/helion/my_cloud/definition/</screen></listitem>
                    <listitem><para>Edit the <literal>control_plane.yml</literal> to create a new cluster, such as
                        with <literal>ceph-mon</literal>, as shown
                        here.</para>
<screen>clusters:
  - name: cluster1
    cluster-prefix: c1
    server-role: CONTROLLER-ROLE
    member-count: 3
    allocation-policy: strict
    service-components:
      - lifecycle-manager
      - ntp-server
      ...

  <emphasis role="bold">- name: ceph-mon
    cluster-prefix: ceph-mon
    server-role: CEP-MON-ROLE
    min-count: 3
    allocation-policy: strict
    service-components:
      - ntp-client
      - ceph-monitor</emphasis>

  - name: rgw
    cluster-prefix: rgw
    server-role: RGW-ROLE
    ...</screen></listitem>
                    <listitem><para>Edit the <literal>~/helion/my_cloud/definition/data/servers.yml</literal> file
                        to define the Ceph monitor node (monitor services). The following example
                        shows three nodes for monitor services. We recommend using an odd number of
                        monitor
                        nodes.</para>
<screen># Ceph Monitor Nodes
- id: ceph-mon1
  ip-addr: 10.13.111.141
  server-group: RACK1
  role: CEP-MON-ROLE
  nic-mapping: MY-4PORT-SERVER
  mac-addr: "f0:92:1c:05:69:10"
  ilo-ip: 10.12.8.217
  ilo-password: password
  ilo-user: admin

- id: ceph-mon2
  ip-addr: 10.13.111.142
  server-group: RACK2
  role: CEP-MON-ROLE
  nic-mapping: MY-4PORT-SERVER
  mac-addr: "83:92:1c:55:69:b0"
  ilo-ip: 10.12.8.218
  ilo-password: password
  ilo-user: admin

- id: ceph-mon3
  ip-addr: 10.13.111.143
  server-group: RACK3
  role: CEP-MON-ROLE
  nic-mapping: MY-4PORT-SERVER
  mac-addr: "d9:92:1c:25:69:e0"
  ilo-ip: 10.12.8.219
  ilo-password: password
  ilo-user: admin

# Ceph RGW Nodes
- id: rgw1
  ...</screen></listitem>
                    <listitem><para>Edit the
                            <literal>~/helion/my_cloud/definition/data/net_interfaces.yml</literal>
                        file to define a new network interface set for your Ceph monitors, as shown here.</para>
<screen>## This defines the interface used for management
## traffic such as logging, monitoring, etc.
- name: CEP-MON-INTERFACES
  network-interfaces:
    - name: BOND0
      device:
          name: bond0
      bond-data:
          options:
              mode: active-backup
              miimon: 200
              primary: hed1
          provider: linux
          devices:
            - name: hed1
            - name: hed2
      network-groups:
        - MANAGEMENT

- name: RGW-INTERFACES
  network-interfaces:
  ...</screen>
</listitem>
                    <listitem><para>Edit
                            <literal>~/helion/my_cloud/definition/data/disks_ceph_monitor.yml</literal>
                        to define the disk model for monitor nodes.
                        </para>
<screen>disk-models:
- name: CEP-MON-DISKS
  # Disk model to be used for Ceph monitor nodes
  # /dev/sda_root is used as a volume group for /, /var/log and /var/crash
  # sda_root is a templated value to align with whatever partition is really used
  # This value is checked in os config and replaced by the partition actually used
  # on sda e.g. sda1 or sda5

  volume-groups:
    - name: hlm-vg
      physical-volumes:
        - /dev/sda_root

      logical-volumes:
      # The policy is not to consume 100% of the space of each volume group.
      # 5% should be left free for snapshots and to allow for some flexibility.
        - name: root
          size: 30%
          fstype: ext4
          mount: /
        - name: log
          size: 45%
          mount: /var/log
          fstype: ext4
          mkfs-opts: -O large_file
        - name: crash
          size: 20%
          mount: /var/crash
          fstype: ext4
          mkfs-opts: -O large_file
      consumer:
         name: os</screen></listitem>
                    <listitem><para>Edit the <literal>~/helion/my_cloud/definition/data/server_roles.yml</literal>
                        file to define a new server role for your Ceph monitors:
                        </para>
<screen>- name: CEP-MON-ROLE
  interface-model: CEP-MON-INTERFACES
  disk-model: CEP-MON-DISKS</screen></listitem>
                    <listitem><para>Commit your
                        configuration:</para>
<screen>cd ~/helion/hos/ansible
git add -A
git commit -m "&lt;commit message&gt;"</screen></listitem>
                    <listitem><para>Run the following playbook to add your nodes into Cobbler:
                        </para>
<screen>cd ~/helion/hos/ansible/
ansible-playbook -i hosts/localhost cobbler-deploy.yml</screen></listitem>
                    <listitem><para>To reimage all the nodes using PXE, run the following playbook:
                        </para>
<screen>cd ~/helion/hos/ansible/
ansible-playbook -i hosts/localhost bm-reimage.yml</screen></listitem>
                    <listitem><para>Run the configuration processor:
                        </para>
<screen>cd ~/helion/hos/ansible/
ansible-playbook -i hosts/localhost config-processor-run.yml</screen></listitem>
                    <listitem><para>Update your deployment directory with this playbook:
                        </para>
<screen>cd ~/helion/hos/ansible/
ansible-playbook -i hosts/localhost ready-deployment.yml</screen></listitem>
                    <listitem><para>Deploy these changes:
                        </para>
<screen>cd ~/scratch/ansible/next/hos/ansible
ansible-playbook -i hosts/verb_hosts site.yml</screen></listitem>
                </orderedlist>
</sidebar>
        <sidebar xml:id="single-vlan-for-all-ceph-traffic"><para><emphasis role="bold">Using a Single VLAN for All Ceph Traffic (Management, Client, and Internal OSD)</emphasis></para>
<para>You can use a single VLAN to transmit all Ceph traffic. This configuration is
                recommended for a small cluster deployment.</para>
<para><!----></para>
<para>Perform
                the following steps to to configure
                    <literal>Entry-scale-kvm-ceph-single-network</literal>. </para>
<orderedlist xml:id="ol_d5k_4ss_lw">
                    <listitem><para> Log in to the lifecycle manager.</para>
</listitem>
                    <listitem><para>Copy the <literal>entry-scale-kvm-ceph</literal> input model to the
                            <literal>~/helion/my_cloud/definition</literal> directory before you begin
                        the editing
                        process:</para>
<screen>cp -r ~/helion/examples/entry-scale-kvm-ceph/* ~/helion/my_cloud/definition/</screen></listitem>
                    <listitem><para>Validate that NIC interfaces are correctly specified in
                            <literal>nic_mapping.yml</literal> for servers that are used in the
                        cloud.</para>
</listitem>
                    <listitem><para>Ensure that you have at least two NICs for Ceph nodes to create a bonded
                        interface for it.</para>
</listitem>
                    <listitem><para>Validate that your servers are mapped to a correct NIC interface
                        specification in <literal>servers.yml</literal>.

                        The following is an example of a server node used for OSD
                        deployment:</para>
<screen># Ceph OSD Nodes
    - id: osd1
      ip-addr: 192.168.10.9
      role: OSD-ROLE
      server-group: RACK1
      nic-mapping: MY-2PORT-SERVER
      mac-addr: "8b:f6:9e:ca:3b:78"
      ilo-ip: 192.168.9.9
      ilo-password: password
      ilo-user: admin</screen></listitem>
                    <listitem><para>Delete the OSD-INTERNAL and OSD-CLIENT network groups from
                            <literal>network_groups.yml</literal>. This is necessary because only the
                        management network is used for Ceph traffic, thus OSD-INTERNAL and
                        OSD-CLIENT network groups are not required. </para>
</listitem>
                    <listitem><para>Define <literal>net_interfaces.yml</literal> to use only management network
                        groups, as shown
                        here.</para>
<screen>  - name: CONTROLLER-INTERFACES
      network-interfaces:
        - name: BOND0
          device:
              name: bond0
          bond-data:
              options:
                  mode: active-backup
                  miimon: 200
                  primary: hed3
              provider: linux
              devices:
                - name: hed3
                - name: hed4
          network-groups:
            - EXTERNAL-API
            - EXTERNAL-VM
            - GUEST
            - MANAGEMENT

    - name: COMPUTE-INTERFACES
      network-interfaces:
        - name: BOND0
          device:
              name: bond0
          bond-data:
              options:
                  mode: active-backup
                  miimon: 200
                  primary: hed3
              provider: linux
              devices:
                - name: hed3
                - name: hed4
          network-groups:
            - EXTERNAL-VM
            - GUEST
            - MANAGEMENT

    - name: OSD-INTERFACES
      network-interfaces:
        - name: BOND0
          device:
              name: bond0
          bond-data:
              options:
                  mode: active-backup
                  miimon: 200
                  primary: hed3
              provider: linux
              devices:
                - name: hed3
                - name: hed4
          network-groups:
            - MANAGEMENT

    - name: RGW-INTERFACES
      network-interfaces:
        - name: BOND0
          device:
              name: bond0
          bond-data:
              options:
                  mode: active-backup
                  miimon: 200
                  primary: hed3
              provider: linux
              devices:
                - name: hed3
                - name: hed4
          network-groups:
            - MANAGEMENT</screen></listitem>
                    <listitem><para>Delete VLAN information for OSD-INTERNAL-NET and OSD-CLIENT-NET from
                            <literal>networks.yml</literal>. Only Management VLANs are used.</para>
</listitem>
                    <listitem><para>After you set up your configuration files, perform steps <emphasis role="bold">8 to 13</emphasis> in
                            <link xlink:href="#config_ceph/deploying-monitor-on-standalone-node">Deploying the Monitor Service on Standalone
                        Nodes</link>.</para>
</listitem>
                </orderedlist>
<para><emphasis role="bold">Using Two VLANs: For Management and Client Traffic and for
                    Internal OSD Traffic</emphasis></para>
<para>You can use dual VLANs to transmit Ceph traffic. In this configuration one VLAN
                transmits management and client traffic and the other VLAN transmits internal OSD
                traffic. A separate bonded interface for two VLANs is used with four NICs. This
                configuration provides two aspects:</para>
<itemizedlist xml:id="ul_ypz_qgy_lw">
                    <listitem><para>Use of two networks, such as VLANs. </para>
</listitem>
                    <listitem><para>Use of separate bonded interfaces for each VLAN (different from what is
                        provided in <literal>entry-scale-kvm-ceph</literal>). </para>
</listitem>
                </itemizedlist>
</sidebar>
        <para>The use of separate NICs segregates traffic at the interface level and requires your
            server to have at least four NICs. But using a separate bonded interface for each VLAN
            is not mandatory, and thus you can use a single bonded interface (or server with only
            two NICs) for Ceph deployment. </para>

        <!---->
        <para><!----></para>

        <para>Perform the following steps to to configure
                <literal>Entry-scale-kvm-ceph-dual-network</literal>. </para>

        <orderedlist xml:id="ol_cxs_5ts_lw">
                <listitem><para>Log in to the lifecycle manager.</para>
</listitem>
                <listitem><para>Copy the <literal>entry-scale-kvm-ceph</literal> input model to the
                        <literal>~/helion/my_cloud/definition</literal> directory before you begin the
                    editing
                    process:</para>
<screen>cp -r ~/helion/examples/entry-scale-kvm-ceph/* ~/helion/my_cloud/definition/</screen></listitem>
                <listitem><para>Validate that NIC interfaces are correctly specified in
                        <literal>nic_mapping.yml</literal> for servers that are used in the cloud. For
                    Ceph OSD nodes, four port servers are required. You can use
                        <emphasis role="bold">HP-DL360-4PORT</emphasis> as it is defined in <literal>nic_mapping.yml</literal>
                    of <literal>entry-scale-kvm-ceph</literal> or define a new NIC mapping (as shown
                    here) for new sets of servers having four port
                    servers.</para>
<screen> - name: HP-4PORT-SERVER
      physical-ports:
        - logical-name: hed1
          type: simple-port
          bus-address: "0000:07:00.0"

        - logical-name: hed2
          type: simple-port
          bus-address: "0000:08:00.0"

        - logical-name: hed3
          type: simple-port
          bus-address: "0000:09:00.0"

        - logical-name: hed4
          type: simple-port
          bus-address: "0000:0a:00.0"</screen></listitem>
                <listitem><para>Modify OSD nodes to use four port servers, as shown here. Change the NIC mapping
                    attribute from <emphasis role="bold">HP-DL360-4PORT</emphasis> to use any other name defined in
                        <literal>nic_mapping.yml</literal>.</para>
<screen> # Ceph OSD Nodes
    - id: osd1
      ip-addr: 192.168.10.9
      role: OSD-ROLE
      server-group: RACK1
      nic-mapping: HP-DL360-4PORT
      mac-addr: "8b:f6:9e:ca:3b:78"
      ilo-ip: 192.168.9.9
      ilo-password: password
      ilo-user: admin</screen></listitem>
                <listitem><para>Delete OSD-CLIENT from <literal>network_groups.yml</literal>. Note that no
                    dedicated network group exists for client traffic. Only the management network
                    group is used for client traffic.</para>
</listitem>
                <listitem><para>Edit <literal>net_interfaces.yml</literal> with a bonded NIC as shown
                    here.</para>
<screen> - name: CONTROLLER-INTERFACES
      network-interfaces:
        - name: BOND0
          device:
              name: bond0
          bond-data:
              options:
                  mode: active-backup
                  miimon: 200
                  primary: hed1
              provider: linux
              devices:
                - name: hed1
                - name: hed2
          network-groups:
            - EXTERNAL-API
            - EXTERNAL-VM
            - GUEST
            - MANAGEMENT

    - name: COMPUTE-INTERFACES
      network-interfaces:
        - name: BOND0
          device:
              name: bond0
          bond-data:
              options:
                  mode: active-backup
                  miimon: 200
                  primary: hed1
              provider: linux
              devices:
                - name: hed1
                - name: hed2
          network-groups:
            - EXTERNAL-VM
            - GUEST
            - MANAGEMENT

    - name: OSD-INTERFACES
      network-interfaces:
        - name: BOND0
          device:
              name: bond0
          bond-data:
              options:
                  mode: active-backup
                  miimon: 200
                  primary: hed1
              provider: linux
              devices:
                - name: hed1
                - name: hed2
          network-groups:
            - MANAGEMENT
        - name: BOND1
          device:
              name: bond1
          bond-data:
              options:
                  mode: active-backup
                  miimon: 200
                  primary: hed3
              provider: linux
              devices:
                - name: hed3
                - name: hed4
          network-groups:
            - OSD-INTERNAL

    - name: RGW-INTERFACES
     network-interfaces:
        - name: BOND0
          device:
              name: bond0
          bond-data:
              options:
                  mode: active-backup
                  miimon: 200
                  primary: hed1
              provider: linux
              devices:
                - name: hed1
                - name: hed2
          network-groups:
            - MANAGEMENT</screen></listitem>
                <listitem><para>Delete OSD-CLIENT from <literal>server_groups.yml</literal>. </para>
</listitem>
                <listitem><para>Delete VLAN information for OSD-CLIENT-NET from <literal>networks.yml</literal>.
                    Only management VLANs are used for client traffic. </para>
</listitem>
                <listitem><para>After you set up your configuration files, perform steps <emphasis role="bold">8 to 13</emphasis> in <link xlink:href="#config_ceph/deploying-monitor-on-standalone-node">Installing Monitor on Standalone Node</link>.</para>
</listitem>
            </orderedlist>

        <formalpara><title>RADOS Gateway</title></formalpara>
        <sidebar xml:id="install-rados-gateway-on-cluster-node-that-host-ceph-monitor"><para><emphasis role="bold">Installing RADOS Gateway on Dedicated Cluster Nodes that Host the Ceph Monitor
                Service</emphasis></para>
<para>You can configure RADOS Gateway to install on one or more dedicated cluster nodes
                hosting the Ceph monitor service as follows:</para>
<orderedlist xml:id="ol_hfp_q22_sv">
                <listitem><para>Remove the sections for servers in the
                        <literal>~/helion/my_cloud/definition/data/servers.yml</literal> file that
                    have the <literal>role: RGW-ROLE</literal> attribute.</para>
</listitem>
                <listitem><para>Edit the <literal>~/helion/my_cloud/definition/data/control_plane.yml</literal>
                    file and add the following lines to the <literal>service-components</literal>
                    section for the cluster nodes that have the <literal>server-role:
                        MON-ROLE</literal>
                    attribute.</para>
<screen>- ceph-radosgw
- apache2</screen></listitem>
                <listitem><para>Edit the <literal>~/helion/my_cloud/definition/data/net_interfaces.yml</literal>
                    file to remove the RGW-INTERFACES section. This section defines RADOS Gateway
                    network interfaces, which are not required in this configuration:
                    </para>
<screen> - name: RGW-INTERFACES
   network-interfaces:
     - name: BOND0
       device:
          name: bond0
       bond-data:
          options:
             mode: active-backup
             miimon: 200
             primary: hed3
          provider: linux
          devices:
             - name: hed3
             - name: hed4
       network-groups:
         - MANAGEMENT
         - OSD-CLIENT</screen></listitem>
                <listitem><para>After you set up your configuration files, perform steps <emphasis role="bold">8 to 13</emphasis> in <link xlink:href="#config_ceph/deploying-monitor-on-standalone-node">Deploying the Monitor on Standalone Nodes</link>. </para>
</listitem>
            </orderedlist></sidebar>
        <sidebar xml:id="install-rados-gateway-on-controller-nodes"><para><emphasis role="bold">Installing RADOS Gateway on Controller Nodes</emphasis></para>
<para>You can configure RADOS Gateway to install on controller nodes. To do this, perform
                the following steps:</para>
<orderedlist xml:id="ol_kfp_q22_sv">
                <listitem><para>Remove the sections for servers in the
                        <literal>~/helion/my_cloud/definition/data/servers.yml</literal> file that
                    have the <literal>role: RGW-ROLE</literal> attribute.</para>
</listitem>
                <listitem><para>Edit the <literal>~/helion/my_cloud/definition/data/net_interfaces.yml</literal>
                    file to remove the RGW-INTERFACES section. This section defines RADOS Gateway
                    network interfaces, which are not required in this configuration:
                    </para>
<screen> - name: RGW-INTERFACES
   network-interfaces:
     - name: BOND0
       device:
          name: bond0
       bond-data:
          options:
             mode: active-backup
             miimon: 200
             primary: hed3
          provider: linux
          devices:
             - name: hed3
             - name: hed4
       network-groups:
         - MANAGEMENT
         - OSD-CLIENT</screen></listitem>
                <listitem><para>Edit the <literal>~/helion/my_cloud/definition/data/control_plane.yml</literal>
                    file and add the following line to <literal>service-components</literal> for the
                    cluster with the <literal>server-role: CONTROLLER-ROLE</literal>
                    attribute.</para>
<screen>- ceph-radosgw</screen></listitem>
                <listitem><para>After you set up your configuration files, perform steps <emphasis role="bold">8 to 13</emphasis> in <link xlink:href="#config_ceph/deploying-monitor-on-standalone-node">Deploying the Monitor Service on Standalone Nodes</link>.</para>
</listitem>
            </orderedlist></sidebar>
        <sidebar xml:id="install-more-two-rados-gateway-servers"><para><emphasis role="bold">Installing More than Two RADOS Gateway Servers</emphasis></para>
<para>To deploy more than two RADOS Gateway servers, you need to add a section to the
                    <literal>~/helion/my_cloud/definition/data/servers.yml</literal> file for each
                additional RADOS Gateway node.</para>
</sidebar>
        <sidebar xml:id="ceph-deployment-vcp" userlevel="INTERNAL"><title>Ceph Deployment with Virtual Control Plane
                </title><para>&kw-hos-tm;
            &kw-hos-version; supports the deployment of control plane elements on
            virtual machines which can be co-located on one baremetal machine or spread across three
            baremetal machines. The baremetal machine in this context is termed as VM factory
            host(s). The following aspects must be considered while deploying Ceph with the virtual
            control plane.</para>
<orderedlist xml:id="ol_wg4_s25_mx">
                <listitem><para>Deploy OSD and monitor node on a single VM factor host - It is applicable to X1
                    cloud model. The number of VM factory host is one. Therefore, Ceph cluster will
                    not have HA support as there will be only one instance of the monitor component
                    of Ceph.</para>
</listitem>
                <listitem><para>Deploy OSD and monitor on set of three VM factor hosts - It is applicable to S1
                    cloud model.</para>
</listitem>
                <listitem><para>Deploy monitor on set of three VM factor hosts but OSD nodes are deployed
                    independently as a resource nodes - It is applicable to M1 cloud model.</para>
</listitem>
            </orderedlist><para>The following aspects must be considered while deploying Ceph with virtual
                control plane:</para>
<orderedlist xml:id="ol_zgr_djn_kx">
                    <listitem><para>Scale-out of cluster - Adding a new set of monitor or OSD nodes is not
                        validated by the engineering team. Although, technically it is feasible but
                        not recommended because it can have a significant performance impact.
                        However, one can increase a cluster capacity by adding more disks to the VM
                        factory host and configuring them as OSD (a scale-in path to increase
                        capacity) nodes.</para>
</listitem>
                    <listitem><para>Deployment of RADOS Gateway on VM factory host is not formally
                        supported.</para>
</listitem>
                    <listitem><para>Performance of Ceph components (OSD and monitor nodes) are sensitive to
                        compute resources i.e. memory, core CPU, and so on. It is strongly
                        recommended to plan and allocate minimum amount of resources for Ceph
                        components to avoid resource contention because same set of machines will be
                        running the control plane elements and the Ceph components. Starvation of
                        resources might causes impact on the performance and the stability of the
                        Ceph clusters health. Consider the following resource aspect for planning:
                            </para>
<itemizedlist xml:id="ul_a3t_c3n_kx">
                            <listitem><para>CPU</para>
</listitem>
                            <listitem><para>RAM</para>
</listitem>
                            <listitem><para>Disk space for monitoring logs</para>
</listitem>
                        </itemizedlist><para>The following section focus on the change of the input model for X1
                            and S1 model ONLY. The change in the input model for M1 model is similar
                            except that OSD node is deployed as the resource nodes. For other
                            aspects of cluster management like upgrade, adding new set of disks,
                            stopping and starting services and so on, you can follow the similar
                            approach that is used for the deployment of Ceph cluster using &kw-hos-tm; .</para>
</listitem>
                </orderedlist>
<para><emphasis role="bold">Steps to deploy Ceph on VM factory host(s)</emphasis></para>
<orderedlist xml:id="ol_hts_y3n_kx">
                    <listitem><para>Log in to the lifecycle manager.</para>
</listitem>
                    <listitem><para>Go to <literal>~/helion/my_cloud/definition/</literal>.</para>
</listitem>
                    <listitem><para>Edit your <literal>control_plane.yml</literal> file to add
                            <literal>ceph-osd</literal> and <literal>ceph-monitor</literal> components
                        to the vmfactory nodes. For
                        example:</para>
<screen> - name: vmfactory
          resource-prefix: vmf
          server-role: HLM-HYPERVISOR-ROLE
          min-count:
          allocation-policy: strict
          service-components:
            - ntp-server
            - ceph-osd
            - ceph-monitor
            - lifecycle-manager
            - tempest
            - openstack-client</screen></listitem>
                    <listitem><para>Edit <literal>disks_vmfactory.yml</literal> file of VM factory hosts to define
                        data and journal disks for OSD. For example, the following disk model
                        illustrates the usage of <literal>/dev/sdd</literal>,
                            <literal>/dev/sde</literal>, and <literal>/dev/sdf</literal> as data disks
                        and <literal>/dev/sdg</literal> as journal disks for OSD. Disks allocated to
                        OSD must not be used for any other
                        purpose.</para>
<screen>---
   product:
     version: 2

   disk-models:
     - name: HLM-HYPERVISOR-DISKS
       volume-groups:
	- name: hlm-vg
        physical-volumes:
         - /dev/sda_root
        logical-volumes:
        # The policy is not to consume 100% of the space of each volume group.
        # 5% should be left free for snapshots and to allow for some flexibility.
          - name: root
            size: 35%
            fstype: ext4
            mount: /
          - name: log
            size: 50%
            mount: /var/log
            fstype: ext4
            mkfs-opts: -O large_file
          - name: crash
            size: 10%
            mount: /var/crash
            fstype: ext4
            mkfs-opts: -O large_file
      - name: vg-images
        # this VG is dedicated to libvirt images to keep VM IOPS off the OS disk
        physical-volumes:
          - /dev/sdb
          - /dev/sdc
        logical-volumes:
          - name: images
            size: 95%
            mount: /var/lib/libvirt/images
            fstype: ext4
            mkfs-opts: -O large_file

   device-groups:
      - name: ceph-osd-disks
        devices:
       - name: /dev/sdd
       - name: /dev/sde
       - name: /dev/sdf
        consumer:
           name: ceph
           attrs:
             usage: data
             journal_disk: /dev/sdg</screen></listitem>
                    <listitem><para>Commit your configuration to the local git
                        repo.</para>
<screen>cd ~/helion/hos/ansible
git add -A
git commit -m "My config or other commit message"</screen></listitem>
                    <listitem><para>After setting up the configuration files, continue with the installation
                        procedure mentioned at <xref linkend="install_kvm"/>. </para>
</listitem>
                </orderedlist>
</sidebar>
    </section>
