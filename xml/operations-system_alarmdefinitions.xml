<?xml version="1.0"?>
<!DOCTYPE section [
 <!ENTITY % entities SYSTEM "entity-decl.ent"> %entities;
]>
<section xmlns="http://docbook.org/ns/docbook" xmlns:xi="http://www.w3.org/2001/XInclude"
  xmlns:xlink="http://www.w3.org/1999/xlink" xml:id="system_alarmdefinitions">
 <title>System Alarms</title>
 <para>
  These alarms show under the System section and are setup per
  <literal>hostname</literal> and/or <literal>mount_point</literal>.
 </para>
 <informaltable colsep="1" rowsep="1">
  <tgroup cols="5">
   <colspec colname="c1" colnum="1"/>
   <colspec colname="c2" colnum="2"/>
   <colspec colname="c3" colnum="3"/>
   <colspec colname="c4" colnum="4"/>
   <colspec colname="c5" colnum="5"/>
   <thead>
    <row>
     <entry>Service</entry>
     <entry>Alarm Name</entry>
     <entry>Description</entry>
     <entry>Likely Cause</entry>
     <entry>Mitigation Tasks to Perform</entry>
    </row>
   </thead>
   <tbody>
    <row>
     <entry morerows="11">system</entry>
     <entry>CPU Usage</entry>
     <entry>Alarms on high CPU usage.</entry>
     <entry>Heavy load or runaway processes.</entry>
     <entry>Log onto the reporting host and diagnose the heavy CPU usage.</entry>
    </row>
    <row>
     <entry>&elasticsearch; Low Watermark</entry>
     <entry>
<screen>component = elasticsearch</screen>
      <para>
       &elasticsearch; disk low watermark. Backup indices. If high watermark is
       reached, indices will be deleted. Adjust curator_low_watermark_percent,
       curator_high_watermark_percent, and
       elasticsearch_max_total_indices_size_in_bytes if needed.
      </para>
     </entry>
     <entry>Running out of disk space for <literal>/var/lib/elasticsearch</literal>.</entry>
     <entry>
      <para>
       Free up space by removing indices (backing them up first if desired).
       Alternatively, adjust <literal>curator_low_watermark_percent</literal>,
       <literal>curator_high_watermark_percent</literal>, and/or
       <literal>elasticsearch_max_total_indices_size_in_bytes</literal> if
       needed.
      </para>
      <para>
       For more information about how to back up your centralized logs, see
       <xref linkend="central_log_configure_settings"/>.
      </para>
     </entry>
    </row>
    <row>
     <entry>&elasticsearch; High Watermark</entry>
     <entry>
<screen>component = elasticsearch</screen>
      <para>
       &elasticsearch; disk high watermark. Attempting to delete indices to free
       disk space. Adjust <literal>curator_low_watermark_percent</literal>,
       <literal>curator_high_watermark_percent</literal>, and
       <literal>elasticsearch_max_total_indices_size_in_bytes</literal> if
       needed.
      </para>
     </entry>
     <entry>Running out of disk space for <literal>/var/lib/elasticsearch</literal>.</entry>
     <entry>
      <para>
       Verify that disk space was freed up by the curator. If needed, free up
       additional space by removing indices (backing them up first if desired).
       Alternatively, adjust curator_low_watermark_percent,
       curator_high_watermark_percent, and/or
       elasticsearch_max_total_indices_size_in_bytes if needed.
      </para>
      <para>
       For more information about how to back up your centralized logs, see
       <xref linkend="central_log_configure_settings"/>.
      </para>
     </entry>
    </row>
    <row>
     <entry>Log Partition Low Watermark</entry>
     <entry>The <literal>/var/log</literal> disk space usage has crossed the low watermark. If
            the high watermark is reached, <literal>logrotate</literal> will be run to free up disk
            space. Adjust <literal>var_log_low_watermark_percent</literal> if needed.</entry>
     <entry>This could be due to a service set to <literal>DEBUG</literal> instead of
              <literal>INFO</literal> level. Another reason could be due to a repeating error
            message filling up the log files. Finally, it could be due to log rotate not configured
            properly so old log files are not being deleted properly.</entry>
     <entry>Find the service that is consuming too much disk space. Look at the logs. If
              <literal>DEBUG</literal> log entries exist, set the logging level to
              <literal>INFO</literal>. If the logs are repeatedly logging an error message, do what
            is needed to resolve the error. If old log files exist, configure log rotate to remove
            them. You could also choose to remove old log files by hand after backing them up if
            needed.</entry>
    </row>
    <row>
     <entry>Log Partition High Watermark</entry>
     <entry>The <literal>/var/log</literal> volume is running low on disk space. Logrotate will
            be run now to free up space. Adjust <literal>var_log_high_watermark_percent</literal> if
            needed.</entry>
     <entry>This could be due to a service set to <literal>DEBUG</literal> instead of
              <literal>INFO</literal> level. Another reason could be due to a repeating error
            message filling up the log files. Finally, it could be due to log rotate not configured
            properly so old log files are not being deleted properly.</entry>
     <entry>Find the service that is consuming too much disk space. Look at the logs. If
              <literal>DEBUG</literal> log entries exist, set the logging level to
              <literal>INFO</literal>. If the logs are repeatedly logging an error message, do what
            is needed to resolve the error. If old log files exist, configure log rotate to remove
            them. You could also choose to remove old log files by hand after backing them up if
            needed.</entry>
    </row>
    <row>
     <entry>Crash Dump Count</entry>
     <entry>Alarms if it receives any metrics with <literal>crash.dump_count</literal> &gt;
            0</entry>
     <entry>When a crash dump is generated by kdump, the crash dump file is put into the
              <literal>/var/crash</literal> directory by default. Any crash dump files in this
            directory will cause the <literal>crash.dump_count</literal> metric to show a value
            greater than 0.</entry>
     <entry>
      <para>
       Analyze the crash dump file(s) located in <literal>/var/crash</literal>
       on the host that generated the alarm to try to determine if a service or
       hardware caused the crash.
      </para>
      <para>
       Move the file to a new location so that a developer can take a look at
       it. Make sure all of the processes are back up after the crash (run the
       <literal>&lt;service&gt;-status.yml</literal> playbooks). When the
       <literal>/var/crash</literal> directory is empty the Crash Dump Count
       alarm should transition back to OK.
      </para>
     </entry>
    </row>
    <row>
     <entry>Disk Inode Usage</entry>
     <entry>Nearly out of inodes for a partition, as indicated by the
              <literal>mount_point</literal> reported.</entry>
     <entry>Many files on the disk.</entry>
     <entry>Investigate cleanup of data or migration to other partitions.<!---->
     </entry>
    </row>
    <row>
     <entry>Disk Usage</entry>
     <entry>High disk usage, as indicated by the <literal>mount_point</literal>
            reported.</entry>
     <entry>Large files on the disk.</entry>
     <entry>
      <para>
       Investigate cleanup of data or migration to other partitions.
      </para>
     </entry>
    </row>
    <row>
     <entry>Host Status</entry>
     <entry>
      <para>
       Alerts when a host is unreachable.
      </para>
<screen>test_type = ping</screen>
     </entry>
     <entry>Host or network is down.</entry>
     <entry>If a single host, attempt to restart the system. If multiple hosts, investigate
            network issues.</entry>
    </row>
    <row>
     <entry>Memory Usage</entry>
     <entry>High memory usage.</entry>
     <entry>Overloaded system or services with memory leaks.</entry>
     <entry>Log onto the reporting host to investigate high memory users.</entry>
    </row>
    <row>
     <entry>Network Errors</entry>
     <entry>Alarms on a high network error rate.</entry>
     <entry>Bad network or cabling.</entry>
     <entry>Take this host out of service until the network can be fixed.</entry>
    </row>
    <row>
     <entry>NTP Time Sync</entry>
     <entry>Alarms when the NTP time offset is high.</entry>
     <entry/>
     <entry>
      <para>
       Log in to the reported host and check if the ntp service is running.
      </para>
      <para>
       If it is running, then use these steps:
      </para>
      <orderedlist>
       <listitem>
        <para>
         Stop the service:
        </para>
<screen>service ntpd stop</screen>
       </listitem>
       <listitem>
        <para>
         Resynchronize the node's time:
        </para>
<screen>/usr/sbin/ntpdate -b  &lt;ntp-server&gt;</screen>
       </listitem>
       <listitem>
        <para>
         Restart the ntp service:
        </para>
<screen>service ntp start</screen>
       </listitem>
       <listitem>
        <para>
         Restart rsyslog:
        </para>
<screen>service rsyslog restart</screen>
       </listitem>
      </orderedlist>
     </entry>
    </row>
   </tbody>
  </tgroup>
 </informaltable>
</section>
