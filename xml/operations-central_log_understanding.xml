<?xml version="1.0"?>
<?xml-stylesheet href="urn:x-suse:xslt:profiling:docbook51-profile.xsl"
 type="text/xml"
 title="Profiling step"?>
<!DOCTYPE section [
 <!ENTITY % entities SYSTEM "entities.ent"> %entities;
]>
<section xmlns="http://docbook.org/ns/docbook" xmlns:xi="http://www.w3.org/2001/XInclude" xmlns:xlink="http://www.w3.org/1999/xlink" xml:id="central_log_concepts">
 <title>Understanding the Centralized Logging Service</title>
 <para>
  The Centralized Logging feature collects logs on a central system, rather
  than leaving the logs scattered across the network. The administrator can use
  a single Kibana interface to view log information in charts, graphs, tables,
  histograms, and other forms.
 </para>
 <section xml:id="idg-all-operations-central_log_understanding-xml-2">
  <title>What Components are Part of Centralized Logging?</title>
  <para>
   Centralized logging consists of several components, detailed below:
  </para>
  <itemizedlist>
   <listitem>
    <formalpara>
     <title>Administrator's Browser:</title>
     <para>
      &opscon; can be used to access logging alarms or to access Kibana's
      dashboards to review logging data.
     </para>
    </formalpara>
   </listitem>
   <listitem>
    <formalpara>
     <title>Apache Website for Kibana:</title>
     <para>
      A standard Apache website that proxies web/REST requests to the Kibana
      NodeJS server.
     </para>
    </formalpara>
   </listitem>
   <listitem>
    <formalpara>
     <title>Beaver:</title>
     <para>
      A Python daemon that collects information in log files and sends it to
      the Logging API (monasca-log API) over a secure connection.
     </para>
    </formalpara>
   </listitem>
   <listitem>
    <formalpara>
     <title>Cloud Auditing Data Federation (CADF):</title>
     <para>
      Defines a standard, full-event model anyone can use to fill in the
      essential data needed to certify, self-manage and self-audit
      application security in cloud environments.
     </para>
    </formalpara>
   </listitem>
   <listitem>
    <formalpara>
     <title>Centralized Logging and Monitoring (CLM):</title>
     <para>
      Used to evaluate and troubleshoot your &productname; distributed cloud
      environment from a single location.
     </para>
    </formalpara>
   </listitem>
   <listitem>
    <para>
     <emphasis role="bold">Curator:</emphasis> a tool provided by
     &elasticsearch; to manage indices.
    </para>
   </listitem>
   <listitem>
    <formalpara>
     <title>&elasticsearch;:</title>
     <para>
      A data store offering fast indexing and querying.
     </para>
    </formalpara>
   </listitem>
   <listitem>
    <formalpara>
     <title>&productname;:</title>
     <para>
      Provides public, private, and managed cloud solutions to get you moving
      on your cloud journey.
     </para>
    </formalpara>
   </listitem>
   <listitem>
    <formalpara>
     <title>JavaScript Object Notation (JSON) log file:</title>
     <para>
      A file stored in the JSON format and used to exchange data. JSON uses
      JavaScript syntax, but the JSON format is text only. Text can be read
      and used as a data format by any programming language. This format is
      used by the Beaver and Logstash components.
     </para>
    </formalpara>
   </listitem>
   <listitem>
    <formalpara>
     <title>Kafka:</title>
     <para>
      A messaging broker used for collection of &productname; centralized
      logging data across nodes. It is highly available, scalable and
      performant. Kafka stores logs in disk instead of memory and is
      therefore more tolerant to consumer down times.
     </para>
    </formalpara>
    <important>
     <para>
      Make sure not to undersize your Kafka partition or the data retention
      period may be lower than expected. If the Kafka partition capacity is
      lower than 85%, the retention period will increase to 30 minutes. Over
      time Kafka will also eject old data.
     </para>
<!--

      &productname; receives more data
      than expected, then Kafka will fall back to the following settings:

     <informaltable colsep="1" rowsep="1">
      <tgroup cols="2">
       <colspec colname="c1" colnum="1" colwidth="1.0*"/>
       <colspec colname="c2" colnum="2" colwidth="1.0*"/>
       <thead>
        <row>
         <entry>Kafka Partition Capacity</entry>
         <entry>Retention Period</entry>
        </row>
       </thead>
       <tbody>
        <row>
         <entry>85%</entry>
         <entry>30 minutes</entry>
        </row>
        <row>
         <entry>25%</entry>
         <entry>Default</entry>
        </row>
       </tbody>
      </tgroup>
     </informaltable>
     <para>
      Over time Kafka will also eject old data.
     </para>
-->
    </important>
   </listitem>
   <listitem>
    <formalpara>
     <title>Kibana:</title>
     <para>
      A client/server application with rich dashboards to visualize the data
      in &elasticsearch; through a web browser. Kibana enables you to create
      charts and graphs using the log data.
     </para>
    </formalpara>
   </listitem>
   <listitem>
    <para>
     <emphasis role="bold">Logging API (monasca-log-api):</emphasis> &kw-hos;
     API provides a standard REST interface to store logs. It uses Keystone
     authentication and role-based access control support.
    </para>
   </listitem>
   <listitem>
    <formalpara>
     <title>Logstash:</title>
     <para>
      A log processing system for receiving, processing and outputting logs.
      Logstash retrieves logs from Kafka, processes and enriches the data,
      then stores the data in &elasticsearch;.
     </para>
    </formalpara>
   </listitem>
   <listitem>
    <formalpara>
     <title>MML Service Node:</title>
     <para>
      Metering, Monitoring, and Logging (MML) service node. All services
      associated with metering, monitoring, and logging run on a dedicated
      three-node cluster. Three nodes are required for high availability with
      quorum.
     </para>
    </formalpara>
   </listitem>
   <listitem>
    <formalpara>
     <title>Monasca:</title>
     <para>
      &ostack; monitoring at scale infrastructure for the cloud that supports
      alarms and reporting.
     </para>
    </formalpara>
   </listitem>
   <listitem>
    <formalpara>
     <title>&ostack; Service</title>
     <para>
      An &ostack; service process that requires logging services.
     </para>
    </formalpara>
   </listitem>
   <listitem>
    <formalpara>
     <title>Oslo.log</title>
     <para>
      An &ostack; library for log handling. The library functions automate
      configuration, deployment and scaling of complete, ready-for-work
      application platforms. Some PaaS solutions, such as Cloud Foundry,
      combine operating systems, containers, and orchestrators with developer
      tools, operations utilities, metrics, and security to create a
      developer-rich solution.
     </para>
    </formalpara>
   </listitem>
   <listitem>
    <formalpara>
     <title>Text log:</title>
     <para>
      A type of file used in the logging process that contains human-readable
      records.
     </para>
    </formalpara>
   </listitem>
  </itemizedlist>
  <para>
   These components are configured to work out-of-the-box and the admin should
   be able to view log data using the default configurations.
  </para>
  <para>
   In addition to each of the services, Centralized Logging also processes logs
   for the following features:
  </para>
  <itemizedlist>
   <listitem>
    <para>
     HAProxy
    </para>
   </listitem>
   <listitem>
    <para>
     Syslog
    </para>
   </listitem>
   <listitem>
    <para>
     keepalived
    </para>
   </listitem>
  </itemizedlist>
  <para>
   The purpose of the logging service is to provide a common logging
   infrastructure with centralized user access. Since there are numerous
   services and applications running in each node of a &kw-hos; cloud, and
   there could be hundreds of nodes, all of these services and applications can
   generate enough log files to make it very difficult to search for specific
   events in log files across all of the nodes. Centralized Logging addresses
   this issue by sending log messages in real time to a central &elasticsearch;,
   Logstash, and Kibana cluster. In this cluster they are indexed and organized
   for easier and visual searches. The following illustration describes the
   architecture used to collect operational logs.
  </para>
  <informalfigure>
   <mediaobject>
    <imageobject role="fo">
     <imagedata fileref="media-logservice_arch.png" width="75%"/>
    </imageobject>
    <imageobject role="html">
     <imagedata fileref="media-logservice_arch.png"/>
    </imageobject>
   </mediaobject>
  </informalfigure>
  <note>
   <para>
    The arrows come from the active (requesting) side to the passive
    (listening) side. The active side is always the one providing
    credentials, so the arrows may also be seen as coming from the credential
    holder to the application requiring authentication.
   </para>
  </note>
 </section>
 <!-- FIXME: Make this a real procedure. - sknorr, 2018-03-29 -->
 <section xml:id="log_arch_st1to2">
  <title>Steps 1- 2</title>
  <para>
   Services configured to generate log files record the data. Beaver listens
   for changes to the files and sends the log files to the Logging Service. The
   first step the Logging service takes is to re-format the original log file
   to a new log file with text only and to remove all network operations. In
   Step 1a, the Logging service uses the Oslo.log library to re-format the file
   to text-only. In Step 1b, the Logging service uses the Python-Logstash
   library to format the original audit log file to a JSON file.
  </para>
  <variablelist>
   <varlistentry>
    <term>Step 1a</term>
    <listitem>
     <para>
      Beaver watches configured service operational log files for changes and
      reads incremental log changes from the files.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>Step 1b</term>
    <listitem>
     <para>
      Beaver watches configured service operational log files for changes and
      reads incremental log changes from the files.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>Step 2a</term>
    <listitem>
     <para>
      The monascalog transport of Beaver makes a token request call to Keystone
      passing in credentials. The token returned is cached to avoid multiple
      network round-trips.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>Step 2b</term>
    <listitem>
     <para>
      The monascalog transport of Beaver batches multiple logs (operational or
      audit) and posts them to the monasca-log-api VIP over a secure
      connection. Failure logs are written to the local Beaver log.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>Step 2c</term>
    <listitem>
     <para>
      The REST API client for monasca-log-api makes a token-request call to
      Keystone passing in credentials. The token returned is cached to avoid
      multiple network round-trips.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>Step 2d</term>
    <listitem>
     <para>
      The REST API client for monasca-log-api batches multiple logs
      (operational or audit) and posts them to the monasca-log-api VIP over a
      secure connection.
     </para>
    </listitem>
   </varlistentry>
  </variablelist>
 </section>
 <section xml:id="log_arch_st3ab">
  <title>Steps 3a- 3b</title>
  <para>
   The Logging API (monasca-log API) communicates with Keystone to validate the
   incoming request, and then sends the logs to Kafka.
  </para>
  <variablelist>
   <varlistentry>
    <term>Step 3a</term>
    <listitem>
     <para>
      The monasca-log-api WSGI pipeline is configured to validate incoming
      request tokens with Keystone. The keystone middleware used for this
      purpose is configured to use the monasca-log-api admin user, password and
      project that have the required keystone role to validate a token.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>Step 3b</term>
    <listitem>
     <para>
      Monasca-log-api sends log messages to Kafka using a language-agnostic TCP
      protocol.
     </para>
    </listitem>
   </varlistentry>
  </variablelist>
 </section>
 <section xml:id="log_arch_st4to8">
  <title>Steps 4- 8</title>
  <para>
   Logstash pulls messages from Kafka, identifies the log type, and transforms
   the messages into either the audit log format or operational format. Then
   Logstash sends the messages to &elasticsearch;, using either an audit or
   operational indices.
  </para>
  <variablelist>
   <varlistentry>
    <term>Step 4</term>
    <listitem>
     <para>
      Logstash input workers pull log messages from the Kafka-Logstash topic
      using TCP.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>Step 5</term>
    <listitem>
     <para>
      This Logstash filter processes the log message in-memory in the request
      pipeline. Logstash identifies the log type from this field.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>Step 6</term>
    <listitem>
     <para>
      This Logstash filter processes the log message in-memory in the request
      pipeline. If the message is of audit-log type, Logstash transforms it
      from the monasca-log-api envelope format to the original CADF format.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>Step 7</term>
    <listitem>
     <para>
      This Logstash filter determines which index should receive the log
      message. There are separate indices in &elasticsearch; for operational
      versus audit logs.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>Step 8</term>
    <listitem>
     <para>
      Logstash output workers write the messages read from Kafka to the daily
      index in the local &elasticsearch; instance.
     </para>
    </listitem>
   </varlistentry>
  </variablelist>
 </section>
 <section xml:id="log_arch_st9to12">
  <title>Steps 9- 12</title>
  <para>
   When an administrator who has access to the guest network accesses the
   Kibana client and makes a request, Apache forwards the request to the Kibana
   NodeJS server. Then the server uses the &elasticsearch; REST API to service
   the client requests.
  </para>
  <variablelist>
   <varlistentry>
    <term>Step 9</term>
    <listitem>
     <para>
      An administrator who has access to the guest network accesses the Kibana
      client to view and search log data. The request can originate from the
      external network in the cloud through a tenant that has a pre-defined
      access route to the guest network.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>Step 10</term>
    <listitem>
     <para>
      An administrator who has access to the guest network uses a web browser
      and points to the Kibana URL. This allows the user to search logs and
      view Dashboard reports.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>Step 11</term>
    <listitem>
     <para>
      The authenticated request is forwarded to the Kibana NodeJS server to
      render the required dashboard, visualization, or search page.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>Step 12</term>
    <listitem>
     <para>
      The Kibana NodeJS web server uses the &elasticsearch; REST API in localhost
      to service the UI requests.
     </para>
    </listitem>
   </varlistentry>
  </variablelist>
 </section>
 <section xml:id="log_arch_st13to15">
  <title>Steps 13- 15</title>
  <para>
   Log data is backed-up and deleted in the final steps.
  </para>
  <variablelist>
   <varlistentry>
    <term>Step 13</term>
    <listitem>
     <para>
      A daily cron job running in the ELK node runs curator to prune old
      &elasticsearch; log indices.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>Step 14</term>
    <listitem>
     <para>
      The curator configuration is done at the deployer node through the
      Ansible role logging-common. Curator is scripted to then prune or clone
      old indices based on this configuration.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>Step 15</term>
    <listitem>
     <para>
      The audit logs are configured to be backed up by the &kw-hos; Freezer
      product. For more information about Freezer (and Bura), see
      <xref linkend="bura_overview"/>.
     </para>
    </listitem>
   </varlistentry>
  </variablelist>
 </section>
 <section xml:id="retaining_logs">
  <title>How Long are Log Files Retained?</title>
  <para>
   The logs that are centrally stored are saved to persistent storage as
   &elasticsearch; indices. These indices are stored in the partition
   <literal>/var/lib/elasticsearch</literal> on each of the &elasticsearch;
   cluster nodes. Out of the box, logs are stored in one &elasticsearch; index
   per service. As more days go by, the number of indices stored in this disk
   partition grows. Eventually the partition fills up. If they are
   <emphasis role="bold">open</emphasis>, each of these indices takes up CPU
   and memory. If these indices are left unattended they will continue to
   consume system resources and eventually deplete them.
  </para>
  <para>
   &elasticsearch;, by itself, does not prevent this from happening.
  </para>
  <para>
   &kw-hos; uses a tool called curator that is developed by the &elasticsearch;
   community to handle these situations. &kw-hos; installs and uses a curator
   in conjunction with several configurable settings. This curator is called by
   cron and performs the following checks:
  </para>
  <itemizedlist>
   <listitem>
    <para>
     <emphasis role="bold">First Check.</emphasis> The hourly cron job checks
     to see if the currently used &elasticsearch; partition size is over the
     value set in:
    </para>
<screen>curator_low_watermark_percent</screen>
    <para>
     If it is higher than this value, the curator deletes old indices according
     to the value set in:
    </para>
<screen>curator_num_of_indices_to_keep</screen>
   </listitem>
   <listitem>
    <para>
     <emphasis role="bold">Second Check.</emphasis> Another check is made to
     verify if the partition size is below the high watermark percent. If it is
     still too high, curator will delete all indices except the current one
     that is over the size as set in:
    </para>
<screen>curator_max_index_size_in_gb</screen>
   </listitem>
   <listitem>
    <para>
     <emphasis role="bold">Third Check.</emphasis> A third check verifies if
     the partition size is still too high. If it is, curator will delete all
     indices except the current one.
    </para>
   </listitem>
   <listitem>
    <para>
     <emphasis role="bold">Final Check.</emphasis> A final check verifies if
     the partition size is still high. If it is, an error message is written to
     the log file but the current index is NOT deleted.
    </para>
   </listitem>
  </itemizedlist>
  <para>
   In the case of an extreme network issue, log files can run out of disk space
   in under an hour. To avoid this &kw-hos; uses a shell script called
   <literal>logrotate_if_needed.sh</literal>. The cron process runs this script
   every 5 minutes to see if the size of <literal>/var/log</literal> has
   exceeded the high_watermark_percent (95% of the disk, by default). If it is
   at or above this level, <literal>logrotate_if_needed.sh</literal> runs the
   <literal>logrotate</literal> script to rotate logs and to free up extra
   space. This script helps to minimize the chance of running out of disk space
   on <literal>/var/log</literal>.
  </para>
 </section>
 <section xml:id="rotating_logs">
  <title>How Are Logs Rotated?</title>
  <para>
   &kw-hos; uses the cron process which in turn calls Logrotate to provide
   rotation, compression, and removal of log files. Each log file can be
   rotated hourly, daily, weekly, or monthly. If no rotation period is set then
   the log file will only be rotated when it grows too large.
  </para>
  <para>
   Rotating a file means that the Logrotate process creates a copy of the log
   file with a new extension, for example, the .1 extension, then empties the
   contents of the original file. If a .1 file already exists, then that file
   is first renamed with a .2 extension. If a .2 file already exists, it is
   renamed to .3, etc., up to the maximum number of rotated files specified in
   the settings file. When Logrotate reaches the last possible file extension,
   it will delete the last file first on the next rotation. By the time the
   Logrotate process needs to delete a file, the results will have been copied
   to &elasticsearch;, the central logging database.
  </para>
  <para>
   The log rotation setting files can be found in the following directory
  </para>
<screen>~/scratch/ansible/next/ardana/ansible/roles/logging-common/vars</screen>
  <para>
   These files allow you to set the following options:
  </para>
  <variablelist>
   <varlistentry>
    <term>Service</term>
    <listitem>
     <para>
      The name of the service that creates the log entries.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>Rotated Log Files</term>
    <listitem>
     <para>
      List of log files to be rotated. These files are kept locally on the
      server and will continue to be rotated. If the file is also listed as
      Centrally Logged, it will also be copied to &elasticsearch;.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>Frequency</term>
    <listitem>
     <para>
      The timing of when the logs are rotated. Options include:hourly, daily,
      weekly, or monthly.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>Max Size</term>
    <listitem>
     <para>
      The maximum file size the log can be before it is rotated out.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>Rotation</term>
    <listitem>
     <para>
      The number of log files that are rotated.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>Centrally Logged Files</term>
    <listitem>
     <para>
      These files will be indexed by &elasticsearch; and will be available for
      searching in the Kibana user interface.
     </para>
    </listitem>
   </varlistentry>
  </variablelist>
  <para>
   As an example, Freezer, the Backup and Restore (BURA) service, may be
   configured to create log files by setting the Rotated Log Files section to
   contain:
  </para>
<screen>/var/log/freezer/freezer-scheduler.log</screen>
  <para>
   This configuration means that in the /var/log/freezer-agent directory, in a
   live environment, there should be a file called freezer-scheduler.log. As
   the log file grows, the cron process runs every hour to check the log file
   size against the settings in the configuration files. The example
   freezer-agent settings are described below.
  </para>
  <informaltable colsep="1" rowsep="1">
   <tgroup cols="7">
    <colspec colname="c1" colnum="1" colwidth="1.32*"/>
    <colspec colname="c2" colnum="2" colwidth="1*"/>
    <colspec colname="c3" colnum="3" colwidth="3.26*"/>
    <colspec colname="c4" colnum="4" colwidth="1.46*"/>
    <colspec colname="c5" colnum="5" colwidth="1.05*"/>
    <colspec colname="c6" colnum="6" colwidth="1.4*"/>
    <colspec colname="c7" colnum="7" colwidth="3.38*"/>
    <thead>
     <row>
      <entry>Service</entry>
      <entry>Node Type</entry>
      <entry>Rotated Log Files</entry>
      <entry>Frequency</entry>
      <entry>Max Size</entry>
      <entry>Rotation</entry>
      <entry>Centrally Logged Files</entry>
     </row>
    </thead>
    <tbody>
     <row>
      <entry>
       <para>
        Freezer
       </para>
      </entry>
      <entry>
       <para>
        Control
       </para>
      </entry>
      <entry>
       <para>
        /var/log/freezer/freezer-scheduler.log
       </para>
       <para>
        /var/log/freezer/freezer-agent-json.log
       </para>
      </entry>
      <entry>
       <para>
        Daily
       </para>
      </entry>
      <entry>
       <para>
        45 MB
       </para>
      </entry>
      <entry>
       <para>
        7
       </para>
      </entry>
      <entry>
       <para>
        /var/log/freezer-agent/freezer-agent-json.log
       </para>
      </entry>
     </row>
    </tbody>
   </tgroup>
  </informaltable>
  <para>
   For the <literal>freezer-scheduler.log</literal> file specifically, the
   information in the table tells the Logrotate process that the log file is to
   be rotated daily, and it can have a maximum size of 45 MB. After a week of
   log rotation, you might see something similar to this list:
  </para>
<screen>freezer-scheduler.log at 10K
freezer-scheduler.log.1 at 123K
freezer-scheduler.log.2.gz at 13K
freezer-scheduler.log.3.gz at 17K
freezer-scheduler.log.4.gz at 128K
freezer-scheduler.log.5.gz at 22K
freezer-scheduler.log.6.gz at 323K
freezer-scheduler.log.7.gz at 123K</screen>
  <para>
   Since the Rotation value is set to 7 for this log file, there will never be
   a <literal>freezer-scheduler.log.8.gz</literal>. When the cron process runs
   its checks, if the <literal>freezer-scheduler.log</literal> size is more
   than 45 MB, then Logrotate rotates the file.
  </para>
  <para>
   In this example, the following log files are rotated:
  </para>
<screen>/var/log/freezer/freezer-scheduler.log
/var/log/freezer/freezer-agent-json.log</screen>
  <para>
   However, in this example, only the following file is centrally logged with
   &elasticsearch;:
  </para>
<screen>/var/log/freezer/freezer-agent-json.log</screen>
  <para>
   Only files that are listed in the <emphasis role="bold">Centrally Logged
   Files</emphasis> section are copied to &elasticsearch;.
  </para>
  <para>
   All of the variables for the Logrotate process are found in the following
   file:
  </para>
<screen>~/scratch/ansible/next/ardana/ansible/roles/logging-ansible/logging-common/defaults/main.yml</screen>
  <para>
   Cron runs Logrotate hourly. Every 5 minutes another process is run called
   <emphasis role="bold">"logrotate_if_needed"</emphasis> which uses a
   watermark value to determine if the Logrotate process needs to be run. If
   the <emphasis role="bold">"high watermark"</emphasis> has been reached, and
   the /var/log partition is more than 95% full (by default - this can be
   adjusted), then Logrotate will be run within 5 minutes.
  </para>
 </section>
 <section xml:id="BUElasticsearch">
  <title>Are Log Files Backed-Up To &elasticsearch;?</title>
  <para>
   While centralized logging is enabled out of the box, the backup of these
   logs is not. The reason is because Centralized Logging relies on the
   &elasticsearch; FileSystem Repository plugin, which in turn requires shared
   disk partitions to be configured and accessible from each of the
   &elasticsearch; nodes. Since there are multiple ways to setup a shared disk
   partition, &kw-hos; allows you to choose an approach that works best for
   your deployment before enabling the back-up of log files to &elasticsearch;.
  </para>
  <para>
   If you enable automatic back-up of centralized log files, then all the logs
   collected from the cloud nodes will be backed-up to &elasticsearch;. Every
   hour, in the management controller nodes where &elasticsearch; is setup, a
   cron job runs to check if &elasticsearch; is running low on disk space. If the
   check succeeds, it further checks if the backup feature is enabled. If
   enabled, the cron job saves a snapshot of the &elasticsearch; indices to the
   configured shared disk partition using curator. Next, the script starts
   deleting the oldest index and moves down from there checking each time if
   there is enough space for &elasticsearch;. A check is also made to ensure that
   the backup runs only once a day.
  </para>
  <para>
   For steps on how to enable automatic back-up, see
   <xref linkend="central_log_configure_settings"/>.
  </para>
 </section>
</section>
