<?xml version="1.0"?>
<?xml-stylesheet href="urn:x-suse:xslt:profiling:docbook51-profile.xsl"
 type="text/xml"
 title="Profiling step"?>
<!DOCTYPE section [
 <!ENTITY % entities SYSTEM "entities.ent"> %entities;
]>
<section xmlns="http://docbook.org/ns/docbook" xmlns:xi="http://www.w3.org/2001/XInclude" xmlns:xlink="http://www.w3.org/1999/xlink" xml:id="dpdk_troubleshooting">
 <title>Troubleshooting DPDK</title>
 <itemizedlist>
  <listitem>
   <para>
    <xref linkend="hardware"/>
   </para>
  </listitem>
  <listitem>
   <para>
    <xref linkend="system"/>
   </para>
  </listitem>
  <listitem>
   <para>
    <xref linkend="inputModel"/>
   </para>
  </listitem>
  <listitem>
   <para>
    <xref linkend="reboot"/>
   </para>
  </listitem>
  <listitem>
   <para>
    <xref linkend="software"/>
   </para>
  </listitem>
  <listitem>
   <para>
    <xref linkend="runtime"/>
   </para>
  </listitem>
  <listitem>
   <para>
    <xref linkend="errors"/>
   </para>
  </listitem>
 </itemizedlist>
 <section xml:id="hardware">
  <title>Hardware configuration</title>
  <para>
   Because there are several variations of hardware, it is up to you to verify
   that the hardware is configured properly.
  </para>
  <itemizedlist>
<!-- Commented in DITA original: -->
<!--<li outputclass="highlightThis">Only Intel Niantic (82599) and Mellanox
ConnectX-3 Pro NICs are supported.</li>-->
   <listitem>
    <para>
     Only Intel based compute nodes are supported. There is no DPDK available
     for AMD-based CPUs.
    </para>
   </listitem>
   <listitem>
    <para>
     PCI-PT must be enabled for the NIC that will be used with DPDK.
    </para>
   </listitem>
   <listitem>
    <para>
     When using Intel Niantic and the igb_uio driver, the VT-d must be enabled
     in the BIOS.
    </para>
   </listitem>
   <listitem>
    <para>
     For DL360 Gen9 systems, the BIOS shared-memory
     <xref linkend="pcipt-gen9"/>.
    </para>
   </listitem>
   <listitem>
    <para>
     Adequate memory must be available for <xref linkend="hugepages"/> usage.
    </para>
   </listitem>
   <listitem>
    <para>
     Hyper-threading can be enabled but is not required for base functionality.
    </para>
   </listitem>
   <listitem>
    <para>
     Determine the PCI slot that the DPDK NIC(s) are installed in to
     determine the associated NUMA node.
    </para>
   </listitem>
   <listitem>
    <para>
     Only the Intel Haswell, Broadwell, and Skylake microarchitectures are
     supported.
     Intel Sandy Bridge is not supported.
    </para>
   </listitem>
  </itemizedlist>
 </section>
 <section xml:id="system">
  <title>System configuration</title>
  <itemizedlist>
   <listitem>
    <para>
     Only &sle_repo; compute nodes are supported.
    </para>
   </listitem>
   <listitem>
    <para>
     If a NIC port is used with PCI-PT, SRIOV-only, or PCI-PT+SRIOV, then it
     cannot be used with DPDK. They are mutually exclusive. This is because DPDK
     depends on an OvS bridge which does not exist if you use any combination of
     PCI-PT and SRIOV. You can use DPDK, SRIOV-only, and PCI-PT on difference
     interfaces of the same server.
    </para>
   </listitem>
   <listitem>
    <para>
     There is an association between the PCI slot for the NIC and a NUMA node.
     Make sure to use logical CPU cores that are on the NUMA node associated to
     the NIC. Use the following to determine which CPUs are on which NUMA node.
    </para>
<screen>&prompt.ardana;lscpu

Architecture:          x86_64
CPU op-mode(s):        32-bit, 64-bit
Byte Order:            Little Endian
CPU(s):                48
On-line CPU(s) list:   0-47
Thread(s) per core:    2
Core(s) per socket:    12
Socket(s):             2
NUMA node(s):          2
Vendor ID:             GenuineIntel
CPU family:            6
Model:                 63
Model name:            Intel(R) Xeon(R) CPU E5-2650L v3 @ 1.80GHz
Stepping:              2
CPU MHz:               1200.000
CPU max MHz:           1800.0000
CPU min MHz:           1200.0000
BogoMIPS:              3597.06
Virtualization:        VT-x
L1d cache:             32K
L1i cache:             32K
L2 cache:              256K
L3 cache:              30720K
NUMA node0 CPU(s):     0-11,24-35
NUMA node1 CPU(s):     12-23,36-47</screen>
   </listitem>
  </itemizedlist>
 </section>
 <section xml:id="inputModel">
  <title>Input model configuration</title>
  <itemizedlist>
   <listitem>
    <para>
     If you do not specify a driver for a DPDK device, the igb_uio will be
     selected as default.
    </para>
   </listitem>
   <listitem>
    <para>
     DPDK devices must be named <literal>dpdk&lt;port-id&gt;</literal> where
     the port-id starts at 0 and increments sequentially.
    </para>
   </listitem>
   <listitem>
    <para>
     Tenant networks supported are untagged VXLAN and VLAN.
    </para>
   </listitem>
   <listitem>
    <!-- SLES 12 SP3 has OVS 2.7 already. Do we still need the following? -
    sknorr, 2018-07-05 -->
    <para>
     Jumbo Frames MTU does not work with DPDK OvS. There is an upstream patch
     most likely showing up in OvS 2.6 and it cannot be backported due to
     changes this patch relies upon.
    </para>
   </listitem>
   <listitem>
    <para>
     Sample VXLAN model
    </para>
   </listitem>
   <listitem>
    <para>
     Sample VLAN model
    </para>
   </listitem>
  </itemizedlist>
 </section>
 <section xml:id="reboot">
  <title>Reboot requirements</title>
  <para>
   A reboot of a compute node must be performed when an input model change
   causes the following:
  </para>
  <orderedlist>
   <listitem>
    <para>
     After the initial <filename>site.yml</filename> play on a new &ostack;
     environment
    </para>
   </listitem>
   <listitem>
    <para>
     Changes to an existing &ostack; environment that modify the
     <literal>/etc/default/grub</literal> file, such as
    </para>
    <itemizedlist>
     <listitem>
      <para>
       hugepage allocations
      </para>
     </listitem>
     <listitem>
      <para>
       CPU isolation
      </para>
     </listitem>
     <listitem>
      <para>
       iommu changes
      </para>
     </listitem>
    </itemizedlist>
   </listitem>
   <listitem>
    <para>
     Changes to a NIC port usage type, such as
    </para>
    <itemizedlist>
     <listitem>
      <para>
       moving from DPDK to any combination of PCI-PT and SRIOV
      </para>
     </listitem>
     <listitem>
      <para>
       moving from DPDK to kernel based eth driver
      </para>
     </listitem>
    </itemizedlist>
   </listitem>
  </orderedlist>
 </section>
 <section xml:id="software">
  <title>Software configuration</title>
  <para>
   The input model is processed by the Configuration Processor which eventually
   results in changes to the OS. There are several files that should be checked
   to verify the proper settings were applied. In addition, after the inital
   site.yml play is run all compute nodes must be rebooted in order to pickup
   changes to the <filename>/etc/default/grub</filename> file for hugepage
   reservation, CPU isolation and iommu settings.
  </para>
  <para>
   <emphasis role="bold">Kernel settings</emphasis>
  </para>
  <para>
   Check <filename>/etc/default/grub</filename> for the following
  </para>
  <orderedlist>
   <listitem>
    <para>
     hugepages
    </para>
   </listitem>
   <listitem>
    <para>
     CPU isolation
    </para>
   </listitem>
   <listitem>
    <para>
     that iommu is in passthru mode if the igb_uio driver is in use
    </para>
   </listitem>
  </orderedlist>
  <para>
   <emphasis role="bold">Open vSwitch settings</emphasis>
  </para>
  <para>
   Check <filename>/etc/default/openvswitch-switchf</filename> for
  </para>
  <orderedlist>
   <listitem>
    <para>
     using the <literal>--dpdk</literal> option
    </para>
   </listitem>
   <listitem>
    <para>
     core 0 set aside for EAL and kernel to share
    </para>
   </listitem>
   <listitem>
    <para>
     cores assigned to PMD drivers, at least two for each DPDK device
    </para>
   </listitem>
   <listitem>
    <para>
     verify that memory is reserved with socket-mem option
    </para>
   </listitem>
   <listitem>
    <para>
     Once
     <link xlink:href="https://jira.hpcloud.net/browse/VNETCORE-2509">VNETCORE-2509</link>
     merges also verify that the umask is 022 and the group is libvirt-qemu
    </para>
   </listitem>
  </orderedlist>
  <para>
   <emphasis role="bold">DPDK settings</emphasis>
  </para>
  <orderedlist>
   <listitem>
    <para>
     check <filename>/etc/dpdk/interfacesf</filename> for the correct DPDK devices
    </para>
   </listitem>
  </orderedlist>
 </section>
 <section xml:id="runtime">
  <title>DPDK runtime</title>
  <para>
   All non-bonded DPDK devices will be added to individual OvS bridges. The
   bridges will be named <literal>br-dpdk0</literal>,
   <literal>br-dpdk1</literal>, etc. The name of the OvS bridge for bonded DPDK
   devices will be <literal>br-dpdkbond0</literal>,
   <literal>br-dpdkbond1</literal>, etc.
  </para>
  <orderedlist>
   <listitem>
    <para>
     Since each PMD thread is in a polling loop, it will use 100% of the CPU.
     Thus for two PMDs you would expect to see the ovs-vswitchd process running
     at 200%. This can be verified by running
    </para>
<screen>&prompt.ardana;top

top - 16:45:42 up 4 days, 22:24,  1 user,  load average: 2.03, 2.10, 2.14
Tasks: 384 total,   2 running, 382 sleeping,   0 stopped,   0 zombie
%Cpu(s):  9.0 us,  0.2 sy,  0.0 ni, 90.8 id,  0.0 wa,  0.0 hi,  0.0 si,  0.0 st
KiB Mem:  13171580+total, 10356851+used, 28147296 free,   257196 buffers
KiB Swap:        0 total,        0 used,        0 free.  1085868 cached Mem

  PID USER      PR  NI    VIRT    RES    SHR S  %CPU %MEM     TIME+ COMMAND
 1522 root      10 -10 6475196 287780  10192 S 200.4  0.2  14250:20 ovs-vswitchd</screen>
   </listitem>
<!---->
   <listitem>
    <para>
     Verify that <literal>ovs-vswitchd</literal> is running with
    </para>
<screen>--dpdk option. ps -ef | grep ovs-vswitchd </screen>
   </listitem>
   <listitem>
    <para>
     PMD thread(s) are started when a DPDK port is added to an OvS bridge.
     Verify the port is on the bridge.
    </para>
<screen>&prompt.sudo;ovs-vsctl show</screen>
   </listitem>
   <listitem>
    <para>
     A DPDK port cannot be added to an OvS bridge unless it is bound to a
     driver. Verify that the DPDK port is bound.
    </para>
<screen>&prompt.sudo;dpdk_nic_bind -s</screen>
   </listitem>
   <listitem>
    <para>
     Verify that the proper number of hugepages is on the correct NUMA node
    </para>
<screen>&prompt.sudo;virsh freepages --all</screen>
    <para>
     or
    </para>
<screen>&prompt.sudo;grep -R "" /sys/kernel/mm/hugepages/ /proc/sys/vm/*huge*</screen>
   </listitem>
   <listitem>
    <para>
     Verify that the VM and the DPDK PMD threads have both mapped the same
     hugepage(s)
    </para>
<screen># this will yield 2 process ids, use the 2nd one
&prompt.sudo;ps -ef | grep ovs-vswitchd
&prompt.sudo;ls -l /proc/<replaceable>PROCESS-ID</replaceable>/fd | grep huge

# if running more than 1 VM you will need to figure out which one to use
&prompt.sudo;ps -ef | grep qemu
&prompt.sudo;ls -l /proc/<replaceable>PROCESS-ID</replaceable>/fd | grep huge </screen>
   </listitem>
  </orderedlist>
 </section>
 <section xml:id="errors">
  <title>Errors</title>
  <para>
   <emphasis role="bold">VM does not get fixed IP</emphasis>
  </para>
  <orderedlist>
   <listitem>
    <para>
     DPDK Poll Mode drivers (PMD) communicates with the VM by direct access of
     the VM hugepage. If a VM is not created using hugepages
     (see <xref linkend="hugepages"/>),
     there is no way for DPDK to communicate with the VM and the VM will never
     be connected to the network.
    </para>
   </listitem>
   <listitem>
    <para>
     It has been observed that the DPDK communication with VM fails if the
     shared-memory is not disabled in BIOS for DL360 Gen9.
    </para>
   </listitem>
  </orderedlist>
  <para>
   <emphasis role="bold">Vestiges of non-existent DPDK devices</emphasis>
  </para>
  <orderedlist>
   <listitem>
    <para>
     Incorrect input models that do not use the correct DPDK device name or
     do not use sequential port IDs starting at 0 may leave non-existent devices
     in the OvS database. While this does not affect proper functionality it may
     be confusing.
    </para>
   </listitem>
  </orderedlist>
  <para>
   <emphasis role="bold">Startup issues</emphasis>
  </para>
  <orderedlist>
   <listitem>
    <para>
     Running the following will help diagnose startup issues with ovs-vswitchd:
    </para>
<screen>&prompt.sudo;journalctl -u openvswitch.service --all</screen>
   </listitem>
  </orderedlist>
 </section>
</section>
