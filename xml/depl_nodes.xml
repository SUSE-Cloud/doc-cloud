<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE chapter
[
  <!ENTITY % entities SYSTEM "entity-decl.ent">
  %entities;
]>
<!-- Converted by suse-upgrade version 1.1 -->
<chapter xmlns="http://docbook.org/ns/docbook" xmlns:xi="http://www.w3.org/2001/XInclude" xmlns:xlink="http://www.w3.org/1999/xlink" version="5.0" xml:id="cha.depl.ostack">
 <title>Deploying the &ostack; Services</title>
 <info>
  <dm:docmanager xmlns:dm="urn:x-suse:ns:docmanager">
   <dm:maintainer>fs</dm:maintainer>
   <dm:status>editing</dm:status>
   <dm:deadline/>
   <dm:priority/>
   <dm:translation>no</dm:translation>
   <dm:languages/>
  </dm:docmanager>
 </info>
 <para>
  After the nodes are installed and configured you can start deploying the
  &ostack; components to finalize the installation. The components need to be
  deployed in a given order, because they depend on one another. The
  <guimenu>Pacemaker</guimenu> component for an &hasetup; is the only exception
  from this rule&mdash;it can be set up at any time. However, when deploying
  &productname; from scratch, we recommend deploying the
  <guimenu>Pacemaker</guimenu> proposal(s) first. Deployment for all components
  is done from the &crow; Web interface through recipes, so-called
  <quote>&barcl;s</quote>. (See <xref linkend="sec.depl.services"/> for a table
  of all roles and services, and how to start and stop them.)
 </para>
 <para>
  The components controlling the cloud, including storage management and
  control components, need to be installed on the &contrnode;(s) (refer to
  <xref linkend="sec.depl.arch.components.control"/> for more information).
  However, you may <emphasis>not</emphasis> use your &contrnode;(s) as a
  compute node or storage host for &o_objstore;. Do not install the components
  <guimenu>swift-storage</guimenu> and <guimenu>nova-compute-*</guimenu> on the
  &contrnode;(s). These components must be installed on dedicated &stornode;s
  and &compnode;s.
 </para>
 <para>
  When deploying an &hasetup;, the  &contrnode;s are replaced by one or more
  controller clusters consisting of at least two nodes, and three are
  recommended. We recommend setting up three separate clusters for data,
  services, and networking. See <xref linkend="sec.depl.req.ha"/> for more
  information on requirements and recommendations for an &hasetup;.
 </para>
 <para>
  The &ostack; components need to be deployed in the following order. For
  general instructions on how to edit and deploy &barcl;s, refer to
  <xref linkend="sec.depl.ostack.barclamps"/>. Any optional components that you
  elect to use must be installed in their correct order.
 </para>
 <orderedlist spacing="normal">
  <listitem>
<!-- Pacemaker -->
   <para>
    <xref linkend="sec.depl.ostack.pacemaker" xrefstyle="select:title nopage"/>
   </para>
  </listitem>
  <listitem>
<!-- Database -->
   <para>
    <xref linkend="sec.depl.ostack.db" xrefstyle="select:title nopage"/>
   </para>
  </listitem>
  <listitem>
<!-- RabbitMQ -->
   <para>
    <xref linkend="sec.depl.ostack.rabbit" xrefstyle="select:title nopage"/>
   </para>
  </listitem>
  <listitem>
<!-- Keystone -->
   <para>
    <xref linkend="sec.depl.ostack.keystone" xrefstyle="select:title nopage"/>
   </para>
  </listitem>
  <listitem>
<!-- Swift -->
   <para>
    <xref linkend="sec.depl.ostack.swift" xrefstyle="select:title nopage"/>
   </para>
  </listitem>
  <listitem>
<!-- Glance -->
   <para>
    <xref linkend="sec.depl.ostack.glance" xrefstyle="select:title nopage"/>
   </para>
  </listitem>
<!-- Cinder -->
  <listitem>
   <para>
    <xref linkend="sec.depl.ostack.cinder" xrefstyle="select:title nopage"/>
   </para>
  </listitem>
<!-- Neutrum -->
  <listitem>
   <para>
    <xref linkend="sec.depl.ostack.quantum" xrefstyle="select:title nopage"/>
   </para>
  </listitem>
<!-- Nova -->
  <listitem>
   <para>
    <xref linkend="sec.depl.ostack.nova" xrefstyle="select:title nopage"/>
   </para>
  </listitem>
<!-- Horizon -->
  <listitem>
   <para>
    <xref linkend="sec.depl.ostack.dash" xrefstyle="select:title nopage"/>
   </para>
  </listitem>
<!-- Heat -->
  <listitem>
   <para>
    <xref linkend="sec.depl.ostack.heat" xrefstyle="select:title nopage"/>
   </para>
  </listitem>
<!-- Ceilometer -->
  <listitem>
   <para>
    <xref linkend="sec.depl.ostack.ceilometer" xrefstyle="select:title nopage"/>
   </para>
  </listitem>
<!-- Manila -->
  <listitem>
   <para>
    <xref linkend="sec.depl.ostack.manila" xrefstyle="select:title nopage"/>
   </para>
  </listitem>
<!-- Tempest -->
  <listitem>
   <para>
    <xref linkend="sec.depl.ostack.tempest" xrefstyle="select:title nopage"/>
   </para>
  </listitem>
<!-- Magnum -->
  <listitem>
   <para>
    <xref linkend="sec.depl.ostack.magnum" xrefstyle="select:title nopage"/>
   </para>
  </listitem>
<!-- Monasca -->
  <listitem>
   <para>
    <xref linkend="sec.depl.ostack.monasca" xrefstyle="select:title nopage"/>
   </para>
  </listitem>
 </orderedlist>
 <sect1 xml:id="sec.depl.ostack.pacemaker">
  <title>Deploying Pacemaker (Optional, &haSetup; Only)</title>

  <para>
   To make the &cloud; controller functions and the &compnode;s highly
   available, set up one or more clusters by deploying Pacemaker (see
   <xref
   linkend="sec.depl.req.ha"/> for details). Since it is possible (and
   recommended) to deploy more than one cluster, a separate proposal needs to
   be created for each cluster.
  </para>

  <para>
   Deploying Pacemaker is optional. In case you do not want to deploy it, skip
   this section and start the node deployment by deploying the database as
   described in <xref linkend="sec.depl.ostack.db"/>.
  </para>

  <note>
   <title>Number of Cluster Nodes</title>
   <para>
    To set up a cluster, at least two nodes are required. <!-- Since SOC 8
    doesn't support DRBD this should be either removed or replaces wirh
    something else.

    If you are setting up
    a cluster for storage with replicated storage via DRBD (for example for a
    cluster for the database and RabbitMQ), exactly two nodes are required. For
    all other setups an odd number of nodes with a minimum of three nodes is
    strongly recommended. --> See <xref linkend="sec.depl.reg.ha.general"/> for
    more information.
   </para>
  </note>

  <para>
   To create a proposal, go to <menuchoice> <guimenu>Barclamps</guimenu>
   <guimenu>OpenStack</guimenu> </menuchoice> and click <guimenu>Edit</guimenu>
   for the Pacemaker &barcl;. A drop-down box where you can enter a name and a
   description for the proposal opens. Click <guimenu>Create</guimenu> to open
   the configuration screen for the proposal.
  </para>

  <informalfigure>
   <mediaobject>
    <textobject><phrase>Create Pacemaker Proposal</phrase>
    </textobject>
    <imageobject role="fo">
     <imagedata fileref="depl_barclamp_pacemaker_proposal.png" width="100%" format="png"/>
    </imageobject>
    <imageobject role="html">
     <imagedata fileref="depl_barclamp_pacemaker_proposal.png" width="75%" format="png"/>
    </imageobject>
   </mediaobject>
  </informalfigure>

  <important xml:id="ann.depl.ostack.pacemaker.prop_name">
   <title>Proposal Name</title>
   <para>
    The name you enter for the proposal will be used to generate host names for
    the virtual IP addresses of HAProxy. By default, the names follow this
    scheme:
   </para>
   <simplelist>
    <member><literal>cluster-<replaceable>PROPOSAL_NAME</replaceable>.<replaceable>FQDN</replaceable></literal>
    (for the internal name)</member>
    <member><literal>public-cluster-<replaceable>PROPOSAL_NAME</replaceable>.<replaceable>FQDN</replaceable></literal>
    (for the public name)</member>
   </simplelist>
   <para>
    For example, when <replaceable>PROPOSAL_NAME</replaceable> is set to
    <literal>data</literal>, this results in the following names:
   </para>
   <simplelist>
    <member><literal>cluster-data.&exampledomain;</literal>
    </member>
    <member><literal>public-cluster-data.&exampledomain;</literal>
    </member>
   </simplelist>
   <para>
    For requirements regarding SSL encryption and certificates, see
    <xref
     linkend="sec.depl.req.ssl"/>.
   </para>
  </important>

  <para>
   The following options are configurable in the Pacemaker configuration
   screen:
  </para>

  <variablelist>
   <varlistentry>
    <term>Transport for Communication</term>
    <listitem>
     <para>
      Choose a technology used for cluster communication. You can choose
      between <guimenu>Multicast (UDP)</guimenu>, sending a message to multiple
      destinations, or <guimenu>Unicast (UDPU)</guimenu>, sending a message to
      a single destination. By default unicast is used.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><guimenu>Policy when cluster does not have quorum</guimenu>
    </term>
    <listitem>
     <para>
      Whenever communication fails between one or more nodes and the rest of
      the cluster a <quote>cluster partition</quote> occurs. The nodes of a
      cluster are split in partitions but are still active. They can only
      communicate with nodes in the same partition and are unaware of the
      separated nodes. The cluster partition that has the majority of nodes is
      defined to have <quote>quorum</quote>.
     </para>
     <para>
      This configuration option defines what to do with the cluster
      partition(s) that do not have the quorum. See
      <link xlink:href="&suse-onlinedoc;/sle-ha-12/book_sleha/data/sec_ha_config_basics_global.html"/>,
      section <citetitle>Option no-quorum-policy</citetitle> for details.
     </para>
     <para>
      The recommended setting is to choose <guimenu>Stop</guimenu>. However,
      <guimenu>Ignore</guimenu> is enforced for two-node clusters to ensure
      that the remaining node continues to operate normally in case the other
      node fails. For clusters using shared resources, choosing
      <guimenu>freeze</guimenu> may be used to ensure that these resources
      continue to be available.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry xml:id="vle.pacemaker.barcl.stonith">
    <term>STONITH: Configuration mode for &stonith;
    </term>
    <listitem>
     <para>
      <quote>Misbehaving</quote> nodes in a cluster are shut down to prevent
      them from causing trouble. This mechanism is called &stonith;
      (<quote>Shoot the other node in the head</quote>). &stonith; can be
      configured in a variety of ways, refer to
      <link xlink:href="&suse-onlinedoc;/sle-ha-12/book_sleha/data/cha_ha_fencing.html"/>
      for details. The following configuration options exist:
     </para>
     <variablelist>
      <varlistentry>
       <term><guimenu>Configured manually</guimenu>
       </term>
       <listitem>
        <para>
         &stonith; will not be configured when deploying the &barcl;. It needs
         to be configured manually as described in
         <link xlink:href="&suse-onlinedoc;/sle-ha-12/book_sleha/data/cha_ha_fencing.html"/>.
         For experts only.
        </para>
       </listitem>
      </varlistentry>
      <varlistentry>
       <term><guimenu>Configured with IPMI data from the IPMI &barcl;</guimenu>
       </term>
       <listitem>
        <para>
         Using this option automatically sets up &stonith; with data received
         from the IPMI &barcl;. Being able to use this option requires that
         IPMI is configured for all cluster nodes. This should be done by
         default. To check or change the IPMI deployment, go to <menuchoice>
         <guimenu>Barclamps</guimenu> <guimenu>Crowbar</guimenu>
         <guimenu>IPMI</guimenu> <guimenu>Edit</guimenu> </menuchoice>. Also
         make sure the <guimenu>Enable BMC</guimenu> option is set to
         <guimenu>true</guimenu> on this &barcl;.
        </para>
        <important>
         <title>&stonith; Devices Must Support IPMI</title>
         <para>
          To configure &stonith; with the IPMI data, <emphasis>all</emphasis>
          &stonith; devices must support IPMI. Problems with this setup may
          occur with IPMI implementations that are not strictly standards
          compliant. In this case it is recommended to set up &stonith; with
          &stonith; block devices (SBD).
         </para>
        </important>
       </listitem>
      </varlistentry>
      <varlistentry>
       <term><guimenu>Configured with &stonith; Block Devices (SBD)</guimenu>
       </term>
       <listitem>
        <para>
         This option requires manually setting up shared storage and a watchdog
         on the cluster nodes before applying the proposal. To do so, proceed
         as follows:
        </para>
        <orderedlist spacing="normal">
         <listitem>
          <para>
           Prepare the shared storage. The path to the shared storage device
           must be persistent and consistent across all nodes in the cluster.
           The SBD device must not use host-based RAID or cLVM2.
          </para>
         </listitem>
         <listitem>
          <para>
           Install the package <systemitem class="resource">sbd</systemitem> on
           all cluster nodes.
          </para>
         </listitem>
         <listitem>
          <para>
           Initialize the SBD device with by running the following command.
           Make sure to replace
           <filename>/dev/<replaceable>SBD</replaceable></filename> with the
           path to the shared storage device.
          </para>
<screen>sbd -d /dev/<replaceable>SBD</replaceable> create</screen>
          <para>
           Refer to
           <link xlink:href="&suse-onlinedoc;/sle-ha-12/book_sleha/data/sec_ha_storage_protect_fencing.html#pro_ha_storage_protect_sbd_create"/>
           for details.
          </para>
         </listitem>
        </orderedlist>
        <para>
         In <guimenu>Kernel module for watchdog</guimenu>, specify the
         respective kernel module to be used. Find the most commonly used
         watchdog drivers in the following table:
        </para>
<!--taroth 2016-11-28: table taken from
         https://github.com/SUSE/doc-sleha/blob/develop/xml/ha_storage_protection.xml,
         pro.ha.storage.protect.watchdog-->
        <informaltable>
         <tgroup cols="2">
          <thead>
           <row>
            <entry>Hardware</entry>
            <entry>Driver</entry>
           </row>
          </thead>
          <tbody>
           <row>
            <entry>HP</entry>
            <entry><systemitem class="resource">hpwdt</systemitem>
            </entry>
           </row>
           <row>
            <entry>Dell, Fujitsu, Lenovo (Intel TCO)</entry>
            <entry><systemitem class="resource">iTCO_wdt</systemitem>
            </entry>
           </row>
           <row>
            <entry>Xen VM (DomU)</entry>
            <entry><systemitem class="resource">xen_xdt</systemitem>
            </entry>
           </row>
           <row>
            <entry>Generic</entry>
            <entry><systemitem class="resource">softdog</systemitem>
            </entry>
           </row>
          </tbody>
         </tgroup>
        </informaltable>
        <para>
         If your hardware is not listed above, either ask your hardware vendor
         for the right name or check the following directory for a list of
         choices:
         <filename>/lib/modules/<replaceable>KERNEL_VERSION</replaceable>/kernel/drivers/watchdog</filename>.
        </para>
        <para>
         Alternatively, list the drivers that have been installed with your
         kernel version:
        </para>
<screen>&prompt.root;<command>rpm</command> -ql kernel-<replaceable>VERSION</replaceable> | <command>grep</command> watchdog</screen>
        <para>
         If the nodes need different watchdog modules, leave the text box
         empty.
        </para>
        <para>
         After the shared storage has been set up, specify the path using the
         <quote>by-id</quote> notation
         (<filename>/dev/disk/by-id/<replaceable>DEVICE</replaceable></filename>).
         It is possible to specify multiple paths as a comma-separated list.
        </para>
        <para>
         Deploying the &barcl; will automatically complete the SBD setup on the
         cluster nodes by starting the SBD daemon and configuring the fencing
         resource.
        </para>
       </listitem>
      </varlistentry>
      <varlistentry>
       <term><guimenu>
         Configured with one shared resource for the whole cluster
        </guimenu>
       </term>
       <listitem>
        <para>
         All nodes will use the identical configuration. Specify the
         <guimenu>Fencing Agent</guimenu> to use and enter
         <guimenu>Parameters</guimenu> for the agent.
        </para>
        <para>
         To get a list of &stonith; devices which are supported by the High
         Availability Extension, run the following command on an already
         installed cluster nodes: <command>stonith -L</command>. The list of
         parameters depends on the respective agent. To view a list of
         parameters use the following command:
        </para>
<screen>stonith -t <replaceable>agent</replaceable> -n</screen>
       </listitem>
      </varlistentry>
      <varlistentry>
       <term><guimenu>Configured with one resource per node</guimenu>
       </term>
       <listitem>
        <para>
         All nodes in the cluster use the same <guimenu>Fencing
         Agent</guimenu>, but can be configured with different parameters. This
         setup is, for example, required when nodes are in different chassis
         and therefore need different IPMI parameters.
        </para>
        <para>
         To get a list of &stonith; devices which are supported by the High
         Availability Extension, run the following command on an already
         installed cluster nodes: <command>stonith -L</command>. The list of
         parameters depends on the respective agent. To view a list of
         parameters use the following command:
        </para>
<screen>stonith -t <replaceable>agent</replaceable> -n</screen>
       </listitem>
      </varlistentry>
      <varlistentry>
       <term><guimenu>Configured for nodes running in libvirt</guimenu>
       </term>
       <listitem>
        <para>
         Use this setting for completely virtualized test installations. This
         option is not supported.
        </para>
       </listitem>
      </varlistentry>
     </variablelist>
    </listitem>
   </varlistentry>
   <varlistentry xml:id="var.depl.ostack.pacemaker.corosync_fencing">
    <term>STONITH: Do not start corosync on boot after fencing</term>
    <listitem>
     <para>
      With &stonith;, Pacemaker clusters with two nodes may sometimes hit an
      issue known as &stonith; deathmatch where each node kills the other one,
      resulting in both nodes rebooting all the time. Another similar issue in
      Pacemaker clusters is the fencing loop, where a reboot caused by
      &stonith; will not be enough to fix a node and it will be fenced again
      and again.
     </para>
     <para>
      This setting can be used to limit these issues. When set to
      <guimenu>true</guimenu>, a node that has not been properly shut down or
      rebooted will not start the services for Pacemaker on boot. Instead, the
      node will wait for action from the &cloud; operator. When set to
      <guimenu>false</guimenu>, the services for Pacemaker will always be
      started on boot. The <guimenu>Automatic</guimenu> value is used to have
      the most appropriate value automatically picked: it will be
      <guimenu>true</guimenu> for two-node clusters (to avoid &stonith;
      deathmatches), and <guimenu>false</guimenu> otherwise.
     </para>
     <para>
      When a node boots but not starts corosync because of this setting, then
      the node's status is in the <guimenu>Node Dashboard</guimenu> is set to
      "<literal>Problem</literal>" (red dot).
      <!-- dpopov 2018-05-24:
     This should be updated once SOC7->SOC8 instructions are available

     To make this node usable again,
      see <xref linkend="sec.deploy.ha_recovery.contr.node.add"/>. -->
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>Mail Notifications: Enable Mail Notifications</term>
    <listitem>
     <para>
      Get notified of cluster node failures via e-mail. If set to
      <guimenu>true</guimenu>, you need to specify which <guimenu>SMTP
      Server</guimenu> to use, a prefix for the mails' subject and sender and
      recipient addresses. Note that the SMTP server must be accessible by the
      cluster nodes.
     </para>
    </listitem>
   </varlistentry>
   <!-- FIXME: Remove or replace DRDB
   <varlistentry>
    <term>DRBD: Prepare Cluster for DRBD</term>
    <listitem>
     <para>
      Set up DRBD for replicated storage on the cluster. This option requires a
      two-node cluster with a spare hard disk for each node. The disks should
      have a minimum size of 100 GB. Using DRBD is recommended for making the
      database and RabbitMQ highly available. For other clusters, set this
      option to <guimenu>False</guimenu>.
     </para>
    </listitem>
   </varlistentry> -->
   <varlistentry>
    <term><guimenu>HAProxy: Public name for public virtual IP</guimenu>
    </term>
    <listitem>
     <para>
      The public name is the host name that will be used instead of the
      generated public name (see
      <xref linkend="ann.depl.ostack.pacemaker.prop_name"/>) for the public
      virtual IP address of &haproxy;. (This is the case when registering
      public endpoints, for example). Any name specified here needs to be
      resolved by a name server placed outside of the &cloud; network.
     </para>
    </listitem>
   </varlistentry>
  </variablelist>

  <figure>
   <title>The Pacemaker &Barcl;</title>
   <mediaobject>
    <imageobject role="fo">
     <imagedata fileref="depl_barclamp_pacemaker.png" width="100%" format="png"/>
    </imageobject>
    <imageobject role="html">
     <imagedata fileref="depl_barclamp_pacemaker.png" width="75%" format="png"/>
    </imageobject>
   </mediaobject>
  </figure>

  <para>
   The Pacemaker component consists of the following roles. Deploying the
   <guimenu>hawk-server</guimenu> role is optional:
  </para>

  <variablelist>
   <varlistentry>
    <term><guimenu>pacemaker-cluster-member</guimenu>
    </term>
    <listitem>
     <para>
      Deploy this role on all nodes that should become member of the cluster.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><guimenu>hawk-server</guimenu>
    </term>
    <listitem>
     <para>
      Deploying this role is optional. If deployed, sets up the &hawk; Web
      interface which lets you monitor the status of the cluster. The Web
      interface can be accessed via
      <literal>https://<replaceable>IP-ADDRESS</replaceable>:7630</literal>. The
      default hawk credentials are username <literal>hacluster</literal>, password
      <literal>crowbar</literal>.
      </para>
      <para>
      The password is visible and editable in the <guimenu>Custom</guimenu> view of the Pacemaker barclamp, and also in the <literal>"corosync":</literal> section of the
      <guimenu>Raw</guimenu> view.
  </para>
  <para>
      Note that the GUI on &cloud; can only be used to monitor the cluster
      status and not to change its configuration.
     </para>
     <para>
      <guimenu>hawk-server</guimenu> may be deployed on at least one cluster
      node. It is recommended to deploy it on all cluster nodes.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><guimenu>pacemaker-remote</guimenu>
    </term>
    <listitem>
     <para>
      Deploy this role on all nodes that should become members of the
      &compnode;s cluster. They will run as Pacemaker remote nodes that are
      controlled by the cluster, but do not affect quorum. Instead of the
      complete cluster stack, only the <literal>pacemaker-remote</literal>
      component will be installed on this nodes.
     </para>
    </listitem>
   </varlistentry>
  </variablelist>

  <figure>
   <title>The Pacemaker &Barcl;: Node Deployment Example</title>
   <mediaobject>
    <imageobject role="fo">
     <imagedata fileref="depl_barclamp_pacemaker_node_deployment.png" width="100%" format="png"/>
    </imageobject>
    <imageobject role="html">
     <imagedata fileref="depl_barclamp_pacemaker_node_deployment.png" width="100%" format="png"/>
    </imageobject>
   </mediaobject>
  </figure>

  <para>
   After a cluster has been successfully deployed, it is listed under
   <guimenu>Available Clusters</guimenu> in the <guimenu>Deployment</guimenu>
   section and can be used for role deployment like a regular node.
  </para>

  <warning>
   <title>Deploying Roles on Single Cluster Nodes</title>
   <para>
    When using clusters, roles from other &barcl;s must never be deployed to
    single nodes that are already part of a cluster. The only exceptions from
    this rule are the following roles:
   </para>
   <itemizedlist mark="bullet" spacing="normal">
    <listitem>
     <para>
      cinder-volume
     </para>
    </listitem>
    <listitem>
     <para>
      swift-proxy + swift-dispersion
     </para>
    </listitem>
    <listitem>
     <para>
      swift-ring-compute
     </para>
    </listitem>
    <listitem>
     <para>
      swift-storage
     </para>
    </listitem>
   </itemizedlist>
  </warning>

<!--taroth 2017-02-01: commenting as it is still not possible to update this screenshot-->

<!-- <figure>
   <title>Available Clusters in the Deployment Section</title>
   <mediaobject>
    <imageobject role="fo">
     <imagedata fileref="depl_barclamp_database_cluster_deployment.png" width="100%" format="png"/>
    </imageobject>
    <imageobject role="html">
     <imagedata fileref="depl_barclamp_database_cluster_deployment.png" width="75%" format="png"/>
    </imageobject>
   </mediaobject>
  </figure>-->

  <important>
   <title>Service Management on the Cluster</title>
   <para>
    After a role has been deployed on a cluster, its services are managed by
    the HA software. You must <emphasis>never</emphasis> manually start or stop
    an HA-managed service, nor configure it to start on boot. Services may only
    be started or stopped by using the cluster management tools Hawk or the crm
    shell. See
    <link xlink:href="&suse-onlinedoc;/sle-ha-12/book_sleha/data/sec_ha_config_basics_resources.html"/>
    for more information.
   </para>
  </important>

  <note>
   <title>Testing the Cluster Setup</title>
   <para>
    To check whether all cluster resources are running, either use the &hawk;
    Web interface or run the command <command>crm_mon</command>
    <option>-1r</option>. If it is not the case, clean up the respective
    resource with <command>crm</command> <option>resource</option>
    <option>cleanup</option> <replaceable>RESOURCE</replaceable> , so it gets
    respawned.
   </para>
   <para>
    Also make sure that &stonith; correctly works before continuing with the
    &cloud; setup. This is especially important when having chosen a &stonith;
    configuration requiring manual setup. To test if &stonith; works, log in to
    a node on the cluster and run the following command:
   </para>
<screen>pkill -9 corosync</screen>
   <para>
    In case &stonith; is correctly configured, the node will reboot.
   </para>
   <para>
    Before testing on a production cluster, plan a maintenance window in case
    issues should arise.
   </para>
  </note>
 </sect1>
 <sect1 xml:id="sec.depl.ostack.db">
  <title>Deploying the Database</title>

  <para>
   The very first service that needs to be deployed is the
   <guimenu>Database</guimenu>. The database component is using &mariadb; and
   is used by all other components. It must be installed on a &contrnode;. The
   Database can be made highly available by deploying it on a cluster.
  </para>

  <remark condition="clarity">
   2014-03-28 - fs: How to set up shared storage or DRBD for the data?
  </remark>

  <para>
   The only attribute you may change is the maximum number of database
   connections (<guimenu>Global Connection Limit</guimenu>). The default value
   should usually work&mdash;only change it for large deployments in case the
   log files show database connection failures.
  </para>

  <figure>
   <title>The Database &Barcl;</title>
   <mediaobject>
    <imageobject role="fo">
     <imagedata fileref="depl_barclamp_database.png" width="100%" format="png"/>
    </imageobject>
    <imageobject role="html">
     <imagedata fileref="depl_barclamp_database.png" width="75%" format="png"/>
    </imageobject>
   </mediaobject>
  </figure>

  <sect2 xml:id="sec.depl.ostack.db.mariadb">
    <title>Deploying &mariadb;</title>
    <para>Deploying the database requires the use of &mariadb;</para>

    <note>
      <title>&mariadb; and HA</title>
      <para>
        &mariadb; back end features full HA support based on the Galera
        clustering technology. The HA setup requires an odd number of
        nodes. The recommended number of nodes is 3.
      </para>
    </note>

    <sect3 xml:id="sec.depl.ostack.db.mariadb.ssl">
      <title>SSL Configuration</title>

      <para>
        SSL can be enabled with either a stand-alone or cluster deployment.
        The replication traffic between database nodes is not encrypted,
        whilst traffic between the database server(s) and clients are, so
        a separate network for the database servers is recommended.
      </para>

      <para>
        Certificates can be provided, or the barcamp can generate self-signed
        certificates. The certificate filenames are configurable in the
        barclamp, and the directories <literal>/etc/mysql/ssl/certs</literal>
        and <literal>/etc/mysql/ssl/private</literal> to use the defaults will
        need to be created before the barclamp is applied. The CA certificate
        and the certificate for &mariadb; to use both go into
        <literal>/etc/mysql/ssl/certs</literal>. The appropriate private key
        for the certificate is placed into the
        <literal>/etc/mysql/ssl/private</literal> directory. As long as the
        files are readable when the barclamp is deployed, permissions can be
        tightened after a successful deployment once the appropriate UNIX
        groups exist.
      </para>

      <para>
        The Common Name (CN) for the SSL certificate must be <literal>fully
        qualified server name</literal> for single host deployments, and
        cluster-<literal>cluster name</literal>.<literal>full domain
        name</literal> for cluster deployments.
      </para>

      <note>
        <title>Certificate validation errors</title>
        <para>
          If certificate validation errors are causing issues with deploying
          other barclamps (for example, when creating databases or users) you
          can check the configuration with
          <command>mysql --ssl-verify-server-cert</command> which will perform
          the same verification that Crowbar does when connecting to the
          database server.
        </para>
      </note>

      <para>
        If certificates are supplied, the CA certificate and its full trust
        chain must be in the <literal>ca.pem</literal> file. The certificate
        must be trusted by the machine (or all cluster members in a cluster
        deployment), and it must be available on all client machines &mdash;
        IE, if the &ostack; services are deployed on separate machines or
        cluster members they will all require the CA certificate to be in
        <literal>/etc/mysql/ssl/certs</literal> as well as trusted by the
        machine.
      </para>
    </sect3>

  <sect3>
  <title>&mariadb; Configuration Options</title>
  <figure>
   <title>&mariadb; Configuration</title>
   <mediaobject>
    <imageobject role="fo">
     <imagedata fileref="depl_barclamp_database_mariadb.png" width="100%" format="png"/>
    </imageobject>
    <imageobject role="html">
     <imagedata fileref="depl_barclamp_database_mariadb.png" width="75%" format="png"/>
    </imageobject>
   </mediaobject>
  </figure>

    <para>
      The following configuration settings are available via the <guimenu>Database</guimenu> &barcl;
      graphical interface:
    </para>
    <variablelist>
      <varlistentry>
        <term>
          Datadir
        </term>
        <listitem>
          <para>
            Path to a directory for storing database data.
          </para>
        </listitem>
      </varlistentry>
      <varlistentry>
        <term>
          Maximum Number of Simultaneous Connections
        </term>
        <listitem>
          <para>
            The maximum number of simultaneous client connections.
          </para>
        </listitem>
      </varlistentry>
      <varlistentry>
        <term>
          Number of days after the binary logs can be automatically removed
        </term>
        <listitem>
          <para>
            A period after which the binary logs are removed.
          </para>
        </listitem>
      </varlistentry>
            <varlistentry>
        <term>
          Slow Query Logging
        </term>
        <listitem>
          <para>
            When enabled, all queries that take longer than usual to execute
            are logged to a separate log file (by default, it's
            <filename>/var/log/mysql/mysql_slow.log</filename>). This can be
            useful for debugging.
          </para>
        </listitem>
      </varlistentry>
    </variablelist>

    <warning>
      <title>&mariadb; Deployment Restriction</title>
      <para>
        When &mariadb; is used as the database back end, the <guimenu>monasca-server</guimenu>
        role cannot be deployed to the node with the
        <guimenu>database-server</guimenu> role. These two roles cannot
        coexist due to the fact that
        &o_monitor; uses its own &mariadb; instance.
      </para>
    </warning>
  </sect3>

  </sect2>
 </sect1>

 <sect1 xml:id="sec.depl.ostack.rabbit">
  <title>Deploying RabbitMQ</title>

  <para>
   The RabbitMQ messaging system enables services to communicate with the other
   nodes via Advanced Message Queue Protocol (AMQP). Deploying it is mandatory.
   RabbitMQ needs to be installed on a &contrnode;. RabbitMQ can be made highly
   available by deploying it on a cluster. We recommend not changing the
   default values of the proposal's attributes.
  </para>

  <variablelist>
   <varlistentry>
    <term><guimenu>Virtual Host</guimenu>
    </term>
    <listitem>
     <para>
      Name of the default virtual host to be created and used by the RabbitMQ
      server (<literal>default_vhost</literal> configuration option in
      <filename>rabbitmq.config</filename>).
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>Port</term>
    <listitem>
     <para>
      Port the RabbitMQ server listens on (<literal>tcp_listeners</literal>
      configuration option in <filename>rabbitmq.config</filename>).
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>User</term>
    <listitem>
     <para>
      RabbitMQ default user (<literal>default_user</literal> configuration
      option in <filename>rabbitmq.config</filename>).
     </para>
    </listitem>
   </varlistentry>
  </variablelist>

  <figure>
   <title>The RabbitMQ &Barcl;</title>
   <mediaobject>
    <imageobject role="fo">
     <imagedata fileref="depl_barclamp_rabbitmq.png" width="100%" format="png"/>
    </imageobject>
    <imageobject role="html">
     <imagedata fileref="depl_barclamp_rabbitmq.png" width="75%" format="png"/>
    </imageobject>
   </mediaobject>
  </figure>

  <sect2 xml:id="sec.depl.ostack.rabbit.ha">
   <title>&haSetup; for RabbitMQ</title>
   <para>
    To make RabbitMQ highly available, deploy it on a cluster instead of a
    single &contrnode;. This also requires shared storage for the cluster that
    hosts the RabbitMQ data. We recommend using a dedicated cluster to deploy
    RabbitMQ together with the database,
    since both components require shared storage.
   </para>
   <para>
    Deploying RabbitMQ on a cluster makes an additional <guimenu>High
    Availability</guimenu> section available in the
    <guimenu>Attributes</guimenu> section of the proposal. Configure the
    <guimenu>Storage Mode</guimenu> in this section.
   </para>
  </sect2>

  <sect2 xml:id="sec.depl.ostack.rabbitmq.ssl">
   <title>SSL Configuration for RabbitMQ</title>
   <para>
    The RabbitMQ barclamp supports securing traffic via SSL. This is similar to
    the SSL support in other barclamps, but with these differences:
   </para>
   <itemizedlist>
    <listitem>
     <para>
      RabbitMQ can listen on two ports at the same time, typically port 5672
      for unsecured and port 5671 for secured traffic.
     </para>
    </listitem>
    <listitem>
     <para>
      The &o_meter; pipeline for &ostack; &o_objstore; cannot be passed
      SSL-related parameters. When SSL is enabled for RabbitMQ the &o_meter;
      pipeline in &o_objstore; is turned off, rather than sending it over an
      unsecured channel.
     </para>
    </listitem>
   </itemizedlist>
<para>
The following steps are the fastest way to set up and test a new SSL certificate authority (CA).
</para>

<procedure xml:id="pro.rabbitmq-test">
<step>
<para>
  In the RabbitMQ barclamp set <guimenu>Enable SSL</guimenu> to <guimenu>true</guimenu>, and <guimenu>Generate (self-signed) certificates (implies insecure)</guimenu>
    to <literal>true</literal>, then apply the barclamp. The barclamp will create a new CA, enter the correct settings in <filename>/etc/rabbitmq/rabbitmq.config</filename>, and start RabbitMQ.
</para>
</step>
<step>
<para>
Test your new CA with OpenSSL, substituting the hostname of your control node:</para>
<screen>
openssl s_client -connect d52-54-00-59-e5-fd:5671
[...]
Verify return code: 18 (self signed certificate)
</screen>
 <para>
  This outputs a lot of information, including a copy of the server's public certificate, protocols, ciphers, and the chain of trust.
</para>
</step>
    <step>
     <para>
      The last step is to configure client services to use SSL to access the
      RabbitMQ service. (See
      <link xlink:href="https://www.rabbitmq.com/ssl.html"/> for a complete reference).
     </para>
    </step>
</procedure>

<para>
    It is preferable to set up your own CA. The best practice is to use a commercial certificate authority. You may also deploy your own self-signed certificates, provided that your cloud is not publicly-accessible, and only for your internal use. Follow these steps to enable your own CA in RabbitMQ and deploy it to &cloud;:
   </para>
   <procedure xml:id="pro.rabbitmq-production">
    <step>
     <para>
      Configure the RabbitMQ barclamp to use the control node's
      certificate authority (CA), if it already has one, or create a CA specifically for RabbitMQ and configure the barclamp to use that. (See <xref linkend="sec.depl.req.ssl"/>, and the RabbitMQ manual has a detailed howto on creating your CA at <link xlink:href="http://www.rabbitmq.com/ssl.html"/>, with customizations for .NET and Java clients.)
    </para>
     <figure>
      <title>SSL Settings for RabbitMQ Barclamp</title>
      <mediaobject>
       <imageobject role="fo">
        <imagedata fileref="rabbitmq-ssl-1.png" width="100%" format="PNG"/>
       </imageobject>
       <imageobject role="html">
        <imagedata fileref="rabbitmq-ssl-1.png" width="100%" format="PNG"/>
       </imageobject>
       <textobject role="description"><phrase>Example RabbitMQ SSL barclamp configuration</phrase>
       </textobject>
      </mediaobject>
     </figure>
    </step>

   </procedure>

   <para>
    The configuration options in the RabbitMQ barclamp allow tailoring the barclamp to your SSL setup.
</para>
  <variablelist>
   <varlistentry>
    <term><guimenu>Enable SSL</guimenu>
    </term>
     <listitem>
      <para>
          Set this to <guimenu>True</guimenu> to expose all of your configuration options.
      </para>
     </listitem>
    </varlistentry>
   <varlistentry>
    <term><guimenu>SSL Port</guimenu>
    </term>
     <listitem>
      <para>
      RabbitMQ's SSL listening port. The default is 5671.
      </para>
     </listitem>
    </varlistentry>
   <varlistentry>
    <term><guimenu>Generate (self-signed) certificates (implies insecure)</guimenu>
    </term>
     <listitem>
      <para>
    When this is set to <literal>true</literal>, self-signed certificates are automatically generated and copied to the correct locations on the control node, and all other barclamp options are set automatically. This is the fastest way to apply and test the barclamp. Do not use this on production systems. When this is set to <literal>false</literal> the remaining options are exposed.
      </para>
     </listitem>
    </varlistentry>
   <varlistentry>
    <term><guimenu>SSL Certificate File</guimenu>
    </term>
     <listitem>
      <para>
     The location of your public root CA certificate.
      </para>
     </listitem>
    </varlistentry>
   <varlistentry>
    <term><guimenu>SSL (Private) Key File</guimenu>
    </term>
     <listitem>
      <para>
     The location of your private server key.
      </para>
     </listitem>
    </varlistentry>
   <varlistentry>
    <term><guimenu>Require Client Certificate</guimenu>
    </term>
     <listitem>
      <para>
          This goes with <guimenu>SSL CA Certificates File</guimenu>. Set to <guimenu>true</guimenu> to require clients to present SSL certificates to RabbitMQ.
      </para>
     </listitem>
    </varlistentry>
   <varlistentry>
    <term><guimenu>SSL CA Certificates File</guimenu>
    </term>
     <listitem>
      <para>
     Trust client certificates presented by the clients that are signed by other CAs. You'll need to store copies of the CA certificates; see "Trust the Client's Root CA" at <link xlink:href="http://www.rabbitmq.com/ssl.html"/>.
      </para>
     </listitem>
    </varlistentry>
   <varlistentry>
    <term><guimenu>SSL Certificate is insecure (for instance, self-signed)</guimenu>
    </term>
     <listitem>
      <para>
      When this is set to <guimenu>false</guimenu>, clients validate the RabbitMQ server certificate with the <guimenu>SSL client CA</guimenu> file.
      </para>
     </listitem>
    </varlistentry>
   <varlistentry>
    <term><guimenu>SSL client CA file (used to validate rabbitmq server certificate)</guimenu>
    </term>
     <listitem>
      <para>
        Tells clients of RabbitMQ where to find the CA bundle that validates the certificate presented by the RabbitMQ server, when <guimenu>SSL Certificate is insecure (for instance, self-signed)</guimenu> is set to <guimenu>false</guimenu>.
      </para>
     </listitem>
    </varlistentry>
   </variablelist>
  </sect2>
  <sect2 xml:id="sec.depl.ostack.rabbitmq.send-notifications">
   <title>Configuring Clients to Send Notifications</title>
   <para>
    &rabbit; has an option called <literal>Configure clients to send
    notifications</literal>. It defaults to <literal>false</literal>, which
    means no events will be sent. It is required to be set to
    <literal>true</literal> for &o_meter;, &o_monitor;, and any other services
    consuming notifications. When it is set to <literal>true</literal>,
    &ostack; services are configured to submit lifecycle audit events to the
    <literal>notification</literal> &rabbit; queue.
   </para>
   <para>
    This option should only be enabled if an active consumer is configured,
    otherwise events will accumulate on the &rabbit; server, clogging up CPU,
    memory, and disk storage.
   </para>
   <para>
    Any accumulation can be cleared by running:
   </para>
   <screen>&dollar; rabbitmqctl -p /openstack purge_queue notifications.info
&dollar; rabbitmqctl -p /openstack purge_queue notifications.error</screen>
  </sect2>
 </sect1>
 <sect1 xml:id="sec.depl.ostack.keystone">
  <title>Deploying &o_ident;</title>

  <para>
   Keystone is another core component that is used by all
   other &ostack; components. It provides authentication and authorization
   services. Keystone needs to be installed on a
   &contrnode;. &o_ident; can be made highly available by deploying it on a
   cluster. You can configure the following parameters of this &barcl;:
  </para>

  <variablelist>
   <varlistentry>
    <term><guimenu>Algorithm for Token Generation</guimenu>
    </term>
    <listitem>
     <para>
      Set the algorithm used by &o_ident; to generate the tokens. You can
      choose between <literal>Fernet</literal> (the default) or
      <literal>UUID</literal>. Note that for performance and security reasons
      it is strongly recommended to use <literal>Fernet</literal>.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><guimenu>Region Name</guimenu>
    </term>
    <listitem>
     <para>
      Allows customizing the region name that crowbar is going to manage.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><guimenu>Default Credentials: Default Tenant</guimenu>
    </term>
    <listitem>
     <para>
      Tenant for the users. Do not change the default value of
      <literal>openstack</literal>.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><guimenu>
      Default Credentials: Administrator User Name/Password
     </guimenu>
    </term>
    <listitem>
     <para>
      User name and password for the administrator.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><guimenu>
      Default Credentials: Create Regular User
     </guimenu>
    </term>
    <listitem>
     <para>
      Specify whether a regular user should be created automatically. Not
      recommended in most scenarios, especially in an LDAP environment.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><guimenu>
      Default Credentials: Regular User Username/Password
     </guimenu>
    </term>
    <listitem>
     <para>
      User name and password for the regular user. Both the regular user and
      the administrator accounts can be used to log in to the &cloud; &dash;.
      However, only the administrator can manage &o_ident; users and access.
     </para>
     <figure>
      <title>The &o_ident; &Barcl;</title>
      <mediaobject>
       <imageobject role="fo">
        <imagedata fileref="depl_barclamp_keystone.png" width="100%" format="png"/>
       </imageobject>
       <imageobject role="html">
        <imagedata fileref="depl_barclamp_keystone.png" width="75%" format="png"/>
       </imageobject>
      </mediaobject>
     </figure>
    </listitem>
   </varlistentry>
   <varlistentry xml:id="sec.depl.ostack.keystone.ssl">
    <term>SSL Support: Protocol
    </term>
    <listitem>
     <para>
      When you use the default value <guimenu>HTTP</guimenu>, public
      communication will not be encrypted. Choose <guimenu>HTTPS</guimenu> to
      use SSL for encryption. See <xref linkend="sec.depl.req.ssl"/> for
      background information and <xref linkend="sec.depl.inst.nodes.post.ssl"/>
      for installation instructions. The following additional configuration
      options will become available when choosing <guimenu>HTTPS</guimenu>:
     </para>
     <variablelist>
      <varlistentry>
       <term><guimenu>Generate (self-signed) certificates</guimenu>
       </term>
       <listitem>
        <para>
         When set to <literal>true</literal>, self-signed certificates are
         automatically generated and copied to the correct locations. This
         setting is for testing purposes only and should never be used in
         production environments!
        </para>
       </listitem>
      </varlistentry>
      <varlistentry>
       <term><guimenu>SSL Certificate File</guimenu> / <guimenu>SSL (Private) Key
        File</guimenu>
       </term>
       <listitem>
        <para>
         Location of the certificate key pair files.
        </para>
       </listitem>
      </varlistentry>
      <varlistentry>
       <term><guimenu>SSL Certificate is insecure</guimenu>
       </term>
       <listitem>
        <para>
         Set this option to <literal>true</literal> when using self-signed
         certificates to disable certificate checks. This setting is for
         testing purposes only and should never be used in production
         environments!
        </para>
       </listitem>
      </varlistentry>
      <varlistentry>
       <term><guimenu>SSL CA Certificates File</guimenu>
       </term>
       <listitem>
        <para>
         Specify the absolute path to the CA certificate. This field is
         mandatory, and leaving it blank will cause the &barcl; to fail. To fix
         this issue, you have to provide the absolute path to the CA
         certificate, restart the <systemitem>apache2</systemitem> service, and
         re-deploy the &barcl;.
        </para>
        <para>
         When the certificate is not already trusted by the pre-installed list
         of trusted root certificate authorities, you need to provide a
         certificate bundle that includes the root and all intermediate CAs.
        </para>
        <figure>
         <title>The SSL Dialog</title>
         <mediaobject>
          <imageobject role="fo">
           <imagedata fileref="depl_barclamp_ssl.png" width="100%" format="png"/>
          </imageobject>
          <imageobject role="html">
           <imagedata fileref="depl_barclamp_ssl.png" width="75%" format="png"/>
          </imageobject>
         </mediaobject>
        </figure>
       </listitem>
      </varlistentry>
     </variablelist>
    </listitem>
   </varlistentry>
  </variablelist>

<sect2 xml:id="sec.depl.ostack.keystone.ldap">
 <title>Authenticating with LDAP</title>
 <para>
  &o_ident; has the ability to separate
  identity back-ends by domains. &cloud; &productnumber; uses this method for
  authenticating users.
 </para>
 <para>
  The &o_ident; &barcl; sets up a &mariadb; database by default. Configuring
  an LDAP back-end is done in the <guimenu>Raw</guimenu> view.
 </para>
 <procedure>
     <step>
         <para>
             Set <guimenu>"domain_specific_drivers": true,</guimenu>
         </para>
     </step>
     <step>
         <para>
             Then in the <guimenu>"domain_specific_config":</guimenu> section
             configure a map with domain names as keys, and configuration as
             values. In the default proposal the domain name key is
             <guimenu>"ldap_users"</guimenu>, and the keys are the two required
             sections for an LDAP-based identity driver configuration, the <guimenu>[identity]</guimenu> section which sets the driver, and the <guimenu>[ldap]</guimenu> section which sets the LDAP connection options. You may configure multiple domains, each with its own configuration.
             </para>
         </step>
 </procedure>
 <para>
     You may make this available to &o_dash; by setting <guimenu>multi_domain_support</guimenu> to <guimenu>true</guimenu> in the &o_dash; &barcl;.
 </para>
 <para>
Users in the LDAP-backed domain have to know the name of the domain in order to authenticate, and must use the  &o_ident; v3 API endpoint. (See the &ostack; manuals, <link xlink:href="https://docs.openstack.org/keystone/latest/admin/identity-domain-specific-config.html">Domain-specific Configuration</link> and <link xlink:href="https://docs.openstack.org/keystone/latest/admin/identity-integrate-with-ldap.html">Integrate Identity with LDAP</link>, for additional details.)
 </para>
</sect2>



  <!-- cs 2017-08-01: re https://bugzilla.suse.com/show_bug.cgi?id=1045314
       do we want to give an LDAP example? Configuration is so variable,
and admins can see the available options in the barclamp -->

  <!-- fs 2017-05-04: Commenting per bsc#1037531
       * no longer valid
       * needs to be rewritten for SOC8, see
         https://docs.openstack.org/admin-guide/identity-integrate-with-ldap.html

  <sect2 xml:id="sec.depl.ostack.keystone.ldap">
   <title>LDAP Authentication with &o_ident;</title>
   <para>
    <remark condition="clarity">
     2015-01-21 - fs: TODO: bsc #914070
    </remark>
    By default &o_ident; uses an SQL database back-end store for
    authentication. LDAP can be used in addition to the default, or as an
    alternative. Using LDAP requires the &contrnode; on which
    &o_ident; is installed to be able to contact the LDAP server. See
    <xref linkend="sec.depl.inst.admserv.post.network"/> for instructions on how to
    adjust the network setup.
   </para>
   <sect3 xml:id="sec.depl.ostack.keystone.ldap.ldap">
    <title>Using LDAP for Authentication</title>
    <para>
     To configure LDAP as an alternative to the SQL database back-end store,
     you need to open the &o_ident; &barcl; <guimenu>Attribute
     </guimenu>configuration in <guimenu>Raw</guimenu> mode. Search for the
     <guimenu>ldap</guimenu> section.
    </para>
    <figure xml:id="fig.keystone.ldap">
     <title>The &o_ident; &Barcl;: Raw Mode</title>
     <mediaobject>
      <imageobject role="fo">
       <imagedata fileref="depl_barclamp_keystone_raw.png" width="100%" format="png"/>
      </imageobject>
      <imageobject role="html">
       <imagedata fileref="depl_barclamp_keystone_raw.png" width="75%" format="png"/>
      </imageobject>
     </mediaobject>
    </figure>
    <para>
     Adjust the settings according to your LDAP setup. The default
     configuration does not include all attributes that can be
     set. You'll find a complete list of options is available in the file
     <filename>/opt/dell/chef/data_bags/crowbar/bc-template-keystone.schema</filename>
     on the &admserv; (search for <literal>ldap</literal>). There are
     three types of attribute values: strings (for example, the value for
     <literal>url</literal>:<literal>"ldap://localhost"</literal>), bool
     (for example, the value for <literal>use_dumb_member</literal>:
     <literal>false</literal>) and integer (for example, the value for
     <literal>page_size</literal>: <literal>0</literal>). Attribute names
     and string values always need to be quoted with double quotes; bool and
     integer values must not be quoted.
    </para>
    <important>
     <title>Using LDAP over SSL (ldaps) Is Recommended</title>
     <para>
      In a production environment you should use LDAP over SSL
      (ldaps), otherwise passwords will be transferred as plain text.
     </para>
    </important>
   </sect3>
   <sect3 xml:id="sec.depl.ostack.keystone.ldap.hybrid">
    <title>Using Hybrid Authentication</title>
    <para>
     The Hybrid LDAP back-end allows creating a mixed LDAP/SQL setup. This
     is especially useful when an existing LDAP server should be used to
     authenticate cloud users. The system and service users (administrators
     and operators) needed to set up and manage &cloud; will be managed
     in the local SQL database. Assignments of users to projects and roles
     will also be stored in the local database.
    </para>
    <para>
     In this scenario the LDAP Server can be read-only for &cloud;
     installation, and no Schema modifications are required. Therefore
     managing LDAP users from within &cloud; is not possible and must be done using your established tools for LDAP user management. All user
     that are create with the &o_ident; command line client or the
     Horizon Web UI will be stored in the local SQL database.
    </para>
    <para>
     To configure hybrid authentication, proceed as follows:
    </para>
    <procedure>
     <step>
      <para>
       Open the &o_ident; &barcl; <guimenu>Attribute
       </guimenu>configuration in <guimenu>Raw</guimenu> mode (see
       <xref linkend="fig.keystone.ldap"/>).
      </para>
     </step>
     <step>
      <para>
       Set the identity and assignment drivers to the hybrid back-end:
      </para>
<screen> "identity": {
    "driver": "hybrid"
  },
  "assignment": {
    "driver": "hybrid"
  }</screen>
     </step>
     <step>
      <para>
       Adjust the settings according to your LDAP setup in the
       <guimenu>ldap</guimenu> section. Since the LDAP back-end is only used
       to acquire information on users (but not on projects and roles),
       only the user-related settings matter here. See the following
       example of settings that may need to be adjusted:
      </para>
<screen>  "ldap": {
    "url": "ldap://localhost",
    "user": "",
    "password": "",
    "suffix": "cn=example,cn=com",
    "user_tree_dn": "cn=example,cn=com",
    "query_scope": "one",
    "user_id_attribute": "cn",
    "user_enabled_emulation_dn": "",
    "tls_req_cert": "demand",
    "user_attribute_ignore": "tenant_id,tenants",
    "user_objectclass": "inetOrgPerson",
    "user_mail_attribute": "mail",
    "user_filter": "",
    "use_tls": false,
    "user_allow_create": false,
    "user_pass_attribute": "userPassword",
    "user_enabled_attribute": "enabled",
    "user_enabled_default": "True",
    "page_size": 0,
    "tls_cacertdir": "",
    "tls_cacertfile": "",
    "user_enabled_mask": 0,
    "user_allow_update": true,
    "group_allow_update": true,
    "user_enabled_emulation": false,
    "user_name_attribute": "cn"
    "group_ad_nesting": false,
    "use_pool": true,
    "pool_size": 10,
    "pool_retry_max": 3
  }</screen>
      <para>
       To access the LDAP server anonymously, leave the values for
       <guimenu>user</guimenu> and <guimenu>password</guimenu> empty.
      </para>
     </step>
    </procedure>
   </sect3>
  </sect2>
-->

  <sect2 xml:id="sec.depl.ostack.keystone.ha">
   <title>&haSetup; for &o_ident;</title>
   <para>
    Making &o_ident; highly available requires no special
    configuration&mdash;it is sufficient to deploy it on a cluster.
   </para>
  </sect2>
 </sect1>
 <sect1 xml:id="sec.depl.ostack.swift">
  <title>Deploying &o_objstore; (optional)</title>

  <para>
   &o_objstore; adds an object storage service to &cloud; for storing single
   files such as images or snapshots. It offers high data security by storing
   the data redundantly on a pool of &stornode;s&mdash;therefore &o_objstore;
   needs to be installed on at least two dedicated nodes.
  </para>

<!--
  <para>
   It is recommended not to change the defaults in the &barcl; proposal,
   unless you know exactly what you require. However you should change the
   <guimenu>Cluster Admin Password</guimenu>. If you plan to change the
   <guimenu>Zone</guimenu> value, it is important to know that you need at
   least as many &stornode;s as <guimenu>Zones</guimenu>.
  </para>
-->

  <para>
   To properly configure &o_objstore; it is important to understand how it
   places the data. Data is always stored redundantly within the hierarchy. The
   &o_objstore; hierarchy in &cloud; is formed out of zones, nodes, hard disks,
   and logical partitions. Zones are physically separated clusters, for example
   different server rooms each with its own power supply and network segment. A
   failure of one zone must not affect another zone. The next level in the
   hierarchy are the individual &o_objstore; storage nodes (on which
   <guimenu>swift-storage</guimenu> has been deployed), followed by the hard
   disks. Logical partitions come last.
  </para>

  <para>
   &o_objstore; automatically places three copies of each object on the highest
   hierarchy level possible. If three zones are available, then each copy of
   the object will be placed in a different zone. In a one zone setup with more
   than two nodes, the object copies will each be stored on a different node.
   In a one zone setup with two nodes, the copies will be distributed on
   different hard disks. If no other hierarchy element fits, logical partitions
   are used.
  </para>

  <para>
   The following attributes can be set to configure &o_objstore;:
  </para>

  <variablelist>
   <varlistentry>
    <term><guimenu>Allow Public Containers</guimenu>
    </term>
    <listitem>
     <para>
      Set to <literal>true</literal> to enable public access to containers.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><guimenu>Enable Object Versioning</guimenu>
    </term>
    <listitem>
     <para>
      If set to true, a copy of the current version is archived each time an
      object is updated.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><guimenu>Zones</guimenu>
    </term>
    <listitem>
     <para>
      Number of zones (see above). If you do not have different independent
      installations of storage nodes, set the number of zones to
      <literal>1</literal>.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><guimenu>Create 2^X Logical Partitions</guimenu>
    </term>
    <listitem>
     <para>
      Partition power. The number entered here is used to compute the number of
      logical partitions to be created in the cluster. The number you enter is
      used as a power of 2 (2^X).
     </para>
     <para>
      We recommend using a minimum of 100 partitions per disk. To measure the
      partition power for your setup, multiply the number of disks from all
      &o_objstore; nodes by 100, and then round up to the nearest power of two.
      Keep in mind that the first disk of each node is not used by
      &o_objstore;, but rather for the operating system.
     </para>
     <formalpara>
      <title>Example: 10 &o_objstore; nodes with 5 hard disks each</title>
      <para>
       Four hard disks on each node are used for &o_objstore;, so there is a
       total of forty disks. 40 x 100 = 4000. The nearest power of two, 4096,
       equals 2^12. So the partition power that needs to be entered is
       <literal>12</literal>.
      </para>
     </formalpara>
     <important>
      <title>Value Cannot be Changed After the Proposal Has Been Deployed</title>
      <para>
       Changing the number of logical partition after &o_objstore; has been
       deployed is not supported. Therefore the value for the partition power
       should be calculated from the maximum number of partitions this cloud
       installation is likely going to need at any point in time.
      </para>
     </important>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><guimenu>Minimum Hours before Partition is reassigned</guimenu>
    </term>
    <listitem>
     <para>
      This option sets the number of hours before a logical partition is
      considered for relocation. <literal>24</literal> is the recommended
      value.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><guimenu>Replicas</guimenu>
    </term>
    <listitem>
     <para>
      The number of copies generated for each object. The number of replicas
      depends on the number of disks and zones.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><guimenu>Replication interval (in seconds)</guimenu>
    </term>
    <listitem>
     <para>
      Time (in seconds) after which to start a new replication process.
     </para>
    </listitem>
   </varlistentry>
<!-- fs 2016-01-26: No longer present in Cloud6
   <varlistentry>
    <term>Cluster Admin Password</term>
    <listitem>
     <para>
      The &o_objstore; administrator password.
     </para>
    </listitem>
   </varlistentry>
   -->
   <varlistentry>
    <term><guimenu>Debug</guimenu>
    </term>
    <listitem>
     <para>
      Shows debugging output in the log files when set to
      <literal>true</literal>.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><guimenu>SSL Support: Protocol</guimenu>
    </term>
    <listitem>
     <para>
      Choose whether to encrypt public communication (<guimenu>HTTPS</guimenu>)
      or not (<guimenu>HTTP</guimenu>). If you choose <guimenu>HTTPS</guimenu>,
      you have two options. You can either <guimenu>Generate (self-signed)
      certificates</guimenu> or provide the locations for the certificate key
      pair files. Using self-signed certificates is for testing purposes only
      and should never be used in production environments!
     </para>
    </listitem>
   </varlistentry>
  </variablelist>

  <figure>
   <title>The &o_objstore; &Barcl;</title>
   <mediaobject>
    <imageobject role="fo">
     <imagedata fileref="depl_barclamp_swift.png" width="100%" format="png"/>
    </imageobject>
    <imageobject role="html">
     <imagedata fileref="depl_barclamp_swift.png" width="75%" format="png"/>
    </imageobject>
   </mediaobject>
  </figure>

  <para>
   Apart from the general configuration described above, the &o_objstore;
   &barcl; lets you also activate and configure <guimenu>Additional
   Middlewares</guimenu>. The features these middlewares provide can be used
   via the &o_objstore; command line client only. The Ratelimit and S3
   middleware provide for the most interesting features, and we recommend
   enabling other middleware only for specific use-cases.
  </para>

  <variablelist>
   <varlistentry>
    <term><guimenu>S3 Middleware</guimenu>
    </term>
    <listitem>
     <para>
      Provides an S3 compatible API on top of &o_objstore;.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><guimenu>StaticWeb</guimenu>
    </term>
    <listitem>
     <para>
      Serve container data as a static Web site with an index file and optional
      file listings. See
      <link xlink:href="http://docs.openstack.org/developer/swift/middleware.html#staticweb"/>
      for details.
     </para>
     <para>
      This middleware requires setting <guimenu>Allow Public
      Containers</guimenu> to <literal>true</literal>.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><guimenu>TempURL</guimenu>
    </term>
    <listitem>
     <para>
      Create URLs to provide time-limited access to objects. See
      <link xlink:href="http://docs.openstack.org/developer/swift/middleware.html#tempurl"/>
      for details.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><guimenu>FormPOST</guimenu>
    </term>
    <listitem>
     <para>
      Upload files to a container via Web form. See
      <link xlink:href="http://docs.openstack.org/developer/swift/middleware.html#formpost"/>
      for details.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><guimenu>Bulk</guimenu>
    </term>
    <listitem>
     <para>
      Extract TAR archives into a Swift account, and delete multiple objects or
      containers with a single request. See
      <link xlink:href="http://docs.openstack.org/developer/swift/middleware.html#module-swift.common.middleware.bulk"/>
      for details.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><guimenu>Cross-domain</guimenu>
    </term>
    <listitem>
     <para>
      Interact with the Swift API via Flash, Java, and Silverlight from an
      external network. See
      <link xlink:href="http://docs.openstack.org/developer/swift/middleware.html#module-swift.common.middleware.crossdomain"/>
      for details.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><guimenu>Domain Remap</guimenu>
    </term>
    <listitem>
     <para>
      Translates container and account parts of a domain to path parameters
      that the &o_objstore; proxy server understands. Can be used to create
      short URLs that are easy to remember, for example by rewriting
      <literal>home.&exampleuser_plain;.&exampledomain;/$ROOT/&exampleuser_plain;/home/myfile</literal>
      to <literal>home.&exampleuser_plain;.&exampledomain;/myfile</literal>.
      See
      <link xlink:href="http://docs.openstack.org/developer/swift/middleware.html#module-swift.common.middleware.domain_remap"/>
      for details.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><guimenu>Ratelimit</guimenu>
    </term>
    <listitem>
     <para>
      Throttle resources such as requests per minute to provide denial of
      service protection. See
      <link xlink:href="http://docs.openstack.org/developer/swift/middleware.html#module-swift.common.middleware.ratelimit"/>
      for details.
     </para>
    </listitem>
   </varlistentry>
  </variablelist>

  <para>
   The &o_objstore; component consists of four different roles. Deploying
   <guimenu>swift-dispersion</guimenu> is optional:
  </para>

  <variablelist>
   <varlistentry>
    <term><guimenu>swift-storage</guimenu>
    </term>
    <listitem>
     <para>
      The virtual object storage service. Install this role on all dedicated
      &o_objstore; &stornode;s (at least two), but not on any other node.
     </para>
     <warning>
      <title>swift-storage Needs Dedicated Machines</title>
      <para>
       Never install the swift-storage service on a node that runs other
       &ostack; components.
      </para>
     </warning>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><guimenu>swift-ring-compute</guimenu>
    </term>
    <listitem>
     <para>
      The ring maintains the information about the location of objects,
      replicas, and devices. It can be compared to an index that is used by
      various &ostack; components to look up the physical location of objects.
      <guimenu>swift-ring-compute</guimenu> must only be installed on a single
      node, preferably a &contrnode;.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><guimenu>swift-proxy</guimenu>
    </term>
    <listitem>
     <para>
      The &o_objstore; proxy server takes care of routing requests to
      &o_objstore;. Installing a single instance of
      <guimenu>swift-proxy</guimenu> on a &contrnode; is recommended. The
      <guimenu>swift-proxy</guimenu> role can be made highly available by
      deploying it on a cluster.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><guimenu>swift-dispersion</guimenu>
    </term>
    <listitem>
     <para>
      Deploying <guimenu>swift-dispersion</guimenu> is optional. The
      &o_objstore; dispersion tools can be used to test the health of the
      cluster. It creates a heap of dummy objects (using 1% of the total space
      available). The state of these objects can be queried using the
      swift-dispersion-report query. <guimenu>swift-dispersion</guimenu> needs
      to be installed on a &contrnode;.
     </para>
    </listitem>
   </varlistentry>
  </variablelist>

  <figure>
   <title>The &o_objstore; &Barcl;: Node Deployment Example</title>
   <mediaobject>
    <imageobject role="fo">
     <imagedata fileref="depl_barclamp_swift_node_deployment.png" width="100%" format="png"/>
    </imageobject>
    <imageobject role="html">
     <imagedata fileref="depl_barclamp_swift_node_deployment.png" width="75%" format="png"/>
    </imageobject>
   </mediaobject>
  </figure>

  <sect2 xml:id="sec.depl.ostack.swift.ha">
   <title>&haSetup; for &swift;</title>
   <para>
    &swift; replicates by design, so there is no need for a special &hasetup;.
    Make sure to fulfill the requirements listed in
    <xref linkend="sec.depl.reg.ha.storage.swift"/>.
   </para>
  </sect2>
 </sect1>
 <sect1 xml:id="sec.depl.ostack.glance">
  <title>Deploying &o_img;</title>

  <para>
   &o_img; provides discovery, registration, and delivery services for virtual
   disk images. An image is needed to start an &vmguest;&mdash;it is its
   pre-installed root-partition. All images you want to use in your cloud to
   boot &vmguest;s from, are provided by &o_img;. &o_img; must be deployed onto
   a &contrnode;. &o_img; can be made highly available by deploying it on a
   cluster.
  </para>

  <para>
   There are a lot of options to configure &o_img;. The most important ones are
   explained below&mdash;for a complete reference refer to
   <link xlink:href="http://github.com/crowbar/crowbar/wiki/Glance-barclamp"/>.
  </para>

  <important xml:id="note.glance.api.versions">
   <title>&o_img; API Versions</title>
   <para>
    As of &productname; 7, the &o_img; API v1 is no longer enabled by default.
    Instead, &o_img; API v2 is used by default.
   </para>
   <para>
    If you need to re-enable API v1 for compatibility reasons:
   </para>
   <orderedlist>
    <listitem>
     <para>
      Switch to the <guimenu>Raw</guimenu> view of the &o_img; &barcl;.
     </para>
    </listitem>
    <listitem>
     <para>
      Search for the <literal>enable_v1</literal> entry and set it to
      <literal>true</literal>:
     </para>
<screen>"enable_v1": true</screen>
     <para>
      In new installations, this entry is set to <literal>false</literal> by
      default. When upgrading from an older version of &productname; it is set
      to <literal>true</literal> by default.
     </para>
    </listitem>
    <listitem>
     <para>
      Apply your changes.
     </para>
    </listitem>
   </orderedlist>
  </important>

  <variablelist>
   <varlistentry>
    <term><guimenu>Image Storage: Default Storage Store</guimenu>
    </term>
    <listitem>
     <formalpara>
      <title><guimenu>File</guimenu></title>
      <para>
       Images are stored in an image file on the &contrnode;.
      </para>
     </formalpara>
     <formalpara>
      <title><guimenu>Cinder</guimenu></title>
      <para>
       Provides volume block storage to &productname;. Use it to
       store images.
      </para>
     </formalpara>
     <formalpara>
      <title><guimenu>Swift</guimenu></title>
      <para>
       Provides an object storage service to &productname;.
      </para>
     </formalpara>
     <formalpara>
      <title><guimenu>Rados</guimenu></title>
      <para>
       &storage; (based on Ceph) provides block storage service to
       &productname;.
      </para>
     </formalpara>
     <formalpara>
      <title><guimenu>&vmware;</guimenu></title>
      <para>
       If you are using &vmware; as a hypervisor, it is recommended to use
       <guimenu>&vmware;</guimenu> for storing images. This will make starting
       &vmware; instances much faster.
      </para>
     </formalpara>
     <formalpara>
      <title><guimenu>Expose Backend Store Location</guimenu></title>
      <para>
       If this is set to <guimenu>true</guimenu>, the API will communicate the
       direct URl of the image's back-end location to HTTP clients. Set to
       <guimenu>false</guimenu> by default.
      </para>
     </formalpara>
     <para>
      Depending on the storage back-end, there are additional configuration
      options available:
     </para>
     <bridgehead renderas="sect4"><guimenu>File Store Parameters</guimenu>
     </bridgehead>
     <para>
      Only required if <guimenu>Default Storage Store</guimenu> is set to
      <guimenu>File</guimenu>.
     </para>
     <variablelist>
      <varlistentry>
       <term><guimenu>Image Store Directory</guimenu>
       </term>
       <listitem>
        <para>
         Specify the directory to host the image file. The directory specified
         here can also be an NFS share. See
         <xref linkend="sec.depl.inst.nodes.post.nfs"/> for more information.
        </para>
       </listitem>
      </varlistentry>
     </variablelist>
     <bridgehead renderas="sect4"><guimenu>Swift Store Parameters</guimenu>
     </bridgehead>
     <para>
      Only required if <guimenu>Default Storage Store</guimenu> is set to
      <guimenu>Swift</guimenu>.
     </para>
     <variablelist>
      <varlistentry>
       <term><guimenu>Swift Container</guimenu>
       </term>
       <listitem>
        <para>
         Set the name of the container to use for the images in &o_objstore;.
        </para>
       </listitem>
      </varlistentry>
     </variablelist>
     <bridgehead renderas="sect4"><guimenu>RADOS Store Parameters</guimenu>
     </bridgehead>
     <para>
      Only required if <guimenu>Default Storage Store</guimenu> is set to
      <guimenu>Rados</guimenu>.
     </para>
     <variablelist>
      <varlistentry>
       <term>RADOS User for CephX Authentication</term>
       <listitem>
        <para>
         If you are using an external &ceph; cluster, specify the user you have
         set up for &o_img; (see <xref
         linkend="sec.depl.inst.nodes.post.ceph_ext"/> for more information).
        </para>
       </listitem>
      </varlistentry>
      <varlistentry>
       <term>RADOS Pool for Glance images</term>
       <listitem>
        <para>
         If you are using a &cloud; internal &ceph; setup, the pool you specify
         here is created if it does not exist. If you are using an external
         &ceph; cluster, specify the pool you have set up for &o_img; (see
         <xref linkend="sec.depl.inst.nodes.post.ceph_ext"/> for more
         information).
        </para>
       </listitem>
      </varlistentry>
     </variablelist>
     <bridgehead renderas="sect4"><guimenu>&vmware; Store Parameters</guimenu>
     </bridgehead>
     <para>
      Only required if <guimenu>Default Storage Store</guimenu> is set to
      <guimenu>&vmware;</guimenu>.
     </para>
     <variablelist>
      <varlistentry>
       <term><guimenu>vCenter Host/IP Address</guimenu>
       </term>
       <listitem>
        <para>
         Name or IP address of the vCenter server.
        </para>
       </listitem>
      </varlistentry>
      <varlistentry>
       <term><guimenu>vCenter Username</guimenu> / <guimenu>vCenter
        Password</guimenu>
       </term>
       <listitem>
        <para>
         vCenter login credentials.
        </para>
       </listitem>
      </varlistentry>
      <varlistentry>
       <term><guimenu>Datastores for Storing Images</guimenu>
       </term>
       <listitem>
        <para>
         A comma-separated list of datastores specified in the format:
         <replaceable>DATACENTER_NAME</replaceable>:<replaceable>DATASTORE_NAME</replaceable>
        </para>
       </listitem>
      </varlistentry>
      <varlistentry>
       <term><guimenu>
         Path on the datastore, where the glance images will be
         stored
        </guimenu>
       </term>
       <listitem>
        <para>
         Specify an absolute path here.
        </para>
       </listitem>
      </varlistentry>
     </variablelist>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><guimenu>SSL Support: Protocol</guimenu>
    </term>
    <listitem>
     <para>
      Choose whether to encrypt public communication (<guimenu>HTTPS</guimenu>)
      or not (<guimenu>HTTP</guimenu>). If you choose <guimenu>HTTPS</guimenu>,
      refer to <xref linkend="sec.depl.ostack.keystone.ssl"/> for configuration
      details.
     </para>
    </listitem>
   </varlistentry>
<!-- fs 2016-01-26: No longer present in Cloud6
   <varlistentry>
    <term><guimenu>API: Bind to All Addresses</guimenu>
    </term>
    <listitem>
     <para>
      Set this option to <guimenu>true</guimenu> to enable users to upload
      images to &o_img;. If unset, only the operator can upload images.
     </para>
    </listitem>
    </varlistentry>
    -->
   <varlistentry>
    <term><guimenu>Caching</guimenu>
    </term>
    <listitem>
     <para>
      Enable and configure image caching in this section. By default, image
      caching is disabled. You can see this the Raw view of your Nova barclamp:
     </para>
<screen>image_cache_manager_interval = -1</screen>
     <para>
      This option sets the number of seconds to wait between runs of the image
      cache manager. Disabling it means that the cache manager will not
      automatically remove the unused images from the cache, so if you have
      many &o_img; images and are running out of storage you must manually
      remove the unused images from the cache. We recommend leaving this option
      disabled as it is known to cause issues, especially with shared storage.
      The cache manager may remove images still in use, e.g. when network
      outages cause synchronization problems with compute nodes.
     </para>
     <para>
      If you wish to enable caching, re-enable it in a custom Nova
      configuration file, for example
      <filename>/etc/nova/nova.conf.d/500-nova.conf</filename>. This sets the
      interval to four minutes:
     </para>
<screen>image_cache_manager_interval = 2400</screen>
     <para>
      See <xref linkend="cha.depl.ostack.configs"/> for more information on
      custom configurations.
     </para>
     <para>
      Learn more about &o_img;'s caching feature at
      <link xlink:href="http://docs.openstack.org/developer/glance/cache.html"/>.
     </para>
    </listitem>
   </varlistentry>
<!-- fs 2016-01-26: No longer present in Cloud6
   <varlistentry>
    <term><guimenu>Database: SQL Idle Timeout</guimenu>
    </term>
    <listitem>
     <para>
      Time after which idle database connections will be dropped.
     </para>
    </listitem>
    </varlistentry>
    -->
   <varlistentry>
    <term><guimenu>Logging: Verbose Logging</guimenu>
    </term>
    <listitem>
     <para>
      Shows debugging output in the log files when set to
      <guimenu>true</guimenu>.
     </para>
    </listitem>
   </varlistentry>
  </variablelist>

  <figure>
   <title>The &o_img; &Barcl;</title>
   <mediaobject>
    <imageobject role="fo">
     <imagedata fileref="depl_barclamp_glance.png" width="100%" format="png"/>
    </imageobject>
    <imageobject role="html">
     <imagedata fileref="depl_barclamp_glance.png" width="75%" format="png"/>
    </imageobject>
   </mediaobject>
  </figure>

  <sect2 xml:id="sec.depl.ostack.glance.ha">
   <title>&haSetup; for &o_img;</title>
   <para>
    &o_img; can be made highly available by deploying it on a cluster. We
    strongly recommended doing this for the image data as well. The recommended
    way is to use &o_objstore; or an external &ceph; cluster for the image
    repository. If you are using a directory on the node instead (file storage
    back-end), you should set up shared storage on the cluster for it.
   </para>
  </sect2>
 </sect1>
 <sect1 xml:id="sec.depl.ostack.cinder">
  <title>Deploying &o_blockstore;</title>

  <para>
   &o_blockstore;, the successor of Nova Volume, provides volume block storage.
   It adds persistent storage to an &vmguest; that will persist until deleted,
   contrary to ephemeral volumes that only persist while the &vmguest; is
   running.
  </para>

  <para>
   &o_blockstore; can provide volume storage by using different back-ends such
   as local file, one or more local disks, &ceph; (RADOS), &vmware;, or network
   storage solutions from EMC, EqualLogic, Fujitsu, NetApp or Pure Storage.
   Since &productname; 5, &o_blockstore; supports using several back-ends
   simultaneously. It is also possible to deploy the same network storage
   back-end multiple times and therefore use different installations at the
   same time.
  </para>

  <para>
   The attributes that can be set to configure &o_blockstore; depend on the
   back-end. The only general option is <guimenu>SSL Support:
   Protocol</guimenu> (see <xref linkend="sec.depl.ostack.keystone.ssl"/> for
   configuration details).
  </para>

  <tip>
   <title>Adding or Changing a Back-End</title>
   <para>
    When first opening the &o_blockstore; &barcl;, the default
    proposal&mdash;<guimenu>Raw Devices</guimenu>&mdash;is already available
    for configuration. To optionally add a back-end, go to the section
    <guimenu>Add New Cinder Back-End</guimenu> and choose a <guimenu>Type Of
    Volume</guimenu> from the drop-down box. Optionally, specify the
    <guimenu>Name for the Backend</guimenu>. This is recommended when deploying
    the same volume type more than once. Existing back-end configurations
    (including the default one) can be deleted by clicking the trashcan icon if
    no longer needed. Note that you must configure at least one back-end.
   </para>
  </tip>

  <bridgehead renderas="sect2"><guimenu>Raw devices</guimenu> (local disks)
  </bridgehead>

  <variablelist>
   <varlistentry>
    <term><guimenu>Disk Selection Method</guimenu>
    </term>
    <listitem>
     <para>
      Choose whether to use the <guimenu>First Available</guimenu> disk or
      <guimenu>All Available</guimenu> disks. <quote>Available disks</quote>
      are all disks currently not used by the system. Note that one disk
      (usually <filename>/dev/sda</filename>) of every block storage node is
      already used for the operating system and is not available for
      &o_blockstore;.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><guimenu>Name of Volume</guimenu>
    </term>
    <listitem>
     <para>
      Specify a name for the &o_blockstore; volume.
     </para>
    </listitem>
   </varlistentry>
  </variablelist>

  <bridgehead renderas="sect3"><guimenu>EMC</guimenu> (EMC² Storage)
  </bridgehead>

  <variablelist>
   <varlistentry>
    <term><guimenu>IP address of the ECOM server</guimenu> / <guimenu>Port of the ECOM server</guimenu>
    </term>
    <listitem>
     <para>
      IP address and Port of the ECOM server.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><guimenu>Username for accessing the ECOM server</guimenu> / <guimenu>Password for accessing the ECOM server</guimenu>
    </term>
    <listitem>
     <para>
      Login credentials for the ECOM server.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><guimenu>VMAX port groups to expose volumes managed by this backend</guimenu>
    </term>
    <listitem>
     <para>
      VMAX port groups that expose volumes managed by this back-end.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><guimenu>Serial number of the VMAX Array</guimenu>
    </term>
    <listitem>
     <para>
      Unique VMAX array serial number.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><guimenu>Pool name within a given array</guimenu>
    </term>
    <listitem>
     <para>
      Unique pool name within a given array.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><guimenu>FAST Policy name to be used</guimenu>
    </term>
    <listitem>
     <para>
      Name of the FAST Policy to be used. When specified, volumes managed by
      this back-end are managed as under FAST control.
     </para>
    </listitem>
   </varlistentry>
  </variablelist>

  <para>
   For more information on the EMC driver refer to the &ostack; documentation
   at
   <link xlink:href="http://docs.openstack.org/liberty/config-reference/content/emc-vmax-driver.html"/>.
  </para>

  <bridgehead renderas="sect3"><guimenu>EqualLogic</guimenu>
  </bridgehead>

  <para>
   EqualLogic drivers are included as a technology preview and are not
   supported.
  </para>

  <bridgehead renderas="sect2"><guimenu>Fujitsu ETERNUS DX</guimenu>
  </bridgehead>

  <variablelist>
   <varlistentry>
    <term><guimenu>Connection Protocol</guimenu>
    </term>
    <listitem>
     <para>
      Select the protocol used to connect, either
      <guimenu>FibreChannel</guimenu> or <guimenu>iSCSI</guimenu>.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><guimenu>IP for SMI-S</guimenu> / <guimenu>Port for SMI-S</guimenu>
    </term>
    <listitem>
     <para>
      IP address and port of the ETERNUS SMI-S Server.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><guimenu>Username for SMI-S</guimenu> / <guimenu>Password for SMI-S</guimenu>
    </term>
    <listitem>
     <para>
      Login credentials for the ETERNUS SMI-S Server.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><guimenu>Snapshot (Thick/RAID Group) Pool Name</guimenu>
    </term>
    <listitem>
     <para>
      Storage pool (RAID group) in which the volumes are created. Make sure
      that the RAID group on the server has already been created. If a RAID
      group that does not exist is specified, the RAID group is built from
      unused disk drives. The RAID level is automatically determined by the
      ETERNUS DX Disk storage system.
     </para>
    </listitem>
   </varlistentry>
  </variablelist>

  <bridgehead renderas="sect2"><guimenu>Hitachi HUSVM</guimenu>
  </bridgehead>

  <para>
   For information on configuring the Hitachi HUSVM back-end, refer to
   <link xlink:href="http://docs.openstack.org/ocata/config-reference/block-storage/drivers/hitachi-storage-volume-driver.html"/>.
  </para>

  <bridgehead renderas="sect2"><guimenu>NetApp</guimenu>
  </bridgehead>

  <variablelist>
   <varlistentry>
    <term><guimenu>Storage Family Type</guimenu> / <guimenu>Storage Protocol</guimenu>
    </term>
    <listitem>
     <para>
      &cloud; can use <quote>Data ONTAP</quote> in <guimenu>7-Mode</guimenu>,
      or in <guimenu>Clustered Mode</guimenu>. In <guimenu>7-Mode</guimenu>
      vFiler will be configured, in <guimenu>Clustered Mode</guimenu> vServer
      will be configured. The <guimenu>Storage Protocol</guimenu> can be set to
      either <guimenu>iSCSI</guimenu> or <guimenu>NFS</guimenu>. Choose the
      driver and the protocol your NetApp is licensed for.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><guimenu>Server host name</guimenu>
    </term>
    <listitem>
     <para>
      The management IP address for the 7-Mode storage controller, or the
      cluster management IP address for the clustered Data ONTAP.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><guimenu>Transport Type</guimenu>
    </term>
    <listitem>
     <para>
      Transport protocol for communicating with the storage controller or
      clustered Data ONTAP. Supported protocols are HTTP and HTTPS. Choose the
      protocol your NetApp is licensed for.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><guimenu>Server port</guimenu>
    </term>
    <listitem>
     <para>
      The port to use for communication. Port 80 is usually used for HTTP, 443
      for HTTPS.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><guimenu>Username for accessing NetApp</guimenu> / <guimenu>Password for Accessing NetApp</guimenu>
    </term>
    <listitem>
     <para>
      Login credentials.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><guimenu>
      The vFiler Unit Name for provisioning &ostack; volumes (netapp_vfiler)
     </guimenu>
    </term>
    <listitem>
     <para>
      The vFiler unit to be used for provisioning of &ostack; volumes. This
      setting is only available in <guimenu>7-Mode</guimenu>.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><guimenu>Restrict provisioning on iSCSI to these volumes (netapp_volume_list)</guimenu>
    </term>
    <listitem>
     <para>
      Provide a list of comma-separated volume names to be used for
      provisioning. This setting is only available when using iSCSI as storage
      protocol.
     </para>
    </listitem>
   </varlistentry>
  </variablelist>

  <bridgehead renderas="sect2"><guimenu>NFS</guimenu>
  </bridgehead>

  <variablelist>
   <varlistentry>
    <term><guimenu>List of NFS Exports</guimenu>
    </term>
    <listitem>
     <para>
      A list of available file systems on an NFS server. Enter your NFS mountpoints
      in the <guimenu>List of NFS Exports</guimenu> form in this format: <replaceable>host:mountpoint -o options</replaceable>. For example:
     </para>
<screen>
host1:/srv/nfs/share1 /mnt/nfs/share1 -o rsize=8192,wsize=8192,timeo=14,intr
</screen>
    </listitem>
   </varlistentry>
  </variablelist>

  <bridgehead renderas="sect2"><guimenu>Pure Storage (FlashArray)</guimenu>
  </bridgehead>

  <variablelist>
   <varlistentry>
    <term><guimenu>IP address of the management VIP</guimenu>
    </term>
    <listitem>
     <para>
      IP address of the FlashArray management VIP
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><guimenu>API token for the FlashArray</guimenu>
    </term>
    <listitem>
     <para>
      API token for access to the FlashArray
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><guimenu>iSCSI CHAP authentication enabled</guimenu>
    </term>
    <listitem>
     <para>
      Enable or disable iSCSI CHAP authentication
     </para>
    </listitem>
   </varlistentry>
  </variablelist>

  <para>
   For more information on the Pure Storage FlashArray driver refer to the &ostack; documentation
   at
   <link xlink:href="https://docs.openstack.org/ocata/config-reference/block-storage/drivers/pure-storage-driver.html"/>.
  </para>

  <bridgehead renderas="sect3"><guimenu>RADOS</guimenu> (&ceph;)
  </bridgehead>

  <variablelist>
   <varlistentry>
    <term><guimenu>Use Ceph Deployed by Crowbar</guimenu>
    </term>
    <listitem>
     <para>
      Select <guimenu>false</guimenu>, if you are using an external &ceph; cluster (see
      <xref linkend="sec.depl.inst.nodes.post.ceph_ext"/> for setup
      instructions).
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><guimenu>RADOS pool for Cinder volumes</guimenu>
    </term>
    <listitem>
     <para>
      Name of the pool used to store the &o_blockstore; volumes.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><guimenu>
      RADOS user (Set Only if Using CephX authentication)
     </guimenu>
    </term>
    <listitem>
     <para>
      &ceph; user name.
     </para>
    </listitem>
   </varlistentry>
  </variablelist>

  <bridgehead renderas="sect3"><guimenu>&vmware; Parameters</guimenu>
  </bridgehead>

  <variablelist>
   <varlistentry>
    <term><guimenu>vCenter Host/IP Address</guimenu>
    </term>
    <listitem>
     <para>
      Host name or IP address of the vCenter server.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><guimenu>vCenter Username</guimenu> / <guimenu>vCenter
     Password</guimenu>
    </term>
    <listitem>
     <para>
      vCenter login credentials.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><guimenu>vCenter Cluster Names for Volumes</guimenu>
    </term>
    <listitem>
     <para>
      Provide a comma-separated list of cluster names.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><guimenu>Folder for Volumes</guimenu>
    </term>
    <listitem>
     <para>
      Path to the directory used to store the &o_blockstore; volumes.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><guimenu>CA file for verifying the vCenter certificate</guimenu>
    </term>
    <listitem>
     <para>
      Absolute path to the vCenter CA certificate.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><guimenu>
      vCenter SSL Certificate is insecure (for instance, self-signed)
     </guimenu>
    </term>
    <listitem>
     <para>
      Default value: <literal>false</literal> (the CA truststore is used for
      verification). Set this option to <literal>true</literal> when using
      self-signed certificates to disable certificate checks. This setting is
      for testing purposes only and must not be used in production
      environments!
     </para>
    </listitem>
   </varlistentry>
  </variablelist>

  <bridgehead renderas="sect2"><guimenu>Local file</guimenu>
  </bridgehead>

  <variablelist>
   <varlistentry>
    <term><guimenu>Volume File Name</guimenu>
    </term>
    <listitem>
     <para>
      Absolute path to the file to be used for block storage.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><guimenu>Maximum File Size (GB)</guimenu>
    </term>
    <listitem>
     <para>
      Maximum size of the volume file. Make sure not to overcommit the size,
      since it will result in data loss.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><guimenu>Name of Volume</guimenu>
    </term>
    <listitem>
     <para>
      Specify a name for the &o_blockstore; volume.
     </para>
    </listitem>
   </varlistentry>
  </variablelist>

  <note>
   <title>Using <guimenu>Local File</guimenu> for &blockstore;</title>
   <para>
    Using a file for block storage is not recommended for production systems,
    because of performance and data security reasons.
   </para>
  </note>

  <bridgehead renderas="sect3"><guimenu>Other driver</guimenu>
  </bridgehead>

  <para>
   Lets you manually pick and configure a driver. Only use this option for
   testing purposes, as it is not supported.
  </para>

  <figure>
   <title>The &o_blockstore; &Barcl;</title>
   <mediaobject>
    <imageobject role="fo">
     <imagedata fileref="depl_barclamp_cinder.png" width="100%" format="png"/>
    </imageobject>
    <imageobject role="html">
     <imagedata fileref="depl_barclamp_cinder.png" width="75%" format="png"/>
    </imageobject>
   </mediaobject>
  </figure>

  <para>
   The &o_blockstore; component consists of two different roles:
  </para>

  <variablelist>
   <varlistentry>
    <term><guimenu>cinder-controller</guimenu>
    </term>
    <listitem>
     <para>
      The &o_blockstore; controller provides the scheduler and the API.
      Installing <guimenu>cinder-controller</guimenu> on a &contrnode; is
      recommended.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><guimenu>cinder-volume</guimenu>
    </term>
    <listitem>
     <para>
      The virtual block storage service. It can be installed on a &contrnode;.
      However, we recommend deploying it on one or more dedicated nodes
      supplied with sufficient networking capacity to handle the increase in
      network traffic.
     </para>
    </listitem>
   </varlistentry>
  </variablelist>

  <figure>
   <title>The &o_blockstore; &Barcl;: Node Deployment Example</title>
   <mediaobject>
    <imageobject role="fo">
     <imagedata fileref="depl_barclamp_cinder_node_deployment.png" width="100%" format="png"/>
    </imageobject>
    <imageobject role="html">
     <imagedata fileref="depl_barclamp_cinder_node_deployment.png" width="75%" format="png"/>
    </imageobject>
   </mediaobject>
  </figure>

  <sect2 xml:id="sec.depl.ostack.cinder.ha">
   <title>&haSetup; for &o_blockstore;</title>
   <para>
    Both the <guimenu>cinder-controller</guimenu> and the
    <guimenu>cinder-volume</guimenu> role can be deployed on a cluster.
   </para>
   <note>
    <title>Moving <guimenu>cinder-volume</guimenu> to a Cluster</title>
    <para>
     If you need to re-deploy <guimenu>cinder-volume</guimenu> role from a
     single machine to a cluster environment, the following will happen:
     Volumes that are currently attached to instances will continue to work,
     but adding volumes to instances will not succeed.
    </para>
    <para>
     To solve this issue, run the following script once on each node that
     belongs to the <guimenu>cinder-volume</guimenu> cluster:
     <filename>/usr/bin/cinder-migrate-volume-names-to-cluster</filename>.
    </para>
    <para>
     The script is automatically installed by &crow; on every machine or
     cluster that has a <guimenu>cinder-volume</guimenu> role applied to it.
    </para>
   </note>
   <para>
    In combination with &ceph; or a network storage solution, deploying
    &o_blockstore; in a cluster minimizes the potential downtime. For
    <guimenu>cinder-volume</guimenu> to be applicable to a cluster, the role
    needs all &o_blockstore; backends to be configured for non-local
    storage. If you are using local volumes or raw devices in any of your
    volume backends, you cannot apply <guimenu>cinder-volume</guimenu> to a
    cluster.
   </para>
  </sect2>
 </sect1>
 <sect1 xml:id="sec.depl.ostack.quantum">
  <title>Deploying &o_netw;</title>

  <para>
   &o_netw; provides network connectivity between interface devices managed by
   other &ostack; components (most likely &o_comp;). The service works by
   enabling users to create their own networks and then attach interfaces to
   them.
  </para>

  <para>
   &o_netw; must be deployed on a &contrnode;. You first need to choose a core
   plug-in&mdash;<guimenu>ml2</guimenu> or <guimenu>vmware</guimenu>. Depending
   on your choice, more configuration options will become available.
  </para>

  <para>
   The <guimenu>vmware</guimenu> option lets you use an existing &vmware; NSX
   installation. Using this plugin is not a prerequisite for the &vmware; vSphere
   hypervisor support. However, it is needed when wanting to have security
   groups supported on &vmware; compute nodes. For all other scenarios, choose
   <guimenu>ml2</guimenu>.
  </para>

  <para>
   The only global option that can be configured is <guimenu>SSL
   Support</guimenu>. Choose whether to encrypt public communication
   (<guimenu>HTTPS</guimenu>) or not (<guimenu>HTTP</guimenu>). If choosing
   <guimenu>HTTPS</guimenu>, refer to
   <xref linkend="sec.depl.ostack.keystone.ssl"/> for configuration details.
  </para>

  <bridgehead renderas="sect2"><guimenu>ml2</guimenu> (Modular Layer 2)
  </bridgehead>

  <variablelist>
   <varlistentry>
    <term><guimenu>Modular Layer 2 Mechanism Drivers</guimenu>
    </term>
    <listitem>
     <para>
      Select which mechanism driver(s) shall be enabled for the ml2 plugin. It
      is possible to select more than one driver by holding the
      <keycap function="control"/> key while clicking. Choices are:
     </para>
     <formalpara>
      <title><guimenu>openvswitch</guimenu></title>
      <para>
       Supports GRE, VLAN and VXLAN networks (to be configured via the
       <guimenu>Modular Layer 2 type drivers</guimenu> setting). VXLAN is the
       default.
      </para>
     </formalpara>
     <formalpara>
      <title><guimenu>linuxbridge</guimenu></title>
      <para>
       Supports VLANs only. Requires to specify the <guimenu>Maximum Number of
       VLANs</guimenu>.
      </para>
     </formalpara>
     <formalpara>
      <title><guimenu>cisco_nexus</guimenu></title>
      <para>
       Enables &o_netw; to dynamically adjust the VLAN settings of the ports of
       an existing Cisco Nexus switch when instances are launched. It also
       requires <guimenu>openvswitch</guimenu> which will automatically be
       selected. With <guimenu>Modular Layer 2 type drivers</guimenu>,
       <guimenu>vlan</guimenu> must be added. This option also requires to
       specify the <guimenu>Cisco Switch Credentials</guimenu>. See
       <xref
       linkend="app.deploy.cisco"/> for details.
      </para>
     </formalpara>
     <formalpara>
      <title><guimenu>vmware_dvs</guimenu></title>
      <para>
       vmware_dvs driver makes it possible to use &o_netw; for networking in a
       &vmware;-based environment. Choosing <guimenu>vmware_dvs</guimenu>,
       automatically selects the required <guimenu>openswitch</guimenu>, <guimenu>vxlan</guimenu>, and
       <guimenu>vlan</guimenu> drivers. In the <guimenu>Raw</guimenu> view,
       it is also possible to configure two additional attributes:
       <guimenu>clean_on_start</guimenu> (clean up the DVS portgroups on the
       target vCenter Servers when neutron-server is restarted) and
       <guimenu>precreate_networks</guimenu> (create DVS portgroups
       corresponding to networks in advance, rather than when virtual machines are attached to these networks).
      </para>
     </formalpara>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><guimenu>Use Distributed Virtual Router Setup</guimenu>
    </term>
    <listitem>
     <para>
      With the default setup, all intra-&compnode; traffic flows through the
      network &contrnode;. The same is true for all traffic from floating IPs.
      In large deployments the network &contrnode; can therefore quickly become
      a bottleneck. When this option is set to <guimenu>true</guimenu>, network
      agents will be installed on all compute nodes. This will de-centralize
      the network traffic, since &compnode;s will be able to directly
      <quote>talk</quote> to each other. Distributed Virtual Routers (DVR)
      require the <guimenu>openvswitch</guimenu> driver and will not work with
      the <guimenu>linuxbridge</guimenu> driver. For details on DVR refer to
      <link xlink:href="https://wiki.openstack.org/wiki/Neutron/DVR"/>.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><guimenu>Modular Layer 2 Type Drivers</guimenu>
    </term>
    <listitem>
     <para>
      This option is only available when having chosen the
      <guimenu>openvswitch</guimenu> or the <guimenu>cisco_nexus</guimenu>
      mechanism drivers. Options are <guimenu>vlan</guimenu>,
      <guimenu>gre</guimenu> and <guimenu>vxlan</guimenu>. It is possible to
      select more than one driver by holding the <keycap function="control"/>
      key while clicking.
     </para>
     <para>
      When multiple type drivers are enabled, you need to select the
      <guimenu>Default Type Driver for Provider Network</guimenu>, that will be
      used for newly created provider networks. This also includes the
      <literal>nova_fixed</literal> network, that will be created when applying
      the &o_netw; proposal. When manually creating provider networks with the
      <command>neutron</command> command, the default can be overwritten with
      the <option>--provider:network_type
      <replaceable>type</replaceable></option> switch. You will also need to
      set a <guimenu>Default Type Driver for Tenant Network</guimenu>. It is
      not possible to change this default when manually creating tenant
      networks with the <command>neutron</command> command. The non-default
      type driver will only be used as a fallback.
     </para>
     <para>
      Depending on your choice of the type driver, more configuration options
      become available.
     </para>
     <formalpara>
      <title><guimenu>gre</guimenu></title>
      <para>
       Having chosen <guimenu>gre</guimenu>, you also need to specify the start
       and end of the tunnel ID range.
      </para>
     </formalpara>
     <formalpara>
      <title><guimenu>vlan</guimenu></title>
      <para>
       The option <guimenu>vlan</guimenu> requires you to specify the
       <guimenu>Maximum number of VLANs</guimenu>.
      </para>
     </formalpara>
     <formalpara>
      <title><guimenu>vxlan</guimenu></title>
      <para>
       Having chosen <guimenu>vxlan</guimenu>, you also need to specify the
       start and end of the VNI range.
      </para>
     </formalpara>
    </listitem>
   </varlistentry>
  </variablelist>

  <important>
   <title>Drivers for the &vmware; &compnode;</title>
   <para>
    &o_netw; must not be deployed with the <literal>openvswitch with
    gre</literal> plug-in. See <xref linkend="app.deploy.vmware"/> for details.
   </para>
  </important>

  <bridgehead renderas="sect2"><guimenu>z/VM Configuration</guimenu>
  </bridgehead>

  <variablelist>
   <varlistentry>
    <term>xCAT Host/IP Address</term>
    <listitem>
     <para>
      Host name or IP address of the xCAT Management Node.
      <remark>
            2016-12-29 - dpopov: DEVS FIXME Please check whether this is correct.</remark>
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>xCAT Username/Password</term>
    <listitem>
     <para>
      xCAT login credentials.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>rdev list for physnet1 vswitch uplink (if available)</term>
    <listitem>
     <para>
      List of rdev addresses that should be connected to this vswitch.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>xCAT IP Address on Management Network</term>
    <listitem>
     <para>
      IP address of the xCAT management interface.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>Net Mask of Management Network</term>
    <listitem>
     <para>
      Net mask of the xCAT management interface.
      <remark>
            2016-12-29 - dpopov: DEVS FIXME Please check whether this is correct.</remark>
     </para>
    </listitem>
   </varlistentry>
  </variablelist>

  <bridgehead renderas="sect2"><guimenu>vmware</guimenu>
  </bridgehead>

  <para>
   This plug-in requires to configure access to the &vmware; NSX service.
  </para>

  <variablelist>
   <varlistentry>
    <term><guimenu>&vmware; NSX User Name/Password</guimenu>
    </term>
    <listitem>
     <para>
      Login credentials for the &vmware; NSX server. The user needs to have
      administrator permissions on the NSX server.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><guimenu>&vmware; NSX Controllers</guimenu>
    </term>
    <listitem>
     <para>
      Enter the IP address and the port number
      (<replaceable>IP-ADDRESS</replaceable>:<replaceable>PORT</replaceable>)
      of the controller API endpoint. If the port number is omitted, port 443
      will be used. You may also enter multiple API endpoints
      (comma-separated), provided they all belong to the same controller
      cluster. When multiple API endpoints are specified, the plugin will load
      balance requests on the various API endpoints.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><guimenu>UUID of the NSX Transport Zone/Gateway Service</guimenu>
    </term>
    <listitem>
     <para>
      The UUIDs for the transport zone and the gateway service can be obtained
      from the NSX server. They will be used when networks are created.
     </para>
    </listitem>
   </varlistentry>
  </variablelist>

  <figure>
   <title>The &o_netw; &Barcl;</title>
   <mediaobject>
    <imageobject role="fo">
     <imagedata fileref="depl_barclamp_network.png" width="100%" format="png"/>
    </imageobject>
    <imageobject role="html">
     <imagedata fileref="depl_barclamp_network.png" width="75%" format="png"/>
    </imageobject>
   </mediaobject>
  </figure>

  <para>
   The &o_netw; component consists of two different roles:
  </para>

  <variablelist>
   <varlistentry>
    <term><guimenu>neutron-server</guimenu>
    </term>
    <listitem>
     <para>
      <guimenu>neutron-server</guimenu> provides the scheduler and the API. It
      needs to be installed on a &contrnode;.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><guimenu>neutron-network</guimenu>
    </term>
    <listitem>
     <para>
      This service runs the various agents that manage the network traffic of
      all the cloud instances. It acts as the DHCP and DNS server and as a
      gateway for all cloud instances. It is recommend to deploy this role on a
      dedicated node supplied with sufficient network capacity.
     </para>
    </listitem>
   </varlistentry>
  </variablelist>

  <figure>
   <title>The &o_netw; &barcl;</title>
   <mediaobject>
    <imageobject role="fo">
     <imagedata fileref="depl_barclamp_neutron_node_deployment.png" width="100%" format="png"/>
    </imageobject>
    <imageobject role="html">
     <imagedata fileref="depl_barclamp_neutron_node_deployment.png" width="75%" format="png"/>
    </imageobject>
   </mediaobject>
  </figure>

  <sect2 xml:id="sec.depl.ostack.network.infoblox">
   <title>Using Infoblox IPAM Plug-in</title>
   <para>
    In the &o_netw; &barcl;, you can enable support for the infoblox IPAM
    plug-in and configure it. For configuration, the
    <literal>infoblox</literal> section contains the subsections
    <literal>grids</literal> and <literal>grid_defaults</literal>.
   </para>
   <variablelist>
    <varlistentry>
     <term>grids</term>
     <listitem>
      <para>
       This subsection must contain at least one entry. For each entry, the
       following parameters are required:
      </para>
      <itemizedlist>
       <listitem>
        <para>
         admin_user_name
        </para>
       </listitem>
       <listitem>
        <para>
         admin_password
        </para>
       </listitem>
       <listitem>
        <para>
         grid_master_host
        </para>
       </listitem>
       <listitem>
        <para>
         grid_master_name
        </para>
       </listitem>
       <listitem>
        <para>
         data_center_name
        </para>
       </listitem>
      </itemizedlist>
      <para>
       You can also add multiple entries to the <literal>grids</literal>
       section. However, the upstream infoblox agent only supports a single
       grid currently.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>grid_defaults</term>
     <listitem>
      <para>
       This subsection contains the default settings that are used for each
       grid (unless you have configured specific settings within the
       <literal>grids</literal> section).
      </para>
     </listitem>
    </varlistentry>
   </variablelist>
   <para>
    For detailed information on all infoblox-related configuration settings,
    see
    <link
     xlink:href="https://github.com/openstack/networking-infoblox/blob/master/doc/source/installation.rst"/>.
   </para>
   <para>
    Currently, all configuration options for infoblox are only available in the
    <literal>raw</literal> mode of the &o_netw; &barcl;. To enable support for
    the infoblox IPAM plug-in and configure it, proceed as follows:
   </para>
   <procedure>
    <step>
     <para>
      <guimenu>Edit</guimenu> the &o_netw; &barcl; proposal or create a new
      one.
     </para>
    </step>
    <step>
     <para>
      Click <guimenu>Raw</guimenu> and search for the following section:
     </para>
<screen>"use_infoblox": false,</screen>
    </step>
    <step>
     <para>
      To enable support for the infoblox IPAM plug-in, change this entry to:
     </para>
<screen>"use_infoblox": true,</screen>
    </step>
    <step>
     <para>
      In the <literal>grids</literal> section, configure at least one grid by
      replacing the example values for each parameter with real values.
     </para>
    </step>
    <step>
     <para>
      If you need specific settings for a grid, add some of the parameters from
      the <literal>grid_defaults</literal> section to the respective grid entry
      and adjust their values.
     </para>
     <para>
      Otherwise &crow; applies the default setting to each grid when you save
      the &barcl; proposal.
     </para>
    </step>
    <step>
     <para>
      Save your changes and apply them.
     </para>
    </step>
   </procedure>
  </sect2>

  <sect2 xml:id="sec.depl.ostack.network.ha">
   <title>&haSetup; for &o_netw;</title>
   <para>
    &o_netw; can be made highly available by deploying
    <guimenu>neutron-server</guimenu> and <guimenu>neutron-network</guimenu> on
    a cluster. While <guimenu>neutron-server</guimenu> may be deployed on a
    cluster shared with other services, it is strongly recommended to use a
    dedicated cluster solely for the <guimenu>neutron-network</guimenu> role.
   </para>
  </sect2>
  <sect2 xml:id="sec.setup.multi.ext.networks">
  <title>Setting Up Multiple External Networks</title>

  <para>
   This section shows you how to create external networks on &cloud;.
  </para>

  <sect3 xml:id="sec.config.multi.ext.networks">
   <title>New Network Configurations</title>
   <procedure>
    <step>
     <para>
      If you have not yet deployed &crow;, add the following configuration to
      <filename>/etc/crowbar/network.json</filename>
      to set up an external network, using the name of your new network, VLAN
      ID, and network addresses. If you have already deployed &crow;, then add
      this configuration to the <guimenu>Raw</guimenu> view of the Network &Barcl;.
     </para>
<screen>"<replaceable>public2</replaceable>": {
          "conduit": "intf1",
          "vlan": <replaceable>600</replaceable>,
          "use_vlan": true,
          "add_bridge": false,
          "subnet": "<replaceable>192.168.135.128</replaceable>",
          "netmask": "<replaceable>255.255.255.128</replaceable>",
          "broadcast": "<replaceable>192.168.135.255</replaceable>",
          "ranges": {
            "host": { "start": "<replaceable>192.168.135.129</replaceable>",
               "end": "<replaceable>192.168.135.254</replaceable>" }
          }
    },</screen>
    </step>
    <step>
     <para>
      Modify the <parameter>additional_external_networks</parameter> in the
      <guimenu>Raw</guimenu> view of the &o_netw; &Barcl; with the name of your
      new external network.
     </para>
     </step>
     <step>
     <para>
       Apply both barclamps, and it may also be necessary to re-apply the &o_comp;
       &Barcl;.
   </para>
    </step>
    <step>
    <para>
      Then follow the steps in the next section to create the new external network.
    </para>
    </step>
   </procedure>
  </sect3>

  <sect3 xml:id="sec.confignet">
   <title>Create the New External Network</title>
   <para>
    The following steps add the network settings, including IP address pools,
    gateway, routing, and virtual switches to your new network.
   </para>
   <procedure xml:id="pro.confignet">
    <step>
     <para>
      Set up interface mapping using either Open vSwitch (OVS) or Linuxbridge.
      For Open vSwitch run the following command:
     </para>
<screen>openstack network create <replaceable>public2</replaceable> --provider:network_type flat \
 --provider:physical_network <replaceable>public2</replaceable> --router:external=True</screen>
     <para>
      For Linuxbridge run the following command:
     </para>
<screen> openstack network create --router:external True --provider:physical_network physnet1 \
 --provider:network_type vlan --provider:segmentation_id <replaceable>600</replaceable></screen>
    </step>
    <step>
     <para>
      If a different network is used then &crow; will create a new interface
      mapping. Then you can use a flat network:
     </para>
<screen>
openstack network create <replaceable>public2</replaceable> --provider:network_type flat \
 --provider:physical_network <replaceable>public2</replaceable> --router:external=True
</screen>
    </step>
    <step>
     <para>
      Create a subnet:
     </para>
<screen>openstack subnet create --name <replaceable>public2</replaceable> --allocation-pool \
 start=<replaceable>192.168.135.2</replaceable>,end=<replaceable>192.168.135.127</replaceable> --gateway <replaceable>192.168.135.1</replaceable> <replaceable>public2</replaceable> \
 <replaceable>192.168.135.0/24</replaceable> --enable_dhcp False</screen>
    </step>
    <step>
     <para>
      Create a router, <replaceable>router2</replaceable>:
     </para>
<screen>openstack router create <replaceable>router2</replaceable></screen>
    </step>
    <step>
     <para>
      Connect <replaceable>router2</replaceable> to the new external network:
     </para>
<screen>openstack router set <replaceable>router2</replaceable>  <replaceable>public2</replaceable></screen>
    </step>
    <step>
     <para>
      Create a new private network and connect it to
      <replaceable>router2</replaceable>
     </para>
<screen>openstack network create priv-net
openstack subnet create priv-net --gateway <replaceable>10.10.10.1 10.10.10.0/24</replaceable> \
 --name priv-net-sub
openstack router add subnet <replaceable>router2</replaceable> priv-net-sub</screen>
    </step>
    <step>
     <para>
      Boot a VM on priv-net-sub and set a security group that allows SSH.
     </para>
    </step>
    <step>
     <para>
      Assign a floating IP address to the VM, this time from network
      <replaceable>public2</replaceable>.
     </para>
    </step>
    <step>
     <para>
      From the node verify that SSH is working by opening an SSH session to the
      VM.
     </para>
    </step>
   </procedure>
  </sect3>

  <sect3 xml:id="sec.howbridges">
   <title>How the Network Bridges are Created</title>
   <para>
    For OVS, a new bridge will be created by &crow;, in this case
    <literal>br-public2</literal>. In the bridge mapping the new network will
    be assigned to the bridge. The interface specified in
    <filename>/etc/crowbar/network.json</filename> (in this case eth0.600) will
    be plugged into <literal>br-public2</literal>. The new public network can
    be created in &o_netw; using the new public network name as
    <parameter>provider:physical_network</parameter>.
   </para>
   <para>
    For Linuxbridge, &crow; will check the interface associated with
    <replaceable>public2</replaceable>. If this is the same as physnet1 no
    interface mapping will be created. The new public network can be created in
    &o_netw; using physnet1 as physical network and specifying the correct VLAN
    ID:
   </para>
<screen>openstack network create <replaceable>public2</replaceable> --router:external True \
 --provider:physical_network physnet1 --provider:network_type vlan \
 --provider:segmentation_id <replaceable>600</replaceable>
</screen>
   <para>
    A bridge named <varname>brq-NET_ID</varname> will be created and the
    interface specified in <filename>/etc/crowbar/network.json</filename> will
    be plugged into it. If a new interface is associated in
    <filename>/etc/crowbar/network.json</filename> with
    <replaceable>public2</replaceable> then &crow; will add a new interface
    mapping and the second public network can be created using
    <replaceable>public2</replaceable> as the physical network:
   </para>
<screen>openstack network create <replaceable>public2</replaceable> --provider:network_type flat \
 --provider:physical_network <replaceable>public2</replaceable> --router:external=True</screen>
  </sect3>
 </sect2>

 </sect1>
 <sect1 xml:id="sec.depl.ostack.nova">
  <title>Deploying &o_comp;</title>

  <para>
   &o_comp; provides key services for managing the &cloud;, sets up the
   &compnode;s. &cloud; currently supports KVM, Xen and &vmware; vSphere. The
   unsupported QEMU option is included to enable test setups with virtualized
   nodes. The following attributes can be configured for &o_comp;:
   <remark condition="clarity">
    2016-02-05 - fs: FIXME z/VM Configuration
   </remark>
  </para>

  <variablelist>
   <varlistentry>
    <term><guimenu>
      Scheduler Options: Virtual RAM to Physical RAM allocation ratio
     </guimenu>
    </term>
    <listitem>
     <para>
      Set the <quote>overcommit ratio</quote> for RAM for &vmguest;s on the
      &compnode;s. A ratio of <literal>1.0</literal> means no overcommitment.
      Changing this value is not recommended.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><guimenu>
      Scheduler Options: Virtual CPU to Physical CPU allocation ratio
     </guimenu>
    </term>
    <listitem>
     <para>
      Set the <quote>overcommit ratio</quote> for CPUs for &vmguest;s on the
      &compnode;s. A ratio of <literal>1.0</literal> means no overcommitment.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><guimenu>
      Scheduler Options: Virtual Disk to Physical Disk allocation ratio
     </guimenu>
    </term>
    <listitem>
     <para>
      Set the <quote>overcommit ratio</quote> for virtual disks for &vmguest;s
      on the &compnode;s. A ratio of <literal>1.0</literal> means no
      overcommitment.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><guimenu>
      Scheduler Options: Reserved Memory for Nova Compute hosts (MB)
     </guimenu>
    </term>
    <listitem>
     <para>
      Amount of reserved host memory that is not used for allocating VMs by
      Nova Compute.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><guimenu>Live Migration Support: Enable Libvirt Migration</guimenu>
    </term>
    <listitem>
     <para>
      Allows to move &kvm; and &xen; &vmguest;s to a different &compnode;
      running the same hypervisor (cross hypervisor migrations are not
      supported). Useful when a &compnode; needs to be shut down or rebooted
      for maintenance or when the load of the &compnode; is very high.
      &Vmguest;s can be moved while running (Live Migration).
     </para>
     <warning>
      <title>Libvirt Migration and Security</title>
      <para>
       Enabling the libvirt migration option will open a TCP port on the
       &compnode;s that allows access to all &vmguest;s from all machines in
       the admin network. Ensure that only authorized machines have access to
       the admin network when enabling this option.
      </para>
     </warning>

     <tip>
      <title>Specifying Network for Live Migration</title>
      <para>
        It is possible to change a network to live migrate images. This is done
        in the raw view of the &o_comp; &barcl;. In the
        <literal>migration</literal> section, change the
        <varname>network</varname> attribute to the appropriate value (for
        example, <literal>storage</literal> for &ceph;).
      </para>
     </tip>

    </listitem>
   </varlistentry>
   <varlistentry>
    <term>Live Migration Support: Setup Shared Storage</term>
    <listitem>
     <para>
      Sets up a directory <filename>/var/lib/nova/instances</filename> on the
      &contrnode; on which <guimenu>nova-controller</guimenu> is running. This
      directory is exported via NFS to all compute nodes and will host a copy
      of the root disk of <emphasis>all</emphasis> &xen; &vmguest;s. This setup
      is required for live migration of &xen; &vmguest;s (but not for &kvm;)
      and is used to provide central handling of instance data. Enabling this
      option is only recommended if &xen; live migration is
      required&mdash;otherwise it should be disabled.
     </para>
     <warning>
      <title>Do Not Set Up Shared Storage When &vmguest;s are Running</title>
      <para>
       Setting up shared storage in a &cloud; where &vmguest;s are running will
       result in connection losses to all running &vmguest;s. It is strongly
       recommended to set up shared storage when deploying &cloud;. If it needs
       to be done at a later stage, make sure to shut down all &vmguest;s prior
       to the change.
      </para>
     </warning>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><guimenu>KVM Options: Enable Kernel Samepage Merging</guimenu>
    </term>
    <listitem>
     <para>
      Kernel SamePage Merging (KSM) is a Linux Kernel feature which merges
      identical memory pages from multiple running processes into one memory
      region. Enabling it optimizes memory usage on the &compnode;s when using
      the &kvm; hypervisor at the cost of slightly increasing CPU usage.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>&vmware; vCenter Settings</term>
    <listitem>
     <para>
      Setting up &vmware; support is described in a separate section. See
      <xref linkend="app.deploy.vmware"/>.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>SSL Support: Protocol</term>
    <listitem>
     <para>
      Choose whether to encrypt public communication (<guimenu>HTTPS</guimenu>)
      or not (<guimenu>HTTP</guimenu>). If choosing
      <guimenu>HTTPS</guimenu>,refer to
      <xref linkend="sec.depl.ostack.keystone.ssl"/> for configuration details.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>VNC Settings: NoVNC Protocol</term>
    <listitem>
     <para>
      After having started an instance you can display its VNC console in the
      &ostack; &dash; (&o_dash;) via the browser using the noVNC
      implementation. By default this connection is not encrypted and can
      potentially be eavesdropped.
     </para>
     <para>
      Enable encrypted communication for noVNC by choosing
      <guimenu>HTTPS</guimenu> and providing the locations for the certificate
      key pair files.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><guimenu>Logging: Verbose Logging</guimenu>
    </term>
    <listitem>
     <para>
      Shows debugging output in the log files when set to
      <guimenu>true</guimenu>.
     </para>
    </listitem>
   </varlistentry>
  </variablelist>

  <note xml:id="note.custom.vendor">
   <title>Custom Vendor Data for Instances</title>
   <para>
    You can pass custom vendor data to all VMs via &o_comp;'s metadata server.
    For example, information about a custom SMT server can be used by the
    &suse; guest images to automatically configure the repositories for the
    guest.
   </para>
   <orderedlist>
    <listitem>
     <para>
      To pass custom vendor data, switch to the <guimenu>Raw</guimenu> view of
      the &o_comp; &barcl;.
     </para>
    </listitem>
    <listitem>
     <para>
      Search for the following section:
     </para>
<screen>"metadata": {
  "vendordata": {
    "json": "{}"
  }
}</screen>
    </listitem>
    <listitem>
     <para>
      As value of the <literal>json</literal> entry, enter valid JSON data. For
      example:
     </para>
<screen>"metadata": {
  "vendordata": {
    "json": "{\"<replaceable>CUSTOM_KEY</replaceable>\": \"<replaceable>CUSTOM_VALUE</replaceable>\"}"
  }
}</screen>
     <para>
      The string needs to be escaped because the &barcl; file is in JSON
      format, too.
     </para>
    </listitem>
   </orderedlist>
   <para>
    Use the following command to access the custom vendor data from inside a
    VM:
   </para>
<screen>curl -s http://<replaceable>METADATA_SERVER</replaceable>/openstack/latest/vendor_data.json</screen>
   <para>
    The IP address of the metadata server is always the same from within a VM.
    For more details, see
    <link
    xlink:href="https://www.suse.com/communities/blog/vms-get-access-metadata-neutron/"/>.
   </para>
  </note>

  <figure>
   <title>The &o_comp; &Barcl;</title>
   <mediaobject>
    <imageobject role="fo">
     <imagedata fileref="depl_barclamp_nova.png" width="100%" format="png"/>
    </imageobject>
    <imageobject role="html">
     <imagedata fileref="depl_barclamp_nova.png" width="75%" format="png"/>
    </imageobject>
   </mediaobject>
  </figure>

  <para>
   The &o_comp; component consists of eight different roles:
  </para>

  <variablelist>
   <varlistentry>
    <term><guimenu>nova-controller</guimenu>
    </term>
    <listitem>
     <para>
      Distributing and scheduling the &vmguest;s is managed by the
      <guimenu>nova-controller</guimenu>. It also provides networking and
      messaging services. <guimenu>nova-controller</guimenu> needs to be
      installed on a &contrnode;.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><guimenu>nova-compute-kvm</guimenu> /
    <guimenu>nova-compute-qemu</guimenu> /
    <guimenu>nova-compute-vmware</guimenu> /
    <guimenu>nova-compute-xen</guimenu> /
    <guimenu>nova-compute-zvm</guimenu>
    </term>
    <listitem>
     <para>
      Provides the hypervisors (&kvm;, QEMU, &vmware; vSphere, &xen;, and z/VM)
      and tools needed to manage the &vmguest;s. Only one hypervisor can be
      deployed on a single compute node. To use different hypervisors in your
      cloud, deploy different hypervisors to different &compnode;s. A
      <literal>nova-compute-*</literal> role needs to be installed on every
      &compnode;. However, not all hypervisors need to be deployed.
     </para>
     <para>
      Each image that will be made available in &cloud; to start an &vmguest;
      is bound to a hypervisor. Each hypervisor can be deployed on multiple
      &compnode;s (except for the &vmware; vSphere role, see below). In a
      multi-hypervisor deployment you should make sure to deploy the
      <literal>nova-compute-*</literal> roles in a way, that enough compute
      power is available for each hypervisor.
     </para>
     <note>
      <title>Re-assigning Hypervisors</title>
      <para>
       Existing <literal>nova-compute-*</literal> nodes can be changed in a
       production &cloud; without service interruption. You need to
       <quote>evacuate</quote>
<!-- (see TODO) -->
       the node, re-assign a new <literal>nova-compute</literal> role via the
       &o_comp; &barcl; and <guimenu>Apply</guimenu> the change.
       <guimenu>nova-compute-vmware</guimenu> can only be deployed on a single
       node.
      </para>
     </note>
     <important>
      <title>Deploying &vmware; vSphere (vmware)</title>
      <para>
       <remark condition="clarity">
        2013-08-05 - fs: What network requirements/adjustments are needed ??
       </remark>
       &vmware; vSphere is not supported <quote>natively</quote> by
       &cloud;&mdash;it rather delegates requests to an existing vCenter. It
       requires preparations at the vCenter and post install adjustments of the
       &compnode;. See <xref linkend="app.deploy.vmware"/> for instructions.
       <guimenu>nova-compute-vmware</guimenu> can only be deployed on a single
       &compnode;.
      </para>
     </important>
    </listitem>
   </varlistentry>
  </variablelist>

  <figure>
   <title>The &o_comp; &Barcl;: Node Deployment Example with Two KVM Nodes</title>
   <mediaobject>
    <imageobject role="fo">
     <imagedata fileref="depl_barclamp_nova_node_deployment.png" width="100%" format="png"/>
    </imageobject>
    <imageobject role="html">
     <imagedata fileref="depl_barclamp_nova_node_deployment.png" width="75%" format="png"/>
    </imageobject>
   </mediaobject>
  </figure>

  <para>
    When deploying a <guimenu>nova-compute-vmware</guimenu> node with the
    <guimenu>vmware_dvs</guimenu> ML2 driver enabled in the &o_netw; &barcl;, the following
    new attributes are also available in the <guimenu>vcenter</guimenu> section of the
    <guimenu>Raw</guimenu> mode:<guimenu>dvs_name</guimenu> (the name of the
    DVS switch configured on the target vCenter cluster) and
    <guimenu>dvs_security_groups</guimenu> (enable or disable implementing
    security groups through DVS traffic rules).
  </para>

  <para>
    It is important to specify the correct <guimenu>dvs_name</guimenu>
    value, as the &barcl; expects the  DVS switch to be preconfigured on the
    target &vmware; vCenter cluster.
  </para>

  <warning>
    <title>vmware_dvs must be enabled</title>
    <para>
      Deploying <guimenu>nova-compute-vmware</guimenu> nodes will not result in
      a functional cloud setup if the <guimenu>vmware_dvs</guimenu> ML2 plugin
      is not enabled in the &o_netw; &barcl;.
     </para>
    </warning>

  <sect2 xml:id="sec.depl.ostack.nova.ha">
   <title>&haSetup; for &o_comp;</title>
   <para>
    Making <guimenu>nova-controller</guimenu> highly available requires no
    special configuration&mdash;it is sufficient to deploy it on a cluster.
   </para>
   <para>
    To enable &ha; for &compnode;s, deploy the following roles to one or more
    clusters with remote nodes:
   </para>
   <itemizedlist>
    <listitem>
     <para>
      nova-compute-kvm
     </para>
    </listitem>
    <listitem>
     <para>
      nova-compute-qemu
     </para>
    </listitem>
    <listitem>
     <para>
      nova-compute-xen
     </para>
    </listitem>
    <listitem>
     <para>
      ec2-api
     </para>
    </listitem>
   </itemizedlist>
   <para>
    The cluster to which you deploy the roles above can be completely
    independent of the one to which the role <literal>nova-controller</literal>
    is deployed.
   </para>
   <para>
    However, the <literal>nova-controller</literal> and
    <literal>ec2-api</literal> roles must be deployed the same way (either
    <emphasis>both</emphasis> to a cluster or <emphasis>both</emphasis> to
    individual nodes. This is due to &crow; design limitations.
   </para>
   <tip>
    <title>Shared Storage</title>
    <para>
     It is recommended to use shared storage for the
     <filename>/var/lib/nova/instances</filename> directory, to ensure that
     ephemeral disks will be preserved during recovery of VMs from failed
     compute nodes. Without shared storage, any ephemeral disks will be lost,
     and recovery will rebuild the VM from its original image.
    </para>
    <para>
     If an external NFS server is used, enable the following option in the
     &o_comp; &barcl; proposal: <guimenu>Shared Storage for Nova instances has
     been manually configured</guimenu>.
    </para>
   </tip>
  </sect2>
 </sect1>
 <sect1 xml:id="sec.depl.ostack.dash">
  <title>Deploying &o_dash; (&ostack; &dash;)</title>

  <para>
   The last component that needs to be deployed is &o_dash;, the &ostack;
   &dash;. It provides a Web interface for users to start and stop &vmguest;s
   and for administrators to manage users, groups, roles, etc. &o_dash; should
   be installed on a &contrnode;. To make &o_dash; highly available, deploy it
   on a cluster.
  </para>

  <para>
   The following attributes can be configured:
  </para>

  <variablelist>
   <varlistentry>
    <term>Session Timeout</term>
    <listitem>
     <para>
      Timeout (in minutes) after which a user is been logged out automatically.
      The default value is set to four hours (240 minutes).
     </para>
     <note>
      <title>Timeouts Larger than Four Hours</title>
      <para>
       Every &o_dash; session requires a valid &o_ident; token. These tokens
       also have a lifetime of four hours (14400 seconds). Setting the &o_dash;
       session timeout to a value larger than 240 will therefore have no
       effect, and you will receive a warning when applying the &barcl;.
      </para>
      <para>
       To successfully apply a timeout larger than four hours, you first need
       to adjust the &o_ident; token expiration accordingly. To do so, open the
       &o_ident; &barcl; in <guimenu>Raw</guimenu> mode and adjust the value of
       the key <literal>token_expiration</literal>. Note that the value has to
       be provided in <emphasis>seconds</emphasis>. When the change is
       successfully applied, you can adjust the &o_dash; session timeout (in
       <emphasis>minutes</emphasis>). Note that extending the &o_ident; token
       expiration may cause scalability issues in large and very busy &cloud;
       installations.
      </para>
     </note>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><guimenu>
      User Password Validation: Regular expression used for password
      validation
     </guimenu>
    </term>
    <listitem>
     <para>
      Specify a regular expression with which to check the password. The
      default expression (<literal>.{8,}</literal>) tests for a minimum length
      of 8 characters. The string you enter is interpreted as a Python regular
      expression (see
      <link xlink:href="http://docs.python.org/2.7/library/re.html#module-re"/>
      for a reference).
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><guimenu>
      User Password Validation: Text to display if the password does not pass
      validation
     </guimenu>
    </term>
    <listitem>
     <para>
      Error message that will be displayed in case the password validation
      fails.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>SSL Support: Protocol</term>
    <listitem>
     <para>
      Choose whether to encrypt public communication (<guimenu>HTTPS</guimenu>)
      or not (<guimenu>HTTP</guimenu>). If choosing <guimenu>HTTPS</guimenu>,
      you have two choices. You can either <guimenu>Generate (self-signed)
      certificates</guimenu> or provide the locations for the certificate key
      pair files and,&mdash;optionally&mdash; the certificate chain file. Using
      self-signed certificates is for testing purposes only and should never be
      used in production environments!
     </para>
    </listitem>
   </varlistentry>
  </variablelist>

  <figure>
   <title>The &o_dash; &Barcl;</title>
   <mediaobject>
    <imageobject role="fo">
     <imagedata fileref="depl_barclamp_nova_dashboard.png" width="100%" format="png"/>
    </imageobject>
    <imageobject role="html">
     <imagedata fileref="depl_barclamp_nova_dashboard.png" width="75%" format="png"/>
    </imageobject>
   </mediaobject>
  </figure>

  <sect2 xml:id="sec.depl.ostack.dash.ha">
   <title>&haSetup; for &o_dash;</title>
   <para>
    Making &o_dash; highly available requires no special configuration&mdash;it
    is sufficient to deploy it on a cluster.
   </para>
  </sect2>
 </sect1>
 <sect1 xml:id="sec.depl.ostack.heat">
  <title>Deploying &o_orch; (Optional)</title>

  <para>
   &o_orch; is a template-based orchestration engine that enables you to, for
   example, start workloads requiring multiple servers or to automatically
   restart &vmguest;s if needed. It also brings auto-scaling to &cloud; by
   automatically starting additional &vmguest;s if certain criteria are met.
   For more information about &o_orch; refer to the &ostack; documentation at
   <link xlink:href="http://docs.openstack.org/developer/heat/"/>.
  </para>

  <para>
   &o_orch; should be deployed on a &contrnode;. To make &o_orch; highly
   available, deploy it on a cluster.
  </para>

  <para>
   The following attributes can be configured for &o_orch;:
  </para>

  <variablelist>
   <varlistentry>
    <term><guimenu>Verbose Logging</guimenu>
    </term>
    <listitem>
     <para>
      Shows debugging output in the log files when set to
      <guimenu>true</guimenu>.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>SSL Support: Protocol</term>
    <listitem>
     <para>
      Choose whether to encrypt public communication (<guimenu>HTTPS</guimenu>)
      or not (<guimenu>HTTP</guimenu>). If choosing
      <guimenu>HTTPS</guimenu>, refer to
      <xref linkend="sec.depl.ostack.keystone.ssl"/> for configuration details.
     </para>
    </listitem>
   </varlistentry>
  </variablelist>

  <figure>
   <title>The &o_orch; &Barcl;</title>
   <mediaobject>
    <imageobject role="fo">
     <imagedata fileref="depl_barclamp_heat.png" width="100%" format="png"/>
    </imageobject>
    <imageobject role="html">
     <imagedata fileref="depl_barclamp_heat.png" width="75%" format="png"/>
    </imageobject>
   </mediaobject>
  </figure>

  <sect2 xml:id="sec.depl.ostack.heat.delegated_roles">
   <title>Enabling Identity Trusts Authorization (Optional)</title>
   <para>
    Heat uses Keystone Trusts to delegate a subset of user roles to the
    &o_orch; engine for deferred operations (see
    <link
    xlink:href="http://hardysteven.blogspot.de/2014/04/heat-auth-model-updates-part-1-trusts.html">Steve
    Hardy's blog</link> for details). It can either delegate all user roles or
    only those specified in the <literal>trusts_delegated_roles</literal>
    setting. Consequently, all roles listed in
    <literal>trusts_delegated_roles</literal> need to be assigned to a user,
    otherwise the user will not be able to use &o_orch;.
   </para>
   <para>
    The recommended setting for <literal>trusts_delegated_roles</literal> is
    <literal>member</literal>, since this is the default role most users are
    likely to have. This is also the default setting when installing &cloud;
    from scratch.
   </para>
   <para>
    On installations where this setting is introduced through an upgrade,
    <literal>trusts_delegated_roles</literal> will be set to
    <literal>heat_stack_owner</literal>. This is a conservative choice to
    prevent breakage in situations where unprivileged users may already have
    been assigned the <literal>heat_stack_owner</literal> role to enable them
    to use Heat but lack the <literal>member</literal> role. As long as you can
    ensure that all users who have the <literal>heat_stack_owner</literal> role
    also have the <literal>member</literal> role, it is both safe and
    recommended to change trusts_delegated_roles to <literal>member</literal>.
    <!--commented out re https://bugzilla.suse.com/show_bug.cgi?id=1037531 ,
    since the latter is the default role assigned by our hybrid LDAP back-end
    among others.-->
   </para>
   <para>
    To view or change the trusts_delegated_role setting you need to open the
    &o_orch; &barcl; and click <guimenu>Raw</guimenu> in the
    <guimenu>Attributes</guimenu> section. Search for the
    <literal>trusts_delegated_roles</literal> setting and modify the list of
    roles as desired.
   </para>
   <figure>
    <title>the &o_orch; &barcl;: Raw Mode</title>
    <mediaobject>
     <imageobject role="fo">
      <imagedata fileref="depl_barclamp_heat_raw.png" width="100%" format="png"/>
     </imageobject>
     <imageobject role="html">
      <imagedata fileref="depl_barclamp_heat_raw.png" width="75%" format="png"/>
     </imageobject>
    </mediaobject>
   </figure>
   <warning>
    <title>Empty Value</title>
    <para>
     An empty value for <literal>trusts_delegated_roles</literal> will delegate
     <emphasis>all</emphasis> of user roles to Heat. This may create a security
     risk for users who are assigned privileged roles, such as
     <literal>admin</literal>, because these privileged roles will also be
     delegated to the &o_orch; engine when these users create &o_orch; stacks.
    </para>
   </warning>
  </sect2>

  <sect2 xml:id="sec.depl.ostack.heat.ha">
   <title>&haSetup; for &o_orch;</title>
   <para>
    Making &o_orch; highly available requires no special configuration&mdash;it
    is sufficient to deploy it on a cluster.
   </para>
  </sect2>
 </sect1>
 <sect1 xml:id="sec.depl.ostack.ceilometer">
  <title>Deploying &o_meter; (Optional)</title>

  <para>
   <remark condition="clarity">
    2013-10-04 - fs: Which software/billing solution can make use of the
    ceilometer data?
   </remark>
   &o_meter; collects CPU and networking data from &cloud;. This data can be
   used by a billing system to enable customer billing. Deploying &o_meter; is
   optional.
  </para>

  <para>
   For more information about &o_meter; refer to the &ostack; documentation at
   <link xlink:href="http://docs.openstack.org/developer/ceilometer/"/>.
  </para>

  <important>
   <title>&o_meter; Restrictions</title>
   <para>
    As of &productname; 8 data measuring is only supported for
    &kvm;, &xen; and Windows &vmguest;s. Other hypervisors and &cloud; features
    such as object or block storage will not be measured.
   </para>
  </important>

  <para>
   The following attributes can be configured for &o_meter;:
  </para>

  <variablelist>
   <varlistentry>
    <term>
     Interval used for CPU/disk/network/other meter updates (in seconds)
    </term>
    <listitem>
     <para>
      Specify an interval in seconds after which &o_meter; performs an update
      of the specified meter.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>Evaluation interval for threshold alarms (in seconds)</term>
    <listitem>
     <para>
      Set the interval after which to check whether to raise an alarm because a
      threshold has been exceeded. For performance reasons, do not set a value
      lower than the default (60s).
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>How long are metering/event samples kept in the database (in days)
    </term>
    <listitem>
     <para>
      Specify how long to keep the data. -1 means that samples are kept in the
      database forever.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>Verbose Logging
    </term>
    <listitem>
     <para>
      Shows debugging output in the log files when set to
      <guimenu>true</guimenu>.
     </para>
    </listitem>
   </varlistentry>
  </variablelist>

  <figure>
   <title>The &o_meter; &Barcl;</title>
   <mediaobject>
    <imageobject role="fo">
     <imagedata fileref="depl_barclamp_ceilometer.png" width="100%" format="png"/>
    </imageobject>
    <imageobject role="html">
     <imagedata fileref="depl_barclamp_ceilometer.png" width="75%" format="png"/>
    </imageobject>
   </mediaobject>
  </figure>

  <variablelist>
   <varlistentry xml:id="sec.depl.ostack.ceilometer.ssl">
    <term>SSL Support: Protocol
        </term>
    <listitem>
     <para>
      With the default value <guimenu>HTTP</guimenu> enabled, public
      communication is not be encrypted. Choose <guimenu>HTTPS</guimenu> to use
      SSL for encryption. See <xref linkend="sec.depl.req.ssl"/> for background
      information and <xref linkend="sec.depl.inst.nodes.post.ssl"/> for
      installation instructions. The following additional configuration options
      will become available when choosing <guimenu>HTTPS</guimenu>:
     </para>
     <variablelist>
      <varlistentry>
       <term><guimenu>Generate (self-signed) certificates</guimenu>
       </term>
       <listitem>
        <para>
         When set to <literal>true</literal>, self-signed certificates are
         automatically generated and copied to the correct locations. This
         setting is for testing purposes only and should never be used in
         production environments!
        </para>
       </listitem>
      </varlistentry>
      <varlistentry>
       <term><guimenu>SSL Certificate File</guimenu> / <guimenu>SSL (Private) Key
        File</guimenu>
       </term>
       <listitem>
        <para>
         Location of the certificate key pair files.
        </para>
       </listitem>
      </varlistentry>
      <varlistentry>
       <term><guimenu>SSL Certificate is insecure</guimenu>
       </term>
       <listitem>
        <para>
         Set this option to <literal>true</literal> when using self-signed
         certificates to disable certificate checks. This setting is for
         testing purposes only and should never be used in production
         environments!
        </para>
       </listitem>
      </varlistentry>
      <varlistentry>
       <term><guimenu>SSL CA Certificates File</guimenu>
       </term>
       <listitem>
        <para>
         Specify the absolute path to the CA certificate. This field is
         mandatory, and leaving it blank will cause the &barcl; to fail. To fix
         this issue, you have to provide the absolute path to the CA
         certificate, restart the <systemitem>apache2</systemitem> service, and
         re-deploy the &barcl;.
        </para>
        <para>
         When the certificate is not already trusted by the pre-installed list
         of trusted root certificate authorities, you need to provide a
         certificate bundle that includes the root and all intermediate CAs.
        </para>
       </listitem>
      </varlistentry>
     </variablelist>
    </listitem>
   </varlistentry>
  </variablelist>

  <para>
   The &o_meter; component consists of five different roles:
  </para>

  <variablelist>
   <varlistentry>
    <term><guimenu>ceilometer-server</guimenu>
    </term>
    <listitem>
     <para>
      The &o_meter; API server role. This role needs to be deployed on a
      &contrnode;. &o_meter; collects approximately 200 bytes of data per hour
      and &vmguest;. Unless you have a very huge number of &vmguest;s, there is
      no need to install it on a dedicated node.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><guimenu>ceilometer-polling</guimenu>
    </term>
    <listitem>
     <para>
      The polling agent listens to the message bus to collect data. It needs to
      be deployed on a &contrnode;. It can be deployed on the same node as
      <guimenu>ceilometer-server</guimenu>.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><guimenu>ceilometer-agent</guimenu>
    </term>
    <listitem>
     <para>
      The compute agents collect data from the compute nodes. They need to be
      deployed on all &kvm; and &xen; compute nodes in your cloud (other
      hypervisors are currently not supported).
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><guimenu>ceilometer-swift-proxy-middleware</guimenu>
    </term>
    <listitem>
     <para>
      An agent collecting data from the &swift; nodes. This role needs to be
      deployed on the same node as swift-proxy.
     </para>
    </listitem>
   </varlistentry>
  </variablelist>

  <figure>
   <title>The &o_meter; &Barcl;: Node Deployment</title>
   <mediaobject>
    <imageobject role="fo">
     <imagedata fileref="depl_barclamp_ceilometer_deployment.png" width="100%" format="png"/>
    </imageobject>
    <imageobject role="html">
     <imagedata fileref="depl_barclamp_ceilometer_deployment.png" width="75%" format="png"/>
    </imageobject>
   </mediaobject>
  </figure>

  <sect2 xml:id="sec.depl.ostack.ceilometer.ha">
   <title>&haSetup; for &o_meter;</title>
   <para>
    Making &o_meter; highly available requires no special
    configuration&mdash;it is sufficient to deploy the roles
    <guimenu>ceilometer-server</guimenu> and
    <guimenu>ceilometer-polling</guimenu> on a cluster. If you are using MySQL
    or PostgreSQL, you can use two nodes.
   </para>
  </sect2>
 </sect1>

 <sect1 xml:id="sec.depl.ostack.manila">
  <title>Deploying &o_sharefs;</title>

  <para>
   &o_sharefs; provides coordinated access to shared or distributed file
   systems, similar to what &o_blockstore; does for block storage. These file
   systems can be shared between &vmguest;s in &cloud;.
  </para>

  <para>
   &o_sharefs; uses different back-ends. As of &productname; 8 currently
   supported back-ends include <guimenu>Hitachi HNAS</guimenu>, <guimenu>NetApp
   Driver</guimenu>, and <guimenu>CephFS</guimenu>. Two more back-end options,
   <guimenu>Generic Driver</guimenu> and <guimenu>Other Driver</guimenu> are
   available for testing purposes and are not supported.
  </para>

  <note xml:id="note.limit.cephfs">
   <title>Limitations for CephFS Back-end</title>
   <para>
    &o_sharefs; uses some CephFS features that are currently
    <emphasis>not</emphasis> supported by the &cloudos; CephFS kernel
    client:
   </para>
   <itemizedlist>
    <listitem>
     <para>
      RADOS namespaces
     </para>
    </listitem>
    <listitem>
     <para>
      MDS path restrictions
     </para>
    </listitem>
    <listitem>
     <para>
      Quotas
     </para>
    </listitem>
   </itemizedlist>
   <para>
    As a result, to access CephFS shares provisioned by &o_sharefs;, you must
    use ceph-fuse. For details, see
    <link
    xlink:href="http://docs.openstack.org/developer/manila/devref/cephfs_native_driver.html"/>.
   </para>
  </note>

  <para>
   When first opening the &o_sharefs; &barcl;, the default proposal
   <guimenu>Generic Driver</guimenu> is already available for configuration. To
   replace it, first delete it by clicking the trashcan icon and then choose a
   different back-end in the section <guimenu>Add new Manila Backend</guimenu>.
   Select a <guimenu>Type of Share</guimenu> and&mdash;optionally&mdash;provide
   a <guimenu>Name for Backend</guimenu>. Activate the back-end with
   <guimenu>Add Backend</guimenu>. Note that at least one back-end must be
   configured.
  </para>

  <para>
   The attributes that can be set to configure Cinder depend on the back-end:
  </para>

  <bridgehead renderas="sect2"><guimenu>Back-end: Generic</guimenu>
  </bridgehead>

  <para>
   The generic driver is included as a technology preview and is not supported.
  </para>

  <bridgehead renderas="sect2"><guimenu>Hitachi HNAS</guimenu>
  </bridgehead>

  <variablelist>
   <varlistentry>
    <term><guimenu>Specify which EVS this backend is assigned to</guimenu>
    </term>
    <listitem>
     <para>
      Provide the name of the Enterprise Virtual Server that the selected
      back-end is assigned to.
      <remark condition="accuracy">2017-01-03 - dpopov: DEVS FIXME Please check whether this is correct.</remark>
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><guimenu>Specify IP for mounting shares</guimenu>
    </term>
    <listitem>
     <para>
      IP address for mounting shares.
      <remark condition="info">2017-01-03 - dpopov: DEVS FIXME More info needed.</remark>
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><guimenu>Specify file-system name for creating shares</guimenu>
    </term>
    <listitem>
     <para>
      Provide a file-system name for creating shares.
      <remark condition="info">2017-01-03 - dpopov: DEVS FIXME More info needed.</remark>
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><guimenu>HNAS management interface IP</guimenu>
    </term>
    <listitem>
     <para>
      IP address of the HNAS management interface for communication between
      Manila controller and HNAS.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><guimenu>HNAS username Base64 String</guimenu>
    </term>
    <listitem>
     <para>
      HNAS username Base64 String required to perform tasks like creating
      file-systems and network interfaces.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><guimenu>HNAS user password</guimenu>
    </term>
    <listitem>
     <para>
      HNAS user password. Required only if private key is not provided.
      <remark condition="accuracy">2017-01-03 - dpopov: DEVS FIXME Please check whether this is correct.</remark>
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><guimenu>RSA/DSA private key</guimenu>
    </term>
    <listitem>
     <para>
      RSA/DSA private key necessary for connecting to HNAS. Required only if
      password is not provided.
      <remark condition="accuracy">2017-01-03 - dpopov: DEVS FIXME Please check whether this is correct.</remark>
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><guimenu>The time to wait for stalled HNAS jobs before aborting</guimenu>
    </term>
    <listitem>
     <para>
      Time in seconds to wait before aborting stalled HNAS jobs.
     </para>
    </listitem>
   </varlistentry>
  </variablelist>

  <bridgehead renderas="sect2"><guimenu>Back-end: Netapp</guimenu>
  </bridgehead>

  <variablelist>
   <varlistentry>
    <term><guimenu>Name of the Virtual Storage Server (vserver)</guimenu>
    </term>
    <listitem>
     <para>
      Host name of the Virtual Storage Server.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><guimenu>Server Host Name</guimenu>
    </term>
    <listitem>
     <para>
      The name or IP address for the storage controller or the cluster.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><guimenu>Server Port</guimenu>
    </term>
    <listitem>
     <para>
      The port to use for communication. Port 80 is usually used for HTTP, 443
      for HTTPS.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><guimenu>User name/Password for Accessing NetApp</guimenu>
    </term>
    <listitem>
     <para>
      Login credentials.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><guimenu>Transport Type</guimenu>
    </term>
    <listitem>
     <para>
      Transport protocol for communicating with the storage controller or
      cluster. Supported protocols are HTTP and HTTPS. Choose the protocol your
      NetApp is licensed for.
     </para>
    </listitem>
   </varlistentry>
  </variablelist>

  <bridgehead renderas="sect2"><guimenu>Back-end: CephFS</guimenu>
  </bridgehead>

  <variablelist>
   <varlistentry>
    <term>Use Ceph deployed by Crowbar</term>
    <listitem>
     <para>
      Set to <systemitem>true</systemitem> to use Ceph deployed with Crowbar.
     </para>
    </listitem>
   </varlistentry>
  </variablelist>

  <bridgehead renderas="sect2"><guimenu>Back-end: Manual</guimenu>
  </bridgehead>

  <para>
   Lets you manually pick and configure a driver. Only use this option for
   testing purposes, it is not supported.
  </para>

  <figure>
   <title>The &o_sharefs; &Barcl;</title>
   <mediaobject>
    <imageobject role="fo">
     <imagedata fileref="depl_barclamp_manila.png" width="100%" format="png"/>
    </imageobject>
    <imageobject role="html">
     <imagedata fileref="depl_barclamp_manila.png" width="75%" format="png"/>
    </imageobject>
   </mediaobject>
  </figure>

  <para>
   The &o_sharefs; component consists of two different roles:
  </para>

  <variablelist>
   <varlistentry>
    <term><guimenu>manila-server</guimenu>
    </term>
    <listitem>
     <para>
      The &o_sharefs; server provides the scheduler and the API. Installing it
      on a &contrnode; is recommended.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><guimenu>manila-share</guimenu>
    </term>
    <listitem>
     <para>
      The shared storage service. It can be installed on a &contrnode;, but it
      is recommended to deploy it on one or more dedicated nodes supplied with
      sufficient disk space and networking capacity, since it will generate a
      lot of network traffic.
     </para>
    </listitem>
   </varlistentry>
  </variablelist>

  <figure>
   <title>The &o_sharefs; &Barcl;: Node Deployment Example</title>
   <mediaobject>
    <imageobject role="fo">
     <imagedata fileref="depl_barclamp_manila_node_deployment.png" width="100%" format="png"/>
    </imageobject>
    <imageobject role="html">
     <imagedata fileref="depl_barclamp_manila_node_deployment.png" width="75%" format="png"/>
    </imageobject>
   </mediaobject>
  </figure>

  <sect2 xml:id="sec.depl.ostack.manila.ha">
   <title>&haSetup; for &o_sharefs;</title>
   <para>
    While the <guimenu>manila-server</guimenu> role can be deployed on a
    cluster, deploying <guimenu>manila-share</guimenu> on a cluster is not
    supported. Therefore it is generally recommended to deploy
    <guimenu>manila-share</guimenu> on several nodes&mdash;this ensures the
    service continues to be available even when a node fails.
   </para>
  </sect2>
 </sect1>

 <sect1 xml:id="sec.depl.ostack.tempest">
  <title>Deploying &o_testsuite; (Optional)</title>

  <para>
   &o_testsuite; is an integration test suite for &cloud; written in Python. It
   contains multiple integration tests for validating your &cloud; deployment.
   For more information about &o_testsuite; refer to the &ostack; documentation
   at <link
   xlink:href="http://docs.openstack.org/developer/tempest/"/>.
  </para>

  <important>
   <title>Technology Preview</title>
   <para>
    &o_testsuite; is only included as a technology preview and not supported.
   </para>
   <para>
    &o_testsuite; may be used for testing whether the intended setup will run
    without problems. It should not be used in a production environment.
   </para>
  </important>

  <para>
   &o_testsuite; should be deployed on a &contrnode;.
  </para>

  <para>
   The following attributes can be configured for &o_testsuite;:
  </para>

  <variablelist>
   <varlistentry>
    <term><guimenu>Choose User name / Password</guimenu>
    </term>
    <listitem>
     <para>
      Credentials for a regular user. If the user does not exist, it will be
      created.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><guimenu>Choose Tenant</guimenu>
    </term>
    <listitem>
     <para>
      Tenant to be used by &o_testsuite;. If it does not exist, it will be
      created. It is safe to stick with the default value.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><guimenu>Choose Tempest Admin User name/Password</guimenu>
    </term>
    <listitem>
     <para>
      Credentials for an admin user. If the user does not exist, it will be
      created.
     </para>
    </listitem>
   </varlistentry>
  </variablelist>

  <figure>
   <title>The &o_testsuite; &Barcl;</title>
   <mediaobject>
    <imageobject role="fo">
     <imagedata fileref="depl_barclamp_tempest.png" width="100%" format="png"/>
    </imageobject>
    <imageobject role="html">
     <imagedata fileref="depl_barclamp_tempest.png" width="75%" format="png"/>
    </imageobject>
   </mediaobject>
  </figure>

  <tip>
   <title>Running Tests</title>
   <para>
    To run tests with &o_testsuite;, log in to the &contrnode; on which
    &o_testsuite; was deployed. Change into the directory
    <filename>/var/lib/openstack-tempest-test</filename>. To get an overview of
    available commands, run:
   </para>
<screen>./tempest --help</screen>
   <para>
    To serially invoke a subset of all tests (<quote>the gating
    smoketests</quote>) to help validate the working functionality of your
    local cloud instance, run the following command. It will save the output to
    a log file
    <filename>tempest_<replaceable>CURRENT_DATE</replaceable>.log</filename>.
   </para>
<screen>./tempest run --smoke --serial 2>&amp;1 \
| tee "tempest_$(date +%Y-%m-%d_%H%M%S).log"</screen>
  </tip>

  <sect2 xml:id="sec.depl.ostack.tempest.ha">
   <title>&haSetup; for &o_testsuite;</title>
   <para>
    &o_testsuite; cannot be made highly available.
   </para>
  </sect2>
 </sect1>
 <sect1 xml:id="sec.depl.ostack.magnum">
  <title>Deploying &o_container; (Optional)</title>

  <para>
   &o_container; is an &ostack; project which offers container orchestration
   engines for deploying and managing containers as first class resources in
   &ostack;.
  </para>

  <para>
   For more information about &o_container;, see the &ostack; documentation at
   <link xlink:href="http://docs.openstack.org/developer/magnum/"/>.
  </para>

  <para>
   For information on how to deploy a Kubernetes cluster (either from command
   line or from the &o_dash; &dash;), see the &cloudsuppl;. It is available
   from <link xlink:href="https://www.suse.com/documentation/cloud"></link>.
  </para>

  <para>
   The following <guimenu>Attributes</guimenu> can be configured for
   &o_container;:
  </para>

  <variablelist>
   <varlistentry>
    <term><guimenu>Trustee Domain</guimenu>: <guimenu>Delegate trust to
    cluster users if required</guimenu></term>
    <listitem>
     <para>
      Deploying Kubernetes clusters in a cloud without an Internet connection
      (as described in <link xlink:href="https://www.suse.com/documentation/suse-openstack-cloud-7/singlehtml/book_cloud_suppl/book_cloud_suppl.html#sec.deploy.kubernetes.without">Deploying a Kubernetes Cluster Without Internet Access</link>))
      requires the <literal>registry_enabled</literal> option in its cluster
      template set to <literal>true</literal>. To make this offline scenario
      work, you also need to set the <guimenu>Delegate trust to cluster users if
      required</guimenu> option to <literal>true</literal>. This restores the old,
      insecure behavior for clusters with the
      <literal>registry-enabled</literal> or <literal>volume_driver=Rexray</literal> options enabled.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><guimenu>Trustee Domain</guimenu>: <guimenu>Domain Name</guimenu></term>
    <listitem>
     <para>
      Domain name to use for creating trustee for bays.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><guimenu>Logging</guimenu>: <guimenu>Verbose</guimenu></term>
    <listitem>
     <para>
      Increases the amount of information that is written to the log files when
      set to <guimenu>true</guimenu>.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><guimenu>Logging</guimenu>: <guimenu>Debug</guimenu></term>
    <listitem>
     <para>
      Shows debugging output in the log files when set to <guimenu>true</guimenu>.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><guimenu>Certificate Manager</guimenu>: <guimenu>Plugin</guimenu>
    </term>
    <listitem>
     <para>
      To store certificates, either use the <guimenu>Barbican</guimenu>
      &ostack; service, a local directory (<guimenu>Local</guimenu>), or the
      <guimenu>Magnum Database (x590keypair)</guimenu>.
     </para>
     <note>
      <title>&secret_store; As Certificate Manager</title>
      <para>
       If you choose to use &secret_store; for managing certificates, make sure
       that the &secret_store; &barcl; is enabled.
      </para>
     </note>
    </listitem>
   </varlistentry>
  </variablelist>

  <figure>
   <title>The &o_container; &Barcl;</title>
   <mediaobject>
    <imageobject role="fo">
     <imagedata fileref="depl_barclamp_magnum_attributes.png" width="100%" format="png"/>
    </imageobject>
    <imageobject role="html">
     <imagedata fileref="depl_barclamp_magnum_attributes.png" width="75%" format="png"/>
    </imageobject>
   </mediaobject>
  </figure>

  <para>
   The &o_container; &barcl; consists of the following roles:
   <guimenu>magnum-server</guimenu>. It can either be deployed on a &contrnode;
   or on a cluster&mdash;see <xref linkend="sec.depl.ostack.magnum.ha"/>. When
   deploying the role onto a &contrnode;, additional RAM is required for the
   &o_container; server. It is recommended to only deploy the role to a
   &contrnode; that has 16 GB RAM.
  </para>

  <sect2 xml:id="sec.depl.ostack.magnum.ha">
   <title>&haSetup; for &o_container;</title>
   <para>
    Making &o_container; highly available requires no special configuration. It
    is sufficient to deploy it on a cluster.
   </para>
  </sect2>
 </sect1>
 <sect1 xml:id="sec.depl.ostack.barbican">
  <title>Deploying &secret_store; (Optional)</title>

  <para>
   &secret_store; is a component designed for storing secrets in a secure and
   standardized manner protected by &o_ident; authentication. Secrets include
   SSL certificates and passwords used by various &ostack; components.
  </para>

  <para>
   &secret_store; settings can be configured in <literal>Raw</literal> mode
   only. To do this, open the &secret_store; &barcl; <guimenu>Attribute
   </guimenu>configuration in <guimenu>Raw</guimenu> mode.
  </para>

  <figure>
   <title>The &secret_store; &Barcl;: Raw Mode</title>
   <mediaobject>
    <imageobject role="fo">
     <imagedata fileref="depl_barclamp_barbican_raw.png" width="100%" format="png"/>
    </imageobject>
    <imageobject role="html">
     <imagedata fileref="depl_barclamp_barbican_raw.png" width="75%" format="png"/>
    </imageobject>
   </mediaobject>
  </figure>

  <para>
   When configuring &secret_store;, pay particular attention to the following
   settings:
  </para>

  <itemizedlist>
   <listitem>
    <para>
     <literal>bind_host</literal> Bind host for the &secret_store; API service
    </para>
   </listitem>
   <listitem>
    <para>
     <literal>bind_port</literal> Bind port for the &secret_store; API service
    </para>
   </listitem>
   <listitem>
    <para>
     <literal>processes</literal> Number of API processes to run in Apache
    </para>
   </listitem>
   <listitem>
    <para>
     <literal>ssl</literal> Enable or disable SSL
    </para>
   </listitem>
   <listitem>
    <para>
     <literal>threads</literal> Number of API worker threads
    </para>
   </listitem>
   <listitem>
    <para>
     <literal>debug</literal> Enable or disable debug logging
    </para>
   </listitem>
   <listitem>
    <para>
     <literal>enable_keystone_listener</literal> Enable or disable the
     &o_ident; listener services
    </para>
   </listitem>
   <listitem>
    <para>
     <literal>kek</literal> An encryption key (fixed-length 32-byte
     Base64-encoded value) for &secret_store;'s
     <systemitem>simple_crypto</systemitem> plugin. If left unspecified, the
     key will be generated automatically.
    </para>
    <note>
     <title>Existing Encryption Key</title>
     <para>
      If you plan to restore and use the existing &secret_store; database after
      a full reinstall (including a complete wipe of the Crowbar node), make
      sure to save the specified encryption key beforehand. You will need to
      provide it after the full reinstall in order to access the data in the
      restored &secret_store; database.
     </para>
    </note>
   </listitem>
  </itemizedlist>

  <variablelist>
   <varlistentry xml:id="sec.depl.ostack.barbican.ssl">
    <term>SSL Support: Protocol
    </term>
    <listitem>
     <para>
      With the default value <guimenu>HTTP</guimenu>, public communication will
      not be encrypted. Choose <guimenu>HTTPS</guimenu> to use SSL for
      encryption. See <xref linkend="sec.depl.req.ssl"/> for background
      information and <xref linkend="sec.depl.inst.nodes.post.ssl"/> for
      installation instructions. The following additional configuration options
      will become available when choosing <guimenu>HTTPS</guimenu>:
     </para>
     <variablelist>
      <varlistentry>
       <term><guimenu>Generate (self-signed) certificates</guimenu>
       </term>
       <listitem>
        <para>
         When set to <literal>true</literal>, self-signed certificates are
         automatically generated and copied to the correct locations. This
         setting is for testing purposes only and should never be used in
         production environments!
        </para>
       </listitem>
      </varlistentry>
      <varlistentry>
       <term><guimenu>SSL Certificate File</guimenu> / <guimenu>SSL (Private) Key
        File</guimenu>
       </term>
       <listitem>
        <para>
         Location of the certificate key pair files.
        </para>
       </listitem>
      </varlistentry>
      <varlistentry>
       <term><guimenu>SSL Certificate is insecure</guimenu>
       </term>
       <listitem>
        <para>
         Set this option to <literal>true</literal> when using self-signed
         certificates to disable certificate checks. This setting is for
         testing purposes only and should never be used in production
         environments!
        </para>
       </listitem>
      </varlistentry>
      <varlistentry>
       <term><guimenu>SSL CA Certificates File</guimenu>
       </term>
       <listitem>
        <para>
         Specify the absolute path to the CA certificate. This field is
         mandatory, and leaving it blank will cause the &barcl; to fail. To fix
         this issue, you have to provide the absolute path to the CA
         certificate, restart the <systemitem>apache2</systemitem> service, and
         re-deploy the &barcl;.
        </para>
        <para>
         When the certificate is not already trusted by the pre-installed list
         of trusted root certificate authorities, you need to provide a
         certificate bundle that includes the root and all intermediate CAs.
        </para>
        <figure>
         <title>The SSL Dialog</title>
         <mediaobject>
          <imageobject role="fo">
           <imagedata fileref="depl_barbican_ssl.png" width="100%" format="png"/>
          </imageobject>
          <imageobject role="html">
           <imagedata fileref="depl_barbican_ssl.png" width="75%" format="png"/>
          </imageobject>
         </mediaobject>
        </figure>
       </listitem>
      </varlistentry>
     </variablelist>
    </listitem>
   </varlistentry>
  </variablelist>

  <sect2 xml:id="sec.depl.ostack.barbican.ha">
   <title>&haSetup; for &secret_store;</title>
   <para>
    To make &secret_store; highly available, assign the
    <guimenu>barbican-controller</guimenu> role to the Controller Cluster.
   </para>
  </sect2>
 </sect1>
 <sect1 xml:id="sec.depl.ostack.sahara">
  <title>Deploying &dataproc;</title>

  <para>
   &dataproc; provides users with simple means to provision data processing
   frameworks (such as Hadoop, Spark, and Storm) on &ostack;. This is
   accomplished by specifying configuration parameters such as the framework
   version, cluster topology, node hardware details, etc.
  </para>

  <variablelist>
   <varlistentry>
    <term>Logging: Verbose</term>
    <listitem>
     <para>
      Set to <systemitem>true</systemitem> to increase the amount of
      information written to the log files.
     </para>
    </listitem>
   </varlistentry>
  </variablelist>

  <figure>
   <title>The &dataproc; Barclamp</title>
   <mediaobject>
    <imageobject role="fo">
     <imagedata fileref="depl_barclamp_sahara.png" width="100%" format="png"/>
    </imageobject>
    <imageobject role="html">
     <imagedata fileref="depl_barclamp_sahara.png" width="75%" format="png"/>
    </imageobject>
   </mediaobject>
  </figure>

  <sect2 xml:id="sec.depl.ostack.sahara.ha">
   <title>&haSetup; for &dataproc;</title>
   <para>
    Making &dataproc; highly available requires no special configuration. It is
    sufficient to deploy it on a cluster.
   </para>
  </sect2>
 </sect1>
 <sect1 xml:id="sec.depl.ostack.monasca">
  <title>Deploying &o_monitor;</title>

  <para>
   &o_monitor; is an open-source monitoring-as-a-service solution that
   integrates with &ostack;. &o_monitor; is designed for scalability, high
   performance, and fault tolerance.
  </para>

  <para>
   Accessing the <guimenu>Raw</guimenu> interface is not required for
   day-to-day operation. But as not all &o_monitor; settings are exposed in the
   &barcl; graphical interface (for example, various performance tuneables), it
   is recommended to configure &o_monitor; in the <guimenu>Raw</guimenu>
   mode. Below are the options that can be configured via the
   <guimenu>Raw</guimenu> interface of the &o_monitor; &barcl;.
  </para>

  <figure>
   <title>The &o_monitor; &barcl; Raw Mode</title>
   <mediaobject>
    <imageobject role="fo">
     <imagedata fileref="depl_barclamp_monasca_raw.png" width="100%" format="png"/>
    </imageobject>
    <imageobject role="html">
     <imagedata fileref="depl_barclamp_monasca_raw.png" width="75%" format="png"/>
    </imageobject>
   </mediaobject>
  </figure>

  <bridgehead renderas="sect3"><guimenu>agent: settings for openstack-monasca-agent</guimenu>
  </bridgehead>

  <variablelist>
   <varlistentry>
    <term>keystone</term>
    <listitem>
     <para>
      Contains &o_ident; credentials that the agents use to send metrics. Do
      not change these options, as they are configured by &crow;.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>insecure</term>
    <listitem>
     <para>
      Specifies whether SSL certificates are verified when communicating with
      &o_ident;. If set to <literal>false</literal>, the
      <literal>ca_file</literal> option must be specified.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>ca_file</term>
    <listitem>
     <para>
      Specifies the location of a CA certificate that is used for verifying
      &o_ident;'s SSL certificate.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>log_dir</term>
    <listitem>
     <para>
      Path for storing log files. The specified path must exist. Do not change
      the default <filename>/var/log/monasca-agent</filename> path.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>log_level</term>
    <listitem>
     <para>
      Agent's log level. Limits log messages to the specified level and above.
      The following levels are available: Error, Warning, Info (default), and
      Debug.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>check_frequency</term>
    <listitem>
     <para>
      Interval in seconds between running agents' checks.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>num_collector_threads</term>
    <listitem>
     <para>
      Number of simultaneous collector threads to run. This refers to the
      maximum number of different collector plug-ins (for example,
      <literal>http_check</literal>) that are allowed to run simultaneously. The
      default value <literal>1</literal> means that plug-ins are run
      sequentially.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>pool_full_max_retries</term>
    <listitem>
     <para>
      If a problem with the results from multiple plug-ins results blocks the
      entire thread pool (as specified by the
      <systemitem>num_collector_threads</systemitem> parameter), the collector
      exits, so it can be restarted by the
      <systemitem>supervisord</systemitem>. The parameter
      <systemitem>pool_full_max_retries</systemitem> specifies when this event
      occurs. The collector exits when the defined number of consecutive
      collection cycles have ended with the thread pool completely full.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>plugin_collect_time_warn</term>
    <listitem>
     <para>
      Upper limit in seconds for any collection plug-in's run time. A warning
      is logged if a plug-in runs longer than the specified limit.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>max_measurement_buffer_size</term>
    <listitem>
     <para>
      Maximum number of measurements to buffer locally if the &o_monitor; API
      is unreachable. Measurements will be dropped in batches, if the API is
      still unreachable after the specified number of messages are buffered.
      The default <literal>-1</literal> value indicates unlimited buffering.
      Note that a large buffer increases the agent's memory usage.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>backlog_send_rate</term>
    <listitem>
     <para>
      Maximum number of measurements to send when the local measurement buffer
      is flushed.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>amplifier</term>
    <listitem>
     <para>
      Number of extra dimensions to add to metrics sent to the &o_monitor; API.
      This option is intended for load testing purposes only. Do not enable the
      option in production! The default <literal>0</literal> value disables the
      addition of dimensions.
     </para>
    </listitem>
   </varlistentry>
  </variablelist>

  <bridgehead renderas="sect3"><guimenu>log_agent: settings for openstack-monasca-log-agent</guimenu>
  </bridgehead>

  <variablelist>
   <varlistentry>
    <term>max_data_size_kb</term>
    <listitem>
     <para>
      Maximum payload size in kilobytes for a request sent to the &o_monitor;
      log API.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>num_of_logs</term>
    <listitem>
     <para>
      Maximum number of log entries the log agent sends to the &o_monitor; log
      API in a single request. Reducing the number increases performance.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>elapsed_time_sec</term>
    <listitem>
     <para>
      Time interval in seconds between sending logs to the &o_monitor; log API.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>delay</term>
    <listitem>
     <para>
      Interval in seconds for checking whether
      <literal>elapsed_time_sec</literal> has been reached.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>keystone</term>
    <listitem>
     <para>
      &o_ident; credentials the log agents use to send logs to the &o_monitor;
      log API. Do not change this option manually, as it is configured by
      &crow;.
     </para>
    </listitem>
   </varlistentry>
  </variablelist>

  <bridgehead renderas="sect3"><guimenu>api: Settings for openstack-monasca-api</guimenu>
  </bridgehead>

  <variablelist>
   <varlistentry>
    <term>bind_host</term>
    <listitem>
     <para>
      Interfaces <literal>monasca-api</literal> listens on. Do not change this
      option, as it is configured by &crow;.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>processes</term>
    <listitem>
     <para>
      Number of processes to spawn.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>threads</term>
    <listitem>
     <para>
      Number of WSGI worker threads to spawn.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>log_level</term>
    <listitem>
     <para>
      Log level for <systemitem>openstack-monasca-api</systemitem>. Limits log
      messages to the specified level and above. The following levels are
      available: Critical, Error, Warning, Info (default), Debug, and Trace.
     </para>
    </listitem>
   </varlistentry>
  </variablelist>

  <bridgehead renderas="sect3"><guimenu>elasticsearch: server-side settings for elasticsearch</guimenu>
  </bridgehead>

  <variablelist>
   <varlistentry>
    <term>repo_dir</term>
    <listitem>
     <para>
      List of directories for storing &elasticsearch; snapshots. Must be created
      manually and be writeable by the
      <systemitem
          class="username">elasticsearch</systemitem> user.
      Must contain at least one entry in order for the snapshot functionality
      to work.
     </para>
     <variablelist>
      <varlistentry>
       <term>heap_size</term>
       <listitem>
        <para>
         Sets the heap size. We recommend setting heap size at 50% of the
         available memory, but not more than 31 GB. The default of 4 GB is
         likely too small and should be increased if possible.
        </para>
       </listitem>
      </varlistentry>
      <varlistentry>
       <term>limit_memlock</term>
       <listitem>
        <para>
         The maximum size that may be locked into memory in bytes
        </para>
       </listitem>
      </varlistentry>
      <varlistentry>
       <term>limit_nofile</term>
       <listitem>
        <para>
         The maximum number of open file descriptors
        </para>
       </listitem>
      </varlistentry>
      <varlistentry>
       <term>limit_nproc</term>
       <listitem>
        <para>
         The maximum number of processes
        </para>
       </listitem>
      </varlistentry>
      <varlistentry>
       <term>vm_max_map_count</term>
       <listitem>
        <para>
         The maximum number of memory map areas a process may have.
        </para>
       </listitem>
      </varlistentry>
     </variablelist>
    </listitem>
   </varlistentry>
  </variablelist>
  <para>
   For instructions on creating an &elasticsearch; snapshot, see
   <xref linkend="backup-recovery"/>.
  </para>

  <bridgehead renderas="sect3"><guimenu>elasticsearch_curator: settings for
    elastisearch-curator</guimenu>
  </bridgehead>

  <para>
   <systemitem>elasticsearch-curator</systemitem> removes old and large
   elasticsearch indices. The settings below determine its behavior.
  </para>

  <variablelist>
   <varlistentry>
    <term>delete_after_days</term>
    <listitem>
     <para>
      Time threshold for deleting indices. Indices older the specified number
      of days are deleted. This parameter is unset by default, so indices are
      kept indefinitely.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>delete_after_size</term>
    <listitem>
     <para>
      Maximum size in megabytes of indices. Indices larger than the specified
      size are deleted. This parameter is unset by default, so indices are kept
      irrespective of their size.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>delete_exclude_index</term>
    <listitem>
     <para>
      List of indices to exclude from
      <systemitem>elasticsearch-curator</systemitem> runs. By default, only the
      <filename>.kibana</filename> files are excluded.
     </para>
    </listitem>
   </varlistentry>
  </variablelist>

  <bridgehead renderas="sect3"><guimenu>kafka: tunables for
Kafka</guimenu>
  </bridgehead>

  <variablelist>
   <varlistentry>
    <term>log_retention_hours</term>
    <listitem>
     <para>
      Number of hours for retaining log segments in Kafka's on-disk log.
      Messages older than the specified value are dropped.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>log_retention_bytes</term>
    <listitem>
     <para>
      Maximum size for Kafka's on-disk log in bytes. If the log grows beyond
      this size, the oldest log segments are dropped.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>topics</term>
    <listitem>
     <para>
      list of topics
     </para>
     <itemizedlist>
      <listitem>
       <para>
        metrics
       </para>
      </listitem>
      <listitem>
       <para>
        events
       </para>
      </listitem>
      <listitem>
       <para>
        alarm-state-transitions
       </para>
      </listitem>
      <listitem>
       <para>
        alarm-notifications
       </para>
      </listitem>
      <listitem>
       <para>
        retry-notifications
       </para>
      </listitem>
      <listitem>
       <para>
        60-seconds-notifications
       </para>
      </listitem>
      <listitem>
       <para>
        log
       </para>
      </listitem>
      <listitem>
       <para>
        transformed-log
       </para>
      </listitem>
     </itemizedlist>
     <para>
      The following are options of every topic:
     </para>
     <variablelist>
      <varlistentry>
       <term>replicas</term>
       <listitem>
        <para>
         Controls how many servers replicate each message that is written
        </para>
       </listitem>
      </varlistentry>
      <varlistentry>
       <term>partitions</term>
       <listitem>
        <para>
         Controls how many logs the topic is sharded into
        </para>
       </listitem>
      </varlistentry>
      <varlistentry>
       <term>config_options</term>
       <listitem>
        <para>
         Map of configuration options is described in the <link
         xlink:href="https://kafka.apache.org/documentation/#topicconfigs">Apache
         Kafka documentation</link>
        </para>
       </listitem>
      </varlistentry>
     </variablelist>
    </listitem>
   </varlistentry>
  </variablelist>
  <para>
   These parameters only affect first time installations. Parameters may be
   changed after installation with scripts available from <link xlink:href="https://kafka.apache.org/documentation/#basic_ops">Apache Kafka</link>.
  </para>
  <para>
   Kafka does not support reducing the number of partitions for a topic.
  </para>

  <bridgehead renderas="sect3"><guimenu>notification:</guimenu>
  </bridgehead>

  <variablelist>
   <varlistentry>
    <term>email_enabled</term>
    <listitem>
     <para>
      Enable or disable email alarm notifications.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>email_smtp_host</term>
    <listitem>
     <para>
      SMTP smarthost for sending alarm notifications.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>email_smtp_port</term>
    <listitem>
     <para>
      Port for the SMTP smarthost.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>email_smtp_user</term>
    <listitem>
     <para>
      User name for authenticating against the smarthost.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>email_smtp_password</term>
    <listitem>
     <para>
      Password for authenticating against the smarthost.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>email_smtp_from_address</term>
    <listitem>
     <para>
      Sender address for alarm notifications.
     </para>
    </listitem>
   </varlistentry>
  </variablelist>

  <bridgehead renderas="sect3"><guimenu>master: configuration for
monasca-installer on the &crow; node</guimenu>
  </bridgehead>

  <variablelist>
   <varlistentry>
    <term>influxdb_retention_policy</term>
    <listitem>
     <para>
      Number of days to keep metrics records in influxdb.
     </para>
     <para>
      For an overview of all supported values, see
      <link
         xlink:href="https://docs.influxdata.com/influxdb/v1.1/query_language/database_management/#create-retention-policies-with-create-retention-policy"></link>.
     </para>
<!--taroth/dpopov 2017-05-15: as the installation chapter has been removed
       from both SOC Monitoring Operator Guides, there's no internal information
       we can point to for now, that's why we need to point to the URl above
      for now-->
    </listitem>
   </varlistentry>
  </variablelist>

<bridgehead renderas="sect3"><guimenu>monasca: settings for libvirt and &ceph;
monitoring</guimenu>
  </bridgehead>
  <variablelist>
   <varlistentry>
    <term>
     monitor_libvirt
    </term>
    <listitem>
     <para>
      The global switch for toggling libvirt monitoring. If set to
      <parameter>true</parameter>, libvirt metrics will be gathered on all
      libvirt based &compnode;s. This setting is available in the &crow; UI.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>
     monitor_ceph
    </term>
    <listitem>
     <para>
      The global switch for toggling &ceph; monitoring. If set to
      <parameter>true</parameter>, &ceph; metrics will be gathered on all
      &ceph;-based &compnode;s. This setting is available in &crow; UI. If the
      &ceph; cluster has been set up independently, &crow; ignores this setting.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>
     cache_dir
    </term>
    <listitem>
     <para>
      The directory where monasca-agent will locally cache various metadata
      about locally running VMs on each &compnode;.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>
     customer_metadata
    </term>
    <listitem>
     <para>
      Specifies the list of instance metadata keys to be included as dimensions
      with customer metrics. This is useful for providing more information
      about an instance.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>
     disk_collection_period
    </term>
    <listitem>
     <para>
      Specifies a minimum interval in seconds for collecting disk metrics.
      Increase this value to reduce I/O load. If the value is lower than the
      global agent collection period (<option>check_frequency</option>), it
      will be ignored in favor of the global collection period.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>
     max_ping_concurrency
    </term>
    <listitem>
     <para>
      Specifies the number of ping command processes to run concurrently when
      determining whether the VM is reachable. This should be set to a value
      that allows the plugin to finish within the agent's collection period,
      even if there is a networking issue. For example, if the expected number
      of VMs per &compnode; is 40 and each VM has one IP address, then the
      plugin will take at least 40 seconds to do the ping checks in the
      worst-case scenario where all pings fail (assuming the default timeout of
      1 second). Increasing <option>max_ping_concurrency</option> allows the
      plugin to finish faster.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>
     metadata
    </term>
    <listitem>
     <para>
      Specifies the list of &o_comp; side instance metadata keys to be included
      as dimensions with the cross-tenant metrics for the
      <guimenu>monasca</guimenu> project. This is useful for providing more
      information about an instance.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>
     nova_refresh
    </term>
    <listitem>
     <para>
      Specifies the number of seconds between calls to the &o_comp; API to
      refresh the instance cache. This is helpful for updating VM hostname and
      pruning deleted instances from the cache. By default, it is set to 14,400
      seconds (four hours). Set to 0 to refresh every time the Collector runs,
      or to <parameter>None</parameter> to disable regular refreshes entirely.
      In this case, the instance cache will only be refreshed when a new
      instance is detected.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>
     ping_check
    </term>
    <listitem>
     <para>
      Includes the entire ping command (without the IP address, which is
      automatically appended) to perform a ping check against instances. The
      <literal>NAMESPACE</literal> keyword is automatically replaced with the
      appropriate network namespace for the VM being monitored. Set to
      <parameter>False</parameter> to disable ping checks.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>
     vnic_collection_period
    </term>
    <listitem>
     <para>
      Specifies a minimum interval in seconds for collecting disk metrics.
      Increase this value to reduce I/O load. If the value is lower than the
      global agent collection period (<option>check_frequency</option>), it
      will be ignored in favor of the global collection period.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>
     vm_cpu_check_enable
    </term>
    <listitem>
     <para>
      Toggles the collection of VM CPU metrics. Set to
      <parameter>true</parameter> to enable.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>
     vm_disks_check_enable
    </term>
    <listitem>
     <para>
      Toggles the collection of VM disk metrics. Set to
      <parameter>true</parameter> to enable.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>
     vm_extended_disks_check_enable
    </term>
    <listitem>
     <para>
      Toggles the collection of extended disk metrics. Set to
      <parameter>true</parameter> to enable.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>
     vm_network_check_enable
    </term>
    <listitem>
     <para>
      Toggles the collection of VM network metrics. Set to
      <parameter>true</parameter> to enable.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>
     vm_ping_check_enable
    </term>
    <listitem>
     <para>
      Toggles ping checks for checking whether a host is alive. Set to
      <parameter>true</parameter> to enable.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>
     vm_probation
    </term>
    <listitem>
     <para>
      Specifies a period of time (in seconds) in which to suspend metrics from
      a newly-created VM. This is to prevent quickly-obsolete metrics in an
      environment with a high amount of instance churn (VMs created and
      destroyed in rapid succession). The default probation length is 300
      seconds (5 minutes). Set to 0 to disable VM probation. In this case,
      metrics are recorded immediately after a VM is created.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>
     vnic_collection_period
    </term>
    <listitem>
     <para>
      Specifies a minimum interval in seconds for collecting VM network
      metrics. Increase this value to reduce I/O load. If the value is lower
      than the global agent collection period
      (<parameter>check_frequency</parameter>), it will be ignored in favor of
      the global collection period.
     </para>
    </listitem>
   </varlistentry>
  </variablelist>

  <bridgehead renderas="sect3"><guimenu>Deployment</guimenu>
  </bridgehead>

  <para>
   The &o_monitor; component consists of following roles:
  </para>

  <variablelist>
   <varlistentry>
    <term>monasca-server</term>
    <listitem>
     <para>
      &o_monitor; server-side components that are deployed by &chef;.
      Currently, this only creates &o_ident; resources required by &o_monitor;,
      such as users, roles, endpoints, etc. The rest is left to the
      Ansible-based <systemitem>monasca-installer</systemitem> run by the
      <systemitem>monasca-master</systemitem> role.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>monasca-master</term>
    <listitem>
     <para>
      Runs the Ansible-based <systemitem>monasca-installer</systemitem> from
      the &crow; node. The installer deploys the &o_monitor; server-side
      components to the node that has the
      <systemitem>monasca-server</systemitem> role assigned to it. These
      components are <systemitem>openstack-monasca-api</systemitem>, and
      <systemitem>openstack-monasca-log-api</systemitem>, as well as all the
      back-end services they use.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>monasca-agent</term>
    <listitem>
     <para>
      Deploys <systemitem>openstack-monasca-agent</systemitem> that is
      responsible for sending metrics to <systemitem>monasca-api</systemitem>
      on nodes it is assigned to.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>monasca-log-agent</term>
    <listitem>
     <para>
      Deploys <systemitem>openstack-monasca-log-agent</systemitem> responsible
      for sending logs to <systemitem>monasca-log-api</systemitem> on nodes it
      is assigned to.
     </para>
    </listitem>
   </varlistentry>
  </variablelist>

  <figure>
   <title>The &o_monitor; &Barcl;: Node Deployment Example</title>
   <mediaobject>
    <imageobject role="fo">
     <imagedata fileref="depl_barclamp_monasca_node_deployment.png" width="100%" format="png"/>
    </imageobject>
    <imageobject role="html">
     <imagedata fileref="depl_barclamp_monasca_node_deployment.png" width="75%" format="png"/>
    </imageobject>
   </mediaobject>
  </figure>
 </sect1>

<sect1 xml:id="sec.depl.ostack.ironic">
 <title>Deploying &o_iron; (optional)</title>
 <para>
  &o_iron; is the &ostack; bare metal service for provisioning physical
  machines. Refer to the &ostack; <link
  xlink:href="https://docs.openstack.org/ironic/latest/">developer and admin
  manual</link> for information on drivers, and administering &o_iron;.
 </para>

 <para>
  Deploying the &o_iron; &barcl; is done in five steps:
 </para>
<itemizedlist>
  <listitem>
      <para>
Set options in the Custom view of the &barcl;.
</para>
</listitem>
  <listitem>
      <para>
List the <literal>enabled_drivers</literal> in the Raw view.
</para>
</listitem>
  <listitem>
      <para>
Configure the Ironic network in <filename>network.json</filename>.
</para>
</listitem>
  <listitem>
      <para>
Apply the &barcl; to a Control Node.
</para>
</listitem>
  <listitem>
      <para>
          Apply the <guimenu>nova-compute-ironic</guimenu> role to the same node
          you applied the Ironic barclamp to, in place of the other
          <guimenu>nova-compute-*</guimenu> roles.
</para>
</listitem>
</itemizedlist>

<sect2 xml:id="sec.depl.ostack.ironic.custom-view">
    <title>Custom View Options</title>
    <para>
        Currently, there are two options in the Custom view of the &barcl;.
    </para>
    <variablelist>
<varlistentry>
    <term><guimenu>Enable automated node cleaning</guimenu>
    </term>
    <listitem>
     <para>
         Node cleaning prepares the node to accept a new workload. When you set this to <guimenu>true</guimenu>,
         Ironic collects a list of cleaning steps from the Power, Deploy, Management, and RAID
         interfaces of the driver assigned to the node. Ironic automatically prioritizes and
         executes the cleaning steps, and changes the state of the node to "cleaning". When cleaning
         is complete the state becomes "available". After a new workload is assigned to the machine
         its state changes to "active".
     </para>
     <para>
        <guimenu>false</guimenu> disables automatic cleaning, and you must configure and apply
        node cleaning manually. This requires the admin to create and prioritize the cleaning steps,
        and to set up a cleaning network. Apply manual cleaning when you have long-running or
        destructive tasks that you wish to monitor and control more closely.
        (See <link
     xlink:href="https://docs.openstack.org/ironic/latest/admin/cleaning.html">Node Cleaning</link>.)
        </para>
    </listitem>
   </varlistentry>
<varlistentry>
    <term><guimenu>SSL Support: Protocol</guimenu>
    </term>
    <listitem>
     <para>
         SSL support is not yet enabled, so the only option is <guimenu>HTTP</guimenu>.
        </para>
    </listitem>
   </varlistentry>
  </variablelist>
  <figure>
   <title>The Ironic &barcl; Custom view</title>
   <mediaobject>
    <imageobject role="fo">
     <imagedata fileref="ironic-1.png" width="75%" format="png"/>
    </imageobject>
    <imageobject role="html">
     <imagedata fileref="ironic-1.png" width="75%" format="png"/>
    </imageobject>
   </mediaobject>
  </figure>
</sect2>
  <sect2 xml:id="sec.depl.ostack.ironic-drivers">
  <title>Ironic Drivers</title>
  <para>
    You must enter the Raw view of &barcl; and specify a list of drivers to load during service initialization.
    <literal>pxe_ipmitool</literal> is the recommended default Ironic driver. It uses the
    Intelligent Platform Management Interface (IPMI) to control the power state
    of your bare metal machines, creates the appropriate PXE configurations
    to start them, and then performs the steps to provision and configure the machines.</para>

    <screen>"enabled_drivers": ["pxe_ipmitool"],</screen>
    <para>
     See <link
     xlink:href="https://docs.openstack.org/ironic/latest/admin/drivers.html">Ironic
     Drivers</link> for more information.
    </para>
  </sect2>

  <sect2>
      <title>Example Ironic Network Configuration</title>
      <para>
          This is a complete Ironic <filename>network.json</filename> example, using
          the default <filename>network.json</filename>, followed by a diff that shows
          the Ironic-specific configurations.</para>
      <example xml:id="ex.ironic.network.json">
  <title>Example network.json</title>
<screen>
{
  "start_up_delay": 30,
  "enable_rx_offloading": true,
  "enable_tx_offloading": true,
  "mode": "single",
  "teaming": {
    "mode": 1
  },
  "interface_map": [
    {
      "bus_order": [
        "0000:00/0000:00:01",
        "0000:00/0000:00:03"
      ],
      "pattern": "PowerEdge R610"
    },
    {
      "bus_order": [
        "0000:00/0000:00:01.1/0000:01:00.0",
        "0000:00/0000:00:01.1/0000.01:00.1",
        "0000:00/0000:00:01.0/0000:02:00.0",
        "0000:00/0000:00:01.0/0000:02:00.1"
      ],
      "pattern": "PowerEdge R620"
    },
    {
      "bus_order": [
        "0000:00/0000:00:01",
        "0000:00/0000:00:03"
      ],
      "pattern": "PowerEdge R710"
    },
    {
      "bus_order": [
        "0000:00/0000:00:04",
        "0000:00/0000:00:02"
      ],
      "pattern": "PowerEdge C6145"
    },
    {
      "bus_order": [
        "0000:00/0000:00:03.0/0000:01:00.0",
        "0000:00/0000:00:03.0/0000:01:00.1",
        "0000:00/0000:00:1c.4/0000:06:00.0",
        "0000:00/0000:00:1c.4/0000:06:00.1"
      ],
      "pattern": "PowerEdge R730xd"
    },
    {
      "bus_order": [
        "0000:00/0000:00:1c",
        "0000:00/0000:00:07",
        "0000:00/0000:00:09",
        "0000:00/0000:00:01"
      ],
      "pattern": "PowerEdge C2100"
    },
    {
      "bus_order": [
        "0000:00/0000:00:01",
        "0000:00/0000:00:03",
        "0000:00/0000:00:07"
      ],
      "pattern": "C6100"
    },
    {
      "bus_order": [
        "0000:00/0000:00:01",
        "0000:00/0000:00:02"
      ],
      "pattern": "product"
    }
  ],
  "conduit_map": [
    {
      "conduit_list": {
        "intf0": {
          "if_list": [
            "1g1",
            "1g2"
          ]
        },
        "intf1": {
          "if_list": [
            "1g1",
            "1g2"
          ]
        },
        "intf2": {
          "if_list": [
            "1g1",
            "1g2"
          ]
        },
        "intf3": {
          "if_list": [
            "1g1",
            "1g2"
          ]
        }
      },
      "pattern": "team/.*/.*"
    },
    {
      "conduit_list": {
        "intf0": {
          "if_list": [
            "?1g1"
          ]
        },
        "intf1": {
          "if_list": [
            "?1g2"
          ]
        },
        "intf2": {
          "if_list": [
            "?1g1"
          ]
        },
        "intf3": {
          "if_list": [
            "?1g2"
          ]
        }
      },
      "pattern": "dual/.*/.*"
    },
    {
      "conduit_list": {
        "intf0": {
          "if_list": [
            "?1g1"
          ]
        },
        "intf1": {
          "if_list": [
            "?1g1"
          ]
        },
        "intf2": {
          "if_list": [
            "?1g1"
          ]
        },
        "intf3": {
          "if_list": [
            "?1g2"
          ]
        }
      },
      "pattern": "single/.*/.*ironic.*"
    },
    {
      "conduit_list": {
        "intf0": {
          "if_list": [
            "?1g1"
          ]
        },
        "intf1": {
          "if_list": [
            "?1g1"
          ]
        },
        "intf2": {
          "if_list": [
            "?1g1"
          ]
        },
        "intf3": {
          "if_list": [
            "?1g1"
          ]
        }
      },
      "pattern": "single/.*/.*"
    },
    {
      "conduit_list": {
        "intf0": {
          "if_list": [
            "?1g1"
          ]
        },
        "intf1": {
          "if_list": [
            "1g1"
          ]
        },
        "intf2": {
          "if_list": [
            "1g1"
          ]
        },
        "intf3": {
          "if_list": [
            "1g1"
          ]
        }
      },
      "pattern": ".*/.*/.*"
    },
    {
      "conduit_list": {
        "intf0": {
          "if_list": [
            "1g1"
          ]
        },
        "intf1": {
          "if_list": [
            "?1g1"
          ]
        },
        "intf2": {
          "if_list": [
            "?1g1"
          ]
        },
        "intf3": {
          "if_list": [
            "?1g1"
          ]
        }
      },
      "pattern": "mode/1g_adpt_count/role"
    }
  ],
  "networks": {
    "ironic": {
      "conduit": "intf3",
      "vlan": 100,
      "use_vlan": false,
      "add_bridge": false,
      "add_ovs_bridge": false,
      "bridge_name": "br-ironic",
      "subnet": "192.168.128.0",
      "netmask": "255.255.255.0",
      "broadcast": "192.168.128.255",
      "router": "192.168.128.1",
      "router_pref": 50,
      "ranges": {
        "admin": {
          "start": "192.168.128.10",
          "end": "192.168.128.11"
        },
        "dhcp": {
          "start": "192.168.128.21",
          "end": "192.168.128.254"
        }
      },
      "mtu": 1500
    },
    "storage": {
      "conduit": "intf1",
      "vlan": 200,
      "use_vlan": true,
      "add_bridge": false,
      "mtu": 1500,
      "subnet": "192.168.125.0",
      "netmask": "255.255.255.0",
      "broadcast": "192.168.125.255",
      "ranges": {
        "host": {
          "start": "192.168.125.10",
          "end": "192.168.125.239"
        }
      }
    },
    "public": {
      "conduit": "intf1",
      "vlan": 300,
      "use_vlan": true,
      "add_bridge": false,
      "subnet": "192.168.122.0",
      "netmask": "255.255.255.0",
      "broadcast": "192.168.122.255",
      "router": "192.168.122.1",
      "router_pref": 5,
      "ranges": {
        "host": {
          "start": "192.168.122.2",
          "end": "192.168.122.127"
        }
      },
      "mtu": 1500
    },
    "nova_fixed": {
      "conduit": "intf1",
      "vlan": 500,
      "use_vlan": true,
      "add_bridge": false,
      "add_ovs_bridge": false,
      "bridge_name": "br-fixed",
      "subnet": "192.168.123.0",
      "netmask": "255.255.255.0",
      "broadcast": "192.168.123.255",
      "router": "192.168.123.1",
      "router_pref": 20,
      "ranges": {
        "dhcp": {
          "start": "192.168.123.1",
          "end": "192.168.123.254"
        }
      },
      "mtu": 1500
    },
    "nova_floating": {
      "conduit": "intf1",
      "vlan": 300,
      "use_vlan": true,
      "add_bridge": false,
      "add_ovs_bridge": false,
      "bridge_name": "br-public",
      "subnet": "192.168.122.128",
      "netmask": "255.255.255.128",
      "broadcast": "192.168.122.255",
      "ranges": {
        "host": {
          "start": "192.168.122.129",
          "end": "192.168.122.254"
        }
      },
      "mtu": 1500
    },
    "bmc": {
      "conduit": "bmc",
      "vlan": 100,
      "use_vlan": false,
      "add_bridge": false,
      "subnet": "192.168.124.0",
      "netmask": "255.255.255.0",
      "broadcast": "192.168.124.255",
      "ranges": {
        "host": {
          "start": "192.168.124.162",
          "end": "192.168.124.240"
        }
      },
      "router": "192.168.124.1"
    },
    "bmc_vlan": {
      "conduit": "intf2",
      "vlan": 100,
      "use_vlan": true,
      "add_bridge": false,
      "subnet": "192.168.124.0",
      "netmask": "255.255.255.0",
      "broadcast": "192.168.124.255",
      "ranges": {
        "host": {
          "start": "192.168.124.161",
          "end": "192.168.124.161"
        }
      }
    },
    "os_sdn": {
      "conduit": "intf1",
      "vlan": 400,
      "use_vlan": true,
      "add_bridge": false,
      "mtu": 1500,
      "subnet": "192.168.130.0",
      "netmask": "255.255.255.0",
      "broadcast": "192.168.130.255",
      "ranges": {
        "host": {
          "start": "192.168.130.10",
          "end": "192.168.130.254"
        }
      }
    },
    "admin": {
      "conduit": "intf0",
      "vlan": 100,
      "use_vlan": false,
      "add_bridge": false,
      "mtu": 1500,
      "subnet": "192.168.124.0",
      "netmask": "255.255.255.0",
      "broadcast": "192.168.124.255",
      "router": "192.168.124.1",
      "router_pref": 10,
      "ranges": {
        "admin": {
          "start": "192.168.124.10",
          "end": "192.168.124.11"
        },
        "dhcp": {
          "start": "192.168.124.21",
          "end": "192.168.124.80"
        },
        "host": {
          "start": "192.168.124.81",
          "end": "192.168.124.160"
        },
        "switch": {
          "start": "192.168.124.241",
          "end": "192.168.124.250"
        }
      }
    }
  }
}
</screen>
</example>
<example xml:id="ex.ironic.network.json-diff">
  <title>Diff of Ironic Configuration</title>
  <para>
      This diff should help you separate the Ironic items from the default
      <filename>network.json</filename>.
</para>
<screen>
--- network.json        2017-06-07 09:22:38.614557114 +0200
+++ ironic_network.json 2017-06-05 12:01:15.927028019 +0200
@@ -91,6 +91,12 @@
             "1g1",
             "1g2"
           ]
+        },
+        "intf3": {
+          "if_list": [
+            "1g1",
+            "1g2"
+          ]
         }
       },
       "pattern": "team/.*/.*"
@@ -111,6 +117,11 @@
           "if_list": [
             "?1g1"
           ]
+        },
+        "intf3": {
+          "if_list": [
+            "?1g2"
+          ]
         }
       },
       "pattern": "dual/.*/.*"
@@ -131,6 +142,36 @@
           "if_list": [
             "?1g1"
           ]
+        },
+        "intf3": {
+          "if_list": [
+            "?1g2"
+          ]
+        }
+      },
+      "pattern": "single/.*/.*ironic.*"
+    },
+    {
+      "conduit_list": {
+        "intf0": {
+          "if_list": [
+            "?1g1"
+          ]
+        },
+        "intf1": {
+          "if_list": [
+            "?1g1"
+          ]
+        },
+        "intf2": {
+          "if_list": [
+            "?1g1"
+          ]
+        },
+        "intf3": {
+          "if_list": [
+            "?1g1"
+          ]
         }
       },
       "pattern": "single/.*/.*"
@@ -151,6 +192,11 @@
           "if_list": [
             "1g1"
           ]
+        },
+        "intf3": {
+          "if_list": [
+            "1g1"
+          ]
         }
       },
       "pattern": ".*/.*/.*"
@@ -171,12 +217,41 @@
           "if_list": [
             "?1g1"
           ]
+        },
+        "intf3": {
+          "if_list": [
+            "?1g1"
+          ]
         }
       },
       "pattern": "mode/1g_adpt_count/role"
     }
   ],
   "networks": {
+    "ironic": {
+      "conduit": "intf3",
+      "vlan": 100,
+      "use_vlan": false,
+      "add_bridge": false,
+      "add_ovs_bridge": false,
+      "bridge_name": "br-ironic",
+      "subnet": "192.168.128.0",
+      "netmask": "255.255.255.0",
+      "broadcast": "192.168.128.255",
+      "router": "192.168.128.1",
+      "router_pref": 50,
+      "ranges": {
+        "admin": {
+          "start": "192.168.128.10",
+          "end": "192.168.128.11"
+        },
+        "dhcp": {
+          "start": "192.168.128.21",
+          "end": "192.168.128.254"
+        }
+      },
+      "mtu": 1500
+    },
     "storage": {
       "conduit": "intf1",
       "vlan": 200,
   </screen>
</example>
</sect2>
  </sect1>

  <sect1 xml:id="sec.depl.ostack.final">
  <title>How to Proceed</title>

  <para>
   With a successful deployment of the &ostack; &dash;, the &productname;
   installation is finished. To be able to test your setup by starting an
   &vmguest; one last step remains to be done&mdash;uploading an image to the
   &o_img; component. Refer to the &cloudsuppl;, chapter <citetitle>Manage
   images</citetitle>
<!--<xref linkend="sec.adm.cli.img"/>-->
   for instructions. Images for &cloud; can be built in SUSE Studio. Refer to
   the &cloudsuppl;, section <citetitle>Building Images with
   &susestudio;</citetitle>.
  </para>

  <para>
   Now you can hand over to the cloud administrator to set up users, roles,
   flavors, etc.&mdash;refer to the &cloudadmin; for details. The default
   credentials for the &ostack; &dash; are user name <literal>admin</literal>
   and password <literal>crowbar</literal>.
  </para>
  </sect1>

  <sect1 xml:id="crow-ses-integration">
   <title>&ses; integration</title>
   <para>
    &productname; supports integration with &ses; (SES), enabling &ceph; block
    storage as well as image storage services in &cloud;.
   </para>
   <bridgehead renderas="sect2">Enabling SES Integration
   </bridgehead>
   <para>
    To enable SES integration on &crow;, an SES configuration file must be
    uploaded to &crow;. SES integration functionality is included in the
    <literal>crowbar-core</literal> package and can be used with the &crow; UI
    or CLI (<literal>crowbarctl</literal>). The SES configuration file
    describes various aspects of the &ceph; environment, and keyrings for each
    user and pool created in the &ceph; environment for &cloud-crow; services.
   </para>
   <bridgehead renderas="sect2">SES Configuration
   </bridgehead>
   <para>
    For SES deployments that are version 5.5 and higher, a Salt runner is used
    to create all the users and pools. It also generates a YAML
    configuration that is needed to integrate with &cloud;. The integration
    runner creates separate users for &o_blockstore;, &o_blockstore; backup
    (not used by &crow; currently) and &o_img;. Both the &o_blockstore; and
    &o_comp; services have the same user, because &o_blockstore; needs
    access to create objects that &o_comp; uses.
   </para>
   <para>
    Configure SES with the following steps:
   </para>
   <procedure>
    <step>
     <para>
      Login as <literal>root</literal> and run the SES 5.5 Salt runner on the
      Salt admin host.
     </para>
     <screen>&prompt.root;salt-run --out=yaml openstack.integrate prefix=mycloud</screen>
     <para>
      The prefix parameter allows pools to be created with the specified
      prefix. By using different prefix parameters, multiple cloud deployments
      can support different users and pools on the same SES deployment.
     </para>
    </step>
    <step>
     <para>
      A YAML file is created with content similar to the following
      example:
     </para>
     <screen>ceph_conf:
     cluster_network: 10.84.56.0/21
     fsid: d5d7c7cb-5858-3218-a36f-d028df7b0673
     mon_host: 10.84.56.8, 10.84.56.9, 10.84.56.7
     mon_initial_members: ses-osd1, ses-osd2, ses-osd3
     public_network: 10.84.56.0/21
cinder:
     key: ABCDEFGaxefEMxAAW4zp2My/5HjoST2Y87654321==
     rbd_store_pool: mycloud-cinder
     rbd_store_user: cinder
cinder-backup:
     key: AQBb8hdbrY2bNRAAqJC2ZzR5Q4yrionh7V5PkQ==
     rbd_store_pool: mycloud-backups
     rbd_store_user: cinder-backup
glance:
     key: AQD9eYRachg1NxAAiT6Hw/xYDA1vwSWLItLpgA==
     rbd_store_pool: mycloud-glance
     rbd_store_user: glance
nova:
     rbd_store_pool: mycloud-nova
radosgw_urls:
     - http://10.84.56.7:80/swift/v1
     - http://10.84.56.8:80/swift/v1</screen>
    </step>
    <step>
     <para>
      Upload the generated YAML file to &crow; using the UI or
      <literal>crowbarctl</literal> CLI.
     </para>
    </step>
    <step>
     <para>
      If the Salt runner is not available, you must manually create pools and
      users to allow &cloud; services to use the SES/Ceph cluster. Pools and
      users must be created for &o_blockstore;, &o_comp;, and
      &o_img;. Instructions for creating and managing pools, users and keyrings
      can be found in the <link
      xlink:href="https://www.suse.com/documentation/suse-enterprise-storage-5/">
      SUSE Enterprise Storage</link> Administration Guide in the Key Management
      section.
     </para>
     <para>
      After the required pools and users are set up on the &ses;/&ceph;
      cluster, create an SES configuration file in YAML format (using the
      example template above). Upload this file to &crow; using the UI or
      <literal>crowbarctl</literal> CLI.
     </para>
    </step>
    <step>
     <para>
      As indicated above, the SES configuration file can be uploaded to &crow;
      using the UI or <literal>crowbarctl</literal> CLI.
     </para>
     <itemizedlist>
      <listitem>
       <para>
        From the main &crow; UI, the upload page is under
        <menuchoice><guimenu>Utilities</guimenu> <guimenu>SUSE Enterprise
        Storage</guimenu></menuchoice>.
       </para>
       <para>
        If a configuration is already stored in &crow;, it will be visible in
        the upload page. A newly uploaded configuration will replace existing
        one. The new configuration will be applied to the cloud on the next
        <literal>chef-client</literal> run. There is no need to reapply
        proposals.
       </para>
       <para>
        Configurations can also be deleted from &crow;. After deleting a
        configuration, you must manually update and reapply all proposals that
        used SES integration.
       </para>
      </listitem>
      <listitem>
       <para>
        With the <literal>crowbarctl</literal> CLI, the command <command>crowbarctl ses
        upload <replaceable>FILE</replaceable></command> accepts a path to the
        SES configuration file.
       </para>
      </listitem>
     </itemizedlist>
    </step>
   </procedure>
   <bridgehead renderas="sect2">Cloud Service Configuration
   </bridgehead>
   <para>
    SES integration with &cloud; services is implemented with relevant &Barcl;s
    and installed with the <literal>crowbar-openstack</literal> package.
   </para>
   <variablelist>
    <varlistentry>
     <term>&o_img;</term>
     <listitem>
      <para>
       Set <literal>Use SES Configuration</literal> to <literal>true</literal>
       under <literal>RADOS Store Parameters</literal>. The &o_img; &barcl;
       pulls the uploaded SES configuration from &crow; when applying the
       &o_img; proposal and on <literal>chef-client</literal> runs. If the SES
       configuration is uploaded before the &o_img; proposal is created,
       <literal>Use SES Configuration</literal> is enabled automatically
       upon proposal creation.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>&o_blockstore;</term>
     <listitem>
      <para>
       Create a new &rados; backend and set <literal>Use SES
       Configuration</literal> to <literal>true</literal>. The &o_blockstore;
       &barcl; pulls the uploaded SES configuration from &crow; when applying the
       &o_blockstore; proposal and on <literal>chef-client</literal> runs. If
       the SES configuration was uploaded before the &o_blockstore; proposal
       was created, a <literal>ses-ceph</literal> &rados; backend is created
       automatically on proposal creation with <literal>Use SES
       Configuration</literal> already enabled.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>&o_comp;</term>
     <listitem>
      <para>
       No special configuration is needed for &o_comp;. It uses the
       configuration from the &o_blockstore; &barcl; to connect with volumes
       stored in SES.
      </para>
     </listitem>
    </varlistentry>
   </variablelist>
  </sect1>

 <sect1 xml:id="sec.depl.services">
  <title>Roles and Services in &productname;</title>

  <para>
   The following table lists all roles (as defined in the &barcl;s), and their
   associated services. As of &productname; 8, this list is work in
   progress. Services can be manually started and stopped with the commands
   <command>systemctl start <replaceable>SERVICE</replaceable></command> and
   <command>systemctl stop <replaceable>SERVICE</replaceable></command>.
  </para>

  <informaltable>
   <tgroup cols="2">
    <colspec colnum="1" colname="1" colwidth="35*"/>
    <colspec colnum="2" colname="2" colwidth="65*"/>
    <thead>
     <row>
      <entry>
       <para>
        Role
       </para>
      </entry>
      <entry>
       <para>
        Service
       </para>
      </entry>
     </row>
    </thead>
    <tbody>
<!-- ceilometer-agent -->
     <row>
      <entry>
       <para>
        ceilometer-agent
       </para>
      </entry>
      <entry><systemitem class="service">
       openstack-ceilometer-agent-compute
      </systemitem>
      </entry>
     </row>
<!-- ceilometer-polling -->
     <row>
      <entry morerows="5">
       <para>
        ceilometer-polling
       </para>
       <para>
        ceilometer-server
       </para>
       <para>
        ceilometer-swift-proxy-middleware
       </para>
      </entry>
      <entry>
       <para>
        <systemitem class="service"> openstack-ceilometer-agent-notification
        </systemitem>
       </para>
      </entry>
     </row>
     <row>
      <entry>
       <para>
        <systemitem class="service"> openstack-ceilometer-alarm-evaluator
        </systemitem>
       </para>
      </entry>
     </row>
     <row>
      <entry>
       <para>
        <systemitem class="service"> openstack-ceilometer-alarm-notifier
        </systemitem>
       </para>
      </entry>
     </row>
     <row>
      <entry>
       <para>
        <systemitem class="service"> openstack-ceilometer-api </systemitem>
       </para>
      </entry>
     </row>
     <row>
      <entry>
       <para>
        <systemitem class="service"> openstack-ceilometer-collector
        </systemitem>
       </para>
      </entry>
     </row>
     <row>
      <entry>
       <para>
        <systemitem class="service"> openstack-ceilometer-polling </systemitem>
       </para>
      </entry>
     </row>
<!-- cinder-controller -->
     <row>
      <entry morerows="1">
       <para>
        cinder-controller
       </para>
      </entry>
      <entry>
       <para>
        <systemitem class="service">openstack-cinder-api</systemitem>
       </para>
      </entry>
     </row>
     <row>
      <entry>
       <para>
        <systemitem class="service">openstack-cinder-scheduler</systemitem>
       </para>
      </entry>
     </row>
<!-- cinder-volume -->
     <row>
      <entry>
       <para>
        cinder-volume
       </para>
      </entry>
      <entry>
       <para>
        <systemitem class="service">openstack-cinder-volume</systemitem>
       </para>
      </entry>
     </row>
<!-- database-server -->
     <row>
      <entry>
       <para>
        database-server
       </para>
      </entry>
      <entry>
       <para>
        <systemitem class="service">postgresql</systemitem>
       </para>
      </entry>
     </row>
<!-- glance-server -->
     <row>
      <entry morerows="1">
       <para>
        glance-server
       </para>
      </entry>
      <entry>
       <para>
        <systemitem class="service">openstack-glance-api</systemitem>
       </para>
      </entry>
     </row>
     <row>
      <entry>
       <para>
        <systemitem class="service">openstack-glance-registry</systemitem>
       </para>
      </entry>
     </row>
<!-- heat-server -->
     <row>
      <entry morerows="3">
       <para>
        heat-server
       </para>
      </entry>
      <entry>
       <para>
        <systemitem class="service">openstack-heat-api-cfn</systemitem>
       </para>
      </entry>
     </row>
     <row>
      <entry>
       <para>
        <systemitem class="service">openstack-heat-api-cloudwatch</systemitem>
       </para>
      </entry>
     </row>
     <row>
      <entry>
       <para>
        <systemitem class="service">openstack-heat-api</systemitem>
       </para>
      </entry>
     </row>
     <row>
      <entry>
       <para>
        <systemitem class="service">openstack-heat-engine</systemitem>
       </para>
      </entry>
     </row>
<!-- horizon -->
     <row>
      <entry>
       <para>
        horizon
       </para>
      </entry>
      <entry>
       <para>
        <systemitem class="service">apache2</systemitem>
       </para>
      </entry>
     </row>
<!-- keystone-server -->
     <row>
      <entry>
       <para>
        keystone-server
       </para>
      </entry>
      <entry>
       <para>
        <systemitem class="service">openstack-keystone</systemitem>
       </para>
      </entry>
     </row>
<!-- manila-server -->
     <row>
      <entry morerows="1">
       <para>
        manila-server
       </para>
      </entry>
      <entry>
       <para>
        <systemitem class="service">openstack-manila-api</systemitem>
       </para>
      </entry>
     </row>
     <row>
      <entry>
       <para>
        <systemitem class="service">openstack-manila-scheduler</systemitem>
       </para>
      </entry>
     </row>
<!-- manila-share -->
     <row>
      <entry>
       <para>
        manila-share
       </para>
      </entry>
      <entry>
       <para>
        <systemitem class="service">openstack-manila-share</systemitem>
       </para>
      </entry>
     </row>
<!-- neutron-server -->
     <row>
      <entry>
       <para>
        neutron-server
       </para>
      </entry>
      <entry>
       <para>
        <systemitem class="service">openstack-neutron</systemitem>
       </para>
      </entry>
     </row>
<!-- nova-compute-* -->
     <row>
      <entry morerows="1">
       <para>
        nova-compute-*
       </para>
      </entry>
      <entry>
       <para>
        <systemitem class="service">openstack-nova-compute</systemitem>
       </para>
      </entry>
     </row>
     <row>
      <entry>
       <para>
        <systemitem class="service"> openstack-neutron-openvswitch-agent
        </systemitem> (when neutron is deployed with openvswitch)
       </para>
      </entry>
     </row>
<!-- nova-controller -->
     <row>
      <entry morerows="6">
       <para>
        nova-controller
       </para>
      </entry>
      <entry>
       <para>
        <systemitem class="service">openstack-nova-api</systemitem>
       </para>
      </entry>
     </row>
     <row>
      <entry>
       <para>
        <systemitem class="service">openstack-nova-cert</systemitem>
       </para>
      </entry>
     </row>
     <row>
      <entry>
       <para>
        <systemitem class="service">openstack-nova-conductor</systemitem>
       </para>
      </entry>
     </row>
     <row>
      <entry>
       <para>
        <systemitem class="service">openstack-nova-consoleauth</systemitem>
       </para>
      </entry>
     </row>
     <row>
      <entry>
       <para>
        <systemitem class="service">openstack-nova-novncproxy</systemitem>
       </para>
      </entry>
     </row>
     <row>
      <entry>
       <para>
        <systemitem class="service">openstack-nova-objectstore</systemitem>
       </para>
      </entry>
     </row>
     <row>
      <entry>
       <para>
        <systemitem class="service">openstack-nova-scheduler</systemitem>
       </para>
      </entry>
     </row>
<!-- rabbitmq-server -->
     <row>
      <entry>
       <para>
        rabbitmq-server
       </para>
      </entry>
      <entry>
       <para>
        <systemitem class="service">rabbitmq-server</systemitem>
       </para>
      </entry>
     </row>
<!-- swift-dispersion -->
     <row>
      <entry>
       <para>
        swift-dispersion
       </para>
      </entry>
      <entry>
       <para>
        none
       </para>
      </entry>
     </row>
<!-- swift-proxy -->
     <row>
      <entry>
       <para>
        swift-proxy
       </para>
      </entry>
      <entry>
       <para>
        <systemitem class="service">openstack-swift-proxy</systemitem>
       </para>
      </entry>
     </row>
<!-- swift-ring-compute -->
     <row>
      <entry>
       <para>
        swift-ring-compute
       </para>
      </entry>
      <entry>
       <para>
        none
       </para>
      </entry>
     </row>
<!-- swift-storage -->
     <row>
      <entry morerows="13">
       <para>
        swift-storage
       </para>
      </entry>
      <entry>
       <para>
        <systemitem class="service">openstack-swift-account-auditor</systemitem>
       </para>
      </entry>
     </row>
     <row>
      <entry>
       <para>
        <systemitem class="service">openstack-swift-account-reaper</systemitem>
       </para>
      </entry>
     </row>
     <row>
      <entry>
       <para>
        <systemitem class="service">openstack-swift-account-replicator</systemitem>
       </para>
      </entry>
     </row>
     <row>
      <entry>
       <para>
        <systemitem class="service">openstack-swift-account</systemitem>
       </para>
      </entry>
     </row>
     <row>
      <entry>
       <para>
        <systemitem class="service">openstack-swift-container-auditor</systemitem>
       </para>
      </entry>
     </row>
     <row>
      <entry>
       <para>
        <systemitem class="service">openstack-swift-container-replicator</systemitem>
       </para>
      </entry>
     </row>
     <row>
      <entry>
       <para>
        <systemitem class="service">openstack-swift-container-sync</systemitem>
       </para>
      </entry>
     </row>
     <row>
      <entry>
       <para>
        <systemitem class="service">openstack-swift-container-updater</systemitem>
       </para>
      </entry>
     </row>
     <row>
      <entry>
       <para>
        <systemitem class="service">openstack-swift-container</systemitem>
       </para>
      </entry>
     </row>
     <row>
      <entry>
       <para>
        <systemitem class="service">openstack-swift-object-auditor</systemitem>
       </para>
      </entry>
     </row>
     <row>
      <entry>
       <para>
        <systemitem class="service">openstack-swift-object-expirer</systemitem>
       </para>
      </entry>
     </row>
     <row>
      <entry>
       <para>
        <systemitem class="service">openstack-swift-object-replicator</systemitem>
       </para>
      </entry>
     </row>
     <row>
      <entry>
       <para>
        <systemitem class="service">openstack-swift-object-updater</systemitem>
       </para>
      </entry>
     </row>
     <row>
      <entry>
       <para>
        <systemitem class="service">openstack-swift-object</systemitem>
       </para>
      </entry>
     </row>
    </tbody>
   </tgroup>
  </informaltable>
 </sect1>
 <sect1 xml:id="sec.deploy.crowbatch.description">
  <title>Crowbar Batch Command</title>

  <para>
   This is the documentation for the <command>crowbar batch</command>
   subcommand.
  </para>

  <para>
   <command>crowbar batch</command> provides a quick way of creating, updating,
   and applying &crow; proposals. It can be used to:
  </para>

  <itemizedlist mark="bullet" spacing="normal">
   <listitem>
    <para>
     Accurately capture the configuration of an existing &crow; environment.
    </para>
   </listitem>
   <listitem>
    <para>
     Drive &crow; to build a complete new environment from scratch.
    </para>
   </listitem>
   <listitem>
    <para>
     Capture one &cloud; environment and then reproduce it on another set of
     hardware (provided hardware and network configuration match to an
     appropriate extent).
    </para>
   </listitem>
   <listitem>
    <para>
     Automatically update existing proposals.
    </para>
   </listitem>
  </itemizedlist>

  <para>
   As the name suggests, <command>crowbar batch</command> is intended to be run
   in <quote>batch mode</quote> that is mostly unattended. It has two modes of
   operation:
  </para>

  <variablelist>
   <varlistentry>
    <term>crowbar batch export
    </term>
    <listitem>
     <para>
      Exports a YAML file which describes existing proposals and how their
      parameters deviate from the default proposal values for that &barcl;.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>crowbar batch build
    </term>
    <listitem>
     <para>
      Imports a YAML file in the same format as above. Uses it to build new
      proposals if they do not yet exist. Updates the existing proposals so
      that their parameters match those given in the YAML file.
     </para>
    </listitem>
   </varlistentry>
  </variablelist>

  <sect2 xml:id="sec.deploy.crowbatch.yaml">
   <title>YAML file format</title>
   <para>
    Here is an example YAML file. At the top-level there is a proposals array,
    each entry of which is a hash representing a proposal:
   </para>
<screen>proposals:
- barclamp: provisioner
  # Proposal name defaults to 'default'.
  attributes:
    shell_prompt: USER@ALIAS:CWD SUFFIX
- barclamp: database
  # Default attributes are good enough, so we just need to assign
  # nodes to roles:
  deployment:
    elements:
      database-server:
        - "@@controller1@@"
- barclamp: rabbitmq
  deployment:
    elements:
      rabbitmq-server:
        - "@@controller1@@"</screen>
   <note>
    <title>Reserved Indicators in YAML</title>
    <para>
     Note that the characters <literal>@</literal> and <literal>`</literal> are
     reserved indicators in YAML. They can appear anywhere in a string
     <emphasis>except at the beginning</emphasis>. Therefore a string such as
     <literal>@@controller1@@</literal> needs to be quoted using double quotes.
    </para>
   </note>
  </sect2>

  <sect2 xml:id="sec.deploy.crowbatch.yaml.attributes">
   <title>Top-level proposal attributes</title>
   <variablelist>
    <varlistentry>
     <term>barclamp
     </term>
     <listitem>
      <para>
       Name of the &barcl; for this proposal (required).
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>name
     </term>
     <listitem>
      <para>
       Name of this proposal (optional; default is <literal>default</literal>).
       In <command>build</command> mode, if the proposal does not already
       exist, it will be created.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>attributes
     </term>
     <listitem>
      <para>
       An optional nested hash containing any attributes for this proposal
       which deviate from the defaults for the &barcl;.
      </para>
      <para>
       In <command>export</command> mode, any attributes set to the default
       values are excluded to keep the YAML as short and readable as possible.
      </para>
      <para>
       In <command>build</command> mode, these attributes are deep-merged with
       the current values for the proposal. If the proposal did not already
       exist, batch build will create it first. The attributes are merged with
       the default values for the &barcl;'s proposal.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>wipe_attributes
     </term>
     <listitem>
      <para>
       An optional array of paths to nested attributes which should be removed
       from the proposal.
      </para>
      <para>
       Each path is a period-delimited sequence of attributes; for example
       <literal>pacemaker.stonith.sbd.nodes</literal> would remove all SBD
       nodes from the proposal if it already exists. If a path segment contains
       a period, it should be escaped with a backslash, for example
       <literal>segment-one.segment\.two.segment_three</literal>.
      </para>
      <para>
       This removal occurs before the deep merge described above.
<!--For
       example, a batch build with a YAML file which includes
       <literal>pacemaker.stonith.sbd.nodes</literal> in
       <literal>wipe_attributes</literal> of a pacemaker &barcl; proposal
       would ensure that at the end of the run, only SBD nodes listed in the
       attributes sibling hash would be used. -->
       For example, think of a YAML file which includes a Pacemaker &barcl;
       proposal where the <literal>wipe_attributes</literal> entry contains
       <literal>pacemaker.stonith.sbd.nodes</literal>. A batch build with this
       YAML file ensures that only SBD nodes listed in the <literal>attributes
       sibling</literal> hash are used at the end of the run. In contrast,
       without the <literal>wipe_attributes</literal> entry, the given SBD
       nodes would be appended to any SBD nodes already defined in the
       proposal.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>deployment
     </term>
     <listitem>
      <para>
       A nested hash defining how and where this proposal should be deployed.
      </para>
      <para>
       In <command>build</command> mode, this hash is deep-merged in the same
       way as the attributes hash, except that the array of elements for each
       &chef; role is reset to the empty list before the deep merge. This
       behavior may change in the future.
      </para>
     </listitem>
    </varlistentry>
   </variablelist>
  </sect2>

  <sect2 xml:id="sec.deploy.crowbatch.yaml.subst">
   <title>Node Alias Substitutions</title>
   <para>
    A string like <literal>@@<replaceable>node</replaceable>@@</literal> (where
    <replaceable>node</replaceable> is a node alias) will be substituted for
    the name of that node, no matter where the string appears in the YAML file.
    For example, if <literal>controller1</literal> is a &crow; alias for node
    <literal>d52-54-02-77-77-02.mycloud.com</literal>, then
    <literal>@@controller1@@</literal> will be substituted for that host name.
    This allows YAML files to be reused across environments.
   </para>
  </sect2>

  <sect2 xml:id="sec.deploy.crowbatch.options">
   <title>Options</title>
   <para>
    In addition to the standard options available to every
    <command>crowbar</command> subcommand (run <command>crowbar batch
    --help</command> for a full list), there are some extra options
    specifically for <command>crowbar batch</command>:
   </para>
   <variablelist>
    <varlistentry>
     <term>--include &lt;barclamp[.proposal]&gt;
    </term>
     <listitem>
      <para>
       Only include the &barcl; / proposals given.
      </para>
      <para>
       This option can be repeated multiple times. The inclusion value can
       either be the name of a &barcl; (for example,
       <literal>pacemaker</literal>) or a specifically named proposal within
       the &barcl; (for example, <literal>pacemaker.network_cluster</literal>).
      </para>
      <para>
       If it is specified, then only the &barcl; / proposals specified are
       included in the build or export operation, and all others are ignored.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>--exclude &lt;barclamp[.proposal]&gt;
    </term>
     <listitem>
      <para>
       This option can be repeated multiple times. The exclusion value is the
       same format as for <option>--include</option>. The &barcl;s / proposals
       specified are excluded from the build or export operation.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>--timeout &lt;seconds&gt;
    </term>
     <listitem>
      <para>
       Change the timeout for &crow; API calls.
      </para>
      <para>
       As &chef;'s run lists grow, some of the later &ostack; &barcl; proposals
       (for example &o_comp;, &o_dash;, or &o_orch;) can take over 5 or even 10
       minutes to apply. Therefore you may need to increase this timeout to 900
       seconds in some circumstances.
      </para>
     </listitem>
    </varlistentry>
   </variablelist>
  </sect2>
 </sect1>
</chapter>
