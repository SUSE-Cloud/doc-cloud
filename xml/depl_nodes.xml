<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE chapter
[
  <!ENTITY % entities SYSTEM "entity-decl.ent">
  %entities;
]>
<!-- Converted by suse-upgrade version 1.1 -->
<chapter xmlns="http://docbook.org/ns/docbook" xmlns:xi="http://www.w3.org/2001/XInclude" xmlns:xlink="http://www.w3.org/1999/xlink" version="5.0" xml:id="cha.depl.ostack">
 <title>Deploying the &ostack; Services</title>
 <info>
<dm:docmanager xmlns:dm="urn:x-suse:ns:docmanager">
    <dm:maintainer>fs</dm:maintainer>
    <dm:status>editing</dm:status>
    <dm:deadline/>
    <dm:priority/>
    <dm:translation>no</dm:translation>
    <dm:languages/>
</dm:docmanager>
 </info>
<para>
  After the nodes are installed and configured you can start deploying the
  &ostack; services to finalize the installation. The services need to be
  deployed in a given order, because they depend on one another. The
  <guimenu>Pacemaker</guimenu> service for an &hasetup; is the only
  exception from this rule&mdash;it can be set up at any time. However,
  when deploying &productname; from scratch, it is recommended to deploy
  the <guimenu>Pacemaker</guimenu> proposal(s) first. Deployment for all
  services is done from the &crow; Web interface through recipes,
  so-called <quote>&barcl;s</quote>.
 </para>
 <para>
  The services controlling the cloud (including storage management and
  control services) need to be installed on the &contrnode;(s) (refer to
  <xref linkend="sec.depl.arch.components.control"/> for more information).
  However, you may <emphasis>not</emphasis> use your &contrnode;(s) as a
  compute node or storage host for &o_objstore; or &ceph;. Here is a
  list with services that may <emphasis>not</emphasis> be installed on the
  &contrnode;(s): <guimenu>swift-storage</guimenu>, all &ceph;
  services, <guimenu>nova-compute-*</guimenu>. These services need to be
  installed on dedicated nodes.
 </para>
 <para>
  When deploying an &hasetup;, the controller nodes are replaced by one or
  more controller clusters consisting of at least two nodes (three are
  recommended). Setting up three separate clusters&mdash;for data,
  services, and networking&mdash;is recommended. See
  <xref linkend="sec.depl.req.ha"/> for more information on requirements and
  recommendations for an &hasetup;.
 </para>
 <para>
  The &ostack; services need to be deployed in the following order. For
  general instructions on how to edit and deploy &barcl;, refer to
  <xref linkend="sec.depl.ostack.barclamps"/>. Deploying Pacemaker (only
  needed for an &hasetup;), &o_objstore; and &ceph; is optional;
  all other services must be deployed.
 </para>
 <orderedlist spacing="normal">
  <listitem>
<!-- Pacemaker -->
   <para>
    <xref linkend="sec.depl.ostack.pacemaker" xrefstyle="select:title nopage"/>
   </para>
  </listitem>
  <listitem>
<!-- Database -->
   <para>
    <xref linkend="sec.depl.ostack.db" xrefstyle="select:title nopage"/>
   </para>
  </listitem>
  <listitem>
<!-- RabbitMQ -->
   <para>
    <xref linkend="sec.depl.ostack.rabbit" xrefstyle="select:title nopage"/>
   </para>
  </listitem>
  <listitem>
<!-- Keystone -->
   <para>
    <xref linkend="sec.depl.ostack.keystone" xrefstyle="select:title nopage"/>
   </para>
  </listitem>
  <listitem>
<!-- Ceph -->
   <para>
    <xref linkend="sec.depl.ostack.ceph" xrefstyle="select:title nopage"/>
   </para>
  </listitem>
  <listitem>
<!-- Swift -->
   <para>
    <xref linkend="sec.depl.ostack.swift" xrefstyle="select:title nopage"/>
   </para>
  </listitem>
  <listitem>
<!-- Glance -->
   <para>
    <xref linkend="sec.depl.ostack.glance" xrefstyle="select:title nopage"/>
   </para>
  </listitem>
<!-- Cinder -->
  <listitem>
   <para>
    <xref linkend="sec.depl.ostack.cinder" xrefstyle="select:title nopage"/>
   </para>
  </listitem>
<!-- Manila -->
  <listitem>
   <para>
    <xref linkend="sec.depl.ostack.manila" xrefstyle="select:title nopage"/>
   </para>
  </listitem>
<!-- Neutrum -->
  <listitem>
   <para>
    <xref linkend="sec.depl.ostack.quantum" xrefstyle="select:title nopage"/>
   </para>
  </listitem>
<!-- Nova -->
  <listitem>
   <para>
    <xref linkend="sec.depl.ostack.nova" xrefstyle="select:title nopage"/>
   </para>
  </listitem>
<!-- Horizon -->
  <listitem>
   <para>
    <xref linkend="sec.depl.ostack.dash" xrefstyle="select:title nopage"/>
   </para>
  </listitem>
<!-- Heat -->
  <listitem>
   <para>
    <xref linkend="sec.depl.ostack.heat" xrefstyle="select:title nopage"/>
   </para>
  </listitem>
<!-- Ceilometer -->
  <listitem>
   <para>
    <xref linkend="sec.depl.ostack.ceilometer" xrefstyle="select:title nopage"/>
   </para>
  </listitem>
<!-- Trove -->
  <listitem>
   <para>
    <xref linkend="sec.depl.ostack.trove" xrefstyle="select:title nopage"/>
   </para>
  </listitem>
<!-- Tempest -->
  <listitem>
   <para>
    <xref linkend="sec.depl.ostack.tempest" xrefstyle="select:title nopage"/>
   </para>
  </listitem>
 </orderedlist>
 <sect1 xml:id="sec.depl.ostack.barclamps">
  <title>&Barcl;s</title>

  <para>
   The &ostack; services are automatically installed on the nodes by using
   so-called &barcl;s&mdash;a set of recipes, templates, and installation
   instructions. A &barcl; is configured via a so-called proposal. A proposal
   contains the configuration of the service(s) associated with the barclamp
   and a list of machines onto which the &barcl; should be deployed.
  </para>
  <para>
   All existing &barcl;s can be accessed from the &crow; Web interface by
   clicking <guimenu>Barclamps</guimenu>. To create or edit &barcl; proposals
   and deploy them, proceed as follows:
  </para>
  <procedure>
   <step>
    <para>
     Open a browser and point it to the &crow; Web interface available on the
     &admserv;, for example <literal>http://192.168.124.10/</literal>. Log in
     as user <systemitem class="username">crowbar</systemitem>. The password
     is <literal>crowbar</literal>by default, if you have not changed it.
    </para>
    <para>
     Click <guimenu>Barclamps</guimenu> to open the <guimenu>All
     Barclamps</guimenu> menu. Alternatively you may filter the list to
     <guimenu>Crowbar</guimenu> or <guimenu>OpenStack</guimenu> &barcl;s
     by choosing the respective option from <guimenu>Barclamps</guimenu>.
     The <guimenu>Crowbar</guimenu> &barcl;s contain general recipes for
     setting up and configuring all nodes, while the
     <guimenu>OpenStack</guimenu> &barcl;s are dedicated to &ostack; service
     deployment and configuration.
    </para>
   </step>
   <step>
    <para>
     You can either <guimenu>Create</guimenu> a proposal or
     <guimenu>Edit</guimenu> an existing one.
    </para>
    <para>
     Most &ostack; &barcl;s consist of two sections: the
     <guimenu>Attributes</guimenu> section lets you change the
     configuration, and the <guimenu>Node Deployment</guimenu> section lets
     you choose onto which nodes to deploy the &barcl;.
    </para>
   </step>
   <step>
    <para> To edit the <guimenu>Attributes</guimenu> section, change the values
     via the Web form. Alternatively you can directly edit the configuration
     file by clicking <guimenu>Raw</guimenu>. </para>
    <warning>
     <title>Raw Mode</title>
     <para> If you switch between <guimenu>Raw</guimenu> mode and Web form
       (<guimenu>Custom</guimenu> mode), make sure to <guimenu>Save</guimenu>
      your changes before switching, otherwise they will be lost. </para>
    </warning>
   </step>
   <step>
    <para>To assign nodes to a role, use the <guimenu>Deployment</guimenu>
     section of the &ostack; &barcl;. It shows the <guimenu>Available
      Nodes</guimenu> that you can assign to the roles belonging to the
     &barcl;. If the &barcl; contains roles that can also be deployed to
     a cluster and if you have deployed the Pacemaker &barcl;, the
      <guimenu>Deployment</guimenu> section of the &barcl; will additionally
     show a list of <guimenu>Available Clusters</guimenu> and of
      <guimenu>Available Remotes</guimenu>. The latter are clusters that contain
     both <quote>normal</quote> nodes and Pacemaker remote nodes. See <xref
      linkend="sec.depl.reg.ha.compute"/> for the basic details. </para>
    <para> One or more nodes are usually automatically pre-selected for
     available roles. If this pre-selection does not meet your requirements,
     click the <guimenu>Remove</guimenu> icon next to the role to remove the
     assignment. Assign a node or cluster of your choice by dragging the
     respective entry from the list of <guimenu>Available Nodes</guimenu>,
      <guimenu>Available Clusters</guimenu>, or <guimenu>Available
      Remotes</guimenu> to the desired role and dropping it onto the
      <emphasis>role name</emphasis>. Do <emphasis>not</emphasis> drop a node or
     cluster onto the text box&mdash;this is rather used to filter the list
     of available nodes or clusters!</para>
    <para>If you try to assign clusters or clusters with remote nodes to roles
     that can only be assigned to individual nodes, the &crow; &wi; will
     refuse to accept and show an error message. If you assign a cluster with
     remote nodes to a role that can only be applied to <quote>normal</quote>
     (Corosync) nodes, this role will only be applied to the Corosync nodes of
     that cluster but not to the remote nodes of the same cluster.</para>
   </step>
   <step>
    <para>
     To save and deploy your edits, click <guimenu>Apply</guimenu>. To save
     your changes without deploying them, click <guimenu>Save</guimenu>. To
     remove the complete proposal, click <guimenu>Delete</guimenu>. A
     proposal that already has been deployed can only be deleted manually,
     see <xref linkend="sec.depl.ostack.barclamps.delete"/> for details.
    </para>
    <para>
     If you deploy a proposal onto a node where a previous one is still
     active, the new proposal will overwrite the old one.
    </para>
    <note>
     <title>Wait Until a Proposal has been Deployed</title>
     <para>
      Deploying a proposal might take some time (up to several minutes). It
      is strongly recommended to always wait until you see the note
      <quote>Successfully applied the proposal</quote> before proceeding on
      to the next proposal.
     </para>
    </note>
   </step>
  </procedure>

  <warning>
   <title>&barcl; Deployment Failure</title>
   <para>
    In case the deployment of a &barcl; fails, make sure to fix the
    reason that has caused the failure and deploy the &barcl; again.
    Refer to the respective troubleshooting section at
    <xref linkend="sec.depl.trouble.faq.ostack"/> for help. A deployment
    failure may leave your node in an inconsistent state.
   </para>
  </warning>

  <sect2 xml:id="sec.depl.ostack.barclamps.delete">
   <title>Delete a Proposal that Already has been Deployed</title>
   <para>
    To delete a proposal that already has been deployed, you first need to
    <guimenu>Deactivate</guimenu> it in the &crow; Web interface.
    Deactivating a proposal will remove software and services having been
    deployed by this proposal from the affected nodes. After a proposal has
    been deactivated, you can <guimenu>Delete</guimenu> it in the &crow;
    Web interface and <guimenu>Create</guimenu> a new proposal from the
    &barcl; overview.
   </para>
  </sect2>

  <sect2 xml:id="sec.depl.ostack.barclamps.queues">
   <title>Queuing/Dequeuing Proposals</title>
   <para>
    When a proposal is applied to one or more nodes that are nor yet
    available for deployment (for example because they are rebooting or have
    not been fully installed, yet), the proposal will be put in a queue. A
    message like
   </para>
<screen>Successfully queued the proposal until the following become ready: d52-54-00-6c-25-44</screen>
   <para>
    will be shown when having applied the proposal. A new button
    <guimenu>Dequeue</guimenu> will also become available. Use it to cancel
    the deployment of the proposal by removing it from the queue.
   </para>
  </sect2>
 </sect1>

 <sect1 xml:id="sec.depl.ostack.pacemaker">
  <title>Deploying Pacemaker (Optional, &haSetup; Only)</title>

  <para>
   By setting up one or more clusters by deploying Pacemaker, you can make the
   &cloud; controller functions and the &compnode;s highly available (see <xref
   linkend="sec.depl.req.ha"/> for details). Since it is possible (and
   recommended) to deploy more than one cluster, a separate proposal needs to
   be created for each cluster.
  </para>

  <para>
   Deploying Pacemaker is optional. In case you do not want to deploy it,
   skip this section and start the node deployment by deploying the database
   as described in <xref linkend="sec.depl.ostack.db"/>.
  </para>

  <note>
   <title>Number of Cluster Nodes</title>
   <para>
    To set up a cluster, at least two nodes are required. If setting up a
    cluster for storage with replicated storage via DRBD (for example for a cluster for
    the database and RabbitMQ), exactly two nodes are required. For all
    other setups an odd number of nodes with a minimum of three nodes is
    strongly recommended. See <xref linkend="sec.depl.reg.ha.general"/> for
    more information.
   </para>
  </note>

  <para>
   To create a proposal, go to <menuchoice> <guimenu>Barclamps</guimenu>
   <guimenu>OpenStack</guimenu> </menuchoice> and click
   <guimenu>Edit</guimenu> for the Pacemaker &barcl;. A drop-down box
   where you can enter a name and a description for the proposal opens.
   Click <guimenu>Create</guimenu> to open the configuration screen for the
   proposal.
  </para>

  <informalfigure>
   <mediaobject>
    <imageobject role="fo">
     <imagedata fileref="depl_barclamp_pacemaker_proposal.png" width="100%" format="png"/>
    </imageobject>
    <imageobject role="html">
     <imagedata fileref="depl_barclamp_pacemaker_proposal.png" width="75%" format="png"/>
    </imageobject>
   </mediaobject>
  </informalfigure>

  <important xml:id="ann.depl.ostack.pacemaker.prop_name">
   <title>Proposal Name</title>
   <para>
    The name you enter for the proposal will be used to generate host names
    for the virtual IPs of HAProxy. The name uses the following scheme:
   </para>
<screen><replaceable>NAME</replaceable>.cluster-<replaceable>PROPOSAL_NAME</replaceable>.<replaceable>FQDN</replaceable></screen>
   <para>
    When <replaceable>PROPOSAL_NAME</replaceable> is set to <literal>data</literal>, this results
    in, for example,
    <literal>controller.cluster-data.&exampledomain;</literal>.
   <remark>taroth 2016-02-08: what about the first replaceable? who defines <replaceable>NAME</replaceable> and
    where to define it?</remark></para>
  </important>

  <para>
   The following options are configurable in the Pacemaker configuration
   screen:
  </para>

  <variablelist>
   <varlistentry>
    <term>Transport for Communication</term>
    <listitem>
     <para>
      Choose a technology used for cluster communication. You can select
      between <guimenu>Multicast (UDP)</guimenu>, (sending a message to
      multiple destinations) or <guimenu>Unicast (UDPU)</guimenu> (sending a
      message to a single destination). By default multicast is used.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><guimenu>Policy when cluster does not have quorum</guimenu>
    </term>
    <listitem>
     <para>
      Whenever communication fails between one or more nodes and the rest of
      the cluster a <quote>cluster partition</quote> occurs. The nodes of a
      cluster are split in partitions but are still active. They can only
      communicate with nodes in the same partition and are unaware of the
      separated nodes. The cluster partition that has the majority of nodes
      is defined to have <quote>quorum</quote>.
     </para>
     <para>
      This configuration option defines what to do with the cluster
      partition(s) that do not have the quorum. See
      <link xlink:href="https://www.suse.com/documentation/sle_ha/book_sleha/data/sec_ha_configuration_basics_global.html"/>,
      section <citetitle>Option no-quorum-policy</citetitle> for details.
     </para>
     <para>
      The recommended setting is to choose <guimenu>Stop</guimenu>. However,
      <guimenu>Ignore</guimenu> is enforced for two-node clusters to ensure
      that the remaining node continues to operate normally in case the
      other node fails. For clusters using shared resources, choosing
      <guimenu>freeze</guimenu> may be used to ensure that these resources
      continue to be available.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry xml:id="vle.pacemaker.barcl.stonith">
    <term>STONITH: Configuration mode for &stonith;
    </term>
    <listitem>
     <para>
      <quote>Misbehaving</quote> nodes in a cluster are shut down to prevent
      it from causing trouble. This mechanism is called &stonith;
      (<quote>Shoot the other node in the head</quote>). &stonith; can be
      configured in a variety of ways, refer to
      <link xlink:href="https://www.suse.com/documentation/sle_ha/book_sleha/data/cha_ha_fencing.html"/>
      for details. The following configuration options exist:
     </para>
     <variablelist>
      <varlistentry>
       <term><guimenu>Configured manually</guimenu>
       </term>
       <listitem>
        <para>
         &stonith; will not be configured when deploying the &barcl;.
         It needs to be configured manually as described in
         <link xlink:href="https://www.suse.com/documentation/sle_ha/book_sleha/data/sec_ha_fencing_config.html"/>.
         For experts only.
        </para>
       </listitem>
      </varlistentry>
      <varlistentry>
       <term><guimenu>Configured with IPMI data from the IPMI &barcl;</guimenu>
       </term>
       <listitem>
        <para>
         Using this option automatically sets up &stonith; with data
         received from the IPMI &barcl;. Being able to use this option
         requires that IPMI is configured for all cluster nodes. This should
         be done by default, when deploying cloud. To check or change the
         IPMI deployment, go to <menuchoice> <guimenu>Barclamps</guimenu>
         <guimenu>Crowbar</guimenu> <guimenu>IPMI</guimenu>
         <guimenu>Edit</guimenu> </menuchoice>. Also make sure the
         <guimenu>Enable BMC</guimenu> option is set to
         <guimenu>true</guimenu> on this &barcl;.
        </para>
        <important>
         <title>&stonith; Devices Must Support IPMI</title>
         <para>
          To configure &stonith; with the IPMI data,
          <emphasis>all</emphasis> &stonith; devices must support IPMI.
          Problems with this setup may occur with IPMI implementations that
          are not strictly standards compliant. In this case it is
          recommended to set up &stonith; with &stonith; block devices
          (SBD).
         </para>
        </important>
       </listitem>
      </varlistentry>
      <varlistentry>
       <term><guimenu>Configured with &stonith; Block Devices (SBD)</guimenu>
       </term>
       <listitem>
        <para>
         This option requires to manually set up shared storage
<!-- and a watchdog -->
         on the cluster nodes before applying the proposal. To do so,
         proceed as follows:
        </para>
        <orderedlist spacing="normal">
         <listitem>
          <para>
           Prepare the shared storage. It needs to be reachable by all nodes
           and must not use host-based RAID, cLVM2, or DRBD.
          </para>
         </listitem>
         <listitem>
          <para>
           Install the package <systemitem class="resource">sbd</systemitem>
           on all cluster nodes.
          </para>
         </listitem>
         <listitem>
          <para>
           Initialize SBD device with by running the following command. Make
           sure to replace <replaceable>/dev/SBD</replaceable> with the path
           to the shared storage device.
          </para>
<screen>sbd -d <replaceable>/dev/SBD</replaceable> create</screen>
          <para>
           Refer to
           <link xlink:href="https://www.suse.com/documentation/sle_ha/book_sleha/data/sec_ha_storage_protect_fencing.html#pro_ha_storage_protect_sbd_create"/>
           for details.
          </para>
         </listitem>
        </orderedlist>
        <para>
         After the shared storage has been set up, specify the path using
         the <quote>by-id</quote> notation
         (<filename>/dev/disk/by-id/<replaceable>DEVICE</replaceable></filename>).
         It is possible to specify multiple paths as a comma-separated list.
        </para>
        <para>
         Deploying the &barcl; will automatically complete the SBD setup
         on the cluster nodes by starting the SBD daemon and configuring the
         fencing resource.
        </para>
       </listitem>
      </varlistentry>
      <varlistentry>
       <term><guimenu>
         Configured with one shared resource for the whole cluster
        </guimenu>
       </term>
       <listitem>
        <para>
         All nodes will use the exact same configuration. Specify the
         <guimenu>Fencing Agent</guimenu> to use and enter
         <guimenu>Parameters</guimenu> for the agent.
        </para>
        <para>
         To get a list of &stonith; devices which are supported by the
         High Availability Extension, run the following command on an
         already installed cluster nodes: <command>stonith -L</command>. The
         list of parameters depends on the respective agent. To view a list
         of parameters use the following command: <command>stonith
         -t</command> <replaceable>agent</replaceable> -n.
        </para>
       </listitem>
      </varlistentry>
      <varlistentry>
       <term><guimenu>Configured with one resource per node</guimenu>
       </term>
       <listitem>
        <para>
         All nodes in the cluster use the same <guimenu>Fencing
         Agent</guimenu>, but can be configured with different parameters.
         This setup is, for example, required when nodes are in different
         chassis and therefore need different ILO parameters.
        </para>
        <para>
         To get a list of &stonith; devices which are supported by the
         High Availability Extension, run the following command on an
         already installed cluster nodes: <command>stonith -L</command>. The
         list of parameters depends on the respective agent. To view a list
         of parameters use the following command: <command>stonith
         -t</command> <replaceable>agent</replaceable> -n.
        </para>
       </listitem>
      </varlistentry>
      <varlistentry>
       <term><guimenu>Configured for nodes running in libvirt</guimenu>
       </term>
       <listitem>
        <para>
         Use this setting for completely virtualized test installations.
         This option is not supported.
        </para>
       </listitem>
      </varlistentry>
     </variablelist>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>STONITH: Do not start corosync on boot after fencing</term>
    <listitem>
     <para>
      With &stonith;, Pacemaker clusters with two nodes may sometimes hit
      an issue known as &stonith; deathmatch where each node kills the
      other one, resulting in both nodes rebooting all the time. Another
      similar issue in Pacemaker clusters is the fencing loop, where a
      reboot caused by &stonith; will not be enough to fix a node and it
      will be fenced again and again.
     </para>
     <para>
      This setting can be used to limit these issues. When set to
      <guimenu>true</guimenu>, a node that has not been properly shut down
      or rebooted will not start the services for Pacemaker on boot, and
      will wait for action from the &cloud; operator. When set to
      <guimenu>false</guimenu>, the services for Pacemaker will always be
      started on boot. The <guimenu>Automatic</guimenu> value is used to
      have the most appropriate value automatically picked: it will be
      <guimenu>true</guimenu> for two-node clusters (to avoid &stonith;
      deathmatches), and <guimenu>false</guimenu> otherwise.
     </para>
     <para>
      When a node will boot but not start corosync because of this setting,
      then the node will be displayed with its status set to
      "<literal>Problem</literal>" (red bullet) in the <guimenu>Node
      Dashboard</guimenu>. To make this node usable again, the following
      steps need to be performed:
     </para>
     <orderedlist spacing="normal">
      <listitem>
       <para>
        Connect to the node via SSH from the &admserv; and run one of
        <command>rcopenais start</command> or <command>rm
        /var/spool/corosync/block_automatic_start</command>. Waiting for the
        next periodic of a <command>chef-client</command> run, or manually
        running <command>chef-client</command> is also recommended.
       </para>
      </listitem>
      <listitem>
       <para>
        On the &admserv;, run the following command to update the status
        of the node specified with <replaceable>NODE</replaceable>.
       </para>
<screen>crowbar crowbar transition <replaceable>NODE</replaceable> ready</screen>
      </listitem>
     </orderedlist>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>Mail Notifications: Enable Mail Notifications</term>
    <listitem>
     <para>
      Get notified of cluster node failures via e-mail. If set to
      <guimenu>true</guimenu>, you need to specify which <guimenu>SMTP
      Server</guimenu> to use, a prefix for the mails' subject and sender
      and recipient addresses. Note that the SMTP server must be accessible
      by the cluster nodes.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>DRBD: Prepare Cluster for DRBD</term>
    <listitem>
     <para>
      Set up DRBD for replicated storage on the cluster. This option
      requires a two-node cluster with a spare hard disk for each node. The
      disks should have a minimum size of 100 GB. Using DRBD is recommended
      for making the database and RabbitMQ highly available. For other
      clusters, set this option to <guimenu>False</guimenu>.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><guimenu>HAProxy: Public name for public virtual IP</guimenu>
    </term>
    <listitem>
     <para>
      The public name is the host name that will be used instead of the
      generated public name (see
      <xref linkend="ann.depl.ostack.pacemaker.prop_name"/>) for the public
      virtual IP of &haproxy; (when registering public endpoints, for
      instance). Any name specified here needs to be resolved by a name
      server placed outside of the &cloud; network.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><guimenu>Pacemaker GUI: Password for hacluster user in &hawk;</guimenu>
    </term>
    <listitem>
     <para>
      The password for the user
      <systemitem class="username">hacluster</systemitem>. The default value
      is <literal>crowbar</literal>.
     </para>
    </listitem>
   </varlistentry>
  </variablelist>

  <figure>
   <title>The Pacemaker &Barcl;</title>
   <mediaobject>
    <imageobject role="fo">
     <imagedata fileref="depl_barclamp_pacemaker.png" width="100%" format="png"/>
    </imageobject>
    <imageobject role="html">
     <imagedata fileref="depl_barclamp_pacemaker.png" width="75%" format="png"/>
    </imageobject>
   </mediaobject>
  </figure>

  <para>
   The Pacemaker service consists of the following roles. Deploying the
   <guimenu>hawk-server</guimenu> role is optional:
  </para>

  <variablelist>
   <varlistentry>
    <term><guimenu>pacemaker-cluster-member</guimenu>
    </term>
    <listitem>
     <para>
      Deploy this role on all nodes that should become member of the cluster
      except for the one where <guimenu>pacemaker-cluster-founder</guimenu>
      is deployed.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><guimenu>hawk-server</guimenu>
    </term>
    <listitem>
     <para>
      Deploying this role is optional. If deployed, sets up the &hawk;
      Web interface which lets you monitor the status of the cluster. The
      Web interface can be accessed via
      <literal>http://<replaceable>IP-ADDRESS</replaceable>:7630</literal>.
      Note that the GUI on &cloud; can only be used to monitor the
      cluster status and not to change its configuration.
     </para>
     <para>
      <guimenu>hawk-server</guimenu> may be deployed on at least one cluster
      node. It is recommended to deploy it on all cluster nodes.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>pacemaker-remote</term>
    <listitem>
     <para>Deploy this role on all nodes that should become members of the
      &compnode;s cluster. They will run as Pacemaker remote nodes that are part
      of the cluster, but do not take part in quorum. Instead of the complete
      cluster stack, only the <literal>pacemaker-remote</literal> service will be
      installed on this nodes.
      </para>
    </listitem>
   </varlistentry>
  </variablelist>

  <figure>
   <title>The Pacemaker &Barcl;: Node deployment Example</title>
   <mediaobject>
    <imageobject role="fo">
     <imagedata fileref="depl_barclamp_pacemaker_node_deployment.png" width="100%" format="png"/>
    </imageobject>
    <imageobject role="html">
     <imagedata fileref="depl_barclamp_pacemaker_node_deployment.png" width="75%" format="png"/>
    </imageobject>
   </mediaobject>
  </figure>

  <para>
   After a cluster has been successfully deployed, it is listed under
   <guimenu>Available Clusters</guimenu> in the
   <guimenu>Deployment</guimenu> section and can be used for role deployment
   like a regular node.
  </para>

  <warning>
   <title>Deploying Roles on Single Cluster Nodes</title>
   <para>
    When using clusters, roles from other &barcl;s must never be deployed
    to single nodes that are already part of a cluster. The only exceptions
    from this rule are the following roles:
   </para>
   <itemizedlist mark="bullet" spacing="normal">
    <listitem>
     <para>
      cinder-volume
     </para>
    </listitem>
    <listitem>
     <para>
      swift-proxy + swift-dispersion
     </para>
    </listitem>
    <listitem>
     <para>
      swift-ring-compute
     </para>
    </listitem>
    <listitem>
     <para>
      swift-storage
     </para>
    </listitem>
   </itemizedlist>
  </warning>

  <figure>
   <title>Available Clusters in the Deployment Section</title>
   <mediaobject>
    <imageobject role="fo">
     <imagedata fileref="depl_barclamp_database_cluster_deployment.png" width="100%" format="png"/>
    </imageobject>
    <imageobject role="html">
     <imagedata fileref="depl_barclamp_database_cluster_deployment.png" width="75%" format="png"/>
    </imageobject>
   </mediaobject>
  </figure>
  <important>
   <title>Service Management on the Cluster</title>
   <para>
    After a role has been deployed on a cluster, its services are managed by
    the HA software. You must <emphasis>never ever</emphasis> manually start
    or stop an HA-managed service or configure it to start on boot (for
    example by running <command>insserv</command> for the service). Services
    may only be started or stopped by using the cluster management tools Hawk,
    the Pacemaker GUI or the crm shell. See
    <link xlink:href="https://www.suse.com/documentation/sle_ha/book_sleha/data/sec_ha_configuration_basics_resources.html"/>
    for more information.
   </para>
  </important>
  <note>
   <title>Testing the Cluster Setup</title>
   <para>
    To check whether all cluster resources are running, either use the
    &hawk; Web interface or run the command <command>crm_mon</command>
    <option>-1r</option>. If it is not the case, clean up the respective
    resource with <command>crm</command> <option>resource</option>
    <option>cleanup</option> <replaceable>RESOURCE</replaceable> , so it
    gets respawned.
   </para>
   <para>
    Also make sure that &stonith; correctly works before continuing with
    the &cloud; setup. This is especially important when having chosen a
    &stonith; configuration requiring manual setup. To test if
    &stonith; works, log in to a node on the cluster and run the
    following command:
   </para>
<screen>pkill -9 corosync</screen>
   <para>
    In case &stonith; is correctly configured, the node will reboot.
   </para>
   <para>
    Before testing on a production cluster, plan a maintenance window in
    case issues should arise.
   </para>
  </note>
 </sect1>
 <sect1 xml:id="sec.depl.ostack.db">
  <title>Deploying the Database</title>

  <para>
   The very first service that needs to be deployed is the
   <guimenu>Database</guimenu>. The database service is using PostgreSQL and
   is used by all other services. It must be installed on a &contrnode;.
   The Database can be made highly available by deploying it on a cluster.
  </para>

  <remark condition="clarity">
   2014-03-28 - fs: How to set up shared storage or DRBD for the data?
  </remark>

  <para>
   The only attribute you may change is the maximum number of database
   connections (<guimenu>Global Connection Limit </guimenu>). The default
   value should usually work&mdash;only change it for large deployments
   in case the log files show database connection failures.
  </para>

  <figure>
   <title>The Database &Barcl;</title>
   <mediaobject>
    <imageobject role="fo">
     <imagedata fileref="depl_barclamp_database.png" width="100%" format="png"/>
    </imageobject>
    <imageobject role="html">
     <imagedata fileref="depl_barclamp_database.png" width="75%" format="png"/>
    </imageobject>
   </mediaobject>
  </figure>

  <sect2 xml:id="sec.depl.ostack.db.ha">
   <title>&haSetup; for the Database</title>
   <para>
    To make the database highly available, deploy it on a cluster rather
    than on a single &contrnode;. This also makes it necessary to provide
    shared storage for the cluster that hosts the database data. This can
    either be achieved by setting up a cluster with DRBD support (see
    <xref linkend="sec.depl.ostack.pacemaker"/>) or by using
    <quote>traditional</quote> shared storage like an NFS share. It is
    recommended to use a dedicated cluster to deploy the database together
    with RabbitMQ, since both services require shared storage.
   </para>
   <para>
    Deploying the database on a cluster makes an additional <guimenu>High
    Availability</guimenu> section available in the
    <guimenu>Attributes</guimenu> section of the proposal. Configure the
    <guimenu>Storage Mode</guimenu> in this section. There are two options:
   </para>
   <variablelist>
    <varlistentry>
     <term><guimenu>DRBD</guimenu>
     </term>
     <listitem>
      <para>
       This option requires a two-node cluster that has been set up with
       DRBD. Also specify the <guimenu>Size to Allocate for DRBD Device (in
       Gigabytes)</guimenu>. The suggested value of 50 GB should be
       sufficient.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>Shared Storage</term>
     <listitem>
      <para>
       Use a shared block device or an NFS mount for shared storage.
       Concordantly with the mount command, you need to specify three
       attributes: <guimenu>Name of Block Device or NFS Mount
       Specification</guimenu> (the mount point), the <guimenu>Filesystem
       Type</guimenu> and the <guimenu>Mount Options</guimenu>. Refer to
       <command>man 8 mount</command> for details on file system types and
       mount options.
      </para>
     </listitem>
    </varlistentry>
   </variablelist>

   <important>
    <title>NFS Export Options for Shared Storage</title>
    <para>
     An NFS share that is to be used as a shared storage for a cluster needs
     to be exported on the NFS server with the following options:
    </para>
    <screen>rw,async,insecure,no_subtree_check,no_root_squash</screen>
    <para>
     In case mounting the NFS share on the cluster nodes fails, change the
     export options and re-apply the proposal. Before doing so, however, you
     need to clean up the respective resources on the cluster nodes as
     described in <link xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://www.suse.com/documentation/sle_ha/book_sleha/data/sec_ha_config_gui_manage.html"/>.
    </para>
   </important>

   <important>
    <title>Ownership of a Shared NFS Directory</title>
    <para>
     The shared NFS directory that is used for the PostgreSQL database needs
     to be owned by the same user ID and group ID as of the
     <systemitem class="username">postgres</systemitem> user on the HA
     database cluster.
    </para>
    <para>
     To get the IDs log in to one of the HA database cluster machines and
     issue the following commands:
    </para>
<screen>id -g postgres
getent group postgres | cut -d: -f3</screen>
    <para>
     The first command returns the numeric user ID, the second one the
     numeric group ID. Now log in to the NFS server and change the ownership
     of the shared NFS directory, for example:
    </para>
<screen>chown <replaceable>UID</replaceable>.<replaceable>GID</replaceable> /exports/cloud/db</screen>
    <para>
     Replace <replaceable>UID</replaceable> and
     <replaceable>GID</replaceable> by the respective numeric values
     retrieved above.
    </para>
   </important>
   <warning>
    <title>Re-Deploying &cloud; with Shared Storage</title>
    <para>
     When re-deploying &cloud; and reusing a shared storage hosting a
     database files from a previous installation, the installation may fail,
     because the old database will be used. Always delete the old database
     from the shared storage before re-deploying &cloud;.
    </para>
   </warning>
  </sect2>
 </sect1>
 <sect1 xml:id="sec.depl.ostack.rabbit">
  <title>Deploying RabbitMQ</title>

  <para>
   The RabbitMQ messaging system enables services to communicate with the
   other nodes via Advanced Message Queue Protocol (AMQP). Deploying it is
   mandatory. RabbitMQ needs to be installed on a &contrnode;. RabbitMQ
   can be made highly available by deploying it on a cluster.It is
   recommended not to change the default values of the proposal's
   attributes.
  </para>

  <variablelist>
   <varlistentry>
    <term><guimenu>Virtual Host</guimenu>
    </term>
    <listitem>
     <para>
      Name of the default virtual host to be created and used by the
      RabbitMQ server (<literal>default_vhost</literal> configuration option
      in <filename>rabbitmq.config</filename>).
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>Port</term>
    <listitem>
     <para>
      Port the RabbitMQ server listens on (<literal>tcp_listeners</literal>
      configuration option in <filename>rabbitmq.config</filename>).
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>User</term>
    <listitem>
     <para>
      RabbitMQ default user (<literal>default_user</literal> configuration
      option in <filename>rabbitmq.config</filename>).
     </para>
    </listitem>
   </varlistentry>
  </variablelist>

  <figure>
   <title>The RabbitMQ &Barcl;</title>
   <mediaobject>
    <imageobject role="fo">
     <imagedata fileref="depl_barclamp_rabbitmq.png" width="100%" format="png"/>
    </imageobject>
    <imageobject role="html">
     <imagedata fileref="depl_barclamp_rabbitmq.png" width="75%" format="png"/>
    </imageobject>
   </mediaobject>
  </figure>

  <sect2 xml:id="sec.depl.ostack.rabbit.ha">
   <title>&haSetup; for RabbitMQ</title>
   <para>
    To make RabbitMQ highly available, deploy it on a cluster rather than on
    a single &contrnode;. This also makes it necessary to provide shared
    storage for the cluster that hosts the RabbitMQ data. This can either be
    achieved by setting up a cluster with DRBD support (see
    <xref linkend="sec.depl.ostack.pacemaker"/>) or by using
    <quote>traditional</quote> shared storage like an NFS share. It is
    recommended to use a dedicated cluster to deploy RabbitMQ together with
    the database, since both services require shared storage.
   </para>
   <para>
    Deploying RabbitMQ on a cluster makes an additional <guimenu>High
    Availability</guimenu> section available in the
    <guimenu>Attributes</guimenu> section of the proposal. Configure the
    <guimenu>Storage Mode</guimenu> in this section. There are two options:
   </para>
   <variablelist>
    <varlistentry>
     <term><guimenu>DRBD</guimenu>
     </term>
     <listitem>
      <para>
       This option requires a two-node cluster that has been set up with
       DRBD. Also specify the <guimenu>Size to Allocate for DRBD Device (in
       Gigabytes)</guimenu>. The suggested value of 50 GB should be
       sufficient.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>Shared Storage</term>
     <listitem>
      <para>
       Use a shared block device or an NFS mount for shared storage.
       Concordantly with the mount command, you need to specify three
       attributes: <guimenu>Name of Block Device or NFS Mount
       Specification</guimenu> (the mount point), the <guimenu>Filesystem
       Type</guimenu> and the <guimenu>Mount Options</guimenu>.
      </para>
     </listitem>
    </varlistentry>
   </variablelist>

   <important>
    <title>NFS Export Options for Shared Storage</title>
    <para>
     An NFS share that is to be used as a shared storage for a cluster needs
     to be exported on the NFS server with the following options:
    </para>
    <screen>rw,async,insecure,no_subtree_check,no_root_squash</screen>
    <para>
     In case mounting the NFS share on the cluster nodes fails, change the
     export options and re-apply the proposal. Before doing so, however, you
     need to clean up the respective resources on the cluster nodes as
     described in <link xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://www.suse.com/documentation/sle_ha/book_sleha/data/sec_ha_config_gui_manage.html"/>.
    </para>
   </important>

  </sect2>
 </sect1>
 <sect1 xml:id="sec.depl.ostack.keystone">
  <title>Deploying &o_ident;</title>

  <para>
   <guimenu>Keystone</guimenu> is another core component that is used by all
   other &ostack; services. It provides authentication and authorization
   services. <guimenu>Keystone</guimenu> needs to be installed on a
   &contrnode;. &o_ident; can be made highly available by deploying it
   on a cluster. You can configure the following parameters of this
   &barcl;:
  </para>

  <variablelist>
   <varlistentry>
    <term><guimenu>Algorithm for Token Generation</guimenu>
    </term>
    <listitem>
     <para>
      Set the algorithm used by &o_ident; to generate the tokens. It is
      strongly recommended to use <literal>PKI</literal>, since it will
      reduce network traffic.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><guimenu>Region Name</guimenu>
    </term>
    <listitem>
     <para>
      Allows to customize the region name that crowbar is going to manage.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><guimenu>Default Credentials: Default Tenant</guimenu>
    </term>
    <listitem>
     <para>
      Tenant for the users. Do not change the default value of
      <literal>openstack</literal>.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>
     <guimenu>
      Default Credentials: Regular User/Administrator User Name/Password
     </guimenu>
    </term>
    <listitem>
     <para>
      User name and password for the regular user and the administrator.
      Both accounts can be used to log in to the &cloud; &dash; to
      manage &o_ident; users and access.
     </para>
     <figure>
      <title>The &o_ident; &Barcl;</title>
      <mediaobject>
       <imageobject role="fo">
        <imagedata fileref="depl_barclamp_keystone.png" width="100%" format="png"/>
       </imageobject>
       <imageobject role="html">
        <imagedata fileref="depl_barclamp_keystone.png" width="75%" format="png"/>
       </imageobject>
      </mediaobject>
     </figure>
    </listitem>
   </varlistentry>
   <varlistentry xml:id="sec.depl.ostack.keystone.ssl">
    <term>SSL Support: Protocol
    </term>
    <listitem>
     <para>
      When sticking with the default value <guimenu>HTTP</guimenu>, public
      communication will not be encrypted. Choose <guimenu>HTTPS</guimenu>
      to use SSL for encryption. See <xref linkend="sec.depl.req.ssl"/> for
      background information and
      <xref linkend="sec.depl.inst.nodes.post.ssl"/> for installation
      instructions. The following additional configuration options will
      become available when choosing <guimenu>HTTPS</guimenu>:
     </para>
     <variablelist>
      <varlistentry>
       <term><guimenu>Generate (self-signed) certificates</guimenu>
       </term>
       <listitem>
        <para>
         When set to <literal>true</literal>, self-signed certificates are
         automatically generated and copied to the correct locations. This
         setting is for testing purposes only and should never be used in
         production environments!
        </para>
       </listitem>
      </varlistentry>
      <varlistentry>
       <term><guimenu>SSL Certificate File</guimenu> / <guimenu>SSL (Private) Key
        File</guimenu>
       </term>
       <listitem>
        <para>
         Location of the certificate key pair files.
        </para>
       </listitem>
      </varlistentry>
      <varlistentry>
       <term><guimenu>SSL Certificate is insecure</guimenu>
       </term>
       <listitem>
        <para>
         Set this option to <literal>true</literal> when using self-signed
         certificates to disable certificate checks. This setting is for
         testing purposes only and should never be used in production
         environments!
        </para>
       </listitem>
      </varlistentry>
      <varlistentry>
       <term><guimenu>Require Client Certificate</guimenu>
       </term>
       <listitem>
        <para>
         Set this option to <literal>true</literal> when using your own
         certificate authority (CA) for signing. Having done so, you also
         need to specify a path to the <guimenu>CA Certificates
         File</guimenu>. If your certificates are signed by a trusted third
         party organization, <guimenu>Require Client Certificate</guimenu>
         should be set to <guimenu>false</guimenu>, since the
         <quote>official</quote> certificate authorities (CA) are already
         known by the system.
        </para>
       </listitem>
      </varlistentry>
      <varlistentry>
       <term><guimenu>SSL CA Certificates File</guimenu></term>
       <listitem>
        <para>
         Specify the absolute path to the CA certificate here. This option can
         only be changed if <guimenu>Require Client Certificate</guimenu> was
         set to <literal>true</literal>.
        </para>
        <figure>
         <title>The SSL Dialog</title>
         <mediaobject>
          <imageobject role="fo">
           <imagedata fileref="depl_barclamp_ssl.png" width="100%" format="png"/>
          </imageobject>
          <imageobject role="html">
           <imagedata fileref="depl_barclamp_ssl.png" width="75%" format="png"/>
          </imageobject>
         </mediaobject>
        </figure>
       </listitem>
      </varlistentry>
     </variablelist>
    </listitem>
   </varlistentry>
  </variablelist>

  <sect2 xml:id="sec.depl.ostack.keystone.ldap">
   <title>LDAP Authentication with &o_ident;</title>
   <para>
    <remark condition="clarity">
     2015-01-21 - fs: TODO: bsc #914070
    </remark>
    By default &o_ident; uses an SQL database back-end store for
    authentication. LDAP can be used in addition to the default or as an
    alternative. Using LDAP requires the &contrnode; on which
    &o_ident; is installed to be able to contact the LDAP server. See
    <xref linkend="app.deploy.network_json"/> for instructions on how to
    adjust the network setup.
   </para>
   <sect3 xml:id="sec.depl.ostack.keystone.ldap.ldap">
    <title>Using LDAP for Authentication</title>
    <para>
     To configure LDAP as an alternative to the SQL database back-end store,
     you need to open the &o_ident; &barcl; <guimenu>Attribute
     </guimenu>configuration in <guimenu>Raw</guimenu> mode. Search for the
     <guimenu>ldap</guimenu> section.
    </para>
    <figure xml:id="fig.keystone.ldap">
     <title>The &o_ident; &Barcl;: Raw Mode</title>
     <mediaobject>
      <imageobject role="fo">
       <imagedata fileref="depl_barclamp_keystone_raw.png" width="100%" format="png"/>
      </imageobject>
      <imageobject role="html">
       <imagedata fileref="depl_barclamp_keystone_raw.png" width="75%" format="png"/>
      </imageobject>
     </mediaobject>
    </figure>
    <para>
     Adjust the settings according to your LDAP setup. The default
     configuration does not include all attributes that can be
     set&mdash;a complete list of options is available in the file
     <filename>/opt/dell/chef/data_bags/crowbar/bc-template-keystone.schema</filename>
     on the &admserv; (search for <literal>ldap</literal>). There are
     three types of attribute values: strings (for example, the value for
     <literal>url</literal>:<literal>"ldap://localhost"</literal>), bool
     (for example, the value for <literal>use_dumb_member</literal>:
     <literal>false</literal>) and integer for example, the value for
     <literal>page_size</literal>: <literal>0</literal>). Attribute names
     and string values always need to be quoted with double quotes; bool and
     integer values must not be quoted.
    </para>
    <important>
     <title>Using LDAP over SSL (ldaps) is recommended</title>
     <para>
      In a production environment, it is recommended to use LDAP over SSL
      (ldaps), otherwise passwords will be transferred as plain text.
     </para>
    </important>
   </sect3>
   <sect3 xml:id="sec.depl.ostack.keystone.ldap.hybrid">
    <title>Using Hybrid Authentication</title>
    <para>
     The Hybrid LDAP back-end allows to create a mixed LDAP/SQL setup. This
     is especially useful when an existing LDAP server should be used to
     authenticate cloud users. The system and service users (administrators
     and operators) needed to set up and manage &cloud; will be managed
     in the local SQL database. Assignments of users to projects and roles
     will also be stored in the local database.
    </para>
    <para>
     In this scenario the LDAP Server can be read-only for &cloud;
     installation and no Schema modifications are required. Therefore
     managing LDAP users from within &cloud; is not possible and needs to
     be done using your established tools for LDAP user management. All user
     that are create with the &o_ident; command line client or the
     Horizon Web UI will be stored in the local SQL database.
    </para>
    <para>
     To configure hybrid authentication, proceed as follows:
    </para>
    <procedure>
     <step>
      <para>
       Open the &o_ident; &barcl; <guimenu>Attribute
       </guimenu>configuration in <guimenu>Raw</guimenu> mode (see
       <xref linkend="fig.keystone.ldap"/>).
      </para>
     </step>
     <step>
      <para>
       Set the identity and assignment drivers to the hybrid back-end:
      </para>
<screen> "identity": {
    "driver": "keystone.identity.backends.hybrid.Identity"
  },
  "assignment": {
    "driver": "keystone.assignment.backends.hybrid.Assignment"
  }</screen>
     </step>
     <step>
      <para>
       Adjust the settings according to your LDAP setup in the
       <guimenu>ldap</guimenu> section. Since the LDAP backend is only used
       to acquire information on users (but not on projects and roles),
       only the user related settings matter here. See the following
       example of settings that may need to be adjusted:
      </para>
<screen>  "ldap": {
    "url": "ldap://localhost",
    "user": "",
    "password": "",
    "suffix": "cn=example,cn=com",
    "user_tree_dn": "cn=example,cn=com",
    "query_scope": "one",
    "user_id_attribute": "cn",
    "user_enabled_emulation_dn": "",
    "tls_req_cert": "demand",
    "user_attribute_ignore": "tenant_id,tenants",
    "user_objectclass": "inetOrgPerson",
    "user_mail_attribute": "mail",
    "user_filter": "",
    "use_tls": false,
    "user_allow_create": false,
    "user_pass_attribute": "userPassword",
    "user_enabled_attribute": "enabled",
    "user_enabled_default": "True",
    "page_size": 0,
    "tls_cacertdir": "",
    "tls_cacertfile": "",
    "user_enabled_mask": 0,
    "user_allow_update": true,
    "group_allow_update": true,
    "user_enabled_emulation": false,
    "user_name_attribute": "cn"
  }</screen>
      <para>
       To access the LDAP server anonymously, leave the values for
       <guimenu>user</guimenu> and <guimenu>password</guimenu> empty.
      </para>
     </step>
    </procedure>
   </sect3>
  </sect2>

  <sect2 xml:id="sec.depl.ostack.keystone.ha">
   <title>&haSetup; for &o_ident;</title>
   <para>
    Making &o_ident; highly available requires no special
    configuration&mdash;it is sufficient to deploy it on a cluster.
   </para>
  </sect2>
 </sect1>
 <sect1 xml:id="sec.depl.ostack.ceph">
  <title>Deploying &ceph; (optional)</title>

  <para>
   &ceph; adds a redundant block storage service to &cloud;. It lets
   you store persistent devices that can be mounted from &vmguest;s. It
   offers high data security by storing the data redundantly on a pool of
   &stornode;s&mdash;therefore &ceph; needs to be installed on at
   least three dedicated nodes. All &ceph; nodes need to run &slsa;
   12&mdash;starting with &productname; 5, deploying &ceph; on
   &slsa; 11 SP3 nodes is no longer possible. For detailed information on
   how to provide the required repositories, refer to
   <xref linkend="sec.depl.adm_conf.repos.scc"/>. For &ceph; at least
   four nodes are required. If deploying the optional Calamari server for
   &ceph; management and monitoring, an additional node is required.
  </para>

  <para>
   For more information on the &ceph; project, visit
   <link xlink:href="http://ceph.com/"/>.
  </para>

  <para>
   The &ceph; &barcl; has the following configuration options:
  </para>

  <variablelist>
   <varlistentry>
    <term><guimenu>Disk Selection Method</guimenu>
    </term>
    <listitem>
     <para>
      Choose whether to only use the first available disk or all available
      disks. <quote>Available disks</quote> are all disks currently not used
      by the system. Note that one disk (usually
      <filename>/dev/sda</filename>) of every block storage node is already
      used for the operating system and is not available for &ceph;.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><guimenu>Number of Replicas of an Object</guimenu></term>
    <listitem>
     <para>
      For data security, stored objects are not only stored once, but
      redundantly. Specify the number of copies that should be stored for each
      object with this setting. The number includes the object itself. If you
      for example want the object plus two copies, specify 3.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><guimenu>SSL Support for RadosGW</guimenu>
    </term>
    <listitem>
     <para>
      Choose whether to encrypt public communication
      (<guimenu>HTTPS</guimenu>) or not (<guimenu>HTTP</guimenu>). If
      choosing <guimenu>HTTPS</guimenu>, you need to specify the locations for
      the certificate key pair files.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><guimenu>Calamari Credentials</guimenu>
    </term>
    <listitem>
     <para>
      Calamari is a Web front-end for managing and analyzing the &ceph;
      cluster. Provide administrator credentials (user name, password,
      e-mail address) in this section. Once &ceph; has bee deployed you
      can log in to Calamari with these credentials. Deploying Calamari is
      optional&mdash;leave these input fields empty when not deploying
      Calamari.
     </para>
    </listitem>
   </varlistentry>
  </variablelist>

  <figure>
   <title>The &ceph; &Barcl;</title>
   <mediaobject>
    <imageobject role="fo">
     <imagedata fileref="depl_barclamp_ceph.png" width="100%" format="png"/>
    </imageobject>
    <imageobject role="html">
     <imagedata fileref="depl_barclamp_ceph.png" width="75%" format="png"/>
    </imageobject>
   </mediaobject>
  </figure>

  <para>
   The &ceph; service consists of the following different roles:
  </para>

  <variablelist>
   <varlistentry>
    <term><guimenu>ceph-osd</guimenu>
    </term>
    <listitem>
     <para>
      The virtual block storage service. Install this role on all dedicated
      &ceph; &stornode;s (at least three), but not on any other node.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><guimenu>ceph-mon</guimenu>
    </term>
    <listitem>
     <para>
      Cluster monitor daemon for the &ceph; distributed file system.
      <guimenu>ceph-mon</guimenu> needs to be installed on three or five
      &stornode;s running <guimenu>ceph-osd</guimenu>.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><guimenu>ceph-calamari</guimenu>
    </term>
    <listitem>
     <para>
      Sets up the Calamari Web interface which lets you manage the &ceph;
      cluster. Deploying it is optional. The Web interface can be accessed via
      http://<replaceable>IP-ADDRESS</replaceable>/ (where
      <replaceable>IP-ADDRESS</replaceable> is the address of the machine
      where <guimenu>ceph-calamari</guimenu> is deployed on).
      <guimenu>ceph-calamari</guimenu> needs to be installed on a dedicated
      node&mdash;it is <emphasis>not</emphasis> possible to install it on a
      nodes running other services.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><guimenu>ceph-radosgw</guimenu>
    </term>
    <listitem>
     <para>
      The HTTP REST gateway for &ceph;. Install it on a &stornode;
      running <guimenu>ceph-osd</guimenu>.
     </para>
    </listitem>
   </varlistentry>
  </variablelist>

  <important>
   <title>&ceph; Needs Dedicated Nodes</title>
   <para>
    Never deploy on a node that runs non-&ceph; &ostack; services. The
    only services that may be deployed together on a &ceph; node, are
    <guimenu>ceph-osd</guimenu>, <guimenu>ceph-mon</guimenu> and
    <guimenu>ceph-radosgw</guimenu>. All &ceph; nodes need to run
    &slsa; 12&mdash;starting with &productname; 5, deploying
    &ceph; on &slsa; 11 SP3 nodes is no longer possible.
   </para>
  </important>

  <figure>
   <title>The &ceph; &Barcl;: Node Deployment Example</title>
   <mediaobject>
    <imageobject role="fo">
     <imagedata fileref="depl_barclamp_ceph_node_deployment.png" width="100%" format="png"/>
    </imageobject>
    <imageobject role="html">
     <imagedata fileref="depl_barclamp_ceph_node_deployment.png" width="75%" format="png"/>
    </imageobject>
   </mediaobject>
  </figure>

  <sect2 xml:id="sec.depl.ostack.ceph.ha">
   <title>&haSetup; for &ceph;</title>
   <para>
    &ceph; is HA-enabled by design, so there is no need for a special
    &hasetup;.
   </para>
  </sect2>
 </sect1>
 <sect1 xml:id="sec.depl.ostack.swift">
  <title>Deploying &o_objstore; (optional)</title>

  <para>
   &o_objstore; adds an object storage service to &cloud; that lets
   you store single files such as images or snapshots. It offers high data
   security by storing the data redundantly on a pool of
   &stornode;s&mdash;therefore &o_objstore; needs to be installed
   on at least two dedicated nodes.
  </para>

<!--
  <para>
   It is recommended not to change the defaults in the &barcl; proposal,
   unless you know exactly what you require. However you should change the
   <guimenu>Cluster Admin Password</guimenu>. If you plan to change the
   <guimenu>Zone</guimenu> value, it is important to know that you need at
   least as many &stornode;s as <guimenu>Zones</guimenu>.
  </para>
-->

  <para>
   To be able to properly configure &o_objstore; it is important to
   understand how it places the data. Data is always stored redundantly
   within the hierarchy. The &o_objstore; hierarchy in &cloud; is
   formed out of zones, nodes, hard disks, and logical partitions. Zones are
   physically separated clusters, for example different server rooms each
   with its own power supply and network segment. A failure of one zone must
   not affect another zone. The next level in the hierarchy are the
   individual &o_objstore; storage nodes (on which
   <guimenu>swift-storage</guimenu> has been deployed) followed by the hard
   disks. Logical partitions come last.
  </para>

  <para>
   &o_objstore; automatically places three copies of each object on the
   highest hierarchy level possible. If three zones are available, the each
   copy of the object will be placed in a different zone. In a one zone
   setup with more than two nodes, the object copies will each be stored on
   a different node. In a one zone setup with two nodes, the copies will be
   distributed on different hard disks. If no other hierarchy element fits,
   logical partitions are used.
  </para>

  <para>
   The following attributes can be set to configure &o_objstore;:
  </para>

  <variablelist>
   <varlistentry>
    <term><guimenu>Allow Public Containers</guimenu>
    </term>
    <listitem>
     <para>
      Allows to enable public access to containers if set to
      <literal>true</literal>.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>Enable Object Versioning</term>
    <listitem>
     <para>
      If set to true, a copy of the current version is archived, each time
      an object is updated.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><guimenu>Zones</guimenu>
    </term>
    <listitem>
     <para>
      Number of zones (see above). If you do not have different independent
      installations of storage nodes, set the number of zones to
      <literal>1</literal>.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>Create 2^X Logical Partitions</term>
    <listitem>
     <para>
      Partition power. The number entered here is used to compute the number
      of logical partitions to be created in the cluster by using it as a
      power of 2 (2^X).
     </para>
     <para>
      It is recommended to use a minimum of 100 partitions per disk. To
      measure the partition power for your setup, do the following: Multiply
      the number of disks from all &o_objstore; nodes with 100 and then
      round up to the nearest power of two. Keep in mind that the first disk
      of each node is not used by &o_objstore;, but rather for the
      operating system.
     </para>
     <formalpara>
      <title>Example: 10 &o_objstore; nodes with 5 hard disks each</title>
      <para>
       Four hard disks on each node are used for &o_objstore;, so there
       is a total of forty disks. Multiplied with 100 gives 4000. The
       nearest power of two, 4096, equals 2^12. So the partition power that
       needs to be entered is <literal>12</literal>.
      </para>
     </formalpara>
     <important>
      <title>Value Cannot be Changed After the Proposal Has Been Deployed</title>
      <para>
       Changing the number of logical partition after &o_objstore; has
       been deployed is not supported. Therefore the value for the partition
       power should be calculated from the maximum number of partitions this
       cloud installation is likely going to need at any point in time.
      </para>
     </important>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>Minimum Hours before Partition is reassigned</term>
    <listitem>
     <para>
      This option sets the number of hours before a logical partition is
      considered for relocation. <literal>24</literal> is the recommended
      value.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><guimenu>Replicas</guimenu>
    </term>
    <listitem>
     <para>
      The number of copies generated for each object. Set this value to
      <literal>3</literal>, the tested and recommended value.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>Replication interval (in seconds)</term>
    <listitem>
     <para>
      Time (in seconds) after which to start a new replication process.
     </para>
    </listitem>
   </varlistentry>
   <!-- fs 2016-01-26: No longer present in Cloud6
   <varlistentry>
    <term>Cluster Admin Password</term>
    <listitem>
     <para>
      The &o_objstore; administrator password.
     </para>
    </listitem>
   </varlistentry>
   -->
   <varlistentry>
    <term>Debug</term>
    <listitem>
     <para>
      Shows debugging output in the log files when set to
      <literal>true</literal>.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>SSL Support: Protocol</term>
    <listitem>
     <para>
      Choose whether to encrypt public communication
      (<guimenu>HTTPS</guimenu>) or not (<guimenu>HTTP</guimenu>). If choosing
      <guimenu>HTTPS</guimenu>, you have two choices. You can either
      <guimenu>Generate (self-signed) certificates</guimenu> or provide the
      locations for the certificate key pair files. Using self-signed
      certificates is for testing purposes only and should never be used in
      production environments!
     </para>
    </listitem>
   </varlistentry>
  </variablelist>

  <figure>
   <title>The &o_objstore; &Barcl;</title>
   <mediaobject>
    <imageobject role="fo">
     <imagedata fileref="depl_barclamp_swift.png" width="100%" format="png"/>
    </imageobject>
    <imageobject role="html">
     <imagedata fileref="depl_barclamp_swift.png" width="75%" format="png"/>
    </imageobject>
   </mediaobject>
  </figure>

  <para>
   Apart from the general configuration described above, the &o_objstore;
   &barcl; lets you also activate and configure <guimenu>Additional
   Middlewares</guimenu>. The features these middlewares provide can be used
   via the &o_objstore; command line client only. The Ratelimit and S3
   middlewares certainly provide for the most interesting features, whereas
   it is recommended to only enable further middlewares for specific
   use-cases.
  </para>

  <variablelist>
   <varlistentry>
    <term><guimenu>S3 Middleware</guimenu>
    </term>
    <listitem>
     <para>
      Provides an S3 compatible API on top of &o_objstore;.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><guimenu>StaticWeb</guimenu>
    </term>
    <listitem>
     <para>
      Enables to serve container data as a static Web site with an index
      file and optional file listings. See
      <link xlink:href="http://docs.openstack.org/developer/swift/middleware.html#staticweb"/>
      for details.
     </para>
     <para>
      This middleware requires to set <guimenu>Allow Public
      Containers</guimenu> to <literal>true</literal>.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><guimenu>TempURL</guimenu>
    </term>
    <listitem>
     <para>
      Enables to create URLs to provide time limited access to objects. See
      <link xlink:href="http://docs.openstack.org/developer/swift/middleware.html#tempurl"/>
      for details.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><guimenu>FormPOST</guimenu>
    </term>
    <listitem>
     <para>
      Enables to upload files to a container via Web form. See
      <link xlink:href="http://docs.openstack.org/developer/swift/middleware.html#formpost"/>
      for details.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><guimenu>Bulk</guimenu>
    </term>
    <listitem>
     <para>
      Enables the possibility to extract tar files into a swift account and
      to delete multiple objects or containers with a single request. See
      <link xlink:href="http://docs.openstack.org/developer/swift/middleware.html#module-swift.common.middleware.bulk"/>
      for details.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><guimenu>Cross Domain</guimenu>
    </term>
    <listitem>
     <para>
      Allows to interact with the Swift API via Flash, Java and Silverlight
      from an external network. See
      <link xlink:href="http://docs.openstack.org/developer/swift/middleware.html#module-swift.common.middleware.crossdomain"/>
      for details.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><guimenu>Domain Remap</guimenu>
    </term>
    <listitem>
     <para>
      Translates container and account parts of a domain to path parameters
      that the &o_objstore; proxy server understands. Can be used to
      create short URLs that are easy to remember, for example by rewriting
      <literal>home.&exampleuser;.&exampledomain;/$ROOT/exampleuser;/home/myfile</literal>
      to <literal>home.&exampleuser;.&exampledomain;/myfile</literal>.
      See
      <link xlink:href="http://docs.openstack.org/developer/swift/middleware.html#module-swift.common.middleware.domain_remap"/>
      for details.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><guimenu>Ratelimit</guimenu>
    </term>
    <listitem>
     <para>
      Ratelimit enables you to throttle resources such as requests per
      minute to provide denial of service protection. See
      <link xlink:href="http://docs.openstack.org/developer/swift/middleware.html#module-swift.common.middleware.ratelimit"/>
      for details.
     </para>
    </listitem>
   </varlistentry>
  </variablelist>

  <para>
   The &o_objstore; service consists of four different roles. Deploying
   <guimenu>swift-dispersion</guimenu> is optional:
  </para>

  <variablelist>
   <varlistentry>
    <term>swift-storage
    </term>
    <listitem>
     <para>
      The virtual object storage service. Install this role on all dedicated
      &o_objstore; &stornode;s (at least two), but not on any other
      node.
     </para>
     <warning>
      <title>swift-storage Needs Dedicated Machines</title>
      <para>
       Never install the swift-storage service on a node that runs other
       &ostack; services.
      </para>
     </warning>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><guimenu>swift-ring-compute</guimenu>
    </term>
    <listitem>
     <para>
      The ring maintains the information about the location of objects,
      replicas, and devices. It can be compared to an index, that is used by
      various &ostack; services to look up the physical location of
      objects. <guimenu>swift-ring-compute</guimenu> must only be installed
      on a single node; it is recommended to use a &contrnode;.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><guimenu>swift-proxy</guimenu>
    </term>
    <listitem>
     <para>
      The &o_objstore; proxy server takes care of routing requests to
      &o_objstore;. Installing a single instance of
      <guimenu>swift-proxy</guimenu> on a &contrnode; is recommended. The
      <guimenu>swift-proxy</guimenu> role can be made highly available by
      deploying it on a cluster.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><guimenu>swift-dispersion</guimenu>
    </term>
    <listitem>
     <para>
      Deploying <guimenu>swift-dispersion</guimenu> is optional. The
      &o_objstore; dispersion tools can be used to test the health of the
      cluster. It creates a heap of dummy objects (using 1% of the total
      space available). The state of these objects can be queried using the
      swift-dispersion-report query. <guimenu>swift-dispersion</guimenu>
      needs to be installed on a &contrnode;.
     </para>
    </listitem>
   </varlistentry>
  </variablelist>

  <figure>
   <title>The &o_objstore; &Barcl;: Node Deployment Example</title>
   <mediaobject>
    <imageobject role="fo">
     <imagedata fileref="depl_barclamp_swift_node_deployment.png" width="100%" format="png"/>
    </imageobject>
    <imageobject role="html">
     <imagedata fileref="depl_barclamp_swift_node_deployment.png" width="75%" format="png"/>
    </imageobject>
   </mediaobject>
  </figure>

  <sect2 xml:id="sec.depl.ostack.swift.ha">
   <title>&haSetup; for &swift;</title>
   <para>
    &swift; replicates by design, so there is no need for a special
    &hasetup;. Make sure to fulfill the requirements listed in
    <xref linkend="sec.depl.reg.ha.storage.swift"/>.
   </para>
  </sect2>
 </sect1>
 <sect1 xml:id="sec.depl.ostack.glance">
  <title>Deploying &o_img;</title>

  <para>
   &o_img; provides discovery, registration, and delivery services for
   virtual disk images. An image is needed to start an
   &vmguest;&mdash;it is its pre-installed root-partition. All images
   you want to use in your cloud to boot &vmguest;s from, are provided by
   &o_img;. &o_img; must be deployed onto a &contrnode;. &o_img;
   can be made highly available by deploying it on a cluster.
  </para>

  <para>
   There are a lot of options to configure &o_img;. The most important
   ones are explained below&mdash;for a complete reference refer to
   <link xlink:href="http://github.com/crowbar/crowbar/wiki/Glance--barclamp"/>.
  </para>

  <variablelist>
<!-- fs 2014-07-10
   <varlistentry>
    <term><guimenu>Notification Strategy</guimenu></term>
    <listitem>
     <para>
      &o_img; notifications can be used for auditing and troubleshooting. By
      default they (<guimenu>Noop</guimenu>) are disabled. When choosing
      <guimenu>RabbitMQ</guimenu>, notifications are send to the RabbitMQ
      service.
     </para>
    </listitem>
   </varlistentry>
-->
   <varlistentry>
    <term><guimenu>Image Storage: Default Storage Store</guimenu>
    </term>
    <listitem>
     <para>
      Choose whether to use &o_objstore; or &ceph;
      (<guimenu>Rados</guimenu>) to store the images. If you have deployed
      neither of these services, the images can alternatively be stored in
      an image file on the &contrnode; (<guimenu>File</guimenu>). If you
      have deployed &o_objstore; or &ceph;, it is recommended to use
      it for &o_img; as well.
     </para>
     <para>
      If using VMware as a hypervisor, it is recommended to use it for
      storing images, too (<guimenu>VMWare</guimenu>). This will make
      starting VMware instances much faster.
     </para>
     <para>
      Depending on the storage back-end, there are additional configuration
      options available:
     </para>
     <bridgehead renderas="sect2"><guimenu>File Store Parameters</guimenu>
     </bridgehead>
     <variablelist>
      <varlistentry>
       <term><guimenu>Image Store Directory</guimenu>
       </term>
       <listitem>
        <para>
         Specify the directory to host the image file. The directory
         specified here can also be an NFS share. See
         <xref linkend="sec.depl.inst.nodes.post.nfs"/> for more
         information.
        </para>
       </listitem>
      </varlistentry>
     </variablelist>
     <bridgehead renderas="sect2"><guimenu>Swift Store Parameters</guimenu>
     </bridgehead>
     <variablelist>
      <varlistentry>
       <term><guimenu>Swift Container</guimenu>
       </term>
       <listitem>
        <para>
         Set the name of the container to use for the images in
         &o_objstore;.
        </para>
       </listitem>
      </varlistentry>
     </variablelist>
     <bridgehead renderas="sect2"><guimenu>RADOS Store Parameters</guimenu>
     </bridgehead>
     <variablelist>
      <varlistentry>
       <term>RADOS User for CephX Authentication</term>
       <listitem>
        <para>
         If using a &cloud; internal &ceph; setup, the user you
         specify here is created in case it does not exist. If using an
         external &ceph; cluster, specify the user you have set up for
         &o_img; (see <xref linkend="sec.depl.inst.nodes.post.ceph_ext"/>
         for more information).
        </para>
       </listitem>
      </varlistentry>
      <varlistentry>
       <term>RADOS Pool for Glance images</term>
       <listitem>
        <para>
         If using a &cloud; internal &ceph; setup, the pool you
         specify here is created in case it does not exist. If using an
         external &ceph; cluster, specify the pool you have set up for
         &o_img; (see <xref linkend="sec.depl.inst.nodes.post.ceph_ext"/>
         for more information).
        </para>
       </listitem>
      </varlistentry>
     </variablelist>
     <bridgehead renderas="sect2"><guimenu>VMWare Store Parameters</guimenu>
     </bridgehead>
     <variablelist>
      <varlistentry>
       <term><guimenu>vCenter Host/IP Address</guimenu>
       </term>
       <listitem>
        <para>
         Name or IP address of the vCenter server.
        </para>
       </listitem>
      </varlistentry>
      <varlistentry>
       <term><guimenu>vCenter User name</guimenu> / <guimenu>vCenter
        Password</guimenu>
       </term>
       <listitem>
        <para>
         vCenter login credentials.
        </para>
       </listitem>
      </varlistentry>
      <varlistentry>
       <term><guimenu>Datastores for Storing Images</guimenu>
       </term>
       <listitem>
        <para>
         A comma-separated list of datastores specified in the format:
         <replaceable>DATACENTER_NAME</replaceable>:<replaceable>DATASTORE_NAME</replaceable>
        </para>
       </listitem>
      </varlistentry>
      <varlistentry>
       <term>
        <guimenu>
         Path on the datastore, where the glance images will be
         stored
        </guimenu>
       </term>
       <listitem>
        <para>
         Specify an absolute path here.
        </para>
       </listitem>
      </varlistentry>
     </variablelist>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><guimenu>SSL Support: Protocol</guimenu></term>
    <listitem>
     <para>
      Choose whether to encrypt public communication
      (<guimenu>HTTPS</guimenu>) or not (<guimenu>HTTP</guimenu>). If
      choosing <guimenu>HTTPS</guimenu>, refer to
      <xref linkend="sec.depl.ostack.keystone.ssl"/> for configuration
      details.
     </para>
    </listitem>
   </varlistentry>
   <!-- fs 2016-01-26: No longer present in Cloud6
   <varlistentry>
    <term><guimenu>API: Bind to All Addresses</guimenu>
    </term>
    <listitem>
     <para>
      Set this option to <guimenu>true</guimenu> to enable users to upload
      images to &o_img;. If unset, only the operator can upload images.
     </para>
    </listitem>
    </varlistentry>
    -->
   <varlistentry>
    <term><guimenu>Caching</guimenu>
    </term>
    <listitem>
     <para>
      Enable and configure image caching in this section. By default, image
      caching is disabled. Learn more about &o_img;'s caching feature at
      <link xlink:href="http://docs.openstack.org/developer/glance/cache.html"/>.
     </para>
    </listitem>
   </varlistentry>
   <!-- fs 2016-01-26: No longer present in Cloud6
   <varlistentry>
    <term><guimenu>Database: SQL Idle Timeout</guimenu>
    </term>
    <listitem>
     <para>
      Time after which idle database connections will be dropped.
     </para>
    </listitem>
    </varlistentry>
    -->
   <varlistentry>
    <term><guimenu>Logging: Verbose Logging</guimenu>
    </term>
    <listitem>
     <para>
      Shows debugging output in the log files when set to
      <guimenu>true</guimenu>.
     </para>
    </listitem>
   </varlistentry>
  </variablelist>

  <figure>
   <title>The &o_img; &Barcl;</title>
   <mediaobject>
    <imageobject role="fo">
     <imagedata fileref="depl_barclamp_glance.png" width="100%" format="png"/>
    </imageobject>
    <imageobject role="html">
     <imagedata fileref="depl_barclamp_glance.png" width="75%" format="png"/>
    </imageobject>
   </mediaobject>
  </figure>

  <sect2 xml:id="sec.depl.ostack.glance.ha">
   <title>&haSetup; for &o_img;</title>
   <para>
    &o_img; can be made highly available by deploying it on a cluster. It
    is also strongly recommended to do so for the image data, too. The
    recommended way to achieve this is to use &o_objstore; or an external
    &ceph; cluster for the image repository. If using a directory on the
    node instead (file storage back-end), you should set up shared storage
    on the cluster for it.
   </para>
  </sect2>
 </sect1>
 <sect1 xml:id="sec.depl.ostack.cinder">
  <title>Deploying &o_blockstore;</title>

  <para>
   &o_blockstore;, the successor of Nova Volume, provides volume block
   storage. It adds persistent storage to an &vmguest; that will persist
   until deleted (contrary to ephemeral volumes that will only persist while
   the &vmguest; is running).
  </para>

  <para>
   &o_blockstore; can provide volume storage by using different back-ends
   such as local file, one or more local disks, &ceph; (RADOS), VMware or
   network storage solutions from EMC, EqualLogic, Fujitsu or NetApp. Since
   &productname; 5, &o_blockstore; supports using several back-ends
   simultaneously. It is also possible to deploy the same network storage
   back-end multiple times and therefore use different installations at the
   same time.
  </para>

  <para>
   The attributes that can be set to configure &o_blockstore; depend on
   the backend. The only general option is
   <guimenu>SSL Support: Protocol</guimenu> (see
   <xref linkend="sec.depl.ostack.keystone.ssl"/> for configuration
   details).
  </para>

  <tip>
   <title>Adding or Changing a Back-End</title>
   <para>
    When first opening the &o_blockstore; &barcl;, the default
    proposal&mdash;<guimenu>Raw Devices</guimenu>&mdash; is already available
    for configuration. To optionally add a back-end, go to the section
    <guimenu>Add New Cinder Back-End</guimenu> and choose a <guimenu>Type Of
    Volume</guimenu> from the drop-down box. Optionally, specify the
    <guimenu>Name for the Backend</guimenu>. This is recommended when
    deploying the same volume type more than once. Existing back-end
    configurations (including the default one) can be deleted by clicking the
    trashcan icon if no longer needed. Note that at least one back-end must be
    configured.
   </para>
  </tip>

  <bridgehead renderas="sect2"><guimenu>Raw devices</guimenu> (local disks)
  </bridgehead>

  <variablelist>
   <varlistentry>
    <term><guimenu>Disk Selection Method</guimenu>
    </term>
    <listitem>
     <para>
      Choose whether to only use the <guimenu>First Available</guimenu> disk
      or <guimenu>All Available</guimenu> disks. <quote>Available
      disks</quote> are all disks, currently not used by the system. Note
      that one disk (usually <filename>/dev/sda</filename>) of every block
      storage node is already used for the operating system and is not
      available for &o_blockstore;.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><guimenu>Name of Volume</guimenu>
    </term>
    <listitem>
     <para>
      Specify a name for the &o_blockstore; volume.
     </para>
    </listitem>
   </varlistentry>
  </variablelist>

  <bridgehead renderas="sect3"><guimenu>EMC</guimenu> (EMC Storage)
  </bridgehead>

  <variablelist>
   <varlistentry>
    <term><guimenu>IP address of the ECOM server</guimenu> / <guimenu>Port of the ECOM server</guimenu>
    </term>
    <listitem>
     <para>
      IP address and Port of the ECOM server.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><guimenu>Unisphere for VMAX Masking View</guimenu>
    </term>
    <listitem>
     <para>
      For VMAX, the user needs to create an initial setup on the Unisphere
      for VMAX server first. It needs to contain an initiator group, a
      storage group and a port group and needs to be put in a masking view.
      This masking view needs to be specified here.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><guimenu>User Name / Password for accessing the ECOM server</guimenu>
    </term>
    <listitem>
     <para>
      Login credentials for the ECOM server.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><guimenu>Thin pool where user wants to create volume from</guimenu>
    </term>
    <listitem>
     <para>
      Only thin LUNs are supported by the plugin. Thin pools can be created
      using Unisphere for VMAX and VNX.
     </para>
    </listitem>
   </varlistentry>
  </variablelist>

  <para>
   For more information on the EMC driver refer to the &ostack;
   documentation at
   <link xlink:href="http://docs.openstack.org/liberty/config-reference/content/emc-vmax-driver.html"/>.
  </para>

  <bridgehead renderas="sect3"><guimenu>EqualLogic</guimenu>
  </bridgehead>

  <para>
   EqualLogic drivers are included as a technology preview and are not
   supported.
  </para>

  <bridgehead renderas="sect2"><guimenu>Fujitsu ETERNUS DX</guimenu>
  </bridgehead>

  <variablelist>
   <varlistentry>
    <term><guimenu>Connection Protocol</guimenu>
    </term>
    <listitem>
     <para>
      Select the protocol used to connect, either
      <guimenu>FibreChannel</guimenu> or <guimenu>iSCSI</guimenu>.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>IP / Port for SMI-S</term>
    <listitem>
     <para>
      IP address and port of the ETERNUS SMI-S Server.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>Username / Password for SMI-S</term>
    <listitem>
     <para>
      Login credentials for the ETERNUS SMI-S Server.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>Snapshot (Thick/RAID Group) Pool Name</term>
    <listitem>
     <para>
      Storage pool (RAID group) in which the volumes are created. Make sure
      to have created that RAID group on the server in advance. If a RAID
      group that does not exist is specified, the RAID group is created by
      using unused disk drives. The RAID level is automatically determined
      by the ETERNUS DX Disk storage system.
     </para>
    </listitem>
   </varlistentry>
  </variablelist>

  <bridgehead renderas="sect2"><guimenu>NetApp</guimenu>
  </bridgehead>

  <variablelist>
   <varlistentry>
    <term><guimenu>Storage Family Type/Storage Protocol</guimenu>
    </term>
    <listitem>
     <para>
      &cloud; can either use <quote>Data ONTAP</quote> in
      <guimenu>7-Mode</guimenu> or in <guimenu>Clustered Mode</guimenu>. In
      <guimenu>7-Mode</guimenu> vFiler will be configured, in
      <guimenu>Clustered Mode</guimenu> vServer will be configured. The
      <guimenu>Storage Protocoll</guimenu> can either be set to
      <guimenu>iSCSI</guimenu> or <guimenu>NFS</guimenu>. Choose the driver
      and the protocol your NetApp is licensed for.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><guimenu>Server host name</guimenu>
    </term>
    <listitem>
     <para>
      The management IP address for the 7-Mode storage controller or the
      cluster management IP address for the clustered Data ONTAP.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><guimenu>Transport Type</guimenu>
    </term>
    <listitem>
     <para>
      Transport protocol for communicating with the storage controller or
      clustered Data ONTAP. Supported protocols are HTTP and HTTPS. Choose
      the protocol your NetApp is licensed for.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><guimenu>Server port</guimenu>
    </term>
    <listitem>
     <para>
      The port to use for communication. Port 80 is usually used for HTTP,
      443 for HTTPS.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><guimenu>User Name/Password for Accessing NetApp</guimenu>
    </term>
    <listitem>
     <para>
      Login credentials.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><guimenu>
      The vFiler Unit Name for provisioning OpenStack volumes (netapp_vfiler)
     </guimenu>
    </term>
    <listitem>
     <para>
      The vFiler unit to be used for provisioning of &ostack; volumes.
      This setting is only available in <guimenu>7-Mode</guimenu>.
     </para>
    </listitem>
   </varlistentry>

   <varlistentry>
    <term><guimenu>Restrict provisioning on iSCSI to these volumes (netapp_volume_list)</guimenu>
    </term>
    <listitem>
     <para>
      Provide a list of comma-separated volumes names to be used for
      provisioning. This setting is only available when using iSCSI as
      storage protocol.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><guimenu>Name of the Virtual Storage Server ((netapp_vserver)</guimenu>
    </term>
    <listitem>
     <para>
      Host name of the Virtual Storage Server. This setting is only
      available in <guimenu>Clustered Mode</guimenu> when using NFS as
      storage protocol.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><guimenu>List of Netapp NFS Exports</guimenu>
    </term>
    <listitem>
     <para>
      Provide a list of NFS Exports from the Virtual Storage Server. Specify
      one entry per line in the form of <replaceable>host
      name</replaceable>:/<replaceable>volume/path</replaceable>
      <replaceable>mount-options</replaceable>. Specifying mount options is
      optional. This setting is only available when using NFS as storage
      protocol.
     </para>
    </listitem>
   </varlistentry>
  </variablelist>

  <bridgehead renderas="sect3"><guimenu>RADOS</guimenu> (&ceph;)
  </bridgehead>

  <variablelist>
   <varlistentry>
    <term><guimenu>Use Ceph Deployed by Crowbar</guimenu>
    </term>
    <listitem>
     <para>
      Select <guimenu>true</guimenu> if you have deployed &ceph; with
      &cloud;. In case you are using an external &ceph; cluster (see
      <xref linkend="sec.depl.inst.nodes.post.ceph_ext"/> for setup
      instructions), select <guimenu>false</guimenu>.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><guimenu>Path to Ceph Configuration File</guimenu>
    </term>
    <listitem>
     <para>
      This configuration option is only available, if using an external &ceph;
      cluster. Specify the path to the <filename>ceph.conf</filename>
      file&mdash;the default value (<filename>/etc/ceph/ceph.conf</filename>)
      should be fitting if you have followed the setup instructions in <xref
      linkend="sec.depl.inst.nodes.post.ceph_ext"/>.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><guimenu>Path to Ceph Admin Keyring</guimenu>
    </term>
    <listitem>
     <para>
      This configuration option is only available, if using an external &ceph;
      cluster. If you have had access to the admin keyring file, the path is
      <filename>/etc/ceph/ceph.client.admin.keyring</filename>. If you have
      created your own keyring, use
      <filename>/etc/ceph/ceph.client.cinder.keyring</filename>. See <xref
      linkend="sec.depl.inst.nodes.post.ceph_ext"/> dor more information.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><guimenu>RADOS pool for Cinder volumes</guimenu>
    </term>
    <listitem>
     <para>
      Name of the pool used to store the &o_blockstore; volumes.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>
     <guimenu>
      RADOS user (Set Only if Using CephX authentication)
     </guimenu>
    </term>
    <listitem>
     <para>
      &ceph; user name.
     </para>
    </listitem>
   </varlistentry>
  </variablelist>

  <bridgehead renderas="sect3"><guimenu>VMware</guimenu>
  </bridgehead>

  <variablelist>
   <varlistentry>
    <term><guimenu>vCenter Host/IP Address</guimenu>
    </term>
    <listitem>
     <para>
      Host name or IP address of the vCenter server.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><guimenu>vCenter Username</guimenu> / <guimenu>vCenter
     Password</guimenu>
    </term>
    <listitem>
     <para>
      vCenter login credentials.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><guimenu>vCenter Cluster Names for Volumes</guimenu></term>
    <listitem>
     <para>
      Provide a comma-separated list of cluster names.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><guimenu>Folder for Volumes</guimenu></term>
    <listitem>
     <para>
      Path to the folder used to store the &o_blockstore; volumes.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>
     <guimenu>CA file for verifying the vCenter certificate</guimenu>
    </term>
    <listitem>
     <para>
      Absolute path to the vCenter CA certificate.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>
     <guimenu>
      vCenter SSL Certificate is insecure (for instance, self-signed)
     </guimenu>
    </term>
    <listitem>
     <para>
      Set this option to true when using self-signed certificates to disable
      certificate checks. This setting is for testing purposes only and should
      never be used in production environments!
     </para>
    </listitem>
   </varlistentry>
  </variablelist>

  <bridgehead renderas="sect2"><guimenu>Local file</guimenu>
  </bridgehead>

  <variablelist>
   <varlistentry>
    <term><guimenu>Volume File Name</guimenu>
    </term>
    <listitem>
     <para>
      Absolute path to the file to be used for block storage.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><guimenu>Maximum File Size (GB)</guimenu>
    </term>
    <listitem>
     <para>
      Maximum size of the volume file. Make sure not to overcommit the size,
      since it will result in data loss.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><guimenu>Name of Volume</guimenu>
    </term>
    <listitem>
     <para>
      Specify a name for the &o_blockstore; volume.
     </para>
    </listitem>
   </varlistentry>
  </variablelist>

  <note>
   <title>Using <guimenu>Local File</guimenu> for block storage</title>
   <para>
    Using a file for block storage is not recommended for production
    systems, because of performance and data security reasons.
   </para>
  </note>

  <bridgehead renderas="sect3"><guimenu>Other driver</guimenu>
  </bridgehead>

  <para>
   Lets you manually pick and configure a driver. Only use this option for
   testing purposes, it is not supported.
  </para>

  <figure>
   <title>The &o_blockstore; &Barcl;</title>
   <mediaobject>
    <imageobject role="fo">
     <imagedata fileref="depl_barclamp_cinder.png" width="100%" format="png"/>
    </imageobject>
    <imageobject role="html">
     <imagedata fileref="depl_barclamp_cinder.png" width="75%" format="png"/>
    </imageobject>
   </mediaobject>
  </figure>

  <para>
   The &o_blockstore; service consists of two different roles:
  </para>

  <variablelist>
   <varlistentry>
    <term><guimenu>cinder-controller</guimenu>
    </term>
    <listitem>
     <para>
      The &o_blockstore; controller provides the scheduler and the API.
      Installing <guimenu>cinder-controller</guimenu> on a &contrnode; is
      recommended.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><guimenu>cinder-volume</guimenu>
    </term>
    <listitem>
     <para>
      The virtual block storage service. It can be installed on a
      &contrnode;, but it is recommended to deploy it on one or more
      dedicated nodes supplied with sufficient networking capacity, since it
      will generate a lot of network traffic.
     </para>
    </listitem>
   </varlistentry>
  </variablelist>

  <figure>
   <title>The &o_blockstore; &Barcl;: Node Deployment Example</title>
   <mediaobject>
    <imageobject role="fo">
     <imagedata fileref="depl_barclamp_cinder_node_deployment.png" width="100%" format="png"/>
    </imageobject>
    <imageobject role="html">
     <imagedata fileref="depl_barclamp_cinder_node_deployment.png" width="75%" format="png"/>
    </imageobject>
   </mediaobject>
  </figure>

  <sect2 xml:id="sec.depl.ostack.cinder.ha">
   <title>&haSetup; for &o_blockstore;</title>
   <para>
    While the <guimenu>cinder-controller</guimenu> role can be deployed on a
    cluster, deploying <guimenu>cinder-volume</guimenu> on a cluster is not
    supported. Therefore it is generally recommended to deploy
    <guimenu>cinder-volume</guimenu> on several nodes&mdash;this ensures
    the service continues to be available even when a node fails. In
    addition with &ceph; or a network storage solution, such a setup
    minimizes the potential downtime.
   </para>
   <para>
    In case using &ceph; or a network storage is no option, you need to
    set up a shared storage directory (for example with NFS), mount it on
    all cinder volume nodes and use the <guimenu>Local File</guimenu>
    back-end with this shared directory. Using <guimenu>Raw
    Devices</guimenu> is not an option, since local disks cannot be shared.
   </para>
  </sect2>
 </sect1>
 <sect1 xml:id="sec.depl.ostack.manila">
  <title>Deploying &o_sharefs;</title>
  <para>
   &o_sharefs; provides coordinated access to shared or distributed file
   systems, similar to what &o_blockstore; does for block storage. These file
   systems can be shared between &vmguest;s in &cloud;.
  </para>
  <para>
   &o_sharefs; uses different back-ends. As of &productname; &productnumber;
   the only back-end that is currently supported is the <guimenu>NetApp
   Driver</guimenu>. Two more back-end options, <guimenu>Generic
   Driver</guimenu> and <guimenu>Other Driver</guimenu> are available for
   testing puposes and are not supported.
  </para>
  <para>
   When first opening the &o_sharefs; &barcl;, the default proposal
   <guimenu>Generic Driver</guimenu> is already available for
   configuration. To replace it, first delete it by clicking the trashcan
   icon and then choose a different back-end in the section <guimenu>Add new
   Manila Backend</guimenu>. Select a <guimenu>Type of Share</guimenu>
   and&mdash;optionally&mdash;provide a <guimenu>Name for
   Backend</guimenu>. Activate the back-end with <guimenu>Add
   Backend</guimenu>. Note that at least one back-end must be configured.
  </para>
  <para>
   The attributes that can be set to configure Cinder depend on the back-end:
  </para>

  <bridgehead renderas="sect2"><guimenu>Back-end: Generic</guimenu>
  </bridgehead>

  <para>
   The generic driver is included as a technology preview and is not
   supported.
  </para>

  <bridgehead renderas="sect2"><guimenu>Back-end: Netapp</guimenu>
  </bridgehead>

  <variablelist>
   <varlistentry>
    <term>
     <guimenu>Name of the Virtual Storage Server (vserver)</guimenu>
    </term>
    <listitem>
     <para>
      Host name of the Virtual Storage Server.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><guimenu>Server Host Name</guimenu></term>
    <listitem>
     <para>
      The name or IP address for the storage controller or the cluster.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><guimenu>Server Port</guimenu></term>
    <listitem>
     <para>
      The port to use for communication. Port 80 is usually used for HTTP, 443
      for HTTPS.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><guimenu>User name/Password for Accessing NetApp</guimenu></term>
    <listitem>
     <para>
      Login credentials.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><guimenu>Transport Type</guimenu></term>
    <listitem>
     <para>
      Transport protocol for communicating with the storage controller or
      cluster. Supported protocols are HTTP and HTTPS. Choose the protocol
      your NetApp is licensed for.
     </para>
    </listitem>
   </varlistentry>
  </variablelist>

  <bridgehead renderas="sect2"><guimenu>Back-end: Manual</guimenu>
  </bridgehead>

  <para>
   Lets you manually pick and configure a driver. Only use this option for
   testing purposes, it is not supported.
  </para>

  <figure>
   <title>The &o_sharefs; &Barcl;</title>
   <mediaobject>
    <imageobject role="fo">
     <imagedata fileref="depl_barclamp_manila.png" width="100%" format="png"/>
    </imageobject>
    <imageobject role="html">
     <imagedata fileref="depl_barclamp_manila.png" width="75%" format="png"/>
    </imageobject>
   </mediaobject>
  </figure>

  <para>
   The &o_sharefs; service consists of two different roles:
  </para>

  <variablelist>
   <varlistentry>
    <term><guimenu>manila-server</guimenu>
    </term>
    <listitem>
     <para>
      The &o_sharefs; server provides the scheduler and the API. Installing it
      on a &contrnode; is recommended.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><guimenu>manila-share</guimenu>
    </term>
    <listitem>
     <para>
      The shared storage service. It can be installed on a &contrnode;, but
      it is recommended to deploy it on one or more dedicated nodes supplied
      with sufficient disk space and networking capacity, since it will
      generate a lot of network traffic.
     </para>
    </listitem>
   </varlistentry>
  </variablelist>

  <figure>
   <title>The &o_sharefs; &Barcl;: Node Deployment Example</title>
   <mediaobject>
    <imageobject role="fo">
     <imagedata fileref="depl_barclamp_manila_node_deployment.png" width="100%" format="png"/>
    </imageobject>
    <imageobject role="html">
     <imagedata fileref="depl_barclamp_manila_node_deployment.png" width="75%" format="png"/>
    </imageobject>
   </mediaobject>
  </figure>

  <sect2 xml:id="sec.depl.ostack.manila.ha">
   <title>&haSetup; for &o_sharefs;</title>
   <para>
    While the <guimenu>manila-server</guimenu> role can be deployed on a
    cluster, deploying <guimenu>manila-share</guimenu> on a cluster is not
    supported. Therefore it is generally recommended to deploy
    <guimenu>manila-share</guimenu> on several nodes&mdash;this ensures the
    service continues to be available even when a node fails.
   </para>
  </sect2>

 </sect1>

 <sect1 xml:id="sec.depl.ostack.quantum">
  <title>Deploying &o_netw;</title>

  <para>
   &o_netw; provides network connectivity between interface devices
   managed by other &ostack; services (most likely &o_comp;). The
   service works by enabling users to create their own networks and then
   attach interfaces to them.
  </para>

  <para>
   &o_netw; must be deployed on a &contrnode;. You first need to
   choose a core plug-in&mdash;<guimenu>ml2</guimenu> or
   <guimenu>vmware</guimenu>. Depending on your choice, more configuration
   options will become available.
  </para>

  <para>
   The <guimenu>vmware</guimenu> option lets you use an existing VMWare NSX
   installation. Using this plugin is not a prerequisite for the VMWare
   vSphere hypervisor support. However, it is needed when wanting to have
   security groups supported on VMWare compute nodes. For all other
   scenarios, choose <guimenu>ml2</guimenu>.
  </para>

  <para>
   The only global option that can be configured is <guimenu>SSL
   Support</guimenu>. Choose whether to encrypt public communication
   (<guimenu>HTTPS</guimenu>) or not (<guimenu>HTTP</guimenu>). If choosing
   <guimenu>HTTPS</guimenu>, refer to
   <xref linkend="sec.depl.ostack.keystone.ssl"/> for configuration details.
  </para>

  <bridgehead renderas="sect2"><guimenu>ml2</guimenu> (Modular Layer 2)
  </bridgehead>

  <variablelist>
   <varlistentry>
    <term><guimenu>Modular Layer 2 Mechanism Drivers</guimenu>
    </term>
    <listitem>
     <para>
      Select which mechanism driver(s) shall be enabled for the ml2
      plugin.It is possible to select more than one driver by holding the
      <keycap function="control"/> key while clicking. Choices are:
     </para>
     <formalpara>
      <title><guimenu>openvswitch</guimenu></title>
      <para>
       Supports GRE, VLAN and VLANX networks (to be configured via the
       <guimenu>Modular Layer 2 type drivers</guimenu> setting).
      </para>
     </formalpara>
     <formalpara>
      <title><guimenu>linuxbridge</guimenu></title>
      <para>
       Supports VLANs only. Requires to specify the <guimenu>Maximum Number
       of VLANs</guimenu>.
      </para>
     </formalpara>
     <formalpara>
      <title><guimenu>cisco_nexus</guimenu></title>
      <para>
       Enables &o_netw; to dynamically adjust the VLAN settings of the ports
       of an existing Cisco Nexus switch when instances are launched.  It also
       requires <guimenu>openvswitch</guimenu> which will automatically be
       selected. With <guimenu>Modular Layer 2 type drivers</guimenu>,
       <guimenu>vlan</guimenu> must be added. This option also requires to
       specify the <guimenu>Cisco Switch Credentials</guimenu>. See <xref
       linkend="app.deploy.cisco"/> for details.
      </para>
     </formalpara>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><guimenu>Use Distributed Virtual Router Setup</guimenu></term>
    <listitem>
     <para>
      With the default setup, all intra-&compnode; traffic flows through the
      network &contrnode;. The same is true for all traffic from floating
      IPs. In large deplyoments the network &contrnode; can therefore quickly
      become a bottleneck. When this option is set to <guimenu>true</guimenu>,
      network agents will be installed on all compute nodes to de-centralize
      the network traffic. For details refer to <link
      xlink:href="http://specs.openstack.org/openstack/neutron-specs/specs/juno/neutron-ovs-dvr.html"/>.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><guimenu>Modular Layer 2 Type Drivers</guimenu>
    </term>
    <listitem>
     <para>
      This option is only available when having chosen the
      <guimenu>openvswitch</guimenu> or the <guimenu>cisco_nexus</guimenu>
      mechanism drivers. Options are <guimenu>vlan</guimenu>,
      <guimenu>gre</guimenu> and <guimenu>vxlan</guimenu>. It is possible to
      select more than one driver by holding the <keycap function="control"/>
      key while clicking.
     </para>
     <para>
      When multiple type drivers are enabled, you need to select the
      <guimenu>Default Type Driver for Provider Network</guimenu>, that will
      be used for newly created provider networks. This also includes the
      <literal>nova_fixed</literal> network, that will be created when
      applying the &o_netw; proposal.  When manually creating provider
      networks with the <command>neutron</command> command, the default can be
      overwritten with the <option>--provider:network_type
      <replaceable>type</replaceable></option> switch. You will also need to
      set a <guimenu>Default Type Driver for Tenant Network</guimenu>. It is
      not possible to change this default when manually creating tenant
      networks with the <command>neutron</command> command. The non-default
      type driver will only be used as a fallback.
     </para>
     <para>
      Depending on your choice of the type driver, more configuration options
      become available.
     </para>
     <formalpara>
      <title><guimenu>gre</guimenu></title>
      <para>
       Having chosen <guimenu>gre</guimenu>, you also need to specify the
       start and end of the tunnel ID range.
      </para>
     </formalpara>
     <formalpara>
      <title><guimenu>vlan</guimenu></title>
      <para>
       The option <guimenu>vlan</guimenu> requires you to specify the
       <guimenu>Maximum number of VLANs</guimenu>.
      </para>
     </formalpara>
     <formalpara>
      <title><guimenu>vxlan</guimenu></title>
      <para>
       Having chosen <guimenu>vxlan</guimenu>, you also need to specify the
       start and end of the VNI range.
      </para>
     </formalpara>
    </listitem>
   </varlistentry>
  </variablelist>

  <bridgehead renderas="sect2"><guimenu>vmware</guimenu>
  </bridgehead>

  <para>
   This plug-in requires to configure access to the VMWare NSX service.
  </para>

  <variablelist>
   <varlistentry>
    <term><guimenu>VMWare NSX User Name/Password</guimenu>
    </term>
    <listitem>
     <para>
      Login credentials for the VMWare NSX server. The user needs to have
      administrator permissions on the NSX server.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><guimenu>VMWare NSX Controllers</guimenu>
    </term>
    <listitem>
     <para>
      Enter the IP address and the port number
      (<replaceable>IP-ADDRESS</replaceable>:<replaceable>PORT</replaceable>)
      of the controller API endpoint. If the port number is omitted, port
      443 will be used. You may also enter multiple API endpoints
      (comma-separated), provided they all belong to the same controller
      cluster. When multiple API endpoints are specified, the plugin will
      load balance requests on the various API endpoints.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><guimenu>UUID of the NSX Transport Zone/Gateway Service</guimenu>
    </term>
    <listitem>
     <para>
      The UUIDs for the transport zone and the gateway service can be
      obtained from the NSX server. They will be used when networks are
      created.
     </para>
    </listitem>
   </varlistentry>
  </variablelist>

  <figure>
   <title>The &o_netw; &Barcl;</title>
   <mediaobject>
    <imageobject role="fo">
     <imagedata fileref="depl_barclamp_network.png" width="100%" format="png"/>
    </imageobject>
    <imageobject role="html">
     <imagedata fileref="depl_barclamp_network.png" width="75%" format="png"/>
    </imageobject>
   </mediaobject>
  </figure>

  <para>
   The &o_netw; service consists of two different roles:
  </para>

  <variablelist>
   <varlistentry>
    <term><guimenu>neutron-server</guimenu>
    </term>
    <listitem>
     <para>
      <guimenu>neutron-server</guimenu> provides the scheduler and the API.
      It needs to be installed on a &contrnode;.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><guimenu>neutron-network</guimenu>
    </term>
    <listitem>
     <para>
      This service runs the various agents that manage the network traffic
      of all the cloud instances. It acts as the DHCP and DNS server and as
      a gateway for all cloud instances. It is recommend to deploy this role
      on a dedicated node supplied with sufficient network capacity.
     </para>
    </listitem>
   </varlistentry>
  </variablelist>

  <figure>
   <title>The &o_netw; &barcl;</title>
   <mediaobject>
    <imageobject role="fo">
     <imagedata fileref="depl_barclamp_neutron_node_deployment.png" width="100%" format="png"/>
    </imageobject>
    <imageobject role="html">
     <imagedata fileref="depl_barclamp_neutron_node_deployment.png" width="75%" format="png"/>
    </imageobject>
   </mediaobject>
  </figure>

  <sect2 xml:id="sec.depl.ostack.network.ha">
   <title>&haSetup; for &o_netw;</title>
   <para>
    &o_netw; can be made highly available by deploying
    <guimenu>neutron-server</guimenu> and <guimenu>neutron-network</guimenu> on a
    cluster. While <guimenu>neutron-server</guimenu> may be deployed on a
    cluster shared with other services, it is strongly recommended to use a
    dedicated cluster solely for the <guimenu>neutron-network</guimenu> role.
   </para>
   <para>
    If the network is configured to use openvswitch with VLAN support each
    node of the cluster <guimenu>neutron-network</guimenu> runs on needs at least
    four network cards (because two separate bonds, one for the nova_fixed
    and another one for the other networks, are needed).
   </para>
  </sect2>
 </sect1>
 <sect1 xml:id="sec.depl.ostack.nova">
  <title>Deploying &o_comp;</title>

  <para>
   &o_comp; provides key services for managing the &cloud;, sets up
   the &compnode;s. &cloud; currently supports KVM, Xen and Microsoft
   Hyper V and VMWare vSphere. The unsupported QEMU option is included to
   enable test setups with virtualized nodes. The following attributes can
   be configured for &o_comp;:
   <remark condition="clarity">
    2016-02-05 - fs: FIXME z/VM Configuration
   </remark>

  </para>

  <variablelist>
   <varlistentry>
    <term><guimenu>
      Scheduler Options: Virtual RAM to Physical RAM allocation ratio
     </guimenu>
    </term>
    <listitem>
     <para>
      Set the <quote>overcommit ratio</quote> for RAM for &vmguest;s on
      the &compnode;s. A ratio of <literal>1.0</literal> means no
      overcommitment. Changing this value is not recommended.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><guimenu>
      Scheduler Options: Virtual CPU to Physical CPU allocation ratio
     </guimenu>
    </term>
    <listitem>
     <para>
      Set the <quote>overcommit ratio</quote> for CPUs for &vmguest;s on
      the &compnode;s. A ratio of <literal>1.0</literal> means no
      overcommitment.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><guimenu>Live Migration Support: Enable Libvirt Migration</guimenu>
    </term>
    <listitem>
     <para>
      Allows to move &kvm; and &xen; &vmguest;s to a different
      &compnode; running the same hypervisor (cross hypervisor migrations
      are not supported). Useful when a &compnode; needs to be shut down
      or rebooted for maintenance or when the load of the &compnode; is
      very high. &Vmguest;s can be moved while running (Live Migration).
     </para>
     <warning>
      <title>Libvirt Migration and Security</title>
      <para>
       Enabling the libvirt migration option will open a TCP port on the
       &compnode;s that allows to access to all &vmguest;s from all
       machines in the admin network. Ensure that only authorized machines
       have access to the admin network when enabling this option.
      </para>
     </warning>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>Live Migration Support: Setup Shared Storage</term>
    <listitem>
     <para>
      Sets up a directory <filename>/var/lib/nova/instances</filename> on the
      &contrnode; on which <guimenu>nova-controller</guimenu> is
      running. This directory is exported via NFS to all compute nodes and
      will host a copy of the root disk of <emphasis>all</emphasis> &xen;
      &vmguest;s. This setup is required for live migration of &xen;
      &vmguest;s (but not for &kvm;) and is used to provide central
      handling of instance data. Enabling this option is only recommended if
      &xen; live migration is required&mdash;otherwise it should be disabled.
     </para>
     <warning>
      <title>Do not Set Up Shared Storage When &vmguest;s are Running</title>
      <para>
       Setting up shared storage in a &cloud; where &vmguest;s are
       running will result in connection losses to all running
       &vmguest;s. It is strongly recommended to set up shared storage
       when deploying &cloud;. If it needs to be done at a later stage,
       make sure to shut down all &vmguest;s prior to the change.
      </para>
     </warning>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><guimenu>KVM Options: Enable Kernel Samepage Merging</guimenu>
    </term>
    <listitem>
     <para>
      Kernel SamePage Merging (KSM) is a Linux Kernel feature which merges
      identical memory pages from multiple running processes into one memory
      region. Enabling it optimizes memory usage on the &compnode;s when
      using the &kvm; hypervisor at the cost of slightly increasing CPU
      usage.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>VMware vCenter Settings</term>
    <listitem>
     <para>
      Setting up VMware support is described in a separate section. See
      <xref linkend="app.deploy.vmware"/>.
     </para>
    </listitem>
   </varlistentry>
<!--
   <varlistentry>
    <term>z/VM Configuration</term>
    <listitem>
     <para>
      FIXME
     </para>
    </listitem>
   </varlistentry>
-->
   <varlistentry>
    <term>SSL Support: Protocol</term>
    <listitem>
     <para>
      Choose whether to encrypt public communication
      (<guimenu>HTTPS</guimenu>) or not (<guimenu>HTTP</guimenu>). If
      choosing <guimenu>HTTPS</guimenu>,refer to
      <xref linkend="sec.depl.ostack.keystone.ssl"/> for configuration
      details.
     </para>
    </listitem>
   </varlistentry>
    <varlistentry>
    <term>VNC Settings: Keymap</term>
    <listitem>
     <para>
      Change the default VNC keymap for &vmguest;s. By default
      <literal>en-us</literal> is used.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>VNC Settings: NoVNC Protocol</term>
    <listitem>
     <para>
      After having started an instance you can display its VNC console in
      the &ostack; &dash; (&o_dash;) via the browser using the
      noVNC implementation. By default this connection is not encrypted and
      can potentially be eavesdropped.
     </para>
     <para>
      Enable encrypted communication for noVNC by choosing
      <guimenu>HTTPS</guimenu> and providing the locations for the certificate
      key pair files.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><guimenu>Logging: Verbose Logging</guimenu>
    </term>
    <listitem>
     <para>
      Shows debugging output in the log files when set to
      <guimenu>true</guimenu>.
     </para>
    </listitem>
   </varlistentry>
  </variablelist>

  <figure>
   <title>The &o_comp; &Barcl;</title>
   <mediaobject>
    <imageobject role="fo">
     <imagedata fileref="depl_barclamp_nova.png" width="100%" format="png"/>
    </imageobject>
    <imageobject role="html">
     <imagedata fileref="depl_barclamp_nova.png" width="75%" format="png"/>
    </imageobject>
   </mediaobject>
  </figure>

  <para>
   The &o_comp; service consists of eight different roles:
  </para>

  <variablelist>
   <varlistentry>
    <term><guimenu>nova-controller</guimenu>
    </term>
    <listitem>
     <para>
      Distributing and scheduling the &vmguest;s is managed by the
      <guimenu>nova-controller</guimenu>. It also provides networking
      and messaging services. <guimenu>nova-controller</guimenu> needs
      to be installed on a &contrnode;.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><guimenu>nova-compute-docker</guimenu> /
    <guimenu>nova-compute-hyperv</guimenu> /
    <guimenu>nova-compute-kvm</guimenu> /
    <guimenu>nova-compute-qemu</guimenu> /
    <guimenu>nova-compute-vmware</guimenu> /
    <guimenu>nova-compute-xen</guimenu> /
    <guimenu>nova-compute-zvm</guimenu>
    </term>
    <listitem>
     <para>
      Provides the hypervisors (Docker, &hyper;, &kvm;, QEMU, VMware vSphere,
      &xen;, and z/VM) and tools needed to manage the &vmguest;s. Only one
      hypervisor can be deployed on a single compute node but you can use
      different hypervisors in your cloud by deploying different hypervisors
      to different &compnode;s. A <literal>nova-compute-*</literal>
      role needs to be installed on every &compnode;. However, not all
      hypervisors need to be deployed.
     </para>
     <para>
      Each image that will be made available in &cloud; to start an
      &vmguest; is bound to a hypervisor. Each hypervisor can be deployed
      on multiple &compnode;s (except for the VMWare vSphere role, see
      below). In a multi-hypervisor deployment you should make sure to
      deploy the <literal>nova-compute-*</literal> roles in a way, that
      enough compute power is available for each hypervisor.
     </para>
     <note>
      <title>Re-assigning Hypervisors</title>
      <para>
       <remark condition="clarity">
        2013-08-05 - fs: Is this true?
       </remark>
       Existing <literal>nova-compute-*</literal> nodes can be changed
       in a productive &cloud; without service interruption. You need to
       <quote>evacuate</quote>
<!-- (see TODO) -->
       the node, re-assign a new <literal>nova-compute</literal> role
       via the &o_comp; &barcl; and <guimenu>Apply</guimenu> the
       change. <guimenu>nova-compute-vmware</guimenu> can only be
       deployed on a single node.
      </para>
     </note>
     <important>
      <title>Deploying &hyper;</title>
      <para>
       <literal>nova-compute-hyperv</literal> can only be deployed to
       &compnode;s running either Microsoft &hyper; Server or Windows
       Server 2012. Being able to set up such &compnode;s requires to set
       up a netboot environment for Windows. Refer to
       <xref linkend="app.deploy.hyperv"/> for details.
      </para>
      <para>
       The default password for &hyper; &compnode;s will be
       <quote>crowbar</quote>.
      </para>
     </important>
     <important>
      <title>Deploying VMware vSphere (vmware)</title>
      <para>
       <remark condition="clarity">
        2013-08-05 - fs: What network requirements/adjustments are needed ??
       </remark>
       VMware vSphere is not supported <quote>natively</quote> by
       &cloud;&mdash;it rather delegates requests to an existing
       vCenter. It requires preparations at the vCenter and post install
       adjustments of the &compnode;. See
       <xref linkend="app.deploy.vmware"/> for instructions.
       <guimenu>nova-compute-vmware</guimenu> can only be deployed on
       a single &compnode;.
      </para>
     </important>
    </listitem>
   </varlistentry>
  </variablelist>

  <figure>
   <title>The &o_comp; &Barcl;: Node Deployment Example with Two KVM Nodes</title>
   <mediaobject>
    <imageobject role="fo">
     <imagedata fileref="depl_barclamp_nova_node_deployment.png" width="100%" format="png"/>
    </imageobject>
    <imageobject role="html">
     <imagedata fileref="depl_barclamp_nova_node_deployment.png" width="75%" format="png"/>
    </imageobject>
   </mediaobject>
  </figure>

  <sect2 xml:id="sec.depl.ostack.nova.ha">
   <title>&haSetup; for &o_comp;</title>
   <para> Making <guimenu>nova-controller</guimenu> highly available requires no
    special configuration&mdash;it is sufficient to deploy it on a cluster.
     <remark condition="clarity"> 2016-02-08 - taroth: DEVs, do we still need
     the sentence above? or is it superfluous now that we can have a fully
     fledged HA setup for Comp Nodes?</remark>
   </para>
   <para>To enable &ha; for &compnode;s, deploy the following roles to
    one or more clusters with remote nodes:</para>
   <itemizedlist>
    <listitem>
     <para>nova-compute-kvm</para>
    </listitem>
    <listitem>
     <para>nova-compute-qemu</para>
    </listitem>
    <listitem>
     <para>nova-compute-xen</para>
    </listitem>
   </itemizedlist>
   <para>
    <remark>taroth 2016-02-09: not sure if we need to mention the following here
     - in case we should advise to use separate cluster for controller and
     compute nodes (see https://bugzilla.suse.com/show_bug.cgi?id=964205#c4), I
     would remove the following sentence</remark>The cluster to which you deploy
    the roles above can be completely independent of the one to which the role
     <literal>nova-controller</literal> is deployed (as
     <literal>nova-controller</literal> needs to be installed on a
    &contrnode;).</para>
  </sect2>
 </sect1>
 <sect1 xml:id="sec.depl.ostack.dash">
  <title>Deploying &o_dash; (&ostack; &dash;)</title>

  <para>
   The last service that needs to be deployed is &o_dash;, the
   &ostack; &dash;. It provides a Web interface for users to start and
   stop &vmguest;s and for administrators to manage users, groups, roles,
   etc. &o_dash; should be installed on a &contrnode;. To make
   &o_dash; highly available, deploy it on a cluster.
  </para>

  <para>
   The following attributes can be configured:
  </para>

  <variablelist>
   <varlistentry>
    <term>Session Timeout</term>
    <listitem>
     <para>
      Timeout (in minutes) after which a user is been logged out
      automatically. The default value is set to 4 hours (240 minutes).
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>
     <guimenu>
      User Password Validation: Regular expression used for password
      validation
     </guimenu>
    </term>
    <listitem>
     <para>
      Specify a regular expression with which to check the password. The
      default expression (<literal>.{8,}</literal>) tests for a minimum length
      of 8 characters. The string you enter is interpreted as a Python regular
      expression (see
      <link xlink:href="http://docs.python.org/2.7/library/re.html#module-re"/>
      for a reference).
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><guimenu>
      User Password Validation: Text to display if the password does not pass
      validation
     </guimenu>
    </term>
    <listitem>
     <para>
      Error message that will be displayed in case the password validation
      fails.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>SSL Support: Protocol</term>
    <listitem>
     <para>
      Choose whether to encrypt public communication
      (<guimenu>HTTPS</guimenu>) or not (<guimenu>HTTP</guimenu>). If choosing
      <guimenu>HTTPS</guimenu>, you have two choices. You can either
      <guimenu>Generate (self-signed) certificates</guimenu> or provide the
      locations for the certificate key pair files
      and,&mdash;optionally&mdash; the certificate chain file. Using
      self-signed certificates is for testing purposes only and should never
      be used in production environments!
     </para>
    </listitem>
   </varlistentry>
  </variablelist>

  <figure>
   <title>The &o_dash; &Barcl;</title>
   <mediaobject>
    <imageobject role="fo">
     <imagedata fileref="depl_barclamp_nova_dashboard.png" width="100%" format="png"/>
    </imageobject>
    <imageobject role="html">
     <imagedata fileref="depl_barclamp_nova_dashboard.png" width="75%" format="png"/>
    </imageobject>
   </mediaobject>
  </figure>

  <sect2 xml:id="sec.depl.ostack.dash.ha">
   <title>&haSetup; for &o_dash;</title>
   <para>
    Making &o_dash; highly available requires no special
    configuration&mdash;it is sufficient to deploy it on a cluster.
   </para>
  </sect2>
 </sect1>
 <sect1 xml:id="sec.depl.ostack.heat">
  <title>Deploying &o_orch; (Optional)</title>

  <para>
   &o_orch; is a template-based orchestration engine that enables you to,
   for example, start workloads requiring multiple servers or to
   automatically restart &vmguest;s if needed. It also brings
   auto-scaling to &cloud; by automatically starting additional
   &vmguest;s if certain criteria are met. For more information about
   &o_orch; refer to the &ostack; documentation at
   <link xlink:href="http://docs.openstack.org/developer/heat/"/>.
  </para>

  <para>
   &o_orch; should be deployed on a &contrnode;. To make &o_orch;
   highly available, deploy it on a cluster.
  </para>

  <para>
   The following attributes can be configured for &o_orch;:
  </para>

  <variablelist>
   <varlistentry>
    <term><guimenu>Verbose Logging</guimenu>
    </term>
    <listitem>
     <para>
      Shows debugging output in the log files when set to
      <guimenu>true</guimenu>.
     </para>
    </listitem>
   </varlistentry>
  </variablelist>

  <figure>
   <title>The &o_orch; &Barcl;</title>
   <mediaobject>
    <imageobject role="fo">
     <imagedata fileref="depl_barclamp_heat.png" width="100%" format="png"/>
    </imageobject>
    <imageobject role="html">
     <imagedata fileref="depl_barclamp_heat.png" width="75%" format="png"/>
    </imageobject>
   </mediaobject>
  </figure>

  <sect2 xml:id="sec.depl.ostack.heat.ha">
   <title>&haSetup; for &o_orch;</title>
   <para>
    Making &o_orch; highly available requires no special
    configuration&mdash;it is sufficient to deploy it on a cluster.
   </para>
  </sect2>
 </sect1>
 <sect1 xml:id="sec.depl.ostack.ceilometer">
  <title>Deploying &o_meter; (Optional)</title>

  <para>
   <remark condition="clarity">
    2013-10-04 - fs: Which software/billing solution can make use of the
    ceilometer data?
   </remark>
   &o_meter; collects CPU and networking data from &cloud;. This data
   can be used by a billing system to enable customer billing. Deploying
   &o_meter; is optional.
  </para>

  <para>
   For more information about &o_meter; refer to the &ostack;
   documentation at
   <link xlink:href="http://docs.openstack.org/developer/ceilometer/"/>.
  </para>

  <important>
   <title>&o_meter; Restrictions</title>
   <para>
    As of &productname; &productnumber; data measuring is only
    supported for &kvm;, &xen; and Windows &vmguest;s. Other hypervisors and
    &cloud; features such as object or block storage will not be
    measured.
   </para>
  </important>

  <para>
   The following attributes can be configured for &o_meter;:
  </para>

  <variablelist>
   <varlistentry>
    <term>Interval used for CPU/disk/network/other meter updates (in seconds)</term>
    <listitem>
     <para>
      Specify an interval in seconds after which &o_meter; performs an
      update of the specified meter.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>Evaluation interval for threshold alarms (in seconds)</term>
    <listitem>
     <para>
      Set the interval after which to check whether to raise an alarm
      because a threshold has been exceeded. For performance reasons, do not
      set a value lower than the default (60s).
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>Use MongoDB instead of standard database
    </term>
    <listitem>
     <para>
      &o_meter; collects a huge amount of data, which is written to a
      database. In a production system it is recommended to use a separate
      database for &o_meter; rather than the standard database that is
      also used by the other &cloud; services. MongoDB is optimized to
      write a lot of data. As of &productname; &productnumber; MongoDB
      is only included as a technology preview and not supported.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>How long are metering/event samples kept in the database (in days)
    </term>
    <listitem>
     <para>
      Specify how long to keep the data. -1 means that samples are kept in
      the database forever.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>Verbose Logging
    </term>
    <listitem>
     <para>
      Shows debugging output in the log files when set to
      <guimenu>true</guimenu>.
     </para>
    </listitem>
   </varlistentry>
  </variablelist>

  <figure>
   <title>The &o_meter; &Barcl;</title>
   <mediaobject>
    <imageobject role="fo">
     <imagedata fileref="depl_barclamp_ceilometer.png" width="100%" format="png"/>
    </imageobject>
    <imageobject role="html">
     <imagedata fileref="depl_barclamp_ceilometer.png" width="75%" format="png"/>
    </imageobject>
   </mediaobject>
  </figure>

  <para>
   The &o_meter; service consists of five different roles:
  </para>

  <variablelist>
   <varlistentry>
    <term><guimenu>ceilometer-server</guimenu>
    </term>
    <listitem>
     <para>
      The &o_meter; API server role. This role needs to be deployed on a
      &contrnode;. &o_meter; collects approximately 200 bytes of data
      per hour and &vmguest;. Unless you have a very huge number of
      &vmguest;s, there is no need to install it on a dedicated node.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><guimenu>ceilometer-polling</guimenu></term>
    <listitem>
     <para>
      The polling agent listens to the message bus to collect data. It needs
      to be deployed on a &contrnode;. It can be deployed on the same
      node as <guimenu>ceilometer-server</guimenu>.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><guimenu>ceilometer-agent</guimenu></term>
    <listitem>
     <para>
      The compute agents collect data from the compute nodes. They need to be
      deployed on all &kvm; and &xen; compute nodes in your cloud (other
      hypervisors&mdash;except for &hyper;&mdash; are currently not
      supported).
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><guimenu>ceilometer-agent-hyperv</guimenu></term>
    <listitem>
     <para>
      This compute agents collect data from the compute nodes running on
      Microsoft Windows. It needs need to be deployed on all &hyper;
      &compnode;s in your cloud.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><guimenu>ceilometer-swift-proxy-middleware</guimenu></term>
    <listitem>
     <para>
      An agent collecting data from the &swift; nodes. This role needs to
      be deployed on the same node as swift-proxy.
     </para>
    </listitem>
   </varlistentry>
  </variablelist>

  <figure>
   <title>The &o_meter; &Barcl;: Node Deployment</title>
   <mediaobject>
    <imageobject role="fo">
     <imagedata fileref="depl_barclamp_ceilometer_deployment.png" width="100%" format="png"/>
    </imageobject>
    <imageobject role="html">
     <imagedata fileref="depl_barclamp_ceilometer_deployment.png" width="75%" format="png"/>
    </imageobject>
   </mediaobject>
  </figure>

  <sect2 xml:id="sec.depl.ostack.ceilometer.ha">
   <title>&haSetup; for &o_meter;</title>
   <para>
    Making &o_meter; highly available requires no special
    configuration&mdash;it is sufficient to deploy the roles
    <guimenu>ceilometer-server</guimenu> and
    <guimenu>ceilometer-polling</guimenu> on a cluster.
   </para>
  </sect2>
 </sect1>
 <sect1 xml:id="sec.depl.ostack.trove">
  <title>Deploying &o_dbaas; (Optional)</title>

  <para>
   &o_dbaas; is a Database-as-a-Service for &cloud;. It provides
   database instances which can be used by all &vmguest;s. With
   &o_dbaas; being deployed, &cloud; users no longer need to deploy
   and maintain their own database applications. For more information about
   &o_dbaas;; refer to the &ostack; documentation at
   <link xlink:href="http://docs.openstack.org/developer/trove/"/>.
  </para>

  <important>
   <title>Technology Preview</title>
   <para>
    &o_dbaas; is only included as a technology preview and not supported.
   </para>
  </important>

  <para>
   &o_dbaas; should be deployed on a dedicated &contrnode;.
  </para>

  <para>
   The following attributes can be configured for &o_dbaas;:
  </para>

  <variablelist>
   <varlistentry>
    <term><guimenu>Enable Trove Volume Support</guimenu>
    </term>
    <listitem>
     <para>
      When enabled, &o_dbaas; will use a &o_blockstore; volume to
      store the data.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><guimenu>Logging: Verbose</guimenu>
    </term>
    <listitem>
     <para>
      Increases the amount of information that is written to the log files
      when set to <guimenu>true</guimenu>.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><guimenu>Logging: Debug</guimenu>
    </term>
    <listitem>
     <para>
      Shows debugging output in the log files when set to
      <guimenu>true</guimenu>.
     </para>
    </listitem>
   </varlistentry>
  </variablelist>

  <figure>
   <title>The &o_dbaas; &Barcl;</title>
   <mediaobject>
    <imageobject role="fo">
     <imagedata fileref="depl_barclamp_trove.png" width="100%" format="png"/>
    </imageobject>
    <imageobject role="html">
     <imagedata fileref="depl_barclamp_trove.png" width="75%" format="png"/>
    </imageobject>
   </mediaobject>
  </figure>

  <sect2 xml:id="sec.depl.ostack.trove.ha">
   <title>&haSetup; for &o_dbaas;</title>
   <para>
    A &haSetup; for &o_dbaas; is currently not supported.
   </para>
  </sect2>
 </sect1>
 <sect1 xml:id="sec.depl.ostack.tempest">
  <title>Deploying &o_testsuite; (Optional)</title>

  <para>
   &o_testsuite; is an integration test suite for &cloud; written in
   Python. It contains multiple integration tests for validating your &cloud;
   deployment. For more information about &o_testsuite; refer to the &ostack;
   documentation at <link
   xlink:href="http://docs.openstack.org/developer/tempest/"/>.
  </para>

  <important>
   <title>Technology Preview</title>
   <para>
    &o_testsuite; is only included as a technology preview and not supported.
   </para>
   <para>
    &o_testsuite; may be used for testing whether the intended setup will run
    without problems. It should not be used in a production environment.
   </para>
  </important>

  <para>
   &o_testsuite; should be deployed on a &contrnode;.
  </para>

  <para>
   The following attributes can be configured for &o_testsuite;:
  </para>

  <variablelist>
   <varlistentry>
    <term><guimenu>Choose User name / Password</guimenu>
    </term>
    <listitem>
     <para>
      Credentials for a regular user. If the user does not exist, it will be
      created.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><guimenu>Choose Tenant</guimenu>
    </term>
    <listitem>
     <para>
      Tenant to be used by &o_testsuite;. If it does not exist, it will be
      created. It is safe to stick with the default value.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><guimenu>Choose Tempest Admin User name/Password</guimenu>
    </term>
    <listitem>
     <para>
      Credentials for an admin user. If the user does not exist, it will be
      created.
     </para>
    </listitem>
   </varlistentry>
  </variablelist>

  <figure>
   <title>The &o_testsuite; &Barcl;</title>
   <mediaobject>
    <imageobject role="fo">
     <imagedata fileref="depl_barclamp_tempest.png" width="100%" format="png"/>
    </imageobject>
    <imageobject role="html">
     <imagedata fileref="depl_barclamp_tempest.png" width="75%" format="png"/>
    </imageobject>
   </mediaobject>
  </figure>

  <sect2 xml:id="sec.depl.ostack.tempest.ha">
   <title>&haSetup; for &o_testsuite;</title>
   <para>
    &o_testsuite; cannot be made highly available.
   </para>
  </sect2>
 </sect1>
 <sect1 xml:id="sec.depl.ostack.final">
  <title>How to Proceed</title>

  <para>
   With a successful deployment of the &ostack; &dash;, the
   &productname; installation is finished. To be able to test your setup
   by starting an &vmguest; one last step remains to be
   done&mdash;uploading an image to the &o_img; service. Refer to the
   &cloudsuppl;, chapter <citetitle>Manage images</citetitle>
<!--<xref linkend="sec.adm.cli.img"/>-->
   for instructions. Images for &cloud; can be built in SUSE Studio.
   Refer to the &cloudsuppl;, section <citetitle>Building Images with
   &susestudio;</citetitle>.
  </para>

  <para>
   Now you can hand over to the cloud administrator to set up users, roles,
   flavors, etc.&mdash;refer to the &cloudadmin; for details. The
   default credentials for the &ostack; &dash; are user name
   <literal>admin</literal> and password <literal>crowbar</literal>.
  </para>
 </sect1>
</chapter>
