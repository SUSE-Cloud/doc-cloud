<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE chapter
[
  <!ENTITY % entities SYSTEM "entity-decl.ent">
  %entities;
]>
<!-- Converted by suse-upgrade version 1.1 -->
<chapter xmlns="http://docbook.org/ns/docbook" xmlns:xi="http://www.w3.org/2001/XInclude" xmlns:xlink="http://www.w3.org/1999/xlink" version="5.0" xml:id="cha.depl.ostack">
 <title>Deploying the &ostack; Services</title>
 <info>
  <dm:docmanager xmlns:dm="urn:x-suse:ns:docmanager">
   <dm:maintainer>fs</dm:maintainer>
   <dm:status>editing</dm:status>
   <dm:deadline/>
   <dm:priority/>
   <dm:translation>no</dm:translation>
   <dm:languages/>
  </dm:docmanager>
 </info>
 <para>
  After the nodes are installed and configured you can start deploying the
  &ostack; components to finalize the installation. The components need to be
  deployed in a given order, because they depend on one another. The
  <guimenu>Pacemaker</guimenu> component for an &hasetup; is the only exception
  from this rule&mdash;it can be set up at any time. However, when deploying
  &productname; from scratch, we recommend deploying the
  <guimenu>Pacemaker</guimenu> proposal(s) first. Deployment for all components
  is done from the &crow; Web interface through recipes, so-called
  <quote>&barcl;s</quote>. (See <xref linkend="sec.depl.services"/> for a table
  of all roles and services, and how to start and stop them.)
 </para>
 <para>
  The components controlling the cloud, including storage management and
  control components, need to be installed on the &contrnode;(s) (refer to
  <xref linkend="sec.depl.arch.components.control"/> for more information).
  However, you may <emphasis>not</emphasis> use your &contrnode;(s) as a
  compute node or storage host for &o_objstore; or &ceph;. These components may
  <emphasis>not</emphasis> be installed on the &contrnode;(s):
  <guimenu>swift-storage</guimenu>, all &ceph; components, and
  <guimenu>nova-compute-*</guimenu>. These components must be installed on
  dedicated nodes.
 </para>
 <para>
  When deploying an &hasetup;, the controller nodes are replaced by one or more
  controller clusters consisting of at least two nodes, and three are
  recommended. We recommend setting up three separate clusters for data,
  services, and networking. See <xref linkend="sec.depl.req.ha"/> for more
  information on requirements and recommendations for an &hasetup;.
 </para>
 <para>
  The &ostack; components need to be deployed in the following order. For
  general instructions on how to edit and deploy &barcl;s, refer to
  <xref linkend="sec.depl.ostack.barclamps"/>. Any optional components that you
  elect to use must be installed in their correct order.
 </para>
 <orderedlist spacing="normal">
  <listitem>
<!-- Pacemaker -->
   <para>
    <xref linkend="sec.depl.ostack.pacemaker" xrefstyle="select:title nopage"/>
   </para>
  </listitem>
  <listitem>
<!-- Database -->
   <para>
    <xref linkend="sec.depl.ostack.db" xrefstyle="select:title nopage"/>
   </para>
  </listitem>
  <listitem>
<!-- RabbitMQ -->
   <para>
    <xref linkend="sec.depl.ostack.rabbit" xrefstyle="select:title nopage"/>
   </para>
  </listitem>
  <listitem>
<!-- Keystone -->
   <para>
    <xref linkend="sec.depl.ostack.keystone" xrefstyle="select:title nopage"/>
   </para>
  </listitem>
  <listitem>
<!-- Ceph -->
   <para>
    <xref linkend="sec.depl.ostack.ceph" xrefstyle="select:title nopage"/>
   </para>
  </listitem>
  <listitem>
<!-- Swift -->
   <para>
    <xref linkend="sec.depl.ostack.swift" xrefstyle="select:title nopage"/>
   </para>
  </listitem>
  <listitem>
<!-- Glance -->
   <para>
    <xref linkend="sec.depl.ostack.glance" xrefstyle="select:title nopage"/>
   </para>
  </listitem>
<!-- Cinder -->
  <listitem>
   <para>
    <xref linkend="sec.depl.ostack.cinder" xrefstyle="select:title nopage"/>
   </para>
  </listitem>
<!-- Neutrum -->
  <listitem>
   <para>
    <xref linkend="sec.depl.ostack.quantum" xrefstyle="select:title nopage"/>
   </para>
  </listitem>
<!-- Nova -->
  <listitem>
   <para>
    <xref linkend="sec.depl.ostack.nova" xrefstyle="select:title nopage"/>
   </para>
  </listitem>
<!-- Horizon -->
  <listitem>
   <para>
    <xref linkend="sec.depl.ostack.dash" xrefstyle="select:title nopage"/>
   </para>
  </listitem>
<!-- Heat -->
  <listitem>
   <para>
    <xref linkend="sec.depl.ostack.heat" xrefstyle="select:title nopage"/>
   </para>
  </listitem>
<!-- Ceilometer -->
  <listitem>
   <para>
    <xref linkend="sec.depl.ostack.ceilometer" xrefstyle="select:title nopage"/>
   </para>
  </listitem>
<!-- Aodh -->
  <listitem>
   <para>
    <xref linkend="sec.depl.ostack.aodh" xrefstyle="select:title nopage"/>
   </para>
  </listitem>
<!-- Manila -->
  <listitem>
   <para>
    <xref linkend="sec.depl.ostack.manila" xrefstyle="select:title nopage"/>
   </para>
  </listitem>
<!-- Trove -->
  <listitem>
   <para>
    <xref linkend="sec.depl.ostack.trove" xrefstyle="select:title nopage"/>
   </para>
  </listitem>
<!-- Tempest -->
  <listitem>
   <para>
    <xref linkend="sec.depl.ostack.tempest" xrefstyle="select:title nopage"/>
   </para>
  </listitem>
<!-- Magnum -->
  <listitem>
   <para>
    <xref linkend="sec.depl.ostack.magnum" xrefstyle="select:title nopage"/>
   </para>
  </listitem>
<!-- Monasca -->
  <listitem>
   <para>
    <xref linkend="sec.depl.ostack.monasca" xrefstyle="select:title nopage"/>
   </para>
  </listitem>
 </orderedlist>
 <sect1 xml:id="sec.depl.ostack.pacemaker">
  <title>Deploying Pacemaker (Optional, &haSetup; Only)</title>

  <para>
   To make the &cloud; controller functions and the &compnode;s highly
   available, set up one or more clusters by deploying Pacemaker (see
   <xref
   linkend="sec.depl.req.ha"/> for details). Since it is possible (and
   recommended) to deploy more than one cluster, a separate proposal needs to
   be created for each cluster.
  </para>

  <para>
   Deploying Pacemaker is optional. In case you do not want to deploy it, skip
   this section and start the node deployment by deploying the database as
   described in <xref linkend="sec.depl.ostack.db"/>.
  </para>

  <note>
   <title>Number of Cluster Nodes</title>
   <para>
    To set up a cluster, at least two nodes are required. If you are setting up
    a cluster for storage with replicated storage via DRBD (for example for a
    cluster for the database and RabbitMQ), exactly two nodes are required. For
    all other setups an odd number of nodes with a minimum of three nodes is
    strongly recommended. See <xref linkend="sec.depl.reg.ha.general"/> for
    more information.
   </para>
  </note>

  <para>
   To create a proposal, go to <menuchoice> <guimenu>Barclamps</guimenu>
   <guimenu>OpenStack</guimenu> </menuchoice> and click <guimenu>Edit</guimenu>
   for the Pacemaker &barcl;. A drop-down box where you can enter a name and a
   description for the proposal opens. Click <guimenu>Create</guimenu> to open
   the configuration screen for the proposal.
  </para>

  <informalfigure>
   <mediaobject>
    <textobject><phrase>Create Pacemaker Proposal</phrase>
    </textobject>
    <imageobject role="fo">
     <imagedata fileref="depl_barclamp_pacemaker_proposal.png" width="100%" format="png"/>
    </imageobject>
    <imageobject role="html">
     <imagedata fileref="depl_barclamp_pacemaker_proposal.png" width="75%" format="png"/>
    </imageobject>
   </mediaobject>
  </informalfigure>

  <important xml:id="ann.depl.ostack.pacemaker.prop_name">
   <title>Proposal Name</title>
   <para>
    The name you enter for the proposal will be used to generate host names for
    the virtual IP addresses of HAProxy. By default, the names follow this
    scheme:
   </para>
   <simplelist>
    <member><literal>cluster-<replaceable>PROPOSAL_NAME</replaceable>.<replaceable>FQDN</replaceable></literal>
    (for the internal name)</member>
    <member><literal>public.cluster-<replaceable>PROPOSAL_NAME</replaceable>.<replaceable>FQDN</replaceable></literal>
    (for the public name)</member>
   </simplelist>
   <para>
    For example, when <replaceable>PROPOSAL_NAME</replaceable> is set to
    <literal>data</literal>, this results in the following names:
   </para>
   <simplelist>
    <member><literal>cluster-data.&exampledomain;</literal>
    </member>
    <member><literal>public.cluster-data.&exampledomain;</literal>
    </member>
   </simplelist>
   <para>
    For requirements regarding SSL encryption and certificates, see
    <xref
     linkend="sec.depl.req.ssl"/>.
   </para>
  </important>

  <para>
   The following options are configurable in the Pacemaker configuration
   screen:
  </para>

  <variablelist>
   <varlistentry>
    <term>Transport for Communication</term>
    <listitem>
     <para>
      Choose a technology used for cluster communication. You can choose
      between <guimenu>Multicast (UDP)</guimenu>, sending a message to multiple
      destinations, or <guimenu>Unicast (UDPU)</guimenu>, sending a message to
      a single destination. By default unicast is used.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><guimenu>Policy when cluster does not have quorum</guimenu>
    </term>
    <listitem>
     <para>
      Whenever communication fails between one or more nodes and the rest of
      the cluster a <quote>cluster partition</quote> occurs. The nodes of a
      cluster are split in partitions but are still active. They can only
      communicate with nodes in the same partition and are unaware of the
      separated nodes. The cluster partition that has the majority of nodes is
      defined to have <quote>quorum</quote>.
     </para>
     <para>
      This configuration option defines what to do with the cluster
      partition(s) that do not have the quorum. See
      <link xlink:href="&suse-onlinedoc;/sle-ha-12/book_sleha/data/sec_ha_config_basics_global.html"/>,
      section <citetitle>Option no-quorum-policy</citetitle> for details.
     </para>
     <para>
      The recommended setting is to choose <guimenu>Stop</guimenu>. However,
      <guimenu>Ignore</guimenu> is enforced for two-node clusters to ensure
      that the remaining node continues to operate normally in case the other
      node fails. For clusters using shared resources, choosing
      <guimenu>freeze</guimenu> may be used to ensure that these resources
      continue to be available.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry xml:id="vle.pacemaker.barcl.stonith">
    <term>STONITH: Configuration mode for &stonith;
    </term>
    <listitem>
     <para>
      <quote>Misbehaving</quote> nodes in a cluster are shut down to prevent
      them from causing trouble. This mechanism is called &stonith;
      (<quote>Shoot the other node in the head</quote>). &stonith; can be
      configured in a variety of ways, refer to
      <link xlink:href="&suse-onlinedoc;/sle-ha-12/book_sleha/data/cha_ha_fencing.html"/>
      for details. The following configuration options exist:
     </para>
     <variablelist>
      <varlistentry>
       <term><guimenu>Configured manually</guimenu>
       </term>
       <listitem>
        <para>
         &stonith; will not be configured when deploying the &barcl;. It needs
         to be configured manually as described in
         <link xlink:href="&suse-onlinedoc;/sle-ha-12/book_sleha/data/cha_ha_fencing.html"/>.
         For experts only.
        </para>
       </listitem>
      </varlistentry>
      <varlistentry>
       <term><guimenu>Configured with IPMI data from the IPMI &barcl;</guimenu>
       </term>
       <listitem>
        <para>
         Using this option automatically sets up &stonith; with data received
         from the IPMI &barcl;. Being able to use this option requires that
         IPMI is configured for all cluster nodes. This should be done by
         default. To check or change the IPMI deployment, go to <menuchoice>
         <guimenu>Barclamps</guimenu> <guimenu>Crowbar</guimenu>
         <guimenu>IPMI</guimenu> <guimenu>Edit</guimenu> </menuchoice>. Also
         make sure the <guimenu>Enable BMC</guimenu> option is set to
         <guimenu>true</guimenu> on this &barcl;.
        </para>
        <important>
         <title>&stonith; Devices Must Support IPMI</title>
         <para>
          To configure &stonith; with the IPMI data, <emphasis>all</emphasis>
          &stonith; devices must support IPMI. Problems with this setup may
          occur with IPMI implementations that are not strictly standards
          compliant. In this case it is recommended to set up &stonith; with
          &stonith; block devices (SBD).
         </para>
        </important>
       </listitem>
      </varlistentry>
      <varlistentry>
       <term><guimenu>Configured with &stonith; Block Devices (SBD)</guimenu>
       </term>
       <listitem>
        <para>
         This option requires manually setting up shared storage and a watchdog
         on the cluster nodes before applying the proposal. To do so, proceed
         as follows:
        </para>
        <orderedlist spacing="normal">
         <listitem>
          <para>
           Prepare the shared storage. The path to the shared storage device
           must be persistent and consistent across all nodes in the cluster.
           The SBD device must not use host-based RAID, cLVM2, nor reside on a
           DRBD* instance.
          </para>
         </listitem>
         <listitem>
          <para>
           Install the package <systemitem class="resource">sbd</systemitem> on
           all cluster nodes.
          </para>
         </listitem>
         <listitem>
          <para>
           Initialize the SBD device with by running the following command.
           Make sure to replace
           <filename>/dev/<replaceable>SBD</replaceable></filename> with the
           path to the shared storage device.
          </para>
<screen>sbd -d /dev/<replaceable>SBD</replaceable> create</screen>
          <para>
           Refer to
           <link xlink:href="&suse-onlinedoc;/sle-ha-12/book_sleha/data/sec_ha_storage_protect_fencing.html#pro_ha_storage_protect_sbd_create"/>
           for details.
          </para>
         </listitem>
        </orderedlist>
        <para>
         In <guimenu>Kernel module for watchdog</guimenu>, specify the
         respective kernel module to be used. Find the most commonly used
         watchdog drivers in the following table:
        </para>
<!--taroth 2016-11-28: table taken from
         https://github.com/SUSE/doc-sleha/blob/develop/xml/ha_storage_protection.xml,
         pro.ha.storage.protect.watchdog-->
        <informaltable>
         <tgroup cols="2">
          <thead>
           <row>
            <entry>Hardware</entry>
            <entry>Driver</entry>
           </row>
          </thead>
          <tbody>
           <row>
            <entry>HP</entry>
            <entry><systemitem class="resource">hpwdt</systemitem>
            </entry>
           </row>
           <row>
            <entry>Dell, Fujitsu, Lenovo (Intel TCO)</entry>
            <entry><systemitem class="resource">iTCO_wdt</systemitem>
            </entry>
           </row>
           <row>
            <entry>VM on z/VM on IBM mainframe</entry>
            <entry><systemitem class="resource">vmwatchdog</systemitem>
            </entry>
           </row>
           <row>
            <entry>Xen VM (DomU)</entry>
            <entry><systemitem class="resource">xen_xdt</systemitem>
            </entry>
           </row>
           <row>
            <entry>Generic</entry>
            <entry><systemitem class="resource">softdog</systemitem>
            </entry>
           </row>
          </tbody>
         </tgroup>
        </informaltable>
        <para>
         If your hardware is not listed above, either ask your hardware vendor
         for the right name or check the following directory for a list of
         choices:
         <filename>/lib/modules/<replaceable>KERNEL_VERSION</replaceable>/kernel/drivers/watchdog</filename>.
        </para>
        <para>
         Alternatively, list the drivers that have been installed with your
         kernel version:
        </para>
<screen>&prompt.root;<command>rpm</command> -ql kernel-<replaceable>VERSION</replaceable> | <command>grep</command> watchdog</screen>
        <para>
         If the nodes need different watchdog modules, leave the text box
         empty.
        </para>
        <para>
         After the shared storage has been set up, specify the path using the
         <quote>by-id</quote> notation
         (<filename>/dev/disk/by-id/<replaceable>DEVICE</replaceable></filename>).
         It is possible to specify multiple paths as a comma-separated list.
        </para>
        <para>
         Deploying the &barcl; will automatically complete the SBD setup on the
         cluster nodes by starting the SBD daemon and configuring the fencing
         resource.
        </para>
       </listitem>
      </varlistentry>
      <varlistentry>
       <term><guimenu>
         Configured with one shared resource for the whole cluster
        </guimenu>
       </term>
       <listitem>
        <para>
         All nodes will use the identical configuration. Specify the
         <guimenu>Fencing Agent</guimenu> to use and enter
         <guimenu>Parameters</guimenu> for the agent.
        </para>
        <para>
         To get a list of &stonith; devices which are supported by the High
         Availability Extension, run the following command on an already
         installed cluster nodes: <command>stonith -L</command>. The list of
         parameters depends on the respective agent. To view a list of
         parameters use the following command:
        </para>
<screen>stonith -t <replaceable>agent</replaceable> -n</screen>
       </listitem>
      </varlistentry>
      <varlistentry>
       <term><guimenu>Configured with one resource per node</guimenu>
       </term>
       <listitem>
        <para>
         All nodes in the cluster use the same <guimenu>Fencing
         Agent</guimenu>, but can be configured with different parameters. This
         setup is, for example, required when nodes are in different chassis
         and therefore need different ILO parameters.
        </para>
        <para>
         To get a list of &stonith; devices which are supported by the High
         Availability Extension, run the following command on an already
         installed cluster nodes: <command>stonith -L</command>. The list of
         parameters depends on the respective agent. To view a list of
         parameters use the following command:
        </para>
<screen>stonith -t <replaceable>agent</replaceable> -n</screen>
       </listitem>
      </varlistentry>
      <varlistentry>
       <term><guimenu>Configured for nodes running in libvirt</guimenu>
       </term>
       <listitem>
        <para>
         Use this setting for completely virtualized test installations. This
         option is not supported.
        </para>
       </listitem>
      </varlistentry>
     </variablelist>
    </listitem>
   </varlistentry>
   <varlistentry xml:id="var.depl.ostack.pacemaker.corosync_fencing">
    <term>STONITH: Do not start corosync on boot after fencing</term>
    <listitem>
     <para>
      With &stonith;, Pacemaker clusters with two nodes may sometimes hit an
      issue known as &stonith; deathmatch where each node kills the other one,
      resulting in both nodes rebooting all the time. Another similar issue in
      Pacemaker clusters is the fencing loop, where a reboot caused by
      &stonith; will not be enough to fix a node and it will be fenced again
      and again.
     </para>
     <para>
      This setting can be used to limit these issues. When set to
      <guimenu>true</guimenu>, a node that has not been properly shut down or
      rebooted will not start the services for Pacemaker on boot. Instead, the
      node will wait for action from the &cloud; operator. When set to
      <guimenu>false</guimenu>, the services for Pacemaker will always be
      started on boot. The <guimenu>Automatic</guimenu> value is used to have
      the most appropriate value automatically picked: it will be
      <guimenu>true</guimenu> for two-node clusters (to avoid &stonith;
      deathmatches), and <guimenu>false</guimenu> otherwise.
     </para>
     <para>
      When a node boots but not starts corosync because of this setting, then
      the node's status is in the <guimenu>Node Dashboard</guimenu> is set to
      "<literal>Problem</literal>" (red dot). To make this node usable again,
      see <xref linkend="sec.deploy.ha_recovery.contr.node.add"/>.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>Mail Notifications: Enable Mail Notifications</term>
    <listitem>
     <para>
      Get notified of cluster node failures via e-mail. If set to
      <guimenu>true</guimenu>, you need to specify which <guimenu>SMTP
      Server</guimenu> to use, a prefix for the mails' subject and sender and
      recipient addresses. Note that the SMTP server must be accessible by the
      cluster nodes.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>DRBD: Prepare Cluster for DRBD</term>
    <listitem>
     <para>
      Set up DRBD for replicated storage on the cluster. This option requires a
      two-node cluster with a spare hard disk for each node. The disks should
      have a minimum size of 100 GB. Using DRBD is recommended for making the
      database and RabbitMQ highly available. For other clusters, set this
      option to <guimenu>False</guimenu>.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><guimenu>HAProxy: Public name for public virtual IP</guimenu>
    </term>
    <listitem>
     <para>
      The public name is the host name that will be used instead of the
      generated public name (see
      <xref linkend="ann.depl.ostack.pacemaker.prop_name"/>) for the public
      virtual IP address of &haproxy;. (This is the case when registering
      public endpoints, for example). Any name specified here needs to be
      resolved by a name server placed outside of the &cloud; network.
     </para>
    </listitem>
   </varlistentry>
  </variablelist>

  <figure>
   <title>The Pacemaker &Barcl;</title>
   <mediaobject>
    <imageobject role="fo">
     <imagedata fileref="depl_barclamp_pacemaker.png" width="100%" format="png"/>
    </imageobject>
    <imageobject role="html">
     <imagedata fileref="depl_barclamp_pacemaker.png" width="75%" format="png"/>
    </imageobject>
   </mediaobject>
  </figure>

  <para>
   The Pacemaker component consists of the following roles. Deploying the
   <guimenu>hawk-server</guimenu> role is optional:
  </para>

  <variablelist>
   <varlistentry>
    <term><guimenu>pacemaker-cluster-member</guimenu>
    </term>
    <listitem>
     <para>
      Deploy this role on all nodes that should become member of the cluster.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><guimenu>hawk-server</guimenu>
    </term>
    <listitem>
     <para>
      Deploying this role is optional. If deployed, sets up the &hawk; Web
      interface which lets you monitor the status of the cluster. The Web
      interface can be accessed via
      <literal>https://<replaceable>IP-ADDRESS</replaceable>:7630</literal>.
      Note that the GUI on &cloud; can only be used to monitor the cluster
      status and not to change its configuration.
     </para>
     <para>
      <guimenu>hawk-server</guimenu> may be deployed on at least one cluster
      node. It is recommended to deploy it on all cluster nodes.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><guimenu>pacemaker-remote</guimenu>
    </term>
    <listitem>
     <para>
      Deploy this role on all nodes that should become members of the
      &compnode;s cluster. They will run as Pacemaker remote nodes that are
      controlled by the cluster, but do not affect quorum. Instead of the
      complete cluster stack, only the <literal>pacemaker-remote</literal>
      component will be installed on this nodes.
     </para>
    </listitem>
   </varlistentry>
  </variablelist>

  <figure>
   <title>The Pacemaker &Barcl;: Node Deployment Example</title>
   <mediaobject>
    <imageobject role="fo">
     <imagedata fileref="depl_barclamp_pacemaker_node_deployment.png" width="100%" format="png"/>
    </imageobject>
    <imageobject role="html">
     <imagedata fileref="depl_barclamp_pacemaker_node_deployment.png" width="100%" format="png"/>
    </imageobject>
   </mediaobject>
  </figure>

  <para>
   After a cluster has been successfully deployed, it is listed under
   <guimenu>Available Clusters</guimenu> in the <guimenu>Deployment</guimenu>
   section and can be used for role deployment like a regular node.
  </para>

  <warning>
   <title>Deploying Roles on Single Cluster Nodes</title>
   <para>
    When using clusters, roles from other &barcl;s must never be deployed to
    single nodes that are already part of a cluster. The only exceptions from
    this rule are the following roles:
   </para>
   <itemizedlist mark="bullet" spacing="normal">
    <listitem>
     <para>
      cinder-volume
     </para>
    </listitem>
    <listitem>
     <para>
      swift-proxy + swift-dispersion
     </para>
    </listitem>
    <listitem>
     <para>
      swift-ring-compute
     </para>
    </listitem>
    <listitem>
     <para>
      swift-storage
     </para>
    </listitem>
   </itemizedlist>
  </warning>

<!--taroth 2017-02-01: commenting as it is still not possible to update this screenshot-->

<!-- <figure>
   <title>Available Clusters in the Deployment Section</title>
   <mediaobject>
    <imageobject role="fo">
     <imagedata fileref="depl_barclamp_database_cluster_deployment.png" width="100%" format="png"/>
    </imageobject>
    <imageobject role="html">
     <imagedata fileref="depl_barclamp_database_cluster_deployment.png" width="75%" format="png"/>
    </imageobject>
   </mediaobject>
  </figure>-->

  <important>
   <title>Service Management on the Cluster</title>
   <para>
    After a role has been deployed on a cluster, its services are managed by
    the HA software. You must <emphasis>never</emphasis> manually start or stop
    an HA-managed service, nor configure it to start on boot. Services may only
    be started or stopped by using the cluster management tools Hawk or the crm
    shell. See
    <link xlink:href="&suse-onlinedoc;/sle-ha-12/book_sleha/data/sec_ha_config_basics_resources.html"/>
    for more information.
   </para>
  </important>

  <note>
   <title>Testing the Cluster Setup</title>
   <para>
    To check whether all cluster resources are running, either use the &hawk;
    Web interface or run the command <command>crm_mon</command>
    <option>-1r</option>. If it is not the case, clean up the respective
    resource with <command>crm</command> <option>resource</option>
    <option>cleanup</option> <replaceable>RESOURCE</replaceable> , so it gets
    respawned.
   </para>
   <para>
    Also make sure that &stonith; correctly works before continuing with the
    &cloud; setup. This is especially important when having chosen a &stonith;
    configuration requiring manual setup. To test if &stonith; works, log in to
    a node on the cluster and run the following command:
   </para>
<screen>pkill -9 corosync</screen>
   <para>
    In case &stonith; is correctly configured, the node will reboot.
   </para>
   <para>
    Before testing on a production cluster, plan a maintenance window in case
    issues should arise.
   </para>
  </note>
 </sect1>
 <sect1 xml:id="sec.depl.ostack.db">
  <title>Deploying the Database</title>

  <para>
   The very first service that needs to be deployed is the
   <guimenu>Database</guimenu>. The database component is using PostgreSQL and
   is used by all other components. It must be installed on a &contrnode;. The
   Database can be made highly available by deploying it on a cluster.
  </para>

  <remark condition="clarity">
   2014-03-28 - fs: How to set up shared storage or DRBD for the data?
  </remark>

  <para>
   The only attribute you may change is the maximum number of database
   connections (<guimenu>Global Connection Limit </guimenu>). The default value
   should usually work&mdash;only change it for large deployments in case the
   log files show database connection failures.
  </para>

  <figure>
   <title>The Database &Barcl;</title>
   <mediaobject>
    <imageobject role="fo">
     <imagedata fileref="depl_barclamp_database.png" width="100%" format="png"/>
    </imageobject>
    <imageobject role="html">
     <imagedata fileref="depl_barclamp_database.png" width="75%" format="png"/>
    </imageobject>
   </mediaobject>
  </figure>

  <sect2 xml:id="sec.depl.ostack.db.ha">
   <title>&haSetup; for the Database</title>
   <para>
    To make the database highly available, deploy it on a cluster instead of a
    single &contrnode;. This also requires shared storage for the cluster that
    hosts the database data. To achieve this, either set up a cluster with DRBD
    support (see <xref linkend="sec.depl.ostack.pacemaker"/>), or use
    <quote>traditional</quote> shared storage like an NFS share. We recommend
    using a dedicated cluster to deploy the database together with RabbitMQ,
    since both components require shared storage.
   </para>
   <para>
    Deploying the database on a cluster makes an additional <guimenu>High
    Availability</guimenu> section available in the
    <guimenu>Attributes</guimenu> section of the proposal. Configure the
    <guimenu>Storage Mode</guimenu> in this section. There are two options:
   </para>
   <variablelist>
    <varlistentry>
     <term><guimenu>DRBD</guimenu>
     </term>
     <listitem>
      <para>
       This option requires a two-node cluster that has been set up with DRBD.
       Also specify the <guimenu>Size to Allocate for DRBD Device (in
       Gigabytes)</guimenu>. The suggested value of 50 GB should be sufficient.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>Shared Storage</term>
     <listitem>
      <para>
       Use a shared block device or an NFS mount for shared storage.
       Concordantly with the mount command, you need to specify three
       attributes: <guimenu>Name of Block Device or NFS Mount
       Specification</guimenu> (the mount point), the <guimenu>Filesystem
       Type</guimenu>, and the <guimenu>Mount Options</guimenu>. Refer to
       <command>man 8 mount</command> for details on file system types and
       mount options.
      </para>
     </listitem>
    </varlistentry>
   </variablelist>
   <important>
    <title>NFS Export Options for Shared Storage</title>
    <para>
     To use an NFS share as shared storage for a cluster, export it on the NFS
     server with the following options:
    </para>
<screen>rw,async,insecure,no_subtree_check,no_root_squash</screen>
    <para>
     If mounting the NFS share on the cluster nodes fails, change the export
     options and re-apply the proposal. However, before doing so, you need to
     clean up the respective resources on the cluster nodes as described in
     <link xmlns:xlink="http://www.w3.org/1999/xlink"
      xlink:href="&suse-onlinedoc;/sle-ha-12/book_sleha/data/sec_ha_config_crm.html#sec_ha_manual_config_cleanup"/>.
    </para>
   </important>
   <important>
    <title>Ownership of a Shared NFS Directory</title>
    <para>
     The shared NFS directory that is used for the PostgreSQL database needs to
     be owned by the same user ID and group ID as of the
     <systemitem class="username">postgres</systemitem> user on the HA database
     cluster.
    </para>
    <para>
     To get the IDs, log in to one of the HA database cluster machines and
     issue the following commands:
    </para>
<screen>id -g postgres
getent group postgres | cut -d: -f3</screen>
    <para>
     The first command returns the numeric user ID, the second one the numeric
     group ID. Now log in to the NFS server and change the ownership of the
     shared NFS directory, for example:
    </para>
<screen>chown <replaceable>UID</replaceable>.<replaceable>GID</replaceable> /exports/cloud/db</screen>
    <para>
     Replace <replaceable>UID</replaceable> and <replaceable>GID</replaceable>
     by the respective numeric values retrieved above.
    </para>
   </important>
   <warning>
    <title>Re-Deploying &cloud; with Shared Storage</title>
    <para>
     When re-deploying &cloud; and reusing a shared storage hosting database
     files from a previous installation, the installation may fail due to the
     old database being used. Always delete the old database from the shared
     storage before re-deploying &cloud;.
    </para>
   </warning>
  </sect2>
 </sect1>
 <sect1 xml:id="sec.depl.ostack.rabbit">
  <title>Deploying RabbitMQ</title>

  <para>
   The RabbitMQ messaging system enables services to communicate with the other
   nodes via Advanced Message Queue Protocol (AMQP). Deploying it is mandatory.
   RabbitMQ needs to be installed on a &contrnode;. RabbitMQ can be made highly
   available by deploying it on a cluster. We recommend not changing the
   default values of the proposal's attributes.
  </para>

  <variablelist>
   <varlistentry>
    <term><guimenu>Virtual Host</guimenu>
    </term>
    <listitem>
     <para>
      Name of the default virtual host to be created and used by the RabbitMQ
      server (<literal>default_vhost</literal> configuration option in
      <filename>rabbitmq.config</filename>).
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>Port</term>
    <listitem>
     <para>
      Port the RabbitMQ server listens on (<literal>tcp_listeners</literal>
      configuration option in <filename>rabbitmq.config</filename>).
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>User</term>
    <listitem>
     <para>
      RabbitMQ default user (<literal>default_user</literal> configuration
      option in <filename>rabbitmq.config</filename>).
     </para>
    </listitem>
   </varlistentry>
  </variablelist>

  <figure>
   <title>The RabbitMQ &Barcl;</title>
   <mediaobject>
    <imageobject role="fo">
     <imagedata fileref="depl_barclamp_rabbitmq.png" width="100%" format="png"/>
    </imageobject>
    <imageobject role="html">
     <imagedata fileref="depl_barclamp_rabbitmq.png" width="75%" format="png"/>
    </imageobject>
   </mediaobject>
  </figure>

  <sect2 xml:id="sec.depl.ostack.rabbit.ha">
   <title>&haSetup; for RabbitMQ</title>
   <para>
    To make RabbitMQ highly available, deploy it on a cluster instead of a
    single &contrnode;. This also requires shared storage for the cluster that
    hosts the RabbitMQ data. To achieve this, either set up a cluster with DRBD
    support (see <xref linkend="sec.depl.ostack.pacemaker"/>) or use
    <quote>traditional</quote> shared storage like an NFS share. We recommend
    using a dedicated cluster to deploy RabbitMQ together with the database,
    since both components require shared storage.
   </para>
   <para>
    Deploying RabbitMQ on a cluster makes an additional <guimenu>High
    Availability</guimenu> section available in the
    <guimenu>Attributes</guimenu> section of the proposal. Configure the
    <guimenu>Storage Mode</guimenu> in this section. There are two options:
   </para>
   <variablelist>
    <varlistentry>
     <term><guimenu>DRBD</guimenu>
     </term>
     <listitem>
      <para>
       This option requires a two-node cluster that has been set up with DRBD.
       Also specify the <guimenu>Size to Allocate for DRBD Device (in
       Gigabytes)</guimenu>. The suggested value of 50 GB should be sufficient.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>Shared Storage</term>
     <listitem>
      <para>
       Use a shared block device or an NFS mount for shared storage.
       Concordantly with the mount command, you need to specify three
       attributes: <guimenu>Name of Block Device or NFS Mount
       Specification</guimenu> (the mount point), the <guimenu>Filesystem
       Type</guimenu>, and the <guimenu>Mount Options</guimenu>.
      </para>
     </listitem>
    </varlistentry>
   </variablelist>
   <important>
    <title>NFS Export Options for Shared Storage</title>
    <para>
     An NFS share for use as a shared storage for a cluster needs to be
     exported on the NFS server with the following options:
    </para>
<screen>rw,async,insecure,no_subtree_check,no_root_squash</screen>
    <para>
     If mounting the NFS share on the cluster nodes fails, change the export
     options and re-apply the proposal. Before doing so, however, you need to
     clean up the respective resources on the cluster nodes as described in
     <link xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="&suse-onlinedoc;/sle-ha-12/book_sleha/data/sec_ha_config_crm.html#sec_ha_manual_config_cleanup"/>.
    </para>
   </important>
  </sect2>

  <sect2 xml:id="sec.depl.ostack.rabbitmq.ssl">
   <title>SSL Configuration for RabbitMQ</title>
   <para>
    The RabbitMQ barclamp supports securing traffic via SSL. This is similar to
    the SSL support in other barclamps, but with these differences:
   </para>
   <itemizedlist>
    <listitem>
     <para>
      RabbitMQ can listen on two ports at the same time, typically port 5672
      for unsecured and port 5671 for secured traffic.
     </para>
    </listitem>
    <listitem>
     <para>
      The &o_meter; pipeline for &ostack; &o_objstore; cannot be passed
      SSL-related parameters. When SSL is enabled for RabbitMQ the &o_meter;
      pipeline in &o_objstore; is turned off, rather than sending it over an
      unsecured channel.
     </para>
    </listitem>
   </itemizedlist>
<para>
The following steps are the fastest way to set up and test a new SSL certificate authority (CA). 
</para>

<procedure xml:id="pro.rabbitmq-test">
<step>
<para>  
  In the RabbitMQ barclamp set <guimenu>Enable SSL</guimenu> to <guimenu>true</guimenu>, and <guimenu>Generate (self-signed) certificates (implies insecure)</guimenu>
    to <literal>true</literal>, then apply the barclamp. The barclamp will create a new CA, enter the correct settings in <filename>/etc/rabbitmq/rabbitmq.config</filename>, and start RabbitMQ.
</para>
</step>
<step>
<para>
Test your new CA with OpenSSL, substituting the hostname of your control node:
<screen>
openssl s_client -connect d52-54-00-59-e5-fd:5671 
[...]
Verify return code: 18 (self signed certificate)
</screen>
  This outputs a lot of information, including a copy of the server's public certificate, protocols, ciphers, and the chain of trust.
</para>
</step>
    <step>
     <para>
      The last step is to configure client services to use SSL to access the
      RabbitMQ service. (See
      <link xlink:href="https://docs.openstack.org/developer/oslo.messaging/newton/opts.html#oslo-messaging-rabbit"/> for a complete reference).
     </para>
    </step>
</procedure>

<para>
    It is preferable to set up your own CA. The best practice is to use a commercial certificate authority. You may also deploy your own self-signed certificates, provided that your cloud is not publicly-accessible, and only for your internal use. Follow these steps to enable your own CA in RabbitMQ and deploy it to &cloud;:
   </para>
   <procedure xml:id="pro.rabbitmq-production">
    <step>
     <para>
      Configure the RabbitMQ barclamp to use the control node's
      certificate authority (CA), if it already has one, or create a CA specifically for RabbitMQ and configure the barclamp to use that. (See <xref linkend="sec.depl.req.ssl"/>, and the RabbitMQ manual has a detailed howto on creating your CA at <link xlink:href="http://www.rabbitmq.com/ssl.html"/>, with customizations for .NET and Java clients.) 
    </para>
     <figure>
      <title>SSL Settings for RabbitMQ Barclamp</title>
      <mediaobject>
       <imageobject role="fo">
        <imagedata fileref="rabbitmq-ssl-1.png" width="100%" format="PNG"/>
       </imageobject>
       <imageobject role="html">
        <imagedata fileref="rabbitmq-ssl-1.png" width="100%" format="PNG"/>
       </imageobject>
       <textobject role="description"><phrase>Example RabbitMQ SSL barclamp configuration</phrase>
       </textobject>
      </mediaobject>
     </figure>
    </step>

   </procedure>
   
   <para>
    The configuration options in the RabbitMQ barclamp allow tailoring the barclamp to your SSL setup.
</para>
  <variablelist>
   <varlistentry>
    <term><guimenu>Enable SSL</guimenu>
    </term>
     <listitem>
      <para>
          Set this to <guimenu>True</guimenu> to expose all of your configuration options.
      </para>
     </listitem>
    </varlistentry>
   <varlistentry>
    <term><guimenu>SSL Port</guimenu>
    </term>
     <listitem>
      <para>
      RabbitMQ's SSL listening port. The default is 5671.
      </para>
     </listitem>
    </varlistentry>
   <varlistentry>
    <term><guimenu>Generate (self-signed) certificates (implies insecure)</guimenu>
    </term>
     <listitem>
      <para>
    When this is set to <literal>true</literal>, self-signed certificates are automatically generated and copied to the correct locations on the control node, and all other barclamp options are set automatically. This is the fastest way to apply and test the barclamp. Do not use this on production systems. When this is set to <literal>false</literal> the remaining options are exposed.
      </para>
     </listitem>
    </varlistentry>    
   <varlistentry>
    <term><guimenu>SSL Certificate File</guimenu>
    </term>
     <listitem>
      <para>
     The location of your public root CA certificate.
      </para>
     </listitem>
    </varlistentry> 
   <varlistentry>
    <term><guimenu>SSL (Private) Key File</guimenu>
    </term>
     <listitem>
      <para>
     The location of your private server key.
      </para>
     </listitem>
    </varlistentry>    
   <varlistentry>
    <term><guimenu>Require Client Certificate</guimenu>
    </term>
     <listitem>
      <para>
          This goes with <guimenu>SSL CA Certificates File</guimenu>. Set to <guimenu>true</guimenu> to require clients to present SSL certificates to RabbitMQ.
      </para>
     </listitem>
    </varlistentry>    
   <varlistentry>
    <term><guimenu>SSL CA Certificates File</guimenu>
    </term>
     <listitem>
      <para>
     Trust client certificates presented by the clients that are signed by other CAs. You'll need to store copies of the CA certificates; see "Trust the Client's Root CA" at <link xlink:href="http://www.rabbitmq.com/ssl.html"/>.
      </para>
     </listitem>
    </varlistentry>    
   <varlistentry>
    <term><guimenu>SSL Certificate is insecure (for instance, self-signed)</guimenu>
    </term>
     <listitem>
      <para>
      When this is set to <guimenu>false</guimenu>, clients validate the RabbitMQ server certificate with the <guimenu>SSL client CA</guimenu> file. 
      </para>
     </listitem>
    </varlistentry>    
   <varlistentry>  
    <term><guimenu>SSL client CA file (used to validate rabbitmq server certificate)</guimenu>
    </term>
     <listitem>
      <para>
        Tells clients of RabbitMQ where to find the CA bundle that validates the certificate presented by the RabbitMQ server, when <guimenu>SSL Certificate is insecure (for instance, self-signed)</guimenu> is set to <guimenu>false</guimenu>. 
      </para>
     </listitem>
    </varlistentry>    
   </variablelist>
  </sect2>
 </sect1>
 <sect1 xml:id="sec.depl.ostack.keystone">
  <title>Deploying &o_ident;</title>

  <para>
   <guimenu>Keystone</guimenu> is another core component that is used by all
   other &ostack; components. It provides authentication and authorization
   services. <guimenu>Keystone</guimenu> needs to be installed on a
   &contrnode;. &o_ident; can be made highly available by deploying it on a
   cluster. You can configure the following parameters of this &barcl;:
  </para>

  <variablelist>
   <varlistentry>
    <term><guimenu>Algorithm for Token Generation</guimenu>
    </term>
    <listitem>
     <para>
      Set the algorithm used by &o_ident; to generate the tokens. You can
      choose between <literal>Fernet</literal> (the default) or
      <literal>UUID</literal>. Note that for performance and security reasons
      it is strongly recommended to use <literal>Fernet</literal>.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><guimenu>Region Name</guimenu>
    </term>
    <listitem>
     <para>
      Allows customizing the region name that crowbar is going to manage.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><guimenu>Default Credentials: Default Tenant</guimenu>
    </term>
    <listitem>
     <para>
      Tenant for the users. Do not change the default value of
      <literal>openstack</literal>.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><guimenu>
      Default Credentials: Administrator User Name/Password
     </guimenu>
    </term>
    <listitem>
     <para>
      User name and password for the administrator.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><guimenu>
      Default Credentials: Create Regular User
     </guimenu>
    </term>
    <listitem>
     <para>
      Specify whether a regular user should be created automatically. Not
      recommended in most scenarios, especially in an LDAP environment.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><guimenu>
      Default Credentials: Regular User Username/Password
     </guimenu>
    </term>
    <listitem>
     <para>
      User name and password for the regular user. Both the regular user and
      the administrator accounts can be used to log in to the &cloud; &dash;.
      However, only the administrator can manage &o_ident; users and access.
     </para>
     <figure>
      <title>The &o_ident; &Barcl;</title>
      <mediaobject>
       <imageobject role="fo">
        <imagedata fileref="depl_barclamp_keystone.png" width="100%" format="png"/>
       </imageobject>
       <imageobject role="html">
        <imagedata fileref="depl_barclamp_keystone.png" width="75%" format="png"/>
       </imageobject>
      </mediaobject>
     </figure>
    </listitem>
   </varlistentry>
   <varlistentry xml:id="sec.depl.ostack.keystone.ssl">
    <term>SSL Support: Protocol
    </term>
    <listitem>
     <para>
      When you use the default value <guimenu>HTTP</guimenu>, public
      communication will not be encrypted. Choose <guimenu>HTTPS</guimenu> to
      use SSL for encryption. See <xref linkend="sec.depl.req.ssl"/> for
      background information and <xref linkend="sec.depl.inst.nodes.post.ssl"/>
      for installation instructions. The following additional configuration
      options will become available when choosing <guimenu>HTTPS</guimenu>:
     </para>
     <variablelist>
      <varlistentry>
       <term><guimenu>Generate (self-signed) certificates</guimenu>
       </term>
       <listitem>
        <para>
         When set to <literal>true</literal>, self-signed certificates are
         automatically generated and copied to the correct locations. This
         setting is for testing purposes only and should never be used in
         production environments!
        </para>
       </listitem>
      </varlistentry>
      <varlistentry>
       <term><guimenu>SSL Certificate File</guimenu> / <guimenu>SSL (Private) Key
        File</guimenu>
       </term>
       <listitem>
        <para>
         Location of the certificate key pair files.
        </para>
       </listitem>
      </varlistentry>
      <varlistentry>
       <term><guimenu>SSL Certificate is insecure</guimenu>
       </term>
       <listitem>
        <para>
         Set this option to <literal>true</literal> when using self-signed
         certificates to disable certificate checks. This setting is for
         testing purposes only and should never be used in production
         environments!
        </para>
       </listitem>
      </varlistentry>
      <varlistentry>
       <term><guimenu>SSL CA Certificates File</guimenu>
       </term>
       <listitem>
        <para>
         Specify the absolute path to the CA certificate. This field is
         mandatory, and leaving it blank will cause the &barcl; to fail. To fix
         this issue, you have to provide the absolute path to the CA
         certificate, restart the <systemitem>apache2</systemitem> service, and
         re-deploy the &barcl;.
        </para>
        <para>
         When the certificate is not already trusted by the pre-installed list
         of trusted root certificate authorities, you need to provide a
         certificate bundle that includes the root and all intermediate CAs.
        </para>
        <figure>
         <title>The SSL Dialog</title>
         <mediaobject>
          <imageobject role="fo">
           <imagedata fileref="depl_barclamp_ssl.png" width="100%" format="png"/>
          </imageobject>
          <imageobject role="html">
           <imagedata fileref="depl_barclamp_ssl.png" width="75%" format="png"/>
          </imageobject>
         </mediaobject>
        </figure>
       </listitem>
      </varlistentry>
     </variablelist>
    </listitem>
   </varlistentry>
  </variablelist>

<!-- fs 2017-05-04: Commenting per bsc#1037531
       * no longer valid
       * needs to be rewritten for SOC8, see
         https://docs.openstack.org/admin-guide/identity-integrate-with-ldap.html

  <sect2 xml:id="sec.depl.ostack.keystone.ldap">
   <title>LDAP Authentication with &o_ident;</title>
   <para>
    <remark condition="clarity">
     2015-01-21 - fs: TODO: bsc #914070
    </remark>
    By default &o_ident; uses an SQL database back-end store for
    authentication. LDAP can be used in addition to the default, or as an
    alternative. Using LDAP requires the &contrnode; on which
    &o_ident; is installed to be able to contact the LDAP server. See
    <xref linkend="sec.depl.inst.admserv.post.network"/> for instructions on how to
    adjust the network setup.
   </para>
   <sect3 xml:id="sec.depl.ostack.keystone.ldap.ldap">
    <title>Using LDAP for Authentication</title>
    <para>
     To configure LDAP as an alternative to the SQL database back-end store,
     you need to open the &o_ident; &barcl; <guimenu>Attribute
     </guimenu>configuration in <guimenu>Raw</guimenu> mode. Search for the
     <guimenu>ldap</guimenu> section.
    </para>
    <figure xml:id="fig.keystone.ldap">
     <title>The &o_ident; &Barcl;: Raw Mode</title>
     <mediaobject>
      <imageobject role="fo">
       <imagedata fileref="depl_barclamp_keystone_raw.png" width="100%" format="png"/>
      </imageobject>
      <imageobject role="html">
       <imagedata fileref="depl_barclamp_keystone_raw.png" width="75%" format="png"/>
      </imageobject>
     </mediaobject>
    </figure>
    <para>
     Adjust the settings according to your LDAP setup. The default
     configuration does not include all attributes that can be
     set. You'll find a complete list of options is available in the file
     <filename>/opt/dell/chef/data_bags/crowbar/bc-template-keystone.schema</filename>
     on the &admserv; (search for <literal>ldap</literal>). There are
     three types of attribute values: strings (for example, the value for
     <literal>url</literal>:<literal>"ldap://localhost"</literal>), bool
     (for example, the value for <literal>use_dumb_member</literal>:
     <literal>false</literal>) and integer (for example, the value for
     <literal>page_size</literal>: <literal>0</literal>). Attribute names
     and string values always need to be quoted with double quotes; bool and
     integer values must not be quoted.
    </para>
    <important>
     <title>Using LDAP over SSL (ldaps) Is Recommended</title>
     <para>
      In a production environment you should use LDAP over SSL
      (ldaps), otherwise passwords will be transferred as plain text.
     </para>
    </important>
   </sect3>
   <sect3 xml:id="sec.depl.ostack.keystone.ldap.hybrid">
    <title>Using Hybrid Authentication</title>
    <para>
     The Hybrid LDAP back-end allows creating a mixed LDAP/SQL setup. This
     is especially useful when an existing LDAP server should be used to
     authenticate cloud users. The system and service users (administrators
     and operators) needed to set up and manage &cloud; will be managed
     in the local SQL database. Assignments of users to projects and roles
     will also be stored in the local database.
    </para>
    <para>
     In this scenario the LDAP Server can be read-only for &cloud;
     installation, and no Schema modifications are required. Therefore
     managing LDAP users from within &cloud; is not possible and must be done using your established tools for LDAP user management. All user
     that are create with the &o_ident; command line client or the
     Horizon Web UI will be stored in the local SQL database.
    </para>
    <para>
     To configure hybrid authentication, proceed as follows:
    </para>
    <procedure>
     <step>
      <para>
       Open the &o_ident; &barcl; <guimenu>Attribute
       </guimenu>configuration in <guimenu>Raw</guimenu> mode (see
       <xref linkend="fig.keystone.ldap"/>).
      </para>
     </step>
     <step>
      <para>
       Set the identity and assignment drivers to the hybrid back-end:
      </para>
<screen> "identity": {
    "driver": "hybrid"
  },
  "assignment": {
    "driver": "hybrid"
  }</screen>
     </step>
     <step>
      <para>
       Adjust the settings according to your LDAP setup in the
       <guimenu>ldap</guimenu> section. Since the LDAP back-end is only used
       to acquire information on users (but not on projects and roles),
       only the user-related settings matter here. See the following
       example of settings that may need to be adjusted:
      </para>
<screen>  "ldap": {
    "url": "ldap://localhost",
    "user": "",
    "password": "",
    "suffix": "cn=example,cn=com",
    "user_tree_dn": "cn=example,cn=com",
    "query_scope": "one",
    "user_id_attribute": "cn",
    "user_enabled_emulation_dn": "",
    "tls_req_cert": "demand",
    "user_attribute_ignore": "tenant_id,tenants",
    "user_objectclass": "inetOrgPerson",
    "user_mail_attribute": "mail",
    "user_filter": "",
    "use_tls": false,
    "user_allow_create": false,
    "user_pass_attribute": "userPassword",
    "user_enabled_attribute": "enabled",
    "user_enabled_default": "True",
    "page_size": 0,
    "tls_cacertdir": "",
    "tls_cacertfile": "",
    "user_enabled_mask": 0,
    "user_allow_update": true,
    "group_allow_update": true,
    "user_enabled_emulation": false,
    "user_name_attribute": "cn"
    "group_ad_nesting": false,
    "use_pool": true,
    "pool_size": 10,
    "pool_retry_max": 3
  }</screen>
      <para>
       To access the LDAP server anonymously, leave the values for
       <guimenu>user</guimenu> and <guimenu>password</guimenu> empty.
      </para>
     </step>
    </procedure>
   </sect3>
  </sect2>
-->

  <sect2 xml:id="sec.depl.ostack.keystone.ha">
   <title>&haSetup; for &o_ident;</title>
   <para>
    Making &o_ident; highly available requires no special
    configuration&mdash;it is sufficient to deploy it on a cluster.
   </para>
  </sect2>
 </sect1>
 <sect1 xml:id="sec.depl.ostack.ceph">
  <title>Deploying &ceph; (optional)</title>

  <para>
   &ceph; (<link xlink:href="http://ceph.com/"/>) adds a redundant block
   storage service to &cloud; for storing persistent devices that can be
   mounted from &vmguest;s. It offers high data security by storing the data
   redundantly on a pool of &stornode;s. Therefore &ceph; needs to be installed
   on at least three dedicated nodes. All &ceph; nodes need to run &slsa; 12.
   For detailed information on how to provide the required repositories, refer
   to <xref
    linkend="sec.depl.adm_conf.repos.scc"/>. If you are deploying
   the optional Calamari server for &ceph; management and monitoring, an
   additional node is required.
  </para>

  <tip>
   <title>&storage;</title>
   <para>
    &storage; is a robust cluster solution based on &ceph;. Refer to
    <link
     xlink:href="https://www.suse.com/documentation/ses-4/"/> for
    more information.
   </para>
  </tip>

  <important>
   <title>Changing &ceph; Client Network</title>
   <para>
    With &ceph; clusters, we recommend to have a separate public and cluster
    networks. &ceph;'s <emphasis>cluster</emphasis> network is where internal
    cluster processes occur, for example data replication. &ceph;'s
    <emphasis>public</emphasis> network is the network on which all client
    traffic occurs.
   </para>
   <para>
    Since &cloud; 7, the &ceph; client network defaults to &crow;'s 'public'
    network, while the cluster network remains on &crow;'s 'storage' network.
    If you need to change the default client network name, open the &ceph;
    &barcl; configuration in a <guimenu>Raw</guimenu> mode before first
    applying the &ceph; proposal, and change the
    <option>client_network</option> attribute to match the name of your
    preferred network.
   </para>
  </important>

  <para>
   The &ceph; &barcl; has the following configuration options:
  </para>

  <variablelist>
   <varlistentry>
    <term><guimenu>Disk Selection Method</guimenu>
    </term>
    <listitem>
     <para>
      Choose whether to only use the first available disk or all available
      disks. <quote>Available disks</quote> are all disks currently not used by
      the system. Note that one disk (usually <filename>/dev/sda</filename>) of
      every block storage node is already used for the operating system and is
      not available for &ceph;.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><guimenu>Number of Replicas of an Object</guimenu>
    </term>
    <listitem>
     <para>
      For data security, stored objects are not only stored once, but
      redundantly. Specify the number of copies that should be stored for each
      object with this setting. The number includes the object itself. For
      example, if you want the object plus two copies, specify 3. However, note
      that the number cannot be higher than the number of available hard disks.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><guimenu>SSL Support for RadosGW</guimenu>
    </term>
    <listitem>
     <para>
      Choose whether to encrypt public communication (<guimenu>HTTPS</guimenu>)
      or not (<guimenu>HTTP</guimenu>). If you choose <guimenu>HTTPS</guimenu>,
      you need to specify the locations for the certificate key pair files.
      Note that both trusted and self-signed certificates are accepted.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><guimenu>Calamari Credentials</guimenu>
    </term>
    <listitem>
     <para>
      Calamari is a Web front-end for managing and analyzing the &ceph;
      cluster. Provide administrator credentials (user name, password, e-mail
      address) in this section. When &ceph; has been deployed you can log in to
      Calamari with these credentials. Deploying Calamari is
      optional&mdash;leave these text boxes empty when not deploying Calamari.
     </para>
    </listitem>
   </varlistentry>
  </variablelist>

  <figure>
   <title>The &ceph; &Barcl;</title>
   <mediaobject>
    <imageobject role="fo">
     <imagedata fileref="depl_barclamp_ceph.png" width="100%" format="png"/>
    </imageobject>
    <imageobject role="html">
     <imagedata fileref="depl_barclamp_ceph.png" width="75%" format="png"/>
    </imageobject>
   </mediaobject>
  </figure>

  <para>
   The &ceph; component consists of the following different roles:
  </para>

  <important>
   <title>Dedicated Nodes</title>
   <para>
    We do not recommend sharing one node by multiple &ceph; components at the
    same time. For example, running a <literal>ceph-mon</literal> service on
    the same node as <literal>ceph-osd</literal> degrades the performance of
    all services hosted on the shared node. This also applies to other
    services, such as Calamari or RADOS Gateway.
   </para>
  </important>

  <variablelist>
   <varlistentry>
    <term><guimenu>ceph-osd</guimenu>
    </term>
    <listitem>
     <para>
      The virtual block storage service. Install this role on all dedicated
      &ceph; &stornode;s (at least three).
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><guimenu>ceph-mon</guimenu>
    </term>
    <listitem>
     <para>
      Cluster monitor daemon for managing the storage map of the &ceph;
      cluster. <guimenu>ceph-mon</guimenu> needs to be installed on at least
      three nodes.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><guimenu>ceph-calamari</guimenu>
    </term>
    <listitem>
     <para>
      Sets up the Calamari Web interface for managing the &ceph; cluster.
      Deploying it is optional. The Web interface can be accessed via
      http://<replaceable>IP-ADDRESS</replaceable>/, where
      <replaceable>IP-ADDRESS</replaceable> is the address of the machine where
      <guimenu>ceph-calamari</guimenu> is deployed.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><guimenu>ceph-radosgw</guimenu>
    </term>
    <listitem>
     <para>
      The HTTP REST gateway for &ceph;. Visit
      <link
       xlink:href="https://www.suse.com/documentation/ses-4/book_storage_admin/data/cha_ceph_gw.html"/>
      for more detailed information.
     </para>
     <tip>
      <title>&rgw; HA Setup</title>
      <para>
       If you need to set up more &rgw;s (and thus create a backup instance in
       case one &rgw; node fails), set up &rgw; on multiple nodes and put an
       HTTP load balancer in front of them. You can choose your preferred
       balancing solution, or use &sle; HA extension (refer to
       <link
        xlink:href="https://www.suse.com/documentation/sle-ha-12/"/>).
      </para>
     </tip>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><guimenu>ceph-mds</guimenu>
    </term>
    <listitem>
     <para>
      The metadata server for the &ceph;FS distributed file system. Install
      this role on one to three nodes to enable &ceph;FS. A file system named
      <literal>cephfs</literal> will automatically be created, along with
      <literal>cephfs_metadata</literal> and <literal>cephfs_data</literal>
      pools. Refer to
      <link
       xlink:href="https://www.suse.com/documentation/ses-4/book_storage_admin/data/cha_ceph_cephfs.html"/>
      for more details.
     </para>
    </listitem>
   </varlistentry>
  </variablelist>

  <important>
   <title>Use Dedicated Nodes</title>
   <para>
    Never deploy &ceph; on a node that runs non-&ceph; &ostack; components. The
    only services that may be deployed together on a &ceph; node are
    <guimenu>ceph-osd</guimenu>, <guimenu>ceph-mon</guimenu>, and
    <guimenu>ceph-radosgw</guimenu>. However, we recommend running each &ceph;
    service on a dedicated host for performance reasons. All &ceph; nodes need
    to run &slsa; 12.
   </para>
  </important>

  <figure>
   <title>The &ceph; &Barcl;: Node Deployment Example</title>
   <mediaobject>
    <imageobject role="fo">
     <imagedata fileref="depl_barclamp_ceph_node_deployment.png" width="100%" format="png"/>
    </imageobject>
    <imageobject role="html">
     <imagedata fileref="depl_barclamp_ceph_node_deployment.png" width="75%" format="png"/>
    </imageobject>
   </mediaobject>
  </figure>

  <sect2 xml:id="sec.depl.ostack.ceph.ha">
   <title>&haSetup; for &ceph;</title>
   <para>
    &ceph; is HA-enabled by design, so there is no need for a special
    &hasetup;.
   </para>
  </sect2>
 </sect1>
 <sect1 xml:id="sec.depl.ostack.swift">
  <title>Deploying &o_objstore; (optional)</title>

  <para>
   &o_objstore; adds an object storage service to &cloud; for storing single
   files such as images or snapshots. It offers high data security by storing
   the data redundantly on a pool of &stornode;s&mdash;therefore &o_objstore;
   needs to be installed on at least two dedicated nodes.
  </para>

<!--
  <para>
   It is recommended not to change the defaults in the &barcl; proposal,
   unless you know exactly what you require. However you should change the
   <guimenu>Cluster Admin Password</guimenu>. If you plan to change the
   <guimenu>Zone</guimenu> value, it is important to know that you need at
   least as many &stornode;s as <guimenu>Zones</guimenu>.
  </para>
-->

  <para>
   To properly configure &o_objstore; it is important to understand how it
   places the data. Data is always stored redundantly within the hierarchy. The
   &o_objstore; hierarchy in &cloud; is formed out of zones, nodes, hard disks,
   and logical partitions. Zones are physically separated clusters, for example
   different server rooms each with its own power supply and network segment. A
   failure of one zone must not affect another zone. The next level in the
   hierarchy are the individual &o_objstore; storage nodes (on which
   <guimenu>swift-storage</guimenu> has been deployed), followed by the hard
   disks. Logical partitions come last.
  </para>

  <para>
   &o_objstore; automatically places three copies of each object on the highest
   hierarchy level possible. If three zones are available, then each copy of
   the object will be placed in a different zone. In a one zone setup with more
   than two nodes, the object copies will each be stored on a different node.
   In a one zone setup with two nodes, the copies will be distributed on
   different hard disks. If no other hierarchy element fits, logical partitions
   are used.
  </para>

  <para>
   The following attributes can be set to configure &o_objstore;:
  </para>

  <variablelist>
   <varlistentry>
    <term><guimenu>Allow Public Containers</guimenu>
    </term>
    <listitem>
     <para>
      Set to <literal>true</literal> to enable public access to containers.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><guimenu>Enable Object Versioning</guimenu>
    </term>
    <listitem>
     <para>
      If set to true, a copy of the current version is archived each time an
      object is updated.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><guimenu>Zones</guimenu>
    </term>
    <listitem>
     <para>
      Number of zones (see above). If you do not have different independent
      installations of storage nodes, set the number of zones to
      <literal>1</literal>.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><guimenu>Create 2^X Logical Partitions</guimenu>
    </term>
    <listitem>
     <para>
      Partition power. The number entered here is used to compute the number of
      logical partitions to be created in the cluster. The number you enter is
      used as a power of 2 (2^X).
     </para>
     <para>
      We recommend using a minimum of 100 partitions per disk. To measure the
      partition power for your setup, multiply the number of disks from all
      &o_objstore; nodes by 100, and then round up to the nearest power of two.
      Keep in mind that the first disk of each node is not used by
      &o_objstore;, but rather for the operating system.
     </para>
     <formalpara>
      <title>Example: 10 &o_objstore; nodes with 5 hard disks each</title>
      <para>
       Four hard disks on each node are used for &o_objstore;, so there is a
       total of forty disks. 40 x 100 = 4000. The nearest power of two, 4096,
       equals 2^12. So the partition power that needs to be entered is
       <literal>12</literal>.
      </para>
     </formalpara>
     <important>
      <title>Value Cannot be Changed After the Proposal Has Been Deployed</title>
      <para>
       Changing the number of logical partition after &o_objstore; has been
       deployed is not supported. Therefore the value for the partition power
       should be calculated from the maximum number of partitions this cloud
       installation is likely going to need at any point in time.
      </para>
     </important>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><guimenu>Minimum Hours before Partition is reassigned</guimenu>
    </term>
    <listitem>
     <para>
      This option sets the number of hours before a logical partition is
      considered for relocation. <literal>24</literal> is the recommended
      value.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><guimenu>Replicas</guimenu>
    </term>
    <listitem>
     <para>
      The number of copies generated for each object. The number of replicas
      depends on the number of disks and zones.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><guimenu>Replication interval (in seconds)</guimenu>
    </term>
    <listitem>
     <para>
      Time (in seconds) after which to start a new replication process.
     </para>
    </listitem>
   </varlistentry>
<!-- fs 2016-01-26: No longer present in Cloud6
   <varlistentry>
    <term>Cluster Admin Password</term>
    <listitem>
     <para>
      The &o_objstore; administrator password.
     </para>
    </listitem>
   </varlistentry>
   -->
   <varlistentry>
    <term><guimenu>Debug</guimenu>
    </term>
    <listitem>
     <para>
      Shows debugging output in the log files when set to
      <literal>true</literal>.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><guimenu>SSL Support: Protocol</guimenu>
    </term>
    <listitem>
     <para>
      Choose whether to encrypt public communication (<guimenu>HTTPS</guimenu>)
      or not (<guimenu>HTTP</guimenu>). If you choose <guimenu>HTTPS</guimenu>,
      you have two options. You can either <guimenu>Generate (self-signed)
      certificates</guimenu> or provide the locations for the certificate key
      pair files. Using self-signed certificates is for testing purposes only
      and should never be used in production environments!
     </para>
    </listitem>
   </varlistentry>
  </variablelist>

  <figure>
   <title>The &o_objstore; &Barcl;</title>
   <mediaobject>
    <imageobject role="fo">
     <imagedata fileref="depl_barclamp_swift.png" width="100%" format="png"/>
    </imageobject>
    <imageobject role="html">
     <imagedata fileref="depl_barclamp_swift.png" width="75%" format="png"/>
    </imageobject>
   </mediaobject>
  </figure>

  <para>
   Apart from the general configuration described above, the &o_objstore;
   &barcl; lets you also activate and configure <guimenu>Additional
   Middlewares</guimenu>. The features these middlewares provide can be used
   via the &o_objstore; command line client only. The Ratelimit and S3
   middleware provide for the most interesting features, and we recommend
   enabling other middleware only for specific use-cases.
  </para>

  <variablelist>
   <varlistentry>
    <term><guimenu>S3 Middleware</guimenu>
    </term>
    <listitem>
     <para>
      Provides an S3 compatible API on top of &o_objstore;.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><guimenu>StaticWeb</guimenu>
    </term>
    <listitem>
     <para>
      Serve container data as a static Web site with an index file and optional
      file listings. See
      <link xlink:href="http://docs.openstack.org/developer/swift/middleware.html#staticweb"/>
      for details.
     </para>
     <para>
      This middleware requires setting <guimenu>Allow Public
      Containers</guimenu> to <literal>true</literal>.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><guimenu>TempURL</guimenu>
    </term>
    <listitem>
     <para>
      Create URLs to provide time-limited access to objects. See
      <link xlink:href="http://docs.openstack.org/developer/swift/middleware.html#tempurl"/>
      for details.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><guimenu>FormPOST</guimenu>
    </term>
    <listitem>
     <para>
      Upload files to a container via Web form. See
      <link xlink:href="http://docs.openstack.org/developer/swift/middleware.html#formpost"/>
      for details.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><guimenu>Bulk</guimenu>
    </term>
    <listitem>
     <para>
      Extract TAR archives into a Swift account, and delete multiple objects or
      containers with a single request. See
      <link xlink:href="http://docs.openstack.org/developer/swift/middleware.html#module-swift.common.middleware.bulk"/>
      for details.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><guimenu>Cross-domain</guimenu>
    </term>
    <listitem>
     <para>
      Interact with the Swift API via Flash, Java, and Silverlight from an
      external network. See
      <link xlink:href="http://docs.openstack.org/developer/swift/middleware.html#module-swift.common.middleware.crossdomain"/>
      for details.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><guimenu>Domain Remap</guimenu>
    </term>
    <listitem>
     <para>
      Translates container and account parts of a domain to path parameters
      that the &o_objstore; proxy server understands. Can be used to create
      short URLs that are easy to remember, for example by rewriting
      <literal>home.&exampleuser_plain;.&exampledomain;/$ROOT/&exampleuser_plain;/home/myfile</literal>
      to <literal>home.&exampleuser_plain;.&exampledomain;/myfile</literal>.
      See
      <link xlink:href="http://docs.openstack.org/developer/swift/middleware.html#module-swift.common.middleware.domain_remap"/>
      for details.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><guimenu>Ratelimit</guimenu>
    </term>
    <listitem>
     <para>
      Throttle resources such as requests per minute to provide denial of
      service protection. See
      <link xlink:href="http://docs.openstack.org/developer/swift/middleware.html#module-swift.common.middleware.ratelimit"/>
      for details.
     </para>
    </listitem>
   </varlistentry>
  </variablelist>

  <para>
   The &o_objstore; component consists of four different roles. Deploying
   <guimenu>swift-dispersion</guimenu> is optional:
  </para>

  <variablelist>
   <varlistentry>
    <term><guimenu>swift-storage</guimenu>
    </term>
    <listitem>
     <para>
      The virtual object storage service. Install this role on all dedicated
      &o_objstore; &stornode;s (at least two), but not on any other node.
     </para>
     <warning>
      <title>swift-storage Needs Dedicated Machines</title>
      <para>
       Never install the swift-storage service on a node that runs other
       &ostack; components.
      </para>
     </warning>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><guimenu>swift-ring-compute</guimenu>
    </term>
    <listitem>
     <para>
      The ring maintains the information about the location of objects,
      replicas, and devices. It can be compared to an index that is used by
      various &ostack; components to look up the physical location of objects.
      <guimenu>swift-ring-compute</guimenu> must only be installed on a single
      node, preferably a &contrnode;.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><guimenu>swift-proxy</guimenu>
    </term>
    <listitem>
     <para>
      The &o_objstore; proxy server takes care of routing requests to
      &o_objstore;. Installing a single instance of
      <guimenu>swift-proxy</guimenu> on a &contrnode; is recommended. The
      <guimenu>swift-proxy</guimenu> role can be made highly available by
      deploying it on a cluster.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><guimenu>swift-dispersion</guimenu>
    </term>
    <listitem>
     <para>
      Deploying <guimenu>swift-dispersion</guimenu> is optional. The
      &o_objstore; dispersion tools can be used to test the health of the
      cluster. It creates a heap of dummy objects (using 1% of the total space
      available). The state of these objects can be queried using the
      swift-dispersion-report query. <guimenu>swift-dispersion</guimenu> needs
      to be installed on a &contrnode;.
     </para>
    </listitem>
   </varlistentry>
  </variablelist>

  <figure>
   <title>The &o_objstore; &Barcl;: Node Deployment Example</title>
   <mediaobject>
    <imageobject role="fo">
     <imagedata fileref="depl_barclamp_swift_node_deployment.png" width="100%" format="png"/>
    </imageobject>
    <imageobject role="html">
     <imagedata fileref="depl_barclamp_swift_node_deployment.png" width="75%" format="png"/>
    </imageobject>
   </mediaobject>
  </figure>

  <sect2 xml:id="sec.depl.ostack.swift.ha">
   <title>&haSetup; for &swift;</title>
   <para>
    &swift; replicates by design, so there is no need for a special &hasetup;.
    Make sure to fulfill the requirements listed in
    <xref linkend="sec.depl.reg.ha.storage.swift"/>.
   </para>
  </sect2>
 </sect1>
 <sect1 xml:id="sec.depl.ostack.glance">
  <title>Deploying &o_img;</title>

  <para>
   &o_img; provides discovery, registration, and delivery services for virtual
   disk images. An image is needed to start an &vmguest;&mdash;it is its
   pre-installed root-partition. All images you want to use in your cloud to
   boot &vmguest;s from, are provided by &o_img;. &o_img; must be deployed onto
   a &contrnode;. &o_img; can be made highly available by deploying it on a
   cluster.
  </para>

  <para>
   There are a lot of options to configure &o_img;. The most important ones are
   explained below&mdash;for a complete reference refer to
   <link xlink:href="http://github.com/crowbar/crowbar/wiki/Glance--barclamp"/>.
  </para>

  <important xml:id="note.glance.api.versions">
   <title>&o_img; API Versions</title>
   <para>
    As of &productname; 7, the &o_img; API v1 is no longer enabled by default.
    Instead, &o_img; API v2 is used by default.
   </para>
   <para>
    If you need to re-enable API v1 for compatibility reasons:
   </para>
   <orderedlist>
    <listitem>
     <para>
      Switch to the <guimenu>Raw</guimenu> view of the &o_img; &barcl;.
     </para>
    </listitem>
    <listitem>
     <para>
      Search for the <literal>enable_v1</literal> entry and set it to
      <literal>true</literal>:
     </para>
<screen>"enable_v1": true</screen>
     <para>
      In new installations, this entry is set to <literal>false</literal> by
      default. When upgrading from an older version of &productname; it is set
      to <literal>true</literal>by default.
     </para>
    </listitem>
    <listitem>
     <para>
      Apply your changes.
     </para>
    </listitem>
   </orderedlist>
  </important>

  <variablelist>
   <varlistentry>
    <term><guimenu>Image Storage: Default Storage Store</guimenu>
    </term>
    <listitem>
     <formalpara>
      <title><guimenu>File</guimenu></title>
      <para>
       Images are stored in an image file on the &contrnode;.
      </para>
     </formalpara>
     <formalpara>
      <title><guimenu>Cinder</guimenu></title>
      <para>
       Use &o_objstore; for image storage. This option is included as a
       technology preview and is not supported by &suse;.
      </para>
     </formalpara>
     <formalpara>
      <title><guimenu>Swift</guimenu></title>
      <para>
       Use &o_img; for image storage. If you have deployed &o_img;, using it
       for image storage is recommended.
      </para>
     </formalpara>
     <formalpara>
      <title><guimenu>Ceph</guimenu></title>
      <para>
       Use &ceph; for image storage. If you have deployed &ceph;, using it for
       image storage is recommended.
      </para>
     </formalpara>
     <formalpara>
      <title><guimenu>VMWare</guimenu></title>
      <para>
       If you are using VMware as a hypervisor, it is recommended to use
       <guimenu>VMWare</guimenu> for storing images. This will make starting
       VMware instances much faster.
      </para>
     </formalpara>
     <formalpara>
      <title><guimenu>Expose Backend Store Location</guimenu></title>
      <para>
       If this is set to <guimenu>true</guimenu>, the API will communicate the
       direct URl of the image's back-end location to HTTP clients. Set to
       <guimenu>false</guimenu> by default.
      </para>
     </formalpara>
     <para>
      Depending on the storage back-end, there are additional configuration
      options available:
     </para>
     <bridgehead renderas="sect4"><guimenu>File Store Parameters</guimenu>
     </bridgehead>
     <para>
      Only required if <guimenu>Default Storage Store</guimenu> is set to
      <guimenu>File</guimenu>.
     </para>
     <variablelist>
      <varlistentry>
       <term><guimenu>Image Store Directory</guimenu>
       </term>
       <listitem>
        <para>
         Specify the directory to host the image file. The directory specified
         here can also be an NFS share. See
         <xref linkend="sec.depl.inst.nodes.post.nfs"/> for more information.
        </para>
       </listitem>
      </varlistentry>
     </variablelist>
     <bridgehead renderas="sect4"><guimenu>Swift Store Parameters</guimenu>
     </bridgehead>
     <para>
      Only required if <guimenu>Default Storage Store</guimenu> is set to
      <guimenu>Swift</guimenu>.
     </para>
     <variablelist>
      <varlistentry>
       <term><guimenu>Swift Container</guimenu>
       </term>
       <listitem>
        <para>
         Set the name of the container to use for the images in &o_objstore;.
        </para>
       </listitem>
      </varlistentry>
     </variablelist>
     <bridgehead renderas="sect4"><guimenu>RADOS Store Parameters</guimenu>
     </bridgehead>
     <para>
      Only required if <guimenu>Default Storage Store</guimenu> is set to
      <guimenu>Rados</guimenu>.
     </para>
     <variablelist>
      <varlistentry>
       <term>RADOS User for CephX Authentication</term>
       <listitem>
        <para>
         If you are using a &cloud; internal &ceph; setup, the user you specify
         here is created if it does not exist. If you are using an external
         &ceph; cluster, specify the user you have set up for &o_img; (see
         <xref linkend="sec.depl.inst.nodes.post.ceph_ext"/> for more
         information).
        </para>
       </listitem>
      </varlistentry>
      <varlistentry>
       <term>RADOS Pool for Glance images</term>
       <listitem>
        <para>
         If you are using a &cloud; internal &ceph; setup, the pool you specify
         here is created if it does not exist. If you are using an external
         &ceph; cluster, specify the pool you have set up for &o_img; (see
         <xref linkend="sec.depl.inst.nodes.post.ceph_ext"/> for more
         information).
        </para>
       </listitem>
      </varlistentry>
     </variablelist>
     <bridgehead renderas="sect4"><guimenu>VMWare Store Parameters</guimenu>
     </bridgehead>
     <para>
      Only required if <guimenu>Default Storage Store</guimenu> is set to
      <guimenu>VMWare</guimenu>.
     </para>
     <variablelist>
      <varlistentry>
       <term><guimenu>vCenter Host/IP Address</guimenu>
       </term>
       <listitem>
        <para>
         Name or IP address of the vCenter server.
        </para>
       </listitem>
      </varlistentry>
      <varlistentry>
       <term><guimenu>vCenter Username</guimenu> / <guimenu>vCenter
        Password</guimenu>
       </term>
       <listitem>
        <para>
         vCenter login credentials.
        </para>
       </listitem>
      </varlistentry>
      <varlistentry>
       <term><guimenu>Datastores for Storing Images</guimenu>
       </term>
       <listitem>
        <para>
         A comma-separated list of datastores specified in the format:
         <replaceable>DATACENTER_NAME</replaceable>:<replaceable>DATASTORE_NAME</replaceable>
        </para>
       </listitem>
      </varlistentry>
      <varlistentry>
       <term><guimenu>
         Path on the datastore, where the glance images will be
         stored
        </guimenu>
       </term>
       <listitem>
        <para>
         Specify an absolute path here.
        </para>
       </listitem>
      </varlistentry>
     </variablelist>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><guimenu>SSL Support: Protocol</guimenu>
    </term>
    <listitem>
     <para>
      Choose whether to encrypt public communication (<guimenu>HTTPS</guimenu>)
      or not (<guimenu>HTTP</guimenu>). If you choose <guimenu>HTTPS</guimenu>,
      refer to <xref linkend="sec.depl.ostack.keystone.ssl"/> for configuration
      details.
     </para>
    </listitem>
   </varlistentry>
<!-- fs 2016-01-26: No longer present in Cloud6
   <varlistentry>
    <term><guimenu>API: Bind to All Addresses</guimenu>
    </term>
    <listitem>
     <para>
      Set this option to <guimenu>true</guimenu> to enable users to upload
      images to &o_img;. If unset, only the operator can upload images.
     </para>
    </listitem>
    </varlistentry>
    -->
   <varlistentry>
    <term><guimenu>Caching</guimenu>
    </term>
    <listitem>
     <para>
      Enable and configure image caching in this section. By default, image
      caching is disabled. You can see this the Raw view of your Nova barclamp:
     </para>
<screen>image_cache_manager_interval = -1</screen>
     <para>
      This option sets the number of seconds to wait between runs of the image
      cache manager. Disabling it means that the cache manager will not
      automatically remove the unused images from the cache, so if you have
      many &o_img; images and are running out of storage you must manually
      remove the unused images from the cache. We recommend leaving this option
      disabled as it is known to cause issues, especially with shared storage.
      The cache manager may remove images still in use, e.g. when network
      outages cause synchronization problems with compute nodes.
     </para>
     <para>
      If you wish to enable caching, re-enable it in a custom Nova
      configuration file, for example
      <filename>/etc/nova/nova.conf.d/100-nova.conf</filename>. This sets the
      the interval to four minutes:
     </para>
<screen>image_cache_manager_interval = 2400</screen>
     <para>
      See <xref linkend="cha.depl.ostack.configs"/> for more information on
      custom configurations.
     </para>
     <para>
      Learn more about &o_img;'s caching feature at
      <link xlink:href="http://docs.openstack.org/developer/glance/cache.html"/>.
     </para>
    </listitem>
   </varlistentry>
<!-- fs 2016-01-26: No longer present in Cloud6
   <varlistentry>
    <term><guimenu>Database: SQL Idle Timeout</guimenu>
    </term>
    <listitem>
     <para>
      Time after which idle database connections will be dropped.
     </para>
    </listitem>
    </varlistentry>
    -->
   <varlistentry>
    <term><guimenu>Logging: Verbose Logging</guimenu>
    </term>
    <listitem>
     <para>
      Shows debugging output in the log files when set to
      <guimenu>true</guimenu>.
     </para>
    </listitem>
   </varlistentry>
  </variablelist>

  <figure>
   <title>The &o_img; &Barcl;</title>
   <mediaobject>
    <imageobject role="fo">
     <imagedata fileref="depl_barclamp_glance.png" width="100%" format="png"/>
    </imageobject>
    <imageobject role="html">
     <imagedata fileref="depl_barclamp_glance.png" width="75%" format="png"/>
    </imageobject>
   </mediaobject>
  </figure>

  <sect2 xml:id="sec.depl.ostack.glance.ha">
   <title>&haSetup; for &o_img;</title>
   <para>
    &o_img; can be made highly available by deploying it on a cluster. We
    strongly recommended doing this for the image data as well. The recommended
    way is to use &o_objstore; or an external &ceph; cluster for the image
    repository. If you are using a directory on the node instead (file storage
    back-end), you should set up shared storage on the cluster for it.
   </para>
  </sect2>
 </sect1>
 <sect1 xml:id="sec.depl.ostack.cinder">
  <title>Deploying &o_blockstore;</title>

  <para>
   &o_blockstore;, the successor of Nova Volume, provides volume block storage.
   It adds persistent storage to an &vmguest; that will persist until deleted,
   contrary to ephemeral volumes that only persist while the &vmguest; is
   running.
  </para>

  <para>
   &o_blockstore; can provide volume storage by using different back-ends such
   as local file, one or more local disks, &ceph; (RADOS), VMware, or network
   storage solutions from EMC, EqualLogic, Fujitsu, or NetApp. Since
   &productname; 5, &o_blockstore; supports using several back-ends
   simultaneously. It is also possible to deploy the same network storage
   back-end multiple times and therefore use different installations at the
   same time.
  </para>

  <para>
   The attributes that can be set to configure &o_blockstore; depend on the
   back-end. The only general option is <guimenu>SSL Support:
   Protocol</guimenu> (see <xref linkend="sec.depl.ostack.keystone.ssl"/> for
   configuration details).
  </para>

  <tip>
   <title>Adding or Changing a Back-End</title>
   <para>
    When first opening the &o_blockstore; &barcl;, the default
    proposal&mdash;<guimenu>Raw Devices</guimenu>&mdash;is already available
    for configuration. To optionally add a back-end, go to the section
    <guimenu>Add New Cinder Back-End</guimenu> and choose a <guimenu>Type Of
    Volume</guimenu> from the drop-down box. Optionally, specify the
    <guimenu>Name for the Backend</guimenu>. This is recommended when deploying
    the same volume type more than once. Existing back-end configurations
    (including the default one) can be deleted by clicking the trashcan icon if
    no longer needed. Note that you must configure at least one back-end.
   </para>
  </tip>

  <bridgehead renderas="sect2"><guimenu>Raw devices</guimenu> (local disks)
  </bridgehead>

  <variablelist>
   <varlistentry>
    <term><guimenu>Disk Selection Method</guimenu>
    </term>
    <listitem>
     <para>
      Choose whether to use the <guimenu>First Available</guimenu> disk or
      <guimenu>All Available</guimenu> disks. <quote>Available disks</quote>
      are all disks currently not used by the system. Note that one disk
      (usually <filename>/dev/sda</filename>) of every block storage node is
      already used for the operating system and is not available for
      &o_blockstore;.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><guimenu>Name of Volume</guimenu>
    </term>
    <listitem>
     <para>
      Specify a name for the &o_blockstore; volume.
     </para>
    </listitem>
   </varlistentry>
  </variablelist>

  <bridgehead renderas="sect3"><guimenu>EMC</guimenu> (EMC² Storage)
  </bridgehead>

  <variablelist>
   <varlistentry>
    <term><guimenu>IP address of the ECOM server</guimenu> / <guimenu>Port of the ECOM server</guimenu>
    </term>
    <listitem>
     <para>
      IP address and Port of the ECOM server.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><guimenu>Username for accessing the ECOM server</guimenu> / <guimenu>Password for accessing the ECOM server</guimenu>
    </term>
    <listitem>
     <para>
      Login credentials for the ECOM server.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><guimenu>VMAX port groups to expose volumes managed by this backend</guimenu>
    </term>
    <listitem>
     <para>
      VMAX port groups that expose volumes managed by this back-end.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><guimenu>Serial number of the VMAX Array</guimenu>
    </term>
    <listitem>
     <para>
      Unique VMAX array serial number.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><guimenu>Pool name within a given array</guimenu>
    </term>
    <listitem>
     <para>
      Unique pool name within a given array.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><guimenu>FAST Policy name to be used</guimenu>
    </term>
    <listitem>
     <para>
      Name of the FAST Policy to be used. When specified, volumes managed by
      this back-end are managed as under FAST control.
     </para>
    </listitem>
   </varlistentry>
  </variablelist>

  <para>
   For more information on the EMC driver refer to the &ostack; documentation
   at
   <link xlink:href="http://docs.openstack.org/liberty/config-reference/content/emc-vmax-driver.html"/>.
  </para>

  <bridgehead renderas="sect3"><guimenu>EqualLogic</guimenu>
  </bridgehead>

  <para>
   EqualLogic drivers are included as a technology preview and are not
   supported.
  </para>

  <bridgehead renderas="sect2"><guimenu>Fujitsu ETERNUS DX</guimenu>
  </bridgehead>

  <variablelist>
   <varlistentry>
    <term><guimenu>Connection Protocol</guimenu>
    </term>
    <listitem>
     <para>
      Select the protocol used to connect, either
      <guimenu>FibreChannel</guimenu> or <guimenu>iSCSI</guimenu>.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><guimenu>IP for SMI-S</guimenu> / <guimenu>Port for SMI-S</guimenu>
    </term>
    <listitem>
     <para>
      IP address and port of the ETERNUS SMI-S Server.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><guimenu>Username for SMI-S</guimenu> / <guimenu>Password for SMI-S</guimenu>
    </term>
    <listitem>
     <para>
      Login credentials for the ETERNUS SMI-S Server.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><guimenu>Snapshot (Thick/RAID Group) Pool Name</guimenu>
    </term>
    <listitem>
     <para>
      Storage pool (RAID group) in which the volumes are created. Make sure
      that the RAID group on the server has already been created. If a RAID
      group that does not exist is specified, the RAID group is built from
      unused disk drives. The RAID level is automatically determined by the
      ETERNUS DX Disk storage system.
     </para>
    </listitem>
   </varlistentry>
  </variablelist>

  <bridgehead renderas="sect2"><guimenu>Hitachi HUSVM</guimenu>
  </bridgehead>

  <para>
   For information on configuring the Hitachi HUSVM back-end, refer to
   <link xlink:href="http://docs.openstack.org/newton/config-reference/block-storage/drivers/hitachi-storage-volume-driver.html"/>.
  </para>

  <bridgehead renderas="sect2"><guimenu>NetApp</guimenu>
  </bridgehead>

  <variablelist>
   <varlistentry>
    <term><guimenu>Storage Family Type</guimenu> / <guimenu>Storage Protocol</guimenu>
    </term>
    <listitem>
     <para>
      &cloud; can use <quote>Data ONTAP</quote> in <guimenu>7-Mode</guimenu>,
      or in <guimenu>Clustered Mode</guimenu>. In <guimenu>7-Mode</guimenu>
      vFiler will be configured, in <guimenu>Clustered Mode</guimenu> vServer
      will be configured. The <guimenu>Storage Protocol</guimenu> can be set to
      either <guimenu>iSCSI</guimenu> or <guimenu>NFS</guimenu>. Choose the
      driver and the protocol your NetApp is licensed for.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><guimenu>Server host name</guimenu>
    </term>
    <listitem>
     <para>
      The management IP address for the 7-Mode storage controller, or the
      cluster management IP address for the clustered Data ONTAP.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><guimenu>Transport Type</guimenu>
    </term>
    <listitem>
     <para>
      Transport protocol for communicating with the storage controller or
      clustered Data ONTAP. Supported protocols are HTTP and HTTPS. Choose the
      protocol your NetApp is licensed for.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><guimenu>Server port</guimenu>
    </term>
    <listitem>
     <para>
      The port to use for communication. Port 80 is usually used for HTTP, 443
      for HTTPS.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><guimenu>Username for accessing NetApp</guimenu> / <guimenu>Password for Accessing NetApp</guimenu>
    </term>
    <listitem>
     <para>
      Login credentials.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><guimenu>
      The vFiler Unit Name for provisioning OpenStack volumes (netapp_vfiler)
     </guimenu>
    </term>
    <listitem>
     <para>
      The vFiler unit to be used for provisioning of &ostack; volumes. This
      setting is only available in <guimenu>7-Mode</guimenu>.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><guimenu>Restrict provisioning on iSCSI to these volumes (netapp_volume_list)</guimenu>
    </term>
    <listitem>
     <para>
      Provide a list of comma-separated volume names to be used for
      provisioning. This setting is only available when using iSCSI as storage
      protocol.
     </para>
    </listitem>
   </varlistentry>
  </variablelist>

  <bridgehead renderas="sect2"><guimenu>NFS</guimenu>
  </bridgehead>

  <variablelist>
   <varlistentry>
    <term><guimenu>List of NFS Exports</guimenu>
    </term>
    <listitem>
     <para>
      A list of available file systems on an NFS server. Enter your NFS mountpoints
      in the <guimenu>List of NFS Exports</guimenu> form in this format: <replaceable>host:mountpoint -o options</replaceable>. For example:
<screen>
host1:/srv/nfs/share1 /mnt/nfs/share1 -o rsize=8192,wsize=8192,timeo=14,intr  
</screen>      
     </para>
    </listitem>
   </varlistentry>
  </variablelist>

  <bridgehead renderas="sect3"><guimenu>RADOS</guimenu> (&ceph;)
  </bridgehead>

  <variablelist>
   <varlistentry>
    <term><guimenu>Use Ceph Deployed by Crowbar</guimenu>
    </term>
    <listitem>
     <para>
      Select <guimenu>true</guimenu> if you have deployed &ceph; with &cloud;.
      If you are using an external &ceph; cluster (see
      <xref linkend="sec.depl.inst.nodes.post.ceph_ext"/> for setup
      instructions), select <guimenu>false</guimenu>.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><guimenu>RADOS pool for Cinder volumes</guimenu>
    </term>
    <listitem>
     <para>
      Name of the pool used to store the &o_blockstore; volumes.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><guimenu>
      RADOS user (Set Only if Using CephX authentication)
     </guimenu>
    </term>
    <listitem>
     <para>
      &ceph; user name.
     </para>
    </listitem>
   </varlistentry>
  </variablelist>

  <bridgehead renderas="sect3"><guimenu>VMware Parameters</guimenu>
  </bridgehead>

  <variablelist>
   <varlistentry>
    <term><guimenu>vCenter Host/IP Address</guimenu>
    </term>
    <listitem>
     <para>
      Host name or IP address of the vCenter server.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><guimenu>vCenter Username</guimenu> / <guimenu>vCenter
     Password</guimenu>
    </term>
    <listitem>
     <para>
      vCenter login credentials.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><guimenu>vCenter Cluster Names for Volumes</guimenu>
    </term>
    <listitem>
     <para>
      Provide a comma-separated list of cluster names.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><guimenu>Folder for Volumes</guimenu>
    </term>
    <listitem>
     <para>
      Path to the directory used to store the &o_blockstore; volumes.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><guimenu>CA file for verifying the vCenter certificate</guimenu>
    </term>
    <listitem>
     <para>
      Absolute path to the vCenter CA certificate.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><guimenu>
      vCenter SSL Certificate is insecure (for instance, self-signed)
     </guimenu>
    </term>
    <listitem>
     <para>
      Default value: <literal>false</literal> (the CA truststore is used for
      verification). Set this option to <literal>true</literal> when using
      self-signed certificates to disable certificate checks. This setting is
      for testing purposes only and must not be used in production
      environments!
     </para>
    </listitem>
   </varlistentry>
  </variablelist>

  <bridgehead renderas="sect2"><guimenu>Local file</guimenu>
  </bridgehead>

  <variablelist>
   <varlistentry>
    <term><guimenu>Volume File Name</guimenu>
    </term>
    <listitem>
     <para>
      Absolute path to the file to be used for block storage.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><guimenu>Maximum File Size (GB)</guimenu>
    </term>
    <listitem>
     <para>
      Maximum size of the volume file. Make sure not to overcommit the size,
      since it will result in data loss.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><guimenu>Name of Volume</guimenu>
    </term>
    <listitem>
     <para>
      Specify a name for the &o_blockstore; volume.
     </para>
    </listitem>
   </varlistentry>
  </variablelist>

  <note>
   <title>Using <guimenu>Local File</guimenu> for &blockstore;</title>
   <para>
    Using a file for block storage is not recommended for production systems,
    because of performance and data security reasons.
   </para>
  </note>

  <bridgehead renderas="sect3"><guimenu>Other driver</guimenu>
  </bridgehead>

  <para>
   Lets you manually pick and configure a driver. Only use this option for
   testing purposes, as it is not supported.
  </para>

  <figure>
   <title>The &o_blockstore; &Barcl;</title>
   <mediaobject>
    <imageobject role="fo">
     <imagedata fileref="depl_barclamp_cinder.png" width="100%" format="png"/>
    </imageobject>
    <imageobject role="html">
     <imagedata fileref="depl_barclamp_cinder.png" width="75%" format="png"/>
    </imageobject>
   </mediaobject>
  </figure>

  <para>
   The &o_blockstore; component consists of two different roles:
  </para>

  <variablelist>
   <varlistentry>
    <term><guimenu>cinder-controller</guimenu>
    </term>
    <listitem>
     <para>
      The &o_blockstore; controller provides the scheduler and the API.
      Installing <guimenu>cinder-controller</guimenu> on a &contrnode; is
      recommended.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><guimenu>cinder-volume</guimenu>
    </term>
    <listitem>
     <para>
      The virtual block storage service. It can be installed on a &contrnode;.
      However, we recommend deploying it on one or more dedicated nodes
      supplied with sufficient networking capacity to handle the increase in
      network traffic.
     </para>
    </listitem>
   </varlistentry>
  </variablelist>

  <figure>
   <title>The &o_blockstore; &Barcl;: Node Deployment Example</title>
   <mediaobject>
    <imageobject role="fo">
     <imagedata fileref="depl_barclamp_cinder_node_deployment.png" width="100%" format="png"/>
    </imageobject>
    <imageobject role="html">
     <imagedata fileref="depl_barclamp_cinder_node_deployment.png" width="75%" format="png"/>
    </imageobject>
   </mediaobject>
  </figure>

  <sect2 xml:id="sec.depl.ostack.cinder.ha">
   <title>&haSetup; for &o_blockstore;</title>
   <para>
    Both the <guimenu>cinder-controller</guimenu> and the
    <guimenu>cinder-volume</guimenu> role can be deployed on a cluster.
   </para>
   <note>
    <title>Moving <guimenu>cinder-volume</guimenu> to a Cluster</title>
    <para>
     If you need to re-deploy <guimenu>cinder-volume</guimenu> role from a
     single machine to a cluster environment, the following will happen:
     Volumes that are currently attached to instances will continue to work,
     but adding volumes to instances will not succeed.
    </para>
    <para>
     To solve this issue, run the following script once on each node that
     belongs to the <guimenu>cinder-volume</guimenu> cluster:
     <filename>/usr/bin/cinder-migrate-volume-names-to-cluster</filename>.
    </para>
    <para>
     The script is automatically installed by &crow; on every machine or
     cluster that has a <guimenu>cinder-volume</guimenu> role applied to it.
    </para>
   </note>
   <para>
    In combination with &ceph; or a network storage solution, deploying
    &o_blockstore; in a cluster minimizes the potential downtime. If using
    &ceph; or a network storage is not an option, you need to set up a shared
    storage directory (for example, with NFS), mount it on all cinder volume
    nodes, and use the <guimenu>Local File</guimenu> back-end with this shared
    directory. Using <guimenu>Raw Devices</guimenu> is not an option, since
    local disks cannot be shared.
   </para>
  </sect2>
 </sect1>
 <sect1 xml:id="sec.depl.ostack.quantum">
  <title>Deploying &o_netw;</title>

  <para>
   &o_netw; provides network connectivity between interface devices managed by
   other &ostack; components (most likely &o_comp;). The service works by
   enabling users to create their own networks and then attach interfaces to
   them.
  </para>

  <para>
   &o_netw; must be deployed on a &contrnode;. You first need to choose a core
   plug-in&mdash;<guimenu>ml2</guimenu> or <guimenu>vmware</guimenu>. Depending
   on your choice, more configuration options will become available.
  </para>

  <para>
   The <guimenu>vmware</guimenu> option lets you use an existing VMWare NSX
   installation. Using this plugin is not a prerequisite for the VMWare vSphere
   hypervisor support. However, it is needed when wanting to have security
   groups supported on VMWare compute nodes. For all other scenarios, choose
   <guimenu>ml2</guimenu>.
  </para>

  <para>
   The only global option that can be configured is <guimenu>SSL
   Support</guimenu>. Choose whether to encrypt public communication
   (<guimenu>HTTPS</guimenu>) or not (<guimenu>HTTP</guimenu>). If choosing
   <guimenu>HTTPS</guimenu>, refer to
   <xref linkend="sec.depl.ostack.keystone.ssl"/> for configuration details.
  </para>

  <bridgehead renderas="sect2"><guimenu>ml2</guimenu> (Modular Layer 2)
  </bridgehead>

  <variablelist>
   <varlistentry>
    <term><guimenu>Modular Layer 2 Mechanism Drivers</guimenu>
    </term>
    <listitem>
     <para>
      Select which mechanism driver(s) shall be enabled for the ml2 plugin. It
      is possible to select more than one driver by holding the
      <keycap function="control"/> key while clicking. Choices are:
     </para>
     <formalpara>
      <title><guimenu>openvswitch</guimenu></title>
      <para>
       Supports GRE, VLAN and VLANX networks (to be configured via the
       <guimenu>Modular Layer 2 type drivers</guimenu> setting).
      </para>
     </formalpara>
     <formalpara>
      <title><guimenu>linuxbridge</guimenu></title>
      <para>
       Supports VLANs only. Requires to specify the <guimenu>Maximum Number of
       VLANs</guimenu>.
      </para>
     </formalpara>
     <formalpara>
      <title><guimenu>cisco_nexus</guimenu></title>
      <para>
       Enables &o_netw; to dynamically adjust the VLAN settings of the ports of
       an existing Cisco Nexus switch when instances are launched. It also
       requires <guimenu>openvswitch</guimenu> which will automatically be
       selected. With <guimenu>Modular Layer 2 type drivers</guimenu>,
       <guimenu>vlan</guimenu> must be added. This option also requires to
       specify the <guimenu>Cisco Switch Credentials</guimenu>. See
       <xref
       linkend="app.deploy.cisco"/> for details.
      </para>
     </formalpara>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><guimenu>Use Distributed Virtual Router Setup</guimenu>
    </term>
    <listitem>
     <para>
      With the default setup, all intra-&compnode; traffic flows through the
      network &contrnode;. The same is true for all traffic from floating IPs.
      In large deployments the network &contrnode; can therefore quickly become
      a bottleneck. When this option is set to <guimenu>true</guimenu>, network
      agents will be installed on all compute nodes. This will de-centralize
      the network traffic, since &compnode;s will be able to directly
      <quote>talk</quote> to each other. Distributed Virtual Routers (DVR)
      require the <guimenu>openvswitch</guimenu> driver and will not work with
      the <guimenu>linuxbridge</guimenu> driver. HyperV &compnode;s will not be
      supported&mdash;network traffic for these nodes will be routed via the
      &contrnode; on which <guimenu>neutron-network</guimenu> is deployed. For
      details on DVR refer to
      <link
      xlink:href="https://wiki.openstack.org/wiki/Neutron/DVR"/>.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><guimenu>Modular Layer 2 Type Drivers</guimenu>
    </term>
    <listitem>
     <para>
      This option is only available when having chosen the
      <guimenu>openvswitch</guimenu> or the <guimenu>cisco_nexus</guimenu>
      mechanism drivers. Options are <guimenu>vlan</guimenu>,
      <guimenu>gre</guimenu> and <guimenu>vxlan</guimenu>. It is possible to
      select more than one driver by holding the <keycap function="control"/>
      key while clicking.
     </para>
     <para>
      When multiple type drivers are enabled, you need to select the
      <guimenu>Default Type Driver for Provider Network</guimenu>, that will be
      used for newly created provider networks. This also includes the
      <literal>nova_fixed</literal> network, that will be created when applying
      the &o_netw; proposal. When manually creating provider networks with the
      <command>neutron</command> command, the default can be overwritten with
      the <option>--provider:network_type
      <replaceable>type</replaceable></option> switch. You will also need to
      set a <guimenu>Default Type Driver for Tenant Network</guimenu>. It is
      not possible to change this default when manually creating tenant
      networks with the <command>neutron</command> command. The non-default
      type driver will only be used as a fallback.
     </para>
     <para>
      Depending on your choice of the type driver, more configuration options
      become available.
     </para>
     <formalpara>
      <title><guimenu>gre</guimenu></title>
      <para>
       Having chosen <guimenu>gre</guimenu>, you also need to specify the start
       and end of the tunnel ID range.
      </para>
     </formalpara>
     <formalpara>
      <title><guimenu>vlan</guimenu></title>
      <para>
       The option <guimenu>vlan</guimenu> requires you to specify the
       <guimenu>Maximum number of VLANs</guimenu>.
      </para>
     </formalpara>
     <formalpara>
      <title><guimenu>vxlan</guimenu></title>
      <para>
       Having chosen <guimenu>vxlan</guimenu>, you also need to specify the
       start and end of the VNI range.
      </para>
     </formalpara>
    </listitem>
   </varlistentry>
  </variablelist>

  <important>
   <title>Drivers for HyperV &compnode;s</title>
   <para>
    HyperV &compnode;s do not support <guimenu>gre</guimenu> and
    <guimenu>vxlan</guimenu>. If your environment includes a heterogeneous mix
    of &compnode;s including HyperV nodes, make sure to select
    <guimenu>vlan</guimenu>. This can be done in addition to the other drivers.
   </para>
  </important>

  <important>
   <title>Drivers for the VMware &compnode;</title>
   <para>
    &o_netw; must not be deployed with the <literal>openvswitch with
    gre</literal> plug-in. See <xref linkend="app.deploy.vmware"/> for details.
   </para>
  </important>

  <bridgehead renderas="sect2"><guimenu>z/VM Configuration</guimenu>
  </bridgehead>

  <variablelist>
   <varlistentry>
    <term>xCAT Host/IP Address</term>
    <listitem>
     <para>
      Host name or IP address of the xCAT Management Node.
      <remark>
            2016-12-29 - dpopov: DEVS FIXME Please check whether this is correct.</remark>
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>xCAT Username/Password</term>
    <listitem>
     <para>
      xCAT login credentials.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>rdev list for physnet1 vswitch uplink (if available)</term>
    <listitem>
     <para>
      List of rdev addresses that should be connected to this vswitch.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>xCAT IP Address on Management Network</term>
    <listitem>
     <para>
      IP address of the xCAT management interface.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>Net Mask of Management Network</term>
    <listitem>
     <para>
      Net mask of the xCAT management interface.
      <remark>
            2016-12-29 - dpopov: DEVS FIXME Please check whether this is correct.</remark>
     </para>
    </listitem>
   </varlistentry>
  </variablelist>

  <bridgehead renderas="sect2"><guimenu>vmware</guimenu>
  </bridgehead>

  <para>
   This plug-in requires to configure access to the VMWare NSX service.
  </para>

  <variablelist>
   <varlistentry>
    <term><guimenu>VMWare NSX User Name/Password</guimenu>
    </term>
    <listitem>
     <para>
      Login credentials for the VMWare NSX server. The user needs to have
      administrator permissions on the NSX server.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><guimenu>VMWare NSX Controllers</guimenu>
    </term>
    <listitem>
     <para>
      Enter the IP address and the port number
      (<replaceable>IP-ADDRESS</replaceable>:<replaceable>PORT</replaceable>)
      of the controller API endpoint. If the port number is omitted, port 443
      will be used. You may also enter multiple API endpoints
      (comma-separated), provided they all belong to the same controller
      cluster. When multiple API endpoints are specified, the plugin will load
      balance requests on the various API endpoints.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><guimenu>UUID of the NSX Transport Zone/Gateway Service</guimenu>
    </term>
    <listitem>
     <para>
      The UUIDs for the transport zone and the gateway service can be obtained
      from the NSX server. They will be used when networks are created.
     </para>
    </listitem>
   </varlistentry>
  </variablelist>

  <figure>
   <title>The &o_netw; &Barcl;</title>
   <mediaobject>
    <imageobject role="fo">
     <imagedata fileref="depl_barclamp_network.png" width="100%" format="png"/>
    </imageobject>
    <imageobject role="html">
     <imagedata fileref="depl_barclamp_network.png" width="75%" format="png"/>
    </imageobject>
   </mediaobject>
  </figure>

  <para>
   The &o_netw; component consists of two different roles:
  </para>

  <variablelist>
   <varlistentry>
    <term><guimenu>neutron-server</guimenu>
    </term>
    <listitem>
     <para>
      <guimenu>neutron-server</guimenu> provides the scheduler and the API. It
      needs to be installed on a &contrnode;.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><guimenu>neutron-network</guimenu>
    </term>
    <listitem>
     <para>
      This service runs the various agents that manage the network traffic of
      all the cloud instances. It acts as the DHCP and DNS server and as a
      gateway for all cloud instances. It is recommend to deploy this role on a
      dedicated node supplied with sufficient network capacity.
     </para>
    </listitem>
   </varlistentry>
  </variablelist>

  <figure>
   <title>The &o_netw; &barcl;</title>
   <mediaobject>
    <imageobject role="fo">
     <imagedata fileref="depl_barclamp_neutron_node_deployment.png" width="100%" format="png"/>
    </imageobject>
    <imageobject role="html">
     <imagedata fileref="depl_barclamp_neutron_node_deployment.png" width="75%" format="png"/>
    </imageobject>
   </mediaobject>
  </figure>

  <sect2 xml:id="sec.depl.ostack.network.infoblox">
   <title>Using Infoblox IPAM Plug-in</title>
   <para>
    In the &o_netw; &barcl;, you can enable support for the infoblox IPAM
    plug-in and configure it. For configuration, the
    <literal>infoblox</literal> section contains the subsections
    <literal>grids</literal> and <literal>grid_defaults</literal>.
   </para>
   <variablelist>
    <varlistentry>
     <term>grids</term>
     <listitem>
      <para>
       This subsection must contain at least one entry. For each entry, the
       following parameters are required:
      </para>
      <itemizedlist>
       <listitem>
        <para>
         admin_user_name
        </para>
       </listitem>
       <listitem>
        <para>
         admin_password
        </para>
       </listitem>
       <listitem>
        <para>
         grid_master_host
        </para>
       </listitem>
       <listitem>
        <para>
         grid_master_name
        </para>
       </listitem>
       <listitem>
        <para>
         data_center_name
        </para>
       </listitem>
      </itemizedlist>
      <para>
       You can also add multiple entries to the <literal>grids</literal>
       section. However, the upstream infoblox agent only supports a single
       grid currently.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>grid_defaults</term>
     <listitem>
      <para>
       This subsection contains the default settings that are used for each
       grid (unless you have configured specific settings within the
       <literal>grids</literal> section).
      </para>
     </listitem>
    </varlistentry>
   </variablelist>
   <para>
    For detailed information on all infoblox-related configuration settings,
    see
    <link
     xlink:href="https://github.com/openstack/networking-infoblox/blob/master/doc/source/installation.rst"/>.
   </para>
   <para>
    Currently, all configuration options for infoblox are only available in the
    <literal>raw</literal> mode of the &o_netw; &barcl;. To enable support for
    the infoblox IPAM plug-in and configure it, proceed as follows:
   </para>
   <procedure>
    <step>
     <para>
      <guimenu>Edit</guimenu> the &o_netw; &barcl; proposal or create a new
      one.
     </para>
    </step>
    <step>
     <para>
      Click <guimenu>Raw</guimenu> and search for the following section:
     </para>
<screen>"use_infoblox": false,</screen>
    </step>
    <step>
     <para>
      To enable support for the infoblox IPAM plug-in, change this entry to:
     </para>
<screen>"use_infoblox": true,</screen>
    </step>
    <step>
     <para>
      In the <literal>grids</literal> section, configure at least one grid by
      replacing the example values for each parameter with real values.
     </para>
    </step>
    <step>
     <para>
      If you need specific settings for a grid, add some of the parameters from
      the <literal>grid_defaults</literal> section to the respective grid entry
      and adjust their values.
     </para>
     <para>
      Otherwise &crow; applies the default setting to each grid when you save
      the &barcl; proposal.
     </para>
    </step>
    <step>
     <para>
      Save your changes and apply them.
     </para>
    </step>
   </procedure>
  </sect2>

  <sect2 xml:id="sec.depl.ostack.network.ha">
   <title>&haSetup; for &o_netw;</title>
   <para>
    &o_netw; can be made highly available by deploying
    <guimenu>neutron-server</guimenu> and <guimenu>neutron-network</guimenu> on
    a cluster. While <guimenu>neutron-server</guimenu> may be deployed on a
    cluster shared with other services, it is strongly recommended to use a
    dedicated cluster solely for the <guimenu>neutron-network</guimenu> role.
   </para>
  </sect2>
 </sect1>
 <sect1 xml:id="sec.depl.ostack.nova">
  <title>Deploying &o_comp;</title>

  <para>
   &o_comp; provides key services for managing the &cloud;, sets up the
   &compnode;s. &cloud; currently supports KVM, Xen and Microsoft Hyper V and
   VMWare vSphere. The unsupported QEMU option is included to enable test
   setups with virtualized nodes. The following attributes can be configured
   for &o_comp;:
   <remark condition="clarity">
    2016-02-05 - fs: FIXME z/VM Configuration
   </remark>
  </para>

  <variablelist>
   <varlistentry>
    <term><guimenu>
      Scheduler Options: Virtual RAM to Physical RAM allocation ratio
     </guimenu>
    </term>
    <listitem>
     <para>
      Set the <quote>overcommit ratio</quote> for RAM for &vmguest;s on the
      &compnode;s. A ratio of <literal>1.0</literal> means no overcommitment.
      Changing this value is not recommended.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><guimenu>
      Scheduler Options: Virtual CPU to Physical CPU allocation ratio
     </guimenu>
    </term>
    <listitem>
     <para>
      Set the <quote>overcommit ratio</quote> for CPUs for &vmguest;s on the
      &compnode;s. A ratio of <literal>1.0</literal> means no overcommitment.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><guimenu>
      Scheduler Options: Virtual Disk to Physical Disk allocation ratio
     </guimenu>
    </term>
    <listitem>
     <para>
      Set the <quote>overcommit ratio</quote> for virtual disks for &vmguest;s
      on the &compnode;s. A ratio of <literal>1.0</literal> means no
      overcommitment.
      <remark condition="accuracy">2017-01-02 - dpopov: DEVS FIXME Check whether this is correct.</remark>
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><guimenu>
      Scheduler Options: Reserved Memory for Nova Compute hosts (MB)
     </guimenu>
    </term>
    <listitem>
     <para>
      Amount of reserved host memory that is not used for allocating VMs by
      Nova Compute.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><guimenu>Live Migration Support: Enable Libvirt Migration</guimenu>
    </term>
    <listitem>
     <para>
      Allows to move &kvm; and &xen; &vmguest;s to a different &compnode;
      running the same hypervisor (cross hypervisor migrations are not
      supported). Useful when a &compnode; needs to be shut down or rebooted
      for maintenance or when the load of the &compnode; is very high.
      &Vmguest;s can be moved while running (Live Migration).
     </para>
     <warning>
      <title>Libvirt Migration and Security</title>
      <para>
       Enabling the libvirt migration option will open a TCP port on the
       &compnode;s that allows access to all &vmguest;s from all machines in
       the admin network. Ensure that only authorized machines have access to
       the admin network when enabling this option.
      </para>
     </warning>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>Live Migration Support: Setup Shared Storage</term>
    <listitem>
     <para>
      Sets up a directory <filename>/var/lib/nova/instances</filename> on the
      &contrnode; on which <guimenu>nova-controller</guimenu> is running. This
      directory is exported via NFS to all compute nodes and will host a copy
      of the root disk of <emphasis>all</emphasis> &xen; &vmguest;s. This setup
      is required for live migration of &xen; &vmguest;s (but not for &kvm;)
      and is used to provide central handling of instance data. Enabling this
      option is only recommended if &xen; live migration is
      required&mdash;otherwise it should be disabled.
     </para>
     <warning>
      <title>Do Not Set Up Shared Storage When &vmguest;s are Running</title>
      <para>
       Setting up shared storage in a &cloud; where &vmguest;s are running will
       result in connection losses to all running &vmguest;s. It is strongly
       recommended to set up shared storage when deploying &cloud;. If it needs
       to be done at a later stage, make sure to shut down all &vmguest;s prior
       to the change.
      </para>
     </warning>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><guimenu>KVM Options: Enable Kernel Samepage Merging</guimenu>
    </term>
    <listitem>
     <para>
      Kernel SamePage Merging (KSM) is a Linux Kernel feature which merges
      identical memory pages from multiple running processes into one memory
      region. Enabling it optimizes memory usage on the &compnode;s when using
      the &kvm; hypervisor at the cost of slightly increasing CPU usage.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><guimenu>z/VM Configuration: xCAT Host/IP Address</guimenu>
    </term>
    <listitem>
     <para>
      IP address of the xCAT management interface.
      <remark condition="accuracy">2017-01-02 - dpopov: DEVS FIXME Please check whether this is accurate.</remark>
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><guimenu>z/VM Configuration: xCAT Username/Password</guimenu>
    </term>
    <listitem>
     <para>
      xCAT login credentials.
      <remark condition="accuracy">2017-01-02 - dpopov: DEVS FIXME Please check whether this is accurate.</remark>
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><guimenu>z/VM Configuration: z/VM disk pool for ephemeral disks</guimenu>
    </term>
    <listitem>
     <para>
      Name of the disk pool for ephemeral disks.
      <remark condition="clarity">2017-01-02 - dpopov: DEVS FIXME More info required.</remark>
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><guimenu>z/VM Configuration: z/VM disk pool type for ephemeral disks (ECKD or FBA)</guimenu>
    </term>
    <listitem>
     <para>
      Choose disk pool type for ephemeral disks.
      <remark condition="clarity">2017-01-02 - dpopov: DEVS FIXME More info required.</remark>
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><guimenu>z/VM Configuration: z/VM Host Managed By xCAT MN</guimenu>
    </term>
    <listitem>
     <para>
      z/VM host managed by xCAT Management Node.
      <remark condition="clarity">2017-01-02 - dpopov: DEVS FIXME More info required.</remark>
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><guimenu>z/VM Configuration: User profile for creating a z/VM userid</guimenu>
    </term>
    <listitem>
     <para>
      User profile to be used for creating a z/VM userid.
      <remark condition="clarity">2017-01-02 - dpopov: DEVS FIXME More info required.</remark>
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><guimenu>z/VM Configuration: Default zFCP SCSI Disk Pool</guimenu>
    </term>
    <listitem>
     <para>
      Default zFCP SCSI disk pool.
      <remark condition="clarity">2017-01-02 - dpopov: DEVS FIXME More info required.</remark>
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><guimenu>z/VM Configuration: The xCAT MN node name</guimenu>
    </term>
    <listitem>
     <para>
      Name of the xCAT Management Node.
      <remark condition="accuracy">2017-01-02 - dpopov: DEVS FIXME Check whether this is accurate.</remark>
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><guimenu>z/VM Configuration: The xCAT MN node public SSH key</guimenu>
    </term>
    <listitem>
     <para>
      Public SSH key of the xCAT Management Node.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>VMware vCenter Settings</term>
    <listitem>
     <para>
      Setting up VMware support is described in a separate section. See
      <xref linkend="app.deploy.vmware"/>.
     </para>
    </listitem>
   </varlistentry>
<!--
   <varlistentry>
    <term>z/VM Configuration</term>
    <listitem>
     <para>
      FIXME
     </para>
    </listitem>
   </varlistentry>
-->
   <varlistentry>
    <term>SSL Support: Protocol</term>
    <listitem>
     <para>
      Choose whether to encrypt public communication (<guimenu>HTTPS</guimenu>)
      or not (<guimenu>HTTP</guimenu>). If choosing
      <guimenu>HTTPS</guimenu>,refer to
      <xref linkend="sec.depl.ostack.keystone.ssl"/> for configuration details.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>VNC Settings: Keymap</term>
    <listitem>
     <para>
      Change the default VNC keymap for instances. By default,
      <literal>en-us</literal> is used. Enter the value in lowercase, either as
      a two character code (such as <literal>de</literal> or
      <literal>jp</literal>) or, as a five character code such as
      <literal>de-ch</literal> or <literal>en-uk</literal>, if applicable.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>VNC Settings: NoVNC Protocol</term>
    <listitem>
     <para>
      After having started an instance you can display its VNC console in the
      &ostack; &dash; (&o_dash;) via the browser using the noVNC
      implementation. By default this connection is not encrypted and can
      potentially be eavesdropped.
     </para>
     <para>
      Enable encrypted communication for noVNC by choosing
      <guimenu>HTTPS</guimenu> and providing the locations for the certificate
      key pair files.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><guimenu>Logging: Verbose Logging</guimenu>
    </term>
    <listitem>
     <para>
      Shows debugging output in the log files when set to
      <guimenu>true</guimenu>.
     </para>
    </listitem>
   </varlistentry>
  </variablelist>

  <note xml:id="note.custom.vendor">
   <title>Custom Vendor Data for Instances</title>
   <para>
    You can pass custom vendor data to all VMs via &o_comp;'s metadata server.
    For example, information about a custom SMT server can be used by the
    &suse; guest images to automatically configure the repositories for the
    guest.
   </para>
   <orderedlist>
    <listitem>
     <para>
      To pass custom vendor data, switch to the <guimenu>Raw</guimenu> view of
      the &o_comp; &barcl;.
     </para>
    </listitem>
    <listitem>
     <para>
      Search for the following section:
     </para>
<screen>"metadata": {
  "vendordata": {
    "json": "{}"
  }
}</screen>
    </listitem>
    <listitem>
     <para>
      As value of the <literal>json</literal> entry, enter valid JSON data. For
      example:
     </para>
<screen>"metadata": {
  "vendordata": {
    "json": "{\"<replaceable>CUSTOM_KEY</replaceable>\": \"<replaceable>CUSTOM_VALUE</replaceable>\"}"
  }
}</screen>
     <para>
      The string needs to be escaped because the &barcl; file is in JSON
      format, too.
     </para>
    </listitem>
   </orderedlist>
   <para>
    Use the following command to access the custom vendor data from inside a
    VM:
   </para>
<screen>curl -s http://<replaceable>METADATA_SERVER</replaceable>/openstack/latest/vendor_data.json</screen>
   <para>
    The IP address of the metadata server is always the same from within a VM.
    For more details, see
    <link
    xlink:href="https://www.suse.com/communities/blog/vms-get-access-metadata-neutron/"/>.
   </para>
  </note>

  <figure>
   <title>The &o_comp; &Barcl;</title>
   <mediaobject>
    <imageobject role="fo">
     <imagedata fileref="depl_barclamp_nova.png" width="100%" format="png"/>
    </imageobject>
    <imageobject role="html">
     <imagedata fileref="depl_barclamp_nova.png" width="75%" format="png"/>
    </imageobject>
   </mediaobject>
  </figure>

  <para>
   The &o_comp; component consists of eight different roles:
  </para>

  <variablelist>
   <varlistentry>
    <term><guimenu>nova-controller</guimenu>
    </term>
    <listitem>
     <para>
      Distributing and scheduling the &vmguest;s is managed by the
      <guimenu>nova-controller</guimenu>. It also provides networking and
      messaging services. <guimenu>nova-controller</guimenu> needs to be
      installed on a &contrnode;.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><guimenu>nova-compute-kvm</guimenu> /
    <guimenu>nova-compute-qemu</guimenu> /
    <guimenu>nova-compute-vmware</guimenu> /
    <guimenu>nova-compute-xen</guimenu> /
    <guimenu>nova-compute-zvm</guimenu>
    </term>
    <listitem>
     <para>
      Provides the hypervisors (&kvm;, QEMU, VMware vSphere, &xen;, and z/VM)
      and tools needed to manage the &vmguest;s. Only one hypervisor can be
      deployed on a single compute node. To use different hypervisors in your
      cloud, deploy different hypervisors to different &compnode;s. A
      <literal>nova-compute-*</literal> role needs to be installed on every
      &compnode;. However, not all hypervisors need to be deployed.
     </para>
     <para>
      Each image that will be made available in &cloud; to start an &vmguest;
      is bound to a hypervisor. Each hypervisor can be deployed on multiple
      &compnode;s (except for the VMWare vSphere role, see below). In a
      multi-hypervisor deployment you should make sure to deploy the
      <literal>nova-compute-*</literal> roles in a way, that enough compute
      power is available for each hypervisor.
     </para>
     <note>
      <title>Re-assigning Hypervisors</title>
      <para>
       <remark condition="clarity">
        2013-08-05 - fs: Is this true?
       </remark>
       Existing <literal>nova-compute-*</literal> nodes can be changed in a
       production &cloud; without service interruption. You need to
       <quote>evacuate</quote>
<!-- (see TODO) -->
       the node, re-assign a new <literal>nova-compute</literal> role via the
       &o_comp; &barcl; and <guimenu>Apply</guimenu> the change.
       <guimenu>nova-compute-vmware</guimenu> can only be deployed on a single
       node.
      </para>
     </note>
     <important>
      <title>Deploying VMware vSphere (vmware)</title>
      <para>
       <remark condition="clarity">
        2013-08-05 - fs: What network requirements/adjustments are needed ??
       </remark>
       VMware vSphere is not supported <quote>natively</quote> by
       &cloud;&mdash;it rather delegates requests to an existing vCenter. It
       requires preparations at the vCenter and post install adjustments of the
       &compnode;. See <xref linkend="app.deploy.vmware"/> for instructions.
       <guimenu>nova-compute-vmware</guimenu> can only be deployed on a single
       &compnode;.
      </para>
     </important>
    </listitem>
   </varlistentry>
  </variablelist>

  <figure>
   <title>The &o_comp; &Barcl;: Node Deployment Example with Two KVM Nodes</title>
   <mediaobject>
    <imageobject role="fo">
     <imagedata fileref="depl_barclamp_nova_node_deployment.png" width="100%" format="png"/>
    </imageobject>
    <imageobject role="html">
     <imagedata fileref="depl_barclamp_nova_node_deployment.png" width="75%" format="png"/>
    </imageobject>
   </mediaobject>
  </figure>

  <sect2 xml:id="sec.depl.ostack.nova.ha">
   <title>&haSetup; for &o_comp;</title>
   <para>
    Making <guimenu>nova-controller</guimenu> highly available requires no
    special configuration&mdash;it is sufficient to deploy it on a cluster.
   </para>
   <para>
    To enable &ha; for &compnode;s, deploy the following roles to one or more
    clusters with remote nodes:
   </para>
   <itemizedlist>
    <listitem>
     <para>
      nova-compute-kvm
     </para>
    </listitem>
    <listitem>
     <para>
      nova-compute-qemu
     </para>
    </listitem>
    <listitem>
     <para>
      nova-compute-xen
     </para>
    </listitem>
    <listitem>
     <para>
      ec2-api
     </para>
    </listitem>
   </itemizedlist>
   <para>
    The cluster to which you deploy the roles above can be completely
    independent of the one to which the role <literal>nova-controller</literal>
    is deployed.
   </para>
   <para>
    However, the <literal>nova-controller</literal> and
    <literal>ec2-api</literal> roles must be deployed the same way (either
    <emphasis>both</emphasis> to a cluster or <emphasis>both</emphasis>to
    individual nodes. This is due to &crow; design limitations.
   </para>
   <tip>
    <title>Shared Storage</title>
    <para>
     It is recommended to use shared storage for the
     <filename>/var/lib/nova/instances</filename> directory, to ensure that
     ephemeral disks will be preserved during recovery of VMs from failed
     compute nodes. Without shared storage, any ephemeral disks will be lost,
     and recovery will rebuild the VM from its original image.
    </para>
    <para>
     If an external NFS server is used, enable the following option in the
     &o_comp; &barcl; proposal: <guimenu>Shared Storage for Nova instances has
     been manually configured</guimenu>.
    </para>
   </tip>
  </sect2>
 </sect1>
 <sect1 xml:id="sec.depl.ostack.dash">
  <title>Deploying &o_dash; (&ostack; &dash;)</title>

  <para>
   The last component that needs to be deployed is &o_dash;, the &ostack;
   &dash;. It provides a Web interface for users to start and stop &vmguest;s
   and for administrators to manage users, groups, roles, etc. &o_dash; should
   be installed on a &contrnode;. To make &o_dash; highly available, deploy it
   on a cluster.
  </para>

  <para>
   The following attributes can be configured:
  </para>

  <variablelist>
   <varlistentry>
    <term>Session Timeout</term>
    <listitem>
     <para>
      Timeout (in minutes) after which a user is been logged out automatically.
      The default value is set to four hours (240 minutes).
     </para>
     <note>
      <title>Timeouts Larger than Four Hours</title>
      <para>
       Every &o_dash; session requires a valid &o_ident; token. These tokens
       also have a lifetime of four hours (14400 seconds). Setting the &o_dash;
       session timeout to a value larger than 240 will therefore have no
       effect, and you will receive a warning when applying the &barcl;.
      </para>
      <para>
       To successfully apply a timeout larger than four hours, you first need
       to adjust the &o_ident; token expiration accordingly. To do so, open the
       &o_ident; &barcl; in <guimenu>Raw</guimenu> mode and adjust the value of
       the key <literal>token_expiration</literal>. Note that the value has to
       be provided in <emphasis>seconds</emphasis>. When the change is
       successfully applied, you can adjust the &o_dash; session timeout (in
       <emphasis>minutes</emphasis>). Note that extending the &o_ident; token
       expiration may cause scalability issues in large and very busy &cloud;
       installations.
      </para>
     </note>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><guimenu>
      User Password Validation: Regular expression used for password
      validation
     </guimenu>
    </term>
    <listitem>
     <para>
      Specify a regular expression with which to check the password. The
      default expression (<literal>.{8,}</literal>) tests for a minimum length
      of 8 characters. The string you enter is interpreted as a Python regular
      expression (see
      <link xlink:href="http://docs.python.org/2.7/library/re.html#module-re"/>
      for a reference).
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><guimenu>
      User Password Validation: Text to display if the password does not pass
      validation
     </guimenu>
    </term>
    <listitem>
     <para>
      Error message that will be displayed in case the password validation
      fails.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>SSL Support: Protocol</term>
    <listitem>
     <para>
      Choose whether to encrypt public communication (<guimenu>HTTPS</guimenu>)
      or not (<guimenu>HTTP</guimenu>). If choosing <guimenu>HTTPS</guimenu>,
      you have two choices. You can either <guimenu>Generate (self-signed)
      certificates</guimenu> or provide the locations for the certificate key
      pair files and,&mdash;optionally&mdash; the certificate chain file. Using
      self-signed certificates is for testing purposes only and should never be
      used in production environments!
     </para>
    </listitem>
   </varlistentry>
  </variablelist>

  <figure>
   <title>The &o_dash; &Barcl;</title>
   <mediaobject>
    <imageobject role="fo">
     <imagedata fileref="depl_barclamp_nova_dashboard.png" width="100%" format="png"/>
    </imageobject>
    <imageobject role="html">
     <imagedata fileref="depl_barclamp_nova_dashboard.png" width="75%" format="png"/>
    </imageobject>
   </mediaobject>
  </figure>

  <sect2 xml:id="sec.depl.ostack.dash.ha">
   <title>&haSetup; for &o_dash;</title>
   <para>
    Making &o_dash; highly available requires no special configuration&mdash;it
    is sufficient to deploy it on a cluster.
   </para>
  </sect2>
 </sect1>
 <sect1 xml:id="sec.depl.ostack.heat">
  <title>Deploying &o_orch; (Optional)</title>

  <para>
   &o_orch; is a template-based orchestration engine that enables you to, for
   example, start workloads requiring multiple servers or to automatically
   restart &vmguest;s if needed. It also brings auto-scaling to &cloud; by
   automatically starting additional &vmguest;s if certain criteria are met.
   For more information about &o_orch; refer to the &ostack; documentation at
   <link xlink:href="http://docs.openstack.org/developer/heat/"/>.
  </para>

  <para>
   &o_orch; should be deployed on a &contrnode;. To make &o_orch; highly
   available, deploy it on a cluster.
  </para>

  <para>
   The following attributes can be configured for &o_orch;:
  </para>

  <variablelist>
   <varlistentry>
    <term><guimenu>Verbose Logging</guimenu>
    </term>
    <listitem>
     <para>
      Shows debugging output in the log files when set to
      <guimenu>true</guimenu>.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>SSL Support: Protocol</term>
    <listitem>
     <para>
      Choose whether to encrypt public communication (<guimenu>HTTPS</guimenu>)
      or not (<guimenu>HTTP</guimenu>). If choosing
      <guimenu>HTTPS</guimenu>,refer to
      <xref linkend="sec.depl.ostack.keystone.ssl"/> for configuration details.
     </para>
    </listitem>
   </varlistentry>
  </variablelist>

  <figure>
   <title>The &o_orch; &Barcl;</title>
   <mediaobject>
    <imageobject role="fo">
     <imagedata fileref="depl_barclamp_heat.png" width="100%" format="png"/>
    </imageobject>
    <imageobject role="html">
     <imagedata fileref="depl_barclamp_heat.png" width="75%" format="png"/>
    </imageobject>
   </mediaobject>
  </figure>

  <sect2 xml:id="sec.depl.ostack.heat.delegated_roles">
   <title>Enabling Identity Trusts Authorization (Optional)</title>
   <para>
    Heat uses Keystone Trusts to delegate a subset of user roles to the
    &o_orch; engine for deferred operations (see
    <link
    xlink:href="http://hardysteven.blogspot.de/2014/04/heat-auth-model-updates-part-1-trusts.html">Steve
    Hardy's blog</link> for details ). It can either delegate all user roles or
    only those specified in the <literal>trusts_delegated_roles</literal>
    setting. Consequently, all roles listed in
    <literal>trusts_delegated_roles</literal> need to be assigned to a user,
    otherwise the user will not be able to use &o_orch;.
   </para>
   <para>
    The recommended setting for <literal>trusts_delegated_roles</literal> is
    <literal>Member</literal>, since this is the default role most users are
    likely to have. This is also the default setting when installing &cloud;
    from scratch.
   </para>
   <para>
    On installations where this setting is introduced through an upgrade,
    <literal>trusts_delegated_roles</literal> will be set to
    <literal>heat_stack_owner</literal>. This is a conservative choice to
    prevent breakage in situations where unprivileged users may already have
    been assigned the <literal>heat_stack_owner</literal> role to enable them
    to use Heat but lack the <literal>Member</literal> role. As long as you can
    ensure that all users who have the <literal>heat_stack_owner</literal> role
    also have the <literal>Member</literal> role, it is both safe and
    recommended to change trusts_delegated_roles to <literal>Member</literal>,
    since the latter is the default role assigned by our hybrid LDAP back-end
    among others.
   </para>
   <para>
    To view or change the trusts_delegated_role setting you need to open the
    &o_orch; &barcl; and click <guimenu>Raw</guimenu> in the
    <guimenu>Attributes</guimenu> section. Search for the
    <literal>trusts_delegated_roles</literal> setting and modify the list of
    roles as desired.
   </para>
   <figure>
    <title>the &o_orch; &barcl;: Raw Mode</title>
    <mediaobject>
     <imageobject role="fo">
      <imagedata fileref="depl_barclamp_heat_raw.png" width="100%" format="png"/>
     </imageobject>
     <imageobject role="html">
      <imagedata fileref="depl_barclamp_heat_raw.png" width="75%" format="png"/>
     </imageobject>
    </mediaobject>
   </figure>
   <warning>
    <title>Empty Value</title>
    <para>
     An empty value for <literal>trusts_delegated_roles</literal> will delegate
     <emphasis>all</emphasis> of user roles to Heat. This may create a security
     risk for users who are assigned privileged roles, such as
     <literal>admin</literal>, because these privileged roles will also be
     delegated to the &o_orch; engine when these users create &o_orch; stacks.
    </para>
   </warning>
  </sect2>

  <sect2 xml:id="sec.depl.ostack.heat.ha">
   <title>&haSetup; for &o_orch;</title>
   <para>
    Making &o_orch; highly available requires no special configuration&mdash;it
    is sufficient to deploy it on a cluster.
   </para>
  </sect2>
 </sect1>
 <sect1 xml:id="sec.depl.ostack.ceilometer">
  <title>Deploying &o_meter; (Optional)</title>

  <para>
   <remark condition="clarity">
    2013-10-04 - fs: Which software/billing solution can make use of the
    ceilometer data?
   </remark>
   &o_meter; collects CPU and networking data from &cloud;. This data can be
   used by a billing system to enable customer billing. Deploying &o_meter; is
   optional.
  </para>

  <para>
   For more information about &o_meter; refer to the &ostack; documentation at
   <link xlink:href="http://docs.openstack.org/developer/ceilometer/"/>.
  </para>

  <important>
   <title>&o_meter; Restrictions</title>
   <para>
    As of &productname; &productnumber; data measuring is only supported for
    &kvm;, &xen; and Windows &vmguest;s. Other hypervisors and &cloud; features
    such as object or block storage will not be measured.
   </para>
  </important>

  <para>
   The following attributes can be configured for &o_meter;:
  </para>

  <variablelist>
   <varlistentry>
    <term>
     Interval used for CPU/disk/network/other meter updates (in seconds)
    </term>
    <listitem>
     <para>
      Specify an interval in seconds after which &o_meter; performs an update
      of the specified meter.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>Evaluation interval for threshold alarms (in seconds)</term>
    <listitem>
     <para>
      Set the interval after which to check whether to raise an alarm because a
      threshold has been exceeded. For performance reasons, do not set a value
      lower than the default (60s).
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>Use MongoDB instead of standard database
    </term>
    <listitem>
     <para>
      &o_meter; collects a large amount of data, which is written to a
      database. In a production system it is recommended to use a separate
      database for &o_meter; rather than the standard database that is also
      used by the other &cloud; components. MongoDB is optimized to write a lot
      of data. As of &productname; &productnumber;, MongoDB is only included as
      a technology preview and not supported.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>How long are metering/event samples kept in the database (in days)
    </term>
    <listitem>
     <para>
      Specify how long to keep the data. -1 means that samples are kept in the
      database forever.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>Verbose Logging
    </term>
    <listitem>
     <para>
      Shows debugging output in the log files when set to
      <guimenu>true</guimenu>.
     </para>
    </listitem>
   </varlistentry>
  </variablelist>

  <figure>
   <title>The &o_meter; &Barcl;</title>
   <mediaobject>
    <imageobject role="fo">
     <imagedata fileref="depl_barclamp_ceilometer.png" width="100%" format="png"/>
    </imageobject>
    <imageobject role="html">
     <imagedata fileref="depl_barclamp_ceilometer.png" width="75%" format="png"/>
    </imageobject>
   </mediaobject>
  </figure>

  <variablelist>
   <varlistentry xml:id="sec.depl.ostack.ceilometer.ssl">
    <term>SSL Support: Protocol
        </term>
    <listitem>
     <para>
      With the default value <guimenu>HTTP</guimenu> enabled, public
      communication is not be encrypted. Choose <guimenu>HTTPS</guimenu> to use
      SSL for encryption. See <xref linkend="sec.depl.req.ssl"/> for background
      information and <xref linkend="sec.depl.inst.nodes.post.ssl"/> for
      installation instructions. The following additional configuration options
      will become available when choosing <guimenu>HTTPS</guimenu>:
     </para>
     <variablelist>
      <varlistentry>
       <term><guimenu>Generate (self-signed) certificates</guimenu>
       </term>
       <listitem>
        <para>
         When set to <literal>true</literal>, self-signed certificates are
         automatically generated and copied to the correct locations. This
         setting is for testing purposes only and should never be used in
         production environments!
        </para>
       </listitem>
      </varlistentry>
      <varlistentry>
       <term><guimenu>SSL Certificate File</guimenu> / <guimenu>SSL (Private) Key
        File</guimenu>
       </term>
       <listitem>
        <para>
         Location of the certificate key pair files.
        </para>
       </listitem>
      </varlistentry>
      <varlistentry>
       <term><guimenu>SSL Certificate is insecure</guimenu>
       </term>
       <listitem>
        <para>
         Set this option to <literal>true</literal> when using self-signed
         certificates to disable certificate checks. This setting is for
         testing purposes only and should never be used in production
         environments!
        </para>
       </listitem>
      </varlistentry>
      <varlistentry>
       <term><guimenu>SSL CA Certificates File</guimenu>
       </term>
       <listitem>
        <para>
         Specify the absolute path to the CA certificate. This field is
         mandatory, and leaving it blank will cause the &barcl; to fail. To fix
         this issue, you have to provide the absolute path to the CA
         certificate, restart the <systemitem>apache2</systemitem> service, and
         re-deploy the &barcl;.
        </para>
        <para>
         When the certificate is not already trusted by the pre-installed list
         of trusted root certificate authorities, you need to provide a
         certificate bundle that includes the root and all intermediate CAs.
        </para>
       </listitem>
      </varlistentry>
     </variablelist>
    </listitem>
   </varlistentry>
  </variablelist>

  <para>
   The &o_meter; component consists of five different roles:
  </para>

  <variablelist>
   <varlistentry>
    <term><guimenu>ceilometer-server</guimenu>
    </term>
    <listitem>
     <para>
      The &o_meter; API server role. This role needs to be deployed on a
      &contrnode;. &o_meter; collects approximately 200 bytes of data per hour
      and &vmguest;. Unless you have a very huge number of &vmguest;s, there is
      no need to install it on a dedicated node.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><guimenu>ceilometer-polling</guimenu>
    </term>
    <listitem>
     <para>
      The polling agent listens to the message bus to collect data. It needs to
      be deployed on a &contrnode;. It can be deployed on the same node as
      <guimenu>ceilometer-server</guimenu>.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><guimenu>ceilometer-agent</guimenu>
    </term>
    <listitem>
     <para>
      The compute agents collect data from the compute nodes. They need to be
      deployed on all &kvm; and &xen; compute nodes in your cloud (other
      hypervisors are currently not supported).
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><guimenu>ceilometer-swift-proxy-middleware</guimenu>
    </term>
    <listitem>
     <para>
      An agent collecting data from the &swift; nodes. This role needs to be
      deployed on the same node as swift-proxy.
     </para>
    </listitem>
   </varlistentry>
  </variablelist>

  <figure>
   <title>The &o_meter; &Barcl;: Node Deployment</title>
   <mediaobject>
    <imageobject role="fo">
     <imagedata fileref="depl_barclamp_ceilometer_deployment.png" width="100%" format="png"/>
    </imageobject>
    <imageobject role="html">
     <imagedata fileref="depl_barclamp_ceilometer_deployment.png" width="75%" format="png"/>
    </imageobject>
   </mediaobject>
  </figure>

  <sect2 xml:id="sec.depl.ostack.ceilometer.ha">
   <title>&haSetup; for &o_meter;</title>
   <para>
    Making &o_meter; highly available requires no special
    configuration&mdash;it is sufficient to deploy the roles
    <guimenu>ceilometer-server</guimenu> and
    <guimenu>ceilometer-polling</guimenu> on a cluster. If you are using
    MongoDB for your back-end the cluster must have a minimum of three nodes.
    If you are using MySQL or PostgreSQL, you may use two nodes. (MongoDB is
    the recommended back-end for best performance.)
   </para>
  </sect2>
 </sect1>
 <sect1 xml:id="sec.depl.ostack.aodh">
  <title>Deploying &aodh;</title>

  <para>
   &aodh; is part of OpenStack Telemetry services. &aodh; enables the ability
   to provide alarms and notifications based on metrics collected by
   Ceilometer. You can configure the following parameters of the &aodh;
   &barcl;:
  </para>

  <variablelist>
   <varlistentry>
    <term><guimenu>Evaluation interval for threshold alarms (in seconds)</guimenu>
    </term>
    <listitem>
     <para>
      Period of evaluation cycle specified as an integer. This value should be
      equal or greater than the configured pipeline interval for collection of
      underlying meters.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>SSL Support: Protocol
    </term>
    <listitem>
     <para>
      Choose <guimenu>HTTPS</guimenu> to use SSL for encryption. See
      <xref linkend="sec.depl.req.ssl"/> for background information and
      <xref linkend="sec.depl.inst.nodes.post.ssl"/> for installation
      instructions. The following additional configuration options will become
      available when choosing <guimenu>HTTPS</guimenu>:
     </para>
     <variablelist>
      <varlistentry>
       <term><guimenu>Generate (self-signed) certificates (implies insecure)</guimenu>
       </term>
       <listitem>
        <para>
         When set to <literal>true</literal>, self-signed certificates are
         automatically generated and copied to the correct locations. This
         setting is for testing purposes only and should never be used in
         production environments!
        </para>
       </listitem>
      </varlistentry>
      <varlistentry>
       <term><guimenu>SSL Certificate File</guimenu> / <guimenu>SSL (Private) Key
        File</guimenu>
       </term>
       <listitem>
        <para>
         Location of the certificate key pair files.
        </para>
       </listitem>
      </varlistentry>
      <varlistentry>
       <term><guimenu>SSL Certificate is insecure (for instance, self-signed)</guimenu>
       </term>
       <listitem>
        <para>
         Set this option to <literal>true</literal> when using self-signed
         certificates to disable certificate checks. This setting is for
         testing purposes only and should never be used in production
         environments!
        </para>
       </listitem>
      </varlistentry>
      <varlistentry>
       <term><guimenu>Require Client Certificate</guimenu>
       </term>
       <listitem>
        <para>
         Set this option to <literal>true</literal> when using your own
         certificate authority (CA) for signing. Having done so, you also need
         to specify a path to the <guimenu>CA Certificates File</guimenu>. If
         your certificates are signed by a trusted third party organization,
         set <guimenu>Require Client Certificate</guimenu> to
         <guimenu>false</guimenu>, since the <quote>official</quote>
         certificate authorities (CA) are already known by the system.
        </para>
       </listitem>
      </varlistentry>
      <varlistentry>
       <term><guimenu>SSL CA Certificates File</guimenu>
       </term>
       <listitem>
        <para>
         Specify the absolute path to the CA certificate here. This option can
         only be changed if <guimenu>Require Client Certificate</guimenu> was
         set to <literal>true</literal>.
        </para>
        <figure>
         <title>The SSL Dialog</title>
         <mediaobject>
          <imageobject role="fo">
           <imagedata fileref="depl_barclamp_aodh_ssl.png" width="100%" format="png"/>
          </imageobject>
          <imageobject role="html">
           <imagedata fileref="depl_barclamp_aodh_ssl.png" width="75%" format="png"/>
          </imageobject>
         </mediaobject>
        </figure>
       </listitem>
      </varlistentry>
      <varlistentry>
       <term><guimenu>Logging: Verbose Logging</guimenu>
       </term>
       <listitem>
        <para>
         Shows debugging output in the log files when set to
         <guimenu>true</guimenu>.
        </para>
       </listitem>
      </varlistentry>
     </variablelist>
    </listitem>
   </varlistentry>
  </variablelist>
 </sect1>
 <sect1 xml:id="sec.depl.ostack.manila">
  <title>Deploying &o_sharefs;</title>

  <para>
   &o_sharefs; provides coordinated access to shared or distributed file
   systems, similar to what &o_blockstore; does for block storage. These file
   systems can be shared between &vmguest;s in &cloud;.
  </para>

  <para>
   &o_sharefs; uses different back-ends. As of &productname; &productnumber;
   currently supported back-ends include <guimenu>Hitachi HNAS</guimenu>,
   <guimenu>NetApp Driver</guimenu>, and <guimenu>CephFS</guimenu>. Two more
   back-end options, <guimenu>Generic Driver</guimenu> and <guimenu>Other
   Driver</guimenu> are available for testing purposes and are not supported.
  </para>

  <note xml:id="note.limit.cephfs">
   <title>Limitations for CephFS Back-end</title>
   <para>
    &o_sharefs; uses some CephFS features that are currently
    <emphasis>not</emphasis> supported by the &sle; 12 SP2 CephFS kernel
    client:
   </para>
   <itemizedlist>
    <listitem>
     <para>
      RADOS namespaces
     </para>
    </listitem>
    <listitem>
     <para>
      MDS path restrictions
     </para>
    </listitem>
    <listitem>
     <para>
      Quotas
     </para>
    </listitem>
   </itemizedlist>
   <para>
    As a result, to access CephFS shares provisioned by &o_sharefs;, you must
    use ceph-fuse. For details, see
    <link
    xlink:href="http://docs.openstack.org/developer/manila/devref/cephfs_native_driver.html"/>.
   </para>
  </note>

  <para>
   When first opening the &o_sharefs; &barcl;, the default proposal
   <guimenu>Generic Driver</guimenu> is already available for configuration. To
   replace it, first delete it by clicking the trashcan icon and then choose a
   different back-end in the section <guimenu>Add new Manila Backend</guimenu>.
   Select a <guimenu>Type of Share</guimenu> and&mdash;optionally&mdash;provide
   a <guimenu>Name for Backend</guimenu>. Activate the back-end with
   <guimenu>Add Backend</guimenu>. Note that at least one back-end must be
   configured.
  </para>

  <para>
   The attributes that can be set to configure Cinder depend on the back-end:
  </para>

  <bridgehead renderas="sect2"><guimenu>Back-end: Generic</guimenu>
  </bridgehead>

  <para>
   The generic driver is included as a technology preview and is not supported.
  </para>

  <bridgehead renderas="sect2"><guimenu>Hitachi HNAS</guimenu>
  </bridgehead>

  <variablelist>
   <varlistentry>
    <term><guimenu>Specify which EVS this backend is assigned to</guimenu>
    </term>
    <listitem>
     <para>
      Provide the name of the Enterprise Virtual Server that the selected
      back-end is assigned to.
      <remark condition="accuracy">2017-01-03 - dpopov: DEVS FIXME Please check whether this is correct.</remark>
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><guimenu>Specify IP for mounting shares</guimenu>
    </term>
    <listitem>
     <para>
      IP address for mounting shares.
      <remark condition="info">2017-01-03 - dpopov: DEVS FIXME More info needed.</remark>
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><guimenu>Specify file-system name for creating shares</guimenu>
    </term>
    <listitem>
     <para>
      Provide a file-system name for creating shares.
      <remark condition="info">2017-01-03 - dpopov: DEVS FIXME More info needed.</remark>
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><guimenu>HNAS management interface IP</guimenu>
    </term>
    <listitem>
     <para>
      IP address of the HNAS management interface for communication between
      Manila controller and HNAS.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><guimenu>HNAS username Base64 String</guimenu>
    </term>
    <listitem>
     <para>
      HNAS username Base64 String required to perform tasks like creating
      file-systems and network interfaces.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><guimenu>HNAS user password</guimenu>
    </term>
    <listitem>
     <para>
      HNAS user password. Required only if private key is not provided.
      <remark condition="accuracy">2017-01-03 - dpopov: DEVS FIXME Please check whether this is correct.</remark>
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><guimenu>RSA/DSA private key</guimenu>
    </term>
    <listitem>
     <para>
      RSA/DSA private key necessary for connecting to HNAS. Required only if
      password is not provided.
      <remark condition="accuracy">2017-01-03 - dpopov: DEVS FIXME Please check whether this is correct.</remark>
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><guimenu>The time to wait for stalled HNAS jobs before aborting</guimenu>
    </term>
    <listitem>
     <para>
      Time in seconds to wait before aborting stalled HNAS jobs.
     </para>
    </listitem>
   </varlistentry>
  </variablelist>

  <bridgehead renderas="sect2"><guimenu>Back-end: Netapp</guimenu>
  </bridgehead>

  <variablelist>
   <varlistentry>
    <term><guimenu>Name of the Virtual Storage Server (vserver)</guimenu>
    </term>
    <listitem>
     <para>
      Host name of the Virtual Storage Server.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><guimenu>Server Host Name</guimenu>
    </term>
    <listitem>
     <para>
      The name or IP address for the storage controller or the cluster.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><guimenu>Server Port</guimenu>
    </term>
    <listitem>
     <para>
      The port to use for communication. Port 80 is usually used for HTTP, 443
      for HTTPS.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><guimenu>User name/Password for Accessing NetApp</guimenu>
    </term>
    <listitem>
     <para>
      Login credentials.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><guimenu>Transport Type</guimenu>
    </term>
    <listitem>
     <para>
      Transport protocol for communicating with the storage controller or
      cluster. Supported protocols are HTTP and HTTPS. Choose the protocol your
      NetApp is licensed for.
     </para>
    </listitem>
   </varlistentry>
  </variablelist>

  <bridgehead renderas="sect2"><guimenu>Back-end: CephFS</guimenu>
  </bridgehead>

  <variablelist>
   <varlistentry>
    <term>Use Ceph deployed by Crowbar</term>
    <listitem>
     <para>
      Set to <systemitem>true</systemitem> to use Ceph deployed with Crowbar.
     </para>
    </listitem>
   </varlistentry>
  </variablelist>

  <bridgehead renderas="sect2"><guimenu>Back-end: Manual</guimenu>
  </bridgehead>

  <para>
   Lets you manually pick and configure a driver. Only use this option for
   testing purposes, it is not supported.
  </para>

  <figure>
   <title>The &o_sharefs; &Barcl;</title>
   <mediaobject>
    <imageobject role="fo">
     <imagedata fileref="depl_barclamp_manila.png" width="100%" format="png"/>
    </imageobject>
    <imageobject role="html">
     <imagedata fileref="depl_barclamp_manila.png" width="75%" format="png"/>
    </imageobject>
   </mediaobject>
  </figure>

  <para>
   The &o_sharefs; component consists of two different roles:
  </para>

  <variablelist>
   <varlistentry>
    <term><guimenu>manila-server</guimenu>
    </term>
    <listitem>
     <para>
      The &o_sharefs; server provides the scheduler and the API. Installing it
      on a &contrnode; is recommended.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><guimenu>manila-share</guimenu>
    </term>
    <listitem>
     <para>
      The shared storage service. It can be installed on a &contrnode;, but it
      is recommended to deploy it on one or more dedicated nodes supplied with
      sufficient disk space and networking capacity, since it will generate a
      lot of network traffic.
     </para>
    </listitem>
   </varlistentry>
  </variablelist>

  <figure>
   <title>The &o_sharefs; &Barcl;: Node Deployment Example</title>
   <mediaobject>
    <imageobject role="fo">
     <imagedata fileref="depl_barclamp_manila_node_deployment.png" width="100%" format="png"/>
    </imageobject>
    <imageobject role="html">
     <imagedata fileref="depl_barclamp_manila_node_deployment.png" width="75%" format="png"/>
    </imageobject>
   </mediaobject>
  </figure>

  <sect2 xml:id="sec.depl.ostack.manila.ha">
   <title>&haSetup; for &o_sharefs;</title>
   <para>
    While the <guimenu>manila-server</guimenu> role can be deployed on a
    cluster, deploying <guimenu>manila-share</guimenu> on a cluster is not
    supported. Therefore it is generally recommended to deploy
    <guimenu>manila-share</guimenu> on several nodes&mdash;this ensures the
    service continues to be available even when a node fails.
   </para>
  </sect2>
 </sect1>
 <sect1 xml:id="sec.depl.ostack.trove">
  <title>Deploying &o_dbaas; (Optional)</title>

  <para>
   &o_dbaas; is a Database-as-a-Service for &cloud;. It provides database
   instances which can be used by all &vmguest;s. With &o_dbaas; being
   deployed, &cloud; users no longer need to deploy and maintain their own
   database applications. For more information about &o_dbaas;; refer to the
   &ostack; documentation at
   <link xlink:href="http://docs.openstack.org/developer/trove/"/>.
  </para>

  <important>
   <title>Technology Preview</title>
   <para>
    &o_dbaas; is only included as a technology preview and not supported.
   </para>
  </important>

  <para>
   &o_dbaas; should be deployed on a dedicated &contrnode;.
  </para>

  <para>
   The following attributes can be configured for &o_dbaas;:
  </para>

  <variablelist>
   <varlistentry>
    <term><guimenu>Enable Trove Volume Support</guimenu>
    </term>
    <listitem>
     <para>
      When enabled, &o_dbaas; will use a &o_blockstore; volume to store the
      data.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><guimenu>Logging: Verbose</guimenu>
    </term>
    <listitem>
     <para>
      Increases the amount of information that is written to the log files when
      set to <guimenu>true</guimenu>.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><guimenu>Logging: Debug</guimenu>
    </term>
    <listitem>
     <para>
      Shows debugging output in the log files when set to
      <guimenu>true</guimenu>.
     </para>
    </listitem>
   </varlistentry>
  </variablelist>

  <figure>
   <title>The &o_dbaas; &Barcl;</title>
   <mediaobject>
    <imageobject role="fo">
     <imagedata fileref="depl_barclamp_trove.png" width="100%" format="png"/>
    </imageobject>
    <imageobject role="html">
     <imagedata fileref="depl_barclamp_trove.png" width="75%" format="png"/>
    </imageobject>
   </mediaobject>
  </figure>

  <sect2 xml:id="sec.depl.ostack.trove.ha">
   <title>&haSetup; for &o_dbaas;</title>
   <para>
    An &haSetup; for &o_dbaas; is currently not supported.
   </para>
  </sect2>
 </sect1>
 <sect1 xml:id="sec.depl.ostack.tempest">
  <title>Deploying &o_testsuite; (Optional)</title>

  <para>
   &o_testsuite; is an integration test suite for &cloud; written in Python. It
   contains multiple integration tests for validating your &cloud; deployment.
   For more information about &o_testsuite; refer to the &ostack; documentation
   at <link
   xlink:href="http://docs.openstack.org/developer/tempest/"/>.
  </para>

  <important>
   <title>Technology Preview</title>
   <para>
    &o_testsuite; is only included as a technology preview and not supported.
   </para>
   <para>
    &o_testsuite; may be used for testing whether the intended setup will run
    without problems. It should not be used in a production environment.
   </para>
  </important>

  <para>
   &o_testsuite; should be deployed on a &contrnode;.
  </para>

  <para>
   The following attributes can be configured for &o_testsuite;:
  </para>

  <variablelist>
   <varlistentry>
    <term><guimenu>Choose User name / Password</guimenu>
    </term>
    <listitem>
     <para>
      Credentials for a regular user. If the user does not exist, it will be
      created.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><guimenu>Choose Tenant</guimenu>
    </term>
    <listitem>
     <para>
      Tenant to be used by &o_testsuite;. If it does not exist, it will be
      created. It is safe to stick with the default value.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><guimenu>Choose Tempest Admin User name/Password</guimenu>
    </term>
    <listitem>
     <para>
      Credentials for an admin user. If the user does not exist, it will be
      created.
     </para>
    </listitem>
   </varlistentry>
  </variablelist>

  <figure>
   <title>The &o_testsuite; &Barcl;</title>
   <mediaobject>
    <imageobject role="fo">
     <imagedata fileref="depl_barclamp_tempest.png" width="100%" format="png"/>
    </imageobject>
    <imageobject role="html">
     <imagedata fileref="depl_barclamp_tempest.png" width="75%" format="png"/>
    </imageobject>
   </mediaobject>
  </figure>

  <tip>
   <title>Running Tests</title>
   <para>
    To run tests with &o_testsuite;, log in to the &contrnode; on which
    &o_testsuite; was deployed. Change into the directory
    <filename>/var/lib/openstack-tempest-test</filename>. To get an overview of
    available commands, run:
   </para>
<screen>./run_tempest.sh --help</screen>
   <para>
    To serially invoke a subset of all tests (<quote>the gating
    smoketests</quote>) to help validate the working functionality of your
    local cloud instance, run the following command. It will save the output to
    a log file
    <filename>tempest_<replaceable>CURRENT_DATE</replaceable>.log</filename>.
   </para>
<screen>./run_tempest.sh --no-virtual-env -serial --smoke 2>&amp;1 \
| tee "tempest_$(date +%Y-%m-%d_%H%M%S).log"</screen>
  </tip>

  <sect2 xml:id="sec.depl.ostack.tempest.ha">
   <title>&haSetup; for &o_testsuite;</title>
   <para>
    &o_testsuite; cannot be made highly available.
   </para>
  </sect2>
 </sect1>
 <sect1 xml:id="sec.depl.ostack.magnum">
  <title>Deploying &o_container; (Optional)</title>

  <para>
   &o_container; is an &ostack; project which offers container orchestration
   engines for deploying and managing containers as first class resources in
   &ostack;.
  </para>

  <para>
   For more information about &o_container;, see the &ostack; documentation at
   <link xlink:href="http://docs.openstack.org/developer/magnum/"/>.
  </para>

  <para>
   For information on how to deploy a Kubernetes cluster (either from command
   line or from the &o_dash; &dash;), see the &cloudsuppl;. It is available
   from <link xlink:href="https://www.suse.com/documentation/cloud"></link>.
  </para>

  <para>
   The following <guimenu>Attributes</guimenu> can be configured for
   &o_container;:
  </para>

  <variablelist>
   <varlistentry>
    <term><guimenu>Logging</guimenu>: <guimenu>Verbose</guimenu>
    </term>
    <listitem>
     <para>
      Increases the amount of information that is written to the log files when
      set to <guimenu>true</guimenu>.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><guimenu>Logging</guimenu>: <guimenu>Debug</guimenu>
    </term>
    <listitem>
     <para>
      Shows debugging output in the log files when set to
      <guimenu>true</guimenu>.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><guimenu>Trustee Domain</guimenu>: <guimenu>Domain Name</guimenu>
    </term>
    <listitem>
     <para>
      Domain name to use for creating trustee for bays.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><guimenu>Certificate Manager</guimenu>: <guimenu>Plugin</guimenu>
    </term>
    <listitem>
     <para>
      To store certificates, either use the <guimenu>Barbican</guimenu>
      &ostack; service, a local directory (<guimenu>Local</guimenu>), or the
      <guimenu>Magnum Database (x590keypair)</guimenu>.
     </para>
     <note>
      <title>&secret_store; As Certificate Manager</title>
      <para>
       If you choose to use &secret_store; for managing certificates, make sure
       that the &secret_store; &barcl; is enabled.
      </para>
     </note>
    </listitem>
   </varlistentry>
  </variablelist>

  <figure>
   <title>The &o_container; &Barcl;</title>
   <mediaobject>
    <imageobject role="fo">
     <imagedata fileref="depl_barclamp_magnum_attributes.png" width="100%" format="png"/>
    </imageobject>
    <imageobject role="html">
     <imagedata fileref="depl_barclamp_magnum_attributes.png" width="75%" format="png"/>
    </imageobject>
   </mediaobject>
  </figure>

  <para>
   The &o_container; &barcl; consists of the following roles:
   <guimenu>magnum-server</guimenu>. It can either be deployed on a &contrnode;
   or on a cluster&mdash;see <xref linkend="sec.depl.ostack.magnum.ha"/>. When
   deploying the role onto a &contrnode;, additional RAM is required for the
   &o_container; server. It is recommended to only deploy the role to a
   &contrnode; that has 16 GB RAM.
  </para>

  <sect2 xml:id="sec.depl.ostack.magnum.ha">
   <title>&haSetup; for &o_container;</title>
   <para>
    Making &o_container; highly available requires no special configuration. It
    is sufficient to deploy it on a cluster.
   </para>
  </sect2>
 </sect1>
 <sect1 xml:id="sec.depl.ostack.barbican">
  <title>Deploying &secret_store; (Optional)</title>

  <para>
   &secret_store; is a component designed for storing secrets in a secure and
   standardized manner protected by &o_ident; authentication. Secrets include
   SSL certificates and passwords used by various &ostack; components.
  </para>

  <para>
   &secret_store; settings can be configured in <literal>Raw</literal> mode
   only. To do this, open the &secret_store; &barcl; <guimenu>Attribute
   </guimenu>configuration in <guimenu>Raw</guimenu> mode.
  </para>

  <figure>
   <title>The &secret_store; &Barcl;: Raw Mode</title>
   <mediaobject>
    <imageobject role="fo">
     <imagedata fileref="depl_barclamp_barbican_raw.png" width="100%" format="png"/>
    </imageobject>
    <imageobject role="html">
     <imagedata fileref="depl_barclamp_barbican_raw.png" width="75%" format="png"/>
    </imageobject>
   </mediaobject>
  </figure>

  <para>
   When configuring &secret_store;, pay particular attention to the following
   settings:
  </para>

  <itemizedlist>
   <listitem>
    <para>
     <literal>bind_host</literal> Bind host for the &secret_store; API service
    </para>
   </listitem>
   <listitem>
    <para>
     <literal>bind_port</literal> Bind port for the &secret_store; API service
    </para>
   </listitem>
   <listitem>
    <para>
     <literal>processes</literal> Number of API processes to run in Apache
    </para>
   </listitem>
   <listitem>
    <para>
     <literal>ssl</literal> Enable or disable SSL
    </para>
   </listitem>
   <listitem>
    <para>
     <literal>threads</literal> Number of API worker threads
    </para>
   </listitem>
   <listitem>
    <para>
     <literal>debug</literal> Enable or disable debug logging
    </para>
   </listitem>
   <listitem>
    <para>
     <literal>enable_keystone_listener</literal> Enable or disable the
     &o_ident; listener services
    </para>
   </listitem>
   <listitem>
    <para>
     <literal>kek</literal> An encryption key (fixed-length 32-byte
     Base64-encoded value) for &secret_store;'s
     <systemitem>simple_crypto</systemitem> plugin. If left unspecified, the
     key will be generated automatically.
    </para>
    <note>
     <title>Existing Encryption Key</title>
     <para>
      If you plan to restore and use the existing &secret_store; database after
      a full reinstall (including a complete wipe of the Crowbar node), make
      sure to save the specified encryption key beforehand. You will need to
      provide it after the full reinstall in order to access the data in the
      restored &secret_store; database.
     </para>
    </note>
   </listitem>
  </itemizedlist>

  <variablelist>
   <varlistentry xml:id="sec.depl.ostack.barbican.ssl">
    <term>SSL Support: Protocol
    </term>
    <listitem>
     <para>
      With the default value <guimenu>HTTP</guimenu>, public communication will
      not be encrypted. Choose <guimenu>HTTPS</guimenu> to use SSL for
      encryption. See <xref linkend="sec.depl.req.ssl"/> for background
      information and <xref linkend="sec.depl.inst.nodes.post.ssl"/> for
      installation instructions. The following additional configuration options
      will become available when choosing <guimenu>HTTPS</guimenu>:
     </para>
     <variablelist>
      <varlistentry>
       <term><guimenu>Generate (self-signed) certificates</guimenu>
       </term>
       <listitem>
        <para>
         When set to <literal>true</literal>, self-signed certificates are
         automatically generated and copied to the correct locations. This
         setting is for testing purposes only and should never be used in
         production environments!
        </para>
       </listitem>
      </varlistentry>
      <varlistentry>
       <term><guimenu>SSL Certificate File</guimenu> / <guimenu>SSL (Private) Key
        File</guimenu>
       </term>
       <listitem>
        <para>
         Location of the certificate key pair files.
        </para>
       </listitem>
      </varlistentry>
      <varlistentry>
       <term><guimenu>SSL Certificate is insecure</guimenu>
       </term>
       <listitem>
        <para>
         Set this option to <literal>true</literal> when using self-signed
         certificates to disable certificate checks. This setting is for
         testing purposes only and should never be used in production
         environments!
        </para>
       </listitem>
      </varlistentry>
      <varlistentry>
       <term><guimenu>SSL CA Certificates File</guimenu>
       </term>
       <listitem>
        <para>
         Specify the absolute path to the CA certificate. This field is
         mandatory, and leaving it blank will cause the &barcl; to fail. To fix
         this issue, you have to provide the absolute path to the CA
         certificate, restart the <systemitem>apache2</systemitem> service, and
         re-deploy the &barcl;.
        </para>
        <para>
         When the certificate is not already trusted by the pre-installed list
         of trusted root certificate authorities, you need to provide a
         certificate bundle that includes the root and all intermediate CAs.
        </para>
        <figure>
         <title>The SSL Dialog</title>
         <mediaobject>
          <imageobject role="fo">
           <imagedata fileref="depl_barbican_ssl.png" width="100%" format="png"/>
          </imageobject>
          <imageobject role="html">
           <imagedata fileref="depl_barbican_ssl.png" width="75%" format="png"/>
          </imageobject>
         </mediaobject>
        </figure>
       </listitem>
      </varlistentry>
     </variablelist>
    </listitem>
   </varlistentry>
  </variablelist>

  <sect2 xml:id="sec.depl.ostack.barbican.ha">
   <title>&haSetup; for &secret_store;</title>
   <para>
    To make &secret_store; highly available, assign the
    <guimenu>barbican-controller</guimenu> role to the Controller Cluster.
   </para>
  </sect2>
 </sect1>
 <sect1 xml:id="sec.depl.ostack.sahara">
  <title>Deploying &dataproc;</title>

  <para>
   &dataproc; provides users with simple means to provision data processing
   frameworks (such as Hadoop, Spark, and Storm) on OpenStack. This is
   accomplished by specifying configuration parameters such as the framework
   version, cluster topology, node hardware details, etc.
  </para>

  <variablelist>
   <varlistentry>
    <term>Logging: Verbose</term>
    <listitem>
     <para>
      Set to <systemitem>true</systemitem> to increase the amount of
      information written to the log files.
     </para>
    </listitem>
   </varlistentry>
  </variablelist>

  <figure>
   <title>The &dataproc; Barclamp</title>
   <mediaobject>
    <imageobject role="fo">
     <imagedata fileref="depl_barclamp_sahara.png" width="100%" format="png"/>
    </imageobject>
    <imageobject role="html">
     <imagedata fileref="depl_barclamp_sahara.png" width="75%" format="png"/>
    </imageobject>
   </mediaobject>
  </figure>

  <sect2 xml:id="sec.depl.ostack.sahara.ha">
   <title>&haSetup; for &dataproc;</title>
   <para>
    Making &dataproc; highly available requires no special configuration. It is
    sufficient to deploy it on a cluster.
   </para>
  </sect2>
 </sect1>
 <sect1 xml:id="sec.depl.ostack.monasca">
  <title>Deploying &o_monitor;</title>

  <para>
   &o_monitor; is an open-source monitoring-as-a-service solution that
   integrates with OpenStack. &o_monitor; is designed for scalability, high
   performance, and fault tolerance.
  </para>

  <para>
   Accessing the <guimenu>Raw</guimenu> interface is not required for
   day-to-day operation. But as not all &o_monitor; settings are exposed in the
   &barcl; graphical interface (for example, various performance turnables), it
   is recommended to configure &o_monitor; in the the <guimenu>Raw</guimenu>
   mode. Below are the options that can be configured via the
   <guimenu>Raw</guimenu> interface of the &o_monitor; &barcl;.
  </para>

  <figure>
   <title>The &o_monitor; &barcl; Raw Mode</title>
   <mediaobject>
    <imageobject role="fo">
     <imagedata fileref="depl_barclamp_monasca_raw.png" width="100%" format="png"/>
    </imageobject>
    <imageobject role="html">
     <imagedata fileref="depl_barclamp_monasca_raw.png" width="75%" format="png"/>
    </imageobject>
   </mediaobject>
  </figure>

  <bridgehead renderas="sect3"><guimenu>agent: settings for openstack-monasca-agent</guimenu>
  </bridgehead>

  <variablelist>
   <varlistentry>
    <term>keystone</term>
    <listitem>
     <para>
      Contains &o_ident; credentials that the agents use to send metrics. Do
      not change these options, as they are configured by &crow;.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>insecure</term>
    <listitem>
     <para>
      Specifies whether SSL certificates are verified when communicating with
      &o_ident;. If set to <literal>false</literal>, the
      <literal>ca_file</literal> option must be specified.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>ca_file</term>
    <listitem>
     <para>
      Specifies the location of a CA certificate that is used for verifying
      &o_ident;'s SSL certificate.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>log_dir</term>
    <listitem>
     <para>
      Path for storing log files. The specified path must exist. Do not change
      the default <filename>/var/log/monasca-agent</filename> path.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>log_level</term>
    <listitem>
     <para>
      Agent's log level. Limits log messages to the specified level and above.
      The following levels are available: Error, Warning, Info (default), and
      Debug.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>check_frequency</term>
    <listitem>
     <para>
      Interval in seconds between running agents' checks.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>num_collector_threads</term>
    <listitem>
     <para>
      Number of simultaneous collector threads to run. This refers to the
      maximum number of different collector plug-ins (for example,
      <literal>http_check</literal>) that are allowed run simultaneously. The
      default value <literal>1</literal> means that plug-ins are run
      sequentially.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>pool_full_max_retries</term>
    <listitem>
     <para>
      If a problem with the results from multiple plug-ins results blocks the
      entire thread pool (as specified by the
      <systemitem>num_collector_threads</systemitem> parameter), the collector
      exits, so it can be restarted by the
      <systemitem>supervisord</systemitem>. The parameter
      <systemitem>pool_full_max_retries</systemitem> specifies when this event
      occurs. The collector exits when the defined number of consecutive
      collection cycles have ended with the thread pool completely full.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>plugin_collect_time_warn</term>
    <listitem>
     <para>
      Upper limit in seconds for any collection plug-in's run time. A warning
      is logged if a plug-in runs longer than the specified limit.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>max_measurement_buffer_size</term>
    <listitem>
     <para>
      Maximum number of measurements to buffer locally if the &o_monitor; API
      is unreachable. Measurements will be dropped in batches, if the API is
      still unreachable after the specified number of messages are buffered.
      The default <literal>-1</literal> value indicates unlimited buffering.
      Note that a large buffer increases the agent's memory usage.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>backlog_send_rate</term>
    <listitem>
     <para>
      Maximum number of measurements to send when the local measurement buffer
      is flushed.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>amplifier</term>
    <listitem>
     <para>
      Number of extra dimensions to add to metrics sent to the &o_monitor; API.
      This option is intended for load testing purposes only. Do not enable the
      option in production! The default <literal>0</literal> value disables the
      addition of dimensions.
     </para>
    </listitem>
   </varlistentry>
  </variablelist>

  <bridgehead renderas="sect3"><guimenu>log_agent: settings for openstack-monasca-log-agent</guimenu>
  </bridgehead>

  <variablelist>
   <varlistentry>
    <term>max_data_size_kb</term>
    <listitem>
     <para>
      Maximum payload size in kilobytes for a request sent to the &o_monitor;
      log API.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>num_of_logs</term>
    <listitem>
     <para>
      Maximum number of log entries the log agent sends to the &o_monitor; log
      API in a single request. Reducing the number increases performance.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>elapsed_time_sec</term>
    <listitem>
     <para>
      Time interval in seconds between sending logs to the &o_monitor; log API.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>delay</term>
    <listitem>
     <para>
      Interval in seconds for checking whether
      <literal>elapsed_time_sec</literal> has been reached.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>keystone</term>
    <listitem>
     <para>
      &o_ident; credentials the log agents use to send logs to the &o_monitor;
      log API. Do not change this option manually, as it is configured by
      &crow;.
     </para>
    </listitem>
   </varlistentry>
  </variablelist>

  <bridgehead renderas="sect3"><guimenu>api: Settings for openstack-monasca-api</guimenu>
  </bridgehead>

  <variablelist>
   <varlistentry>
    <term>bind_host</term>
    <listitem>
     <para>
      Interfaces <literal>monasca-api</literal> listens on. Do not change this
      option, as it is configured by &crow;.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>processes</term>
    <listitem>
     <para>
      Number of processes to spawn.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>threads</term>
    <listitem>
     <para>
      Number of WSGI worker threads to spawn.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>log_level</term>
    <listitem>
     <para>
      Log level for <systemitem>openstack-monasca-api</systemitem>. Limits log
      messages to the specified level and above. The following levels are
      available: Critical, Error, Warning, Info (default), Debug, and Trace.
     </para>
    </listitem>
   </varlistentry>
  </variablelist>

  <bridgehead renderas="sect3"><guimenu>elasticsearch: server-side settings for elasticsearch</guimenu>
  </bridgehead>

  <variablelist>
   <varlistentry>
    <term>repo_dir</term>
    <listitem>
     <para>
      List of directories for storing elasticsearch snapshots. Must be created
      manually and be writeable by the
      <systemitem
          class="username">elasticsearch</systemitem> user.
      Must contain at least one entry in order for the snapshot functionality
      to work.
     </para>
     <para>
      For instructions on creating an elasticsearch snapshot, see the
      &productname; Monitoring <citetitle>&socmmsop;</citetitle>, chapter
      <citetitle>Operation and Maintenance</citetitle>, section
      <citetitle>Database</citetitle>. It is available from
      <link xlink:href="https://www.suse.com/documentation/"></link>.
     </para>
    </listitem>
   </varlistentry>
  </variablelist>

  <bridgehead renderas="sect3"><guimenu>elasticsearch_curator: settings for
    elastisearch-curator</guimenu>
  </bridgehead>

  <para>
   <systemitem>elasticsearch-curator</systemitem> removes old and large
   elasticsearch indices. The settings below determine its behavior.
  </para>

  <variablelist>
   <varlistentry>
    <term>delete_after_days</term>
    <listitem>
     <para>
      Time threshold for deleting indices. Indices older the specified number
      of days are deleted. This parameter is unset by default, so indices are
      kept indefinitely.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>delete_after_size</term>
    <listitem>
     <para>
      Maximum size in megabytes of indices. Indices larger than the specified
      size are deleted. This parameter is unset by default, so indices are kept
      irrespective of their size.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>delete_exclude_index</term>
    <listitem>
     <para>
      List of indices to exclude from
      <systemitem>elasticsearch-curator</systemitem> runs. By default, only the
      <filename>.kibana</filename> files are excluded.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>cron_config</term>
    <listitem>
     <para>
      Specifies when to run <systemitem>elasticsearch-curator</systemitem>.
      Attributes of this parameter correspond to the fields in
      <systemitem>crontab(5)</systemitem>.
     </para>
    </listitem>
   </varlistentry>
  </variablelist>

  <bridgehead renderas="sect3"><guimenu>kafka: tunables for
Kafka</guimenu>
  </bridgehead>

  <variablelist>
   <varlistentry>
    <term>log_retention_hours</term>
    <listitem>
     <para>
      Number of hours for retaining log segments in Kafka's on-disk log.
      Messages older than the specified value are dropped.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>log_retention_bytes</term>
    <listitem>
     <para>
      Maximum size for Kafka's on-disk log in bytes. If the log grows beyond
      this size, the oldest log segments are dropped.
     </para>
    </listitem>
   </varlistentry>
  </variablelist>

  <bridgehead renderas="sect3"><guimenu>master: configuration for
monasca-installer on the &crow; node</guimenu>
  </bridgehead>

  <variablelist>
   <varlistentry>
    <term>influxdb_retention_policy</term>
    <listitem>
     <para>
      Number of days to keep metrics records in influxdb.
     </para>
     <para>
      For an overview of all supported values, see
      <link
         xlink:href="https://docs.influxdata.com/influxdb/v0.9/query_language/database_management/#create-retention-policies-with-create-retention-policy"></link>.
     </para>
<!--taroth/dpopov 2017-05-15: as the installation chapter has been removed
       from both SOC Monitoring Operator Guides, there's no internal information
       we can point to for now, that's why we need to point to the URl above
      for now-->
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>notification_enable_email</term>
    <listitem>
     <para>
      Enable or disable email alarm notifications.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>smtp_host</term>
    <listitem>
     <para>
      SMTP smarthost for sending alarm notifications.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>smtp_port</term>
    <listitem>
     <para>
      Port for the SMTP smarthost.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>smtp_user</term>
    <listitem>
     <para>
      User name for authenticating against the smarthost.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>smtp_password</term>
    <listitem>
     <para>
      Password for authenticating against the smarthost.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>smtp_from_address</term>
    <listitem>
     <para>
      Sender address for alarm notifications.
     </para>
    </listitem>
   </varlistentry>
  </variablelist>

  <bridgehead renderas="sect3"><guimenu>Deployment</guimenu>
  </bridgehead>

  <para>
   The &o_monitor; component consists of following roles:
  </para>

  <variablelist>
   <varlistentry>
    <term>monasca-server</term>
    <listitem>
     <para>
      &o_monitor; server-side components that are deployed by &chef;.
      Currently, this only creates &o_ident; resources required by &o_monitor;,
      such as users, roles, endpoints, etc. The rest is left to the
      Ansible-based <systemitem>monasca-installer</systemitem> run by the
      <systemitem>monasca-master</systemitem> role.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>monasca-master</term>
    <listitem>
     <para>
      Runs the Ansible-based <systemitem>monasca-installer</systemitem> from
      the &crow; node. The installer deploys the &o_monitor; server-side
      components to the node that has the
      <systemitem>monasca-server</systemitem> role assigned to it. These
      components are <systemitem>openstack-monasca-api</systemitem>, and
      <systemitem>openstack-monasca-log-api</systemitem>, as well as all the
      back-end services they use.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>monasca-agent</term>
    <listitem>
     <para>
      Deploys <systemitem>openstack-monasca-agent</systemitem> that is
      responsible for sending metrics to <systemitem>monasca-api</systemitem>
      on nodes it is assigned to.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>monasca-log-agent</term>
    <listitem>
     <para>
      Deploys <systemitem>openstack-monasca-log-agent</systemitem> responsible
      for sending logs to <systemitem>monasca-log-api</systemitem> on nodes it
      is assigned to.
     </para>
    </listitem>
   </varlistentry>
  </variablelist>

  <figure>
   <title>The &o_monitor; &Barcl;: Node Deployment Example</title>
   <mediaobject>
    <imageobject role="fo">
     <imagedata fileref="depl_barclamp_monasca_node_deployment.png" width="100%" format="png"/>
    </imageobject>
    <imageobject role="html">
     <imagedata fileref="depl_barclamp_monasca_node_deployment.png" width="75%" format="png"/>
    </imageobject>
   </mediaobject>
  </figure>
 </sect1>
 <sect1 xml:id="sec.depl.ostack.final">
  <title>How to Proceed</title>

  <para>
   With a successful deployment of the &ostack; &dash;, the &productname;
   installation is finished. To be able to test your setup by starting an
   &vmguest; one last step remains to be done&mdash;uploading an image to the
   &o_img; component. Refer to the &cloudsuppl;, chapter <citetitle>Manage
   images</citetitle>
<!--<xref linkend="sec.adm.cli.img"/>-->
   for instructions. Images for &cloud; can be built in SUSE Studio. Refer to
   the &cloudsuppl;, section <citetitle>Building Images with
   &susestudio;</citetitle>.
  </para>

  <para>
   Now you can hand over to the cloud administrator to set up users, roles,
   flavors, etc.&mdash;refer to the &cloudadmin; for details. The default
   credentials for the &ostack; &dash; are user name <literal>admin</literal>
   and password <literal>crowbar</literal>.
  </para>
 </sect1>
 <sect1 xml:id="sec.depl.services">
  <title>Roles and Services in &productname;</title>

  <para>
   The following table lists all roles (as defined in the &barcl;s), and their
   associated services. As of &productname; &productnumber; this list is work
   in progress. Services can be manually started and stopped with the commands
   <command>systemctl start <replaceable>SERVICE</replaceable></command> and
   <command>systemctl stop <replaceable>SERVICE</replaceable></command>.
  </para>

  <informaltable>
   <tgroup cols="2">
    <colspec colnum="1" colname="1" colwidth="35*"/>
    <colspec colnum="2" colname="2" colwidth="65*"/>
    <thead>
     <row>
      <entry>
       <para>
        Role
       </para>
      </entry>
      <entry>
       <para>
        Service
       </para>
      </entry>
     </row>
    </thead>
    <tbody>
<!-- calamari -->
     <row>
      <entry>
       <para>
        calamari
       </para>
      </entry>
      <entry>
       <para>
        <systemitem class="service">cthulhu</systemitem>
       </para>
      </entry>
     </row>
<!-- ceilometer-agent -->
     <row>
      <entry>
       <para>
        ceilometer-agent
       </para>
      </entry>
      <entry><systemitem class="service">
       openstack-ceilometer-agent-compute
      </systemitem>
      </entry>
     </row>
<!-- ceilometer-polling -->
     <row>
      <entry morerows="5">
       <para>
        ceilometer-polling
       </para>
       <para>
        ceilometer-server
       </para>
       <para>
        ceilometer-swift-proxy-middleware
       </para>
      </entry>
      <entry>
       <para>
        <systemitem class="service"> openstack-ceilometer-agent-notification
        </systemitem>
       </para>
      </entry>
     </row>
     <row>
      <entry>
       <para>
        <systemitem class="service"> openstack-ceilometer-alarm-evaluator
        </systemitem>
       </para>
      </entry>
     </row>
     <row>
      <entry>
       <para>
        <systemitem class="service"> openstack-ceilometer-alarm-notifier
        </systemitem>
       </para>
      </entry>
     </row>
     <row>
      <entry>
       <para>
        <systemitem class="service"> openstack-ceilometer-api </systemitem>
       </para>
      </entry>
     </row>
     <row>
      <entry>
       <para>
        <systemitem class="service"> openstack-ceilometer-collector
        </systemitem>
       </para>
      </entry>
     </row>
     <row>
      <entry>
       <para>
        <systemitem class="service"> openstack-ceilometer-polling </systemitem>
       </para>
      </entry>
     </row>
<!-- ceph-mon -->
     <row>
      <entry>
       <para>
        ceph-mon
       </para>
      </entry>
      <entry>
       <para>
        <systemitem class="service">ceph-mon@*</systemitem>
       </para>
      </entry>
     </row>
<!-- ceph osd -->
     <row>
      <entry>
       <para>
        ceph-osd
       </para>
      </entry>
      <entry>
       <para>
        <systemitem class="service">ceph-osd@*</systemitem>
       </para>
      </entry>
     </row>
<!-- ceph-radosgw -->
     <row>
      <entry>
       <para>
        ceph-radosgw
       </para>
      </entry>
      <entry>
       <para>
        <systemitem class="service">ceph-radosgw@*</systemitem>
       </para>
      </entry>
     </row>
<!-- cinder-controller -->
     <row>
      <entry morerows="1">
       <para>
        cinder-controller
       </para>
      </entry>
      <entry>
       <para>
        <systemitem class="service">openstack-cinder-api</systemitem>
       </para>
      </entry>
     </row>
     <row>
      <entry>
       <para>
        <systemitem class="service">openstack-cinder-scheduler</systemitem>
       </para>
      </entry>
     </row>
<!-- cinder-volume -->
     <row>
      <entry>
       <para>
        cinder-volume
       </para>
      </entry>
      <entry>
       <para>
        <systemitem class="service">openstack-cinder-volume</systemitem>
       </para>
      </entry>
     </row>
<!-- database-server -->
     <row>
      <entry>
       <para>
        database-server
       </para>
      </entry>
      <entry>
       <para>
        <systemitem class="service">postgresql</systemitem>
       </para>
      </entry>
     </row>
<!-- glance-server -->
     <row>
      <entry morerows="1">
       <para>
        glance-server
       </para>
      </entry>
      <entry>
       <para>
        <systemitem class="service">openstack-glance-api</systemitem>
       </para>
      </entry>
     </row>
     <row>
      <entry>
       <para>
        <systemitem class="service">openstack-glance-registry</systemitem>
       </para>
      </entry>
     </row>
<!-- heat-server -->
     <row>
      <entry morerows="3">
       <para>
        heat-server
       </para>
      </entry>
      <entry>
       <para>
        <systemitem class="service">openstack-heat-api-cfn</systemitem>
       </para>
      </entry>
     </row>
     <row>
      <entry>
       <para>
        <systemitem class="service">openstack-heat-api-cloudwatch</systemitem>
       </para>
      </entry>
     </row>
     <row>
      <entry>
       <para>
        <systemitem class="service">openstack-heat-api</systemitem>
       </para>
      </entry>
     </row>
     <row>
      <entry>
       <para>
        <systemitem class="service">openstack-heat-engine</systemitem>
       </para>
      </entry>
     </row>
<!-- horizon -->
     <row>
      <entry>
       <para>
        horizon
       </para>
      </entry>
      <entry>
       <para>
        <systemitem class="service">apache2</systemitem>
       </para>
      </entry>
     </row>
<!-- keystone-server -->
     <row>
      <entry>
       <para>
        keystone-server
       </para>
      </entry>
      <entry>
       <para>
        <systemitem class="service">openstack-keystone</systemitem>
       </para>
      </entry>
     </row>
<!-- manila-server -->
     <row>
      <entry morerows="1">
       <para>
        manila-server
       </para>
      </entry>
      <entry>
       <para>
        <systemitem class="service">openstack-manila-api</systemitem>
       </para>
      </entry>
     </row>
     <row>
      <entry>
       <para>
        <systemitem class="service">openstack-manila-scheduler</systemitem>
       </para>
      </entry>
     </row>
<!-- manila-share -->
     <row>
      <entry>
       <para>
        manila-share
       </para>
      </entry>
      <entry>
       <para>
        <systemitem class="service">openstack-manila-share</systemitem>
       </para>
      </entry>
     </row>
<!-- neutron-server -->
     <row>
      <entry>
       <para>
        neutron-server
       </para>
      </entry>
      <entry>
       <para>
        <systemitem class="service">openstack-neutron</systemitem>
       </para>
      </entry>
     </row>
<!-- nova-compute-* -->
     <row>
      <entry morerows="1">
       <para>
        nova-compute-*
       </para>
      </entry>
      <entry>
       <para>
        <systemitem class="service">openstack-nova-compute</systemitem>
       </para>
      </entry>
     </row>
     <row>
      <entry>
       <para>
        <systemitem class="service"> openstack-neutron-openvswitch-agent
        </systemitem> (when neutron is deployed with openvswitch)
       </para>
      </entry>
     </row>
<!-- nova-controller -->
     <row>
      <entry morerows="6">
       <para>
        nova-controller
       </para>
      </entry>
      <entry>
       <para>
        <systemitem class="service">openstack-nova-api</systemitem>
       </para>
      </entry>
     </row>
     <row>
      <entry>
       <para>
        <systemitem class="service">openstack-nova-cert</systemitem>
       </para>
      </entry>
     </row>
     <row>
      <entry>
       <para>
        <systemitem class="service">openstack-nova-conductor</systemitem>
       </para>
      </entry>
     </row>
     <row>
      <entry>
       <para>
        <systemitem class="service">openstack-nova-consoleauth</systemitem>
       </para>
      </entry>
     </row>
     <row>
      <entry>
       <para>
        <systemitem class="service">openstack-nova-novncproxy</systemitem>
       </para>
      </entry>
     </row>
     <row>
      <entry>
       <para>
        <systemitem class="service">openstack-nova-objectstore</systemitem>
       </para>
      </entry>
     </row>
     <row>
      <entry>
       <para>
        <systemitem class="service">openstack-nova-scheduler</systemitem>
       </para>
      </entry>
     </row>
<!-- rabbitmq-server -->
     <row>
      <entry>
       <para>
        rabbitmq-server
       </para>
      </entry>
      <entry>
       <para>
        <systemitem class="service">rabbitmq-server</systemitem>
       </para>
      </entry>
     </row>
<!-- swift-dispersion -->
     <row>
      <entry>
       <para>
        swift-dispersion
       </para>
      </entry>
      <entry>
       <para>
        none
       </para>
      </entry>
     </row>
<!-- swift-proxy -->
     <row>
      <entry>
       <para>
        swift-proxy
       </para>
      </entry>
      <entry>
       <para>
        <systemitem class="service">openstack-swift-proxy</systemitem>
       </para>
      </entry>
     </row>
<!-- swift-ring-compute -->
     <row>
      <entry>
       <para>
        swift-ring-compute
       </para>
      </entry>
      <entry>
       <para>
        none
       </para>
      </entry>
     </row>
<!-- swift-storage -->
     <row>
      <entry morerows="13">
       <para>
        swift-storage
       </para>
      </entry>
      <entry>
       <para>
        <systemitem class="service">openstack-swift-account-auditor</systemitem>
       </para>
      </entry>
     </row>
     <row>
      <entry>
       <para>
        <systemitem class="service">openstack-swift-account-reaper</systemitem>
       </para>
      </entry>
     </row>
     <row>
      <entry>
       <para>
        <systemitem class="service">openstack-swift-account-replicator</systemitem>
       </para>
      </entry>
     </row>
     <row>
      <entry>
       <para>
        <systemitem class="service">openstack-swift-account</systemitem>
       </para>
      </entry>
     </row>
     <row>
      <entry>
       <para>
        <systemitem class="service">openstack-swift-container-auditor</systemitem>
       </para>
      </entry>
     </row>
     <row>
      <entry>
       <para>
        <systemitem class="service">openstack-swift-container-replicator</systemitem>
       </para>
      </entry>
     </row>
     <row>
      <entry>
       <para>
        <systemitem class="service">openstack-swift-container-sync</systemitem>
       </para>
      </entry>
     </row>
     <row>
      <entry>
       <para>
        <systemitem class="service">openstack-swift-container-updater</systemitem>
       </para>
      </entry>
     </row>
     <row>
      <entry>
       <para>
        <systemitem class="service">openstack-swift-container</systemitem>
       </para>
      </entry>
     </row>
     <row>
      <entry>
       <para>
        <systemitem class="service">openstack-swift-object-auditor</systemitem>
       </para>
      </entry>
     </row>
     <row>
      <entry>
       <para>
        <systemitem class="service">openstack-swift-object-expirer</systemitem>
       </para>
      </entry>
     </row>
     <row>
      <entry>
       <para>
        <systemitem class="service">openstack-swift-object-replicator</systemitem>
       </para>
      </entry>
     </row>
     <row>
      <entry>
       <para>
        <systemitem class="service">openstack-swift-object-updater</systemitem>
       </para>
      </entry>
     </row>
     <row>
      <entry>
       <para>
        <systemitem class="service">openstack-swift-object</systemitem>
       </para>
      </entry>
     </row>
<!-- trove-server -->
     <row>
      <entry morerows="2">
       <para>
        trove-server
       </para>
      </entry>
      <entry>
       <para>
        <systemitem class="service">openstack-trove-api</systemitem>
       </para>
      </entry>
     </row>
     <row>
      <entry>
       <para>
        <systemitem class="service">openstack-trove-conductor</systemitem>
       </para>
      </entry>
     </row>
     <row>
      <entry>
       <para>
        <systemitem class="service">openstack-trove-taskmanager</systemitem>
       </para>
      </entry>
     </row>
    </tbody>
   </tgroup>
  </informaltable>
 </sect1>
</chapter>
