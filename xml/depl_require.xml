<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE chapter
[
  <!ENTITY % entities SYSTEM "entity-decl.ent">
  %entities;
]>
<!-- Converted by suse-upgrade version 1.1 -->
<chapter xmlns="http://docbook.org/ns/docbook" xmlns:xi="http://www.w3.org/2001/XInclude" xmlns:xlink="http://www.w3.org/1999/xlink" version="5.0" xml:id="cha-depl-req">
 <info>
  <title>Considerations and Requirements</title>
  <abstract>
   <para>
    Before deploying &productname;, there are some requirements to meet and
    architectural decisions to make. Read this chapter thoroughly first, as
    some decisions need to be made <emphasis>before</emphasis> deploying
    &cloud;, and they cannot be changed afterward.
   </para>
  </abstract>
 <dm:docmanager xmlns:dm="urn:x-suse:ns:docmanager">
     <dm:maintainer>fs</dm:maintainer>
     <dm:status>editing</dm:status>
     <dm:deadline/>
     <dm:priority/>
     <dm:translation>no</dm:translation>
     <dm:languages/>
 </dm:docmanager>
 </info>
 <sect1 xml:id="sec-depl-req-network">
  <title>Network</title>

  <para>
   &productname; requires a complex network setup consisting of several
   networks that are configured during installation. These networks are for
   exclusive cloud usage. You need a router to access them from an existing network.
  </para>

  <para>
   The network configuration on the nodes in the &cloud; network is
   entirely controlled by &crow;. Any network configuration not created with
   &crow; (for example, with &yast;) will automatically be
   overwritten. After the cloud is deployed, network settings cannot be
   changed.
  </para>

  <figure>
   <title>&cloud; Network: Overview</title>
   <mediaobject>
    <imageobject role="fo">
     <imagedata fileref="cloud_network_overview.png" width="90%" format="PNG"/>
    </imageobject>
    <imageobject role="html">
     <imagedata fileref="cloud_network_overview.png" width="75%" format="PNG"/>
    </imageobject>
   </mediaobject>
  </figure>

  <para>
   The following networks are pre-defined when setting up &productname;.
   The IP addresses listed are the default addresses and can be changed
   using the &yast; &crow; module (see
   <xref linkend="sec-depl-adm-inst-crowbar"/>). It is also possible to
   customize the network setup by manually editing
   the network &barcl; template. See
   <xref linkend="sec-depl-inst-admserv-post-network"/> for detailed instructions.
  </para>

  <variablelist>
   <varlistentry xml:id="vle-netw-adm">
    <term>
     Admin Network (192.168.124/24)
    </term>
    <listitem>
<!-- TODO vuntz 2013-12-04: for reference, we got feedback that we should not touch the IPMI configuration at all by default, so we'll likely do this; that would mean access to the BMC would not be configured by Crowbar by default -->
     <para>
      A private network to access the &admserv; and all nodes for
      administration purposes. The default setup also allows access to the
      BMC (Baseboard Management Controller) data via IPMI (Intelligent
      Platform Management Interface) from this network. If required, BMC
      access can be swapped to a separate network.
     </para>
     <para>
      You have the following options for controlling access to this network:
     </para>
     <itemizedlist mark="bullet" spacing="normal">
      <listitem>
       <para>
        Do not allow access from the outside and keep the admin network
        completely separated.
       </para>
      </listitem>
      <listitem>
       <para>
        Allow access to the &admserv; from a single network (for example,
        your company's administration network) via the <quote>bastion
        network</quote> option configured on an additional network card with
        a fixed IP address.
       </para>
      </listitem>
      <listitem>
       <para>
        Allow access from one or more networks via a gateway.
       </para>
      </listitem>
     </itemizedlist>
    </listitem>
   </varlistentry>
   <varlistentry xml:id="vle-netw-stor">
    <term>
     Storage Network (192.168.125/24)
    </term>
    <listitem>
     <para>
      Private &cloud; internal virtual network. This network is used by
      &ceph; and &o_objstore; only. It should not be accessed by
      users.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>
     Private Network (nova-fixed, 192.168.123/24)
    </term>
    <listitem>
     <para>
      Private &cloud; internal virtual network. This network is used for
      inter-&vmguest; communication and provides access to the outside
      world for the &vmguest;s. The required gateway is automatically provided by &cloud;.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>
     Public Network (nova-floating, public, 192.168.126/24)
    </term>
    <listitem>
     <para>
      The only public network provided by &cloud;. You can access the
      nova &dash; and all &vmguest;s (provided they have been equipped
      with floating IP addresses) on this network. This network can only be accessed
      via a gateway, which must be provided externally. All &cloud;
      users and administrators must have access to the public network.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry xml:id="vle-netw-sdn">
    <term>
     Software Defined Network (os_sdn, 192.168.130/24)
    </term>
    <listitem>
     <para>
      Private &cloud; internal virtual network. This network is used
      when &o_netw; is configured to use openvswitch with
      <literal>GRE</literal> tunneling for the virtual networks. It should
      not be accessible to users.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>The &o_monitor; Monitoring Network</term>
    <listitem>
     <para>
      The &o_monitor; monitoring node needs to have an interface on both the
      admin network and the public network. monasca's backend services will
      listen on the admin network, the API services
      (<systemitem>openstack-monasca-api</systemitem>,
      <systemitem>openstack-monasca-log-api</systemitem>) will listen on all
      interfaces. <systemitem>openstack-monasca-agent</systemitem> and
      <systemitem>openstack-monasca-log-agent</systemitem> will send their logs
      and metrics to the
      <systemitem>monasca-api</systemitem>/<systemitem>monasca-log-api</systemitem>
      services to the monitoring node's public network IP address.
     </para>
    </listitem>
   </varlistentry>
  </variablelist>

  <warning>
   <title>Protect Networks from External Access</title>
   <para>
    For security reasons, protect the following networks from external
    access:
   </para>
   <itemizedlist mark="bullet" spacing="normal">
    <listitem>
     <para>
      <xref linkend="vle-netw-adm" xrefstyle="select:label nopage"/>
     </para>
    </listitem>
    <listitem>
     <para>
      <xref linkend="vle-netw-stor" xrefstyle="select:label nopage"/>
     </para>
    </listitem>
    <listitem>
     <para>
      <xref linkend="vle-netw-sdn" xrefstyle="select:label nopage"/>
     </para>
    </listitem>
   </itemizedlist>
   <para>
    Especially traffic from the cloud instances must not be able to pass
    through these networks.
   </para>
  </warning>
  <important>
   <title>VLAN Settings</title>
   <para>
    As of &productname; 8, using a VLAN for the admin network is
    only supported on a native/untagged VLAN. If you need VLAN support for the
    admin network, it must be handled at switch level.
   </para>
   <para>
    When changing the network configuration with &yast; or by editing
    <filename>/etc/crowbar/network.json</filename> you can define VLAN
    settings for each network. For the networks <literal>nova-fixed</literal>
    and <literal>nova-floating</literal>, however, special rules apply:
   </para>
   <para>
    <emphasis role="bold">nova-fixed</emphasis>: The <guimenu>USE
    VLAN</guimenu> setting will be ignored. However, VLANs will automatically
    be used if deploying &o_netw; with VLAN support (using the plugins
    linuxbridge, openvswitch plus VLAN, or cisco plus VLAN). In this case, you
    need to specify a correct <guimenu>VLAN ID</guimenu> for this network.
   </para>
   <para>
    <emphasis role="bold">nova-floating</emphasis>: When using a VLAN for
    <literal>nova-floating</literal> (which is the default), the <guimenu>USE
    VLAN</guimenu> and <guimenu>VLAN ID</guimenu> settings for
    <guimenu>nova-floating</guimenu> and <guimenu>public</guimenu> must be
    the same. When not using a VLAN for <literal>nova-floating</literal>, it
    must have a different physical network interface than the
    <literal>nova_fixed</literal> network.
   </para>
  </important>

  <note>
   <title>No IPv6 Support</title>
   <para>
    As of &productname; 8, IPv6 is not supported. This applies to the cloud
    internal networks and to the &vmguest;s. You must use static IPv4 addresses
    for all network interfaces on the &admnode;, and disable IPv6 before
    deploying &crow; on the &admnode;.
   </para>
  </note>

  <para>
   The following diagram shows the pre-defined &cloud; network in more
   detail. It demonstrates how the &ostack; nodes and services use the
   different networks.
  </para>

  <figure>
   <title>&cloud; Network: Details</title>
   <mediaobject>
    <imageobject role="fo">
     <imagedata fileref="cloud_network_detail.png" width="100%" format="PNG"/>
    </imageobject>
    <imageobject role="html">
     <imagedata fileref="cloud_network_detail.png" width="100%" format="PNG"/>
    </imageobject>
   </mediaobject>
  </figure>

  <sect2 xml:id="sec-depl-req-network-allocation">
   <title>Network Address Allocation</title>
   <para>
    The default networks set up in &cloud; are class C networks with 256
    IP addresses each. This limits the maximum number of &vmguest;s that
    can be started simultaneously. Addresses within the networks are
    allocated as outlined in the following table. Use the &yast;
    &crow; module to make customizations (see
    <xref linkend="sec-depl-adm-inst-crowbar"/>). The last address in the IP address
    range of each network is always reserved as the broadcast address. This
    assignment cannot be changed.
   </para>
   <para>For an overview of the minimum number of IP addresses needed for each
    of the ranges in the network settings, see <xref linkend="tab-netw-range-ip-min"/>.
   </para>
   <table xml:id="tab-netw-range-ip-min">
    <title>Minimum Number of IP Addresses for Network Ranges</title>
    <tgroup cols="2">
     <colspec colnum="1" colname="1" colwidth="30*"/>
     <colspec colnum="2" colname="2" colwidth="70*"/>
     <thead>
      <row>
       <entry>
        <para>
         Network
        </para>
       </entry>
       <entry>
        <para>
         Required Number of IP addresses
        </para>
       </entry>
      </row>
     </thead>
     <tbody>
      <row>
       <entry>
        <para>
         Admin Network
        </para>
       </entry>
       <entry>
        <itemizedlist>
         <listitem>
          <para>1 IP address per node (&admserv;, &contrnode;s, and
           &compnode;s)</para>
         </listitem>
         <listitem>
          <para>1 VIP address for PostgreSQL</para>
         </listitem>
         <listitem>
          <para>1 VIP address for RabbitMQ</para>
         </listitem>
         <listitem>
          <para>1 VIP address per cluster (per Pacemaker &barcl; proposal)</para>
         </listitem>
        </itemizedlist>
       </entry>
      </row>
      <row>
       <entry>
        <para>
         Public Network
        </para>
       </entry>
       <entry>
        <itemizedlist>
        <listitem>
          <para>1 IP address per node (&contrnode;s and &compnode;s)</para>
        </listitem>
        <listitem>
          <para>1 VIP address per cluster</para>
        </listitem>
        </itemizedlist>
       </entry>
      </row>
      <row>
       <entry>
        <para>
         BMC Network
        </para>
       </entry>
       <entry>
        <itemizedlist>
         <listitem>
          <para>1 IP address per node (&admserv;, &contrnode;s, and
           &compnode;s)</para>
         </listitem>
        </itemizedlist>
       </entry>
      </row>
      <row>
       <entry>
        <para>
         Software Defined Network
        </para>
       </entry>
       <entry>
        <itemizedlist>
         <listitem>
          <para>1 IP address per node (&contrnode;s and
           &compnode;s)</para>
         </listitem>
        </itemizedlist>
       </entry>
      </row>
     </tbody>
    </tgroup>
   </table>
   <note>
    <title>Limitations of the Default Network Proposal</title>
    <para>
     The default network proposal as described below limits the maximum
     number of &compnode;s to 80, the maximum number of floating IP
     addresses to 61 and the maximum number of addresses in the nova_fixed
     network to 204.
    </para>
    <para>
     To overcome these limitations you need to reconfigure the network setup
     by using appropriate address ranges. Do this by either using the
     &yast; &crow; module as described in
     <xref linkend="sec-depl-adm-inst-crowbar"/>, or by manually editing the
     network template file as described in
     <xref linkend="sec-depl-inst-admserv-post-network"/>.
    </para>
   </note>
   <table>
    <title><systemitem class="etheraddress">192.168.124.0/24</systemitem> (Admin/BMC) Network Address Allocation</title>
    <tgroup cols="3">
     <colspec colnum="1" colname="1" colwidth="20*"/>
     <colspec colnum="2" colname="2" colwidth="33*"/>
     <colspec colnum="3" colname="3" colwidth="47*"/>
     <thead>
      <row>
       <entry>
        <para>
         Function
        </para>
       </entry>
       <entry>
        <para>
         Address
        </para>
       </entry>
       <entry>
        <para>
         Remark
        </para>
       </entry>
      </row>
     </thead>
     <tbody>
      <row>
       <entry>
        <para>
         router
        </para>
       </entry>
       <entry>
        <para>
         <systemitem class="etheraddress">192.168.124.1</systemitem>
        </para>
       </entry>
       <entry>
        <para>
         Provided externally.
        </para>
       </entry>
      </row>
      <row>
       <entry>
        <para>
         admin
        </para>
       </entry>
       <entry>
        <para>
         <systemitem class="etheraddress">192.168.124.10</systemitem> -
         <systemitem class="etheraddress">192.168.124.11</systemitem>
        </para>
       </entry>
       <entry>
        <para>
         Fixed addresses reserved for the &admserv;.
        </para>
       </entry>
      </row>
      <row>
       <entry>
        <para>
         DHCP
        </para>
       </entry>
       <entry>
        <para>
         <systemitem class="etheraddress">192.168.124.21</systemitem> -
         <systemitem class="etheraddress">192.168.124.80</systemitem>
        </para>
       </entry>
       <entry>
        <para>
         Address range reserved for node allocation/installation. Determines
         the maximum number of parallel allocations/installations.
        </para>
       </entry>
      </row>
      <row>
       <entry>
        <para>
         host
        </para>
       </entry>
       <entry>
        <para>
         <systemitem class="etheraddress">192.168.124.81</systemitem> -
         <systemitem class="etheraddress">192.168.124.160</systemitem>
        </para>
       </entry>
       <entry>
        <para>
         Fixed addresses for the &ostack; nodes. Determines the maximum
         number of &ostack; nodes that can be deployed.
        </para>
       </entry>
      </row>
      <row>
       <entry>
        <para>
         bmc vlan host
        </para>
       </entry>
       <entry>
        <para>
         <systemitem class="etheraddress">192.168.124.161</systemitem>
        </para>
       </entry>
       <entry>
        <para>
         Fixed address for the BMC VLAN. Used to generate a VLAN tagged
         interface on the Administration Server that can access the BMC
         network. The BMC VLAN must be in the same ranges as BMC, and
         BMC must have VLAN enabled.
        </para>
       </entry>
      </row>
      <row>
       <entry>
        <para>
         bmc host
        </para>
       </entry>
       <entry>
        <para>
         <systemitem class="etheraddress">192.168.124.162</systemitem> -
         <systemitem class="etheraddress">192.168.124.240</systemitem>
        </para>
       </entry>
       <entry>
        <para>
         Fixed addresses for the &ostack; nodes. Determines the maximum
         number of &ostack; nodes that can be deployed.
        </para>
       </entry>
      </row>
      <row>
       <entry>
        <para>
         switch
        </para>
       </entry>
       <entry>
        <para>
         <systemitem class="etheraddress">192.168.124.241</systemitem> -
         <systemitem class="etheraddress">192.168.124.250</systemitem>
        </para>
       </entry>
       <entry>
        <para>
         This range is not used in current releases and might be removed in
         the future.
        </para>
       </entry>
      </row>
     </tbody>
    </tgroup>
   </table>
   <table>
    <title><systemitem class="etheraddress">192.168.125/24</systemitem> (Storage) Network Address Allocation</title>
    <tgroup cols="3">
     <colspec colnum="1" colname="1" colwidth="20*"/>
     <colspec colnum="2" colname="2" colwidth="33*"/>
     <colspec colnum="3" colname="3" colwidth="47*"/>
     <thead>
      <row>
       <entry>
        <para>
         Function
        </para>
       </entry>
       <entry>
        <para>
         Address
        </para>
       </entry>
       <entry>
        <para>
         Remark
        </para>
       </entry>
      </row>
     </thead>
     <tbody>
      <row>
       <entry>
        <para>
         host
        </para>
       </entry>
       <entry>
        <para>
         <systemitem class="etheraddress">192.168.125.10</systemitem> -
         <systemitem class="etheraddress">192.168.125.239</systemitem>
        </para>
       </entry>
       <entry>
        <para>
         Each &stornode; will get an address from this range.
        </para>
       </entry>
      </row>
     </tbody>
    </tgroup>
   </table>
   <table>
    <title><systemitem class="etheraddress">192.168.123/24</systemitem> (Private Network/nova-fixed) Network Address Allocation</title>
    <tgroup cols="3">
     <colspec colnum="1" colname="1" colwidth="20*"/>
     <colspec colnum="2" colname="2" colwidth="33*"/>
     <colspec colnum="3" colname="3" colwidth="47*"/>
     <thead>
      <row>
       <entry>
        <para>
         Function
        </para>
       </entry>
       <entry>
        <para>
         Address
        </para>
       </entry>
       <entry>
        <para>
         Remark
        </para>
       </entry>
      </row>
     </thead>
     <tbody>
      <row>
       <entry>
        <para>
         DHCP
        </para>
       </entry>
       <entry>
        <para>
         <systemitem class="etheraddress">192.168.123.1</systemitem> -
         <systemitem class="etheraddress">192.168.123.254</systemitem>
        </para>
       </entry>
       <entry>
        <para>
         Address range for &vmguest;s, routers and DHCP/DNS agents.
        </para>
       </entry>
      </row>
     </tbody>
    </tgroup>
   </table>
   <table>
    <title><systemitem class="etheraddress">192.168.126/24</systemitem> (Public Network nova-floating, public) Network Address Allocation</title>
    <tgroup cols="3">
     <colspec colnum="1" colname="1" colwidth="20*"/>
     <colspec colnum="2" colname="2" colwidth="33*"/>
     <colspec colnum="3" colname="3" colwidth="47*"/>
     <thead>
      <row>
       <entry>
        <para>
         Function
        </para>
       </entry>
       <entry>
        <para>
         Address
        </para>
       </entry>
       <entry>
        <para>
         Remark
        </para>
       </entry>
      </row>
     </thead>
     <tbody>
      <row>
       <entry>
        <para>
         router
        </para>
       </entry>
       <entry>
        <para>
         <systemitem class="etheraddress">192.168.126.1</systemitem>
        </para>
       </entry>
       <entry>
        <para>
         Provided externally.
        </para>
       </entry>
      </row>
      <row>
       <entry>
        <para>
         public host
        </para>
       </entry>
       <entry>
        <para>
         <systemitem class="etheraddress">192.168.126.2</systemitem> -
         <systemitem class="etheraddress">192.168.126.127</systemitem>
        </para>
       </entry>
       <entry>
        <para>
         Public address range for external &cloud; components such as the
         &ostack; &dash; or the API.
        </para>
       </entry>
      </row>
      <row>
       <entry>
        <para>
         floating host
        </para>
       </entry>
       <entry>
        <para>
         <systemitem class="etheraddress">192.168.126.129</systemitem> -
         <systemitem class="etheraddress">192.168.126.254</systemitem>
        </para>
       </entry>
       <entry>
        <para>
         Floating IP address range. Floating IP addresses can be manually assigned to
         a running &vmguest; to allow to access the guest from the
         outside. Determines the maximum number of &vmguest;s that can
         concurrently be accessed from the outside.
        </para>
        <para>
         The nova_floating network is set up with a netmask of
         255.255.255.192, allowing a maximum number of 61 IP addresses. This
         range is pre-allocated by default and managed by &o_netw;.
        </para>
       </entry>
      </row>
     </tbody>
    </tgroup>
   </table>
   <table>
    <title><systemitem class="etheraddress">192.168.130/24</systemitem> (Software Defined Network) Network Address Allocation</title>
    <tgroup cols="3">
     <colspec colnum="1" colname="1" colwidth="20*"/>
     <colspec colnum="2" colname="2" colwidth="33*"/>
     <colspec colnum="3" colname="3" colwidth="47*"/>
     <thead>
      <row>
       <entry>
        <para>
         Function
        </para>
       </entry>
       <entry>
        <para>
         Address
        </para>
       </entry>
       <entry>
        <para>
         Remark
        </para>
       </entry>
      </row>
     </thead>
     <tbody>
      <row>
       <entry>
        <para>
         host
        </para>
       </entry>
       <entry>
        <para>
         <systemitem class="etheraddress">192.168.130.10</systemitem> -
         <systemitem class="etheraddress">192.168.130.254</systemitem>
        </para>
       </entry>
       <entry>
        <para>
         If &o_netw; is configured with <literal>openvswitch</literal>
         and <literal>gre</literal>, each network node and all
         &compnode;s will get an IP address from this range.
        </para>
       </entry>
      </row>
     </tbody>
    </tgroup>
   </table>
   <note>
    <title>Addresses for Additional Servers</title>
    <para>
     Addresses not used in the ranges mentioned above can be used to add
     additional servers with static addresses to &cloud;. Such servers
     can be used to provide additional services. A &susemgr; server
     inside &cloud;, for example, must be configured using one of
     these addresses.
    </para>
   </note>
  </sect2>

  <sect2 xml:id="sec-depl-req-network-modes">
   <title>Network Modes</title>
   <para>
    &productname; supports different network modes defined in &crow;: <literal>single</literal>, <literal>dual</literal>, and
    <literal>team</literal>. As of &productname; 8, the networking mode
    is applied to all nodes and the &admserv;. That means that all
    machines need to meet the hardware requirements for the chosen mode. The
    network mode can be configured using the &yast; &crow; module
    (<xref linkend="sec-depl-adm-inst-crowbar"/>). The network mode cannot
    be changed after the cloud is deployed.
   </para>
   <para>
    Other, more flexible network mode setups can be configured by manually
    editing the &crow; network configuration files. See <xref
    linkend="sec-depl-inst-admserv-post-network"/> for more information. &suse; or a
    partner can assist you in creating a custom setup within the scope of a
    consulting services agreement (see <link
    xlink:href="http://www.suse.com/consulting/"/> for more information on
    SUSE consulting).
   </para>
   <important>
    <title>Network Device Bonding is Required for HA</title>
    <para>Network device bonding is required for an &hasetup; of
     &productname;. If you are planning to move your cloud to an
     &hasetup; at a later point in time, make sure to use a network
     mode in the &yast; &crow; that supports network device bonding.</para>
    <para>Otherwise a migration to an &hasetup; is not supported.</para>
   </important>
   <sect3 xml:id="sec-depl-req-network-modes-single">
    <title>Single Network Mode</title>
    <para>
     In single mode you use one Ethernet card for all the traffic:
    </para>
    <informalfigure>
     <mediaobject>
       <textobject>
       <phrase>Single Network Mode</phrase>
     </textobject>
      <imageobject role="fo">
       <imagedata fileref="cloud_networking_single_mode.png" width="70%" format="PNG"/>
      </imageobject>
      <imageobject role="html">
       <imagedata fileref="cloud_networking_single_mode.png" width="50%" format="PNG"/>
      </imageobject>
     </mediaobject>
    </informalfigure>
   </sect3>
   <sect3 xml:id="sec-depl-req-network-modes-dual">
    <title>Dual Network Mode</title>
    <para>
     Dual mode needs two Ethernet cards (on all nodes but &admserv;) to completely separate traffic between the Admin Network and the public network:
    </para>
    <informalfigure>
     <mediaobject>
       <textobject>
       <phrase>Dual Network Mode</phrase>
     </textobject>
      <imageobject role="fo">
       <imagedata fileref="cloud_networking_dual_mode.png" width="100%" format="PNG"/>
      </imageobject>
      <imageobject role="html">
       <imagedata fileref="cloud_networking_dual_mode.png" width="70%" format="PNG"/>
      </imageobject>
     </mediaobject>
    </informalfigure>
   </sect3>
   <sect3 xml:id="sec-depl-req-network-modes-teaming">
    <title>Team Network Mode</title>
    <para>
     Team mode is similar to single mode, except that you
     combine several Ethernet cards to a <quote>bond</quote> (network device
     bonding). Team mode needs two or more Ethernet cards.
    </para>
    <informalfigure>
     <mediaobject>
       <textobject>
       <phrase>Team Network Mode</phrase>
     </textobject>
      <imageobject role="fo">
       <imagedata fileref="cloud_networking_team_mode.png" width="70%" format="PNG"/>
      </imageobject>
      <imageobject role="html">
       <imagedata fileref="cloud_networking_team_mode.png" width="50%" format="PNG"/>
      </imageobject>
     </mediaobject>
    </informalfigure>
    <para>
     When using team mode, you must choose a <quote>bonding
     policy</quote> that defines how to use the combined Ethernet cards. You
     can either set them up for fault tolerance, performance (load balancing),
     or a combination of both.
    </para>
   </sect3>
  </sect2>

  <sect2 xml:id="sec-depl-req-network-bastion">
   <title>Accessing the &admserv; via a Bastion Network</title>
   <para>
    Enabling access to the &admserv; from another network requires an external gateway. This option offers
    maximum flexibility, but requires additional hardware and may be less
    secure than you require. Therefore &cloud; offers a second option for
    accessing the &admserv;: the bastion network. You only need a
    dedicated Ethernet card and a static IP address from the external
    network to set it up.
   </para>
   <para>
    The bastion network setup (see <xref
    linkend="sec-depl-adm-inst-crowbar-mode-bastion"/> for setup instructions)
    enables logging in to the &admserv; via SSH from the company network. A
    direct login to other nodes in the cloud is not possible. However, the
    &admserv; can act as a <quote>jump host</quote>: First
    log in to the &admserv; via SSH, then log in via SSH to
    other nodes.
   </para>
  </sect2>

  <sect2 xml:id="sec-depl-req-network-dns">
   <title>DNS and Host Names</title>
   <para>
    The &admserv; acts as a name server for all nodes in the cloud. If
    the &admserv; has access to the outside, then you can add additional
    name servers that are automatically used to forward requests. If
    additional name servers are found on your cloud deployment, the name server
    on the &admserv; is automatically configured to forward requests
    for non-local records to these servers.
   </para>
   <para>
    The &admserv; must have a fully qualified host
    name. The domain name you specify is used for the DNS zone. It is
    required to use a sub-domain such as
    <replaceable>cloud.&exampledomain;</replaceable>. The &admserv;
    must have authority over the domain it is on so that it can
    create records for discovered nodes. As a result, it will not forward
    requests for names it cannot resolve in this domain, and thus cannot
    resolve names for the second-level domain, .e.g. <replaceable>&exampledomain;</replaceable>, other
    than for nodes in the cloud.
   </para>
   <para>
    This host name must not be changed after &cloud; has been deployed.
    The &ostack; nodes are named after their MAC address by default,
    but you can provide aliases, which are easier to remember when
    allocating the nodes. The aliases for the &ostack; nodes can be
    changed at any time. It is useful to have a list of MAC addresses and
    the intended use of the corresponding host at hand when deploying the
    &ostack; nodes.
   </para>
  </sect2>
 </sect1>
 <sect1 xml:id="sec-depl-req-storage">
  <title>Persistent Storage</title>

  <para>
   When talking about <quote>persistent storage</quote> on &productname;,
   there are two completely different aspects to discuss: 1) the block and
   object storage services &cloud; offers, 2) the
   hardware related storage aspects on the different node types.
  </para>

  <note>
   <title>Persistent vs. Ephemeral Storage</title>
   <para>
    Block and object storage are persistent storage models where files or
    images are stored until they are explicitly deleted. &cloud; also
    offers ephemeral storage for images attached to &vmguest;s. These
    ephemeral images only exist during the life of an &vmguest; and are
    deleted when the guest is terminated. See
    <xref linkend="sec-depl-req-storage-hardware-compute"/> for more
    information.
   </para>
  </note>

  <sect2 xml:id="sec-depl-req-storage-services">
   <title>Cloud Storage Services</title>
   <para>
    &cloud; offers two different types of services
    for persistent storage: object and block storage. Object storage lets
    you upload and download files (similar to an FTP server), whereas a
    block storage provides mountable devices (similar to a hard disk
    partition). &cloud; provides a repository to store the
    virtual disk images used to start &vmguest;s.
   </para>
   <variablelist>
    <varlistentry>
     <term>Object Storage with &o_objstore;</term>
     <listitem>
      <para>
       The &ostack; object storage service is called &o_objstore;. The
       storage component of &o_objstore; (swift-storage) must be
       deployed on dedicated nodes where no other cloud services run. Deploy at
       least two &o_objstore; nodes to provide redundant storage. &productname; is configured to
       always use all unused disks on a node for storage.
      </para>
      <para>
       &o_objstore; can optionally be used by &o_img;, the service
       that manages the images used to boot the &vmguest;s. Offering
       object storage with &o_objstore; is optional.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>Block Storage</term>
     <listitem>
      <para>
       Block storage on &cloud; is provided by &o_blockstore;.
       &o_blockstore; can use a variety of storage back-ends, including
       network storage solutions like NetApp or EMC. It is also possible to
       use local disks for block storage. A list of drivers available for
       &o_blockstore; and the features supported for each driver is
       available from the <citetitle>CinderSupportMatrix</citetitle> at
       <link xlink:href="https://wiki.openstack.org/wiki/cinderSupportMatrix"/>.
       &productname; &productnumber; ships with &ostack;
       &latest_ostack;.
      </para>
      <para>
       Alternatively, &o_blockstore; can use &ceph; RBD as a back-end.  &ceph;
       offers data security and speed by storing the the content on a dedicated
       &ceph; cluster.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>The &o_img; Image Repository</term>
     <listitem>
      <para>
       &o_img; provides a catalog and repository for virtual disk images
       used to start the &vmguest;s. &o_img; is installed on a
       &contrnode;. It uses &o_objstore;, &ceph;, or a
       directory on the &contrnode; to store the images. The image
       directory can either be a local directory or an NFS share.
      </para>
     </listitem>
    </varlistentry>
   </variablelist>
  </sect2>

  <sect2 xml:id="sec-depl-req-storage-hardware">
   <title>Storage Hardware Requirements</title>
   <para>
    Each node in &cloud; needs sufficient disk space to store both the operating system and  additional data.
    Requirements and recommendations for the various node types are listed
    below.
   </para>
   <important>
    <title>Choose a Hard Disk for the Operating System Installation</title>
    <para>
     The operating system will always be installed on the
     <emphasis>first</emphasis> hard disk. This is the disk that is listed
     <emphasis>first</emphasis> in the BIOS, the one from which the machine
     will boot. Make sure that the hard disk the operating system is installed on will be recognized as the first disk.
    </para>
   </important>
   <sect3 xml:id="sec-depl-req-storage-hardware-admin">
    <title>&admserv;</title>
    <para>
     If you store the update repositories directly on the &admserv; (see
     <xref linkend="sec-depl-req-repos"/>), we recommend mounting <filename>/srv</filename> on a separate partition or volume with a minimum of &repospace; space.
    </para>
    <para>
     Log files from all nodes in &cloud; are stored on the &admserv;
     under <filename>/var/log</filename>.
     The message service RabbitMQ requires 1 GB of free space
     in <filename>/var</filename> (see
     <xref linkend="sec-deploy-logs-adminserv"/> for a complete list).
    </para>
   </sect3>
   <sect3 xml:id="sec-depl-req-storage-hardware-control">
    <title>&contrnode;s</title>
    <para>
     Depending on how the services are set up, &o_img; and
     &o_blockstore; may require additional disk space on the
     &contrnode; on which they are running. &o_img; may be configured
     to use a local directory, whereas &o_blockstore; may use a local
     image file for storage. For performance and scalability reasons this is
     only recommended for test setups. Make sure there is sufficient free
     disk space available if you use a local file for storage.
    </para>
    <para>
     &o_blockstore; may be configured to use local disks for storage (configuration option <literal>raw</literal>). If you choose this setup, we recommend deploying the <guimenu>cinder-volume</guimenu> role to one or more dedicated &contrnode;s. Those should be equipped with several disks providing sufficient storage space. It may also be necessary to equip this node with two or more bonded network cards, since it will generate heavy network traffic. Bonded network cards require a special setup for this node. For details, refer to <xref linkend="sec-depl-inst-admserv-post-network"/>.
    </para>
    <para>
     Live migration for &xen; &vmguest;s requires exporting <filename>/var/lib/nova/instances</filename> on the &contrnode; hosting <systemitem>nova-controller</systemitem>. This directory will host a copy of the root disk of <emphasis>all</emphasis> &xen; &vmguest;s in the cloud and needs to have sufficient disk space. We strongly recommended using a separate block device for this directory, preferably a RAID device to ensure data security.
    </para>
   </sect3>
   <sect3 xml:id="sec-depl-req-storage-hardware-compute">
    <title>&compnode;s</title>
    <para>
     Unless an instance is started via “Boot from Volume”, it is started with at least one disk, which is a copy of the image from which it has been started. Depending on the flavor you start, the &vmguest; may also have a second, so-called <quote>ephemeral</quote>
     disk. The size of the root disk depends on the image itself.
     Ephemeral disks are always created as sparse image files that grow up
     to a defined size as they are <quote>filled</quote>. By default
     ephemeral disks have a size of 10 GB.
    </para>
    <para>
     Both disks, root images and ephemeral disk, are directly bound to the
     &vmguest; and are deleted when the &vmguest; is terminated.
     These disks are bound to the &compnode; on which the
     &vmguest; has been started. The disks are created under
     <filename>/var/lib/nova</filename> on the &compnode;. Your
     &compnode;s should be equipped with enough disk space to store the
     root images and ephemeral disks.
    </para>
    <note>
     <title>Ephemeral Disks vs. Block Storage</title>
     <para>
      Do not confuse ephemeral disks with persistent block storage. In
      addition to an ephemeral disk, which is automatically provided with
      most &vmguest; flavors, you can optionally add a persistent storage
      device provided by &o_blockstore;. Ephemeral disks are deleted when
      the &vmguest; terminates, while persistent storage devices can be
      reused in another &vmguest;.
     </para>
    </note>
    <para>
     The maximum disk space required on a compute node depends on the
     available flavors. A flavor specifies the number of CPUs, RAM, and disk
     size of an &vmguest;. Several flavors ranging from
     <guimenu>tiny</guimenu> (1 CPU, 512 MB RAM, no ephemeral disk) to
     <guimenu>xlarge</guimenu> (8 CPUs, 8 GB RAM, 10 GB ephemeral disk) are
     available by default. Adding custom flavors, and editing and deleting
     existing flavors is also supported.
    </para>
    <para>
     To calculate the minimum disk space needed on a compute node, you need
     to determine the highest disk-space-to-RAM ratio from your flavors.
     For example:
    </para>
    <simplelist>
     <member>
      Flavor small: 2 GB RAM, 100 GB ephemeral disk =&gt; 50 GB disk /1 GB RAM
     </member>
     <member>
      Flavor large: 8 GB RAM, 200 GB ephemeral disk =&gt; 25 GB disk /1 GB RAM
     </member>
    </simplelist>
    <para>
     So, 50 GB disk /1 GB RAM is the ratio that matters. If you multiply
     that value by the amount of RAM in GB available on your compute node,
     you have the minimum disk space required by ephemeral disks. Pad that
     value with sufficient space for the root disks plus a buffer to leave room for flavors with a higher disk-space-to-RAM ratio in
     the future.
    </para>
    <warning>
     <title>Overcommitting Disk Space</title>
     <para>
      The scheduler that decides in which node an &vmguest; is started
      does not check for available disk space. If there is no disk space
      left on a compute node, this will not only cause data loss on the
      &vmguest;s, but the compute node itself will also stop operating.
      Therefore you must make sure all compute nodes are equipped with
      enough hard disk space.
     </para>
    </warning>
   </sect3>
   <sect3 xml:id="sec-depl-req-storage-hardware-store">
    <title>Storage Nodes (optional)</title> <para> The block storage service
    &ceph; RBD and the object storage service &swift; need to be deployed onto
    dedicated nodes&mdash;it is not possible to mix these services. The &swift;
    component requires at least two machines (more are recommended) to store
    data redundantly. For information on hardware requirements for &ceph;, see
    <link
    xlink:href="https://www.suse.com/documentation/ses-4/book_storage_admin/data/cha_ceph_sysreq.html"/>
    </para>
    <para>
     Each &ceph;/&swift; &stornode; needs at least two hard disks.
     The first one will be used for the operating system installation, while
     the others can be used for storage. We recommend equipping
     the storage nodes with as many disks as possible.
    </para>
    <para>
     Using RAID on &o_objstore; storage nodes is not supported.
     &swift; takes care of redundancy and replication on its own. Using
     RAID with &o_objstore; would also result in a huge performance
     penalty.
    </para>
   </sect3>
  </sect2>
 </sect1>
 <sect1 xml:id="sec-depl-req-ssl">
  <title>SSL Encryption</title>

  <para>
   Whenever non-public data travels over a network it must be encrypted.
   Encryption protects the integrity and confidentiality of data. Therefore
   you should enable SSL support when deploying &cloud; to production. (SSL
   is not enabled by default as it requires you to provide certificates.)
   The following services (and their APIs, if available) can use SSL:
  </para>

  <itemizedlist mark="bullet" spacing="normal">
   <listitem>
    <para>
     &o_blockstore;
    </para>
   </listitem>
   <listitem>
    <para>
     &o_dash;
    </para>
   </listitem>
   <listitem>
    <para>
     &o_img;
    </para>
   </listitem>
   <listitem>
    <para>
     &o_orch;
    </para>
   </listitem>
   <listitem>
    <para>
     &o_ident;
    </para>
   </listitem>
   <listitem>
    <para>
     &o_sharefs;
    </para>
   </listitem>
   <listitem>
    <para>
     &o_netw;
    </para>
   </listitem>
   <listitem>
    <para>
     &o_comp;
    </para>
   </listitem>
   <listitem>
    <para>
     &swift;
    </para>
   </listitem>
   <listitem>
    <para>
     VNC
    </para>
   </listitem>
   <listitem>
    <para>
    &rabbit;
    </para>
   </listitem>
   <listitem>
    <para>
    &o_iron;
    </para>
   </listitem>
   <listitem>
    <para>
    &o_container;
    </para>
   </listitem>
   </itemizedlist>

  <para>
   You have two options for deploying your SSL certificates. You may use a single shared certificate for all services on each node, or provide individual certificates for each service. The minimum requirement is a single certificate for the Control Node and all services installed on it.
  </para>

  <para>
   Certificates must be signed by a trusted authority. Refer to
   <link xlink:href="&suse-onlinedoc;/sles-12/book_sle_admin/data/sec_apache2_ssl.html"/>
   for instructions on how to create and sign them.
  </para>

  <important>
   <title>Host Names</title>
   <para>
    Each SSL certificate is issued for a certain host name and, optionally,
    for alternative host names (via the <literal>AlternativeName</literal>
    option). Each publicly available node in &cloud; has two host
    names&mdash;an internal and a public one. The SSL certificate needs
    to be issued for <emphasis>both</emphasis> internal and public names.
   </para>
   <para>
    The internal name has the following scheme:
   </para>
<screen>d<replaceable>MACADDRESS</replaceable>.<replaceable>FQDN</replaceable></screen>
   <para>
    <replaceable>MACADDRESS</replaceable> is the MAC address of the
    interface used to boot the machine via PXE. All letters are turned
    lowercase and all colons are replaced with dashes. For example,
    <literal>00-00-5E-00-53-00</literal>. <replaceable>FQDN</replaceable> is
    the fully qualified domain name. An example name looks like this:
   </para>
<screen>d00-00-5E-00-53-00.&exampledomain;</screen>
   <para>
    Unless you have entered a custom <guimenu>Public Name</guimenu> for a
    client (see <xref linkend="sec-depl-inst-nodes-install"/> for details),
    the public name is the same as the internal name prefixed by
    <literal>public</literal>:
   </para>
<screen>public-d00-00-5E-00-53-00.&exampledomain;</screen>
   <para>
    To look up the node names open the &crow; Web interface and click the
    name of a node in the <guimenu>Node Dashboard</guimenu>. The names are
    listed as <guimenu>Full Name</guimenu> and <guimenu>Public
    Name</guimenu>.
   </para>
  </important>
 </sect1>
 <sect1 xml:id="sec-depl-req-hardware">
  <title>Hardware Requirements</title>

  <para>
   Precise hardware requirements can only be listed for the &admserv; and
   the &ostack; &contrnode;. The requirements of the &ostack;
   Compute and &stornode;s depends on the number of concurrent
   &vmguest;s and their virtual hardware equipment.
  </para>

  <para>
   A minimum of three machines are required for a &cloud;:
   one &admserv;, one &contrnode;, and one &compnode;. You also need a gateway providing access to the public network.
   Deploying storage requires additional nodes: at least two nodes for
   &o_objstore; and a minimum of four nodes for &ceph;.
  </para>

  <important>
   <title>Virtual/Physical Machines and Architecture</title>
   <para>
    Deploying &cloud; functions to virtual machines is only supported for the
    &admserv;&mdash;all other nodes need to be physical hardware. Although the
    &contrnode; can be virtualized in test environments, this is not
    supported for production systems.
   </para>
   <para>
    &cloud; currently only runs on <literal>x86_64</literal> hardware.
   </para>
  </important>

  <sect2 xml:id="sec-depl-req-hardware-admserv">
   <title>&admserv;</title>
   <itemizedlist mark="bullet" spacing="normal">
    <listitem>
     <para>
      Architecture: x86_64.
     </para>
    </listitem>
    <listitem>
     <para>
      RAM: at least 4 GB, 8 GB recommended. The demand for memory depends on
      the total number of nodes in &cloud;&mdash;the higher the number of
      nodes, the more RAM is needed. A deployment with 50 nodes requires a
      minimum of 24 GB RAM for each &contrnode;.
     </para>
    </listitem>
    <listitem>
     <para>
      Hard disk: at least 50 GB. We recommend putting
      <filename>/srv</filename> on a separate partition with at least
      additional 30 GB of space. Alternatively, you can mount the update
      repositories from another server (see <xref
      linkend="sec-depl-req-repos"/> for details).
     </para>
    </listitem>
    <listitem>
     <para>
      Number of network cards: 1 for single and dual mode, 2 or more for
      team mode. Additional networks such as the bastion network and/or a
      separate BMC network each need an additional network card. See
      <xref linkend="sec-depl-req-network"/> for details.
     </para>
    </listitem>
    <listitem>
     <para>
      Can be deployed on physical hardware or a virtual machine.
     </para>
    </listitem>
   </itemizedlist>
  </sect2>

  <sect2 xml:id="sec-depl-req-hardware-contrnode">
   <title>&contrnode;</title>
   <itemizedlist mark="bullet" spacing="normal">
    <listitem>
     <para>
      Architecture: x86_64.
     </para>
    </listitem>
    <listitem>
     <para>
      RAM: at least 8 GB, 12 GB when deploying a single &contrnode;, and 32 GB
      recommended.
     </para>
    </listitem>
    <listitem>
     <para>
      Number of network cards: 1 for single mode, 2 for dual mode, 2 or more
      for team mode. See <xref linkend="sec-depl-req-network"/> for details.
     </para>
    </listitem>
    <listitem>
     <para>
      Hard disk: See
      <xref linkend="sec-depl-req-storage-hardware-control"/>.
     </para>
    </listitem>
   </itemizedlist>
  </sect2>

  <sect2 xml:id="sec-depl-req-hardware-compnode">
   <title>&compnode;</title>
   <para>
    The &compnode;s need to be equipped with a sufficient amount of RAM
    and CPUs, matching the numbers required by the maximum number of
    &vmguest;s running concurrently. An &vmguest; started in
    &cloud; cannot share resources from several physical nodes. It uses
    the resources of the node on which it was started. So if you
    offer a flavor (see <xref linkend="gloss-flavor"/> for a definition)
    with 8 CPUs and 12 GB RAM, at least one of your nodes should be able to
    provide these resources. Add 1 GB RAM for every two nodes
    (including &contrnode;s and &stornode;s) deployed in your cloud.
   </para>
   <para>
    See <xref linkend="sec-depl-req-storage-hardware-compute"/> for storage
    requirements.
   </para>
  </sect2>

  <sect2 xml:id="sec-depl-req-hardware-stornode">
   <title>&stornode;</title>
   <para>
    Usually a single CPU and a minimum of 4 GB RAM are sufficient for the &stornode;s. Memory requirements increase depending on the total number of
    nodes in &cloud;&mdash;the higher the number of nodes, the more RAM you need. A deployment with 50 nodes requires a minimum of 20 GB for each
    &stornode;. If you use &ceph; as storage, the storage nodes should be
    equipped with an additional 2 GB RAM per OSD (&ceph; object storage
    daemon).
   </para>
   <para>
    For storage requirements, see <xref linkend="sec-depl-req-storage-hardware-store"/>.
   </para>
  </sect2>

  <sect2 xml:id="sec-depl-req-hardware-monnode">
   <title>&monnode;</title>
   <para>
     The &monnode; is a dedicated physical machine that runs the
     <literal>monasca-server</literal> role. This node is used for
     &productname; Monitoring.  Hardware requirements for the &monnode; are as
     follows:
   </para>
   <itemizedlist>
    <listitem>
     <para>
       Architecture: x86_64
     </para>
     </listitem>
    <listitem>
     <para>
       RAM: At least 32 GB, 64 GB or more is recommended
     </para>
     </listitem>
    <listitem>
     <para>
       CPU: At least 8 cores, 16 cores or more is recommended
     </para>
     </listitem>
    <listitem>
     <para>
       Hard Disk: SSD is strongly recommended
     </para>
     </listitem>
   </itemizedlist>
   <para>
     The following formula can be used to calculate the required disk space:
   </para>
   <screen>200 GB + ["number of nodes" * "retention period" * ("space for log
   data/day" + "space for metrics data/day") ]</screen>
   <para>
    The recommended values for the formula are as follows:
   </para>
    <itemizedlist>
    <listitem>
     <para>
       Retention period = 60 days for InfluxDB and &elasticsearch;
     </para>
     </listitem>
    <listitem>
     <para>
       Space for daily log data = 2GB
     </para>
     </listitem>
    <listitem>
     <para>
       Space for daily metrics data = 50MB
     </para>
     </listitem>
   </itemizedlist>
   <para>
     The formula is based on the following log data assumptions:
   </para>
   <itemizedlist>
    <listitem>
     <para>
       Approximately 50 log files per node
     </para>
     </listitem>
    <listitem>
     <para>
       Approximately 1 log entry per file per sec
     </para>
     </listitem>
    <listitem>
     <para>
       200 bytes in size
     </para>
     </listitem>
   </itemizedlist>
   <para>
     The formula is based on the following metrics data assumptions:
   </para>
   <itemizedlist>
   <listitem>
     <para>
       400 metrics per node
     </para>
     </listitem>
   <listitem>
     <para>
      Time interval of 30 seconds
     </para>
     </listitem>
   <listitem>
     <para>
      20 bytes in size
     </para>
     </listitem>
   </itemizedlist>
   <para>
    The formula provides only a rough estimation of the required disk
    space. There are several factors that can affect disk space
    requirements. This includes the exact combination of services that run on
    your &ostack; node actual cloud usage pattern, and whether any or all
    services have debug logging enabled.
   </para>
  </sect2>

 </sect1>
 <sect1 xml:id="sec-depl-req-software">
  <title>Software Requirements</title>

  <para>
   All nodes and the &admserv; in &cloud; run on &cloudos;. Subscriptions for
   the following components are available as one- or three-year subscriptions
   including priority support:
  </para>

  <itemizedlist mark="bullet" spacing="normal">
   <listitem>
    <para>
     &productname; &contrnode; + &productname; &admserv; (including
     entitlements for High Availability and &cloudos;)
    </para>
   </listitem>
   <listitem>
    <para>
     Additional &productname; &contrnode; (including
     entitlements for High Availability and &cloudos;)
    </para>
   </listitem>
   <listitem>
    <para>
     &productname; &compnode; (excluding entitlements for High Availability and
     &cloudos;)
    </para>
   </listitem>
   <listitem>
    <para>
     &productname; &swift; node (excluding entitlements for High Availability
     and &cloudos;)
    </para>
   </listitem>
  </itemizedlist>

  <para>
   &cloudos;, HA entitlements for &compnode;s and &swift; &stornode;s, and entitlements for guest operating systems need
   to be purchased separately. Refer to
   <link xlink:href="http://www.suse.com/products/suse-openstack-cloud/how-to-buy/"/>
   for more information on licensing and pricing.
  </para>
  <para>
   Running an external &ceph; cluster (optional) with &cloud;  requires an additional &storage;
   subscription. Refer to <link
   xlink:href="https://www.suse.com/products/suse-enterprise-storage/"/> and
   <link
   xlink:href="https://www.suse.com/products/suse-openstack-cloud/frequently-asked-questions"/>
   for more information.
  </para>

  <important>
   <title>&suse; Account</title>
   <para>
    A &suse; account is needed for product registration and access to
    update repositories. If you do not already have one, go to
    <link xlink:href="http://www.suse.com/"/> to create it.
   </para>
  </important>

  <sect2 xml:id="sec-depl-req-software-optional">
   <title>Optional Component: &storage;</title>
   <para>
    &productname; can be extended by &storage; for setting up a &ceph; cluster
    providing block storage services. To store virtual disks for &vmguest;s, &cloud; uses block storage provided by the &o_blockstore;
    module. &o_blockstore; itself needs a back-end providing storage. In
    production environments this usually is a network storage
    solution. &o_blockstore; can use a variety of network storage back-ends, among them solutions from EMC, Fujitsu, or NetApp. In case your
    organization does not provide a network storage solution that can be used
    with &cloud;, you can set up a &ceph; cluster with &storage;. &storage;
    provides a reliable and fast distributed storage architecture using
    commodity hardware platforms.
   </para>
  </sect2>

  <sect2 xml:id="sec-depl-req-repos">
   <title>Product and Update Repositories</title>
   <para>
    You need seven software repositories to deploy &cloud; and to keep a running &cloud; up-to-date. This includes the static product repositories, which do not change over the
    product life cycle, and the update repositories, which constantly change.
    The following repositories are needed:
   </para>
   <variablelist>
    <title>Mandatory Repositories</title>
    <varlistentry>
     <term>&cloudos; Product</term>
     <listitem>
      <para>
       The &cloudos; product repository is a copy of the installation
       media (DVD #1) for &sls;. As of &productname;
       8, it is required to have it available locally on the
       &admserv;. This repository requires approximately 3.5 GB of hard
       disk space.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>&productname; &productnumber; Product</term>
     <listitem>
      <para>
       The &productname; &productnumber; product repository is a copy
       of the installation media (DVD #1) for &cloud;. It can either be
       made available remotely via HTTP, or locally on the &admserv;. We recommend the latter since it makes the setup of the &admserv;
       easier. This repository requires approximately 500 MB of hard disk
       space.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>PTF</term>
     <listitem>
      <para>
       This repository is created automatically on the &admserv; when you install the
       &cloud; add-on product. It serves as a repository for
       <quote>Program Temporary Fixes</quote> (PTF), which are part of the
       &suse; support program.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>&sle_repo;-Pool and &cloud_repo;-Pool</term>
     <listitem>
      <para>
       The &sls; and &productname; repositories contain all binary
       RPMs from the installation media, plus pattern information and
       support status metadata. These repositories are served from &scc;
       and need to be kept in synchronization with their sources. Make them  available remotely via an existing &smt; or &susemgr;
       server. Alternatively, make them available locally on the &admserv;
       by installing a local &smt; server, by mounting or synchronizing a
       remote directory, or by copying them.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>&sle_repo;-Updates and &cloud_repo;-Updates</term>
     <listitem>
      <para>
       These repositories contain maintenance updates to packages in the
       corresponding Pool repositories. These repositories are served from
       &scc; and need to be kept synchronized with their sources. Make them available remotely via an existing &smt; or
       &susemgr; server, or locally on the &admserv; by installing a
       local &smt; server, by mounting or synchronizing a remote
       directory, or by regularly copying them.
      </para>
     </listitem>
    </varlistentry>
   </variablelist>
   <para>
    As explained in <xref linkend="sec-depl-req-ha"/>, &contrnode;s in &cloud;
    can optionally be made highly available with the &sle;
    &hasi;. The following repositories are
    required to deploy &slsa; &hasi; nodes:
   </para>
   <variablelist>
    <title>Optional Repositories</title>
    <varlistentry>
     <term>&sleha_repo;-Pool</term>
     <listitem>
      <para>
       The pool repositories contain all binary RPMs from the installation
       media, plus pattern information and support status metadata. These
       repositories are served from &scc; and need to be kept in
       synchronization with their sources. Make them available
       remotely via an existing &smt; or &susemgr; server. Alternatively, make
       them available locally on the &admserv; by installing a local &smt; server, by
       mounting or synchronizing a remote directory, or by copying them.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>&sleha_repo;-Updates</term>
     <listitem>
      <para>
       These repositories contain maintenance updates to packages in the
       corresponding pool repositories. These repositories are served from
       &scc; and need to be kept synchronized with their sources. Make them
       available remotely via an existing &smt; or &susemgr; server, or locally
       on the &admserv; by installing a local &smt; server, by mounting or
       synchronizing a remote directory, or by regularly copying them.
      </para>
     </listitem>
    </varlistentry>
   </variablelist>
   <para>
    The product repositories for &cloudos; and
    &productname; &productnumber; do not change during the life cycle
    of a product. Thus, they can be copied to the destination directory from the
    installation media. However, the pool and update repositories must be
    kept synchronized with their sources on the &scc;. &suse;
    offers two products that synchronize repositories and make
    them available within your organization: &susemgr;
    (<link xlink:href="http://www.suse.com/products/suse-manager/"/>, and
    &smtool; (which ships with &cloudos;).
   </para>
   <para>
    All repositories must be served via HTTP to be
    available for &cloud; deployment. Repositories that are installed on the &admserv; are made available by the Apache Web
    server running on the &admserv;. If your organization already uses
    &susemgr; or &smt;, you can use the repositories provided by these
    servers.
   </para>
   <para>
    Making the repositories locally available on the &admserv; has the
    advantage of a simple network setup within &cloud;, and it allows you to
    seal off the &cloud; network from other networks in your
    organization. Hosting the repositories on a remote server has
    the advantage of using existing resources and services, and it makes
    setting up the &admserv; much easier. However, this requires a custom network setup
    for &cloud;, since the &admserv; needs access to the remote
    server.
   </para>
   <variablelist>
    <varlistentry>
     <term>Installing a &smtool; (&smt;) Server on the &admserv;</term>
     <listitem>
      <para>
       The &smt; server, shipping with &cloudos;, regularly
       synchronizes repository data from &scc; with your local host.
       Installing the &smt; server on the &admserv; is recommended if
       you do not have access to update repositories from elsewhere within
       your organization. This option requires the &admserv; to have Internet access.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>Using a Remote &smt; Server</term>
     <listitem>
      <para>
       If you already run an &smt; server within your organization, you
       can use it within &cloud;. When using a remote &smt; server,
       update repositories are served directly from the &smt; server.
       Each node is configured with these repositories upon its initial
       setup.
      </para>
      <para>
       The &smt; server needs to be accessible from the &admserv; and
       all nodes in &cloud; (via one or more gateways). Resolving the
       server's host name also needs to work.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>Using a &susemgr; Server</term>
     <listitem>
      <para>
       Each client that is managed by &susemgr; needs to register with the
       &susemgr; server. Therefore the &susemgr; support can only be installed
       after the nodes have been deployed. &cloudos; must be set up
       for autoinstallation on the &susemgr; server in order to use repositories
       provided by &susemgr; during node deployment.
      </para>
      <para>
       The server needs to be accessible from the &admserv; and all nodes
       in &cloud; (via one or more gateways). Resolving the server's host
       name also needs to work.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>Using Existing Repositories</term>
     <listitem>
      <para>
       If you can access existing repositories from within your company
       network from the &admserv;, you have the following options: mount, synchronize, or
       manually transfer these repositories to the required locations on the
       &admserv;.
      </para>
     </listitem>
    </varlistentry>
   </variablelist>
  </sect2>
 </sect1>
 <sect1 xml:id="sec-depl-req-ha">
  <title>&ha;</title>

  <para>
   Several components and services in &productname; are potentially single
   points of failure that may cause system downtime and data loss if they
   fail.
  </para>

  <para>
   &productname; provides various mechanisms to ensure that the crucial
   components and services are highly available. The following sections
   provide an overview of components on each node that can be made highly available. For making the &contrnode; functions and the
   &compnode;s highly available, &productname; uses the cluster software &sle;
   &hasi;. Make sure to thoroughly read <xref
   linkend="sec-depl-reg-ha-general"/> to learn about additional requirements for high availability deployments.
  </para>

  <sect2 xml:id="sec-depl-req-ha-admin">
   <title>&ha; of the &admserv;</title>
   <para>
    The &admserv; provides all services needed to manage and deploy all
    other nodes in the cloud. If the &admserv; is not available, new
    cloud nodes cannot be allocated, and you cannot add new roles to cloud
    nodes.
   </para>
   <para>
    However, only two services on the &admserv; are single points of
    failure, without which the cloud cannot continue to run properly: DNS
    and NTP.
   </para>
   <sect3 xml:id="sec-depl-req-ha-admin-spof">
    <title>&admserv;&mdash;Avoiding Points of Failure</title>
    <para>
     To avoid DNS and NTP as potential points of failure, deploy the roles
     <systemitem>dns-server</systemitem> and
     <systemitem>ntp-server</systemitem> to multiple nodes.
    </para>
    <note>
     <title>Access to External Network</title>
     <para>
      If any configured DNS forwarder or NTP external server is not
      reachable through the admin network from these nodes, allocate an
      address in the public network for each node that has the
      <systemitem>dns-server</systemitem> and
      <systemitem>ntp-server</systemitem> roles:
     </para>
<screen>crowbar network allocate_ip default `hostname -f` public host</screen>
     <para>
      Then the nodes can use the public gateway to reach the external
      servers. The change will only become effective after the next run of
      <command>chef-client</command> on the affected nodes.
     </para>
    </note>
   </sect3>
<!-- dpopov 2018-05-24:
     Update when SOC7->SOC8 instructions are available
-->
<!--   <sect3 xml:id="sec-depl-req-ha-admin-recover">
    <title>&admserv;&mdash;Recovery</title>
    <para>
     To minimize recovery time for the &admserv;, follow the backup and
     restore recommendations described in
     <xref linkend="sec-depl-maintenance-backup-admin"/>.
    </para>
   </sect3> -->
  </sect2>

  <sect2 xml:id="sec-depl-reg-ha-control">
   <title>&ha; of the &contrnode;(s)</title>
   <para>
    The &contrnode;(s) usually run a variety of services without which
    the cloud would not be able to run properly.
   </para>
   <sect3 xml:id="sec-depl-reg-ha-control-spof">
    <title>&contrnode;(s)&mdash;Avoiding Points of Failure</title>
    <para>
     To prevent the cloud from avoidable downtime if one or more
     &contrnode;s fail, you can make the
     following roles highly available: <remark condition="clarity">2016-01-08
     - fs: FIXME: Check list of services</remark>
    </para>
    <itemizedlist mark="bullet" spacing="normal">
     <listitem>
      <para>
       <systemitem>database-server</systemitem>
       (<systemitem>database</systemitem> &barcl;)
      </para>
     </listitem>
     <listitem>
      <para>
       <systemitem>keystone-server</systemitem>
       (<systemitem>keystone</systemitem> &barcl;)
      </para>
     </listitem>
     <listitem>
      <para>
       <systemitem>rabbitmq-server</systemitem>
       (<systemitem>rabbitmq</systemitem> &barcl;)
      </para>
     </listitem>
     <listitem>
      <para>
       <systemitem>swift-proxy</systemitem> (<systemitem>swift</systemitem>
       &barcl;)
      </para>
     </listitem>
     <listitem>
      <para>
       <systemitem>glance-server</systemitem>
       (<systemitem>glance</systemitem> &barcl;)
      </para>
     </listitem>
     <listitem>
      <para>
       <systemitem>cinder-controller</systemitem>
       (<systemitem>cinder</systemitem> &barcl;)
      </para>
     </listitem>
     <listitem>
      <para>
       <systemitem>neutron-server</systemitem>
       (<systemitem>neutron</systemitem> &barcl;)
      </para>
     </listitem>
     <listitem>
      <para>
       <systemitem>neutron-network</systemitem> (<systemitem>neutron</systemitem>
       &barcl;)
      </para>
     </listitem>
     <listitem>
      <para>
       <systemitem>nova-controller</systemitem>
       (<systemitem>nova</systemitem> &barcl;)
      </para>
     </listitem>
     <listitem>
      <para>
       <systemitem>nova_dashboard-server</systemitem>
       (<systemitem>nova_dashboard</systemitem> &barcl;)
      </para>
     </listitem>
     <listitem>
      <para>
       <systemitem>ceilometer-server</systemitem>
       (<systemitem>ceilometer</systemitem> &barcl;)
      </para>
     </listitem>
     <listitem>
      <para>
       <systemitem>ceilometer-polling</systemitem>
       (<systemitem>ceilometer</systemitem> &barcl;)
      </para>
     </listitem>
     <listitem>
      <para>
       <systemitem>heat-server</systemitem> (<systemitem>heat</systemitem>
       &barcl;)
      </para>
     </listitem>
    </itemizedlist>
    <para>
     Instead of assigning these roles to individual cloud nodes, you can
     assign them to one or several &ha; clusters. &productname; will
     then use the Pacemaker cluster stack (shipped with the &sle;
     &hasi;) to manage the services. If one &contrnode; fails, the services
     will fail over to another
     &contrnode;. For details on the
     Pacemaker cluster stack and the &sle; &hasi;, refer to the
     &haguide;, available at
     <link xlink:href="&suse-onlinedoc;/sle-ha-12/"/>.
     Note that &sle; &hasi; includes &lvs; as the load-balancer, and
     &productname; uses &haproxy; for this purpose
     (<link xlink:href="http://haproxy.1wt.eu/"/>).
    </para>
    <note>
     <title>Recommended Setup</title>
     <para>
      Though it is possible to use the same cluster for all of the roles
      above, the recommended setup is to use three clusters and to deploy
      the roles as follows:
     </para>
     <itemizedlist mark="bullet" spacing="normal">
      <listitem>
       <para>
        <literal>data</literal> cluster:
        <systemitem>database-server</systemitem> and
        <systemitem>rabbitmq-server</systemitem>
       </para>
      </listitem>
      <listitem>
       <para>
        <literal>network</literal> cluster:
        <systemitem>neutron-network</systemitem> (as the
        <systemitem>neutron-network</systemitem> role may result in heavy network
        load and CPU impact)
       </para>
      </listitem>
      <listitem>
       <para>
        <systemitem>services</systemitem> cluster: all other roles listed
        above (as they are related to API/schedulers)
       </para>
      </listitem>
     </itemizedlist>
     <!--taroth 2016-02-24: fix for https://bugzilla.suse.com/show_bug.cgi?id=881510-->
     <para>&productname; does not support &ha; for the LBaaS service plug-in.
      Thus, failover of a neutron load-balancer to another node
      can only be configured manually by editing the database.</para>
    </note>
    <important>
     <title>Cluster Requirements and Recommendations</title>
     <para>
      For setting up the clusters, some special requirements and
      recommendations apply. For details, refer to
      <xref linkend="sec-depl-reg-ha-general"/>.
     </para>
    </important>
   </sect3>
   <sect3 xml:id="sec-depl-reg-ha-control-recover">
    <title>&contrnode;(s)&mdash;Recovery</title>
    <para>
     Recovery of the &contrnode;(s) is done automatically by the cluster
     software: if one &contrnode; fails, Pacemaker will fail over the
     services to another &contrnode;. If a failed &contrnode; is
     repaired and rebuilt via &crow;, it will be automatically configured
     to join the cluster. At this point Pacemaker will have the option to
     fail back services if required.
    </para>
   </sect3>
  </sect2>

  <sect2 xml:id="sec-depl-reg-ha-compute">
   <title>&ha; of the &compnode;(s)</title>
   <para>
    If a &compnode; fails, all VMs running on that node will go down. While it cannot protect against failures of individual VMs, a
    &ha; setup for &compnode;s helps to minimize VM downtime caused by
    &compnode; failures.  If the <literal>nova-compute</literal> service or
    <literal>libvirtd</literal> fail on a &compnode;, Pacemaker will
    try to automatically recover them.  If recovery fails, or the
    node itself should become unreachable, the node will be fenced and the
    VMs will be moved to a different &compnode;.
   </para>
   <para>
    If you decide to use &ha; for &compnode;s, your &compnode; will
    be run as Pacemaker remote nodes. With the <literal>pacemaker-remote</literal>
    service, &ha; clusters can be extended to control remote nodes without any
    impact on scalability, and without having to install the full cluster stack
    (including <literal>corosync</literal>) on the remote nodes.  Instead, each
    &compnode; only runs the <literal>pacemaker-remote</literal> service. The service
    acts as a proxy, allowing the cluster stack on the <quote>normal</quote>
    cluster nodes to connect to it and to control services remotely. Thus, the
    node is effectively integrated into the cluster as a remote node. In this way,
    the services running on the &ostack; compute nodes can be controlled from the core
    Pacemaker cluster in a lightweight, scalable fashion.
   </para>
   <para> Find more information about the <literal>pacemaker_remote</literal>
    service in
    <citetitle>Pacemaker Remote&mdash;Extending High Availability into
    Virtual Nodes</citetitle>,
    available at <link xlink:href="http://www.clusterlabs.org/doc/"/>. </para>
   <para>To configure &ha; for &compnode;s, you need to adjust the following
   &barcl; proposals:</para>
   <itemizedlist>
    <listitem>
     <para>Pacemaker&mdash;for details, see <xref
       linkend="sec-depl-ostack-pacemaker"/>.</para>
    </listitem>
    <listitem>
     <para>nova&mdash;for details, see <xref linkend="sec-depl-ostack-nova-ha"
      />.</para>
    </listitem>
   </itemizedlist>
  </sect2>

  <sect2 xml:id="sec-depl-reg-ha-storage">
   <title>&ha; of the &stornode;(s)</title>
   <para>
    &productname; offers two different types of storage that can be used
    for the &stornode;s: object storage (provided by the &ostack;
    &swift; component) and block storage (provided by &ceph;).
   </para>
   <para>
    Both already consider &ha; aspects by design, therefore it does not
    require much effort to make the storage highly available.
   </para>
   <sect3 xml:id="sec-depl-reg-ha-storage-swift">
    <title>&swift;&mdash;Avoiding Points of Failure</title>
    <para>
     The &ostack; &objstore; replicates the data by design, provided
     the following requirements are met:
    </para>
    <itemizedlist mark="bullet" spacing="normal">
     <listitem>
      <para>
       The option <guimenu>Replicas</guimenu> in the &o_objstore;
       &barcl; is set to <literal>3</literal>, the tested and recommended
       value.
      </para>
     </listitem>
     <listitem>
      <para>
       The number of &stornode;s needs to be greater than the value set in
       the <guimenu>Replicas</guimenu> option.
      </para>
     </listitem>
    </itemizedlist>
    <procedure>
     <step>
      <para>
       To avoid single points of failure, assign the
       <systemitem>swift-storage</systemitem> role to multiple nodes.
      </para>
     </step>
     <step>
      <para>
       To make the API highly available, assign the
       <systemitem>swift-proxy</systemitem> role to a cluster instead of
       assigning it to a single &contrnode;. See
       <xref linkend="sec-depl-reg-ha-control-spof"/>. Other swift roles
       must not be deployed on a cluster.
      </para>
     </step>
    </procedure>
   </sect3>
   <sect3 xml:id="sec-depl-reg-ha-storage-ceph">
    <title>&ceph;&mdash;Avoiding Points of Failure</title>
    <para>
     &ceph; is a distributed storage solution that can provide &ha;.  For &ha;
     redundant storage and monitors need to be configured in the &ceph;
     cluster. For more information refer to the &storage; documentation at
     <link xlink:href="&suse-onlinedoc;/suse-enterprise-storage-5/index.html"/>.
    </para>
   </sect3>
  </sect2>

  <sect2 xml:id="sec-depl-reg-ha-general">
   <title>Cluster Requirements and Recommendations</title>
   <para>
    When considering setting up one or more &ha; clusters, refer to the
    chapter <citetitle>System Requirements</citetitle> in the
    &haguide; for &sle; &hasi;. The guide is available at
    <link xlink:href="&suse-onlinedoc;/sle-ha-12/"/>.
   </para>
   <para>
    The HA requirements for &contrnode; also apply to &productname;. Note that by
    buying &productname;, you automatically get an entitlement for
    &sle; &hasi;.
   </para>
   <para>
    Especially note the following requirements:
   </para>
   <variablelist>
    <varlistentry xml:id="vle-ha-req-nodes">
     <term>Number of Cluster Nodes</term>
     <listitem>
      <para>
       Each cluster needs to consist of at least three cluster nodes.
      </para>
      <important>
       <title>Odd Number of Cluster Nodes</title>
       <para>
        The Galera cluster needs an <emphasis>odd</emphasis> number of cluster
        nodes with a <emphasis>minimum</emphasis> of three nodes.
       </para>
       <para>
        A cluster needs
        <xref linkend="gloss-quorum" xrefstyle="select:label nopage"/> to
        keep services running. A three-node cluster can tolerate
         failure of only one node at a time, whereas a five-node cluster can
        tolerate failures of two nodes.
       </para>
      </important>
     </listitem>
    </varlistentry>
    <varlistentry xml:id="vle-ha-req-stonith">
     <term>&stonith;</term>
     <listitem>
      <para>
       The cluster software will shut down <quote>misbehaving</quote> nodes
       in a cluster to prevent them from causing trouble. This mechanism is
       called <literal>fencing</literal> or
       <xref linkend="gloss-stonith" xrefstyle="select:title nopage"/>.
      </para>
      <important>
       <title>No Support Without &stonith;</title>
       <para>
        A cluster without &stonith; is not supported.
       </para>
      </important>
      <para>
       For a supported &hasetup;, ensure the following:
      </para>
      <itemizedlist mark="bullet" spacing="normal">
       <listitem>
        <para>
         Each node in the &ha; cluster needs to have at least one
         &stonith; device (usually a hardware device). We strongly
         recommend multiple &stonith; devices per node, unless &stonith; Block Device (SBD) is used.
        </para>
       </listitem>
       <listitem>
        <para>
         The global cluster options <systemitem>stonith-enabled</systemitem>
         and <systemitem>startup-fencing</systemitem> must be set to
         <literal>true</literal>. These options are set automatically when
         deploying the <systemitem>Pacemaker</systemitem> &barcl;. When you change them, you will lose support.
        </para>
       </listitem>
       <listitem>
        <para>
         When deploying the <literal>Pacemaker</literal> service, select a
         <xref linkend="vle-pacemaker-barcl-stonith" xrefstyle="select:label nopage"/>
         that matches your setup. If your &stonith; devices support the
         IPMI protocol, choosing the IPMI option is the easiest way to
         configure &stonith;. Another alternative is SBD. It provides a way to enable &stonith; and fencing
         in clusters without external power switches, but it requires shared
         storage. For SBD requirements, see
         <link xlink:href="http://linux-ha.org/wiki/SBD_Fencing"/>, section
         <citetitle>Requirements</citetitle>.
        </para>
       </listitem>
      </itemizedlist>
      <para>
       For more information, refer to the &haguide;, available at
       <link xlink:href="&suse-onlinedoc;/sle-ha-12/"/>.
       Especially read the following chapters: <citetitle>Configuration and
       Administration Basics</citetitle>, and <citetitle>Fencing and
       &stonith;</citetitle>, <citetitle> Storage Protection</citetitle>.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry xml:id="vle-ha-req-communication">
     <term>Network Configuration</term>
     <listitem>
      <important>
       <title>Redundant Communication Paths</title>
       <para>
        For a supported &hasetup;, it is required to set up cluster
        communication via two or more redundant paths. For this purpose, use
        network device bonding and team network mode in your &crow; network setup. For details, see
        <xref linkend="sec-depl-req-network-modes-teaming"/>. At least two
        Ethernet cards per cluster node are required for network redundancy.
        We advise using team network mode everywhere (not only
        between the cluster nodes) to ensure redundancy.
       </para>
      </important>
      <para>
       For more information, refer to the &haguide;, available at
       <link xlink:href="&suse-onlinedoc;/sle-ha-12/"/>.
       Especially read the following chapter: <citetitle>Network Device
       Bonding</citetitle>.
      </para>
      <para>
       Using a second communication channel (ring) in &corosync; (as an
       alternative to network device bonding) is not supported yet in &productname;.
       By default, &productname; uses the admin network (typically
       <literal>eth0</literal>) for the first &corosync; ring.
      </para>
      <important>
       <title>Dedicated Networks</title>
        <para>
         The <literal>corosync</literal> network communication layer is
         crucial to the health of the cluster. <literal>corosync</literal> traffic always
         goes over the admin network.
        </para>
        <itemizedlist>
         <listitem>
          <para>
           Use redundant communication paths for the <literal>corosync</literal>
           network communication layer.
          </para>
         </listitem>
         <listitem>
          <para>
           Do not place the <literal>corosync</literal> network communication layer
           on interfaces shared with any other networks that could experience heavy
           load, such as the &ostack; public / private / SDN / storage networks.
          </para>
         </listitem>
        </itemizedlist>
        <para>
         Similarly, if SBD over iSCSI is used as a &stonith; device (see
         <xref linkend="vle-ha-req-stonith"/>), do not place the iSCSI traffic on
         interfaces that could experience heavy load, because this might disrupt
         the SBD mechanism.
        </para>
      </important>
     </listitem>
    </varlistentry>
    <varlistentry xml:id="vle-ha-req-storage">
     <term>Storage Requirements</term>
     <listitem>
<!-- taroth 2014-04-14: https://bugzilla.suse.com/show_bug.cgi?id=873373 -->
      <para>
       When using SBD as &stonith; device, additional requirements apply
       for the shared storage. For details, see
       <link xlink:href="http://linux-ha.org/wiki/SBD_Fencing"/>, section
       <citetitle>Requirements</citetitle>.
      </para>
     </listitem>
    </varlistentry>
   </variablelist>
  </sect2>

  <sect2 xml:id="sec-depl-reg-ha-more">
   <title>For More Information</title>
   <para>
    For a basic understanding and detailed information on the &sle;
    &hasi; (including the Pacemaker cluster stack), read the
    &haguide;. It is available at
    <link xlink:href="&suse-onlinedoc;/sle-ha-12/"/>.
   </para>
   <para>
    In addition to the chapters mentioned in
    <xref linkend="sec-depl-reg-ha-general"/>, the following
    chapters are especially recommended:
   </para>
   <itemizedlist mark="bullet" spacing="normal">
    <listitem>
     <para>
      <citetitle>Product Overview</citetitle>
     </para>
    </listitem>
    <listitem>
     <para>
      <citetitle>Configuration and Administration Basics</citetitle>
     </para>
    </listitem>
   </itemizedlist>
   <para>
    The &haguide; also provides comprehensive information about the
    cluster management tools with which you can view and check the cluster status
    in &productname;. They can also be used to look up details like
    configuration of cluster resources or global cluster options. Read the
    following chapters for more information:
   </para>
   <itemizedlist mark="bullet" spacing="normal">
    <listitem>
     <para>
      &haweb;: <citetitle>Configuring and Managing Cluster Resources (Web
      Interface)</citetitle>
     </para>
    </listitem>
    <listitem>
     <para>
      <command>crm.sh</command>: <citetitle> Configuring and Managing
      Cluster Resources (Command Line)</citetitle>
     </para>
    </listitem>
   </itemizedlist>
  </sect2>
 </sect1>
 <sect1 xml:id="sec-depl-req-summary">
  <title>Summary: Considerations and Requirements</title>

  <para>
   As outlined above, there are some important considerations to be made
   before deploying &cloud;. The following briefly summarizes what was
   discussed in detail in this chapter. Keep in mind that as of
   &productname; 8, it is not possible to change some
   aspects such as the network setup when &cloud; is deployed!
   <remark condition="clarity">
    2016-01-08 - fs: FIXME: A HA summary is missing
   </remark>
  </para>
  <itemizedlist mark="bullet" spacing="normal">
   <title>Network</title>
   <listitem>
    <para>
     If you do not want to stick with the default networks and addresses,
     define custom networks and addresses. You need five different networks.
     If you need to separate the admin and the BMC network, a sixth network
     is required. See <xref linkend="sec-depl-req-network"/> for details.
     Networks that share interfaces need to be configured as VLANs.
    </para>
   </listitem>
   <listitem>
    <para>
     The &cloud; networks are completely isolated, therefore it is not
     required to use public IP addresses for them. A class C network as used
     in this documentation may not provide enough addresses for a cloud that
     is supposed to grow. You may alternatively choose addresses from a
     class B or A network.
    </para>
   </listitem>
   <listitem>
    <para>
     Determine how to allocate addresses from your network. Make sure not to
     allocate IP addresses twice. See
     <xref linkend="sec-depl-req-network-allocation"/> for the default
     allocation scheme.
    </para>
   </listitem>
   <listitem>
    <para>
     Define which network mode to use. Keep in mind that all machines within
     the cloud (including the &admserv;) will be set up with the chosen
     mode and therefore need to meet the hardware requirements. See
     <xref linkend="sec-depl-req-network-modes"/> for details.
    </para>
   </listitem>
   <listitem>
    <para>
     Define how to access the admin and BMC network(s): no access from the
     outside (no action is required), via an external gateway (gateway needs
     to be provided), or via bastion network. See
     <xref linkend="sec-depl-req-network-bastion"/> for details.
    </para>
   </listitem>
   <listitem>
    <para>
     Provide a gateway to access the public network (public, nova-floating).
    </para>
<!-- TODO vuntz 2013-12-04: check if also for fixed? -->
   </listitem>
   <listitem>
    <para>
     Make sure the &admserv;'s host name is correctly configured
     (<command>hostname</command> <option>-f</option> needs to return a
     fully qualified host name). If this is not the case, run <menuchoice>
     <guimenu>&yast;</guimenu> <guimenu>Network Services</guimenu>
     <guimenu>Hostnames</guimenu></menuchoice> and add a fully qualified
     host name.
    </para>
   </listitem>
   <listitem>
    <para>
     Prepare a list of MAC addresses and the intended use of the
     corresponding host for all &ostack; nodes.
    </para>
   </listitem>
  </itemizedlist>

  <itemizedlist mark="bullet" spacing="normal">
   <title>Update Repositories</title>
   <listitem>
    <para>
     Depending on your network setup you have different options for
     providing up-to-date update repositories for &sls; and &cloud; for
     &cloud; deployment: using an existing &smt; or &susemgr;
     server, installing &smt; on the &admserv;, synchronizing data
     with an existing repository, mounting remote repositories, or using physical media. Choose the option that best matches your
     needs.
    </para>
   </listitem>
  </itemizedlist>

  <itemizedlist mark="bullet" spacing="normal">
   <title>Storage</title>
   <listitem>
    <para>
     Decide whether you want to deploy the object storage service
     &swift;. If so, you need to deploy at least two nodes with
     sufficient disk space exclusively dedicated to &swift;.
    </para>
   </listitem>
   <listitem>
    <para>
     Decide which back-end to use with &o_blockstore;. If using the
     <guimenu>raw</guimenu> back-end (local disks) we strongly
     recommend using a separate node equipped with several hard disks for
     deploying <literal>cinder-volume</literal>. &ceph; needs a minimum of four exclusive nodes with sufficient disk space.
    </para>
   </listitem>
   <listitem>
    <para>
     Make sure all &compnode;s are equipped with sufficient hard disk
     space.
    </para>
   </listitem>
  </itemizedlist>

  <itemizedlist mark="bullet" spacing="normal">
   <title>SSL Encryption</title>
   <listitem>
    <para>
     Decide whether to use different SSL certificates for the services and
     the API, or whether to use a single certificate.
    </para>
   </listitem>
   <listitem>
    <para>
     Get one or more SSL certificates certified by a trusted third party
     source.
    </para>
   </listitem>
  </itemizedlist>

  <itemizedlist mark="bullet" spacing="normal">
   <title>Hardware and Software Requirements</title>
   <listitem>
    <para>
     Make sure the hardware requirements for the different node types are
     met.
    </para>
   </listitem>
   <listitem>
    <para>
     Make sure to have all required software at hand.
    </para>
   </listitem>
  </itemizedlist>
 </sect1>
 <sect1 xml:id="sec-depl-req-installation">
  <title>Overview of the &cloud; Installation</title>

  <para>
   Deploying and installing &productname; is a multi-step process.
   Start by deploying a basic &sls; installation and the
   &productname; add-on product to the &admserv;. Then the product and
   update repositories need to be set up and the &cloud; network needs to
   be configured. Next, complete the &admserv; setup. After the
   &admserv; is ready, you can start deploying and configuring the
   &ostack; nodes. The complete node deployment is done automatically via
   &crow; and &chef; from the &admserv;. All you need to do is to
   boot the nodes using PXE and to deploy the &ostack; components to them.
  </para>

  <procedure>
   <step>
    <para>
     Install &cloudos; on the &admserv; with the add-on product
     &cloud;. Optionally select the &smtool; (SMT) pattern for installation. See
     <xref linkend="cha-depl-adm-inst"/>.
    </para>
   </step>
   <step>
    <para>
     Optionally set up and configure the &smt; server on the &admserv;. See
     <xref linkend="app-deploy-smt"/>.
    </para>
   </step>
   <step>
    <para>
     Make all required software repositories available on the &admserv;. See
     <xref linkend="cha-depl-repo-conf"/>.
    </para>
   </step>
   <step>
    <para>
     Set up the network on the &admserv;. See
     <xref linkend="sec-depl-adm-inst-network"/>.
    </para>
   </step>
   <step>
    <para>
     Perform the &crow; setup to configure the &cloud; network and to make the
     repository locations known. When the configuration is done, start the
     &inst_crow;. See <xref
     linkend="sec-depl-adm-inst-crowbar"/>.
    </para>
   </step>
   <step>
    <para>
     Boot all nodes onto which the &ostack; components should be deployed
     using PXE and allocate them in the &crow; Web interface to start the
     automatic &sls; installation. See
     <xref linkend="cha-depl-inst-nodes"/>.
    </para>
   </step>
   <step>
    <para>
     Configure and deploy the &ostack; components via the &crow; Web
     interface or command line tools. See <xref linkend="cha-depl-ostack"/>.
    </para>
   </step>
   <step>
    <para>
     When all &ostack; components are up and running, &cloud; is ready.
     The cloud administrator can now upload images to enable users to start
     deploying &vmguest;s. See the &cloudadmin; and the
     &cloudsuppl;.
<!--See <xref linkend="book-cloud-admin"/>.-->
    </para>
   </step>
  </procedure>
 </sect1>
</chapter>
