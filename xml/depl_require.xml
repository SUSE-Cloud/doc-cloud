<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE chapter
[
  <!ENTITY % entities SYSTEM "entity-decl.ent">
  %entities;
]>
<!-- Converted by suse-upgrade version 1.1 -->
<chapter xmlns="http://docbook.org/ns/docbook" xmlns:xi="http://www.w3.org/2001/XInclude" xmlns:xlink="http://www.w3.org/1999/xlink" version="5.0" xml:id="cha.depl.req">
 <info>
  <title>Considerations and Requirements</title>
  <abstract>
   <para>
    Before deploying &productname;, there are a few requirements to be met
    and considerations to be made. Make sure to thoroughly read this
    chapter&mdash;some decisions need to be made <emphasis>before</emphasis>
    deploying &cloud;, since you cannot change them afterwards.
   </para>
  </abstract>
 <dm:docmanager xmlns:dm="urn:x-suse:ns:docmanager">
     <dm:maintainer>fs</dm:maintainer>
     <dm:status>editing</dm:status>
     <dm:deadline/>
     <dm:priority/>
     <dm:translation>no</dm:translation>
     <dm:languages/>
 </dm:docmanager>
 </info>
 <sect1 xml:id="sec.depl.req.network">
  <title>Network</title>

  <para>
   &productname; requires a complex network setup consisting of several
   networks that are configured during installation. These networks are for
   exclusive cloud usage. To access them from an existing network, a router
   is needed.
  </para>

  <para>
   The network configuration on the nodes in the &cloud; network is
   entirely controlled by &crow;. Any network configuration not done with
   &crow; (for example, with &yast;) will automatically be
   overwritten. After the cloud is deployed, network settings cannot be
   changed anymore!
  </para>

  <figure>
   <title>&cloud; Network: Overview</title>
   <mediaobject>
    <imageobject role="fo">
     <imagedata fileref="cloud_network_overview.png" width="90%" format="PNG"/>
    </imageobject>
    <imageobject role="html">
     <imagedata fileref="cloud_network_overview.png" width="75%" format="PNG"/>
    </imageobject>
   </mediaobject>
  </figure>

  <para>
   The following networks are pre-defined when setting up &productname;.
   The IP addresses listed are the default addresses and can be changed
   using the &yast; &crow; module (see
   <xref linkend="sec.depl.adm_inst.crowbar"/>). It is also possible to
   completely customize the network setup. This requires to manually edit
   the network &barcl; template. See
   <xref linkend="app.deploy.network_json"/> for detailed instructions.
  </para>

  <variablelist>
   <varlistentry xml:id="vle.netw.adm">
    <term>
     Admin Network (192.168.124/24)
    </term>
    <listitem>
<!-- TODO vuntz 2013-12-04: for reference, we got feedback that we should not touch the IPMI configuration at all by default, so we'll likely do this; that would mean access to the BMC would not be configured by Crowbar by default -->
     <para>
      A private network to access the &admserv; and all nodes for
      administration purposes. The default setup lets you also access the
      BMC (Baseboard Management Controller) data via IPMI (Intelligent
      Platform Management Interface) from this network. If required, BMC
      access can be swapped to a separate network.
     </para>
     <para>
      You have the following options for controlling access to this network:
     </para>
     <itemizedlist mark="bullet" spacing="normal">
      <listitem>
       <para>
        do not allow access from the outside and keep the admin network
        completely separated
       </para>
      </listitem>
      <listitem>
       <para>
        allow access to the &admserv; from a single network (for example,
        your company's administration network) via the <quote>bastion
        network</quote> option configured on an additional network card with
        a fixed IP address
       </para>
      </listitem>
      <listitem>
       <para>
        allow access from one or more networks via a gateway
       </para>
      </listitem>
     </itemizedlist>
    </listitem>
   </varlistentry>
   <varlistentry xml:id="vle.netw.stor">
    <term>
     Storage Network (192.168.125/24)
    </term>
    <listitem>
     <para>
      Private, &cloud; internal virtual network. This network is used by
      &ceph; and &o_objstore; only. It should not be accessed by
      users.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>
     Private Network (nova-fixed, 192.168.123/24)
    </term>
    <listitem>
     <para>
      Private, &cloud; internal virtual network. This network is used for
      inter-&vmguest; communication and provides access to the outside
      world for the &vmguest;s. The gateway required is also
      automatically provided by &cloud;.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>
     Public Network (nova-floating, public, 192.168.126/24)
    </term>
    <listitem>
     <para>
      The only public network provided by &cloud;. You can access the
      Nova &dash; and all &vmguest;s (provided they have been equipped
      with a floating IP) on this network. This network can only be accessed
      via a gateway, which needs to be provided externally. All &cloud;
      users and administrators need to be able to access the public network.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry xml:id="vle.netw.sdn">
    <term>
     Software Defined Network (os_sdn, 192.168.130/24)
    </term>
    <listitem>
     <para>
      Private, &cloud; internal virtual network. This network is used
      when &o_netw; is configured to use openvswitch with
      <literal>GRE</literal> tunneling for the virtual networks. It should
      not be accessed by users.
     </para>
    </listitem>
   </varlistentry>
  </variablelist>

  <warning>
   <title>Protect Networks from External Access</title>
   <para>
    For security reasons, protect the following networks from external
    access:
   </para>
   <itemizedlist mark="bullet" spacing="normal">
    <listitem>
     <para>
      <xref linkend="vle.netw.adm" xrefstyle="select:label nopage"/>
     </para>
    </listitem>
    <listitem>
     <para>
      <xref linkend="vle.netw.stor" xrefstyle="select:label nopage"/>
     </para>
    </listitem>
    <listitem>
     <para>
      <xref linkend="vle.netw.sdn" xrefstyle="select:label nopage"/>
     </para>
    </listitem>
   </itemizedlist>
   <para>
    Especially traffic from the cloud instances must not be able to pass
    through these networks.
   </para>
  </warning>
  <important>
   <title>VLAN Settings</title>
   <para>
    As of &productname; &productnumber;, using a VLAN for the admin network is
    only supported on a native/untagged VLAN. If you need VLAN support for the
    admin network, it must be handled at switch level.
   </para>
   <para>
    When deploying &compnode;s with Microsoft Hyper-V or Windows Server, you
    must <emphasis>not</emphasis> use openvswitch with gre, but rather
    openvswitch with VLAN (recommended) or linuxbridge as a plugin for
    &o_netw;.
   </para>
   <para>
    When changing the network configuration with &yast; or by editing
    <filename>/etc/crowbar/network.json</filename> you can define VLAN
    settings for each network. For the networks <literal>nova-fixed</literal>
    and <literal>nova-floating</literal>, however, special rules apply:
   </para>
   <para>
    <emphasis role="bold">nova-fixed</emphasis>: The <guimenu>USE
    VLAN</guimenu> setting will be ignored. However, VLANs will automatically
    be used if deploying &o_netw; with VLAN support (using the plugins
    linuxbridge, openvswitch plus VLAN or cisco plus VLAN). In this case, you
    need to specify a correct <guimenu>VLAN ID</guimenu> for this network.
   </para>
   <para>
    <emphasis role="bold">nova-floating</emphasis>: When using a VLAN for
    <literal>nova-floating</literal> (which is the default), the <guimenu>USE
    VLAN</guimenu> and <guimenu>VLAN ID</guimenu> settings for
    <guimenu>nova-floating</guimenu> and <guimenu>public</guimenu> need to be
    the same. When not using a VLAN for <literal>nova-floating</literal>, it
    needs to use a different physical network interface than the
    <literal>nova_fixed</literal> network.
   </para>
  </important>

  <note>
   <title>No IPv6 support</title>
   <para>
    As of &productname; &productnumber;, IPv6 is not supported. This
    applies to the cloud internal networks and to the &vmguest;s.
   </para>
  </note>

  <para>
   The following diagram shows the pre-defined &cloud; network in more
   detail. It demonstrates how the &ostack; nodes and services use the
   different networks.
  </para>

  <figure>
   <title>&cloud; Network: Details</title>
   <mediaobject>
    <imageobject role="fo">
     <imagedata fileref="cloud_network_detail.png" width="100%" format="PNG"/>
    </imageobject>
    <imageobject role="html">
     <imagedata fileref="cloud_network_detail.png" width="100%" format="PNG"/>
    </imageobject>
   </mediaobject>
  </figure>

  <sect2 xml:id="sec.depl.req.network.allocation">
   <title>Network Address Allocation</title>
   <para>
    The default networks set up in &cloud; are class C networks with 256
    IP addresses each. This limits the maximum number of &vmguest;s that
    can be started simultaneously. Addresses within the networks are
    allocated as outlined in the following table. Use the &yast;
    &crow; module to customize (see
    <xref linkend="sec.depl.adm_inst.crowbar"/>). The last address in the IP
    range of each network is always reserved as the broadcast address. This
    assignment cannot be changed.
   </para>
   <note>
    <title>Limitations of the Default Network Proposal</title>
    <para>
     The default network proposal as described below limits the maximum
     number of &compnode;s to 80, the maximum number of floating IP
     addresses to 61 and the maximum number of addresses in the nova_fixed
     network to 204.
    </para>
    <para>
     To overcome these limitations you need to reconfigure the network setup
     by using appropriate address ranges. Do this by either using the
     &yast; &crow; module as described in
     <xref linkend="sec.depl.adm_inst.crowbar"/> or by manually editing the
     network template file as described in
     <xref linkend="app.deploy.network_json"/>.
    </para>
   </note>
   <table>
    <title><systemitem class="etheraddress">192.168.124.0/24</systemitem> (Admin/BMC) Network Address Allocation</title>
    <tgroup cols="3">
     <colspec colnum="1" colname="1" colwidth="20*"/>
     <colspec colnum="2" colname="2" colwidth="33*"/>
     <colspec colnum="3" colname="3" colwidth="47*"/>
     <thead>
      <row>
       <entry>
        <para>
         Function
        </para>
       </entry>
       <entry>
        <para>
         Address
        </para>
       </entry>
       <entry>
        <para>
         Remark
        </para>
       </entry>
      </row>
     </thead>
     <tbody>
      <row>
       <entry>
        <para>
         router
        </para>
       </entry>
       <entry>
        <para>
         <systemitem class="etheraddress">192.168.124.1</systemitem>
        </para>
       </entry>
       <entry>
        <para>
         Provided externally.
        </para>
       </entry>
      </row>
      <row>
       <entry>
        <para>
         admin
        </para>
       </entry>
       <entry>
        <para>
         <systemitem class="etheraddress">192.168.124.10</systemitem> -
         <systemitem class="etheraddress">192.168.124.11</systemitem>
        </para>
       </entry>
       <entry>
        <para>
         Fixed addresses reserved for the &admserv;.
        </para>
       </entry>
      </row>
      <row>
       <entry>
        <para>
         dhcp
        </para>
       </entry>
       <entry>
        <para>
         <systemitem class="etheraddress">192.168.124.21</systemitem> -
         <systemitem class="etheraddress">192.168.124.80</systemitem>
        </para>
       </entry>
       <entry>
        <para>
         Address range reserved for node allocation/installation. Determines
         the maximum number of parallel allocations/installations.
        </para>
       </entry>
      </row>
      <row>
       <entry>
        <para>
         host
        </para>
       </entry>
       <entry>
        <para>
         <systemitem class="etheraddress">192.168.124.81</systemitem> -
         <systemitem class="etheraddress">192.168.124.160</systemitem>
        </para>
       </entry>
       <entry>
        <para>
         Fixed addresses for the &ostack; nodes. Determines the maximum
         number of &ostack; nodes that can be deployed.
        </para>
       </entry>
      </row>
      <row>
       <entry>
        <para>
         bmc vlan host
        </para>
       </entry>
       <entry>
        <para>
         <systemitem class="etheraddress">192.168.124.161</systemitem>
        </para>
       </entry>
       <entry>
        <para>
         Fixed address for the BMC VLAN. Used to generate a VLAN tagged
         interface on the Administration Server that can access the BMC
         network. The BMC VLAN needs to be in the same ranges as BMC, and
         BMC needs to have VLAN enabled.
        </para>
       </entry>
      </row>
      <row>
       <entry>
        <para>
         bmc host
        </para>
       </entry>
       <entry>
        <para>
         <systemitem class="etheraddress">192.168.124.162</systemitem> -
         <systemitem class="etheraddress">192.168.124.240</systemitem>
        </para>
       </entry>
       <entry>
        <para>
         Fixed addresses for the &ostack; nodes. Determines the maximum
         number of &ostack; nodes that can be deployed.
        </para>
       </entry>
      </row>
      <row>
       <entry>
        <para>
         switch
        </para>
       </entry>
       <entry>
        <para>
         <systemitem class="etheraddress">192.168.124.241</systemitem> -
         <systemitem class="etheraddress">192.168.124.250</systemitem>
        </para>
       </entry>
       <entry>
        <para>
         This range is not used in current releases and might be removed in
         the future.
        </para>
       </entry>
      </row>
     </tbody>
    </tgroup>
   </table>
   <table>
    <title><systemitem class="etheraddress">192.168.125/24</systemitem> (Storage) Network Address Allocation</title>
    <tgroup cols="3">
     <colspec colnum="1" colname="1" colwidth="20*"/>
     <colspec colnum="2" colname="2" colwidth="33*"/>
     <colspec colnum="3" colname="3" colwidth="47*"/>
     <thead>
      <row>
       <entry>
        <para>
         Function
        </para>
       </entry>
       <entry>
        <para>
         Address
        </para>
       </entry>
       <entry>
        <para>
         Remark
        </para>
       </entry>
      </row>
     </thead>
     <tbody>
      <row>
       <entry>
        <para>
         host
        </para>
       </entry>
       <entry>
        <para>
         <systemitem class="etheraddress">192.168.125.10</systemitem> -
         <systemitem class="etheraddress">192.168.125.239</systemitem>
        </para>
       </entry>
       <entry>
        <para>
         Each &stornode; will get an address from this range.
        </para>
       </entry>
      </row>
     </tbody>
    </tgroup>
   </table>
   <table>
    <title><systemitem class="etheraddress">192.168.123/24</systemitem> (Private Network/nova-fixed) Network Address Allocation</title>
    <tgroup cols="3">
     <colspec colnum="1" colname="1" colwidth="20*"/>
     <colspec colnum="2" colname="2" colwidth="33*"/>
     <colspec colnum="3" colname="3" colwidth="47*"/>
     <thead>
      <row>
       <entry>
        <para>
         Function
        </para>
       </entry>
       <entry>
        <para>
         Address
        </para>
       </entry>
       <entry>
        <para>
         Remark
        </para>
       </entry>
      </row>
     </thead>
     <tbody>
      <row>
       <entry>
        <para>
         dhcp
        </para>
       </entry>
       <entry>
        <para>
         <systemitem class="etheraddress">192.168.123.1</systemitem> -
         <systemitem class="etheraddress">192.168.123.254</systemitem>
        </para>
       </entry>
       <entry>
        <para>
         Address range for &vmguest;s, routers and DHCP/DNS agents.
        </para>
       </entry>
      </row>
     </tbody>
    </tgroup>
   </table>
   <table>
    <title><systemitem class="etheraddress">192.168.126/24</systemitem> (Public Network nova-floating, public) Network Address Allocation</title>
    <tgroup cols="3">
     <colspec colnum="1" colname="1" colwidth="20*"/>
     <colspec colnum="2" colname="2" colwidth="33*"/>
     <colspec colnum="3" colname="3" colwidth="47*"/>
     <thead>
      <row>
       <entry>
        <para>
         Function
        </para>
       </entry>
       <entry>
        <para>
         Address
        </para>
       </entry>
       <entry>
        <para>
         Remark
        </para>
       </entry>
      </row>
     </thead>
     <tbody>
      <row>
       <entry>
        <para>
         router
        </para>
       </entry>
       <entry>
        <para>
         <systemitem class="etheraddress">192.168.126.1</systemitem>
        </para>
       </entry>
       <entry>
        <para>
         Provided externally.
        </para>
       </entry>
      </row>
      <row>
       <entry>
        <para>
         public host
        </para>
       </entry>
       <entry>
        <para>
         <systemitem class="etheraddress">192.168.126.2</systemitem> -
         <systemitem class="etheraddress">192.168.126.127</systemitem>
        </para>
       </entry>
       <entry>
        <para>
         Public address range for external &cloud; services such as the
         &ostack; &dash; or the API.
        </para>
       </entry>
      </row>
      <row>
       <entry>
        <para>
         floating host
        </para>
       </entry>
       <entry>
        <para>
         <systemitem class="etheraddress">192.168.126.129</systemitem> -
         <systemitem class="etheraddress">192.168.126.254</systemitem>
        </para>
       </entry>
       <entry>
        <para>
         Floating IP address range. Floating IPs can be manually assigned to
         a running &vmguest; to allow to access the guest from the
         outside. Determines the maximum number of &vmguest;s that can
         concurrently be accessed from the outside.
        </para>
        <para>
         The nova_floating network is set up with a netmask of
         255.255.255.192, allowing a maximum number of 61 IP addresses. This
         range is pre-allocated by default and managed by &o_netw;.
        </para>
       </entry>
      </row>
     </tbody>
    </tgroup>
   </table>
   <table>
    <title><systemitem class="etheraddress">192.168.130/24</systemitem> (Software Defined Network) Network Address Allocation</title>
    <tgroup cols="3">
     <colspec colnum="1" colname="1" colwidth="20*"/>
     <colspec colnum="2" colname="2" colwidth="33*"/>
     <colspec colnum="3" colname="3" colwidth="47*"/>
     <thead>
      <row>
       <entry>
        <para>
         Function
        </para>
       </entry>
       <entry>
        <para>
         Address
        </para>
       </entry>
       <entry>
        <para>
         Remark
        </para>
       </entry>
      </row>
     </thead>
     <tbody>
      <row>
       <entry>
        <para>
         host
        </para>
       </entry>
       <entry>
        <para>
         <systemitem class="etheraddress">192.168.130.10</systemitem> -
         <systemitem class="etheraddress">192.168.130.254</systemitem>
        </para>
       </entry>
       <entry>
        <para>
         If &o_netw; is configured with <literal>openvswitch</literal>
         and <literal>gre</literal>, each network node and all
         &compnode;s will get an IP from this range.
        </para>
       </entry>
      </row>
     </tbody>
    </tgroup>
   </table>
   <note>
    <title>Addresses for Additional Servers</title>
    <para>
     Addresses not used in the ranges mentioned above, can be used to add
     additional servers with static addresses to &cloud;. Such servers
     can be used to provide additional services. A &susemgr; server
     inside &cloud;, for example, needs to be configured using one of
     these addresses.
    </para>
   </note>
  </sect2>

  <sect2 xml:id="sec.depl.req.network.modes">
   <title>Network Modes</title>
   <para>
    &productname; supports different network modes: single, dual, and
    teaming. As of &productname; &productnumber;, the networking mode
    is applied to all nodes and the &admserv;. That means that all
    machines need to meet the hardware requirements for the chosen mode. The
    network mode can be configured using the &yast; &crow; module
    (<xref linkend="sec.depl.adm_inst.crowbar"/>). The network mode cannot
    be changed after the cloud is deployed.
   </para>
   <para>
    Other, more flexible network mode setups can be configured by manually
    editing the &crow; network configuration files. See <xref linkend="app.deploy.network_json"/> for more information. &suse; or a
    partner can assist you in creating a custom setup within the scope of a
    consulting services agreement (see <link xlink:href="http://www.suse.com/consulting/"/> for more information on
    SUSE consulting).
   </para>
   <important>
    <title>Teaming Network Mode is Required for HA</title>
    <para>
     Teaming network mode is required for an &hasetup; of &productname;. If
     you are planning to move your cloud to an &hasetup; at a later point in
     time, make sure to deploy &cloud; with teaming network mode from the
     beginning. Otherwise a migration to an &hasetup; is not supported.
    </para>
   </important>
   <sect3 xml:id="sec.depl.req.network.modes.single">
    <title>Single Network Mode</title>
    <para>
     In single mode you just use one Ethernet card for all the traffic:
    </para>
    <informalfigure>
     <mediaobject>
      <imageobject role="fo">
       <imagedata fileref="cloud_networking_single_mode.png" width="70%" format="PNG"/>
      </imageobject>
      <imageobject role="html">
       <imagedata fileref="cloud_networking_single_mode.png" width="50%" format="PNG"/>
      </imageobject>
     </mediaobject>
    </informalfigure>
   </sect3>
   <sect3 xml:id="sec.depl.req.network.modes.dual">
    <title>Dual Network Mode</title>
    <para>
     Dual mode needs two Ethernet cards (on all nodes but &admserv;) and
     allows you to completely separate traffic to/from the Admin Network and
     to/from the public network:
    </para>
    <informalfigure>
     <mediaobject>
      <imageobject role="fo">
       <imagedata fileref="cloud_networking_dual_mode.png" width="100%" format="PNG"/>
      </imageobject>
      <imageobject role="html">
       <imagedata fileref="cloud_networking_dual_mode.png" width="70%" format="PNG"/>
      </imageobject>
     </mediaobject>
    </informalfigure>
   </sect3>
   <sect3 xml:id="sec.depl.req.network.modes.teaming">
    <title>Teaming Network Mode</title>
    <para>
     Teaming mode is almost identical to single mode, except that you
     combine several Ethernet cards to a <quote>bond</quote> (network device
     bonding). Teaming mode needs two or more Ethernet cards.
    </para>
    <informalfigure>
     <mediaobject>
      <imageobject role="fo">
       <imagedata fileref="cloud_networking_team_mode.png" width="70%" format="PNG"/>
      </imageobject>
      <imageobject role="html">
       <imagedata fileref="cloud_networking_team_mode.png" width="50%" format="PNG"/>
      </imageobject>
     </mediaobject>
    </informalfigure>
   </sect3>
  </sect2>

  <sect2 xml:id="sec.depl.req.network.bastion">
   <title>Accessing the &admserv; via a Bastion Network</title>
   <para>
    If you want to enable access to the &admserv; from another network,
    you can do so by providing an external gateway. This option offers
    maximum flexibility, but requires additional machines and may be less
    secure than you require. Therefore &cloud; offers a second option for
    accessing the &admserv;: the bastion network. You only need a
    dedicated Ethernet card and a static IP address from the external
    network to set it up.
   </para>
   <para>
    The bastion network setup enables you to log in to the &admserv; via
    SSH (see <xref linkend="sec.depl.req.network.bastion"/> for setup
    instructions). A direct login to other nodes in the cloud is not
    possible. However, the &admserv; can act as a <quote>jump
    host</quote>: To log in to a node, first log in to the &admserv; via
    SSH. From there, you can log in via SSH to other nodes.
   </para>
  </sect2>

  <sect2 xml:id="sec.depl.req.network.dns">
   <title>DNS and Host Names</title>
   <para>
    The &admserv; acts as a name server for all nodes in the cloud. If
    the admin node has access to the outside, then you can add additional
    name servers that will automatically be used to forward requests. If
    additional name servers are found on cloud deployment, the name server
    on the &admserv; will automatically be configured to forward requests
    for non-local records to those servers.
   </para>
   <para>
    The &admserv; needs to be configured to have a fully qualified host
    name. The domain name you specify will be used for the DNS zone. It is
    required to use a sub-domain such as
    <replaceable>cloud.&exampledomain;</replaceable>. The admin server
    needs to have authority on the domain it is working on (to be able to
    create records for discovered nodes). As a result it will not forward
    requests for names it cannot resolve in this domain and thus cannot
    resolve names for <replaceable>&exampledomain;</replaceable> other
    than the ones in the cloud.
   </para>
   <para>
    This host name must not be changed after &cloud; has been deployed.
    The &ostack; nodes will be named after their MAC address by default,
    but you can provide aliases, which are easier to remember when
    allocating the nodes. The aliases for the &ostack; nodes can be
    changed at any time. It is useful to have a list of MAC addresses and
    the intended use of the corresponding host at hand when deploying the
    &ostack; nodes.
   </para>
  </sect2>
 </sect1>
 <sect1 xml:id="sec.depl.req.storage">
  <title>Persistent Storage</title>

  <para>
   When talking about <quote>persistent storage</quote> on &productname;,
   there are two completely different aspects to discuss: the block and
   object storage services &cloud; offers on the one hand and the
   hardware related storage aspects on the different node types.
  </para>

  <note>
   <title>Persistent vs. Ephemeral Storage</title>
   <para>
    Block and object storage are persistent storage models where files or
    images are stored until they are explicitly deleted. &cloud; also
    offers ephemeral storage for images attached to &vmguest;s. These
    ephemeral images only exist during the life of an &vmguest; and are
    deleted when the guest is terminated. See
    <xref linkend="sec.depl.req.storage.hardware.compute"/> for more
    information.
   </para>
  </note>

  <sect2 xml:id="sec.depl.req.storage.services">
   <title>Cloud Storage Services</title>
   <para>
    As mentioned above, &cloud; offers two different types of services
    for persistent storage: object and block storage. Object storage lets
    you upload and download files (similar to an FTP server), whereas a
    block storage provides mountable devices (similar to a hard-disk
    partition). Furthermore &cloud; provides a repository to store the
    virtual disk images used to start &vmguest;s.
   </para>
   <variablelist>
    <varlistentry>
     <term>Object Storage with &o_objstore;</term>
     <listitem>
      <para>
       The &ostack; object storage service is called &o_objstore;. The
       storage component of &o_objstore; (swift-storage) needs to be
       deployed on dedicated nodes where no other cloud services run. To be
       able to store the objects redundantly, it is required to deploy at
       least two &o_objstore; nodes. &productname; is configured to
       always use all unused disks on a node for storage.
      </para>
      <para>
       &o_objstore; can optionally be used by &o_img;, the service
       that manages the images used to boot the &vmguest;s. Offering
       object storage with &o_objstore; is optional.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>Block Storage</term>
     <listitem>
      <para>
       Block storage on &cloud; is provided by &o_blockstore;.
       &o_blockstore; can use a variety of storage back-ends, among them
       network storage solutions like NetApp or EMC. It is also possible to
       use local disks for block storage. A list of drivers available for
       &o_blockstore; and the features supported for each driver is
       available from the <citetitle>CinderSupportMatrix</citetitle> at
       <link xlink:href="https://wiki.openstack.org/wiki/CinderSupportMatrix"/>.
       &productname; &productnumber; is shipping with &ostack;
       &latest_ostack;.
      </para>
      <para>
       Alternatively, &o_blockstore; can use &ceph; RBD as a back-end.  &ceph;
       offers data security and speed by storing the devices redundantly on
       different servers. &ceph; needs to be deployed on dedicated nodes where
       no other cloud services run. &ceph; requires at least four dedicated
       nodes.  If deploying the optional Calamari server for &ceph; management
       and monitoring, an additional dedicated node is required.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>The &o_img; Image Repository</term>
     <listitem>
      <para>
       &o_img; provides a catalog and repository for virtual disk images
       used to start the &vmguest;s. &o_img; is installed on a
       &contrnode;. It either uses &o_objstore;, &ceph; or a
       directory on the &contrnode; to store the images. The image
       directory can either be a local directory or an NFS share.
      </para>
     </listitem>
    </varlistentry>
   </variablelist>
  </sect2>

  <sect2 xml:id="sec.depl.req.storage.hardware">
   <title>Storage Hardware Requirements</title>
   <para>
    Apart from sufficient disk space to install the &sls; operating
    system, each node in &cloud; needs to store additional data.
    Requirements and recommendations for the various node types are listed
    below.
   </para>
   <important>
<!-- TODO vuntz 2013-12-04: hrm, double-check with dirk, but I think it
          is wrong now? It is using the first disk in PCI order? -->
    <title>Choose a Hard Disk for the Operating System Installation</title>
    <para>
     The operating system will always be installed on the
     <emphasis>first</emphasis> hard disk, the one that is recognized as
     <filename>/dev/sda</filename>. This is the disk that is listed
     <emphasis>first</emphasis> in the BIOS, the one from which the machine
     will boot. If you have nodes with a certain hard disk you want the
     operating system to be installed on, make sure it will be recognized as
     the first disk.
    </para>
   </important>
   <sect3 xml:id="sec.depl.req.storage.hardware.admin">
    <title>&admserv;</title>
    <para>
     If you store the update repositories directly on the &admserv; (see
     <xref linkend="sec.depl.req.repos"/> for details), it is recommended to
     mount <filename>/srv</filename> to a separate partition or volume with
     a minimum of &repospace; space.
    </para>
    <para>
     Log files from all nodes in &cloud; are stored on the &admserv;
     under <filename>/var/log</filename> (see
     <xref linkend="sec.deploy.logs.adminserv"/> for a complete list).
     Furthermore, the message service RabbitMQ requires 1 GB of free space
     in <filename>/var</filename>. Make sure sufficient space is available
     under <filename>/var</filename>.
    </para>
   </sect3>
   <sect3 xml:id="sec.depl.req.storage.hardware.control">
    <title>&contrnode;s</title>
    <para>
     Depending on how the services are set up, &o_img; and
     &o_blockstore; may require additional disk space on the
     &contrnode; on which they are running. &o_img; may be configured
     to use a local directory, whereas &o_blockstore; may use a local
     image file for storage. For performance and scalability reasons this is
     only recommended for test setups. Make sure there is sufficient free
     disk space available if you use a local file for storage.
    </para>
    <para>
     &o_blockstore; may be configured to use local disks for storage
     (configuration option <literal>raw</literal>). If you choose this
     setup, it is recommended to deploy the <guimenu>cinder-volume</guimenu>
     role to one or more dedicated &contrnode;s equipped with several
     disks providing sufficient storage space. It may also be necessary to
     equip this node with two or more bonded network cards (requiring a
     special setup for this node, refer to
     <xref linkend="app.deploy.network_json"/> for details), since it will
     generate heavy network traffic.
    </para>
    <para>
     Live migration for &xen; &vmguest;s requires to export
     <filename>/var/lib/nova/instances</filename> on the &contrnode; hosting
     <systemitem>nova-controller</systemitem>. This directory will host
     a copy of the root disk of <emphasis>all</emphasis> &xen; &vmguest;s in
     the cloud and needs to have sufficient disk space. It is strongly
     recommended to use a separate block device for this directory, preferably
     a RAID device to ensure data security.
    </para>
   </sect3>
   <sect3 xml:id="sec.depl.req.storage.hardware.compute">
    <title>&compnode;s</title>
    <para>
     Unless an &vmguest; is started via <quote>Boot from Volume</quote>,
     it is started with at least one disk&mdash;a copy of the image from
     which it has been started. Depending on the flavor you start, the
     &vmguest; may also have a second, so-called <quote>ephemeral</quote>
     disk. The size of the root disk depends on the image itself, while
     ephemeral disks are always created as sparse image files that grow (up
     to a defined size) when being <quote>filled</quote>. By default
     ephemeral disks have a size of 10 GB.
    </para>
    <para>
     Both disks, root images and ephemeral disk, are directly bound to the
     &vmguest; and are deleted when the &vmguest; is terminated.
     Therefore these disks are bound to the &compnode; on which the
     &vmguest; has been started. The disks are created under
     <filename>/var/lib/nova</filename> on the &compnode;. Your
     &compnode;s should be equipped with enough disk space to store the
     root images and ephemeral disks.
    </para>
    <note>
     <title>Ephemeral Disks vs. Block Storage</title>
     <para>
      Do not confuse ephemeral disks with persistent block storage. In
      addition to an ephemeral disk, which is automatically provided with
      most &vmguest; flavors, you can optionally add a persistent storage
      device provided by &o_blockstore;. Ephemeral disks are deleted when
      the &vmguest; terminates, while persistent storage devices can be
      reused in another &vmguest;.
     </para>
    </note>
    <para>
     The maximum disk space required on a compute node depends on the
     available flavors. A flavor specifies the number of CPUs, RAM and disk
     size of an &vmguest;. Several flavors ranging from
     <guimenu>tiny</guimenu> (1 CPU, 512 MB RAM, no ephemeral disk) to
     <guimenu>xlarge</guimenu> (8 CPUs, 8 GB RAM, 10 GB ephemeral disk) are
     available by default. Adding custom flavors, editing and deleting
     existing flavors is also supported.
    </para>
    <para>
     To calculate the minimum disk space needed on a compute node, you need
     to determine the highest "disk space to RAM" ratio from your flavors.
     Example:
    </para>
    <simplelist>
     <member>
      Flavor small: 2 GB RAM, 100 GB ephemeral disk =&gt; 50 GB disk /1 GB RAM
     </member>
     <member>
      Flavor large: 8 GB RAM, 200 GB ephemeral disk =&gt; 25 GB disk /1 GB RAM
     </member>
    </simplelist>
    <para>
     So, 50 GB disk /1 GB RAM is the ratio that matters. If you multiply
     that value by the amount of RAM in GB available on your compute node,
     you have the minimum disk space required by ephemeral disks. Pad that
     value with sufficient space for the root disks plus a buffer that
     enables you to create flavors with a higher disk space to RAM ratio in
     the future.
    </para>
    <warning>
     <title>Overcommitting Disk Space</title>
     <para>
      The scheduler that decides in which node an &vmguest; is started
      does not check for available disk space. If there is no disk space
      left on a compute node, this will not only cause data loss on the
      &vmguest;s, but the compute node itself will also stop operating.
      Therefore you must make sure all compute nodes are equipped with
      enough hard disk space!
     </para>
    </warning>
   </sect3>
   <sect3 xml:id="sec.depl.req.storage.hardware.store">
    <title>Storage Nodes (optional)</title>
    <para>
     The block-storage service &ceph; RBD and the object storage service
     &swift; need to be deployed onto dedicated nodes&mdash;it is not
     possible to mix these services. &swift; service requires at least
     two machines (more are recommended) to be able to store data
     redundantly. For &ceph; at least four machines are required (more
     are recommended). If deploying the optional Calamari server for
     &ceph; management and monitoring, an additional machine (with
     moderate CPU and RAM requirements) needs to be supplied.
    </para>
    <para>
     Each &ceph;/&swift; &stornode; needs at least two hard disks.
     The first one will be used for the operating system installation, while
     the others can be used for storage purposes. It is recommended to equip
     the storage nodes with as many disks as possible.
    </para>
    <para>
     Using RAID on &o_objstore; storage nodes is not supported.
     &swift; takes care of redundancy and replication on its own. Using
     RAID with &o_objstore; would also result in a huge performance
     penalty.
    </para>
   </sect3>
  </sect2>
 </sect1>
 <sect1 xml:id="sec.depl.req.ssl">
  <title>SSL Encryption</title>

  <para>
   Whenever non-public data travels over a network it needs to be encrypted.
   Encryption protects the integrity and confidentiality of data. Therefore
   you should enable SSL support when deploying &cloud; to production (it
   is not enabled by default since it requires certificates to be provided).
   The following services (and their APIs if available) can use SSL:
  </para>

  <itemizedlist mark="bullet" spacing="normal">
   <listitem>
    <para>
     &o_blockstore;
    </para>
   </listitem>
   <listitem>
    <para>
     &dash;
    </para>
   </listitem>
   <listitem>
    <para>
     &o_img;
    </para>
   </listitem>
   <listitem>
    <para>
     &o_orch;
    </para>
   </listitem>
   <listitem>
    <para>
     &o_ident;
    </para>
   </listitem>
   <listitem>
    <para>
     &o_sharefs;
    </para>
   </listitem>
   <listitem>
    <para>
     &o_netw;
    </para>
   </listitem>
   <listitem>
    <para>
     &o_comp;
    </para>
   </listitem>
   <listitem>
    <para>
     &o_dbaas;
    </para>
   </listitem>
   <listitem>
    <para>
     VNC
    </para>
   </listitem>
  </itemizedlist>

  <para>
   Using SSL requires an SSL certificate either for each node on which the
   services that uses encryption run (services sharing a certificate) or,
   alternatively, a dedicated certificate for each service. A single
   certificate for the &contrnode; is the minimum requirement, where all
   services listed above are installed on the &contrnode; and are sharing
   the certificate.
  </para>

  <para>
   Certificates must be signed by a trusted authority. Refer to
   <link xlink:href="&suse-onlinedoc;/sles-12/book_sle_admin/data/sec_apache2_ssl.html"/>
   for instructions on how to create and sign them.
  </para>

  <important>
   <title>Host Names</title>
   <para>
    Each SSL certificate is issued for a certain host name and, optionally,
    for alternative host names (via the <literal>AlternativeName</literal>
    option). Each publicly available node in &cloud; has two host
    names&mdash;an internal and a public one. The SSL certificate needs
    to be issued for both names.
   </para>
   <para>
    The internal name has the following scheme:
   </para>
<screen>d<replaceable>MAC ADDRESS</replaceable>.<replaceable>FQDN</replaceable></screen>
   <para>
    <replaceable>MAC ADDRESS</replaceable> is the MAC address of the
    interface used to boot the machine via PXE. All letters are turned
    lowercase and all colons are replaced with dashes. For example,
    <literal>52-54-00-8e-ce-e3</literal>. <replaceable>FQDN</replaceable> is
    the fully qualified domain name. An example name looks like this:
   </para>
<screen>d52-54-00-8e-ce-e3.&exampledomain;</screen>
   <para>
    Unless you have entered a custom <guimenu>Public Name</guimenu> for a
    client (see <xref linkend="sec.depl.inst.nodes.install"/> for details),
    the public name is the same as the internal name prefixed by
    <literal>public.</literal>:
   </para>
<screen>public.d52-54-00-8e-ce-e3.&exampledomain;</screen>
   <para>
    To look up the node names open the &crow; Web interface and click the
    name of a node in the <guimenu>Node Dashboard</guimenu>. The names are
    listed as <guimenu>Full Name</guimenu> and <guimenu>Public
    Name</guimenu>.
   </para>
  </important>
 </sect1>
 <sect1 xml:id="sec.depl.req.hardware">
  <title>Hardware Requirements</title>

  <para>
   Precise hardware requirements can only be listed for the &admserv; and
   the &ostack; &contrnode;. The requirements of the &ostack;
   Compute and &stornode;s depends on the number of concurrent
   &vmguest;s and their virtual hardware equipment.
  </para>

  <para>
   The minimum number of machines required for a &cloud; setup is three:
   one &admserv;, one &contrnode;, and one &compnode;. In addition
   to that, a gateway providing access to the public network is required.
   Deploying storage requires additional nodes: at least two nodes for
   &o_objstore; and a minimum of four nodes for &ceph;.
  </para>

  <important>
   <title>Physical Machines and Architecture</title>
   <para>
    All &cloud; nodes need to be physical machines. Although the
    &admserv; and the &contrnode; can be virtualized in test
    environments, this is not supported for production systems.
   </para>
   <para>
    &cloud; currently only runs on <literal>x86_64</literal> hardware.
   </para>
  </important>

  <sect2 xml:id="sec.depl.req.hardware.admserv">
   <title>&admserv;</title>
   <itemizedlist mark="bullet" spacing="normal">
    <listitem>
     <para>
      Architecture: x86_64
     </para>
    </listitem>
    <listitem>
     <para>
      RAM: at least 2 GB, 4 GB recommended
     </para>
    </listitem>
    <listitem>
     <para>
      Hard disk: at least 50 GB. It is recommended to put
      <filename>/srv</filename> on a separate partition with at least
      additional 30 GB of space, unless you mount the update repositories
      from another server (see <xref linkend="sec.depl.req.repos"/> for
      details).
     </para>
    </listitem>
    <listitem>
     <para>
      Number of network cards: 1 for single and dual mode, 2 or more for
      team mode. Additional networks such as the bastion network and/or a
      separate BMC network each need an additional network card. See
      <xref linkend="sec.depl.req.network"/> for details.
     </para>
    </listitem>
   </itemizedlist>
  </sect2>

  <sect2 xml:id="sec.depl.req.hardware.contrnode">
   <title>&contrnode;</title>
   <itemizedlist mark="bullet" spacing="normal">
    <listitem>
     <para>
      Architecture: x86_64
     </para>
    </listitem>
    <listitem>
     <para>
      RAM: at least 2 GB, 12 GB recommended (when deploying a single
      &contrnode;)
     </para>
    </listitem>
    <listitem>
     <para>
      Number of network cards: 1 for single mode, 2 for dual mode, 2 or more
      for team mode. See <xref linkend="sec.depl.req.network"/> for details.
     </para>
    </listitem>
    <listitem>
     <para>
      Hard disk: See
      <xref linkend="sec.depl.req.storage.hardware.control"/>.
     </para>
    </listitem>
   </itemizedlist>
  </sect2>

  <sect2 xml:id="sec.depl.req.hardware.compnode">
   <title>&compnode;</title>
   <para>
    The &compnode;s need to be equipped with a sufficient amount of RAM
    and CPUs, matching the numbers required by the maximum number of
    &vmguest;s running concurrently. An &vmguest; started in
    &cloud; cannot share resources from several physical nodes, but
    rather uses the resources of the node on which it was started. So if you
    offer a flavor (see <xref linkend="gloss.flavor"/> for a definition)
    with 8 CPUs and 12 GB RAM, at least one of your nodes should be able to
    provide these resources.
   </para>
   <para>
    See <xref linkend="sec.depl.req.storage.hardware.compute"/> for storage
    requirements.
   </para>
  </sect2>

  <sect2 xml:id="sec.depl.req.hardware.stornode">
   <title>&stornode;</title>
   <para>
    The &stornode;s are sufficiently equipped with a single CPU and 1 or
    2 GB of RAM. See <xref linkend="sec.depl.req.storage.hardware.store"/>
    for storage requirements.
   </para>
  </sect2>
 </sect1>
 <sect1 xml:id="sec.depl.req.software">
  <title>Software Requirements</title>

  <para>
   All nodes and the &admserv; in &cloud; run on &cloudos;. A &productname;
   subscription will include the following:
  </para>

  <itemizedlist mark="bullet" spacing="normal">
   <listitem>
    <para>
     &productname; &productnumber; for an unlimited
     number of nodes
    </para>
   </listitem>
   <listitem>
    <para>
     an entitlement for &cloudos; for a single &admserv;
    </para>
   </listitem>
   <listitem>
    <para>
     an entitlement for &cloudos; for a single &contrnode;
    </para>
   </listitem>
   <listitem>
    <para>
     an entitlement for the &cloudos; &hasi; for an unlimited
     number of &contrnode;s
    </para>
   </listitem>
  </itemizedlist>

  <para>
   &cloudos;; entitlements for additional nodes (such as &compnode;s,
   storage nodes and additional  &contrnode;s) need to
   be purchased in addition to the &productname; subscription. Refer to
   <link xlink:href="http://www.suse.com/products/suse-openstack-cloud/how-to-buy/"/>
   for more information.
  </para>
  <para>
   Running &ceph; within &cloud; (optional) requires an additional &storage;
   subscription. Refer to <link
   xlink:href="https://www.suse.com/products/suse-enterprise-storage/"/> for
   more information.
  </para>

  <important>
   <title>&suse; Account</title>
   <para>
    A &suse; account is needed for product registration and access to
    update repositories. If you do not already have one, go to
    <link xlink:href="http://www.suse.com/login"/> to create it.
   </para>
  </important>

  <sect2 xml:id="sec.depl.req.software.optional">
   <title>Optional Component: &storage;</title>
   <para>
    &productname; can be extended by &storage; for setting up a &ceph; cluster
    providing block storage services.  To store virtual disks for &vmguest;s
    &cloud; uses block storage provided by the &o_blockstore;
    module. &o_blockstore; itself needs a back-end providing storage. In
    production environments this usually is a network storage
    solution. &o_blockstore; can use a variety of network storage back-ends,
    among them solutions from EMC, Fujitsu or NetApp. in case your
    organization does not provide a network storage solution that can be used
    with &cloud;, you can set up a &ceph; cluster with &storage;. &storage;
    provides a reliable and fast distributed storage architecture using
    commodity hardware platforms.
   </para>
   <para>
    Deploying &storage; (&ceph;) within &productname; is fully
    supported. &ceph; nodes can be deployed using the same interface as for
    all other &cloud; services. It requires a &storage; subscription. See
    <link
    xlink:href="https://www.suse.com/products/suse-enterprise-storage/"/> for
    more information on &storage;.
   </para>
  </sect2>

  <sect2 xml:id="sec.depl.req.repos">
   <title>Product and Update Repositories</title>
   <para>
    To deploy &cloud; and to be able to keep a running &cloud;
    up-to-date, a total of seven software repositories is needed. This
    includes the static product repositories, which do not change over the
    product life cycle and the update repositories, which constantly change.
    The following repositories are needed:
   </para>
   <variablelist>
    <title>Mandatory Repositories</title>
    <varlistentry>
     <term>&cloudos; Product</term>
     <listitem>
      <para>
       The &cloudos; product repository is a copy of the installation
       media (DVD #1) for &sls;. As of &productname;
       &productnumber; it is required to have it available locally on the
       &admserv;. This repository requires approximately 3.5 GB of hard
       disk space.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>&productname; &productnumber; Product</term>
     <listitem>
      <para>
       The &productname; &productnumber; product repository is a copy
       of the installation media (DVD #1) for &cloud;. It can either be
       made available remote via HTTP or locally on the &admserv;. The
       latter is recommended, since it makes the setup of the &admserv;
       easier. This repository requires approximately 500 MB of hard disk
       space.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>PTF</term>
     <listitem>
      <para>
       A repository created automatically on the &admserv; upon the
       &cloud; add-on product installation. It serves as a repository for
       <quote>Program Temporary Fixes</quote> (PTF) which are part of the
       &suse; support program.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>&sle_repo;-Pool and &cloud_repo;-Pool</term>
     <listitem>
      <para>
       The &sls; and &productname; repositories contain all binary
       RPMs from the installation media, plus pattern information and
       support status metadata. These repositories are served from &scc;
       and need to be kept in synchronization with their sources. They can
       be made available remotely via an existing &smt; or &susemgr;
       server or locally on the &admserv; by installing a local &smt;
       server, by mounting or synchronizing a remote directory or by copying
       them.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>&sle_repo;-Updates and &cloud_repo;-Updates</term>
     <listitem>
      <para>
       These repositories contain maintenance updates to packages in the
       corresponding Pool repositories. These repositories are served from
       &scc; and need to be kept synchronized with their sources. They
       can be made available remotely via an existing &smt; or
       &susemgr; server or locally on the &admserv; by installing a
       local &smt; server, by mounting or synchronizing a remote
       directory or by regularly copying them.
      </para>
     </listitem>
    </varlistentry>
   </variablelist>
   <para>
    As explained in <xref linkend="sec.depl.req.ha"/>, &contrnode;s in &cloud;
    can optionally be made highly available with the help of the &sle;
    &hasi;. &productname; also comes with full support for installing a
    storage cluster running &ceph; provided by the &storage;
    extension. Deploying &ceph; is optional. The following repositories are
    required to deploy &slsa; &hasi; and &storage; nodes:
   </para>
   <variablelist>
    <title>Optional Repositories</title>
    <varlistentry>
     <term>&sleha_repo;-Pool and &ses_repo;-Pool</term>
     <listitem>
      <para>
       The pool repositories contain all binary RPMs from the installation
       media, plus pattern information and support status metadata. These
       repositories are served from &scc; and need to be kept in
       synchronization with their sources. They can be made available
       remotely via an existing &smt; or &susemgr; server or locally
       on the &admserv; by installing a local &smt; server, by
       mounting or synchronizing a remote directory or by copying them.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>&sleha_repo;-Updates and &ses_repo;-Updates</term>
     <listitem>
      <para>
       These repositories contain maintenance updates to packages in the
       corresponding pool repositories. These repositories are served from
       &scc; and need to be kept synchronized with their sources. They
       can be made available remotely via an existing &smt; or
       &susemgr; server or locally on the &admserv; by installing a
       local &smt; server, by mounting or synchronizing a remote
       directory or by regularly copying them.
      </para>
     </listitem>
    </varlistentry>
   </variablelist>
   <para>
    Since the product repositories (for &cloudos; and
    &productname; &productnumber;) do not change during the life cycle
    of a product they can be copied to the destination directory from the
    installation media. The pool and update repositories however, need to be
    kept synchronized with their sources on the the &scc;. &suse;
    offers two products taking care of synchronizing repositories and making
    them available within your organization: &susemgr;
    (<link xlink:href="http://www.suse.com/products/suse-manager/"/> and
    &smtool; (shipping with &cloudos;).
   </para>
   <para>
    All repositories need to be served via <literal>http</literal> to be
    available for &cloud; deployment. Repositories that are directly
    available on the &admserv; are made available by the Apache Web
    server running on the &admserv;. If your organization already uses
    &susemgr; or &smt;, you can use the repositories provided by these
    servers.
   </para>
   <para>
    Making the repositories locally available on the &admserv; has the
    advantage of a simple network setup within &cloud;. It also allows you to
    seal off the &cloud; network from other networks in your
    organization. Using a remote server as a source for the repositories has
    the advantage of using existing resources and services. It also makes
    setting up the &admserv; much easier, but requires a custom network setup
    for &cloud;, since the &admserv; needs to be able to access the remote
    server.
   </para>
   <variablelist>
    <varlistentry>
     <term>Installing a &smtool; (&smt;) Server on the &admserv;</term>
     <listitem>
      <para>
       The &smt; server, shipping with &cloudos;, regularly
       synchronizes repository data from &scc; with your local host.
       Installing the &smt; server on the &admserv; is recommended if
       you do not have access to update repositories from elsewhere within
       your organization. This option requires the &admserv; to be able
       to access the Internet.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>Using a remote &smt; Server</term>
     <listitem>
      <para>
       If you already run an &smt; server within your organization, you
       can use it within &cloud;. When using a remote &smt; server,
       update repositories are served directly from the &smt; server.
       Each node is configured with these repositories upon its initial
       setup.
      </para>
      <para>
       The &smt; server needs to be accessible from the &admserv; and
       all nodes in &cloud; (via one or more gateways). Resolving the
       server's host name also needs to work.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>Using a &susemgr; Server</term>
     <listitem>
      <para>
       Each client that is managed by &susemgr; needs to register with the
       &susemgr; server. Therefore the &susemgr; support can only be installed
       after the nodes have been deployed. To also be able to use repositories
       provided by &susemgr; during node deployment, &cloudos; must be set up
       for autoinstallation on the &susemgr; server.
      </para>
      <para>
       The server needs to be accessible from the &admserv; and all nodes
       in &cloud; (via one or more gateways). Resolving the server's host
       name also needs to work.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>Utilizing Existing Repositories</term>
     <listitem>
      <para>
       If you can access existing repositories from within your company
       network from the &admserv;, you can either mount or synchronize or
       manually transfer these repositories to the required locations on the
       &admserv;.
      </para>
     </listitem>
    </varlistentry>
   </variablelist>
  </sect2>
 </sect1>
 <sect1 xml:id="sec.depl.req.ha">
  <title>&ha;</title>

  <para>
   Several components and services in &productname; can become single
   points of failure that may cause system downtime and/or data loss if they
   fail.
  </para>

  <para>
   &productname; provides various mechanisms which can ensure that the crucial
   components and services are highly available. The following sections
   provide an overview of which components on each node you should consider to
   make highly available. For making the &contrnode; functions and the
   &compnode;s highly available, &productname; uses the cluster software &sle;
   &hasi;. Make sure to thoroughly read <xref
   linkend="sec.depl.reg.ha.general"/> that lists additional requirements with
   regard to that.
  </para>

  <sect2 xml:id="sec.depl.req.ha.admin">
   <title>&ha; of the &admserv;</title>
   <para>
    The &admserv; provides all services needed to manage and deploy all
    other nodes in the cloud. If the &admserv; is not available, new
    cloud nodes cannot be allocated, and you cannot add new roles to cloud
    nodes.
   </para>
   <para>
    However, only two services on the &admserv; are single point of
    failures, without which the cloud cannot continue to run properly: DNS
    and NTP.
   </para>
   <sect3 xml:id="sec.depl.req.ha.admin.spof">
    <title>&admserv;&mdash;Avoiding Points of Failure</title>
    <para>
     To avoid DNS and NTP as potential points of failure, deploy the roles
     <systemitem>dns-server</systemitem> and
     <systemitem>ntp-server</systemitem> to multiple nodes.
    </para>
    <note>
     <title>Access to External Network</title>
     <para>
      If any configured DNS forwarder or NTP external server is not
      reachable through the admin network from these nodes, allocate an
      address in the public network for each node that has the
      <systemitem>dns-server</systemitem> and
      <systemitem>ntp-server</systemitem> roles:
     </para>
<screen>crowbar network allocate_ip default `hostname -f` public host</screen>
     <para>
      That way, the nodes can use the public gateway to reach the external
      servers. The change will only become effective after the next run of
      <command>chef-client</command> on the affected nodes.
     </para>
    </note>
   </sect3>
   <sect3 xml:id="sec.depl.req.ha.admin.recover">
    <title>&admserv;&mdash;Recovery</title>
    <para>
     To minimize recovery time for the &admserv;, follow the backup and
     restore recommendations described in
     <xref linkend="sec.depl.maintenance.backup.admin"/>.
    </para>
   </sect3>
  </sect2>

  <sect2 xml:id="sec.depl.reg.ha.control">
   <title>&ha; of the &contrnode;(s)</title>
   <para>
    The &contrnode;(s) usually run a variety of services without which
    the cloud would not be able to run properly.
   </para>
   <sect3 xml:id="sec.depl.reg.ha.control.spof">
    <title>&contrnode;(s)&mdash;Avoiding Points of Failure</title>
    <para>
     To prevent the cloud from avoidable downtime in case one or more
     &contrnode;s fail, &productname; offers you the choice to make the
     following roles highly available: <remark condition="clarity">2016-01-08
     - fs: FIXME: Check list of services</remark>
    </para>
    <itemizedlist mark="bullet" spacing="normal">
     <listitem>
      <para>
       <systemitem>database-server</systemitem>
       (<systemitem>database</systemitem> &barcl;)
      </para>
     </listitem>
     <listitem>
      <para>
       <systemitem>keystone-server</systemitem>
       (<systemitem>keystone</systemitem> &barcl;)
      </para>
     </listitem>
     <listitem>
      <para>
       <systemitem>rabbitmq-server</systemitem>
       (<systemitem>rabbitmq</systemitem> &barcl;)
      </para>
     </listitem>
     <listitem>
      <para>
       <systemitem>swift-proxy</systemitem> (<systemitem>swift</systemitem>
       &barcl;)
      </para>
     </listitem>
     <listitem>
      <para>
       <systemitem>glance-server</systemitem>
       (<systemitem>glance</systemitem> &barcl;)
      </para>
     </listitem>
     <listitem>
      <para>
       <systemitem>cinder-controller</systemitem>
       (<systemitem>cinder</systemitem> &barcl;)
      </para>
     </listitem>
     <listitem>
      <para>
       <systemitem>neutron-server</systemitem>
       (<systemitem>neutron</systemitem> &barcl;)
      </para>
     </listitem>
     <listitem>
      <para>
       <systemitem>neutron-network</systemitem> (<systemitem>neutron</systemitem>
       &barcl;)
      </para>
     </listitem>
     <listitem>
      <para>
       <systemitem>nova-controller</systemitem>
       (<systemitem>nova</systemitem> &barcl;)
      </para>
     </listitem>
     <listitem>
      <para>
       <systemitem>nova_dashboard-server</systemitem>
       (<systemitem>nova_dashboard</systemitem> &barcl;)
      </para>
     </listitem>
     <listitem>
      <para>
       <systemitem>ceilometer-server</systemitem>
       (<systemitem>ceilometer</systemitem> &barcl;)
      </para>
     </listitem>
     <listitem>
      <para>
       <systemitem>ceilometer-polling</systemitem>
       (<systemitem>ceilometer</systemitem> &barcl;)
      </para>
     </listitem>
     <listitem>
      <para>
       <systemitem>heat-server</systemitem> (<systemitem>heat</systemitem>
       &barcl;)
      </para>
     </listitem>
    </itemizedlist>
    <para>
     Instead of assigning these roles to individual cloud nodes, you can
     assign them to one or several &ha; clusters. &productname; will
     then use the Pacemaker cluster stack (shipped with the &sle;
     &hasi;) to manage the services and to fail them over to another
     &contrnode; in case one &contrnode; fails. For details on the
     Pacemaker cluster stack and the &sle; &hasi;, refer to the
     &haguide;, available at
     <link xlink:href="&suse-onlinedoc;/sle_ha/"/>.
     However, whereas &sle; &hasi; includes &lvs; as load-balancer,
     &productname; uses &haproxy; for this purpose
     (<link xlink:href="http://haproxy.1wt.eu/"/>).
    </para>
    <para/>
    <note>
     <title>Recommended Setup</title>
     <para>
      Though it is possible to use the same cluster for all of the roles
      above, the recommended setup is to use three clusters and to deploy
      the roles as follows:
     </para>
     <itemizedlist mark="bullet" spacing="normal">
      <listitem>
       <para>
        <literal>data</literal> cluster:
        <systemitem>database-server</systemitem> and
        <systemitem>rabbitmq-server</systemitem>
       </para>
      </listitem>
      <listitem>
       <para>
        <literal>network</literal> cluster:
        <systemitem>neutron-network</systemitem> (as the
        <systemitem>neutron-network</systemitem> role may result in heavy network
        load and CPU impact)
       </para>
      </listitem>
      <listitem>
       <para>
        &o_dbaas; (always needs to be deployed on a dedicated node)
       </para>
      </listitem>
      <listitem>
       <para>
        <systemitem>services</systemitem> cluster: all other roles listed
        above (as they are related to API/schedulers)
       </para>
      </listitem>
     </itemizedlist>
    </note>
    <important>
     <title>Cluster Requirements and Recommendations</title>
     <para>
      For setting up the clusters, some special requirements and
      recommendations apply. For details, refer to
      <xref linkend="sec.depl.reg.ha.general"/>.
     </para>
    </important>
   </sect3>
   <sect3 xml:id="sec.depl.reg.ha.control.recover">
    <title>&contrnode;(s)&mdash;Recovery</title>
    <para>
     Recovery of the &contrnode;(s) is done automatically by the cluster
     software: if one &contrnode; fails, Pacemaker will fail over the
     services to another &contrnode;. If a failed &contrnode; is
     repaired and rebuilt via &crow;, it will be automatically configured
     to join the cluster, at which point Pacemaker will have the option to
     fail services back if required.
    </para>
   </sect3>
  </sect2>

  <sect2 xml:id="sec.depl.reg.ha.compute">
   <title>&ha; of the &compnode;(s)</title>
   <para> If a &compnode; fails, all VMs running on that node will go down,
    too. While not avoiding failures of individual VMs, an &ha; setup for
    &compnode;s helps you to keep downtimes of &compnode;s short: If the
    nova compute service or libvirt fails on a &compnode; or the node itself
    should become unreachable, the node will be fenced and the VMs will be moved
    to a different &compnode;.</para>
   <para>If you decide to make use of &ha; for &compnode;s, your &compnode; will
    be run as Pacemaker remote nodes. With the <literal>pacemaker_remote</literal>
    service &ha; clusters can be extended to virtual nodes without the need
    to install the cluster stack on the remote nodes. The virtual machines only
    run the <literal>pacemaker_remote</literal> service (which requires almost
    no configuration on the VM's side).The cluster stack on the
     <quote>normal</quote> cluster nodes connects to the
     <literal>pacemaker_remote</literal> service on the VMs to integrate them as
    remote nodes into the cluster. </para>
   <para> Find more information about the <literal>remote_pacemaker</literal>
    service, including multiple use cases with detailed setup instructions in
    <citetitle>Pacemaker Remote&mdash;Extending High Availability into
    Virtual Nodes</citetitle>,
    available at <link xlink:href="http://www.clusterlabs.org/doc/"/>. </para>
   <para>To configure &ha; for &compnode;s, you need to adjust the following
   &barcl; proposals:</para>
   <itemizedlist>
    <listitem>
     <para>Pacemaker&mdash;for details, see <xref
       linkend="sec.depl.ostack.pacemaker"/>.</para>
    </listitem>
    <listitem>
     <para>Nova&mdash;for details, see <xref linkend="sec.depl.ostack.nova.ha"
      />.</para>
    </listitem>
   </itemizedlist>
   <para>
    <remark>taroth 2016-02-09: todo: should we recommend to use *separate*
     clusters for &compnode;s and&contrnode;s (question still open, see
     https://bugzilla.suse.com/show_bug.cgi?id=964205#c4)</remark>
   </para>
  </sect2>

  <sect2 xml:id="sec.depl.reg.ha.storage">
   <title>&ha; of the &stornode;(s)</title>
   <para>
    &productname; offers two different types of storage that can be used
    for the &stornode;s: object storage (provided by the &ostack;
    &swift; component) and block storage (provided by &ceph;).
   </para>
   <para>
    Both already consider &ha; aspects by design, therefore it does not
    require much effort to make the storage highly available.
   </para>
   <sect3 xml:id="sec.depl.reg.ha.storage.swift">
    <title>&swift;&mdash;Avoiding Points of Failure</title>
    <para>
     The &ostack; &objstore; replicates the data by design, provided
     the following requirements are met:
    </para>
    <itemizedlist mark="bullet" spacing="normal">
     <listitem>
      <para>
       The option <guimenu>Replicas</guimenu> in the &o_objstore;
       &barcl; is set to <literal>3</literal>, the tested and recommended
       value.
      </para>
     </listitem>
     <listitem>
      <para>
       The number of &stornode;s needs to be greater than the value set in
       the <guimenu>Replicas</guimenu> option.
      </para>
     </listitem>
    </itemizedlist>
    <procedure>
     <step>
      <para>
       To avoid single points of failure, assign the
       <systemitem>swift-storage</systemitem> role to multiple nodes.
      </para>
     </step>
     <step>
      <para>
       To make the API highly available, too, assign the
       <systemitem>swift-proxy</systemitem> role to a cluster instead of
       assigning it to a single &contrnode;. See
       <xref linkend="sec.depl.reg.ha.control.spof"/>. Other swift roles
       must not be deployed on a cluster.
      </para>
     </step>
    </procedure>
   </sect3>
   <sect3 xml:id="sec.depl.reg.ha.storage.ceph">
    <title>&ceph;&mdash;Avoiding Points of Failure</title>
    <para>
     &ceph; is a distributed storage solution that can provide &ha;.  For &ha;
     redundant storage and monitors need to be configured in the &ceph;
     cluster. For more information refer to <link
     xlink:href="&suse-onlinedoc;/ses-2/"/>.
    </para>
   </sect3>
  </sect2>

  <sect2 xml:id="sec.depl.reg.ha.general">
   <title>Cluster Requirements and Recommendations</title>
   <para>
    When considering to set up one ore more &ha; clusters, refer to the
    chapter <citetitle>System Requirements</citetitle> listed in the
    &haguide; for &sle; &hasi;. The guide is available at
    <link xlink:href="&suse-onlinedoc;/sle_ha/"/>.
   </para>
   <para>
    If you want to make the &contrnode; functions highly available, the
    requirements listed there also apply to &productname;. Note that by
    buying &productname;, you automatically get an entitlement for
    &sle; &hasi;.
   </para>
   <para>
    Especially note the following requirements:
   </para>
   <variablelist>
    <varlistentry xml:id="vle.ha.req.nodes">
     <term>Number of Cluster Nodes</term>
     <listitem>
      <para>
       Each cluster needs to consist of at least two cluster nodes.
      </para>
      <important>
       <title>Odd Number of Cluster Nodes</title>
       <para>
        It is strongly recommended to use an <emphasis>odd</emphasis> number
        of cluster nodes with a <emphasis>minimum</emphasis> of three nodes.
       </para>
       <para>
        A cluster needs
        <xref linkend="gloss.quorum" xrefstyle="select:label nopage"/> to
        keep services running. Therefore a three-node cluster can tolerate
        only failure of one node at a time, whereas a five-node cluster can
        tolerate failures of two nodes etc.
       </para>
      </important>
     </listitem>
    </varlistentry>
    <varlistentry xml:id="vle.ha.req.stonith">
     <term>&stonith;</term>
     <listitem>
      <para>
       The cluster software will shut down <quote>misbehaving</quote> nodes
       in a cluster to prevent them from causing trouble. This mechanism is
       called <literal>fencing</literal> or
       <xref linkend="gloss.stonith" xrefstyle="select:title nopage"/>.
      </para>
      <important>
       <title>No Support Without &stonith;</title>
       <para>
        A cluster without &stonith; is not supported.
       </para>
      </important>
      <para>
       For a supported &hasetup;, ensure the following:
      </para>
      <itemizedlist mark="bullet" spacing="normal">
       <listitem>
        <para>
         Each node in the &ha; cluster needs to have at least one
         &stonith; device (usually a piece of hardware). We strongly
         recommend multiple &stonith; devices per node, unless SBD is
         used.
        </para>
       </listitem>
       <listitem>
        <para>
         The global cluster options <systemitem>stonith-enabled</systemitem>
         and <systemitem>startup-fencing</systemitem> needs to be set to
         <literal>true</literal>. These options are set automatically when
         deploying the <systemitem>Pacemaker</systemitem> &barcl;. As
         soon as you change them, you will lose support.
        </para>
       </listitem>
       <listitem>
        <para>
         When deploying the <literal>Pacemaker</literal> service, select a
         <xref linkend="vle.pacemaker.barcl.stonith" xrefstyle="select:label nopage"/>
         that matches your setup. If your &stonith; devices support the
         IPMI protocol, choosing the IPMI option is the easiest way to
         configure &stonith;. Another alternative is SBD (&stonith;
         Block Device). It provides a way to enable &stonith; and fencing
         in clusters without external power switches, but it requires shared
         storage. For SBD requirements, see
         <link xlink:href="http://linux-ha.org/wiki/SBD_Fencing"/>, section
         <citetitle>Requirements</citetitle>.
        </para>
       </listitem>
      </itemizedlist>
      <para>
       For more information, refer to the &haguide;, available at
       <link xlink:href="&suse-onlinedoc;/sle_ha/"/>.
       Especially read the following chapters: <citetitle>Configuration and
       Administration Basics</citetitle>, and <citetitle>Fencing and
       &stonith;</citetitle>, <citetitle> Storage Protection</citetitle>.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry xml:id="vle.ha.req.communication">
     <term>Network Configuration</term>
     <listitem>
      <important>
       <title>Redundant Communication Paths</title>
       <para>
        For a supported &hasetup;, it is required to set up cluster
        communication via two or more redundant paths. For this purpose, use
        teaming network mode in your network setup. For details, see
        <xref linkend="sec.depl.req.network.modes.teaming"/>. At least two
        Ethernet cards per cluster node are required for network redundancy.
        It is advisable to use teaming network mode everywhere (not just
        between the cluster nodes) to ensure redundancy.
       </para>
      </important>
      <para>
       For more information, refer to the &haguide;, available at
       <link xlink:href="&suse-onlinedoc;/sle_ha/"/>.
       Especially read the following chapters: <citetitle>Network Device
       Bonding</citetitle>, and <citetitle>Installation and Basic
       Setup</citetitle> (section <citetitle>Defining the Communication
       Channels</citetitle>).
      </para>
     </listitem>
    </varlistentry>
    <varlistentry xml:id="vle.ha.req.storage">
     <term>Storage Requirements</term>
     <listitem>
<!-- taroth 2014-04-14: https://bugzilla.suse.com/show_bug.cgi?id=873373 -->
      <para>
       The following services require shared storage:
       <systemitem>database-server</systemitem> and
       <systemitem>rabbitmq-server</systemitem>. For this purpose, use
       either an external NFS share or DRBD.
      </para>
      <para>
       If using an external NFS share, the following additional requirements
       are important:
      </para>
      <itemizedlist mark="bullet" spacing="normal">
       <listitem>
        <para>
         The share needs to be reliably accessible from all cluster nodes via
         redundant communication paths. See
         <xref linkend="vle.ha.req.communication"/>.
        </para>
       </listitem>
       <listitem>
<!-- taroth 2014-04-14: https://bugzilla.suse.com/show_bug.cgi?id=872343 -->
        <para>
         The share needs to have certain settings in
         <filename>/etc/exports</filename> to be usable by the
         <systemitem>database</systemitem> &barcl;. For details, see
         <xref linkend="sec.depl.ostack.db.ha"/> and
         <xref linkend="sec.depl.ostack.rabbit.ha"/>.
        </para>
       </listitem>
      </itemizedlist>
      <para>
       If using DRDB, the following additional requirements are important:
      </para>
      <itemizedlist mark="bullet" spacing="normal">
       <listitem>
        <para>
         Because of a DRBD limitation, the cluster used for
         <systemitem>database-server</systemitem> and
         <systemitem>rabbitmq-server</systemitem> is restricted to two
         nodes.
        </para>
       </listitem>
       <listitem>
        <para>
         All nodes of the cluster that is used for
         <systemitem>database-server</systemitem> and
         <systemitem>rabbitmq-server</systemitem> needs to have an additional
         hard disk that will be used for DRBD. For more information on DRBD,
         see the <citetitle>DRBD</citetitle> chapter in the &haguide;,
         which is available at
         <link xlink:href="&suse-onlinedoc;/sle_ha/"/>.
        </para>
       </listitem>
      </itemizedlist>
      <para>
       When using SBD as &stonith; device, additional requirements apply
       for the shared storage. For details, see
       <link xlink:href="http://linux-ha.org/wiki/SBD_Fencing"/>, section
       <citetitle>Requirements</citetitle>.
      </para>
     </listitem>
    </varlistentry>
   </variablelist>
  </sect2>

  <sect2 xml:id="sec.depl.reg.ha.more">
   <title>For More Information</title>
   <para>
    For a basic understanding and detailed information on the &sle;
    &hasi; (including the Pacemaker cluster stack), read the
    &haguide;. It is available at
    <link xlink:href="&suse-onlinedoc;/sle_ha/"/>.
   </para>
   <para>
    In addition to the chapters mentioned in
    <xref linkend="sec.depl.reg.ha.general"/>, especially the following
    chapters are recommended:
   </para>
   <itemizedlist mark="bullet" spacing="normal">
    <listitem>
     <para>
      <citetitle>Product Overview</citetitle>
     </para>
    </listitem>
    <listitem>
     <para>
      <citetitle>Configuration and Administration Basics</citetitle>
     </para>
    </listitem>
   </itemizedlist>
   <para>
    The &haguide; also provides comprehensive information about the
    cluster management tools which you can view and check the cluster status
    in &productname;. They can also be used to look up details like
    configuration of cluster resources or global cluster options. Read the
    following chapters for more information:
   </para>
   <itemizedlist mark="bullet" spacing="normal">
    <listitem>
     <para>
      &haweb;: <citetitle>Configuring and Managing Cluster Resources (Web
      Interface)</citetitle>
     </para>
    </listitem>
    <listitem>
     <para>
      &hbgui;: <citetitle>Configuring and Managing Cluster Resources
      (GUI)</citetitle>
     </para>
    </listitem>
    <listitem>
     <para>
      <command>crm.sh</command>: <citetitle> Configuring and Managing
      Cluster Resources (Command Line)</citetitle>
     </para>
    </listitem>
   </itemizedlist>
  </sect2>
 </sect1>
 <sect1 xml:id="sec.depl.req.summary">
  <title>Summary: Considerations and Requirements</title>

  <para>
   As outlined above, there are some important considerations to be made
   before deploying &cloud;. The following briefly summarizes what was
   discussed in detail in this chapter. Keep in mind that as of
   &productname; &productnumber; it is not possible to change some
   aspects such as the network setup once &cloud; is deployed!
   <remark condition="clarity">
    2016-01-08 - fs: FIXME: A HA summary is missing
   </remark>
  </para>
  <itemizedlist mark="bullet" spacing="normal">
   <title>Network</title>
   <listitem>
    <para>
     If you do not want to stick with the default networks and addresses,
     define custom networks and addresses. You need five different networks.
     If you need to separate the admin and the BMC network, a sixth network
     is required. See <xref linkend="sec.depl.req.network"/> for details.
     Networks that share interfaces need to be configured as VLANs.
    </para>
   </listitem>
   <listitem>
    <para>
     The &cloud; networks are completely isolated, therefore it is not
     required to use public IP addresses for them. A class C network as used
     in this documentation may not provide enough addresses for a cloud that
     is supposed to grow. You may alternatively choose addresses from a
     class B or A network.
    </para>
   </listitem>
   <listitem>
    <para>
     Determine how to allocate addresses from your network. Make sure not to
     allocate IP addresses twice. See
     <xref linkend="sec.depl.req.network.allocation"/> for the default
     allocation scheme.
    </para>
   </listitem>
   <listitem>
    <para>
     Define which network mode to use. Keep in mind that all machines within
     the cloud (including the &admserv;) will be set up with the chosen
     mode and therefore need to meet the hardware requirements. See
     <xref linkend="sec.depl.req.network.modes"/> for details.
    </para>
   </listitem>
   <listitem>
    <para>
     Define how to access the admin and BMC network(s): no access from the
     outside (no action is required), via an external gateway (gateway needs
     to be provided), or via bastion network. See
     <xref linkend="sec.depl.req.network.bastion"/> for details.
    </para>
   </listitem>
   <listitem>
    <para>
     Provide a gateway to access the public network (public, nova-floating).
    </para>
<!-- TODO vuntz 2013-12-04: check if also for fixed? -->
   </listitem>
   <listitem>
    <para>
     Make sure the admin server's host name is correctly configured
     (<command>hostname</command> <option>-f</option> needs to return a
     fully qualified host name). If this is not the case, run <menuchoice>
     <guimenu>&yast;</guimenu> <guimenu>Network Services</guimenu>
     <guimenu>Hostnames</guimenu></menuchoice> and add a fully qualified
     host name.
    </para>
   </listitem>
   <listitem>
    <para>
     Prepare a list of MAC addresses and the intended use of the
     corresponding host for all &ostack; nodes.
    </para>
   </listitem>
  </itemizedlist>

  <itemizedlist mark="bullet" spacing="normal">
   <title>Update Repositories</title>
   <listitem>
    <para>
     Depending on your network setup you have different options on how to
     provide up-to-date update repositories for &sls; and &cloud; for
     &cloud; deployment: using an existing &smt; or &susemgr;
     server, installing &smt; on the &admserv;, synchronizing data
     with an existing repository, mounting remote repositories or using a
     <quote>Sneakernet</quote>. Choose the option that best matches your
     needs.
    </para>
   </listitem>
  </itemizedlist>

  <itemizedlist mark="bullet" spacing="normal">
   <title>Storage</title>
   <listitem>
    <para>
     Decide whether you want to deploy the object storage service
     &swift;. If so, you need to deploy at least two nodes with
     sufficient disk space exclusively dedicated to &swift;.
    </para>
   </listitem>
   <listitem>
    <para>
     Decide which back-end to use with &o_blockstore;. If using the
     <guimenu>raw</guimenu> back-end (local disks) it is strongly
     recommended to use a separate node equipped with several hard disks for
     deploying <literal>cinder-volume</literal>. If using &ceph;, you
     need to deploy at least four nodes with sufficient disk space
     exclusively dedicated to it.
    </para>
   </listitem>
   <listitem>
    <para>
     Make sure all &compnode;s are equipped with sufficient hard disk
     space.
    </para>
   </listitem>
  </itemizedlist>

  <itemizedlist mark="bullet" spacing="normal">
   <title>SSL Encryption</title>
   <listitem>
    <para>
     Decide whether to use different SSL certificates for the services and
     the API or whether to use a single certificate.
    </para>
   </listitem>
   <listitem>
    <para>
     Get one or more SSL certificates certified by a trusted third party
     source.
    </para>
   </listitem>
  </itemizedlist>

  <itemizedlist mark="bullet" spacing="normal">
   <title>Hardware and Software Requirements</title>
   <listitem>
    <para>
     Make sure the hardware requirements for the different node types are
     met.
    </para>
   </listitem>
   <listitem>
    <para>
     Make sure to have all required software at hand.
    </para>
   </listitem>
  </itemizedlist>
 </sect1>
 <sect1 xml:id="sec.depl.req.installation">
  <title>Overview of the &cloud; Installation</title>

  <para>
   Deploying and installing &productname; is a multi-step process,
   starting by deploying a basic &sls; installation and the
   &productname; add-on product to the &admserv;. Now the product and
   update repositories need to be set up and the &cloud; network needs to
   be configured. Next the &admserv; setup will be finished. After the
   &admserv; is ready, you can start deploying and configuring the
   &ostack; nodes. The complete node deployment is done automatically via
   &crow; and &chef; from the &admserv;. All you need to do is to
   boot the nodes using PXE and to deploy the &ostack; services to them.
  </para>

  <procedure>
   <step>
    <para>
     Install &cloudos; on the &admserv; with the add-on product
     &cloud;. Optionally select the &smtool; pattern for installation. See
     <xref linkend="cha.depl.adm_inst"/>.
    </para>
   </step>
   <step>
    <para>
     Optionally set up and configure the &smt; server on the &admserv;. See
     <xref linkend="app.deploy.smt"/>.
    </para>
   </step>
   <step>
    <para>
     Make all required software repositories available on the &admserv;. See
     <xref linkend="cha.depl.repo_conf"/>.
    </para>
   </step>
   <step>
    <para>
     Set up the network on the &admserv; See
     <xref linkend="sec.depl.adm_inst.network"/>.
    </para>
   </step>
   <step>
    <para>
     Perform the &crow; setup to configure the &cloud; network and to make the
     repository locations known. When the configuration is done, start the
     &inst_crow;. See <xref
     linkend="sec.depl.adm_inst.crowbar"/>.
    </para>
   </step>
   <step>
    <para>
     Boot all nodes onto which the &ostack; components should be deployed
     using PXE and allocate them in the &crow; Web interface to start the
     automatic &sls; installation. See
     <xref linkend="cha.depl.inst.nodes"/>.
    </para>
   </step>
   <step>
    <para>
     Configure and deploy the &ostack; services via the &crow; Web
     interface or command line tools. See <xref linkend="cha.depl.ostack"/>.
    </para>
   </step>
   <step>
    <para>
     When all &ostack; services are up and running, &cloud; is ready.
     The cloud administrator can now upload images to enable users to start
     deploying &vmguest;s. See the &cloudadmin; and the
     &cloudsuppl;.
<!--See <xref linkend="book.cloud.admin"/>.-->
    </para>
   </step>
  </procedure>
 </sect1>
</chapter>
