<?xml version="1.0"?>
<!DOCTYPE section [
<!ENTITY % entities SYSTEM "entities.ent"> %entities;
]>
<section xmlns="http://docbook.org/ns/docbook" xmlns:xi="http://www.w3.org/2001/XInclude"
         xmlns:xlink="http://www.w3.org/1999/xlink" xml:id="telemetry_alarmdefinitions">
 <title>Telemetry Alarms</title>
 <para>
  These alarms show under the Telemetry section of the &productname; Ops Console.
 </para>
 <informaltable colsep="1" rowsep="1">
  <tgroup cols="5">
   <colspec colname="c1" colnum="1"/>
   <colspec colname="c2" colnum="2"/>
   <colspec colname="c3" colnum="3"/>
   <colspec colname="c4" colnum="4"/>
   <colspec colname="c5" colnum="5"/>
   <thead>
    <row>
     <entry>Service</entry>
     <entry>警报名称</entry>
     <entry>描述</entry>
     <entry>可能的原因</entry>
     <entry>故障排除步骤</entry>
    </row>
   </thead>
   <tbody>
    <!---->
    <row>
     <entry morerows="4">telemetry</entry>
     <entry>Process Check</entry>
     <entry>Alarms when the <literal>ceilometer-agent-notification</literal> process is not
      running.</entry>
     <entry>Process has crashed.</entry>
     <entry>
      <para>
       Review the logs on the alarming host in the following location for the
       cause:
      </para>
      <screen>/var/log/ceilometer/ceilometer-agent-notification-json.log</screen>
      <para>
       用以下步骤重启受影响的节点的进程：
      </para>
      <orderedlist xml:id="ol_ybg_3zp_mx">
       <listitem>
        <para>
         登入 &lcm;。
        </para>
       </listitem>
       <listitem>
        <para>
         对受影响的节点使用 Ceilometer 启动剧本：
        </para>
        <screen>cd ~/scratch/ansible/next/hos/ansible
         ansible-playbook -i hosts/verb_hosts ceilometer-start.yml --limit &lt;hostname&gt;</screen>
       </listitem>
      </orderedlist>
     </entry>
    </row>
    <row>
     <entry>Process Check</entry>
     <entry>Alarms when the <literal>ceilometer-polling</literal> process is not
      running.</entry>
     <entry>Process has crashed.</entry>
     <entry>
      <para>
       Review the logs on the alarming host in the following location for the
       cause:
      </para>
      <screen>/var/log/ceilometer/ceilometer-polling-json.log</screen>
      <para>
       用以下步骤重启受影响的节点的进程：
      </para>
      <orderedlist xml:id="ol_zbg_3zp_mx">
       <listitem>
        <para>
         登入 &lcm;。
        </para>
       </listitem>
       <listitem>
        <para>
         对受影响的节点使用 Ceilometer 启动剧本：
        </para>
        <screen>cd ~/scratch/ansible/next/hos/ansible
         ansible-playbook -i hosts/verb_hosts ceilometer-start.yml --limit &lt;hostname&gt;</screen>
       </listitem>
      </orderedlist>
     </entry>
    </row>
    <row>
     <entry>Process Check</entry>
     <entry>Alarms when the <literal>ceilometer-api</literal> process is not running.</entry>
     <entry>Process has crashed.</entry>
     <entry>
      <para>
       Review the logs on the alarming host in the following location for the
       cause:
      </para>
      <screen>/var/log/ceilometer/ceilometer-api-json.log</screen>
      <para>
       用以下步骤重启受影响的节点的进程：
      </para>
      <orderedlist xml:id="ol_dfv_vbt_lx">
       <listitem>
        <para>
         登入 &lcm;。
        </para>
       </listitem>
       <listitem>
        <para>
         对受影响的节点使用 Ceilometer 启动剧本：
        </para>
        <screen>cd ~/scratch/ansible/next/hos/ansible
         ansible-playbook -i hosts/verb_hosts ceilometer-start.yml --limit &lt;hostname&gt;</screen>
       </listitem>
      </orderedlist>
     </entry>
    </row>
    <row>
     <entry>HTTP Status</entry>
     <entry>
      <para>
       Alarms when the specified HTTP endpoint is down or not reachable.
      </para>
      <screen>endpoint_type=host_endpoint</screen>
     </entry>
     <entry>The Ceilometer API on the host defined in the <literal>hostname</literal> is down
      or not reachable.</entry>
     <entry>
      <para>
       Restart Apache on the affected node using these steps:
      </para>
      <orderedlist xml:id="ol_a53_gct_lx">
       <listitem>
        <para>
         登入 &lcm;。
        </para>
       </listitem>
       <listitem>
        <para>
         Confirm the status of Apache with this playbook:
        </para>
        <screen>cd ~/scratch/ansible/next/hos/ansible
         ansible-playbook -i hosts/verb_hosts FND-AP2-status.yml</screen>
       </listitem>
       <listitem>
        <para>
         Stop the Apache service, if necessary:
        </para>
        <screen>cd ~/scratch/ansible/next/hos/ansible
         ansible-playbook -i hosts/verb_hosts FND-AP2-stop.yml --limit &lt;hostname&gt;</screen>
       </listitem>
       <listitem>
        <para>
         Use this playbook against the affected node to restart Apache:
        </para>
        <screen>cd ~/scratch/ansible/next/hos/ansible
         ansible-playbook -i hosts/verb_hosts FND-AP2-start.yml --limit &lt;hostname&gt;</screen>
       </listitem>
      </orderedlist>
     </entry>
    </row>
    <row>
     <entry>HTTP Status</entry>
     <entry>
      <para>
       Alarms when the specified HTTP endpoint is down or not reachable.
      </para>
      <screen>endpoint_type=internal</screen>
     </entry>
     <entry>The Ceilometer API on the internal virtual IP address is down or not
      reachable.</entry>
     <entry>
      <para>
       If this occurs with an <literal>http_status</literal> in error on all
       nodes, then restart Apache on all controllers with these steps:
      </para>
      <orderedlist xml:id="ol_dk3_kct_lx">
       <listitem>
        <para>
         登入 &lcm;。
        </para>
       </listitem>
       <listitem>
        <para>
         Confirm the status of Apache with this playbook:
        </para>
        <screen>cd ~/scratch/ansible/next/hos/ansible
         ansible-playbook -i hosts/verb_hosts FND-AP2-status.yml</screen>
       </listitem>
       <listitem>
        <para>
         Stop the Apache service, if necessary:
        </para>
        <screen>cd ~/scratch/ansible/next/hos/ansible
         ansible-playbook -i hosts/verb_hosts FND-AP2-stop.yml --limit &lt;hostname&gt;</screen>
       </listitem>
       <listitem>
        <para>
         Use this playbook against your controller nodes to restart Apache:
        </para>
        <screen>cd ~/scratch/ansible/next/hos/ansible
         ansible-playbook -i hosts/verb_hosts FND-AP2-start.yml --limit &lt;hostname&gt;</screen>
       </listitem>
      </orderedlist>
      <para>
       If this occurs with a specific host with <literal>http_status</literal>
       in non-error for telemetry, then it should be a haproxy issue and it
       needs to be restarted.
      </para>
      <orderedlist xml:id="ol_acg_3zp_mx">
       <listitem>
        <para>
         登入 &lcm;。
        </para>
       </listitem>
       <listitem>
        <para>
         Confirm the status of haproxy with this playbook:
        </para>
        <screen>cd ~/scratch/ansible/next/hos/ansible
         ansible-playbook -i hosts/verb_hosts FND-CLU-status.yml --limit &lt;hostname&gt;</screen>
       </listitem>
       <listitem>
        <para>
         Stop the haproxy service, if necessary:
        </para>
        <screen>cd ~/scratch/ansible/next/hos/ansible
         ansible-playbook -i hosts/verb_hosts FND-CLU-stop.yml --limit &lt;hostname&gt;</screen>
       </listitem>
       <listitem>
        <para>
         Restart the haproxy service with this playbook:
        </para>
        <screen>cd ~/scratch/ansible/next/hos/ansible
         ansible-playbook -i hosts/verb_hosts FND-CLU-start.yml --limit &lt;hostname&gt;</screen>
       </listitem>
      </orderedlist>
      <para>
       For further troubleshooting, SSH to the affected host and look at the
       folllowing Ceilometer access logs:
      </para>
      <screen>/var/log/ceilometer/ceilometer_modwsgi.log
       /var/log/ceilometer/ceilometer-api.log</screen>
     </entry>
    </row>
    <row>
     <entry>metering</entry>
     <entry>Service Log Directory Size</entry>
     <entry>服务日志目录在使用超出配额的硬盘空间。</entry>
     <entry>这可能是因为服务被设置成 <literal>DEBUG</literal> 而不是 <literal>INFO</literal> 等级。另外可能的原因是一个重复的错误信息填满了日志文件。最后，这也可能是因为日志滚动没有被设置正确，因而旧的日志没有被正常删除。</entry>
     <entry>找到消耗过多硬盘空间的服务，查看日志。如果有 <literal>DEBUG</literal> 日志条目，请把日志等级设为 <literal>INFO</literal>。如果日志在重复记录一个错误信息，请解决该错误信息。如果旧的日志文件存在，配置日志滚动设置来自动移除它们。如果需要，您也可以选择备份后手动删除旧的日志。</entry>
    </row>
    <!---->
    <row>
     <entry morerows="3">kafka</entry>
     <entry>Kafka Persister Metric Consumer Lag</entry>
     <entry>Alarms when the Persister consumer group is not keeping up with the incoming
      messages on the metric topic.</entry>
     <entry>There is a slow down in the system or heavy load.</entry>
     <entry>
      <para>
       Verify that all of the monasca-persister services are up with these
       steps:
      </para>
      <orderedlist xml:id="ol_z3p_2ff_wv">
       <listitem>
        <para>
         Log in to the &lcm;
        </para>
       </listitem>
       <listitem>
        <para>
         Verify that all of the <literal>monasca-persister</literal> services
         are up with this playbook:
        </para>
        <screen>cd ~/scratch/ansible/next/hos/ansible
         ansible-playbook -i hosts/verb_hosts monasca-status.yml --tags monasca-persister</screen>
       </listitem>
      </orderedlist>
      <para>
       Look for high load in the various systems. This alert can fire for
       multiple topics or on multiple hosts. Which alarms are firing can help
       diagnose likely causes. For example, if the alarm is alerting all on one
       machine it could be the machine. If one topic across multiple machines
       it is likely the consumers of that topic, etc.
      </para>
     </entry>
    </row>
    <row>
     <entry>Kafka Alarm Transition Consumer Lag</entry>
     <entry>Alarms when the specified consumer group is not keeping up with the incoming
      messages on the alarm state transistion topic.</entry>
     <entry>There is a slow down in the system or heavy load.</entry>
     <entry>
      <para>
       Check that monasca-thresh and monasca-notification are up.
      </para>
      <para>
       Look for high load in the various systems. This alert can fire for
       multiple topics or on multiple hosts. Which alarms are firing can help
       diagnose likely causes, ie if all on one machine it could be the
       machine. If one topic across multiple machines it is likely the
       consumers of that topic, etc.
      </para>
     </entry>
    </row>
    <row>
     <entry>Kafka Kronos Consumer Lag</entry>
     <entry>Alarms when the Kronos consumer group is not keeping up with the incoming messages
      on the metric topic.</entry>
     <entry>There is a slow down in the system or heavy load.</entry>
     <entry>Look for high load in the various systems. This alert can fire for multiple topics
      or on multiple hosts. Which alarms are firing can help diagnose likely causes, ie if all
      on one machine it could be the machine. If one topic across multiple machines it is
      likely the consumers of that topic, etc.</entry>
    </row>
    <row>
     <entry>Process Check</entry>
     <entry>
      <para>
       特定进程未在运行时的警报。
      </para>
      <screen>process_name = kafka.Kafka</screen>
     </entry>
     <entry/>
     <entry>
      <para>
       用以下步骤重启受影响的节点的进程：
      </para>
      <orderedlist xml:id="ol_lrq_lzp_mx">
       <listitem>
        <para>
         登入 &lcm;。
        </para>
       </listitem>
       <listitem>
        <para>
         Stop the kafka service with this playbook:
        </para>
        <screen>cd ~/scratch/ansible/next/hos/ansible
         ansible-playbook -i hosts/verb_hosts monasca-start.yml --tags kafka</screen>
       </listitem>
       <listitem>
        <para>
         Start the kafka service back up with this playbook:
        </para>
        <screen>cd ~/scratch/ansible/next/hos/ansible
         ansible-playbook -i hosts/verb_hosts monasca-status.yml --tags kafka</screen>
       </listitem>
      </orderedlist>
      <para>
       Review the logs in this location:
      </para>
      <screen>/var/log/kafka/server.log</screen>
     </entry>
    </row>
    <!---->
    <row>
     <entry morerows="7">logging</entry>
     <entry>Beaver Memory Usage</entry>
     <entry>Beaver is using more memory than expected. This may indicate that it cannot forward
      messages and it's queue is filling up. If you continue to see this, see the
      troubleshooting guide.</entry>
     <entry>Overloaded system or services with memory leaks.</entry>
     <entry>Log onto the reporting host to investigate high memory users.</entry>
    </row>
    <row>
     <entry>Audit Log Partition Low Watermark</entry>
     <entry>The <literal>/var/audit</literal> disk space usage has crossed low watermark. If
      the high watermark is reached, logrotate will be run to free up disk space. Adjust
      <literal>var_audit_low_watermark_percent</literal> if needed.</entry>
     <entry>这可能是因为服务被设置成 <literal>DEBUG</literal> 而不是 <literal>INFO</literal> 等级。另外可能的原因是一个重复的错误信息填满了日志文件。最后，这也可能是因为日志滚动没有被设置正确，因而旧的日志没有被正常删除。</entry>
     <entry>找到消耗过多硬盘空间的服务，查看日志。如果有 <literal>DEBUG</literal> 日志条目，请把日志等级设为 <literal>INFO</literal>。如果日志在重复记录一个错误信息，请解决该错误信息。如果旧的日志文件存在，配置日志滚动设置来自动移除它们。如果需要，您也可以选择备份后手动删除旧的日志。</entry>
    </row>
    <row>
     <entry>Audit Log Partition High Watermark</entry>
     <entry>The <literal>/var/audit</literal> volume is running low on disk space. Logrotate
      will be run now to free up space. Adjust
      <literal>var_audit_high_watermark_percent</literal> if needed.</entry>
     <entry>这可能是因为服务被设置成 <literal>DEBUG</literal> 而不是 <literal>INFO</literal> 等级。另外可能的原因是一个重复的错误信息填满了日志文件。最后，这也可能是因为日志滚动没有被设置正确，因而旧的日志没有被正常删除。</entry>
     <entry>找到消耗过多硬盘空间的服务，查看日志。如果有 <literal>DEBUG</literal> 日志条目，请把日志等级设为 <literal>INFO</literal>。如果日志在重复记录一个错误信息，请解决该错误信息。如果旧的日志文件存在，配置日志滚动设置来自动移除它们。如果需要，您也可以选择备份后手动删除旧的日志。</entry>
    </row>
    <row>
     <entry>Elasticsearch Unassigned Shards</entry>
     <entry>
      <screen>component = elasticsearch</screen>
      <para>
       Elasticsearch unassigned shards count is greater than 0.
      </para>
     </entry>
     <entry>Environment could be misconfigured.</entry>
     <entry>
      <para>
       To find the unassigned shards, run the following command on the
       &lcm; from the
       <literal>~/scratch/ansible/next/hos/ansible</literal> directory:
      </para>
      <screen>cd ~/scratch/ansible/next/hos/ansible
       ansible -i hosts/verb_hosts LOG-SVR[0] -m shell -a "curl localhost:9200/_cat/shards?pretty -s" | grep UNASSIGNED</screen>
      <para>
       This should show which shards are unassigned, like this:
      </para>
      <screen>logstash-2015.10.21 4 p UNASSIGNED 11412371  3.2gb 10.241.67.11 Keith Kilham</screen>
      <para>
       The last column shows the name that Elasticsearch uses for the node that
       the unassigned shards are on. To find the actual hostname, run:
      </para>
      <screen>cd ~/scratch/ansible/next/hos/ansible
       ansible -i hosts/verb_hosts LOG-SVR[0] -m shell -a "curl localhost:9200/_nodes/_all/name?pretty -s"</screen>
      <para>
       Once you find the hostname, you can try the following:
      </para>
      <orderedlist xml:id="ol_bcg_3zp_mx">
       <listitem>
        <para>
         Make sure the node is not out of disk space, and free up space if
         needed.
        </para>
       </listitem>
       <listitem>
        <para>
         Restart the node (use caution, as this may affect other services as
         well).
        </para>
       </listitem>
       <listitem>
        <para>
         Check to make sure all versions of Elasticsearch are the same with
         this:
        </para>
        <screen>cd ~/scratch/ansible/next/hos/ansible
         ansible -i hosts/verb_hosts LOG-SVR -m shell -a "curl localhost:9200/_nodes/_local/name?pretty -s" | grep version</screen>
       </listitem>
       <listitem>
        <para>
         Contact customer support.
        </para>
       </listitem>
      </orderedlist>
     </entry>
    </row>
    <row>
     <entry>Elasticsearch Number of Log Entries</entry>
     <entry>
      <screen>component = elasticsearch</screen>
      <para>
       Elasticsearch Number of Log Entries.
      </para>
     </entry>
     <entry>The number of log entries may get too large.</entry>
     <entry>Older versions of Kibana (version 3 and earlier) may hang if the number of log
      entries is too large (e.g. above 40,000), and the page size would need to be small
      enough (about 20,000 results), because if it is larger (e.g. 200,000), it may hang the
      browser, but Kibana 4 should not have this issue.</entry>
    </row>
    <row>
     <entry>Elasticsearch Field Data Evictions</entry>
     <entry>
      <screen>component = elasticsearch</screen>
      <para>
       Elasticsearch Field Data Evictions count is greater than 0.
      </para>
     </entry>
     <entry>Field Data Evictions may be found even though it is nowhere near the limit
      set.</entry>
     <entry>
      <para>
       The <literal>elasticsearch_indices_fielddata_cache_size</literal> is set
       to <literal>unbounded</literal> by default. If this is set by the user
       to a value that is insufficient, you may need to increase this
       configuration parameter or set it to unbounded and run a reconfigure
       using the steps below:
      </para>
      <orderedlist xml:id="ol_ccg_3zp_mx">
       <listitem>
        <para>
         登入 &lcm;。
        </para>
       </listitem>
       <listitem>
        <para>
         Edit the configuration file below and change the value for
         <literal>elasticsearch_indices_fielddata_cache_size</literal> to your
         desired value:
        </para>
        <screen>~/openstack/my_cloud/config/logging/main.yml</screen>
       </listitem>
       <listitem>
        <para>
         Commit the changes to git:
        </para>
        <screen>git add -A
         git commit -a -m "changing Elasticsearch fielddata cache size"</screen>
       </listitem>
       <listitem>
        <para>
         Run the configuration processor:
        </para>
        <screen>cd ~/openstack/ardana/ansible
         ansible-playbook -i hosts/localhost config-processor-run.yml</screen>
       </listitem>
       <listitem>
        <para>
         Update your deployment directory:
        </para>
        <screen>cd ~/openstack/ardana/ansible
         ansible-playbook -i hosts/localhost ready-deployment.yml</screen>
       </listitem>
       <listitem>
        <para>
         Run the Logging reconfigure playbook to deploy the change:
        </para>
        <screen>cd ~/scratch/ansible/next/hos/ansible/
         ansible-playbook -i hosts/verb_hosts kronos-reconfigure.yml</screen>
       </listitem>
      </orderedlist>
     </entry>
    </row>
    <row>
     <entry>Service Log Directory Size</entry>
     <entry>
      <para>
       服务日志目录在使用超出配额的硬盘空间。
      </para>
     </entry>
     <entry>这可能是因为服务被设置成 <literal>DEBUG</literal> 而不是 <literal>INFO</literal> 等级。另外可能的原因是一个重复的错误信息填满了日志文件。最后，这也可能是因为日志滚动没有被设置正确，因而旧的日志没有被正常删除。</entry>
     <entry>找到消耗过多硬盘空间的服务，查看日志。如果有 <literal>DEBUG</literal> 日志条目，请把日志等级设为 <literal>INFO</literal>。如果日志在重复记录一个错误信息，请解决该错误信息。如果旧的日志文件存在，配置日志滚动设置来自动移除它们。如果需要，您也可以选择备份后手动删除旧的日志。</entry>
    </row>
    <row>
     <entry>Process Check</entry>
     <entry>
      <para>
       由 <literal>component</literal> 维度指定的不同 logging 服务警报：
       <literal>process_name</literal> dimension:
      </para>
      <itemizedlist xml:id="ul_dcg_3zp_mx">
       <listitem>
        <para>
         elasticsearch
        </para>
       </listitem>
       <listitem>
        <para>
         logstash
        </para>
       </listitem>
       <listitem>
        <para>
         beaver
        </para>
       </listitem>
       <listitem>
        <para>
         apache2
        </para>
       </listitem>
       <listitem>
        <para>
         kibana
        </para>
       </listitem>
      </itemizedlist>
     </entry>
     <entry>Process has crashed.</entry>
     <entry>
      <para>
       On the affected node, attempt to restart the process.
      </para>
      <para>
       If the elasticsearch process has crashed, use:
      </para>
      <screen>sudo systemctl restart elasticsearch</screen>
      <para>
       If the logstash process has crashed, use:
      </para>
      <screen>sudo systemctl restart logstash</screen>
      <para>
       The rest of the processes can be restarted using similar commands,
       listed here:
      </para>
      <screen>sudo systemctl restart beaver
       sudo systemctl restart apache2
       sudo systemctl restart kibana</screen>
     </entry>
    </row>
    <!---->
    <row>
     <entry morerows="2">monasca-transform</entry>
     <entry>Process Check</entry>
     <entry>
      <screen>process_name = pyspark</screen>
     </entry>
     <entry>Service process has crashed.</entry>
     <entry>
      <para>
       Restart process on affected node. Review logs.
      </para>
      <para>
       Child process of <literal>spark-worker</literal> but created once the
       <literal>monasca-transform</literal> process begins processing streams.
       If the process fails on one node only, along with the pyspark process,
       it's very likely that the <literal>spark-worker</literal> has failed to
       connect to the elected leader of the <literal>spark-master</literal>
       service. In this case the <literal>spark-worker</literal> service should
       be started on the affected node. If on multiple nodes check the
       <literal>spark-worker</literal>, <literal>spark-master</literal> and
       <literal>monasca-transform</literal> services and logs. If the
       <literal>monasca-transform</literal> or <literal>spark</literal>
       services have been interrupted this process may not re-appear for up to
       ten minutes (the stream processing interval).
      </para>
     </entry>
    </row>
    <row>
     <entry>Process Check</entry>
     <entry>
      <screen>process_name = org.apache.spark.executor.CoarseGrainedExecutorBackend</screen>
     </entry>
     <entry>Service process has crashed.</entry>
     <entry>
      <para>
       Restart process on affected node. Review logs.
      </para>
      <para>
       Child process of <literal>spark-worker</literal> but created once the
       <literal>monasca-transform</literal> process begins processing streams.
       If the process fails on one node only, along with the pyspark process,
       it's very likely that the <literal>spark-worker</literal> has failed to
       connect to the elected leader of the <literal>spark-master</literal>
       service. In this case the <literal>spark-worker</literal> service should
       be started on the affected node. If on multiple nodes check the
       <literal>spark-worker</literal>, <literal>spark-master</literal> and
       <literal>monasca-transform</literal> services and logs. If the
       <literal>monasca-transform</literal> or <literal>spark</literal>
       services have been interrupted this process may not re-appear for up to
       ten minutes (the stream processing interval).
      </para>
     </entry>
    </row>
    <row>
     <entry>Process Check</entry>
     <entry>
      <screen>process_name = monasca-transform</screen>
     </entry>
     <entry>Service process has crashed.</entry>
     <entry>
      <para>
       Restart the service on affected node. Review logs.
      </para>
     </entry>
    </row>
    <!---->
    <row>
     <entry morerows="12">monitoring</entry>
     <entry>HTTP Status</entry>
     <entry>
      <screen>component = monasca-persister</screen>
      <para>
       Persister Health Check
      </para>
     </entry>
     <entry>The process has crashed or a dependency is out.</entry>
     <entry>
      <para>
       If the process has crashed, restart it using the steps below. If a
       dependent service is down, address that issue.
      </para>
      <para>
       用以下步骤重启受影响的节点的进程：
      </para>
      <orderedlist xml:id="ol_ecg_3zp_mx">
       <listitem>
        <para>
         登入 &lcm;。
        </para>
       </listitem>
       <listitem>
        <para>
         Check if <literal>monasca-api</literal> is running on all nodes with
         this playbook:
        </para>
        <screen>cd ~/scratch/ansible/next/hos/ansible
         ansible-playbook -i hosts/verb_hosts monasca-status.yml --tags monasca-persister</screen>
       </listitem>
       <listitem>
        <para>
         对受影响的节点使用 Monasca 启动剧本 to restart
         it:
        </para>
        <screen>cd ~/scratch/ansible/next/hos/ansible
         ansible-playbook -i hosts/verb_hosts monasca-start.yml --tags persister</screen>
       </listitem>
       <listitem>
        <para>
         Verify that it is running on all nodes with this playbook:
        </para>
        <screen>cd ~/scratch/ansible/next/hos/ansible
         ansible-playbook -i hosts/verb_hosts monasca-status.yml --tags monasca-persister</screen>
       </listitem>
      </orderedlist>
      <para>
       检查相关日志。
      </para>
     </entry>
    </row>
    <row>
     <entry>HTTP Status</entry>
     <entry>
      <screen>component = monasca-api</screen>
      <para>
       API Health Check
      </para>
     </entry>
     <entry>The process has crashed or a dependency is out.</entry>
     <entry>
      <para>
       If the process has crashed, restart it using the steps below. If a
       dependent service is down, address that issue.
      </para>
      <para>
       用以下步骤重启受影响的节点的进程：
      </para>
      <orderedlist>
       <listitem>
        <para>
         登入 &lcm;。
        </para>
       </listitem>
       <listitem>
        <para>
         Check if <literal>monasca-api</literal> is running on all nodes with
         this playbook:
        </para>
        <screen>cd ~/scratch/ansible/next/hos/ansible
         ansible-playbook -i hosts/verb_hosts monasca-status.yml --tags monasca-api</screen>
       </listitem>
       <listitem>
        <para>
         对受影响的节点使用 Monasca 启动剧本 to restart
         it:
        </para>
        <screen>cd ~/scratch/ansible/next/hos/ansible
         ansible-playbook -i hosts/verb_hosts monasca-start.yml --tags monasca-api</screen>
       </listitem>
       <listitem>
        <para>
         Verify that it is running on all nodes with this playbook:
        </para>
        <screen>cd ~/scratch/ansible/next/hos/ansible
         ansible-playbook -i hosts/verb_hosts monasca-status.yml --tags monasca-api</screen>
       </listitem>
      </orderedlist>
      <para>
       检查相关日志。
      </para>
     </entry>
    </row>
    <row>
     <entry>Monasca Agent Collection Time</entry>
     <entry>Alarms when the elapsed time the <literal>monasca-agent</literal> takes to collect
      metrics is high.</entry>
     <entry>Heavy load on the box or a stuck agent plug-in.</entry>
     <entry>
      <para>
       Address the load issue on the machine. If needed, restart the agent
       using the steps below:
      </para>
      <para>
       Restart the agent on the affected node using these steps:
      </para>
      <orderedlist xml:id="ol_gcg_3zp_mx">
       <listitem>
        <para>
         登入 &lcm;。
        </para>
       </listitem>
       <listitem>
        <para>
         Check if <literal>monasca-agent</literal> is running on all nodes with
         this playbook:
        </para>
        <screen>cd ~/scratch/ansible/next/hos/ansible
         ansible-playbook -i hosts/verb_hosts monasca-agent-status.yml</screen>
       </listitem>
       <listitem>
        <para>
         对受影响的节点使用 Monasca 启动剧本 to restart
         it:
        </para>
        <screen>cd ~/scratch/ansible/next/hos/ansible
         ansible-playbook -i hosts/verb_hosts monasca-start.yml --limit &lt;hostname&gt;</screen>
       </listitem>
       <listitem>
        <para>
         Verify that it is running on all nodes with this playbook:
        </para>
        <screen>cd ~/scratch/ansible/next/hos/ansible
         ansible-playbook -i hosts/verb_hosts monasca-agent-status.yml</screen>
       </listitem>
      </orderedlist>
      <para>
       检查相关日志。
      </para>
     </entry>
    </row>
    <row>
     <entry>Process Check</entry>
     <entry>
      <para>
       特定进程未在运行时的警报。
      </para>
      <screen>component = kafka</screen>
     </entry>
     <entry>Process crashed.</entry>
     <entry>
      <para>
       用以下步骤重启受影响的节点的进程：
      </para>
      <orderedlist>
       <listitem>
        <para>
         登入 &lcm;。
        </para>
       </listitem>
       <listitem>
        <para>
         Check if Kafka is running on all nodes with this playbook:
        </para>
        <screen>cd ~/scratch/ansible/next/hos/ansible
         ansible-playbook -i hosts/verb_hosts monasca-status.yml --tags kafka</screen>
       </listitem>
       <listitem>
        <para>
         对受影响的节点使用 Monasca 启动剧本 to restart
         it:
        </para>
        <screen>cd ~/scratch/ansible/next/hos/ansible
         ansible-playbook -i hosts/verb_hosts monasca-start.yml --tags kafka</screen>
       </listitem>
       <listitem>
        <para>
         Verify that Kafka is running on all nodes with this playbook:
        </para>
        <screen>cd ~/scratch/ansible/next/hos/ansible
         ansible-playbook -i hosts/verb_hosts monasca-status.yml --tags kafka</screen>
       </listitem>
      </orderedlist>
      <para>
       检查相关日志。
      </para>
     </entry>
    </row>
    <row>
     <entry>Process Check</entry>
     <entry>
      <para>
       特定进程未在运行时的警报。
      </para>
      <screen>process_name = monasca-notification</screen>
     </entry>
     <entry>Process crashed.</entry>
     <entry>
      <para>
       用以下步骤重启受影响的节点的进程：
      </para>
      <orderedlist>
       <listitem>
        <para>
         登入 &lcm;。
        </para>
       </listitem>
       <listitem>
        <para>
         Check if <literal>monasca-api</literal> is running on all nodes with
         this playbook:
        </para>
        <screen>cd ~/scratch/ansible/next/hos/ansible
         ansible-playbook -i hosts/verb_hosts monasca-status.yml --tags notification</screen>
       </listitem>
       <listitem>
        <para>
         对受影响的节点使用 Monasca 启动剧本 to restart
         it:
        </para>
        <screen>cd ~/scratch/ansible/next/hos/ansible
         ansible-playbook -i hosts/verb_hosts monasca-start.yml --tags notification</screen>
       </listitem>
       <listitem>
        <para>
         Verify that it is running on all nodes with this playbook:
        </para>
        <screen>cd ~/scratch/ansible/next/hos/ansible
         ansible-playbook -i hosts/verb_hosts monasca-status.yml --tags notification</screen>
       </listitem>
      </orderedlist>
      <para>
       检查相关日志。
      </para>
     </entry>
    </row>
    <row>
     <entry>Process Check</entry>
     <entry>
      <para>
       特定进程未在运行时的警报。
      </para>
      <screen>process_name = monasca-agent</screen>
     </entry>
     <entry>Process crashed.</entry>
     <entry>
      <para>
       Restart the agent on the affected node using these steps:
      </para>
      <orderedlist>
       <listitem>
        <para>
         登入 &lcm;。
        </para>
       </listitem>
       <listitem>
        <para>
         Check if <literal>monasca-agent</literal> is running on all nodes with
         this playbook:
        </para>
        <screen>cd ~/scratch/ansible/next/hos/ansible
         ansible-playbook -i hosts/verb_hosts monasca-agent-status.yml</screen>
       </listitem>
       <listitem>
        <para>
         对受影响的节点使用 Monasca 启动剧本 to restart
         it:
        </para>
        <screen>cd ~/scratch/ansible/next/hos/ansible
         ansible-playbook -i hosts/verb_hosts monasca-start.yml --limit &lt;hostname&gt;</screen>
       </listitem>
       <listitem>
        <para>
         Verify that it is running on all nodes with this playbook:
        </para>
        <screen>cd ~/scratch/ansible/next/hos/ansible
         ansible-playbook -i hosts/verb_hosts monasca-agent-status.yml</screen>
       </listitem>
      </orderedlist>
      <para>
       检查相关日志。
      </para>
     </entry>
    </row>
    <row>
     <entry>Process Check</entry>
     <entry>
      <para>
       特定进程未在运行时的警报。
      </para>
      <screen>process_name = monasca-api</screen>
     </entry>
     <entry>Process crashed.</entry>
     <entry>
      <para>
       > 用以下步骤重启受影响的节点的进程：
       </para>
       <orderedlist>
        <listitem>
         <para>
          登入 &lcm;。
         </para>
        </listitem>
        <listitem>
         <para>
          Check if <literal>monasca-api</literal> is running on all nodes with
          this playbook:
         </para>
         <screen>cd ~/scratch/ansible/next/hos/ansible
          ansible-playbook -i hosts/verb_hosts monasca-status.yml --tags monasca-api</screen>
        </listitem>
        <listitem>
         <para>
          对受影响的节点使用 Monasca 启动剧本 to restart
          it:
         </para>
         <screen>cd ~/scratch/ansible/next/hos/ansible
          ansible-playbook -i hosts/verb_hosts monasca-start.yml --tags monasca-api</screen>
        </listitem>
        <listitem>
         <para>
          Verify that it is running on all nodes with this playbook:
         </para>
         <screen>cd ~/scratch/ansible/next/hos/ansible
          ansible-playbook -i hosts/verb_hosts monasca-status.yml --tags monasca-api</screen>
        </listitem>
       </orderedlist>
       <para>
        检查相关日志。
       </para>
      </entry>
     </row>
     <row>
      <entry>Process Check</entry>
      <entry>
       <para>
        特定进程未在运行时的警报。
       </para>
       <screen>process_name = monasca-persister</screen>
      </entry>
      <entry>Process crashed.</entry>
      <entry>
       <para>
        用以下步骤重启受影响的节点的进程：
       </para>
       <orderedlist>
        <listitem>
         <para>
          登入 &lcm;。
         </para>
        </listitem>
        <listitem>
         <para>
          Check if <literal>monasca-api</literal> is running on all nodes with
          this playbook:
         </para>
         <screen>cd ~/scratch/ansible/next/hos/ansible
          ansible-playbook -i hosts/verb_hosts monasca-status.yml --tags monasca-persister</screen>
        </listitem>
        <listitem>
         <para>
          对受影响的节点使用 Monasca 启动剧本 to restart
          it:
         </para>
         <screen>cd ~/scratch/ansible/next/hos/ansible
          ansible-playbook -i hosts/verb_hosts monasca-start.yml --tags persister</screen>
        </listitem>
        <listitem>
         <para>
          Verify that it is running on all nodes with this playbook:
         </para>
         <screen>cd ~/scratch/ansible/next/hos/ansible
          ansible-playbook -i hosts/verb_hosts monasca-status.yml --tags monasca-persister</screen>
        </listitem>
       </orderedlist>
       <para>
        检查相关日志。
       </para>
      </entry>
     </row>
     <row>
      <entry>Process Check</entry>
      <entry>
       <para>
        特定进程未在运行时的警报。
       </para>
       <screen>process_name = backtype.storm.daemon.nimbus
        component = apache-storm</screen>
      </entry>
      <entry>Process crashed.</entry>
      <entry>
       <para>
        Review the logs in the <literal>/var/log/storm</literal> directory on
        all storm hosts to find the root cause.
       </para>
       <para>
        Restart monasca-thresh, if necessary, with these steps:
       </para>
       <orderedlist>
        <listitem>
         <para>
          登入 &lcm;。
         </para>
        </listitem>
        <listitem>
         <para>
          Check if <literal>monasca-thresh</literal> is running on all nodes
          with this playbook:
         </para>
         <screen>cd ~/scratch/ansible/next/hos/ansible
          ansible-playbook -i hosts/verb_hosts monasca-status.yml --tags thresh</screen>
        </listitem>
        <listitem>
         <para>
          对受影响的节点使用 Monasca 启动剧本 to restart
          it:
         </para>
         <screen>cd ~/scratch/ansible/next/hos/ansible
          ansible-playbook -i hosts/verb_hosts monasca-start.yml --tags thresh</screen>
        </listitem>
        <listitem>
         <para>
          Verify that it is running on all nodes with this playbook:
         </para>
         <screen>cd ~/scratch/ansible/next/hos/ansible
          ansible-playbook -i hosts/verb_hosts monasca-status.yml --tags thresh</screen>
        </listitem>
       </orderedlist>
      </entry>
     </row>
     <row>
      <entry>Process Check</entry>
      <entry>
       <para>
        特定进程未在运行时的警报。
       </para>
       <screen>process_name = backtype.storm.daemon.supervisor
        component = apache-storm</screen>
      </entry>
      <entry>Process crashed.</entry>
      <entry>
       <para>
        Review the logs in the <literal>/var/log/storm</literal> directory on
        all storm hosts to find the root cause.
       </para>
       <para>
        Restart monasca-thresh with these steps:
       </para>
       <orderedlist>
        <listitem>
         <para>
          登入 &lcm;。
         </para>
        </listitem>
        <listitem>
         <para>
          Stop the monasca-thresh service:
         </para>
         <screen>cd ~/scratch/ansible/next/hos/ansible
          ansible-playbook -i hosts/verb_hosts monasca-stop.yml --tags thresh</screen>
        </listitem>
        <listitem>
         <para>
          Start the monasca-thresh service back up:
         </para>
         <screen>cd ~/scratch/ansible/next/hos/ansible
          ansible-playbook -i hosts/verb_hosts monasca-start.yml --tags thresh</screen>
        </listitem>
        <listitem>
         <para>
          Verify that it is running on all nodes with this playbook:
         </para>
         <screen>cd ~/scratch/ansible/next/hos/ansible
          ansible-playbook -i hosts/verb_hosts monasca-status.yml --tags thresh</screen>
        </listitem>
       </orderedlist>
      </entry>
     </row>
     <row>
      <entry>Process Check</entry>
      <entry>
       <para>
        特定进程未在运行时的警报。
       </para>
       <screen>process_name = backtype.storm.daemon.worker
        component = apache-storm</screen>
      </entry>
      <entry>Process crashed.</entry>
      <entry>
       <para>
        Review the logs in the <literal>/var/log/storm</literal> directory on
        all storm hosts to find the root cause.
       </para>
       <para>
        Restart monasca-thresh with these steps:
       </para>
       <orderedlist>
        <listitem>
         <para>
          登入 &lcm;。
         </para>
        </listitem>
        <listitem>
         <para>
          Stop the monasca-thresh service:
         </para>
         <screen>cd ~/scratch/ansible/next/hos/ansible
          ansible-playbook -i hosts/verb_hosts monasca-stop.yml --tags thresh</screen>
        </listitem>
        <listitem>
         <para>
          Start the monasca-thresh service back up:
         </para>
         <screen>cd ~/scratch/ansible/next/hos/ansible
          ansible-playbook -i hosts/verb_hosts monasca-start.yml --tags thresh</screen>
        </listitem>
        <listitem>
         <para>
          Verify that it is running on all nodes with this playbook:
         </para>
         <screen>cd ~/scratch/ansible/next/hos/ansible
          ansible-playbook -i hosts/verb_hosts monasca-status.yml --tags thresh</screen>
        </listitem>
       </orderedlist>
      </entry>
     </row>
     <row>
      <entry>Process Check</entry>
      <entry>
       <para>
        特定进程未在运行时的警报。
       </para>
       <screen>process_name = monasca-thresh
        component = apache-storm</screen>
      </entry>
      <entry>Process crashed.</entry>
      <entry>
       <para>
        用以下步骤重启受影响的节点的进程：
       </para>
       <orderedlist>
        <listitem>
         <para>
          登入 &lcm;。
         </para>
        </listitem>
        <listitem>
         <para>
          Check if <literal>monasca-thresh</literal> is running on all nodes
          with this playbook:
         </para>
         <screen>cd ~/scratch/ansible/next/hos/ansible
          ansible-playbook -i hosts/verb_hosts monasca-status.yml --tags thresh</screen>
        </listitem>
        <listitem>
         <para>
          对受影响的节点使用 Monasca 启动剧本 to restart
          it:
         </para>
         <screen>cd ~/scratch/ansible/next/hos/ansible
          ansible-playbook -i hosts/verb_hosts monasca-start.yml --tags thresh</screen>
        </listitem>
        <listitem>
         <para>
          Verify that it is running on all nodes with this playbook:
         </para>
         <screen>cd ~/scratch/ansible/next/hos/ansible
          ansible-playbook -i hosts/verb_hosts monasca-status.yml --tags thresh</screen>
        </listitem>
       </orderedlist>
       <para>
        检查相关日志。
       </para>
      </entry>
     </row>
     <row>
      <entry>Service Log Directory Size</entry>
      <entry>服务日志目录在使用超出配额的硬盘空间。</entry>
      <entry>The service log directory, as indicated by the <literal>path</literal> dimension,
       is over the 2.5 GB quota.</entry>
      <entry>找到消耗过多硬盘空间的服务，查看日志。如果有 <literal>DEBUG</literal> 日志条目，请把日志等级设为 <literal>INFO</literal>。如果日志在重复记录一个错误信息，请解决该错误信息。如果旧的日志文件存在，配置日志滚动设置来自动移除它们。如果需要，您也可以选择备份后手动删除旧的日志。</entry>
     </row>
     <!---->
     <row>
      <entry morerows="8">vertica</entry>
      <entry>Vertica Resource Rejection</entry>
      <entry>Alarms when a resource rejection occurs.</entry>
      <entry>
       <para>
        Vertica is getting a resource rejection on one of its resource pools.
        Resource pools are a pre-allocated subset of system resources and are
        associated with a queue of queries that will be executed against it.
       </para>
       <para>
        When this alarm occurs, that resource pool is hitting issues due to lack
        of memory, threads, file handles or execution slots when executing its
        queries.
       </para>
      </entry>
      <entry>
       <para>
        To troubleshoot this issue you will need to access the Vertica database
        on the reporting monitoring node, as indicated by the
        <literal>hostname</literal> dimension.
       </para>
       <para>
        To obtain the Vertica database admin user credentials, use these steps:
       </para>
       <orderedlist xml:id="ol_qvm_px4_mx">
        <listitem>
         <para>
          If you do not use data encryption on your &lcm;, use these
          steps:
         </para>
         <orderedlist xml:id="ol_ocg_3zp_mx">
          <listitem>
           <para>
            登入 &lcm;。
           </para>
          </listitem>
          <listitem>
           <para>
            Use this command to obtain the password:
           </para>
           <screen>grep -r dbadmin_user_password ~/scratch/ansible/next/hos/ansible/group_vars/</screen>
          </listitem>
         </orderedlist>
        </listitem>
        <listitem>
         <para>
          If you do use data encryption on your &lcm;, contact
          Support for assistance in obtaining your Vertica database admin
          credentials.
         </para>
        </listitem>
       </orderedlist>
       <para>
        Once you have your credentials, follow these steps:
       </para>
       <orderedlist xml:id="ol_pcg_3zp_mx">
        <listitem>
         <para>
          Log in to the monitoring node as indicated by the
          <literal>hostname</literal> dimension of the alarm.
         </para>
        </listitem>
        <listitem>
         <para>
          Become root:
         </para>
         <screen>sudo su</screen>
        </listitem>
        <listitem>
         <para>
          Run this command, using your Vertica admin password
         </para>
         <screen>/opt/vertica/bin/vsql -U dbadmin -w <emphasis>&lt;vertica_admin_user_password&gt;</emphasis></screen>
        </listitem>
        <listitem>
         <para>
          Query the database and look at its resource rejections to see what is
          causing the failure. This can be obtained by querying the resource
          rejection table. For details on how to do this, see the
          <link
           xlink:href="https://my.vertica.com/docs/7.2.x/HTML/Content/Authoring/SQLReferenceManual/SystemTables/MONITOR/RESOURCE_REJECTIONS.htm"
           >Vertica
           documentation</link> on this topic.
         </para>
        </listitem>
        <listitem>
         <para>
          Once the cause of the failure is determined, you can address the
          problems with the resource pool using the
          <link
           xlink:href="https://my.vertica.com/docs/7.2.x/HTML/index.htm#Authoring/AdministratorsGuide/ResourceManager/ManagingWorkloads.htm%3FTocPath%3DAdministrator"
           >Vertica
           documentation</link> which may include altering specific resource
          pools to allocate more resources.
         </para>
        </listitem>
       </orderedlist>
      </entry>
     </row>
     <row>
      <entry>Vertica Status</entry>
      <entry>
       <para>
        Alarms if a vertica node cannot connect to its database.
       </para>
       <para>
        If both this alarm and the Process Check alarm for the
        <literal>vertica</literal> process go off at the same time, do the
        mitigation steps listed for the Process Check alarm first.
       </para>
      </entry>
      <entry>Process crashed.</entry>
      <entry>
       <para>
        Verify that the database node is down and, if it is, then recover it by
        using these steps:
       </para>
       <orderedlist xml:id="ol_urq_lzp_mx">
        <listitem>
         <para>
          登入 &lcm;。
         </para>
        </listitem>
        <listitem>
         <para>
          运行 Monasca status 剧本：
         </para>
         <screen>cd ~/scratch/ansible/next/hos/ansible
          ansible-playbook -i hosts/verb_hosts monasca-status.yml --tags vertica</screen>
        </listitem>
        <listitem>
         <para>
          Run this playbook to do a recovery on Vertica:
         </para>
         <screen>cd ~/scratch/ansible/next/hos/ansible
          ansible-playbook -i hosts/verb_hosts monasca-vertica-recovery.yml</screen>
        </listitem>
       </orderedlist>
      </entry>
     </row>
     <row>
      <entry>Process Check</entry>
      <entry>
       <para>
        特定进程未在运行时的警报。
       </para>
       <screen>process_name=vertica</screen>
      </entry>
      <entry>Process crashed.</entry>
      <entry>
       <para>
        Review the logs on the alarming host to find the root cause.
       </para>
       <orderedlist xml:id="ol_onk_gfv_qx">
        <listitem>
         <para>
          Log in to the alarming host.
         </para>
        </listitem>
        <listitem>
         <para>
          Run the following commands:
         </para>
         <screen>sudo su dbadmin
          cd /var/vertica
          find . -name vertica.log</screen>
        </listitem>
       </orderedlist>
       <para>
        If the process needs to be restarted, use the following steps:
       </para>
       <orderedlist xml:id="ol_zdd_kfv_qx">
        <listitem>
         <para>
          登入 &lcm;。
         </para>
        </listitem>
        <listitem>
         <para>
          Run this playbook:
         </para>
         <screen>cd ~/scratch/ansible/next/hos/ansible
          ansible-playbook -i hosts/verb_hosts monasca-start.yml --tags vertica</screen>
        </listitem>
        <listitem>
         <para>
          Verify that the process has restarted:
         </para>
         <screen>cd ~/scratch/ansible/next/hos/ansible
          ansible-playbook -i hosts/verb_hosts monasca-status.yml --tags vertica</screen>
        </listitem>
       </orderedlist>
      </entry>
     </row>
     <row>
      <entry>Vertica Disk Rejection</entry>
      <entry>Alarms when vertica can not write to disk.</entry>
      <entry>
       <para>
        Vertica got a rejection error when writing to disk. This can mean one of
        two things:
       </para>
       <itemizedlist xml:id="ul_qcg_3zp_mx">
        <listitem>
         <para>
          That the file system that vertica is writing to is full, or
         </para>
        </listitem>
        <listitem>
         <para>
          There is a disk problem.
         </para>
        </listitem>
       </itemizedlist>
       <para>
        If the disk is full then the load that is on the system is higher then
        expected or the retention period for Vertica is not aggressive enough
        and needs to be addressed.
       </para>
      </entry>
      <entry>
       <para>
        To resolve this issue, use these steps:
       </para>
       <orderedlist xml:id="ol_bhr_drp_mx">
        <listitem>
         <para>
          Log in to the node having the issue, as indicated by the
          <literal>hostname</literal> dimension.
         </para>
        </listitem>
        <listitem>
         <para>
          Check the Vertica filesystem using this command:
         </para>
         <screen>df -h</screen>
         <para>
          If <literal>/var/vertica</literal> shows greater than 90% usage then
          you have hit the limit of the Vertica filesystem.
         </para>
        </listitem>
        <listitem>
         <para>
          If that is the case, then you should recreate your Vertica database
          using the steps documented here: <xref linkend="recover_vertica"/>.
         </para>
        </listitem>
       </orderedlist>
       <para>
        To ensure you do not run into this issue again, you should do the
        following:
       </para>
       <itemizedlist xml:id="ul_rcg_3zp_mx">
        <listitem>
         <para>
          Turn down your metric load.
         </para>
        </listitem>
        <listitem>
         <para>
          Lower the retention period on Vertica, as detailed in
          <xref
           linkend="recover_vertica"/>.
          </para>
         </listitem>
         <listitem>
          <para>
           Allocate more disk space to the Vertica file system.
          </para>
         </listitem>
        </itemizedlist>
        <para>
         If the file system is not showing greater than 90% usage, then the
         Vertica file system could be in a bad state. Verify that the Vertica
         file system can still be written to and that it is functional.
        </para>
       </entry>
      </row>
      <row>
       <entry>Vertica Low Watermark License Usage</entry>
       <entry>Alarms when vertica database is taking up 70 percent of the license size.</entry>
       <entry>Alarms when the amount of raw data in your vertica database takes up 70 percent of
        the current vertica license limit.</entry>
       <entry>
        <para>
         Since the system is approaching the Vertica license limit you may want
         to start looking into these options:
        </para>
        <itemizedlist xml:id="ul_bm4_dsp_mx">
         <listitem>
          <para>
           Contacting Vertica to obtain a new license with more size than the
           current 12 TB
          </para>
         </listitem>
         <listitem>
          <para>
           Look at the metric knobs and see if there are any you can turn down.
          </para>
         </listitem>
         <listitem>
          <para>
           Lowering the Vertica retention period, as detailed in
           <xref
            linkend="recover_vertica"/>.
           </para>
          </listitem>
         </itemizedlist>
        </entry>
       </row>
       <row>
        <entry>Vertica High Watermark License Usage</entry>
        <entry>Alarms when vertica database is taking up 90 percent of the license size.</entry>
        <entry>Alarms when the amount of raw data in your vertica database takes up 90 percent of
         the current vertica license limit.</entry>
        <entry>
         <para>
          If this alarm triggers you will need to use one of the following options
          to resolve this issue:
         </para>
         <itemizedlist>
          <listitem>
           <para>
            Turn down the Vertica retention period, as detailed in
            <xref
             linkend="recover_vertica"/>.
            </para>
           </listitem>
           <listitem>
            <para>
             Turn down the metric load.
            </para>
           </listitem>
          </itemizedlist>
         </entry>
        </row>
        <row>
         <entry>Vertica Node Status</entry>
         <entry>Alarms when the Vertica node is not up.</entry>
         <entry>The vertica node is not up.</entry>
         <entry>
          <para>
           Verify that the database node is down and, if it is, then recover it by
           using these steps:
          </para>
          <orderedlist xml:id="ol_my3_fsp_mx">
           <listitem>
            <para>
             登入 &lcm;。
            </para>
           </listitem>
           <listitem>
            <para>
             运行 Monasca status 剧本：
            </para>
            <screen>cd ~/scratch/ansible/next/hos/ansible
             ansible-playbook -i hosts/verb_hosts monasca-status.yml --tags vertica</screen>
           </listitem>
           <listitem>
            <para>
             Run this playbook to do a recovery on Vertica:
            </para>
            <screen>cd ~/scratch/ansible/next/hos/ansible
             ansible-playbook -i hosts/verb_hosts monasca-vertica-recovery</screen>
           </listitem>
          </orderedlist>
         </entry>
        </row>
        <row>
         <entry>Vertica Connection Status</entry>
         <entry>Alarms when there is no connection to Vertica.</entry>
         <entry>Usually means that the node is down.</entry>
         <entry>
          <para>
           Verify that the database node is down and, if it is, then recover it by
           using these steps:
          </para>
          <orderedlist xml:id="ol_syc_gsp_mx">
           <listitem>
            <para>
             登入 &lcm;。
            </para>
           </listitem>
           <listitem>
            <para>
             运行 Monasca status 剧本：
            </para>
            <screen>cd ~/scratch/ansible/next/hos/ansible
             ansible-playbook -i hosts/verb_hosts monasca-status.yml --tags vertica</screen>
           </listitem>
           <listitem>
            <para>
             Run this playbook to do a recovery on Vertica:
            </para>
            <screen>cd ~/scratch/ansible/next/hos/ansible
             ansible-playbook -i hosts/verb_hosts monasca-vertica-recovery</screen>
           </listitem>
          </orderedlist>
         </entry>
        </row>
        <row>
         <entry>Vertica Projection Ros Count</entry>
         <entry>Alarms when ros count for a projection goes over the limit.</entry>
         <entry>This usually means that the tuple mover is getting backed up and can not merge the
          files on disks. If not addressed it has the potential of putting your vertica database
          into a failed state.</entry>
         <entry>
          <para>
           To troubleshoot this issue you will need to access the Vertica database
           on the reporting monitoring node, as indicated by the
           <literal>hostname</literal> dimension.
          </para>
          <para>
           To obtain the Vertica database admin user credentials, use these steps:
          </para>
          <orderedlist xml:id="ol_vz1_fvp_mx">
           <listitem>
            <para>
             If you do not use data encryption on your &lcm;, use these
             steps:
            </para>
            <orderedlist xml:id="ol_wz1_fvp_mx">
             <listitem>
              <para>
               登入 &lcm;。
              </para>
             </listitem>
             <listitem>
              <para>
               Use this command to obtain the password:
              </para>
              <screen>grep -r dbadmin_user_password ~/scratch/ansible/next/hos/ansible/group_vars/</screen>
             </listitem>
            </orderedlist>
           </listitem>
           <listitem>
            <para>
             If you do use data encryption on your &lcm;, contact
             Support for assistance in obtaining your Vertica database admin
             credentials.
            </para>
           </listitem>
          </orderedlist>
          <para>
           Once you have your credentials, follow these steps:
          </para>
          <orderedlist xml:id="ol_scg_3zp_mx">
           <listitem>
            <para>
             登入 &lcm;。
            </para>
           </listitem>
           <listitem>
            <para>
             Run this playbook to stop the Monasca persister process:
            </para>
            <screen>cd ~/scratch/ansible/next/hos/ansible
             ansible-playbook -i hosts/verb_hosts monasca-stop.yml --tags persister</screen>
           </listitem>
           <listitem>
            <para>
             Manually merge the ros files for Vertica
            </para>
           </listitem>
           <listitem>
            <para>
             Log onto the monitoring node.
            </para>
           </listitem>
           <listitem>
            <para>
             Log into Vertica using the password obtained from the steps above (as
             root)
            </para>
            <screen>sudo su
             /opt/vertica/bin/vsql -U dbadmin -w {{ vertica_admin_user_password }}</screen>
           </listitem>
           <listitem>
            <para>
             Manually merge ros files running the following vsql commands:
            </para>
            <screen>SELECT DO_TM_TASK('mergeout', 'MonAlarms.StateHistory');
             SELECT DO_TM_TASK('mergeout', 'MonMetrics.DefinitionDimensions');
             SELECT DO_TM_TASK('mergeout', 'MonMetrics.Definitions');
             SELECT DO_TM_TASK('mergeout', 'MonMetrics.Dimensions');
             SELECT DO_TM_TASK('mergeout', 'MonMetrics.Measurements');</screen>
           </listitem>
          </orderedlist>
          <para>
           Restart the monasca-persister at this time, using these steps:
          </para>
          <orderedlist xml:id="ol_ozw_gsp_mx">
           <listitem>
            <para>
             登入 &lcm;。
            </para>
           </listitem>
           <listitem>
            <para>
             Run monasca-stop focused on the persister
            </para>
            <screen>cd ~/scratch/ansible/next/hos/ansible
             ansible-playbook -i hosts/verb_hosts monasca-start.yml --tags persister</screen>
           </listitem>
          </orderedlist>
          <para>
           To make sure this does not occur again, increase the Monasca wos
           resource pool max memory.
          </para>
         </entry>
        </row>
       </tbody>
      </tgroup>
     </informaltable>
    </section>
    
