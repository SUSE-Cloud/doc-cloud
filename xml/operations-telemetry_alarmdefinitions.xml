<?xml version="1.0"?>
<!DOCTYPE section [
 <!ENTITY % entities SYSTEM "entity-decl.ent"> %entities;
]>
<section xmlns="http://docbook.org/ns/docbook" xmlns:xi="http://www.w3.org/2001/XInclude"
  xmlns:xlink="http://www.w3.org/1999/xlink" xml:id="telemetry_alarmdefinitions">
 <title>Telemetry Alarms</title>
 <para>
  These alarms show under the Telemetry section of the &productname; &opscon;.
 </para>
 <informaltable colsep="1" rowsep="1">
  <tgroup cols="5">
   <colspec colname="c1" colnum="1"/>
   <colspec colname="c2" colnum="2"/>
   <colspec colname="c3" colnum="3"/>
   <colspec colname="c4" colnum="4"/>
   <colspec colname="c5" colnum="5"/>
   <thead>
    <row>
     <entry>Service</entry>
     <entry>Alarm Name</entry>
     <entry>Description</entry>
     <entry>Likely Cause</entry>
     <entry>Mitigation Tasks to Perform</entry>
    </row>
   </thead>
   <tbody>
    <row>
     <entry morerows="4">telemetry</entry>
     <entry>Process Check</entry>
     <entry>
      <para>
       Alarms when the
       <literal>ceilometer-agent-notification</literal> process is not
       running.
      </para>
     </entry>
     <entry>Process has crashed.</entry>
     <entry>
      <para>
       Review the logs on the alarming host in the following location for the
       cause:
      </para>
<screen>/var/log/ceilometer/ceilometer-agent-notification-json.log</screen>
      <para>
       Restart the process on the affected node using these steps:
      </para>
      <orderedlist>
       <listitem>
        <para>
         Log in to the &clm;.
        </para>
       </listitem>
       <listitem>
        <para>
         Use the Ceilometer start playbook against the affected node:
        </para>
<screen>&prompt.ardana;cd ~/scratch/ansible/next/ardana/ansible
&prompt.ardana;ansible-playbook -i hosts/verb_hosts ceilometer-start.yml --limit &lt;hostname&gt;</screen>
       </listitem>
      </orderedlist>
     </entry>
    </row>
    <row>
     <entry>Process Check</entry>
     <entry>
      <para>
       Alarms when the <literal>ceilometer-polling</literal> process is not
       running.
      </para>
     </entry>
     <entry>Process has crashed.</entry>
     <entry>
      <para>
       Review the logs on the alarming host in the following location for the
       cause:
      </para>
<screen>/var/log/ceilometer/ceilometer-polling-json.log</screen>
      <para>
       Restart the process on the affected node using these steps:
      </para>
      <orderedlist>
       <listitem>
        <para>
         Log in to the &clm;.
        </para>
       </listitem>
       <listitem>
        <para>
         Use the Ceilometer start playbook against the affected node:
        </para>
<screen>&prompt.ardana;cd ~/scratch/ansible/next/ardana/ansible
&prompt.ardana;ansible-playbook -i hosts/verb_hosts ceilometer-start.yml --limit &lt;hostname&gt;</screen>
       </listitem>
      </orderedlist>
     </entry>
    </row>
    <row>
     <entry>Process Check</entry>
     <entry>Alarms when the <literal>ceilometer-api</literal> process is not running.</entry>
     <entry>Process has crashed.</entry>
     <entry>
      <para>
       Review the logs on the alarming host in the following location for the
       cause:
      </para>
<screen>/var/log/ceilometer/ceilometer-api-json.log</screen>
      <para>
       Restart the process on the affected node using these steps:
      </para>
      <orderedlist>
       <listitem>
        <para>
         Log in to the &clm;.
        </para>
       </listitem>
       <listitem>
        <para>
         Use the Ceilometer start playbook against the affected node:
        </para>
<screen>&prompt.ardana;cd ~/scratch/ansible/next/ardana/ansible
&prompt.ardana;ansible-playbook -i hosts/verb_hosts ceilometer-start.yml --limit &lt;hostname&gt;</screen>
       </listitem>
      </orderedlist>
     </entry>
    </row>
    <row>
     <entry>HTTP Status</entry>
     <entry>
      <para>
       Alarms when the specified HTTP endpoint is down or not reachable.
      </para>
<screen>endpoint_type=host_endpoint</screen>
     </entry>
     <entry>The Ceilometer API on the host defined in the <literal>hostname</literal> is down
            or not reachable.</entry>
     <entry>
      <para>
       Restart Apache on the affected node using these steps:
      </para>
      <orderedlist xml:id="ol_a53_gct_lx">
       <listitem>
        <para>
         Log in to the &clm;.
        </para>
       </listitem>
       <listitem>
        <para>
         Confirm the status of Apache with this playbook:
        </para>
<screen>&prompt.ardana;cd ~/scratch/ansible/next/ardana/ansible
&prompt.ardana;ansible-playbook -i hosts/verb_hosts FND-AP2-status.yml</screen>
       </listitem>
       <listitem>
        <para>
         Stop the Apache service, if necessary:
        </para>
<screen>&prompt.ardana;cd ~/scratch/ansible/next/ardana/ansible
&prompt.ardana;ansible-playbook -i hosts/verb_hosts FND-AP2-stop.yml --limit &lt;hostname&gt;</screen>
       </listitem>
       <listitem>
        <para>
         Use this playbook against the affected node to restart Apache:
        </para>
<screen>&prompt.ardana;cd ~/scratch/ansible/next/ardana/ansible
&prompt.ardana;ansible-playbook -i hosts/verb_hosts FND-AP2-start.yml --limit &lt;hostname&gt;</screen>
       </listitem>
      </orderedlist>
     </entry>
    </row>
    <row>
     <entry>HTTP Status</entry>
     <entry>
      <para>
       Alarms when the specified HTTP endpoint is down or not reachable.
      </para>
<screen>endpoint_type=internal</screen>
     </entry>
     <entry>The Ceilometer API on the internal virtual IP address is down or not
            reachable.</entry>
     <entry>
      <para>
       If this occurs with an <literal>http_status</literal> in error on all
       nodes, then restart Apache on all controllers with these steps:
      </para>
      <orderedlist>
       <listitem>
        <para>
         Log in to the &clm;.
        </para>
       </listitem>
       <listitem>
        <para>
         Confirm the status of Apache with this playbook:
        </para>
<screen>&prompt.ardana;cd ~/scratch/ansible/next/ardana/ansible
&prompt.ardana;ansible-playbook -i hosts/verb_hosts FND-AP2-status.yml</screen>
       </listitem>
       <listitem>
        <para>
         Stop the Apache service, if necessary:
        </para>
<screen>&prompt.ardana;cd ~/scratch/ansible/next/ardana/ansible
&prompt.ardana;ansible-playbook -i hosts/verb_hosts FND-AP2-stop.yml --limit &lt;hostname&gt;</screen>
       </listitem>
       <listitem>
        <para>
         Use this playbook against your controller nodes to restart Apache:
        </para>
<screen>&prompt.ardana;cd ~/scratch/ansible/next/ardana/ansible
&prompt.ardana;ansible-playbook -i hosts/verb_hosts FND-AP2-start.yml --limit &lt;hostname&gt;</screen>
       </listitem>
      </orderedlist>
      <para>
       If this occurs with a specific host with <literal>http_status</literal>
       in non-error for telemetry, then it should be a haproxy issue and it
       needs to be restarted.
      </para>
      <orderedlist>
       <listitem>
        <para>
         Log in to the &clm;.
        </para>
       </listitem>
       <listitem>
        <para>
         Confirm the status of haproxy with this playbook:
        </para>
<screen>&prompt.ardana;cd ~/scratch/ansible/next/ardana/ansible
&prompt.ardana;ansible-playbook -i hosts/verb_hosts FND-CLU-status.yml --limit &lt;hostname&gt;</screen>
       </listitem>
       <listitem>
        <para>
         Stop the haproxy service, if necessary:
        </para>
<screen>&prompt.ardana;cd ~/scratch/ansible/next/ardana/ansible
&prompt.ardana;ansible-playbook -i hosts/verb_hosts FND-CLU-stop.yml --limit &lt;hostname&gt;</screen>
       </listitem>
       <listitem>
        <para>
         Restart the haproxy service with this playbook:
        </para>
<screen>&prompt.ardana;cd ~/scratch/ansible/next/ardana/ansible
&prompt.ardana;ansible-playbook -i hosts/verb_hosts FND-CLU-start.yml --limit &lt;hostname&gt;</screen>
       </listitem>
      </orderedlist>
      <para>
       For further troubleshooting, SSH to the affected host and look at the
       folllowing Ceilometer access logs:
      </para>
<screen>/var/log/ceilometer/ceilometer_modwsgi.log
/var/log/ceilometer/ceilometer-api.log</screen>
     </entry>
    </row>
    <row>
     <entry>metering</entry>
     <entry>Service Log Directory Size</entry>
     <entry>Service log directory consuming more disk than its quota.</entry>
     <entry>This could be due to a service set to <literal>DEBUG</literal> instead of
              <literal>INFO</literal> level. Another reason could be due to a repeating error
            message filling up the log files. Finally, it could be due to log rotate not configured
            properly so old log files are not being deleted properly.</entry>
     <entry>Find the service that is consuming too much disk space. Look at the logs. If
              <literal>DEBUG</literal> log entries exist, set the logging level to
              <literal>INFO</literal>. If the logs are repeatedly logging an error message, do what
            is needed to resolve the error. If old log files exist, configure log rotate to remove
            them. You could also choose to remove old log files by hand after backing them up if
            needed.</entry>
    </row>
<!---->
    <row>
     <entry morerows="3">kafka</entry>
     <entry>Kafka Persister Metric Consumer Lag</entry>
     <entry>Alarms when the Persister consumer group is not keeping up with the incoming
            messages on the metric topic.</entry>
     <entry>There is a slow down in the system or heavy load.</entry>
     <entry>
      <para>
       Verify that all of the monasca-persister services are up with these
       steps:
      </para>
      <orderedlist xml:id="ol_z3p_2ff_wv">
       <listitem>
        <para>
         Log in to the &clm;
        </para>
       </listitem>
       <listitem>
        <para>
         Verify that all of the <literal>monasca-persister</literal> services
         are up with this playbook:
        </para>
<screen>cd ~/scratch/ansible/next/ardana/ansible
ansible-playbook -i hosts/verb_hosts monasca-status.yml --tags monasca-persister</screen>
       </listitem>
      </orderedlist>
      <para>
       Look for high load in the various systems. This alert can fire for
       multiple topics or on multiple hosts. Which alarms are firing can help
       diagnose likely causes. For example, if the alarm is alerting all on one
       machine it could be the machine. If one topic across multiple machines
       it is likely the consumers of that topic, etc.
      </para>
     </entry>
    </row>
    <row>
     <entry>Kafka Alarm Transition Consumer Lag</entry>
     <entry>Alarms when the specified consumer group is not keeping up with the incoming
            messages on the alarm state transistion topic.</entry>
     <entry>There is a slow down in the system or heavy load.</entry>
     <entry>
      <para>
       Check that monasca-thresh and monasca-notification are up.
      </para>
      <para>
       Look for high load in the various systems. This alert can fire for
       multiple topics or on multiple hosts. Which alarms are firing can help
       diagnose likely causes. For example:
      </para>
      <itemizedlist>
       <listitem>
        <para>
         If all alarms are on the same machine, the machine could be at fault.
        </para>
       </listitem>
       <listitem>
        <para>
         If one topic is shared across multiple machines, the consumers of
         that topic are likely at fault.
        </para>
       </listitem>
      </itemizedlist>
     </entry>
    </row>
    <row>
     <entry>Kafka Kronos Consumer Lag</entry>
     <entry>Alarms when the Kronos consumer group is not keeping up with the incoming messages
            on the metric topic.</entry>
     <entry>There is a slow down in the system or heavy load.</entry>
     <entry>
      <para>
       Look for high load in the various systems. This alert can fire for
       multiple topics or on multiple hosts. Which alarms are firing can help
       diagnose likely causes. For example:
      </para>
      <itemizedlist>
       <listitem>
        <para>
         If all alarms are on the same machine, the machine could be at fault.
        </para>
       </listitem>
       <listitem>
        <para>
         If one topic is shared across multiple machines, the consumers of
         that topic are likely at fault.
        </para>
       </listitem>
      </itemizedlist>
     </entry>
    </row>
    <row>
     <entry>Process Check</entry>
     <entry>
      <para>
       Alarms when the specified process is not running.
      </para>
<screen>process_name = kafka.Kafka</screen>
     </entry>
     <entry/>
     <entry>
      <para>
       Restart the process on the affected node using these steps:
      </para>
      <orderedlist xml:id="ol_lrq_lzp_mx">
       <listitem>
        <para>
         Log in to the &clm;.
        </para>
       </listitem>
       <listitem>
        <para>
         Stop the kafka service with this playbook:
        </para>
<screen>cd ~/scratch/ansible/next/ardana/ansible
ansible-playbook -i hosts/verb_hosts monasca-start.yml --tags kafka</screen>
       </listitem>
       <listitem>
        <para>
         Start the kafka service back up with this playbook:
        </para>
<screen>cd ~/scratch/ansible/next/ardana/ansible
ansible-playbook -i hosts/verb_hosts monasca-status.yml --tags kafka</screen>
       </listitem>
      </orderedlist>
      <para>
       Review the logs in this location:
      </para>
<screen>/var/log/kafka/server.log</screen>
     </entry>
    </row>
<!---->
    <row>
     <entry morerows="7">logging</entry>
     <entry>Beaver Memory Usage</entry>
     <entry>Beaver is using more memory than expected. This may indicate that it cannot forward
            messages and its queue is filling up. If you continue to see this, see the
            troubleshooting guide.</entry>
     <entry>Overloaded system or services with memory leaks.</entry>
     <entry>Log onto the reporting host to investigate high memory users.</entry>
    </row>
    <row>
     <entry>Audit Log Partition Low Watermark</entry>
     <entry>The <literal>/var/audit</literal> disk space usage has crossed low watermark. If
            the high watermark is reached, logrotate will be run to free up disk space. Adjust
              <literal>var_audit_low_watermark_percent</literal> if needed.</entry>
     <entry>This could be due to a service set to DEBUG instead of INFO level. Another reason
            could be due to a repeating error message filling up the log files. Finally, it could be
            due to log rotate not configured properly so old log files are not being deleted
            properly.</entry>
     <entry>Find the service that is consuming too much disk space. Look at the logs. If DEBUG
            log entries exist, set the logging level to INFO. If the logs are repeatedly logging an
            error message, do what is needed to resolve the error. If old log files exist, configure
            log rotate to remove them. You could also choose to remove old log files by hand after
            backing them up if needed.</entry>
    </row>
    <row>
     <entry>Audit Log Partition High Watermark</entry>
     <entry>The <literal>/var/audit</literal> volume is running low on disk space. Logrotate
            will be run now to free up space. Adjust
              <literal>var_audit_high_watermark_percent</literal> if needed.</entry>
     <entry>This could be due to a service set to DEBUG instead of INFO level. Another reason
            could be due to a repeating error message filling up the log files. Finally, it could be
            due to log rotate not configured properly so old log files are not being deleted
            properly.</entry>
     <entry>Find the service that is consuming too much disk space. Look at the logs. If DEBUG
            log entries exist, set the logging level to INFO. If the logs are repeatedly logging an
            error message, do what is needed to resolve the error. If old log files exist, configure
            log rotate to remove them. You could also choose to remove old log files by hand after
            backing them up if needed.</entry>
    </row>
    <row>
     <entry>&elasticsearch; Unassigned Shards</entry>
     <entry>
<screen>component = elasticsearch</screen>
      <para>
       &elasticsearch; unassigned shards count is greater than 0.
      </para>
     </entry>
     <entry>Environment could be misconfigured.</entry>
     <entry>
      <para>
       To find the unassigned shards, run the following command on the
       &clm; from the
       <literal>~/scratch/ansible/next/ardana/ansible</literal> directory:
      </para>
<screen>cd ~/scratch/ansible/next/ardana/ansible
ansible -i hosts/verb_hosts LOG-SVR[0] -m shell -a "curl localhost:9200/_cat/shards?pretty -s" | grep UNASSIGNED</screen>
      <para>
       This should show which shards are unassigned, like this:
      </para>
<screen>logstash-2015.10.21 4 p UNASSIGNED 11412371  3.2gb 10.241.67.11 Keith Kilham</screen>
      <para>
       The last column shows the name that &elasticsearch; uses for the node that
       the unassigned shards are on. To find the actual host name, run:
      </para>
<screen>cd ~/scratch/ansible/next/ardana/ansible
ansible -i hosts/verb_hosts LOG-SVR[0] -m shell -a "curl localhost:9200/_nodes/_all/name?pretty -s"</screen>
      <para>
       Once you find the host name, you can try the following:
      </para>
      <orderedlist xml:id="ol_bcg_3zp_mx">
       <listitem>
        <para>
         Make sure the node is not out of disk space, and free up space if
         needed.
        </para>
       </listitem>
       <listitem>
        <para>
         Restart the node (use caution, as this may affect other services as
         well).
        </para>
       </listitem>
       <listitem>
        <para>
         Check to make sure all versions of &elasticsearch; are the same with
         this:
        </para>
<screen>cd ~/scratch/ansible/next/ardana/ansible
ansible -i hosts/verb_hosts LOG-SVR -m shell -a "curl localhost:9200/_nodes/_local/name?pretty -s" | grep version</screen>
       </listitem>
       <listitem>
        <para>
         Contact customer support.
        </para>
       </listitem>
      </orderedlist>
     </entry>
    </row>
    <row>
     <entry>&elasticsearch; Number of Log Entries</entry>
     <entry>
<screen>component = elasticsearch</screen>
      <para>
       &elasticsearch; Number of Log Entries.
      </para>
     </entry>
     <entry>The number of log entries may get too large.</entry>
     <entry>Older versions of Kibana (version 3 and earlier) may hang if the number of log
            entries is too large (for example, above 40,000), and the page size would need to be small
            enough (about 20,000 results), because if it is larger (for example, 200,000), it may hang the
            browser, but Kibana 4 should not have this issue.</entry>
    </row>
    <row>
     <entry>&elasticsearch; Field Data Evictions</entry>
     <entry>
<screen>component = elasticsearch</screen>
      <para>
       &elasticsearch; Field Data Evictions count is greater than 0.
      </para>
     </entry>
     <entry>Field Data Evictions may be found even though it is nowhere near the limit
            set.</entry>
     <entry>
      <para>
       The <literal>elasticsearch_indices_fielddata_cache_size</literal> is set
       to <literal>unbounded</literal> by default. If this is set by the user
       to a value that is insufficient, you may need to increase this
       configuration parameter or set it to unbounded and run a reconfigure
       using the steps below:
      </para>
      <orderedlist xml:id="ol_ccg_3zp_mx">
       <listitem>
        <para>
         Log in to the &clm;.
        </para>
       </listitem>
       <listitem>
        <para>
         Edit the configuration file below and change the value for
         <literal>elasticsearch_indices_fielddata_cache_size</literal> to your
         desired value:
        </para>
<screen>~/openstack/my_cloud/config/logging/main.yml</screen>
       </listitem>
       <listitem>
        <para>
         Commit the changes to git:
        </para>
<screen>git add -A
git commit -a -m "changing &elasticsearch; fielddata cache size"</screen>
       </listitem>
       <listitem>
        <para>
         Run the configuration processor:
        </para>
<screen>cd ~/openstack/ardana/ansible
ansible-playbook -i hosts/localhost config-processor-run.yml</screen>
       </listitem>
       <listitem>
        <para>
         Update your deployment directory:
        </para>
<screen>cd ~/openstack/ardana/ansible
ansible-playbook -i hosts/localhost ready-deployment.yml</screen>
       </listitem>
       <listitem>
        <para>
         Run the Logging reconfigure playbook to deploy the change:
        </para>
<screen>cd ~/scratch/ansible/next/ardana/ansible/
ansible-playbook -i hosts/verb_hosts kronos-reconfigure.yml</screen>
       </listitem>
      </orderedlist>
     </entry>
    </row>
    <row>
     <entry>Service Log Directory Size</entry>
     <entry>
      <para>
       Service log directory consuming more disk than its quota.
      </para>
     </entry>
     <entry>This could be due to a service set to <literal>DEBUG</literal> instead of
              <literal>INFO</literal> level. Another reason could be due to a repeating error
            message filling up the log files. Finally, it could be due to log rotate not configured
            properly so old log files are not being deleted properly.</entry>
     <entry>Find the service that is consuming too much disk space. Look at the logs. If
              <literal>DEBUG</literal> log entries exist, set the logging level to
              <literal>INFO</literal>. If the logs are repeatedly logging an error message, do what
            is needed to resolve the error. If old log files exist, configure log rotate to remove
            them. You could also choose to remove old log files by hand after backing them up if
            needed.</entry>
    </row>
    <row>
     <entry>Process Check</entry>
     <entry>
      <para>
       Separate alarms for each of these logging services, specified by the
       <literal>process_name</literal> dimension:
      </para>
      <itemizedlist xml:id="ul_dcg_3zp_mx">
       <listitem>
        <para>
         elasticsearch
        </para>
       </listitem>
       <listitem>
        <para>
         logstash
        </para>
       </listitem>
       <listitem>
        <para>
         beaver
        </para>
       </listitem>
       <listitem>
        <para>
         apache2
        </para>
       </listitem>
       <listitem>
        <para>
         kibana
        </para>
       </listitem>
      </itemizedlist>
     </entry>
     <entry>Process has crashed.</entry>
     <entry>
      <para>
       On the affected node, attempt to restart the process.
      </para>
      <para>
       If the <command>elasticsearch</command> process has crashed, use:
      </para>
<screen>sudo systemctl restart elasticsearch</screen>
      <para>
       If the logstash process has crashed, use:
      </para>
<screen>sudo systemctl restart logstash</screen>
      <para>
       The rest of the processes can be restarted using similar commands,
       listed here:
      </para>
<screen>sudo systemctl restart beaver
sudo systemctl restart apache2
sudo systemctl restart kibana</screen>
     </entry>
    </row>
<!---->
    <row>
     <entry morerows="2">monasca-transform</entry>
     <entry>Process Check</entry>
     <entry>
<screen>process_name = pyspark</screen>
     </entry>
     <entry>Service process has crashed.</entry>
     <entry>
      <para>
       Restart process on affected node. Review logs.
      </para>
      <para>
       Child process of <literal>spark-worker</literal> but created once the
       <literal>monasca-transform</literal> process begins processing streams.
       If the process fails on one node only, along with the pyspark process,
       it is very likely that the <literal>spark-worker</literal> has failed to
       connect to the elected leader of the <literal>spark-master</literal>
       service. In this case the <literal>spark-worker</literal> service should
       be started on the affected node. If on multiple nodes check the
       <literal>spark-worker</literal>, <literal>spark-master</literal> and
       <literal>monasca-transform</literal> services and logs. If the
       <literal>monasca-transform</literal> or <literal>spark</literal>
       services have been interrupted this process may not re-appear for up to
       ten minutes (the stream processing interval).
      </para>
     </entry>
    </row>
    <row>
     <entry>Process Check</entry>
     <entry>
<screen>process_name = org.apache.spark.executor.CoarseGrainedExecutorBackend</screen>
     </entry>
     <entry>Service process has crashed.</entry>
     <entry>
      <para>
       Restart process on affected node. Review logs.
      </para>
      <para>
       Child process of <literal>spark-worker</literal> but created once the
       <literal>monasca-transform</literal> process begins processing streams.
       If the process fails on one node only, along with the pyspark process,
       it is very likely that the <literal>spark-worker</literal> has failed to
       connect to the elected leader of the <literal>spark-master</literal>
       service. In this case the <literal>spark-worker</literal> service should
       be started on the affected node. If on multiple nodes check the
       <literal>spark-worker</literal>, <literal>spark-master</literal> and
       <literal>monasca-transform</literal> services and logs. If the
       <literal>monasca-transform</literal> or <literal>spark</literal>
       services have been interrupted this process may not re-appear for up to
       ten minutes (the stream processing interval).
      </para>
     </entry>
    </row>
    <row>
     <entry>Process Check</entry>
     <entry>
<screen>process_name = monasca-transform</screen>
     </entry>
     <entry>Service process has crashed.</entry>
     <entry>
      <para>
       Restart the service on affected node. Review logs.
      </para>
     </entry>
    </row>
<!---->
    <row>
     <entry morerows="12">monitoring</entry>
     <entry>HTTP Status</entry>
     <entry>
<screen>component = monasca-persister</screen>
      <para>
       Persister Health Check
      </para>
     </entry>
     <entry>The process has crashed or a dependency is out.</entry>
     <entry>
      <para>
       If the process has crashed, restart it using the steps below. If a
       dependent service is down, address that issue.
      </para>
      <para>
       Restart the process on the affected node using these steps:
      </para>
      <orderedlist xml:id="ol_ecg_3zp_mx">
       <listitem>
        <para>
         Log in to the &clm;.
        </para>
       </listitem>
       <listitem>
        <para>
         Check if <literal>monasca-api</literal> is running on all nodes with
         this playbook:
        </para>
<screen>cd ~/scratch/ansible/next/ardana/ansible
ansible-playbook -i hosts/verb_hosts monasca-status.yml --tags monasca-persister</screen>
       </listitem>
       <listitem>
        <para>
         Use the Monasca start playbook against the affected node to restart
         it:
        </para>
<screen>cd ~/scratch/ansible/next/ardana/ansible
ansible-playbook -i hosts/verb_hosts monasca-start.yml --tags persister</screen>
       </listitem>
       <listitem>
        <para>
         Verify that it is running on all nodes with this playbook:
        </para>
<screen>cd ~/scratch/ansible/next/ardana/ansible
ansible-playbook -i hosts/verb_hosts monasca-status.yml --tags monasca-persister</screen>
       </listitem>
      </orderedlist>
      <para>
       Review the associated logs.
      </para>
     </entry>
    </row>
    <row>
     <entry>HTTP Status</entry>
     <entry>
<screen>component = monasca-api</screen>
      <para>
       API Health Check
      </para>
     </entry>
     <entry>The process has crashed or a dependency is out.</entry>
     <entry>
      <para>
       If the process has crashed, restart it using the steps below. If a
       dependent service is down, address that issue.
      </para>
      <para>
       Restart the process on the affected node using these steps:
      </para>
      <orderedlist>
       <listitem>
        <para>
         Log in to the &clm;.
        </para>
       </listitem>
       <listitem>
        <para>
         Check if <literal>monasca-api</literal> is running on all nodes with
         this playbook:
        </para>
<screen>cd ~/scratch/ansible/next/ardana/ansible
ansible-playbook -i hosts/verb_hosts monasca-status.yml --tags monasca-api</screen>
       </listitem>
       <listitem>
        <para>
         Use the Monasca start playbook against the affected node to restart
         it:
        </para>
<screen>cd ~/scratch/ansible/next/ardana/ansible
ansible-playbook -i hosts/verb_hosts monasca-start.yml --tags monasca-api</screen>
       </listitem>
       <listitem>
        <para>
         Verify that it is running on all nodes with this playbook:
        </para>
<screen>cd ~/scratch/ansible/next/ardana/ansible
ansible-playbook -i hosts/verb_hosts monasca-status.yml --tags monasca-api</screen>
       </listitem>
      </orderedlist>
      <para>
       Review the associated logs.
      </para>
     </entry>
    </row>
    <row>
     <entry>Monasca Agent Collection Time</entry>
     <entry>Alarms when the elapsed time the <literal>monasca-agent</literal> takes to collect
            metrics is high.</entry>
     <entry>Heavy load on the box or a stuck agent plug-in.</entry>
     <entry>
      <para>
       Address the load issue on the machine. If needed, restart the agent
       using the steps below:
      </para>
      <para>
       Restart the agent on the affected node using these steps:
      </para>
      <orderedlist xml:id="ol_gcg_3zp_mx">
       <listitem>
        <para>
         Log in to the &clm;.
        </para>
       </listitem>
       <listitem>
        <para>
         Check if <literal>monasca-agent</literal> is running on all nodes with
         this playbook:
        </para>
<screen>cd ~/scratch/ansible/next/ardana/ansible
ansible-playbook -i hosts/verb_hosts monasca-agent-status.yml</screen>
       </listitem>
       <listitem>
        <para>
         Use the Monasca start playbook against the affected node to restart
         it:
        </para>
<screen>cd ~/scratch/ansible/next/ardana/ansible
ansible-playbook -i hosts/verb_hosts monasca-start.yml --limit &lt;hostname&gt;</screen>
       </listitem>
       <listitem>
        <para>
         Verify that it is running on all nodes with this playbook:
        </para>
<screen>cd ~/scratch/ansible/next/ardana/ansible
ansible-playbook -i hosts/verb_hosts monasca-agent-status.yml</screen>
       </listitem>
      </orderedlist>
      <para>
       Review the associated logs.
      </para>
     </entry>
    </row>
    <row>
     <entry>Process Check</entry>
     <entry>
      <para>
       Alarms when the specified process is not running.
      </para>
<screen>component = kafka</screen>
     </entry>
     <entry>Process crashed.</entry>
     <entry>
      <para>
       Restart the process on the affected node using these steps:
      </para>
      <orderedlist>
       <listitem>
        <para>
         Log in to the &clm;.
        </para>
       </listitem>
       <listitem>
        <para>
         Check if Kafka is running on all nodes with this playbook:
        </para>
<screen>cd ~/scratch/ansible/next/ardana/ansible
ansible-playbook -i hosts/verb_hosts monasca-status.yml --tags kafka</screen>
       </listitem>
       <listitem>
        <para>
         Use the Monasca start playbook against the affected node to restart
         it:
        </para>
<screen>cd ~/scratch/ansible/next/ardana/ansible
ansible-playbook -i hosts/verb_hosts monasca-start.yml --tags kafka</screen>
       </listitem>
       <listitem>
        <para>
         Verify that Kafka is running on all nodes with this playbook:
        </para>
<screen>cd ~/scratch/ansible/next/ardana/ansible
ansible-playbook -i hosts/verb_hosts monasca-status.yml --tags kafka</screen>
       </listitem>
      </orderedlist>
      <para>
       Review the associated logs.
      </para>
     </entry>
    </row>
    <row>
     <entry>Process Check</entry>
     <entry>
      <para>
       Alarms when the specified process is not running.
      </para>
<screen>process_name = monasca-notification</screen>
     </entry>
     <entry>Process crashed.</entry>
     <entry>
      <para>
       Restart the process on the affected node using these steps:
      </para>
      <orderedlist>
       <listitem>
        <para>
         Log in to the &clm;.
        </para>
       </listitem>
       <listitem>
        <para>
         Check if <literal>monasca-api</literal> is running on all nodes with
         this playbook:
        </para>
<screen>cd ~/scratch/ansible/next/ardana/ansible
ansible-playbook -i hosts/verb_hosts monasca-status.yml --tags notification</screen>
       </listitem>
       <listitem>
        <para>
         Use the Monasca start playbook against the affected node to restart
         it:
        </para>
<screen>cd ~/scratch/ansible/next/ardana/ansible
ansible-playbook -i hosts/verb_hosts monasca-start.yml --tags notification</screen>
       </listitem>
       <listitem>
        <para>
         Verify that it is running on all nodes with this playbook:
        </para>
<screen>cd ~/scratch/ansible/next/ardana/ansible
ansible-playbook -i hosts/verb_hosts monasca-status.yml --tags notification</screen>
       </listitem>
      </orderedlist>
      <para>
       Review the associated logs.
      </para>
     </entry>
    </row>
    <row>
     <entry>Process Check</entry>
     <entry>
      <para>
       Alarms when the specified process is not running.
      </para>
<screen>process_name = monasca-agent</screen>
     </entry>
     <entry>Process crashed.</entry>
     <entry>
      <para>
       Restart the agent on the affected node using these steps:
      </para>
      <orderedlist>
       <listitem>
        <para>
         Log in to the &clm;.
        </para>
       </listitem>
       <listitem>
        <para>
         Check if <literal>monasca-agent</literal> is running on all nodes with
         this playbook:
        </para>
<screen>cd ~/scratch/ansible/next/ardana/ansible
ansible-playbook -i hosts/verb_hosts monasca-agent-status.yml</screen>
       </listitem>
       <listitem>
        <para>
         Use the Monasca start playbook against the affected node to restart
         it:
        </para>
<screen>cd ~/scratch/ansible/next/ardana/ansible
ansible-playbook -i hosts/verb_hosts monasca-start.yml --limit &lt;hostname&gt;</screen>
       </listitem>
       <listitem>
        <para>
         Verify that it is running on all nodes with this playbook:
        </para>
<screen>cd ~/scratch/ansible/next/ardana/ansible
ansible-playbook -i hosts/verb_hosts monasca-agent-status.yml</screen>
       </listitem>
      </orderedlist>
      <para>
       Review the associated logs.
      </para>
     </entry>
    </row>
    <row>
     <entry>Process Check</entry>
     <entry>
      <para>
       Alarms when the specified process is not running.
      </para>
<screen>process_name = monasca-api</screen>
     </entry>
     <entry>Process crashed.</entry>
     <entry>
      <para>
       >Restart the process on the affected node using these steps:
      </para>
      <orderedlist>
       <listitem>
        <para>
         Log in to the &clm;.
        </para>
       </listitem>
       <listitem>
        <para>
         Check if <literal>monasca-api</literal> is running on all nodes with
         this playbook:
        </para>
<screen>cd ~/scratch/ansible/next/ardana/ansible
ansible-playbook -i hosts/verb_hosts monasca-status.yml --tags monasca-api</screen>
       </listitem>
       <listitem>
        <para>
         Use the Monasca start playbook against the affected node to restart
         it:
        </para>
<screen>cd ~/scratch/ansible/next/ardana/ansible
ansible-playbook -i hosts/verb_hosts monasca-start.yml --tags monasca-api</screen>
       </listitem>
       <listitem>
        <para>
         Verify that it is running on all nodes with this playbook:
        </para>
<screen>cd ~/scratch/ansible/next/ardana/ansible
ansible-playbook -i hosts/verb_hosts monasca-status.yml --tags monasca-api</screen>
       </listitem>
      </orderedlist>
      <para>
       Review the associated logs.
      </para>
     </entry>
    </row>
    <row>
     <entry>Process Check</entry>
     <entry>
      <para>
       Alarms when the specified process is not running.
      </para>
<screen>process_name = monasca-persister</screen>
     </entry>
     <entry>Process crashed.</entry>
     <entry>
      <para>
       Restart the process on the affected node using these steps:
      </para>
      <orderedlist>
       <listitem>
        <para>
         Log in to the &clm;.
        </para>
       </listitem>
       <listitem>
        <para>
         Check if <literal>monasca-api</literal> is running on all nodes with
         this playbook:
        </para>
<screen>cd ~/scratch/ansible/next/ardana/ansible
ansible-playbook -i hosts/verb_hosts monasca-status.yml --tags monasca-persister</screen>
       </listitem>
       <listitem>
        <para>
         Use the Monasca start playbook against the affected node to restart
         it:
        </para>
<screen>cd ~/scratch/ansible/next/ardana/ansible
ansible-playbook -i hosts/verb_hosts monasca-start.yml --tags persister</screen>
       </listitem>
       <listitem>
        <para>
         Verify that it is running on all nodes with this playbook:
        </para>
<screen>cd ~/scratch/ansible/next/ardana/ansible
ansible-playbook -i hosts/verb_hosts monasca-status.yml --tags monasca-persister</screen>
       </listitem>
      </orderedlist>
      <para>
       Review the associated logs.
      </para>
     </entry>
    </row>
    <row>
     <entry>Process Check</entry>
     <entry>
      <para>
       Alarms when the specified process is not running.
      </para>
<screen>process_name = backtype.storm.daemon.nimbus
component = apache-storm</screen>
     </entry>
     <entry>Process crashed.</entry>
     <entry>
      <para>
       Review the logs in the <literal>/var/log/storm</literal> directory on
       all storm hosts to find the root cause.
      </para>
      <note>
       <para>
        The logs containing threshold engine logging are on the 2nd and 3rd
        controller nodes.
       </para>
      </note>
      <para>
       Restart monasca-thresh, if necessary, with these steps:
      </para>
      <orderedlist>
       <listitem>
        <para>
         Log in to the &clm;.
        </para>
       </listitem>
       <listitem>
        <para>
         Check if <literal>monasca-thresh</literal> is running on all nodes
         with this playbook:
        </para>
<screen>cd ~/scratch/ansible/next/ardana/ansible
ansible-playbook -i hosts/verb_hosts monasca-status.yml --tags thresh</screen>
       </listitem>
       <listitem>
        <para>
         Use the Monasca start playbook against the affected node to restart
         it:
        </para>
<screen>cd ~/scratch/ansible/next/ardana/ansible
ansible-playbook -i hosts/verb_hosts monasca-start.yml --tags thresh</screen>
       </listitem>
       <listitem>
        <para>
         Verify that it is running on all nodes with this playbook:
        </para>
<screen>cd ~/scratch/ansible/next/ardana/ansible
ansible-playbook -i hosts/verb_hosts monasca-status.yml --tags thresh</screen>
       </listitem>
      </orderedlist>
     </entry>
    </row>
    <row>
     <entry>Process Check</entry>
     <entry>
      <para>
       Alarms when the specified process is not running.
      </para>
<screen>process_name = backtype.storm.daemon.supervisor
component = apache-storm</screen>
     </entry>
     <entry>Process crashed.</entry>
     <entry>
      <para>
       Review the logs in the <literal>/var/log/storm</literal> directory on
       all storm hosts to find the root cause.
      </para>
      <note>
       <para>
        The logs containing threshold engine logging are on the 2nd and 3rd
        controller nodes.
       </para>
      </note>
      <para>
       Restart monasca-thresh with these steps:
      </para>
      <orderedlist>
       <listitem>
        <para>
         Log in to the &clm;.
        </para>
       </listitem>
       <listitem>
        <para>
         Stop the monasca-thresh service:
        </para>
<screen>cd ~/scratch/ansible/next/ardana/ansible
ansible-playbook -i hosts/verb_hosts monasca-stop.yml --tags thresh</screen>
       </listitem>
       <listitem>
        <para>
         Start the monasca-thresh service back up:
        </para>
<screen>cd ~/scratch/ansible/next/ardana/ansible
ansible-playbook -i hosts/verb_hosts monasca-start.yml --tags thresh</screen>
       </listitem>
       <listitem>
        <para>
         Verify that it is running on all nodes with this playbook:
        </para>
<screen>cd ~/scratch/ansible/next/ardana/ansible
ansible-playbook -i hosts/verb_hosts monasca-status.yml --tags thresh</screen>
       </listitem>
      </orderedlist>
     </entry>
    </row>
    <row>
     <entry>Process Check</entry>
     <entry>
      <para>
       Alarms when the specified process is not running.
      </para>
<screen>process_name = backtype.storm.daemon.worker
component = apache-storm</screen>
     </entry>
     <entry>Process crashed.</entry>
     <entry>
      <para>
       Review the logs in the <literal>/var/log/storm</literal> directory on
       all storm hosts to find the root cause.
      </para>
      <note>
       <para>
        The logs containing threshold engine logging are on the 2nd and 3rd
        controller nodes.
       </para>
      </note>
      <para>
       Restart monasca-thresh with these steps:
      </para>
      <orderedlist>
       <listitem>
        <para>
         Log in to the &clm;.
        </para>
       </listitem>
       <listitem>
        <para>
         Stop the monasca-thresh service:
        </para>
<screen>cd ~/scratch/ansible/next/ardana/ansible
ansible-playbook -i hosts/verb_hosts monasca-stop.yml --tags thresh</screen>
       </listitem>
       <listitem>
        <para>
         Start the monasca-thresh service back up:
        </para>
<screen>cd ~/scratch/ansible/next/ardana/ansible
ansible-playbook -i hosts/verb_hosts monasca-start.yml --tags thresh</screen>
       </listitem>
       <listitem>
        <para>
         Verify that it is running on all nodes with this playbook:
        </para>
<screen>cd ~/scratch/ansible/next/ardana/ansible
ansible-playbook -i hosts/verb_hosts monasca-status.yml --tags thresh</screen>
       </listitem>
      </orderedlist>
     </entry>
    </row>
    <row>
     <entry>Process Check</entry>
     <entry>
      <para>
       Alarms when the specified process is not running.
      </para>
<screen>process_name = monasca-thresh
component = apache-storm</screen>
     </entry>
     <entry>Process crashed.</entry>
     <entry>
      <para>
       Restart the process on the affected node using these steps:
      </para>
      <orderedlist>
       <listitem>
        <para>
         Log in to the &clm;.
        </para>
       </listitem>
       <listitem>
        <para>
         Check if <literal>monasca-thresh</literal> is running on all nodes
         with this playbook:
        </para>
<screen>cd ~/scratch/ansible/next/ardana/ansible
ansible-playbook -i hosts/verb_hosts monasca-status.yml --tags thresh</screen>
       </listitem>
       <listitem>
        <para>
         Use the Monasca start playbook against the affected node to restart
         it:
        </para>
<screen>cd ~/scratch/ansible/next/ardana/ansible
ansible-playbook -i hosts/verb_hosts monasca-start.yml --tags thresh</screen>
       </listitem>
       <listitem>
        <para>
         Verify that it is running on all nodes with this playbook:
        </para>
<screen>cd ~/scratch/ansible/next/ardana/ansible
ansible-playbook -i hosts/verb_hosts monasca-status.yml --tags thresh</screen>
       </listitem>
      </orderedlist>
      <para>
       Review the associated logs.
      </para>
     </entry>
    </row>
    <row>
     <entry>Service Log Directory Size</entry>
     <entry>Service log directory consuming more disk than its quota.</entry>
     <entry>The service log directory, as indicated by the <literal>path</literal> dimension,
            is over the 2.5 GB quota.</entry>
     <entry>Find the service that is consuming too much disk space. Look at the logs. If
              <literal>DEBUG</literal> log entries exist, set the logging level to
              <literal>INFO</literal>. If the logs are repeatedly logging an error message, do what
            is needed to resolve the error. If old log files exist, configure log rotate to remove
            them. You could also choose to remove old log files by hand after backing them up if
            needed.</entry>
    </row>
   </tbody>
  </tgroup>
 </informaltable>
</section>
