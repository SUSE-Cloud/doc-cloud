<?xml version="1.0"?>
<?xml-stylesheet href="urn:x-suse:xslt:profiling:docbook51-profile.xsl"
 type="text/xml"
 title="Profiling step"?>
<!DOCTYPE section [
 <!ENTITY % entities SYSTEM "entities.ent"> %entities;
]>
<section xmlns="http://docbook.org/ns/docbook" xmlns:xi="http://www.w3.org/2001/XInclude" xmlns:xlink="http://www.w3.org/1999/xlink" xml:id="VSC-on-VCP">
 <title>Input Model Changes for VSC Setup on Virtual Control Plane</title>
 <itemizedlist>
  <listitem>
   <para>
    Deploying the VSC via the VM Disk Image
   </para>
  </listitem>
  <listitem>
   <para>
    Deploying the VSC via the playbooks
   </para>
   <itemizedlist>
    <listitem>
     <para>
      Defining the Server-Role for VM hosts
     </para>
    </listitem>
    <listitem>
     <para>
      Add the definition for the HLM_HYPERVISOR_INTERFACES Network Group
     </para>
    </listitem>
    <listitem>
     <para>
      Add the VM Host to the Cloud
     </para>
    </listitem>
    <listitem>
     <para>
      New Service Definition for the DCN-VSC Service
     </para>
    </listitem>
    <listitem>
     <para>
      Control Plane Changes needed to Deploy DCN-VSC on VM Hosts
     </para>
    </listitem>
    <listitem>
     <para>
      Network Groups: Associate the data network to "HLM" network, MANAGEMENT
      to the "MANAGEMENT" network
     </para>
    </listitem>
    <listitem>
     <para>
      DCN Configuration Data Additions
     </para>
    </listitem>
    <listitem>
     <para>
      Setup of the VM-Host and Deployment of services on Controller VMs
     </para>
    </listitem>
   </itemizedlist>
  </listitem>
  <listitem>
   <para>
    Additional Third-Party import needed for VSC deployment
   </para>
  </listitem>
 </itemizedlist>

 <section xml:id="idg-all-networking-VSC_setup_input_model_changes-xml-4">
  <title>Deploying the VSC via the VM Disk Image</title>
  <para>
   The VSC is delivered as a VM definition along with the VM disk image. The
   image already has all the necessary software components in it. Furthermore,
   it is a VXworks based system. To deploy it requires the VM definition xml
   file be customized for the VM host, And then the boot files in the VSC disk
   image have to be updated on the disk image before the VSC VM can be booted
   up. So the steps to create a disk image, installing Hlinux on the VM disk is
   not needed. The steps taken to deploy the VSC VM is
  </para>
  <orderedlist>
   <listitem>
    <para>
     Find a machine to host the VSC VM. If the VSC-VM is going to be running on
     a node that will host other virtual controllers, use the hlm-hypervisor
     role to bring up the VM Host. The plays in the role will be setting up the
     bridges needed by the VSC. JM: you don't need a specific hlm-hypervisor
     role, you just need to specify passthrough-network-groups in the interface
     model of the servers that will host the VSC VMs. The osconfig
     network_interface role will create the bridges needed by the VSC.
    </para>
   </listitem>
   <listitem>
    <para>
     When (1) is done, use site.yml to bring up and customize the VSC VM on the
     VM host:
    </para>
    <orderedlist>
     <listitem>
      <para>
       Defining the virsh networks needed by the VSC and bring up the virsh
       networks
      </para>
     </listitem>
     <listitem>
      <para>
       Customize the VSC boot file
      </para>
     </listitem>
     <listitem>
      <para>
       Boot up the VSC
      </para>
     </listitem>
     <listitem>
      <para>
       Additional configuration steps for the VSC by remotely executing a
       configuration script.
      </para>
     </listitem>
    </orderedlist>
   </listitem>
  </orderedlist>
 </section>

 <section xml:id="deploy_vsc_playbook">
  <title>Deploying the VSC via the playbooks</title>
 </section>
 <section xml:id="changes">
  <title>Changes needed</title>
  <para>
   To set up a VSC VM using the hlm-hypervisor role, changes are needed in an
   input model for:
  </para>
  <orderedlist>
   <listitem>
    <para>
     Create a Server role for the VM host.
    </para>
   </listitem>
   <listitem>
    <para>
     Assign a machine to be the host of the VSC VM. One VSC VM runs on each VM
     host.
    </para>
   </listitem>
   <listitem>
    <para>
     In the input model, the VSC has two services:
     <emphasis role="bold">dcn-vsc</emphasis> and
     <emphasis role="bold">dcn-vsc-data</emphasis>. The
     <emphasis role="bold">dcn-vsc</emphasis> service is used for the cloud’s
     management network and the <emphasis role="bold">dcn-vsc-data</emphasis>
     service is used for the vxlan traffic. Hence it is usually associated with
     a “guest” network group. The association is realized via “component
     endpoints.” So in the network group definition (usually defined in a
     file called <literal>network_groups.yml</literal> in the directory
     <literal>~/openstack/my_cloud/definition/data</literal>), the management
     network group needs the <emphasis role="bold">dcn-vsc</emphasis> as one of
     its <emphasis role="bold">component endpoints</emphasis>. The guest
     network group needs to have <emphasis role="bold">dcn-vsc-data</emphasis>
     as one of its <emphasis role="bold">component endpoints</emphasis>.
     Included below is an example of a <literal>network_group.yml</literal>
     file that is configured to use the VSC. In it the cloud’s management
     network group is MANAGEMENT and guest network group is GUEST.
    </para>
   </listitem>
   <listitem>
    <para>
     New service definition for the DCN-VSC service
    </para>
   </listitem>
   <listitem>
    <para>
     Control Plane changes to add the VM host and to specify the VSC runs on
     the VM host.
    </para>
   </listitem>
   <listitem>
    <para>
     Network Groups: In my vagrant model, I associate the data network to
     "MANAGEMENT" network, VSC MANAGEMENT to the "HLM" network. In general, the
     VSC's management network should be associated with the network-tag for
     carrying management traffic. The VSC's data network should be associated
     with the one that carries VXLAN traffic.
    </para>
   </listitem>
  </orderedlist>
 </section>

 <section xml:id="defining_roles">
  <title>Defining the server-role for VM hosts</title>
  <para>
   In the input model's server_roles.yml file. Add a role called
   'HYPERVISOR-ROLE' for the VM Hosts. So for a cloud with Controller, compute,
   and VRSG-role, the new server_roles.yml file is now:
  </para>
<screen>#
# (c) Copyright 2015 Hewlett Packard Enterprise Development LP
#
# Licensed under the Apache License, Version 2.0 (the "License"); you may
# not use this file except in compliance with the License. You may obtain
# a copy of the License at
#
# http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
# WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
# License for the specific language governing permissions and limitations
# under the License.
#
---
  product:
    version: 2

  server-roles:

    - name: CONTROLLER-ROLE
      interface-model: INTERFACE_SET_1
      disk-model: DISK_SET_CONTROLLER

    - name: COMPUTE-ROLE
      interface-model: INTERFACE_SET_1
      disk-model: DISK_SET_COMPUTE

    - name: VRSG-ROLE
      interface-model: INTERFACE_SET_1
      disk-model: DISK_SET_COMPUTE

# New role for the VM host
    - name: HYPERVISOR-ROLE
      interface-model: HLM_HYPERVISOR_INTERFACES
      disk-model: DISK_SET_COMPUTE</screen>
  <para>
   The definition of the HLM_HYPERVISOR_INTERFACES will be described below.
  </para>
 </section>
 <section xml:id="add_definition">
  <title>Adding the Definition</title>
  <para>
   Adding the VSC-HOST-INTERFACES is declared in a file called
   <literal>net_interfaces.yml</literal> or
   <literal>interfaces_set_1.yml</literal>. Under the VSC-HOST-INTERFACES’s
   lan device used for the management traffic, declare the management network
   as a <emphasis role="bold">passthrough-network-groups</emphasis>. The same
   needs to be done for the lan device used for the vxlan traffic.
  </para>
  <para>
   Example of a VSC-HOST-INTERFACES set:
  </para>
<screen>- name: VSC-HOST-INTERFACES
      network-interfaces:

        - name: hed1
          device:
            name: hed1
          network-groups:
            - MANAGEMENT
          passthrough-network-groups:
            - MANAGEMENT

        - name: hed2
          device:
            name: hed2
          network-groups:
            - HLM

        - name: hed3
          device:
            name: hed3
          network-groups:
            - GUEST
          passthrough-network-groups:
            - GUEST
          </screen>
  <para>
   Example a network_groups.yml file that associates the VSC services with the
   management and guest (vxlan) network groups:
  </para>
<screen>#
# (c) Copyright 2015,2016 Hewlett Packard Enterprise Development LP
#
# Licensed under the Apache License, Version 2.0 (the "License"); you may
# not use this file except in compliance with the License. You may obtain
# a copy of the License at
#
# http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
# WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
# License for the specific language governing permissions and limitations
# under the License.
#
---
  product:
    version: 2

  network-groups:
    - name: HLM
      hostname-suffix: hlm
      component-endpoints:
        - lifecycle-manager
        - lifecycle-manager-target


    #
    # GUEST
    #
    # This is the network group that will be used to provide
    # private networks to VMs
    #
    - name: GUEST
      hostname-suffix: guest
      tags:
        - neutron.networks.vxlan
      component-endpoints:
        - dcn-vsc-data
      # Set the mtu to 1550 to allow VMs to use a 1500 MTU without
      # underlying packet fragmentation.
      # Note: this requires setting a 1550 mtu (or higher) on any
      # untagged network group on the same interface as GUEST so
      # the MANAGEMENT network group must also have the mtu set to 1550.
      #
      #mtu: 1550
    - name: MANAGEMENT
      hostname-suffix: mgmt
      hostname: true

      tags:
        - neutron.networks.vlan:
            provider-physical-network: physnet1


      tls-component-endpoints:
        - barbican-api
        - mysql
        - rabbitmq
      component-endpoints:
        - dcn-vsc
        - default

      routes:
        - default

      load-balancers:
        - provider: ip-cluster
          name: lb
          tls-components:
            - default
          components:
            - vertica
            - nova-metadata
          roles:
            - internal
            - admin
          cert-file: helion-internal-cert

        - provider: ip-cluster
          name: extlb
          # external-name: myhelion.test
          tls-components:
            - default
          roles:
            - public
          cert-file: my-public-padawan-vsa-cert


    - name: EXTERNAL-GW</screen>
 </section>
 <section xml:id="add_VM_host">
  <title>Adding the VM host to the cloud</title>
  <para>
   One machine needs to be used to run each VSC-VM, You can either add the
   machine that will be used for the VSC role (i.e., a node that can be used to
   run the VSC VM) or a machine that will be running the hlm-hypervisor role
   (that runs other controller VMs in addition to the VSC). The example used is
   for running the VSC in a machine in the hlm-hypervisor role. The example
   adds a VM-Host (name vm-host-0001) machine to the server list (usually
   servers.yml) in addition to the controller, two compute nodes, and 1 VRSG
   node. The servers.yml is:
  </para>
<screen>---
  product:
    version: 2

  baremetal:
    subnet: 192.168.10.0
    netmask: 255.255.255.0
    server-interface: eth2

  servers:

    - id: ccn-0001
      ip-addr: 192.168.10.3
      role: CONTROLLER-ROLE
      server-group: RACK1
      mac-addr: b2:72:8d:ac:7c:6f
      ilo-ip: 192.168.9.3
      ilo-password: password
      ilo-user: admin
      nic-mapping: VAGRANT
    - id: cpn-0001
      ip-addr: 192.168.10.4
      role: COMPUTE-ROLE
      server-group: RACK1
      mac-addr: d6:70:c1:36:43:f7
      ilo-ip: 192.168.9.4
      ilo-password: password
      ilo-user: admin
      nic-mapping: VAGRANT

    - id: cpn-0002
      ip-addr: 192.168.10.5
      role: COMPUTE-ROLE
      server-group: RACK1
      mac-addr: 8e:8e:62:a6:ce:76
      ilo-ip: 192.168.9.5
      ilo-password: password
      ilo-user: admin
      nic-mapping: VAGRANT

    - id: vrsg-0001
      ip-addr: 192.168.10.6
      role: VRSG-ROLE
      server-group: RACK1
      mac-addr: 8e:8e:62:a6:ce:77
      ilo-ip: 192.168.9.6
      ilo-password: password
      ilo-user: admin
      nic-mapping: VAGRANT

#######################################################################
#
# New Machine running as the VM-Host
#######################################################################
    - id: vm-host-0001
      ip-addr: 192.168.10.7
      role: HYPERVISOR-ROLE
      server-group: RACK1
      hlm-hypervisor: true
      mac-addr: 8e:8e:62:a6:ce:78
      ilo-ip: 192.168.9.7
      ilo-password: password
      ilo-user: admin
      nic-mapping: VAGRANT</screen>
  <para>
   NOTE that the VSC VM should not be hosted on any of the VRS or VRSG nodes.
   as the VSC VM's host requires the use of standard openvswitch whereas the
   VRS and VRSG nodes uses nuage's openvswitch.
  </para>
 </section>
 <section xml:id="new_service_def">
  <title>New service definition for the DCN-VSC service</title>
  <para>
   The VSC requires IP addresses to be configured on 2 LAN interfaces. One is
   on the management and the other on the data network. In this example, the
   VSC management network is on the "HLM" network. And the VSC's data network
   is on the "MANAGEMENT" network. The association of VSC networks will be
   different in other input models.
<!---->
  </para>
<screen>---
product:
    version: 2
service-components:
-   name: dcn-vsc
    mnemonic: DCN-VSC
    service: dcn
    needs-ip: true
-   name: dcn-vsc-data
    mnemonic: DCN-VSC-DATA
    service: dcn
    needs-ip: true</screen>
  <para>
   These service components will be added to the list of service components
   operating on the VM Host.
  </para>
 </section>
 <section xml:id="control_place_changes">
  <title>Control plane changes needed to seploy DCN-VSC on VM hosts</title>
  <para>
   The changes to the control plane changes are: Add the VM-Host into
   control_plane.yml. The common-service-components list should only have
   lifecycle-manager-target. The other service components in
   common-service-components are added to the controllers' and compute nodes'
   service component list. The need to do this is described in HLM-Hypervisor
   instructions - PB3#Hypervisorinstructions-PB3-%22Empty%22hlmhypervisornode.
   Add dcn-vsc and dcn-vsc-tul to the service-component running on the vm-host
   node.
  </para>
<screen>---
  product:
    version: 2
  control-planes:
    - name: ccp
      region-name: region1
      failure-zones:
        - AZ1
      configuration-data:
        - DCN-CONFIG-CP1
      common-service-components:
        - lifecycle-manager-target
###########################################################################################
#
# The following service components are removed from the common-service-components:
#
#        - freezer-agent
#        - logging-rotate
#        - logging-producer
#        - monasca-agent
#        - stunnel
###########################################################################################
 clusters:
        - name: cluster0
          cluster-prefix: c0
          server-role: CONTROLLER-ROLE
          member-count: 1
          allocation-policy: strict
          service-components:
            - lifecycle-manager
            # Required for testing in (run-test.sh)
            - openstack-client
            - tempest
            - ntp-server
            - swift-ring-builder
            - mysql
            - ip-cluster
            - keystone-api
            - rabbitmq
            - glance-api:
                ha_mode: false
                glance_stores: 'file'
                glance_default_store: 'file'
            - glance-registry
            - cinder-api
            - cinder-scheduler
            - cinder-volume
            - cinder-backup
            - nova-api
            - nova-scheduler
            - nova-conductor
            - nova-console-auth
            - nova-novncproxy
            - neutron-server
            - neutron-ml2-plugin
            - dcn-ml2
            - horizon
            - swift-proxy
            - memcached
            - swift-account
            - swift-container
            - swift-object
            - heat-api
            - heat-api-cfn
            - heat-api-cloudwatch
            - heat-engine
            - ceilometer-api
            - ceilometer-polling
            - ceilometer-agent-notification
            - ceilometer-common
            - zookeeper
            - kafka
            - spark
            - vertica
            - storm
            - monasca-api
            - monasca-persister
            - monasca-notifier
            - monasca-threshold
            - monasca-transform
            - logging-server
            - ops-console-web
            - ops-console-monitor
            - freezer-api
            - barbican-api
            - barbican-worker
            - designate-api
            - designate-central
            - designate-pool-manager
            - designate-zone-manager
            - designate-mdns
            - powerdns
###########################################################################
#
#  Common service components now added to controllers
###########################################################################
            - freezer-agent
            - logging-rotate
            - logging-producer
            - monasca-agent
            - stunnel

      resources:
        - name: compute
          resource-prefix: compute
          server-role: COMPUTE-ROLE
          service-components:
            - ntp-client
            - nova-compute-kvm
            - nova-compute
            - dcn-vrs
            - dcn-metadata-agent
##################################################################################
#
#  Common service components now added to compute nodes
##################################################################################
            - freezer-agent
            - logging-rotate
            - logging-producer
            - monasca-agent
            - stunnel

###################################################################################
#
#  Service components for a VM-Host. Note that the common service components are
#  not added. Since ntp-servers on controllers are not up when running the
#  hlm-hypervisor-setup playbook, we have to make the VM host a NTP server.
#
###################################################################################
       - name: vm-host
          resource-prefix: vm-host
          server-role: HYPERVISOR-ROLE
          service-components:
            - ntp-server
            - dcn-vsc
            - dcn-vsc-data</screen>
 </section>

 <section xml:id="idg-all-networking-VSC_setup_input_model_changes-xml-13">
  <title>Network groups: Associating the data network to the HLM network, MANAGEMENT to the MANAGEMENT network</title>
  <para>
   In the network groups, have the configuration processor allocate an IP
   address for the dcn-vsc service (for the VSC management lan interface) and
   the dcn-vsc-tul service (for the VSC data lan interface). This is done by
   modifying the network_groups.yml file. Add "dcn-vsc" to the HLM network list
   of component endpoints and "dcn-vsc-tul" to the MANAGEMENT network list of
   component endpoints.
  </para>
<screen>---
  product:
    version: 2
  network-groups:
    - name: HLM
      hostname-suffix: hlm
      component-endpoints:
        - lifecycle-manager
        - lifecycle-manager-target
        - dcn-vsc
#         ^^^^^^^ Added dcn-vsc service to network HLM.  This the VSC's mgmt network
#         in this input model.
#

    - name: MANAGEMENT
      hostname-suffix: mgmt
      hostname: true
      tags:
        - neutron.networks.vxlan
        - neutron.networks.vlan:
            provider-physical-network: physnet1
      tls-component-endpoints:
        - barbican-api
        - mysql
        - rabbitmq
      component-endpoints:
        - dcn-vsc-data
#         ^^^^^^^ Added dcn-vsc-data service to network MANAGEMENT.  This the VSC's data network
#         in this input model.
#
        - default
      routes:
        - default
      load-balancers:
        - provider: ip-cluster
          name: lb
          tls-components:
            - default
          components:
            - vertica
            - nova-metadata
          roles:
            - internal
            - admin
          cert-file: helion-internal-cert

        - provider: ip-cluster
          name: extlb
          external-name: myhelion.test
          tls-components:
            - default
          roles:
            - public
          cert-file: my-public-deployerincloud-cert</screen>
 </section>

 <section xml:id="dcn_cnfig">
  <title>DCN Configuration Data Additions</title>
  <para>
   Here are the additional parameters needed to be added to the DCN config data
   (at file &lt;input_model&gt;/data/dcn/dcn_config.yml):
  </para>
<screen>---
  product:
    version: 2
  configuration-data:
    - name:  DCN-CONFIG-CP1
      services:
        - dcn
      data:
        vsd_host_name: vsd.example.com
        vsd_user: OSadmin
        vsd_passwd: OSadmin
        vsd_cms_id: ''
        vsc_active_ip: 192.168.245.253
#
# vsc_passive_ip is commented out because the example here only has 1 VSC.
#       vsc_passive_ip:
#####################################################################################
#
#  Additional Parameters needed for setting up VSC
#
#####################################################################################
        dns_domain_name: example.com
        vsc_mgmt_net: HLM
        vsc_data_net: MANAGEMENT
        vsc_image_name: vsc_singledisk
        vsc_user_name: admin
        vsc_user_pass: admin</screen>
 </section>
 <section xml:id="setup_vm">
  <title>Setup of the VM-Host and deployment of services on controller VMs</title>
  <para>
   Before the site.yml can be run, the VM host can be setup so that site.yml's
   plays to deploy services on the controller VMs hosted on it. To setup the VM
   host, the playbook hlm-hypervisor-setup has to be run right before running
   site.yml:
  </para>
<screen># At this point, the following has been run:
#  1. third-party import.
#  2. configuration processor
#  3. ready-deployment.yml
stack@deployerincloud-ccp-c0-m1-mgmt:~$ cd scratch/ansible/next/hos/ansible
stack@deployerincloud-ccp-c0-m1-mgmt:~/scratch/ansible/next/hos/ansible$ ansible-playbook -i hosts/verb_hosts hlm-hypervisor-setup.yml

# Now with the hypervisor machine setup, run site.yml:
stack@deployerincloud-ccp-c0-m1-mgmt:~/scratch/ansible/next/hos/ansible$ ansible-playbook -i hosts/verb_hosts site.yml</screen>
  <para>
   REFERENCE: HLM-Hypervisor instructions - PB3
  </para>
 </section>
 <section xml:id="additional_third_party">
  <title>Additional third-party import needed for VSC deployment</title>
  <para>
   The VSC deployment is optional. The VSC related playbooks are on the
   lifecycle manager in <literal>~/helion/hos_extensions/dcn/examples</literal>
   directory. The directories "services" and "ansible" needs to be copied to
   the third-party area.
  </para>
<screen>/hos-extensions-dcn/dcn/examples$ ls -altr
total 16
drwxrwxr-x 3 stepenma stepenma 4096 Sep 21 08:01 services
drwxrwxr-x 8 stepenma stepenma 4096 Sep 21 08:01 ..
drwxrwxr-x 4 stepenma stepenma 4096 Sep 21 08:01 .
drwxrwxr-x 5 stepenma stepenma 4096 Sep 23 10:20 ansible</screen>
  <para>
   Therefore when you run the command
  </para>
<screen>~/helion/hos_extensions:$ cp -R dcn ~/third-party/</screen>
  <para>
   The playbooks for DCN-VSC deployment is not copied to the third-party area.
   The contents of the dcn/examples/ansible and dcn/examples/services
   directories also needs to be merged into ~/third-party/dcn/ansible and
   ~/third-party/dcn/services areas. So that the DCN-VSC role appears in
   ~/third-party/dcn/ansible/roles/dcn-vsc/.
  </para>
<screen>cd ~/helion/hos_extensions/dcn/examples
tar cvf /tmp/vsc-ansible.tar ./
cd ~/third-party/dcn
tar xvf /tmp/vsc-ansible.tar</screen>
  <para>
   In addition, the VSC image file "vsc_singledisk.qcow2" needs to be copied to
   the ~/third/party/dcn/ansible/roles/dcn-vsc/files/ directory. The .qcow2
   file is found in DCN-VSC-4.0R5.zip:
  </para>
<screen>/tmp: $ mkdir -p DCN-VSC-4.0R5
cd DCN-VSC-4.0R5
# Download DCN-VSC-4.0R5.zip   Download it then copy it to /tmp/DCN-VSC-4.0R3.
unzip DCN-VSC-4.0R5.zip
tar xzvf DCN-VSC-05-OEM-2041.tar.gz
cd ~/third-party
mkdir -p dcn/ansible/roles/dcn-vsc/files
cp /tmp/DCN-VSC-4.0R5/single_disk/vsc_singledisk.qcow2 dcn/ansible/roles/dcn-vsc/files/. </screen>
  <para>
   Afterwards, run third-party import. The run the configuration processor on
   your input model:
  </para>
<screen>cd ~/helion/hos/ansible
ansible-playbook -i hosts/localhost third-party-import.yml
cd
cp -r &lt;your-model&gt; ~/openstack/my_cloud/definition/
cd ~/helion
git add -A
git commit -m "My config"
cd ~/helion/hos/ansible/
ansible-playbook -i hosts/localhost config-processor-run.yml -e encrypt="" -e rekey=""</screen>
  <para>
   The configuration processor will assign IP addresses to the VSCs. The
   addresses can be found in the file ~/openstack/my_cloud/info/net_info.yml. The
   example below illustrates one such file:
  </para>
<screen>service_ips:
    dcn-vsc:
    -   cluster: vm-host
        cluster_ip: {}
        control_plane: ccp
        hosts:
        -   hostname: deployerincloud-ccp-vm-host0001-DCN-VSC-hlm
            ip_address: 192.168.10.2
            name: deployerincloud-ccp-vm-host0001
        -   hostname: deployerincloud-ccp-vm-host0002-DCN-VSC-hlm
            ip_address: 192.168.10.8
            name: deployerincloud-ccp-vm-host0002
        network: HLM-NET
    dcn-vsc-data:
    -   cluster: vm-host
        cluster_ip: {}
        control_plane: ccp
        hosts:
        -   hostname: deployerincloud-ccp-vm-host0001-DCN-VSC-DATA-mgmt
            ip_address: 192.168.245.4
            name: deployerincloud-ccp-vm-host0001
        -   hostname: deployerincloud-ccp-vm-host0002-DCN-VSC-DATA-mgmt
            ip_address: 192.168.245.6
            name: deployerincloud-ccp-vm-host0002
            network: MANAGEMENT-NET</screen>
  <para>
   The playbook uses the following naming scheme for VSCs
   <literal>vsc-<replaceable>IP_OF_MACHINE_HOSTING_VSC_IN_HEX</replaceable></literal>.
   In the example above, the first VSC is
   running on deployerincloud-ccp-vm-host0001 and the second on
   deployerincloud-ccp-vm-host0002. Suppose the IP address of the first VSC's
   and second VSC's hosts are 10.10.11.21 and 10.10.11.31, respectively, and
   suppose the site's domain name is "example.com", The playbook will name the
   VSC as follows:
  </para>
  <informaltable colsep="1" rowsep="1">
   <tgroup cols="5">
    <colspec colname="c1" colnum="1"/>
    <colspec colname="c2" colnum="2"/>
    <colspec colname="c3" colnum="3"/>
    <colspec colname="c4" colnum="4"/>
    <colspec colname="c5" colnum="5"/>
    <thead>
     <row>
      <entry>VSC Mgmt IP</entry>
      <entry>VSC Data IP</entry>
      <entry>Hosted on</entry>
      <entry>VSC Hostname/Vursh Name of VSC VM</entry>
      <entry>VSC FQDN</entry>
     </row>
    </thead>
    <tbody>
     <row>
      <entry>192.168.10.2</entry>
      <entry>192.168.245.4</entry>
      <entry><para>deployerincloud-ccp-vm-host0001 
        (IP: 10.50.11.21)
       </para>
      </entry>
      <entry>vsc-0A320B15</entry>
      <entry>vsc-0A320B15.example.com</entry>
     </row>
     <row>
      <entry>192.168.10.8</entry>
      <entry>192.168.245.6</entry>
      <entry>deployerincloud-ccp-vm-host0002(IP: 10.50.11.31)</entry>
      <entry>vsc-0A320B1F</entry>
      <entry>vsc-0A320B1F.example.com</entry>
     </row>
    </tbody>
   </tgroup>
  </informaltable>
  <para>
   The DCN-VSC playbooks DO NOT UPDATE the dns-servers with the VSC DNS
   records, the admin have to manually add the VSC FQDNs into the site's DNS
   servers. The admin has to decide which VSC will be the active and the
   standby.. In the example below, the admin decided to use "192.168.245.4" as
   the "active" VSC and "192.168.245.6" as the "passive" (aka "standby") VSC.
   Next, the admin will have to modify the
   ~/openstack/my_cloud/definition/dcn/dcn_config.yml to change the
   "vsc_active_ip" and the "vsc_passive_ip" parameters:
  </para>
<screen>---
  product:
    version: 2
  configuration-data:
    - name:  DCN-CONFIG-CP1
      services:
        - dcn
      data:
        dns_domain_name: example.com
        vsd_host_name: vsd4.example.com
        vsd_user: OSadmin
        vsd_passwd: OSadmin
        vsd_cms_id: ''
        vsc_active_ip:  192.168.245.4   &lt;-- changed by the admin
        # If there is only 1 VSC deployed, do not define vsc_passive_ip.
        # Comment out the line below:
        vsc_passive_ip: 192.168.245.6   &lt;-- changed by the admin
        vsc_mgmt_net: HLM
        vsc_data_net: MANAGEMENT
        vsc_image_name: vsc_singledisk
        vsc_user_name: admin
        vsc_user_pass: admin</screen>
  <para>
   Now do the following the deploy the cloud with the new VSCs:
  </para>
<screen>$ cd ~/helion
$ git commit -a -m "Updated the VSC active and passive IPs in ~/openstack/my_cloud/definition/data/dcn/dcn_config.yml"
$ cd ~/helion/hos/ansible
$ ansible-playbook -i hosts/localhost config-processor-run.yml -e encrypt="" -e rekey=""
$ ansible-playbook -i hosts/localhost ready-deployment.yml
$ cd ~/scratch/ansible/next/hos/ansible
$ ansible-playbook -i hosts/verb_hosts hlm-hypervisor-setup.yml
$ ansible-playbook -i hosts/verb_hosts site.yml</screen>
  <para>
   The VSC-VMs will be deployed before the deployment of VRS and VRS-G. When
   the VRS and VRS-G comes up, they will be using the VSC at "vsc_active_ip".
  </para>
 </section>
</section>
