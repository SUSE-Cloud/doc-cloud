<?xml version="1.0"?>
<!DOCTYPE section [
 <!ENTITY % entities SYSTEM "entities.ent"> %entities;
]>
<section xml:id="config_ceph"
 xmlns="http://docbook.org/ns/docbook" version="5.1"
 xmlns:xi="http://www.w3.org/2001/XInclude"
 xmlns:xlink="http://www.w3.org/1999/xlink">
 <title>Ceph Deployment and Configurations</title>
 <para>
  &kw-hos-tm; &kw-hos-version; Ceph deployment leverages the cloud lifecycle
  operations supported by &lcm;. It provides the
  simplified lifecycle management of critical cluster operations such as
  service check, upgrade, and reconfiguring service components. This section
  assumes that you understand cloud input models and highlights only important
  aspects of cloud input models pertaining to Ceph. We focus on deployment
  aspects of the <literal>entry-scale-kvm-ceph</literal> input model, which is
  the most widely used configuration. You can see
  <xref linkend="sec.config_ceph_alternative"/>
  for the deployment of Ceph with various supported options. To ensure the
  proper deployment and verification of Ceph, it is important to read the
  topics and perform the steps in order. This section provides insight on how
  to alter the <literal>entry-scale-kvm-ceph</literal> input model to deploy
  Ceph with various supported options. We recommend that you deploy your
  supported choice only after evaluating all pros and cons.
 </para>
 <orderedlist>
  <listitem>
   <para>
    <xref linkend="sec.ceph.predeploy"/>
   </para>
   <orderedlist>
    <listitem>
     <para>
      Define an OSD Disk Model for an OSD Disk
     </para>
    </listitem>
    <listitem>
     <para>
      Customize Your Service Configuration
     </para>
    </listitem>
   </orderedlist>
  </listitem>
  <listitem>
   <para>
    <xref linkend="sec.ceph.deploy-ceph"/>
   </para>
  </listitem>
  <listitem>
   <para>
    <xref linkend="sec.ceph.verify-ceph-cluster"/>
   </para>
  </listitem>
 </orderedlist>
 <section xml:id="sec.ceph.predeploy">
  <title>Predeployment</title>
  <para>
   Before you start deploying the &kw-hos-tm; cloud with Ceph, you must
   understand the following aspects of Ceph clusters,
  </para>
  <itemizedlist>
   <listitem xml:id="define-osd">
    <formalpara>
     <title>Define an OSD Disk Model for an OSD Disk</title>
     <para>
      This section focus on expressing the storage requirements of an OSD
      (object-storage daemon) node. OSD nodes have system, data, and journal
      disks.
     </para>
    </formalpara>
    <para>
     System disks are used for OSD components, logging, and other tasks. The
     configuration of data and journal disks in important for Ceph deployment.
     A sample disk model file for the <literal>entry-scale-kvm-ceph</literal>
     cloud is as follows.
    </para>
<screen>
<?dbsuse-fo font-size="0.70em"?>
---
  product:
    version: 2

  disk-models:
  - name: OSD-DISKS
    # Disk model to be used for Ceph OSD nodes
    # /dev/sda_root is used as a volume group for /, /var/log and /var/crash
    # sda_root is a templated value to align with whatever partition is really used
    # This value is checked in os config and replaced by the partition actually used
    # on sda for example sda1 or sda5

    volume-groups:
      - name: ardana-vg
        physical-volumes:
          - /dev/sda_root

        logical-volumes:
        # The policy is not to consume 100% of the space of each volume group.
        # 5% should be left free for snapshots and to allow for some flexibility.
          - name: root
            size: 30%
            fstype: ext4
            mount: /
          - name: log
            size: 45%
            mount: /var/log
            fstype: ext4
            mkfs-opts: -O large_file
          - name: crash
            size: 20%
            mount: /var/crash
            fstype: ext4
            mkfs-opts: -O large_file
        consumer:
           name: os

    # Disks to be used by Ceph
    # Additional disks can be added if available
    device-groups:
      - name: ceph-osd-data-and-journal
        devices:
          - name: /dev/sdc
        consumer:
           name: ceph
           attrs:
             usage: data
             journal_disk: /dev/sdd
      - name: ceph-osd-data-and-shared-journal-set-1
        devices:
          - name: /dev/sde
        consumer:
           name: ceph
           attrs:
             usage: data
             journal_disk: /dev/sdg
      - name: ceph-osd-data-and-shared-journal-set-2
        devices:
          - name: /dev/sdf
        consumer:
           name: ceph
           attrs:
             usage: data
             journal_disk: /dev/sdg</screen>
    <para>
     The disk model has the following parameters:
    </para>
    <informaltable>
     <tgroup cols="2">
      <colspec colname="c1" colnum="1" colwidth="1*"/>
      <colspec colname="c2" colnum="2" colwidth="2*"/>
      <thead>
       <row>
        <entry>Value</entry>
        <entry>Description</entry>
       </row>
      </thead>
      <tbody>
       <row>
        <entry><emphasis role="bold">device-groups</emphasis>
        </entry>
        <entry>
         <para>
          The name of the device group. There can be several device groups,
          which allows different sets of disks to be used for different
          purposes.
         </para>
        </entry>
       </row>
       <row>
        <entry><emphasis role="bold">name</emphasis>
        </entry>
        <entry>
         <para>
          An arbitrary name for the device group. The name must be unique.
         </para>
        </entry>
       </row>
       <row>
        <entry><emphasis role="bold">devices</emphasis>
        </entry>
        <entry>
         <para>
          A list of devices allocated to the device group. A
          <literal>name</literal> field containing <literal>/dev/sdb</literal>,
          <literal>/dev/sdc</literal>, <literal>/d ev/sde</literal>, and
          <literal>/dev/sdf</literal> indicates that the device group is used
          by Ceph.
         </para>
        </entry>
       </row>
       <row>
        <entry><emphasis role="bold">consumer</emphasis>
        </entry>
        <entry>
         <para>
          The service that uses the device group. A <literal>name</literal>
          field containing <emphasis role="bold">ceph</emphasis> indicates that
          the device group is used by Ceph.
         </para>
        </entry>
       </row>
       <row>
        <entry><emphasis role="bold">attrs</emphasis>
        </entry>
        <entry>
         <para>
          Attributes associated with the consumer.
         </para>
        </entry>
       </row>
       <row>
        <entry><emphasis role="bold">usage</emphasis>
        </entry>
        <entry>
         <para>
          Devices for a particular service can have several uses. In the
          preceding sample, the <literal>usage</literal> field contains
          <emphasis role="bold">data</emphasis>, which indicates that the
          device is used for data storage.
         </para>
        </entry>
       </row>
       <row>
        <entry><emphasis role="bold">journal_disk</emphasis> [OPTIONAL]</entry>
        <entry>
         <para>
          The disk to be used for storing journal data. When running multiple
          Ceph OSDs on a single node, a journal disk can be shared between OSDs
          of the node.
         </para>
         <para>
          If you do not specify this value, Ceph stores the journal on the
          OSD's data disk (in a separate partition).
         </para>
        </entry>
       </row>
      </tbody>
     </tgroup>
    </informaltable>
    <para>
     The preceding sample file represents the following:
    </para>
    <itemizedlist>
     <listitem>
      <para>
       The first disk is used for OS and system purposes.
      </para>
     </listitem>
     <listitem>
      <para>
       There are three OSD data disks (sdc, sde, and sdf) and two journal disks
       (sdd and sdg). This configuration shows that we can share journal disks
       for multiple OSDs. It is recommended to use an OSD journal disk for four
       OSD data disks. See <xref linkend="bri.ceph.journal"/> for more
       details.
      </para>
     </listitem>
     <listitem>
      <para>
       The drive type is not mentioned for the journal or data disks. You can
       consume any drive type but we <emphasis role="bold">recommend</emphasis>
       using an SSD (solid-state drive) for the journal disk.
      </para>
     </listitem>
    </itemizedlist>
    <para>
     Although the preceding model illustrates mixed use of the journal disk, we
     strongly advise that you keep journal data separate from OSD data, which
     means that your disk model <emphasis role="bold">should not</emphasis>
     have journal disks shared on the same data disks. For more information,
     see <xref linkend="bri.ceph.journal"/>.
    </para>
    <bridgehead renderas="sect5" xml:id="bri.ceph.journal">Usage of Journal Disk</bridgehead>
    <para>
     &kw-hos-tm; &kw-hos-version; recommends storing the Ceph OSD journal on an
     SSD and the OSD object data on a separate hard disk drive. SSD drives are
     costly, so it saves money to use multiple partitions in a single SSD drive
     for multiple OSD journals. We recommend not more than four or five OSD
     journals on each SSD disk as a reasonable balance between cost and optimal
     performance. If you have too many OSD journals on a single SSD, and the
     journal disk crashes, you might lose your data on those disks. Also, too
     many journals in a single SSD can negatively affect performance.
    </para>
    <para>
     Using an OSD journal as a partition on the data disk itself is supported.
     However, you might see a significant decline in Ceph performance because
     each client request to store an object is first written to the journal
     disk before sending an acknowledgment to the client.
    </para>
    <para>
     The Ceph OSD journal size defaults to 5120 MB (5 GB) in &kw-hos-tm;
     &kw-hos-version;. This value can be changed, but it does not apply to any
     existing journal partitions. It will affect new OSDs created after the
     journal size is changed (whether the journal is on the same disk or a
     separate disk than the data disk). To change the journal size, edit the
     <literal>osd_journal_size</literal> parameter in the
     <filename>~/openstack/my_cloud/config/ceph/settings.yml</filename> file.
    </para>
    <para>
     To summarize:
    </para>
	    <orderedlist>
     <listitem>
      <para>
       Use SSD for the journal disk.
      </para>
     </listitem>
     <listitem>
      <para>
       The ratio of OSD data disks to the journal disk is recommended to 4:1.
      </para>
     </listitem>
     <listitem>
      <para>
       The default journal partition size is 5 GB, which you can change. Actual
       journal size depends upon your disk drive <literal>rpm</literal> and
       expected throughput. The formula is:
      </para>
      <screen>
			<?dbsuse-fo font-size="0.70em"?>
			<replaceable>OSD_JOURNAL_SIZE</replaceable> = 2 * (<replaceable>EXPECTED_THROUGHPUT</replaceable> * <replaceable>FILESTORE_MAX_SYNC_INTERVAL</replaceable>)</screen>
     </listitem>
     <listitem>
      <para>
       The journal size for previously configured OSD disks does not change
       even if you change the <option>osd_journal_size</option> parameter in
       the <filename>~/openstack/my_cloud/config/ceph/settings.yml</filename>
       file.
       If you want to resize the journal partition of previously configured OSD
       disks, you should flush journal data, remove the OSD from the cluster,
       and then add it again.
      </para>
     </listitem>
    </orderedlist>
   </listitem>
  </itemizedlist>
  <itemizedlist xml:id="ul_ldl_p5q_kw">
   <listitem>
    <para>
     <emphasis role="bold">Customize Your Service Configuration</emphasis>
    </para>
    <para>
     You must customize the parameters in the following files:
    </para>
    <itemizedlist>
     <listitem>
      <para>
       Customize the parameters in the
       <literal>~/openstack/my_cloud/config/ceph/settings.yml</literal> file.
      </para>
      <para>
       &kw-hos-tm; makes it easy to configure service parameters. All common
       parameters are available in the
       <filename>~/openstack/my_cloud/config/ceph/settings.yml</filename> file.
       You can deploy your cluster without altering any of the parameters.
       However, we advise that you review and understand the parameters before
       deploying your cluster. The following link will assist you in the
       understanding of Ceph placement-groups:
       <link xlink:href="http://docs.ceph.com/docs/master/rados/operations/placement-groups/"/>.
       The following table provides details about parameters you can change and
       descriptions of those parameters.
      </para>
      <table>
       <title>Core Service Parameters</title>
       <tgroup cols="4">
        <thead>
         <row>
          <entry>Parameter</entry>
          <entry>Description</entry>
          <entry>Default Value</entry>
          <entry>Recommendation</entry>
         </row>
        </thead>
        <tbody>
         <row>
          <entry>ceph_cluster</entry>
          <entry>
           <para>
            The name of the Ceph clusters. The default value is Ceph.
           </para>
          </entry>
          <entry>Ceph</entry>
          <entry>
           <para>
            Customize to suit your requirements.
           </para>
          </entry>
         </row>
         <row>
          <entry>ceph_release</entry>
          <entry>
           <para>
            The name of the Ceph release.
           </para>
          </entry>
          <entry>hammer</entry>
          <entry>
           <para>
            Do not change the default value.
           </para>
          </entry>
         </row>
         <row>
          <entry>osd_pool_default_size</entry>
          <entry>
           <para>
            The number of replicas for objects in the pool.
           </para>
          </entry>
          <entry>3</entry>
          <entry>
           <para>
            Do not lower the default value. The value can be increased to the
            maximum number of OSD nodes in the environment (increasing it
            beyond this limit will cause the cluster to never reach an
            active+clean state).
           </para>
          </entry>
         </row>
         <row>
          <entry>osd_pool_default_pg_num</entry>
          <entry>
           <para>
            The default number of placement groups for a pool. This value
            changes based on the number of OSDs available.
           </para>
          </entry>
          <entry>128</entry>
          <entry>
           <para>
            The value can be changed based on the number of OSD servers/nodes
            in the deployment. Refer to the Ceph PG calculator at
            <link xlink:href="http://ceph.com/pgcalc/">http://ceph.com/pgcalc/</link>
            to customize it.
           </para>
          </entry>
         </row>
         <row>
          <entry>fstype</entry>
          <entry>
           <para>
            Storage filesystem type for OSDs.
           </para>
          </entry>
          <entry>xfs</entry>
          <entry>
           <para>
            Only the xfs file system is certified.
           </para>
          </entry>
         </row>
         <row>
          <entry>zap_data_disk</entry>
          <entry>
           <para>
            Zap partition table and contents of the disk.
           </para>
          </entry>
          <entry>True</entry>
          <entry>
           <para>
            Not recommended to change the default value.
           </para>
          </entry>
         </row>
         <row>
          <entry>persist_mountpoint</entry>
          <entry>
           <para>
            Place to persist OSD data disk mount point.
           </para>
          </entry>
          <entry>fstab</entry>
          <entry>
           <para>
            Not recommended to change the default value (as it ensures that the
            OSD data disks are mounted automatically on a system reboot).
           </para>
          </entry>
         </row>
         <row>
          <entry>osd_settle_time</entry>
          <entry>
           <para>
            The time in seconds to wait for after starting/restarting Ceph OSD
            services.
           </para>
          </entry>
          <entry>10 seconds</entry>
          <entry>
           <para>
            Increase this value only if the number of OSD servers is more than
            three or the servers have a slow network.
           </para>
          </entry>
         </row>
         <row>
          <entry>osd_journal_size</entry>
          <entry>
           <para>
            The size of the journal in megabytes.
           </para>
          </entry>
          <entry>5120</entry>
          <entry>
           <para>
            You can increase this value to achieve optimal use of the journal
            disk (if it is shared between multiple OSDs).
           </para>
          </entry>
         </row>
         <row>
          <entry>data_disk_poll_attempts</entry>
          <entry>
           <para>
            The maximum number of attempts before attempting to activate an OSD
            for a new disk (default value 5).
           </para>
          </entry>
          <entry>5</entry>
          <entry>
           <para>
            Increase this value only if the OSD data disk drives are
            under-performing or slower than expected. Because this parameter
            and <literal>data_disk_poll_interval</literal> (following) have a
            combined effect, we recommend that you consider both while tweaking
            either of them.
           </para>
          </entry>
         </row>
         <row>
          <entry>data_disk_poll_interval</entry>
          <entry>
           <para>
            The time interval in seconds to wait between
            <literal>data_disk_poll_attempts</literal>.
           </para>
          </entry>
          <entry>12</entry>
          <entry>
           <para>
            You can customize this value to suit your requirements. However,
            because this parameter and
            <literal>data_disk_poll_attempts</literal> (preceding) have a
            combined effect, we recommend that you consider both while tweaking
            either of them.
           </para>
          </entry>
         </row>
         <row>
          <entry>osd_max_open_files</entry>
          <entry>Maximum number of file descriptors for OSD.</entry>
          <entry>32768</entry>
          <entry>Do not change the default value.</entry>
         </row>
         <row>
          <entry>mon_default_dir</entry>
          <entry>Directory to store monitor data.</entry>
          <entry><literal>/var/lib/ceph/mon/&lt;ceph_cluster&gt;</literal>
          </entry>
          <entry>Do not change the default value.</entry>
         </row>
         <row>
          <entry>mon_max_open_files</entry>
          <entry>Maximum number of file descriptors for monitor.</entry>
          <entry>16384</entry>
          <entry>Do not change the default value.</entry>
         </row>
         <row>
          <entry>root_bucket</entry>
          <entry>Ceph CRUSH map root bucket name.</entry>
          <entry>default</entry>
          <entry>
           <para>
            Not recommended to change the default value.
           </para>
           <para>
            Changing this value affects CRUSH map and data placement. If you
            change the default value ensure to create a new rule set with a new
            root bucket and map the Ceph storage pools to use a new rule set.
           </para>
           <para>
            It is strongly recommended not to change this value post day-zero
            deployment. Changing the value after deployment results in a newly
            added OSD nodes to go to a newer <literal>root_bucket</literal>. It
            affects placement of the placement groups (data) of the storage
            pools.
           </para>
           <para>
            If you are upgrading cluster(s) from &kw-hos-tm; 3.0 to &kw-hos-tm;
            &kw-hos-version;, you are strongly advice not to change the default
            value of <literal>root_bucket</literal>.
           </para>
          </entry>
         </row>
        </tbody>
       </tgroup>
      </table>
      <table>
       <title>RADOS Gateway (RGW) Parameters</title>
       <tgroup cols="4">
        <colspec colname="c1" colnum="1"/>
        <colspec colname="c2" colnum="2"/>
        <colspec colname="newCol3" colnum="3" colwidth="1*"/>
        <colspec colname="newCol4" colnum="4" colwidth="1*"/>
        <thead>
         <row>
          <entry>Parameter</entry>
          <entry>Description</entry>
          <entry>Default Value</entry>
          <entry>Recommendation</entry>
         </row>
        </thead>
        <tbody>
         <row>
          <entry>radosgw_user</entry>
          <entry>
           <para>
            The name of the Ceph client user for <literal>radosgw</literal>.
           </para>
          </entry>
          <entry>gateway</entry>
          <entry>
           <para>
            Customize to suit your requirements.
           </para>
          </entry>
         </row>
         <row>
          <entry>radosgw_admin_email </entry>
          <entry>
           <para>
            The e-mail address of the server administrator.
           </para>
          </entry>
          <entry><literal>admin@hpe.com</literal>
          </entry>
          <entry>
           <para>
            Update the e-mail address of the server administrator.
           </para>
          </entry>
         </row>
         <row>
          <entry>rgw_keystone_service_type</entry>
          <entry namest="c2" nameend="newCol4">
           <para>
            <emphasis role="bold">DEPRECATED</emphasis>
           </para>
           <para>
            To configure RADOS Gateway before deployment refer to
            <xref linkend="pro.ceph.configure_service_type"/>.
           </para>
          </entry>
         </row>
         <row>
          <entry>rgw_keystone_accepted_roles </entry>
          <entry>
           <para>
            Only users having either of the roles listed here will be able to
            access the Swift APIs of <literal>radosgw</literal>.
           </para>
          </entry>
          <entry><literal>admin</literal>, <emphasis>_member_</emphasis>
          </entry>
          <entry>
           <para>
            Do not change the default value.
           </para>
          </entry>
         </row>
        </tbody>
       </tgroup>
      </table>
      <note>
       <para>
        The default service password for the RADOS Gateway service can be
        modified by following the steps documented at
        <!-- FIXME: <xref href="../../operations/change_service_passwords.xml#servicePasswords/changing-keystone-credentials-for-rgw"> Changing RADOS Gateway Credentials</xref> -->.
       </para>
      </note>
      <para>
      </para>
      <para>
       The following procedure shows how to configure the service type before
       deployment:
      </para>
      <procedure xml:id="pro.ceph.configure_service_type">
       <title>Configuring the RADOS Gateway service type in the Keystone catalog</title>
       <step>
        <para>
         You can configure the RADOS Gateway service type in Keystone catalog
         by replacing the <literal>ceph-object-store</literal> with desired
         value in <literal>~/openstack/hos/services/ceph/rgw.yml</literal> file
         on the &lcm; node.
        </para>
<screen>advertises-to-services:
     -  service-name: KEY-API
        entries:
        -   service-name: ceph-rgw
            <emphasis role="bold">service-type: ceph-object-store</emphasis>
            service-description: "Ceph Object Storage Service"
            url-suffix: "/swift/v1"
</screen>
       </step>
       <step>
        <para>
         After you have modified the
         <emphasis role="bold">service-type</emphasis>, commit the change to
         the local git repository.
        </para>
<screen>cd ~/helion
git checkout site
git add ~/openstack/hos/services/ceph/rgw.yml
git commit -m "Updating the RADOS Gateway service type"</screen>
       </step>
       <step>
        <para>
         Rerun the configuration processor.
        </para>
<screen>cd ~/openstack/ardana/ansible
ansible-playbook -i hosts/localhost config-processor-run.yml</screen>
       </step>
       <step>
        <para>
         Rerun the deployment area preparation playbooks.
        </para>
<screen>cd ~/openstack/ardana/ansible
ansible-playbook -i hosts/localhost ready-deployment.yml</screen>
       </step>
       <step>
        <para>
         Run reconfiguration playbook in deployment area.
        </para>
<screen>cd ~/scratch/ansible/next/ardana/ansible
ansible-playbook -i hosts/verb_hosts keystone-reconfigure.yml</screen>
       </step>
		 </procedure>
      <para>
       <emphasis role="bold">Configuring the RADOS Gateway service type after
       deployment</emphasis>
      </para>
      <para>
       To update the RADOS Gateway service type in a deployed or a running
       cloud, you must delete the <literal>ceph-rgw</literal> service from the
       Keystone catalog and perform the same steps as mentioned in the
       preceding section (<xref linkend="pro.ceph.configure_service_type"/>).
      </para>
      <procedure>
       <step>
        <para>
         To delete the <literal>ceph-rgw</literal> service, you must know the
         service-id. Execute the following command from a controller node.
        </para>
<screen>source ~/keystone.osrc
openstack service list |grep ceph-rgw | awk '{print $2}'
openstack service delete &lt;service-id&gt;</screen>
       </step>
      </procedure>
      <table xml:id="table_fgv_c3m_hw" colsep="1" rowsep="1">
       <title>Ceph client parameters</title>
       <tgroup cols="4">
        <colspec colname="c1" colnum="1"/>
        <colspec colname="c2" colnum="2"/>
        <colspec colname="newCol3" colnum="3" colwidth="1*"/>
        <colspec colname="newCol4" colnum="4" colwidth="1*"/>
        <thead>
         <row>
          <entry>Value</entry>
          <entry>Description</entry>
          <entry>Default Value</entry>
          <entry>Recommendation</entry>
         </row>
        </thead>
        <tbody>
         <row>
          <entry>pg_active_delay_time</entry>
          <entry>The delay time for Ceph PGs to come into active state. </entry>
          <entry>10</entry>
          <entry>
           <para>
            You can increase this value if the number of OSD servers/nodes in
            the deployment is more than three. Because this parameter and
            <emphasis role="bold">pg_active_retries</emphasis> (following) have
            a combined effect, we recommend that you consider both while
            tweaking either of them.
           </para>
          </entry>
         </row>
         <row>
          <entry>pg_active_retries</entry>
          <entry>
           <para>
            The number of retries for Ceph placement groups to come into active
            state with a duration of <literal>pg_active_delay_time</literal>
            seconds between entries.
           </para>
          </entry>
          <entry>5</entry>
          <entry>
           <para>
            You can customize this value to suit your requirements. However,
            because this parameter and
            <emphasis role="bold">pg_active_delay_time</emphasis> (preceding)
            have a combined effect, we recommend that you consider both while
            tweaking either of them.
           </para>
          </entry>
         </row>
        </tbody>
       </tgroup>
      </table>
     </listitem>
     <listitem>
      <para>
       Customize parameters at
       <filename>~/openstack/ardana/ansible/roles/_CEP-CMN/defaults/main.yml</filename>.
      </para>
      <para>
       The following table provides parameter descriptions. You can edit each
       parameter in the <literal>main.yml</literal> file.
      </para>
      <informaltable>
       <tgroup cols="2">
        <colspec colname="c1" colnum="1"/>
        <colspec colname="c2" colnum="2"/>
        <thead>
         <row>
          <entry>Value</entry>
          <entry>Description</entry>
         </row>
        </thead>
        <tbody>
         <row>
          <entry>fsid</entry>
          <entry>
           <para>
            A unique identifier, File System ID, for the Ceph cluster that you
            should generate prior to deploying a cluster (use the
            <literal>uuidgen</literal> command to generate a new FSID). When
            set, this value cannot be changed.
           </para>
          </entry>
         </row>
        </tbody>
       </tgroup>
      </informaltable>
     </listitem>
    </itemizedlist>
   </listitem>
  </itemizedlist>
 </section>
 <section xml:id="sec.ceph.deploy-ceph">
  <title>Deploying Ceph</title>
  <para>
   To deploy a new &kw-hos-tm; Ceph cloud using the default
   <literal>entry-scale-kvm-ceph</literal> model, follow these steps
   <!--, starting with <b>Edit Your Ceph Environment Input Files</b>-->.
  </para>
  <note>
   <itemizedlist>
    <listitem>
     <para>
      In a multi-region cloud, Ceph can be deployed only as a shared service.
      In other words, Ceph services (Monitor, OSD, and RADOS Gateway servers)
      should be deployed in the shared control plane (such that those services
      will run in the same control plane as the Keystone service) ONLY.
     </para>
    </listitem>
    <listitem>
     <para>
      The non-shared control plane(s) need to ensure that the ceph-monitor
      service is specified in the
      <emphasis role="bold">service_components</emphasis> section of uses and
      <emphasis role="bold">imports</emphasis> sections of the control plane
      definition.
     </para>
    </listitem>
    <listitem>
     <para>
      Ensure that all the regions that need access to Ceph have the shared
      control plane included.
     </para>
    </listitem>
   </itemizedlist>
  </note>
  <para>
   <emphasis role="bold">Edit Your Ceph Environment Input Files</emphasis>
  </para>
  <para>
   Perform the following steps:
  </para>
  <procedure>
   <step>
    <para>
     Log in to the &lcm;.
    </para>
   </step>
   <step>
    <para>
     Copy the example configuration files into the required setup directory and
     edit them to contain the details of your environment:
    </para>
<screen>cp -r ~/openstack/examples/entry-scale-kvm-ceph/* \
  ~/openstack/my_cloud/definition/</screen>
    <para>
     Enter your environment information into the configuration files in the
     <literal>~/openstack/my_cloud/definition</literal> directory.
    </para>
    <para>
     You can find details of how to do this at
     <xref linkend="input_model"/>.
    </para>
   </step>
   <step>
    <para>
     Edit the
     <literal>~/openstack/my_cloud/definition/data/servers.yml</literal>
     file and enter details. If you are using alternative RADOS Gateway
     deployments, see
     <xref linkend="sec.config_ceph_alternative"/>
     before editing the <literal>servers.yml</literal> file.
    </para>
<screen> # Ceph OSD Nodes
    - id: osd1
      ip-addr: 192.168.10.9
      role: OSD-ROLE
      server-group: RACK1
      nic-mapping: MY-2PORT-SERVER
      mac-addr: "8b:f6:9e:ca:3b:78"
      ilo-ip: 192.168.9.9
      ilo-password: password
      ilo-user: admin

    - id: osd2
      ip-addr: 192.168.10.10
      role: OSD-ROLE
      server-group: RACK2
      nic-mapping: MY-2PORT-SERVER
      mac-addr: "8b:f6:9e:ca:3b:79"
      ilo-ip: 192.168.9.10
      ilo-password: password
      ilo-user: admin

    - id: osd3
      ip-addr: 192.168.10.11
      role: OSD-ROLE
      server-group: RACK3
      nic-mapping: MY-2PORT-SERVER
      mac-addr: "8b:f6:9e:ca:3b:7a"
      ilo-ip: 192.168.9.11
      ilo-password: password
      ilo-user: admin

# Ceph RGW Nodes
   - id: rgw1
     ip-addr: 192.168.10.12
     role: RGW-ROLE
     server-group: RACK1
     nic-mapping: MY-2PORT-SERVER
     mac-addr: "8b:f6:9e:ca:3b:62"
     ilo-ip: 192.168.9.12
     ilo-password: password
     ilo-user: admin

   - id: rgw2
     ip-addr: 192.168.10.13
     role: RGW-ROLE
     server-group: RACK2
     nic-mapping: MY-2PORT-SERVER
     mac-addr: "8b:f6:9e:ca:3b:63"
     ilo-ip: 192.168.9.13
     ilo-password: password
     ilo-user: admin </screen>
    <para>
     The preceding sample file contains three OSD nodes and two RADOS Gateway
     nodes.
    </para>
   </step>
   <step>
    <para>
     Edit the file
     <literal>~/openstack/my_cloud/definition/data/disks_osd.yml</literal> to
     align the disk model to fit the server specification in your environment.
     For details on disk models, refer to
     <xref linkend="define-osd"/>.
    </para>
    <para>
     Ceph service configuration parameters can be modified as described in
     <xref linkend="sec.ceph.predeploy"/>.
    </para>
   </step>
   <step>
    <para>
     Commit your configuration to the local git repo
     (<xref linkend="using_git"/>):
    </para>
<screen>cd ~/openstack/ardana/ansible
git add -A
git commit -m "My config or other commit message"</screen>
   </step>
   <step>
    <para>
     After you set up your configuration files, continue with the installation
     procedure from <xref linkend="sec.kvm.provision"/>.
    </para>
    <note>
     <para>
      For any troubleshooting information regarding the OSD node failure, see
      <!-- FIXME: <xref linkend="troubleshooting_ceph"/> -->.
     </para>
    </note>
   </step>
 </procedure>
 </section>
 <section xml:id="sec.ceph.verify-ceph-cluster">
  <title>Verifying Ceph Cluster Status</title>
  <para>
   If you have deployed RADOS Gateway with core Ceph, then you need to ensure
   that all service components including RADOS Gateway are functioning as
   expected.
  </para>
  <para>
   Perform the following steps to check the status of the Ceph cluster:
  </para>
  <procedure>
   <title>Verifying Core Ceph</title>
   <step>
    <para>
     Log in to the monitor node.
    </para>
   </step>
   <step>
    <para>
     Execute the following command and make sure that the result is HEALTH_OK
     or HEALTH_WARN:
    </para>
<screen>$ ceph health</screen>
    <para>
     Optionally, you can also set up &lcm; as a Ceph client
     node (refer to <xref linkend="sec.ceph.setup-deployer-node"/>)
     and execute the preceding command from &lcm;.
    </para>
   </step>
  </procedure>
  <para>
   To make sure that a Keystone user can access RADOS Gateway using Swift,
   perform the following steps:
  </para>
  <procedure>
   <title>Verifying RADOS Gateway</title>
   <step>
    <para>
     Log in to a controller node.
    </para>
   </step>
   <step>
    <para>
     Source the <literal>service.osrc</literal> file:
    </para>
<screen>source ~/service.osrc</screen>
   </step>
   <step>
    <para>
     Execute the following command to generate a list of the containers
     associated with the user:
    </para>
<screen>swift --os-service-type ceph-object-store list</screen>
    <para>
     If the containers are listed, this indicates that RADOS Gateway is
     accessible.
    </para>
   </step>
 </procedure>
 </section>
</section>
