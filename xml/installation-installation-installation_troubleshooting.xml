<?xml version="1.0"?>
<!DOCTYPE chapter [
 <!ENTITY % entities SYSTEM "entities.ent"> %entities;
]>
<chapter xml:id="troubleshooting_installation"
 xmlns="http://docbook.org/ns/docbook" version="5.1"
 xmlns:xi="http://www.w3.org/2001/XInclude"
 xmlns:xlink="http://www.w3.org/1999/xlink">
 <title>Troubleshooting the Installation</title>
 <para>
  We have gathered some of the common issues that occur during installation and
  organized them by when they occur during the installation. These sections
  will coincide with the steps labeled in the installation instructions.
 </para>
 <itemizedlist>
  <listitem>
   <para>
     <xref linkend="sec.trouble-deployer_setup"/>
   </para>
  </listitem>
  <listitem>
   <para>
    <xref linkend="sec.trouble-deploy_cobbler"/>
   </para>
  </listitem>
  <listitem>
   <para>
     <xref linkend="sec.trouble-config_processor"/>
   </para>
  </listitem>
  <listitem>
   <para>
    <xref linkend="sec.trouble-deploy_cloud"/>
   </para>
  </listitem>
  <listitem>
   <para>
    <xref linkend="ts_cinder_config"/>
   </para>
  </listitem>
 </itemizedlist>
 <section xml:id="sec.trouble-deployer_setup">
  <title>Issues during &lcm; Setup</title>
  <bridgehead renderas="sect5">Issue: Running the hos-init.bash script when configuring your &lcm; does not complete</bridgehead>
  <para>
   Part of what the <literal>~/hos-3.0.0/hos-init.bash</literal> script does is
   install git and so if your DNS nameserver(s) is not specified in your
   <filename>/etc/resolv.con</filename> file, is not valid, or is not
   functioning properly on your &lcm; then it will not be able to
   complete.
  </para>
  <para>
   To resolve this issue, double check your nameserver in your
   <filename>/etc/resolv.con</filename> file and then re-run the script.
  </para>
 </section>
 <section xml:id="sec.trouble-deploy_cobbler">
  <title>Issues while Provisioning your Baremetal Nodes</title>
  <bridgehead renderas="sect5">Issue: The <filename>bm-power-status.yml</filename> playbook returns an error.</bridgehead>
  <para>
   If the output of the bm-power-status.yml playbook gives you an error like
   the one below for one of your servers then the most likely cause is an issue
   with the information located in the
   <filename>~/helion/my_cloud/definition/servers.yml</filename>
   file. It could be that the information is incorrect or the iLO username you
   are using does not have administrator privileges. Verify all of the
   information for that server and recommit the changes to git and re-run the
   playbook if the information was incorrect. If it is a rights issue, ensure
   the iLO user has administrative privileges.
  </para>
  <para>
   Error:
  </para>
<screen>
<?dbsuse-fo font-size="0.70em"?>
failed: [vsa1] =&gt; {"cmd": "ipmitool -I lanplus -E -N 5 -R 12 -U iLOAdmin -H 10.12.11.185
power status", "failed": true, "rc": 1}
stderr: Error: Unable to establish IPMI v2 / RMCP+ session</screen>
  <para>
   This error below, which may come with running the
   <literal>bm-power-status.yml</literal> playbook, may be resolved by placing
   the<literal> ilo-password</literal> values in your
   <literal>servers.yml</literal> file in double quotes.
  </para>
  <para>
   Error:
  </para>
<screen>failed: [controller2] =&gt; {"failed": true}
msg: ipmi: 'int' object has no attribute 'startswith'</screen>
  <bridgehead renderas="sect5">Cobbler Deployment Fails due to CIDR Error</bridgehead>
  <para>
   The error will look similar to this:
  </para>
<screen>TASK: [cobbler | set variables | Find baremetal nic] *****************
failed: [localhost] =&gt; {"failed": true}
msg: matchcidr: no nic matching 10.242.115.0/24
lo 127.0.0.1/8
eth0 10.242.115.18/26</screen>
  <para>
   The most likely cause of this is that your <literal>netmask</literal> entry
   in the <literal>~/helion/my_cloud/definition/data/servers.yml</literal> file
   is incorrect.
  </para>
  <para>
   To resolve this issue ensure that you have the correct
   <literal>subnet</literal> and <literal>netmask</literal> entries in the
   <filename>~/helion/my_cloud/definition/data/servers.yml</filename> for your
   Management network.
  </para>
  <para>
   You can also verify these values exist in the
   <filename>/etc/network/interfaces.d/ethX</filename> file on your &lcm;.
   These should match the entries in the
   <filename>servers.yml</filename> file.
  </para>
  <para>
   Once you make your corrections to your configuration files, commit the
   changes with these steps:
  </para>
  <procedure>
   <step>
    <para>
     Ensuring that you stay within the <filename>~/helion</filename> directory,
     commit the changes you just made:
    </para>
<screen>cd ~/helion
git commit -a -m "commit message"</screen>
   </step>
   <step>
    <para>
     Redeploy Cobbler:
    </para>
<screen>cd ~/helion/hos/ansible
ansible-playbook -i hosts/localhost cobbler-deploy.yml</screen>
   </step>
  </procedure>
  <bridgehead renderas="sect5">Issue: Configuration changes needed after Cobbler deploy</bridgehead>
  <para>
   If you've made a mistake or wish to change your
   <literal>~/helion/my_cloud/definition/data/servers.yml</literal>
   configuration file after you've already run the
   <literal>cobbler-deploy.yml</literal> playbook, follow these steps to ensure
   Cobbler gets updated with the new server information:
  </para>
  <procedure>
   <step>
    <para>
     Ensure your <literal>servers.yml</literal> file is updated
    </para>
   </step>
   <step>
    <para>
     Ensuring that you stay within the <literal>~/helion</literal> directory,
     commit the changes you just made:
    </para>
<screen>cd ~/helion
git commit -a -m "commit message"</screen>
   </step>
   <step>
    <para>
     Determine which nodes you have entered into Cobbler by using:
    </para>
<screen>sudo cobbler system list</screen>
   </step>
   <step>
    <para>
     Remove the nodes that had the old information from Cobbler using:
    </para>
<screen>sudo cobbler system remove --name &lt;nodename&gt;</screen>
   </step>
   <step>
    <para>
     Re-run the <literal>cobbler-deploy.yml</literal> playbook to update the
     new node definitions to Cobbler:
    </para>
<screen>cd ~/helion/hos/ansible
ansible-playbook -i hosts/localhost cobbler-deploy.yml</screen>
    <note>
     <para>
      If you've also already run the <literal>bm-reimage.yml</literal> playbook
      then read the <xref linkend="sec.install-trouble.reimage"/>
      section below for how to ensure your nodes get re-imaged when re-running
      this playbook.
     </para>
    </note>
   </step>
  </procedure>
  <bridgehead renderas="sect5">Issue: bm-reimage.yml playbook doesn't find any nodes to image</bridgehead>
  <para>
   You may receive this error when running the
   <literal>bm-reimage.yml</literal> playbook:
  </para>
<screen>TASK: [cobbler | get-nodelist | Check we have targets] *****************
failed: [localhost] =&gt; {"failed": true}
msg: There is no default set of nodes for this command, use -e nodelist

FATAL: all hosts have already failed -- aborting</screen>
  <para>
   This behavior occurs when you don't specify the <literal>-e
   nodelist</literal> switch to your command and all of your nodes are marked
   as <literal>netboot-enabled: false</literal> in Cobbler. By default, without
   the <literal>-e nodelist</literal> switch, the
   <literal>bm-reimage.yml</literal> playbook will only reimage the nodes
   marked as <literal>netboot-enabled: True</literal>, which you can verify
   which nodes you have that are marked as such with this command:
  </para>
<screen>sudo cobbler system find --netboot-enabled=1</screen>
  <para>
   If this is on a fresh install and none of your nodes have been imaged with
   the &kw-hos; ISO, then you should be able to remove all of your nodes from
   Cobbler and re-run the <literal>cobbler-deploy.yml</literal> playbook. You
   can do so with these commands:
  </para>
  <procedure>
   <step>
    <para>
     Get a list of your nodes in Cobbler:
    </para>
<screen>sudo cobbler system list</screen>
   </step>
   <step>
    <para>
     Remove each of them with this command:
    </para>
<screen>sudo cobbler system remove --name SYSTEM_NAME</screen>
   </step>
   <step>
    <para>
     Confirm they are all removed in Cobbler:
    </para>
<screen>sudo cobbler system list</screen>
   </step>
   <step>
    <para>
     Re-run <literal>cobbler-deploy.yml</literal>:
    </para>
<screen>cd ~/helion/hos/ansible
ansible-playbook -i hosts/localhost cobbler-deploy.yml</screen>
   </step>
   <step>
    <para>
     Confirm they are all now set to <literal>netboot-enabled: True</literal>:
    </para>
<screen>sudo cobbler system find --netboot-enabled=1</screen>
   </step>
  </procedure>
  <bridgehead renderas="sect5">Issue: The <option>--limit</option> switch does not do anything</bridgehead>
  <para>
   If the <literal>cobbler-deploy.yml</literal> playbook fails, it makes a
   mention that to retry you should use the <literal>--limit</literal> switch,
   as seen below:
  </para>
<screen>to retry, use: --limit @/home/headmin/cobbler-deploy.retry</screen>
  <para>
   This is a standard Ansible message but it is not needed in this context.
   Please use our instructions described above to remove your systems, if
   necessary, from Cobbler before re-running the playbook as the
   <literal>--limit</literal> is not needed. Note that using the
   <literal>--limit</literal> switch does not cause any harm and won't prevent
   the playbook from completing, it's just not a necessary step.
  </para>
  <bridgehead xml:id="sec.install-trouble.install_fail">Issue: Dealing With Nodes that Fail to Install</bridgehead>
  <para>
   The <literal>bm-reimage.yml</literal> playbook will take every node as far
   as it can through the baremetal install process. Nodes that fail will not
   prevent the others from continuing to completion. At the end of the run you
   will get a list of the nodes (if any) that failed to install.
  </para>
  <para>
   If you run <literal>bm-reimage.yml</literal> a second time, by default it
   will target only the failed nodes the second time round (because the others
   are already marked as "completed"). Alternatively you can target specific
   nodes for reimage using <literal>-e nodelist</literal> as described in the
   section below.
  </para>
  <para>
   The places where you are most likely to see a node fail is timeout in the
   "wait for shutdown" step, which means that the node did not successfully
   install an operating system (for example it could be stuck in POST) or a timeout in
   "wait for ssh" at the end of the baremetal install. This means that the node
   did not come back up after being powered on. To fix the issues you'll need
   to connect to the nodes' consoles and investigate.
  </para>
  <bridgehead xml:id="sec.install-trouble.reimage">How to re-image an existing node</bridgehead>
  <para>
   Once &cloudos; has been successfully installed on a node, that
   node will be marked as "installed" and subsequent runs of
   <literal>bm-reimage.yml</literal> (and related playbooks) will not target
   it. This is deliberate so that you can't reimage the node by accident. If
   you do need to reimage existing nodes you will need to use the <literal>-e
   nodelist</literal> option to target them specifically. For example:
  </para>
<screen>
<?dbsuse-fo font-size="0.70em"?>
ansible-playbook -i hosts/localhost bm-reimage.yml -e nodelist=cpn-0044,cpn-0045</screen>
  <note>
   <para>
    You can target all nodes with <literal>-e nodelist=all</literal>
   </para>
  </note>
  <para>
   This will power cycle the specified nodes and reinstall the operating system
   on them, using the existing settings stored in Cobbler.
  </para>
  <para>
   If you want to change settings for a node in the configuration files, see
   <xref linkend="sec.trouble-deploy_cobbler"/>.
  </para>
  <bridgehead renderas="sect5">Issue: Wait for SSH phase in bm-reimage.yml hangs</bridgehead>
  <para>
   This issue has been observed during deployment to systems configured with
   QLogic based BCM578XX network adapters utilizing the bnx2x driver and is
   currently under investigation. The symptom manifests following a cold boot
   during deployment at the "wait for SSH" phase in bm-reimage.yml which will
   result in a hang and eventually timeout, causing the baremetal install to
   fail. The presence of this particular issue can be further confirmed by
   connecting to the remote console of the server via iLO or by checking the
   server's dmesg output for the presence of bnx2_panic_dump messages, similar
   to the following:
  </para>
<screen>
<?dbsuse-fo font-size="0.70em"?>
bnx2x: [bnx2x_prev_unload_common:10433(eth%d)]Failed to empty BRB, hope for the best  ...
bnx2x: [bnx2x_stats_update:1268(eth0)]storm stats were not updated for 3 times
bnx2x: [bnx2x_stats_update:1269(eth0)]driver assert
bnx2x: [bnx2x_panic_dump:929(eth0)]begin crash dump -----------------  .....
bnx2x: [bnx2x_panic_dump:1163(eth0)]end crash dump -----------------</screen>
  <para>
   This workaround is completed by rebooting the server.
  </para>
  <bridgehead renderas="sect5">Subsequent runs of bm-reimage.yml sometimes fail to trigger network install</bridgehead>
  <para>
   If you've run the <literal>site.yml</literal> playbook during an install and
   have a reason to run the <literal>bm-reimage.yml</literal> playbook
   afterwards, it may fail to trigger the network install over PXE due to the
   name of the interface changing during this process.
  </para>
  <para>
   To resolve this issue, you can run the <literal>cobbler-deploy.yml</literal>
   playbook to refresh these settings and then proceed to run
   <literal>bm-reimage.yml</literal>.
  </para>
  <bridgehead renderas="sect5">Issue: Soft lockup at imaging</bridgehead>
  <para>
   When imaging your nodes, if you see a kernel error of the type "BUG: soft
   lockup - CPU#10 stuck...." you should reset the node and make sure it is
   imaged properly next time. Note that depending on when you get the error,
   you may have to rerun <literal>cobbler-deploy.yml</literal>. If the
   bm-reimage playbook says it failed to image the node, then cobbler knows
   this has occurred and you can reset the node. If not, you can follow the
   instructions above for <xref linkend="sec.install-trouble.reimage"/>.
  </para>
  <bridgehead renderas="sect5">Blank Screen Seen When Monitoring the Imaging Step</bridgehead>
  <para>
   If you are watching the <literal>os-install</literal> process on a node via
   the console output, there can be a pause between 2-3 minutes in length where
   nothing gets reported to the console screen. This is just after the grub
   menu has been displayed on a UEFI-based system.
  </para>
  <para>
   This is normal and nothing to be concerned with. The imaging process will
   continue on after this pause.
  </para>
  <bridgehead renderas="sect5">Unable to SSH to a Compute Node after the bm-reimage Step Even Though SSH Keys Are In Place</bridgehead>
  <para>
   If after running the <literal>bm-reimage.yml</literal> playbook you get a
   reported failure when attempting to SSH to one or more Compute nodes stating
   that permission was denied and you have confirmed that the SSH keys are
   correctly coped from the &lcm; to the Compute node then you can
   follow the steps below to resolve this issue.
  </para>
  <para>
   The suspected root cause of this issue is that the bm-reimage playbook is
   finding a different MAC address when attempting to SSH to the Compute node
   than was specified in the <filename>servers.yml</filename> file. This could be
   because you entered an incorrect MAC address or you may have specified the
   same IP address to two different Compute nodes.
  </para>
  <para>
   To resolve this issue you can follow these steps:
  </para>
  <procedure>
   <step>
    <para>
     Remove the Compute node that failed from Cobbler using this command:
    </para>
<screen>sudo cobbler system remove --name &lt;node name&gt;</screen>
    <note>
     <para>
      Use <literal>sudo cobbler system list</literal> to get a list of your
      nodes in Cobbler.
     </para>
    </note>
   </step>
   <step>
    <para>
     Correct the IP and MAC address information in your
     <literal>~/helion/my_cloud/definition/data/servers.yml</literal> file.
    </para>
   </step>
   <step>
    <para>
     Commit your changes to git:
    </para>
<screen>cd ~/helion/hos/ansible
git add -A
git commit -m "My config or other commit message"</screen>
   </step>
   <step>
    <para>
     Re-run the <literal>bm-reimage.yml</literal> playbook and confirm the
     error is not received again.
    </para>
   </step>
  </procedure>
  <bridgehead renderas="sect5">Server Crashes During Imaging</bridgehead>
  <para>
   If your server suffers a kernel panic just after it gets PXE booted, a
   possible cause could be that you have an incorrect bus address for the
   server specified in the <literal>nic_mappings.yml</literal> file.
  </para>
  <para>
   To resolve this issue you will need to obtain the proper bus address. You
   can do this by installing &hlinux; on the node and once you
   receive a terminal prompt you can use the command below to obtain the proper
   bus address:
  </para>
<screen>sudo lspci -D | grep -i eth</screen>
  <para>
   Once you have the proper bus address, follow these steps to finish the
   reimage:
  </para>
  <procedure>
   <step>
    <para>
     Remove the node that failed from Cobbler using this command:
    </para>
<screen>sudo cobbler system remove --name &lt;node name&gt;</screen>
    <note>
     <para>
      Use <literal>sudo cobbler system list</literal> to get a list of your
      nodes in Cobbler.
     </para>
    </note>
   </step>
   <step>
    <para>
     Correct the bus address in your
     <literal>~/helion/my_cloud/definition/data/nic_mappings.yml</literal>
     file.
    </para>
   </step>
   <step>
    <para>
     Commit your changes to git:
    </para>
<screen>cd ~/helion/hos/ansible
git add -A
git commit -m "My config or other commit message"</screen>
   </step>
   <step>
    <para>
     Re-run the <literal>bm-reimage.yml</literal> playbook and confirm the
     error is not received again.
    </para>
   </step>
  </procedure>
  </section>
 <section xml:id="sec.trouble-config_processor">
  <title>Issues while Updating Configuration Files</title>
  <bridgehead renderas="sect5">Configuration Processor Fails Due to Wrong yml Format</bridgehead>
  <para>
   If you receive the error below when running the configuration processor then
   you may have a formatting error:
  </para>
<screen>
<?dbsuse-fo font-size="0.70em"?>
TASK: [fail msg="Configuration processor run failed, see log output above for details"]</screen>
  <para>
   First you should check the ansible log in the location below for more
   details on which yml file in your input model has the error:
  </para>
<screen>~/.ansible/ansible.log</screen>
  <para>
   Check the configuration file to locate and fix the error, keeping in mind
   the following tips below.
  </para>
  <para>
   Check your files to ensure that they don't contain the following:
  </para>
  <itemizedlist>
   <listitem>
    <para>
     Non-ascii characters
    </para>
   </listitem>
   <listitem>
    <para>
     Unneeded spaces
    </para>
   </listitem>
  </itemizedlist>
  <para>
   Once you have fixed the formatting error in your files, commit the changes
   with these steps:
  </para>
  <procedure>
   <step>
    <para>
     Commit your changes to git:
    </para>
<screen>cd ~/helion/hos/ansible
git add -A
git commit -m "My config or other commit message"</screen>
   </step>
   <step>
    <para>
     Re-run the configuration processor playbook and confirm the error is not
     received again.
    </para>
   </step>
  </procedure>
  <bridgehead renderas="sect5">Configuration processor fails with provider network OCTAVIA-MGMT-NET error</bridgehead>
  <para>
   If you receive the error below when running the configuration processor then
   you have not correctly configured your VLAN settings for Octavia.
  </para>
<screen>
<?dbsuse-fo font-size="0.70em"?>
"################################################################################",
"# The configuration processor failed.  ",
"#   config-data-2.0           ERR: Provider network OCTAVIA-MGMT-NET host_routes:
"# destination '192.168.10.0/24' is not defined as a Network in the input model.
""# Add 'external: True' to this host_route if this is for an external network.",
"################################################################################"</screen>
  <para>
   To resolve the issue, ensure that your settings in
   <literal>~/helion/my_cloud/definition/data/neutron/neutron_config.yml</literal>
   are correct for the VLAN setup for Octavia.
  </para>
  <bridgehead renderas="sect5">Changes Made to your Configuration Files</bridgehead>
  <para>
   If you have made corrections to your configuration files and need to re-run
   the Configuration Processor, the only thing you need to do is commit your
   changes to your local git:
  </para>
<screen>cd ~/helion/hos/ansible
git add -A
git commit -m "commit message"</screen>
  <para>
   You can then re-run the configuration processor:
  </para>
<screen>cd ~/helion/hos/ansible
ansible-playbook -i hosts/localhost config-processor-run.yml</screen>
  <bridgehead renderas="sect5">Configuration Processor Fails Because Encryption Key Does Not Meet Requirements</bridgehead>
  <para>
   If you choose to set an encryption password when running the configuration
   processor, you may receive the following error if the chosen password does
   not meet the complexity requirements:
  </para>
<screen>
<?dbsuse-fo font-size="0.70em"?>
################################################################################
# The configuration processor failed.
#   encryption-key ERR: The Encryption Key does not meet the following requirement(s):
#       The Encryption Key must be at least 12 characters
#       The Encryption Key must contain at least 3 of following classes of characters:
#                           Uppercase Letters, Lowercase Letters, Digits, Punctuation
################################################################################</screen>
  <para>
   If you receive the above error, simply run the configuration processor again
   and select a password that meets the complexity requirements detailed in the
   error message:
  </para>
<screen>cd ~/helion/hos/ansible
ansible-playbook -i hosts/localhost config-processor-run.yml</screen>
 </section>
 <section xml:id="sec.trouble-deploy_cloud">
  <title>Issues while Deploying the Cloud</title>
  <bridgehead renderas="sect5">Issue: If the site.yml playbook fails, you can query the log for the reason</bridgehead>
  <para>
   Ansible is good about outputting the errors into the command line output,
   however if you'd like to view the full log for any reason the location is:
  </para>
<screen>~/.ansible/ansible.log</screen>
  <para>
   This log is updated real time as you run ansible playbooks.
  </para>
  <tip>
   <para>
    Use grep to parse through the log. Usage: <literal>grep &lt;text&gt;
    ~/.ansible/ansible.log</literal>
   </para>
  </tip>
  <bridgehead renderas="sect5">Issue: How to Wipe the Disks of your Machines</bridgehead>
  <para>
   If you have re-run the <literal>site.yml</literal> playbook, you may need to
   wipe the disks of your nodes
  </para>
  <para>
   You would generally run the playbook below after re-running the
   <literal>bm-reimage.yml</literal> playbook but before you re-run the
   <literal>site.yml</literal> playbook.
  </para>
<screen>cd ~/scratch/ansible/next/hos/ansible
ansible-playbook -i hosts/verb_hosts wipe_disks.yml</screen>
  <para>
   The playbook will show you the disks to be wiped in the output and allow you
   to confirm that you want to complete this action or abort it if you do not
   want to proceed. You can optionally use the <literal>--limit
   &lt;NODE_NAME&gt;</literal> switch on this playbook to restrict it to
   specific nodes.
  </para>
  <para>
   If you receive an error stating that <literal>osconfig</literal> has already
   run on your nodes then you will need to remove the
   <literal>/etc/hos/osconfig-ran</literal> file on each of the nodes you want
   to wipe with this command:
  </para>
<screen>sudo rm /etc/hos/osconfig-ran</screen>
  <para>
   That will clear this flag and allow the disk to be wiped.
  </para>
  <bridgehead renderas="sect5">Issue: Errors during create_db Task</bridgehead>
  <para>
   If you are doing a fresh installation on top of a previously used system and
   you have not completely wiped your disk prior to the installation, you may
   run into issues when the installation attempts to create new Vertica
   databases. The recommendation here is to wipe your disks and re-start the
   installation, however we have included a playbook for cleaning your Vertica
   databases if they need to be re-used.
  </para>
  <para>
   To run this playbook, follow these steps:
  </para>
<screen>cd ~/scratch/ansible/next/hos/ansible
ansible-playbook -i hosts/verb_hosts monasca-vertica-dbclean.yml</screen>
  <bridgehead renderas="sect5">Issue: Freezer installation fails if an independent network is used for the External_API</bridgehead>
  <para>
   Currently the Freezer installation fails if an independent network is used
   for the External_API. If you intend to deploy the External API
   on an independent network, the following changes need to be made:
  </para>
  <para>
   In <literal>roles/freezer-agent/defaults/main.yml</literal> add the
   following line:
  </para>
<screen>
backup_freezer_api_url: "{{ FRE_API | item('advertises.vips.private[0].url', \
default=' ') }}"</screen>
  <para>
   In <literal>roles/freezer-agent/templates/backup.osrc.j2</literal> add the
   following line:
  </para>
<screen>export OS_FREEZER_URL={{ backup_freezer_api_url }}</screen>
  <bridgehead renderas="sect5">Error Received if Root Logical Volume is Too Small</bridgehead>
  <para>
   When running the <literal>site.yml</literal> playbook, you may receive the
   error below if your root logical-volume is too small:
  </para>
<screen>
<?dbsuse-fo font-size="0.65em"?>
2015-09-29 15:54:02,751 p=26345 u=stack |  TASK: [osconfig | disk config | Extend root LV] ********
2015-09-29 15:54:03,021 p=26345 u=stack |  failed: [helion-ccp-swpac-m1-mgmt] =&gt;
  (item=({'physical_volumes': ['/dev/sda_root'], 'consumer': {'name': 'os'},
  'name': 'hlm-vg'}, {'mount': '/', 'fstype': 'ext4', 'name': 'root', 'size': '10%'})) =&gt;
  {"changed": true, "cmd": ["lvextend", "-l", "10%VG", "/dev/hlm-vg/root"],
  "delta": "0:00:00.022983", "end": "2015-09-29 10:54:18.925855", "failed": true,
  "failed_when_result": true, "item": [{"consumer": {"name":"os"}, "name": "hlm-vg",
  "physical_volumes": ["/dev/sda_root"]}, {"fstype": "ext4", "mount": "/",
  "name": "root", "size": "10%"}], "rc": 3, "start": "2015-09-29 10:54:18.902872",
  "stdout_lines": [], "warnings": []}
2015-09-29 15:54:03,022 p=26345 u=stack |  stderr:   New size given (7128 extents)
  not larger than existing size (7629 extents)</screen>
  <para>
   The specific part of this error to parse out and resolve is:
  </para>
<screen>stderr:   New size given (7128 extents) not larger than existing size (7629 extents)</screen>
  <para>
   The error also references the root volume:
  </para>
<screen>"name": "root", "size": "10%"</screen>
  <para>
   The problem is that the root logical-volume, as specified in the
   <literal>disks_controller.yml</literal> file, is set to
   <literal>10%</literal> of the overall physical volume and this value is too
   small.
  </para>
  <para>
   To resolve this issue you need to ensure that the percentage is set properly
   for the size of your logical-volume. The default values in the configuration
   files is based on a 500GB disk, so if your logical-volumes are smaller you
   may need to increase the percentage so there is enough room.
  </para>
  <bridgehead renderas="sect5">Multiple Keystone Failures Received during site.yml</bridgehead>
  <para>
   If you receive the Keystone error below during your
   <literal>site.yml</literal> run then follow these steps:
  </para>
<screen>
<?dbsuse-fo font-size="0.70em"?>
TASK: [OPS-MON | _keystone_conf | Create Ops Console service in Keystone] *****
failed: [helion-cp1-c1-m1-mgmt] =&gt; {"failed": true}
msg: An unexpected error prevented the server from fulfilling your request.
(HTTP 500) (Request-ID: req-23a09c72-5991-4685-b09f-df242028d742), failed

FATAL: all hosts have already failed -- aborting</screen>
  <para>
   The most likely cause of this error is that the virtual IP address is having
   issues and the Keystone API communication through the virtual IP address is
   not working properly. You will want to check the Keystone log on the
   controller where you will likely see authorization failure errors.
  </para>
  <para>
   Verify that your virtual IP address is active and listening on the proper
   port on all of your controllers using this command:
  </para>
<screen>netstat -tplan | grep 35357</screen>
  <para>
   Ensure that your &lcm; did not pick the wrong (unusable) IP
   address from the list of IP addresses assigned to your Management network.
  </para>
  <para>
   The &lcm; will take the first available IP address after the
   <literal>gateway-ip</literal> defined in your
   <filename>~/helion/my_cloud/definition/data/networks.yml</filename> file. This
   IP will be used as the virtual IP address for that particular network. If
   this IP address is used and reserved for another purpose outside of your
   &kw-hos; deployment then you will receive the error above.
  </para>
  <para>
   To resolve this issue we recommend that you utilize the
   <literal>start-address</literal> and possibly the
   <literal>end-address</literal> (if needed) options in your
   <filename>networks.yml</filename> file to further define which IP addresses
   you want your cloud deployment to use. For more information, see
   <xref linkend="configobj_networks"/>.
  </para>
  <para>
   After you have made changes to your <filename>networks.yml</filename> file,
   follow these steps to commit the changes:
  </para>
  <procedure>
   <step>
    <para>
     Ensuring that you stay within the <filename>~/helion</filename> directory,
     commit the changes you just made:
    </para>
<screen>cd ~/helion
git commit -a -m "commit message"</screen>
   </step>
   <step>
    <para>
     Run the configuration processor:
    </para>
<screen>cd ~/helion/hos/ansible
ansible-playbook -i hosts/localhost config-processor-run.yml</screen>
   </step>
   <step>
    <para>
     Update your deployment directory:
    </para>
<screen>cd ~/helion/hos/ansible
ansible-playbook -i hosts/localhost ready-deployment.yml</screen>
   </step>
   <step>
    <para>
     Re-run the site.yml playbook:
    </para>
<screen>cd ~/scratch/ansible/next/hos/ansible
ansible-playbook -i hosts/verb_hosts site.yml</screen>
   </step>
  </procedure>
  <bridgehead renderas="sect5">VSA installer failed during deployment</bridgehead>
  <para>
   VSA deployment might fails sporadically due to a defect in the &storevirtual;
   VSA installation files for the KVM host. The following error
   messages is displayed and VSA installation will hang for more than 45-60
   minutes:
  </para>
<screen>
<?dbsuse-fo font-size="0.70em"?>
14:10:13 TASK: [VSA-DEP | deploy | Create VSA appliance] ***********************
14:18:32 changed: [hlm003-cp1-vsa0003-mgmt]
15:32:36 fatal: [hlm003-cp1-vsa0002-mgmt] =&gt;
         SSH Error: Shared connection to 10.240.20.200 closed.
15:32:36 It is sometimes useful to re-run the command using -vvvv, which prints
         SSH debug output to help diagnose the issue.
15:32:36 fatal: [hlm003-cp1-vsa0001-mgmt] =&gt; SSH Error: Shared connection to
         10.240.20.199 closed.
15:32:36 It is sometimes useful to re-run the command using -vvvv, which prints
         SSH debug output to help diagnose the issue.
15:32:37 </screen>
  <note>
   <para>
    VSA deployment can take approximately usually 15-30 minutes, depending on
    how many disks are configured. Do not terminate the installation until 60
    minutes have passed and/or you see the error.
   </para>
  </note>
  <para>
   To correct this issue:
  </para>
  <procedure>
   <step>
    <para>
     Log into the failed VSA node.
    </para>
   </step>
   <step>
    <para>
     Change to the root user:
    </para>
<screen>sudo -i</screen>
   </step>
   <step>
    <para>
     Destroy the VSA VM:
    </para>
<screen>virsh destroy &lt;VSA_VM_Name&gt;
virsh undefine &lt;VSA_VM_Name&gt;</screen>
    <para>
     For example:
    </para>
<screen>virsh destroy VSA-VM-vsa2
virsh undefine VSA-VM-vsa2</screen>
    <para>
     To get the name of the VSA VM, execute the following command:
    </para>
<screen>virsh list --all
Id    Name                           State
----------------------------------------------------
 2    VSA-VM-vsa2                    running</screen>
   </step>
   <step>
    <para>
     Destroy the VSA network:
    </para>
<screen>virsh net-destroy vsa-network
virsh net-undefine vsa-network</screen>
   </step>
   <step>
    <para>
     Destroy the VSA storage-pool:
    </para>
<screen>virsh pool-destroy vsa-storage-pool
virsh pool-undefine vsa-storage-pool</screen>
   </step>
   <step>
    <para>
     Remove the <literal>vsa-installer</literal> directory:
    </para>
<screen>rm -rf /home/vsa-installer</screen>
   </step>
   <step>
    <para>
     Remove the VSA image:
    </para>
<screen>rm -rf /mnt/state/vsa-kvm-storage</screen>
   </step>
   <step>
    <para>
     Reboot the VSA VM:
    </para>
<screen>reboot</screen>
   </step>
   <step>
    <para>
     Log into your &lcm;.
    </para>
   </step>
   <step>
    <para>
     Run the deployment playbooks from the resulting scratch directory:
    </para>
<screen>cd ~/scratch/ansible/next/hos/ansible
ansible-playbook -i hosts/verb_hosts site.yml</screen>
   </step>
  </procedure>
  </section>
</chapter>
