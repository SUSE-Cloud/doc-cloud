<?xml version="1.0"?>
<!DOCTYPE chapter [
 <!ENTITY % entities SYSTEM "entities.ent"> %entities;
]>
<chapter xml:id="troubleshooting_installation"
 xmlns="http://docbook.org/ns/docbook" version="5.1"
 xmlns:xi="http://www.w3.org/2001/XInclude"
 xmlns:xlink="http://www.w3.org/1999/xlink">
 <title>Troubleshooting the Installation</title>
 <para>
  We have gathered some of the common issues that occur during installation and
  organized them by when they occur during the installation. These sections
  will coincide with the steps labeled in the installation instructions.
 </para>
 <itemizedlist>
  <listitem>
   <para>
     <xref linkend="sec.trouble-deployer_setup"/>
   </para>
  </listitem>
  <listitem>
   <para>
     <xref linkend="sec.trouble-config_processor"/>
   </para>
  </listitem>
  <listitem>
   <para>
    <xref linkend="sec.trouble-deploy_cloud"/>
   </para>
  </listitem>
 </itemizedlist>
 <section xml:id="sec.trouble-deployer_setup">
  <title>Issues during &lcm; Setup</title>
  <bridgehead renderas="sect5">Issue: Running the ardana-init.bash script when configuring your &lcm; does not complete</bridgehead>
  <para>
   Part of what the <literal>ardana-init.bash</literal> script does is
   install Git. So if your DNS server(s) is/are not specified in your
   <filename>/etc/resolv.conf</filename> file, is not valid, or is not
   functioning properly on your &lcm;, it will not be able to
   complete.
  </para>
  <para>
   To resolve this issue, double check your nameserver in your
   <filename>/etc/resolv.conf</filename> file and then re-run the script.
  </para>
 </section>
 <section xml:id="sec.trouble-config_processor">
  <title>Issues while Updating Configuration Files</title>
  <bridgehead renderas="sect5">Configuration Processor Fails Due to Wrong yml Format</bridgehead>
  <para>
   If you receive the error below when running the configuration processor then
   you may have a formatting error:
  </para>
<screen>
TASK: [fail msg="Configuration processor run failed, see log output above for
details"]</screen>
  <para>
   First you should check the Ansible log in the location below for more
   details on which yml file in your input model has the error:
  </para>
<screen>~/.ansible/ansible.log</screen>
  <para>
   Check the configuration file to locate and fix the error, keeping in mind
   the following tips below.
  </para>
  <para>
   Check your files to ensure that they don't contain the following:
  </para>
  <itemizedlist>
   <listitem>
    <para>
     Non-ascii characters
    </para>
   </listitem>
   <listitem>
    <para>
     Unneeded spaces
    </para>
   </listitem>
  </itemizedlist>
  <para>
   Once you have fixed the formatting error in your files, commit the changes
   with these steps:
  </para>
  <procedure>
   <step>
    <para>
     Commit your changes to Git:
    </para>
<screen>cd ~/openstack/ardana/ansible
git add -A
git commit -m "My config or other commit message"</screen>
   </step>
   <step>
    <para>
     Re-run the configuration processor playbook and confirm the error is not
     received again.
    </para>
   </step>
  </procedure>
  <bridgehead renderas="sect5">Configuration processor fails with provider network OCTAVIA-MGMT-NET error</bridgehead>
  <para>
   If you receive the error below when running the configuration processor then
   you have not correctly configured your VLAN settings for Octavia.
  </para>
<screen>
<?dbsuse-fo font-size="0.70em"?>
"################################################################################",
"# The configuration processor failed.  ",
"#   config-data-2.0           ERR: Provider network OCTAVIA-MGMT-NET host_routes:
"# destination '192.168.10.0/24' is not defined as a Network in the input model.
""# Add 'external: True' to this host_route if this is for an external network.",
"################################################################################"</screen>
  <para>
   To resolve the issue, ensure that your settings in
   <literal>~/openstack/my_cloud/definition/data/neutron/neutron_config.yml</literal>
   are correct for the VLAN setup for Octavia.
  </para>
  <bridgehead renderas="sect5">Changes Made to your Configuration Files</bridgehead>
  <para>
   If you have made corrections to your configuration files and need to re-run
   the Configuration Processor, the only thing you need to do is commit your
   changes to your local Git repository:
  </para>
<screen>cd ~/openstack/ardana/ansible
git add -A
git commit -m "commit message"</screen>
  <para>
   You can then re-run the configuration processor:
  </para>
<screen>cd ~/openstack/ardana/ansible
ansible-playbook -i hosts/localhost config-processor-run.yml</screen>
  <bridgehead renderas="sect5">Configuration Processor Fails Because Encryption Key Does Not Meet Requirements</bridgehead>
  <para>
   If you choose to set an encryption password when running the configuration
   processor, you may receive the following error if the chosen password does
   not meet the complexity requirements:
  </para>
<screen>
<?dbsuse-fo font-size="0.70em"?>
################################################################################
# The configuration processor failed.
#   encryption-key ERR: The Encryption Key does not meet the following requirement(s):
#       The Encryption Key must be at least 12 characters
#       The Encryption Key must contain at least 3 of following classes of characters:
#                           Uppercase Letters, Lowercase Letters, Digits, Punctuation
################################################################################</screen>
  <para>
   If you receive the above error, simply run the configuration processor again
   and select a password that meets the complexity requirements detailed in the
   error message:
  </para>
<screen>cd ~/openstack/ardana/ansible
ansible-playbook -i hosts/localhost config-processor-run.yml</screen>
 </section>
 <section xml:id="sec.trouble-deploy_cloud">
  <title>Issues while Deploying the Cloud</title>
  <bridgehead renderas="sect5">Issue: If the site.yml playbook fails, you can query the log for the reason</bridgehead>
  <para>
   Ansible is good about outputting the errors into the command line output,
   however if you'd like to view the full log for any reason the location is:
  </para>
<screen>~/.ansible/ansible.log</screen>
  <para>
   This log is updated real time as you run Ansible playbooks.
  </para>
  <tip>
   <para>
    Use grep to parse through the log. Usage: <literal>grep &lt;text&gt;
    ~/.ansible/ansible.log</literal>
   </para>
  </tip>
  <bridgehead renderas="sect5">Issue: How to Wipe the Disks of your Machines</bridgehead>
  <para>
   If you have re-run the <literal>site.yml</literal> playbook, you may need to
   wipe the disks of your nodes
  </para>
  <para>
   You would generally run the playbook below after re-running the
   <literal>bm-reimage.yml</literal> playbook but before you re-run the
   <literal>site.yml</literal> playbook.
  </para>
<screen>cd ~/scratch/ansible/next/ardana/ansible
ansible-playbook -i hosts/verb_hosts wipe_disks.yml</screen>
  <para>
   The playbook will show you the disks to be wiped in the output and allow you
   to confirm that you want to complete this action or abort it if you do not
   want to proceed. You can optionally use the <literal>--limit
   &lt;NODE_NAME&gt;</literal> switch on this playbook to restrict it to
   specific nodes.
  </para>
  <para>
   If you receive an error stating that <literal>osconfig</literal> has already
   run on your nodes then you will need to remove the
   <literal>/etc/ardana/osconfig-ran</literal> file on each of the nodes you want
   to wipe with this command:
  </para>
<screen>sudo rm /etc/ardana/osconfig-ran</screen>
  <para>
   That will clear this flag and allow the disk to be wiped.
  </para>
  <bridgehead renderas="sect5">Issue: Freezer installation fails if an independent network is used for the External_API</bridgehead>
  <para>
   The Freezer installation fails if an independent network is used
   for the External_API. If you intend to deploy the External API
   on an independent network, the following changes need to be made:
  </para>
  <para>
   In <literal>roles/freezer-agent/defaults/main.yml</literal> add the
   following line:
  </para>
<screen><?dbsuse-fo font-size="0.65em"?>
backup_freezer_api_url: "{{ FRE_API | item('advertises.vips.private[0].url', default=' ') }}"</screen>
  <para>
   In <literal>roles/freezer-agent/templates/backup.osrc.j2</literal> add the
   following line:
  </para>
<screen>export OS_FREEZER_URL={{ backup_freezer_api_url }}</screen>
  <bridgehead renderas="sect5">Error Received if Root Logical Volume is Too Small</bridgehead>
  <para>
   When running the <literal>site.yml</literal> playbook, you may receive a
   message that includes the error below if your root logical volume is too
   small. This error needs to be parsed out and resolved.
  </para>
<screen>
2015-09-29 15:54:03,022 p=26345 u=stack | stderr: New size given (7128 extents)
not larger than existing size (7629 extents)</screen>
  <para>
   The error message may also reference the root volume:
  </para>
<screen>"name": "root", "size": "10%"</screen>
  <para>
   The problem here is that the root logical volume, as specified in the
   <literal>disks_controller.yml</literal> file, is set to
   <literal>10%</literal> of the overall physical volume and this value is too
   small.
  </para>
  <para>
   To resolve this issue you need to ensure that the percentage is set properly
   for the size of your logical-volume. The default values in the configuration
   files is based on a 500Â GB disk, so if your logical volumes are smaller you
   may need to increase the percentage so there is enough room.
  </para>
  <bridgehead renderas="sect5">Multiple Keystone Failures Received during site.yml</bridgehead>
  <para>
   If you receive the Keystone error below during your
   <literal>site.yml</literal> run then follow these steps:
  </para>
<screen>
<?dbsuse-fo font-size="0.70em"?>
TASK: [OPS-MON | _keystone_conf | Create Ops Console service in Keystone] *****
failed:
[...]
msg: An unexpected error prevented the server from fulfilling your request.
(HTTP 500) (Request-ID: req-23a09c72-5991-4685-b09f-df242028d742), failed

FATAL: all hosts have already failed -- aborting</screen>
  <para>
   The most likely cause of this error is that the virtual IP address is having
   issues and the Keystone API communication through the virtual IP address is
   not working properly. You will want to check the Keystone log on the
   controller where you will likely see authorization failure errors.
  </para>
  <para>
   Verify that your virtual IP address is active and listening on the proper
   port on all of your controllers using this command:
  </para>
<screen>netstat -tplan | grep 35357</screen>
  <para>
   Ensure that your &lcm; did not pick the wrong (unusable) IP
   address from the list of IP addresses assigned to your Management network.
  </para>
  <para>
   The &lcm; will take the first available IP address after the
   <literal>gateway-ip</literal> defined in your
   <filename>~/openstack/my_cloud/definition/data/networks.yml</filename> file.
   This IP will be used as the virtual IP address for that particular network.
   If this IP address is used and reserved for another purpose outside of your
   &kw-hos; deployment then you will receive the error above.
  </para>
  <para>
   To resolve this issue we recommend that you utilize the
   <literal>start-address</literal> and possibly the
   <literal>end-address</literal> (if needed) options in your
   <filename>networks.yml</filename> file to further define which IP addresses
   you want your cloud deployment to use. For more information, see
   <xref linkend="configobj_networks"/>.
  </para>
  <para>
   After you have made changes to your <filename>networks.yml</filename> file,
   follow these steps to commit the changes:
  </para>
  <procedure>
   <step>
    <para>
     Ensuring that you stay within the <filename>~/openstack</filename> directory,
     commit the changes you just made:
    </para>
<screen>cd ~/openstack
git commit -a -m "commit message"</screen>
   </step>
   <step>
    <para>
     Run the configuration processor:
    </para>
<screen>cd ~/openstack/ardana/ansible
ansible-playbook -i hosts/localhost config-processor-run.yml</screen>
   </step>
   <step>
    <para>
     Update your deployment directory:
    </para>
<screen>cd ~/openstack/ardana/ansible
ansible-playbook -i hosts/localhost ready-deployment.yml</screen>
   </step>
   <step>
    <para>
     Re-run the site.yml playbook:
    </para>
<screen>cd ~/scratch/ansible/next/ardana/ansible
ansible-playbook -i hosts/verb_hosts site.yml</screen>
   </step>
  </procedure>
  </section>
</chapter>
