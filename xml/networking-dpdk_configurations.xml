<?xml version="1.0"?>
<?xml-stylesheet href="urn:x-suse:xslt:profiling:docbook51-profile.xsl"
 type="text/xml"
 title="Profiling step"?>
<!DOCTYPE section [
 <!ENTITY % entities SYSTEM "entity-decl.ent"> %entities;
]>
<section xmlns="http://docbook.org/ns/docbook" xmlns:xi="http://www.w3.org/2001/XInclude" xmlns:xlink="http://www.w3.org/1999/xlink" xml:id="dpdk_config">
 <title>DPDK Configurations</title>
 <itemizedlist>
  <listitem>
   <para>
    <xref linkend="base_config"/>
   </para>
  </listitem>
<!-- Commented in DITA original-->
<!--<li><xref href="#dpdk_config/mellanox" format="dita">Mellanox changes to
base configuration</xref></li>-->
  <listitem>
   <para>
    <xref linkend="common_perf"/>
   </para>
  </listitem>
<!-- Commented in DITA original-->
<!--<li><xref href="#dpdk_config/mellanox_perf" format="dita">Performance
considerations for Mellanox ConnectX - 3 Pro
</xref><ul> <li><xref href="#dpdk_config/steering" format="dita">Optimized
steering mode</xref></li> </ul></li>-->
  <listitem>
   <para>
    <xref linkend="multiqueue_config"/>
   </para>
  </listitem>
 </itemizedlist>
 <section xml:id="base_config">
  <title>Base configuration</title>
  <para>
   The following is specific to DL360 Gen9 and BIOS configuration as detailed
   in <xref linkend="dpdk_setup"/>.
  </para>
  <itemizedlist>
   <listitem>
    <para>
     EAL cores - 1, isolate: False in cpu-models
    </para>
   </listitem>
   <listitem>
    <para>
     PMD cores - 1 per NIC port
    </para>
   </listitem>
   <listitem>
    <para>
     Hugepages - 1G per PMD thread
    </para>
   </listitem>
   <listitem>
    <para>
     Memory channels - 4
    </para>
   </listitem>
   <listitem>
    <para>
     Global rx queues - based on needs
    </para>
   </listitem>
  </itemizedlist>
 </section>
<!-- Commented in DITA original -->
<!--
<section id="mellanox">
 <title>Mellanox changes to base configuration</title>
 <ul>
   <li>NOTE: EAL only needs one core</li>
   <li outputclass="highlightThis">Global rx queues: even though the spec says this must be a
     power of 2, there may be problems using 1 or 2. Note that the default behavior in OVS when
     adding a DPDK physical port is to attempt to set up 1 tx queue for each core detected on
     the system plus one more queue for non PMD threads. On a 24-core DL360 Gen9, when
     hyperthreading is enabled there are 48 logical cores and I'm see 49 queuesrequested but
     only 45 configured.
     <?oxy_custom_end?><codeblock>stack@openstack-cp-comp0001-mgmt:~$ sudo ovs-appctl dpctl/show
netdev@ovs-netdev:
lookups: hit:8282 missed:83 lost:0
flows: 36
port 0: ovs-netdev (internal)
port 1: br-int (tap)
port 5: fg-0d4c9325-47 (tap)
port 9: vlan3511
port 3: br-hed0 (tap)
port 2: br-tun (tap)
port 4: br-dpdk0 (tap)
port 6: br-vlan3511 (tap)
port 7: dpdk0 (dpdk: configured_rx_queues=4, configured_tx_queues=45, requested_tx_queues=49)
port 8: hed0</codeblock>
   </li>
   <li>Hugepages - 4G per PMD thread</li>
 </ul>
</section>
-->
 <section xml:id="common_perf">
  <title>Performance considerations common to all NIC types</title>
  <para>
   <emphasis role="bold">Compute host core frequency</emphasis>
  </para>
  <para>
   Host CPUs should be running at maximum performance. The following is a
   script to set that. Note that in this case there are 24 cores. This needs to
   be modified to fit your environment. For a HP DL360 Gen9, the BIOS should be
   configured to use "OS Control Mode" which can be found on the iLO Power
   Settings page.
  </para>
<screen>for i in `seq 0 23`; do echo "performance" &gt; /sys/devices/system/cpu/cpu$i/cpufreq/scaling_governor; done</screen>
  <para>
   <emphasis role="bold">IO non-posted prefetch</emphasis>
  </para>
  <para>
   The DL360 Gen9 should have the IO non-posted prefetch disabled. Experimental
   evidence shows this yields an additional 6-8% performance boost.
  </para>
 </section>
<!-- Commented in DITA original: -->
<!--
<section id="mellanox_perf"><title>Performance considerations for Mellanox ConnectX - 3
    Pro</title><p><b>Optimized steering mode</b>
  </p><p id="steering">Verify the steering mode is
  -7</p><codeblock>cat /sys/module/mlx4_core/parameters/log_num_mgm_entry_size</codeblock> If
  the result is not '-7' then modify /etc/modprobe.d/mlx4.conf
  <codeblock>options mlx4_core log_num_mgm_entry_size=-7</codeblock>
</section>
-->
 <section xml:id="multiqueue_config">
  <title>Multiqueue configuration</title>
  <para>
   In order to use multiqueue, a property must be applied to the Glance image
   and a setting inside the resulting VM must be applied. In this example we
   create a 4 vCPU flavor for DPDK using 1G hugepages.
  </para>
<screen>MI_NAME=dpdk

openstack aggregate create $MI_NAME nova
openstack aggregate add host $MI_NAME openstack-cp-comp0001-mgmt
openstack aggregate add host $MI_NAME openstack-cp-comp0002-mgmt
openstack aggregate set $MI_NAME pinned=true

openstack flavor create $MI_NAME 6 1024 20 4
openstack flavor set $MI_NAME set hw:cpu_policy=dedicated
openstack flavor set $MI_NAME set aggregate_instance_extra_specs:pinned=true
openstack flavor set $MI_NAME set hw:mem_page_size=1048576</screen>
  <para>
   And set the hw_vif_multiqueue_enabled property on the Glance image
  </para>
<screen>&prompt.ardana;openstack image set --property hw_vif_multiqueue_enabled=true <replaceable>IMAGE UUID</replaceable></screen>
  <para>
   Once the VM is booted using the flavor above, inside the VM, choose the
   number of combined rx and tx queues to be equal to the number of vCPUs
  </para>
<screen>&prompt.sudo;ethtool -L eth0 combined 4</screen>
  <para>
   On the hypervisor you can verify that multiqueue has been properly set by
   looking at the qemu process
  </para>
<screen>-netdev type=vhost-user,id=hostnet0,chardev=charnet0,queues=4 -device virtio-net-pci,mq=on,vectors=10,</screen>
  <para>
   Here you can see that 'mq=on' and vectors=10. The formula for vectors is
   2*num_queues+2
  </para>
 </section>
</section>
