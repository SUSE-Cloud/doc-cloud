<?xml version="1.0"?>
<?xml-stylesheet href="urn:x-suse:xslt:profiling:docbook51-profile.xsl"
 type="text/xml"
 title="Profiling step"?>
<!DOCTYPE chapter [
 <!ENTITY % entities SYSTEM "entity-decl.ent"> %entities;
]>
<chapter xmlns="http://docbook.org/ns/docbook" xmlns:xi="http://www.w3.org/2001/XInclude"
 xmlns:xlink="http://www.w3.org/1999/xlink" xml:id="poc-finalise-env">
 <title>Finalise the Environment</title>

 <!-- FIXME Introduction -->
 
 <section xml:id="ostack-ses-cluster-integration-prep">
  <procedure>
   <step>
    <para>
      Prepare Environment for OpenStack and SES Cluster Integration
    </para>
    <substeps>
     <step>
      <para>
        Using the steps defined in the "Gracefully Shut Down the Cloud"
	exercise, shut down the Cloud deployed on the <literal>deployer01</literal>, <literal>controller01</literal>,
	<literal>compute01</literal>, and <literal>compute02</literal> VMs.
      </para>
     </step>
     <step>
      <para>
        When the cloud is shut down gracefully power off the
	<literal>deployer01</literal>, <literal>controller01</literal>,
	<literal>compute01</literal>, and <literal>compute02</literal> VMs.
      </para>
     </step>
     <step>
      <para>
        Power on the <literal>storage-admin</literal>,
	<literal>storage01</literal>, <literal>storage02</literal> and
	<literal>storage03</literal> VMs to start the SES cluster.
      </para>
     </step>
     <step>
      <para>
        Power on the <literal>deployer02</literal> VM.
      </para>
     </step>
    </substeps>
   </step>
   <step>
    <para>
      Initialize the Installer Environment for the SES Integrated Cloud
    </para>
    <substeps>
     <step>
      <para>
       On the <literal>deployer02</literal> node, while logged in as the root
       user, enter the following command to ensure the node is fully
      updated:</para>
<screen>zypper ref
zypper patch -y</screen>
      <para>
       The node should be fully updated. If prompted to reboot, reboot the node and log back in as the <systemitem class="username">root</systemitem> user.
      </para>
     </step>
     <step>
      <para>
        Enter the following command to switch to the <systemitem
	class="username">ardana</systemitem> user:</para>
	<screen>su – ardana</screen>
	<para>
	 You should now be logged in as the <systemitem class="username">ardana</systemitem> user.
      </para>
     </step>
     <step>
      <para>
       Enter the following command to initialize the installer
      environment:</para>
<screen>ARDANA_INIT_AUTO=1 /usr/bin/ardana-init</screen>
     <para>
      You should see some Ansible playbooks run resulting in green
      <literal>ok=</literal> messages and yellow <literal>changed=</literal>
      messages. (If there are any red failed= messages review the output of the commands to determine the cause.)
      </para>
     </step>
     <step>
      <para>
       On the <literal>deployer02</literal> node, open a web browser and point
       to <literal>http://localhost:9085</literal>. You should see the SUSE OpenStack Cloud Deployer screen.
      </para>
     </step>
    </substeps>
   </step>
   <step>
    <para>
      Select the Deployment Model
    </para>
    <substeps>
     <step>
      <para>
        On the <literal>deployer02</literal> node, while logged in as the
	<systemitem class="username">ardana</systemitem> user, open a terminal
	and enter the following commands to download the new example model onto the deployer node:</para>
<screen>cd /var/lib/ardana/openstack/examples/
scp tux@192.168.200.1:~/course_files/SOC312/models/small-scale-kvm-
proof-of-concept.tar ./</screen>
     </step>
     <step>
      <para>
       Enter the following command to extract the downloaded tarball:</para>
<screen>tar xvf small-scale-kvm-proof-of-concept.tar</screen>
      <para>
       You should have a new directory named
       <filename>small-scale-kvm-proof-of-concept</filename>.
       Enter the following command to remove the downloaded tarball (leaving the
 extracted directory):
      </para>
<screen>rm -f small-scale-kvm-proof-of-concept.tar</screen>
      <para>
       The tarball should be gone.
      </para>
     </step>
     <step>
      <para>
       . On the <guimenuitem>Welcome Screen</guimenuitem> of the SUSE OpenStack
       Cloud Deployer, click <guimenuitem>Next</guimenuitem>.
      </para>
     </step>
     <step>
      <para>
       In the <guimenuitem>Choose and OpenStack Model</guimenuitem> screen,
       select <guimenuitem>Small Scale KVM Proof of Concept</guimenuitem>
       from the list of models. Press <guimenuitem>Next</guimenuitem>.
      </para>
     </step>
     <step>
      <para>
        In the <guimenuitem>Cloud Model to Deploy</guimenuitem> screen, select
	<guimenuitem>Lifecycle Manager Nodes</guimenuitem> from the
	<guimenuitem>Mandatory Components</guimenuitem> list. Enter
	<literal>1</literal> into the <guimenuitem>Edit number of nodes</guimenuitem> text box.
      </para>
     </step>
     <step>
      <para>
       Under <guimenuitem>Mandatory Components</guimenuitem>, select
       <guimenuitem>Controller Nodes</guimenuitem>. Enter <literal>1</literal>
       into the <guimenuitem>Edit number of nodes</guimenuitem> text
       box. Select <guimenuitem>Compute Nodes</guimenuitem> from
       <guimenuitem>Additional Components</guimenuitem>.
      </para>
     </step>
     <step>
      <para>
       Enter <literal>1</literal> into the <guimenuitem>Edit minimum number of
       nodes</guimenuitem> text box. Press <guimenuitem>Next</guimenuitem>.
      </para>
     </step>
    </substeps>
   </step>
   <step>
    <para>
     Configure Cloud settings (Cloud DNS, Hostnames, NTP)
    </para>
    <substeps>
     <step>
      <para>
        In the <guimenuitem>Add Servers and Assign Server Roles</guimenuitem>
	screen, press <guimenuitem>Manage Cloud Settings</guimenuitem>.
      </para>
     </step>
     <step>
      <para>
       Switch
	to the <guimenuitem>Cloud Configuration</guimenuitem> tab. Configure
	the settings as follows:
      </para>
      <itemizedlist>
       <listitem>
	<para>
	 <guimenuitem>Host Name Prefix:</guimenuitem> <literal>ardana</literal>
	</para>
       </listitem>
       <listitem>
	<para>
	 <guimenuitem>Host Member Prefix:</guimenuitem> <literal>-m</literal>
	</para>
       </listitem>
       <listitem>
	<para>
	 <guimenuitem>DNS Name Servers:</guimenuitem> <literal>192.168.200.1</literal>
	</para>
       </listitem>
       <listitem>
	<para>
	 <guimenuitem>NTP Servers:</guimenuitem> <literal>192.168.200.1</literal>
	</para>
</listitem>
      </itemizedlist>
      <para>
       Press <guimenuitem>Save</guimenuitem>
      </para>
     </step>
     <step>
      <para>
        Close the <guimenuitem>Manage Cloud Settings</guimenuitem> window.
      </para>
     </step>
    </substeps>
   </step>
   <step>
    <para>
      Manage Cloud Settings (Disk Models)
    </para>
    <substeps>
     <step>
      <para>
        In the <guimenuitem>Server and Server Role Summary</guimenuitem>
	screen, press <guimenuitem>Manage Cloud Settings</guimenuitem>.
      </para>
     </step>
     <step>
      <para>
       In the <guimenuitem>Manage Cloud Settings</guimenuitem> window, switch
       to the <guimenuitem>Disk Models</guimenuitem> tab. You should see the
      following disk models listed:</para>
<screen>COMPUTE-DISKS
CONTROLLER-1TB-DISKS
CONTROLLER-600GB-DISKS
LIFECYCLE-MANAGER-DISKS</screen>
      <para>If you do not see the <literal>LIFECYCLE-MANAGER-DISKS</literal>
      listed, continue with the next steps. If you do see these interface
      models, review each model configuration and make sure that it matches the
      configuration specified here.</para>
     </step>
     <step>
      <para>
        Press <guimenuitem>Add disk model</guimenuitem>. Enter
	<literal>LIFECYCLE-MANAGER-DISKS</literal> in the <guimenuitem>Disk
	model name</guimenuitem> field.
      </para>
     </step>
      <step>
       <para>
	Press <guimenuitem>Add volume group</guimenuitem>. Under
       <guimenuitem>Add volume group</guimenuitem>, enter the following:</para>
<screen>Volume group name: ardana-vg
Physical volume: /dev/sda_root</screen>
      </step>
      <step>
       <para>
	To add a new logical volume, press <guimenuitem>Add logical
	volume</guimenuitem>. Under <guimenuitem>Add logical
	volume</guimenuitem>, enter the following:
       </para>
<screen>Logical volume name: root
Size: 80
Mount: /
File system type: ext4
mkfs command options: (leave empty)</screen>
       <para>
	Press <guimenuitem>Save</guimenuitem>. You should see the new logical
	volume listed under <guimenuitem>Logical volumen</guimenuitem> in
	<guimenuitem>Add volume group</guimenuitem>.
       </para>
      </step>
      <step>
       <para>
	 To add another logical volume, press <guimenuitem>Add logical
	 volume</guimenuitem>. Under <guimenuitem>Add logical volume</guimenuitem>, enter the following:</para>
<screen>Logical volume name: crash
Size: 15
Mount: /var/crash
File system type: ext4
mkfs command options: -O large_file</screen>
       <para>
	Press <guimenuitem>Save</guimenuitem>. You should see the new logical
	volume listed under <guimenuitem>Logical volumen</guimenuitem> in
	<guimenuitem>Add volume group</guimenuitem>.
       </para>
      </step>
      <step>
       <para>
	Save the volume group and disk model configurations.
       </para>
      </step>
    </substeps>
   </step>
       <step>
     <para>
       Manage Cloud Settings (NIC Mappings)
     </para>
     <substeps>
      <step>
       <para>
	 On the <guimenuitem>Server and Server Role Summary</guimenuitem>
	 screen, click <guimenuitem>Manage Cloud Settings</guimenuitem>.
      </para>
      </step>
      <step>
       <para>
	 In the <guimenuitem>Manage Cloud Settings</guimenuitem> window, switch
	 to the <guimenuitem>NIC mappings</guimenuitem> section. You should see
       the following NIC mappings listed:</para>
<screen>COMPUTE-6PORT
CONTROLLER-6PORT
LIFECYCLE-MANAGER-2PORT</screen>
	 <para>If you do not see any of these listed, continue with the
	 procedure. If you do see these listed, review each NIC mapping
	 configuration and make sure that it matches the configuration specified here.
       </para>
      </step>
      <step>
       <para>
	 In the <guimenuitem>Manage Cloud Settings – NIC Mapping</guimenuitem>
	 window, click <guimenuitem>Add NIC Mapping</guimenuitem>. Enter the
	 following to create a NIC mapping for the controller nodes:
       </para>
<screen>NIC Mapping Name: CONTROLLER-6PORT
Port Logical Name: eth0
PCI Address: 0000:00:03.0</screen>
       <para>
	Press + next to the <literal>port/PCI address</literal> line to add a
	line. Enter the following:
       </para>
<screen>Port Logical Name: eth1
PCI Address: 0000:00:07.0</screen>
       <para>
	Press + next again, and enter the following on the created line:
       </para>
<screen>Port Logical Name: eth2
PCI Address: 0000:00:08.0</screen>
       <para>
	Press + next again, and enter the following on the created line:
       </para>
<screen>Port Logical Name: eth3
PCI Address: 0000:00:10.0</screen>
       <para>
	Press + next again, and enter the following on the created line:
       </para>
<screen>Port Logical Name: eth4
PCI Address: 0000:00:11.0</screen>
       <para>
	Press + next again, and enter the following on the created line:
       </para>
<screen>Port Logical Name: eth5
PCI Address: 0000:00:12.0</screen>
      </step>
      <step>
       <para>
	Press <guimenuitem>Save</guimenuitem> to save the changes.
       </para>
      </step>
     </substeps>
       </step>
       <step>
	<para>
	  Configure Compute NIC Mappings
	</para>
	<substeps>
	 <step>
	  <para>
	    Repeat the previous step to create a new NIC mapping for the
	    compute nodes Use <literal>COMPUTE-6PORT</literal> as the name. Use
	    the same port names and PCI addresses as the controllers.
	  </para>
	 </step>
	</substeps>
       </step>
       <step>
	  <para>
	    Configure Lifecycle Manager NIC Mappings
	  </para>
	  <substeps>
	   <step>
	    <para>
	      In the <guimenuitem>Manage Cloud Settings – NIC
	      Mapping</guimenuitem> window, use <guimenuitem>Add NIC
	      Mapping</guimenuitem> to create a NIC mapping for the controller
	    nodes with the following settings:</para>
<screen>NIC Mapping Name: LIFECYCLE-MANAGER-1PORT
Port Logical Name: eth0
PCI Address: 0000:00:03.0</screen>
           <para>Click + next to the port/PCI address line to add a line, and
	   add the following:</para>
<screen>Port Logical Name: eth1
PCI Address: 0000:00:07.0</screen>
	   </step>
	   <step>
	    <para>
	     Press <guimenuitem>Save</guimenuitem> to save the changes.
	    </para>
	   </step>
	  </substeps>
	 </step>
  </procedure>



  <para>
    Use content from Integrate OpenStack with SUSE Enterprise Storage section (Exercise 1)

    Link to SES integration chapter
    Basic config
    Resources
  </para>
 </section>

 <section xml:id="starter-config">
  <title>Starter configuration</title>
  <para>
    External networking
    Image
    Routers
    Sample tenant network
    Security groups
  </para>
 </section>
</chapter>
