<?xml version="1.0" encoding="UTF-8"?>
<?xml-stylesheet href="urn:x-suse:xslt:profiling:docbook51-profile.xsl"
 type="text/xml"
 title="Profiling step"?>
<!DOCTYPE book
[
  <!ENTITY % entities SYSTEM "entity-decl.ent">
  %entities;
]>
<book xmlns="http://docbook.org/ns/docbook" xmlns:xi="http://www.w3.org/2001/XInclude" xmlns:xlink="http://www.w3.org/1999/xlink" version="5.1" xml:lang="en" xml:id="book.supplement">
 <info>
  <title>&cloudsuppl;</title><productname>&productname;</productname>
  <productnumber>&productnumber;</productnumber><date>
<?dbtimestamp format="B d, Y"?></date>
  <xi:include href="copyright_suse_cloud.xml"/>
  <dm:docmanager xmlns:dm="urn:x-suse:ns:docmanager">
   <dm:bugtracker>
    <dm:url>https://bugzilla.suse.com/enter_bug.cgi</dm:url>
    <dm:product>SUSE OpenStack Cloud 7</dm:product>
    <dm:component>Documentation</dm:component>
    <dm:assignee>taroth@suse.com</dm:assignee>
   </dm:bugtracker>
   <dm:maintainer>taroth</dm:maintainer>
   <dm:status>editing</dm:status>
   <dm:deadline/>
   <dm:priority/>
   <dm:translation>no</dm:translation>
   <dm:languages/>
  </dm:docmanager>
 </info>
 <xi:include href="suppl_intro.xml"/>
 <chapter xml:id="cha.suppl.horizon.theme">
<!--https://bugzilla.suse.com/show_bug.cgi?id=917622-->
  <title>Changing the &cloud; &dash; Theme</title>
  <para>
   The &cloud; &dash; theme can now be customized. The default &productname;
   theme is available in the
   <systemitem class="resource">openstack-dashboard-theme-SUSE</systemitem>
   package. If you want to replace it with a custom theme, you can explore the
   package contents as an example. When using a custom theme, make sure the
   resulting package name starts with
   <systemitem class="resource">openstack-dashboard-theme-</systemitem>. Apart
   from that, go to the Horizon &barcl; in &crow;, switch to the
   <guimenu>Raw</guimenu> view and adjust the <guimenu>site_theme</guimenu>
   attribute accordingly. For details about the &barcl;s in &crow;, see the
   &clouddeploy;.
  </para>
 </chapter>
 <chapter xml:id="cha.adm.cli.img">
  <title>Managing Images</title>
  <para>
   <remark>taroth 2017-01-13: for all commands in this chapter, I switched
   from the python clients for individual services to the commands of the unified
   python-openstackclient (like upstream did) - @DEVs: a sanity check would be great
   to make sure I did not miss any changes (in options) etc.</remark>
  </para>
  <para>
   In the &productname; context, images are virtual disk images that represent
   the contents and structure of a storage medium or device, such as a hard
   disk, in a single file. Images are used as a template from which a virtual
   machine can be started. For starting a virtual machine, &productname; always
   uses a copy of the image.
  </para>
  <para>
   Images have both content and metadata. The latter are also called image
   properties.
  </para>
  <para>
   Permissions to manage images are defined by the cloud operator during setup
   of &productname;. Image upload and management may be restricted to cloud
   administrators or cloud operators only.
  </para>
  <para>
   Managing images for &productname; requires the following basic steps:
  </para>
  <procedure>
   <step>
    <para>
     <xref linkend="sec.adm.cli.img.build" xrefstyle="select:title"/>.
     <remark>taroth 2017-01-13: @DEVs: do we still recommend to our customers to
      use SUSE Studio for image building?</remark>
    </para>
    <para>
     For general and hypervisor-specific requirements, refer to
     <xref linkend="sec.adm.cli.img.build.req"/>.
    </para>
   </step>
   <step>
    <para>
     <xref linkend="pro.adm.cli.img.upload" xrefstyle="select:title"/>.
    </para>
    <para>
     Images can either be uploaded to &productname; with the
     unified <command>python-openstackclient</command> from command line or with
     the &productname; &dash;. As the &dash; has some limitations
     regarding image upload and modification of properties, it is recommended
     to use the unified <command>python-openstackclient</command> for comprehensive image
     management.
    </para>
   </step>
   <step>
    <para>
     Specifying Image Properties. You can do so during image upload (using
     <command>openstack&nbsp;image create</command>) or with
     <command>openstack&nbsp;image set</command> after the image has already
     been uploaded. Refer to <xref linkend="pro.adm.cli.img.upload"/> and
     <xref linkend="pro.adm.cli.img.props.modify"/>.
    </para>
    <important>
     <title>Properties for Architecture, Hypervisor and VM Mode</title>
     <para>
      &ostack; &img; does <emphasis>not</emphasis> check nor automatically
      detect any image properties. Therefore you need to specify the
      image&apos;s properties manually.
     </para>
     <para>
      This is especially important when using mixed virtualization environments
      to make sure that an image is only launched on appropriate hypervisors.
      The properties can specify a certain architecture, hypervisor type, or
      application binary interface (ABI) that the image requires.
     </para>
    </important>
   </step>
  </procedure>
  <sect1 xml:id="sec.adm.cli.img.build.req">
   <title>Image Requirements</title>

  <para>
   <remark>taroth 2016-12-28: @DEVs: please review this section and its subsections
   thoroughly as there might have been changes in the meantime that I'm not
   aware of</remark>
  </para>
   <para>
    To build the images to use within the cloud, use &susestudio; or
    &suseonsite; as they provide automatic insertion of management scripts and
    agents. Make sure any images that you build for &productname; fulfill the
    following requirements.
   </para>

   <sect2 xml:id="vl.adm.img.req.general">
    <title>General Image Requirements</title>
    <itemizedlist mark="bullet" spacing="normal">
     <listitem>
      <para>
       The network is set to DHCP.
      </para>
     </listitem>
     <listitem>
      <para>
       The image does not include &yast;2 Firstboot.
      </para>
     </listitem>
     <listitem>
      <para>
       The image does not include any end-user license agreement (EULA)
       dialogs.
      </para>
     </listitem>
     <listitem>
      <para>
       The image contains the
       <systemitem class="resource">cloud-init</systemitem> package. The
       package will be automatically added to the image if the following check
       box in &susestudio; or &suseonsite; is enabled: <guimenu>Integrate with
       &productname;/OpenStack</guimenu>.
      </para>
      <para>
       The <systemitem class="resource">cloud-init</systemitem> package
       contains tools used for communication with the instance metadata API,
       which is provided by &comp;. The metadata API is only accessible from
       inside the VM. The package is needed to pull keypairs into the virtual
       machine that will run the image.
      </para>
     </listitem>
    </itemizedlist>
    <para>
     If you intend to manage the image by the &orch; module, you also need to
     include the following package:
     <systemitem class="resource">openstack-heat-cfntools</systemitem> (part of
     the &productname; ISO).
    </para>
   </sect2>

   <sect2 xml:id="vl.adm.img.req.virt">
    <title>Image Requirements Depending on Hypervisor</title>
    <para>
     For a list of supported VM guests, refer to the &slsreg; &virtual;,
     section <citetitle>Supported VM Guests</citetitle>. It is available at
     <link xlink:href="https://www.suse.com/documentation/sles-12/book_virt/data/virt_support_guests.html"/>.
    </para>
    <para>
     Depending on the virtualization platform on which you want to use the
     image, make sure the image also fulfills the following requirements.
    </para>
    <variablelist>
     <varlistentry>
      <term>&kvm;</term>
      <listitem>
       <para>
        Appliance format: If you are using &susestudio; or &suseonsite; 1.3 to
        build images, use the <literal>&productname;/OpenStack/&kvm;
        (.qcow2)</literal> format.
       </para>
      </listitem>
     </varlistentry>
<!--https://bugzilla.suse.com/show_bug.cgi?id=863876-->
     <varlistentry>
      <term>&xen;</term>
      <listitem>
       <para>
        Appliance format: If you are using &susestudio; or &suseonsite; 1.3 to
        build images, use the <literal>&productname;/OpenStack/&kvm;
        (.qcow2)</literal> format.
       </para>
      </listitem>
     </varlistentry>
     <varlistentry>
      <term>&vmware;</term>
      <listitem>
       <para>
        Appliance format: If you are using &susestudio; or &suseonsite; 1.3 to
        build images, use the <literal>&vmware;/VirtualBox/KVM (.vmdk)</literal>
        format.
       </para>
<!--taroth 2014-02-05: fix for https://bugzilla.suse.com/show_bug.cgi?id=862165#c3,
        as discussed with mjura-->
       <para>
        If you are using &susestudio; or &suseonsite; to build images, the
        resulting image will be a monolithic <literal>sparse</literal> file.
       </para>
       <note>
        <title>Image Conversion</title>
        <para>
         Sparse images can be uploaded to &ostack; &img;. However, it is
         recommended to convert sparse images into a different format before
         uploading them to &ostack; &img; (because starting VMs from sparse
         images may take longer).
        </para>
        <para>
         For a list of supported image types, refer to
         <link xlink:href="https://docs.openstack.org/nova/pike/admin/configuration/hypervisor-vmware.html"/>,
         section <citetitle>Supported image types</citetitle>.
        </para>
        <para>
         For details on how to convert a sparse image into different formats,
         refer to
         <link xlink:href="https://docs.openstack.org/nova/pike/admin/configuration/hypervisor-vmware.html"/>,
         section <citetitle>Optimize images</citetitle>.
        </para>
       </note>
      </listitem>
     </varlistentry>
    </variablelist>
   </sect2>

   <sect2 xml:id="tip.adm.img.req.virt.mult">
<!--https://bugzilla.suse.com/show_bug.cgi?id=836765#22-->
    <title>Images for Use With Multiple Hypervisors</title>
    <para>
     If you build the images for &productname; in &susestudio; or &suseonsite;,
     they are compatible with multiple hypervisors by default&mdash;even if you
     may need to convert the resulting image formats before uploading them to
     &ostack; &img;. See <xref linkend="pro.adm.cli.img.convert"/> for details.
    </para>
    <para>
     If your image is not made in &susestudio; or &suseonsite;, configure the
     image as follows to create an image that can be booted on &kvm; and &xen;,
     for example:
    </para>
    <variablelist>
     <varlistentry>
      <term>/etc/sysconfig/kernel
       </term>
      <listitem>
<screen>INITRD_MODULES="virtio_blk virtio_pci ata_piix ata_generic hv_storvsc"</screen>
      </listitem>
     </varlistentry>
     <varlistentry>
      <term>/boot/grub/menu.lst
       </term>
      <listitem>
       <para>
        To name the partition that should be booted, use:
       </para>
<screen>root=UUID=...</screen>
       <para>
        To find the respective UUID value to use, execute the following
        command:
       </para>
<screen>tune2fs -l /dev/sda2|grep UUID</screen>
      </listitem>
     </varlistentry>
     <varlistentry>
      <term>/etc/fstab
       </term>
      <listitem>
       <para>
        Do not use device names (<filename>/dev/...</filename>), but
        <literal>UUID=...</literal> or <literal>LABEL=root</literal> entries.
        For the latter, add the label <literal>root</literal> to the root file
        system of your image (in this case, <filename>/dev/sda2</filename>):
       </para>
<screen>tune2fs -L root /dev/sda2</screen>
      </listitem>
     </varlistentry>
     <varlistentry>
      <term>Disk Format</term>
      <listitem>
       <para>
        Use <filename>*.qcow2</filename> as disk format for your image.
       </para>
      </listitem>
     </varlistentry>
     <varlistentry>
      <term>Image Properties in &ostack; &img;</term>
      <listitem>
       <para>
        To upload the image to &productname; only once and to use the same
        image for &kvm; and &xen;, specify the following image
        options during or after upload:
       </para>
<screen>--public --container-format bare \
--property&nbsp;architecture=x86_64 \
--property vm_mode=hvm \
--disk-format qcow2</screen>
      </listitem>
     </varlistentry>
    </variablelist>
   </sect2>
  </sect1>
  <sect1 xml:id="sec.adm.cli.img.build">
   <title>Building Images with &susestudio;</title>

   <para>
    When creating an appliance for &productname; the following steps are
    essential:
   </para>

   <procedure>
    <step>
     <para>
      In &susestudio; or &suseonsite;, switch to the <menuchoice>
      <guimenu>Configuration</guimenu> <guimenu>Appliance</guimenu>
      </menuchoice> tab.
     </para>
    </step>
    <step>
     <para>
      Enable the <guimenu>Integrate with &productname;/OpenStack</guimenu>
      check box.
     </para>
    </step>
    <step>
     <para>
      On the <guimenu>Build</guimenu> tab, choose the respective appliance
      format. It mainly depends on the hypervisor on which you want to use the
      image&mdash;see <xref linkend="vl.adm.img.req.virt"/>.
     </para>
    </step>
   </procedure>

   <para>
    For more detailed information on how to build appliance images, refer to
    the &suseonsite; documentation, available at
    <link xlink:href="http://www.suse.com/documentation/suse_studio/"/>.
   </para>
  </sect1>
  <sect1 xml:id="sec.adm.cli.img.props">
   <title>Image Properties</title>

<!-- taroth 2013-08-29: fix for https://bugzilla.suse.com/show_bug.cgi?id=828862-->

   <para>
    Images have both contents and metadata. The latter are also called
    properties. The following properties can be attached to an image in
    &cloud;. Set them from the command line when uploading or modifying images.
    For a list of image properties, see <link
     xlink:href="http://docs.openstack.org/developer/python-openstackclient/command-objects/image.html"/>.
   </para>
  </sect1>
  <sect1 xml:id="sec.adm.cli.img.upload">
   <title>Uploading Images</title>

   <para>
    If you have created an image for
    <literal>&productname;/OpenStack/&kvm;</literal> with &susestudio; or with
    &suseonsite; 1.3, you can upload the image directly as described in
    <xref linkend="pro.adm.cli.img.upload"/>.
   </para>

   <procedure xml:id="pro.adm.cli.img.convert">
    <title>Converting Disk Images to Different Formats</title>
    <step>
<!--taroth 2013-08-30: for the records and later revisions: package
      virt-utils is called qemu-tools in openSUSE or newer (info by dmueller) -->
     <para>
      Make sure the <systemitem class="resource">virt-utils</systemitem>
      package is installed on the machine used for conversion.
     </para>
    </step>
    <step>
     <para>
      Download the image from &susestudio;.
     </para>
    </step>
    <step>
     <para>
      To convert <literal>qcow2</literal> to <literal>vhd</literal> images, use
      the following command:
     </para>
<screen>qemu-img convert -O vpc&nbsp;<replaceable>CURRENT_IMAGE_FILE</replaceable>&nbsp;<replaceable>FINAL_IMAGE_FILE</replaceable>.vhd</screen>
    </step>
   </procedure>

<!--taroth 2014-02-20: converting from raw to qcow2 should not be necessary anymore, still only
    comenting the following step in case it is needed again-->

<!--<step>
    <para>
    To convert the image to a different format, use either of the
    following commands:
    </para>
    <itemizedlist>
    <listitem>
    <para>
    For conversion from <literal>raw</literal> to
    <literal>qcow2</literal>:
    </para>
    <screen>qemu-img convert -c -f raw -O&nbsp;qcow2&nbsp;\
    <replaceable>CURRENT_IMAGE_FILE</replaceable>&nbsp;<replaceable>FINAL_IMAGE_FILE</replaceable>.qcow2</screen>
    </listitem>
    <listitem>
    <para>
    For conversion from <literal>raw</literal> or
    <literal>qcow2</literal> to <literal>vhd</literal>:
    </para>
    <screen>qemu-img convert -O vpc&nbsp;<replaceable>CURRENT_IMAGE_FILE</replaceable>&nbsp;<replaceable>FINAL_IMAGE_FILE</replaceable>.vhd</screen>
    </listitem>
    </itemizedlist>
    </step>-->

   <procedure xml:id="pro.adm.cli.img.upload">
    <title>Uploading Disk Images to &productname;</title>
    <para>
     <remark>taroth 2017-01-13: @DEVs: is the following procedure still correct?
     </remark>
    </para>
    <para>
     Upload a disk image using the <systemitem
      class="resource">python-openstackclient</systemitem> client.</para>
    <para>
     Images are owned by projects and can be <literal>private</literal>
     (accessible to members of the particular project only) or
     <literal>public</literal> (accessible to members of all projects). Private
     images can also be explicitly shared with other projects, so that members
     of those projects can access the images, too. Any image uploaded to
     &ostack; &img; will get an <literal>owner</literal> attribute. By default,
     ownership is set to the primary project of the user that uploads the
     image.
    </para>
    <para>
     Set or modify hypervisor-specific properties with the
     <option>--property&nbsp;<replaceable>key</replaceable>=<replaceable>value</replaceable></option>
     option. This can be done directly during image upload (as shown in the
     examples below). To change the properties after image upload, refer to
     <xref linkend="pro.adm.cli.img.props.modify"/>.
    </para>
    <step>
     <para>
      In a shell, source the &ostack; RC file for the project that you want to
      upload an image to. For details, refer to
      <link xlink:href="http://docs.openstack.org/user-guide/common/cli_set_environment_variables_using_openstack_rc.html"/>.
     </para>
    </step>
    <step xml:id="step.adm.cli.img.upload">
     <para>
      Upload the image using <command>openstack image create</command>. Find some
      example commands for different hypervisors below:
     </para>
     <itemizedlist mark="bullet" spacing="normal">
<!--taroth 2014-20-21: CAVE, the same hypervisor-specific props are mentioned in 2 procedures:
       pro.adm.cli.img.upload and pro.adm.cli.img.props.modify -->
      <listitem>
       <para>
        For &kvm;:
       </para>
<screen>openstack image create <replaceable>IMAGE_NAME</replaceable> \
  --public --container-format bare \
  --property&nbsp;architecture=x86_64 \
  --property&nbsp;hypervisor_type=kvm \
  --disk-format qcow2 \
  --file <replaceable>PATH_TO_IMAGE_FILE</replaceable>.qcow2&nbsp;</screen>
      </listitem>
      <listitem>
       <para>
        For &xen;:
       </para>
<screen>openstack image create <replaceable>IMAGE_NAME</replaceable> \
  --public --container-format bare \
  --property&nbsp;architecture=x86_64 \
  --property&nbsp;hypervisor_type=xen \
  --property&nbsp;vm_mode=xen \
  --disk-format qcow2 \
  --file <replaceable>PATH_TO_IMAGE_FILE</replaceable>.qcow2&nbsp;</screen>
       <note>
        <title>Value of <option>vm_mode</option></title>
        <para>
         For &xen; PV image import, use <option>vm_mode=xen</option>, for &xen;
         HVM image import use <option>vm_mode=hvm</option>.
        </para>
       </note>
<!--taroth 2013-10-17: Xen:
        http://docserv.nue.suse.com/documents/SLES/SLES-xen/single-html/#sec.xen.config.disk-->
      </listitem>
      <listitem>
       <para>
        For &vmware;:
       </para>
<screen>openstack image create <replaceable>IMAGE_NAME</replaceable> \
  --public --container-format bare \
  --property&nbsp;vmware_adaptertype="lsiLogic" \
  --property vmware_disktype="preallocated" \
  --property hypervisor_type=vmware \
  --disk-format vmdk&nbsp;--file&nbsp;<replaceable>PATH_TO_IMAGE_FILE</replaceable>.vmdk</screen>
<!--taroth 2014-02-05: fix for https://bugzilla.suse.com/show_bug.cgi?id=862165#c3,
        as discussed with mjura-->
       <note>
        <title>Value of <literal>vmware_disktype</literal></title>
        <para>
         Depending on which disk type you use, adjust the value of
         <literal>vmware_disktype</literal> accordingly. For an overview of
         which values to use, refer to
         <link
          xlink:href="https://docs.openstack.org/nova/pike/admin/configuration/hypervisor-vmware.html"/>,
         <citetitle>section &ostack; Image Service disk type
         settings</citetitle>.
        </para>
       </note>
      </listitem>
      <listitem>
       <para>
        For Docker:
       </para>
       <para>
        Find an image in the Docker registry you want to use and save it
        locally with <command>docker pull
        <replaceable>IMAGE_NAME</replaceable></command>, where
        <replaceable>IMAGE_NAME</replaceable> is the name of the image in the
        Docker registry. The same name needs to be used when uploading the image
        with the following command:
       </para>
<screen>docker save <replaceable>IMAGE_NAME</replaceable> | openstack image create \
--public --property hypervisor_type=docker \
--container-format=docker --disk-format=raw <replaceable>IMAGE_NAME</replaceable></screen>
       <important>
        <title>Docker Images Need to Run a Long-Living Process</title>
        <para>
         Docker &vmguest;s will only be able to spawn successfully, when
         running a long-living process, for example
         <systemitem
       class="daemon">sshd</systemitem>. Such a process
         can be configured with
         <link
         xlink:href="https://docs.docker.com/engine/reference/builder/#cmd">CMD</link>
         or
         <link
       xlink:href="https://docs.docker.com/engine/reference/builder/#entrypoint">ENTRYPOINT</link>
         in the <filename>Docker</filename>.
        </para>
        <para>
         Alternatively, such a process can be specified on the command line
         with the image property
         <option>os_command_line</option>.
        </para>
<screen>openstack image set --property os_command_line='/usr/sbin/sshd -D' \
       <replaceable>IMAGE_ID</replaceable></screen>
       </important>
      </listitem>
     </itemizedlist>
     <para>
      If the image upload has been successful, a message appears, displaying
      the ID that has been assigned to the image.
     </para>
    </step>
   </procedure>

<!--taroth 2013-08-29: DEVs, is the following still true or can SUSE Studio inject any
    files into an image which make it possible to change the image?- taroth 2013-10-01: for the records:
    according to aj, studio can inject anything but it cannot change an image-->

   <note>
    <title>Updating Images</title>
    <para>
     After having uploaded an image to &cloud;, the image contents cannot be
     modified&mdash;only the image&apos;s metadata, see
     <xref linkend="pro.adm.cli.img.props.modify" xrefstyle="select:label"/>.
     To update image contents, you need to delete the current image and upload
     a modified version of the image. You can also launch an instance from the
     respective image, change it, create a snapshot of the instance and use the
     snapshot as a new image.
    </para>
   </note>
  </sect1>
  <sect1 xml:id="sec.adm.cli.img.props.modify">
   <title>Modifying Image Properties</title>
   <para>
    <remark>taroth 2017-01-13: @DEVs: please check if the following commands are
     still correct</remark>
   </para>
   <para>
    Set or modify hypervisor-specific properties with the
    <option>--property&nbsp;<replaceable>key</replaceable>=<replaceable>value</replaceable></option>
    option. This can be done directly during image upload (see
    <xref linkend="pro.adm.cli.img.upload" xrefstyle="select:label"/>) or after
    the image has been uploaded (as described below).
   </para>

   <procedure xml:id="pro.adm.cli.img.props.modify">
    <title>Modifying Image Properties</title>
    <step>
     <para>
      In a shell, source the &ostack; RC file for the project that you want to
      upload an image to. For details, refer to
      <link xlink:href="http://docs.openstack.org/user-guide/common/cli_set_environment_variables_using_openstack_rc.html"/>.
     </para>
    </step>
    <step>
     <para>
      If you do not know the ID or the exact name of the image whose properties
      you want to modify, look it up with:
     </para>
<screen>openstack image list</screen>
    </step>
    <step>
     <para>
      Use the <command>openstack&nbsp;image&nbsp;set</command> command to set the
      properties for architecture, hypervisor type, and virtual machine mode.
      In the following, find some examples with properties for different
      hypervisors:
     </para>
     <itemizedlist mark="bullet" spacing="normal">
<!--taroth 2014-20-21: CAVE, the same hypervisor-specific props are mentioned in 2 procedures:
       pro.adm.cli.img.upload and pro.adm.cli.img.props.modify -->
      <listitem>
       <para>
        For &kvm;:
       </para>
<screen>openstacke&nbsp;image set&nbsp;<replaceable>IMAGE_ID_OR_NAME</replaceable>&nbsp;\
  --property&nbsp;architecture=x86_64 \
  --property&nbsp;hypervisor_type=kvm</screen>
      </listitem>
      <listitem>
       <para>
        For &xen;:
       </para>
<screen>openstack&nbsp;image set&nbsp;<replaceable>IMAGE_ID_OR_NAME</replaceable>&nbsp;\
  --property&nbsp;architecture=x86_64 \
  --property&nbsp;hypervisor_type=xen \
  --property&nbsp;vm_mode=xen</screen>
       <note>
        <title>Value of <option>vm_mode</option></title>
        <para>
         For &xen; PV image import, use <option>vm_mode=xen</option>, for &xen;
         HVM image import use <option>vm_mode=hvm</option>.
        </para>
       </note>
      </listitem>
      <listitem>
       <para>
        For &vmware;:
       </para>
<screen>openstack&nbsp;image update&nbsp;<replaceable>IMAGE_ID_OR_NAME</replaceable>&nbsp;\
  --property&nbsp;vmware_adaptertype="lsiLogic" \
  --property vmware_disktype="preallocated" \
  --property hypervisor_type=vmware</screen>
<!--taroth 2014-02-05: fix for https://bugzilla.suse.com/show_bug.cgi?id=862165#c3,
        as discussed with mjura-->
       <note>
        <title>Value of <literal>vmware_disktype</literal></title>
        <para>
         Depending on which disk type you use, adjust the value of
         <literal>vmware_disktype</literal> accordingly. For an overview of
         which values to use, refer to
         <link
          xlink:href="https://docs.openstack.org/glance/latest/admin/useful-image-properties.html"/>,
         <citetitle>section &ostack; Image Service disk type
         settings</citetitle>.
        </para>
       </note>
      </listitem>
     </itemizedlist>
    </step>
   </procedure>

   <para>
    For more information about the <systemitem>architecture</systemitem>,
    <systemitem>hypervisor_type</systemitem>, and
    <systemitem>vm_mode</systemitem> properties, refer toq
    <link xlink:href="https://docs.openstack.org/glance/latest/admin/useful-image-properties.html"/>.
   </para>
  </sect1>
  <sect1 xml:id="sec.suppl.img.custom.kernel">
   <title>Using a Custom Kernel to Boot Btrfs Images Under &xen;</title>

   <para>
    On &xen; hosts, starting instances from an image that uses Btrfs as root
    file system may fail with &productname; &productnumber;. As a work-around,
    boot the Btrfs image with a custom kernel to start the instances. Prepare
    the Btrfs image as described in
    <xref
    linkend="pro.suppl.img.brtfs.custom.kernel"/>.
   </para>

   <procedure xml:id="pro.suppl.img.brtfs.custom.kernel">
    <title>Booting Btrfs Images with a Custom Kernel</title>
    <itemizedlist>
     <title>Requirements</title>
     <listitem>
      <para>
       The <systemitem class="resource">python-openstackclient</systemitem> is
       installed. After you have sourced an &ostack; RC file, use the command
       line client to upload images from a machine outside of the cloud.
      </para>
     </listitem>
     <listitem>
      <para>
       To run the
       <systemitem class="resource">python-openstackclient</systemitem>: An
       &ostack; RC file containing the credentials for the &ostack; project to
       which you want to upload the images.
      </para>
     </listitem>
    </itemizedlist>
    <step>
     <para>
      Install the <systemitem class="resource">grub2-xen</systemitem> package.
      It provides the <filename>grub.xen</filename> file required to boot
      para-virtualized VMs that use the Btrfs file system:
     </para>
<screen>&prompt.root;zypper in grub2-x86_64-xen</screen>
    </step>
    <step>
     <para>
      Create a &o_img; image with the kernel from this package. For example:
     </para>
<screen>openstack image create img-grub-xen-x64 \
 --file /usr/lib/grub2/x86_64-xen/grub.xen --public \
 --container-format bare --disk-format raw</screen>
    </step>
    <step>
     <para>
      Create a second image which uses Btrfs as root file system. For example:
     </para>
<screen>openstack image create img-btrfs \
  --file openSUSE-Leap-42.1-JeOS-for-XEN.x86_64.qcow2 \
  --container-format bare --disk-format qcow2</screen>
    </step>
    <step>
     <para>
      Update the image named <literal>img-btrfs</literal> by adding a
      <literal>kernel_id</literal> property to it:
     </para>
<screen>openstack image set 376c245d-24fe-41e2-8abd-655d4ed8da95 \
--property kernel_id=72ad3069-6003-4653-86f2-b5914ce33f66</screen>
     <para>
      where <literal>376c245d-24fe-41e2-8abd-655d4ed8da95</literal> is the ID
      of the image named <literal>img-btrfs</literal> and
      <literal>72ad3069-6003-4653-86f2-b5914ce33f66</literal> is the ID of the
      image named <literal>img-grub-xen-x64</literal>
     </para>
    </step>
    <step>
     <para>
      Boot the image to start the instance:
     </para>
<screen>openstack server create --flavor <replaceable>FLAVOR</replaceable> --image 376c245d-24fe-41e2-8abd-655d4ed8da95</screen>
     <para>
      This results in a domain XML which contains the kernel you need:
     </para>
<screen>&lt;domain type='xen' id='2'>
 &lt;name>instance-00000003&lt;/name>
 &lt;uuid>12b2ce2b-ba1d-4c14-847f-9476dbae7199&lt;/uuid>
 &lt;memory unit='KiB'>524288&lt;/memory>
 &lt;currentMemory unit='KiB'>524288&lt;/currentMemory>
 &lt;vcpu placement='static'>1&lt;/vcpu>
 &lt;bootloader>&lt;/bootloader>
 &lt;os>
   &lt;type>linux&lt;/type>
   &lt;kernel>/var/lib/nova/instances/12b2ce2b-ba1d-4c14-847f-9476dbae7199/kernel&lt;/kernel>
   &lt;cmdline>ro root=/dev/xvda&lt;/cmdline>
 &lt;/os></screen>
    </step>
   </procedure>
  </sect1>
  <sect1 xml:id="sec.adm.cli.img.view">
   <title>Viewing Images and Image Properties, Deleting Images</title>

   <para>
    In the following, find some examples on how to view images or image
    properties or how to remove images from &ostack; &img;.
   </para>

   <variablelist>
    <varlistentry>
     <term>Listing Images</term>
     <listitem>
<screen>openstack image list</screen>
      <para>
       Lists ID, name, and status for all images in
       &img; that the current user can access.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>Showing Metadata for a Particular Image</term>
     <listitem>
<screen>openstack image show <replaceable>IMAGE_ID_OR_NAME</replaceable>&nbsp;</screen>
      <para>
       Shows metadata of the specified image.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>Removing Image Properties</term>
     <listitem>
<screen>openstack image unset --property <replaceable>PROPERTY</replaceable> <replaceable>IMAGE_ID_OR_NAME</replaceable></screen>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>Deleting an Image</term>
     <listitem>
<screen>openstack image delete <replaceable>IMAGE_ID_OR_NAME</replaceable>&nbsp;</screen>
      <para>
       Removes the specified image from &ostack; &img;.
      </para>
     </listitem>
    </varlistentry>
   </variablelist>
  </sect1>
  <sect1 xml:id="sec.adm.cli.img.member">
   <title>Viewing and Modifying Membership of Private Images</title>

   <para>
    In the following, find some examples on how to view or modify membership of
    private images:
    <remark>taroth 2016-12-28: @DEVs, need your help! no idea how to adjust the
    following commands when using the python-openstackclient instead of glance...</remark>
   </para>

   <variablelist>
    <varlistentry>
     <term>Listing Members a Private Image is Shared With</term>
     <listitem>
<screen>glance member-list --image-id <replaceable>IMAGE_ID</replaceable>&nbsp;</screen>
      <para>
       Lists the IDs of the projects whose members have access to the private
       image.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>Listing Private Images Shared With a Member</term>
     <listitem>
<screen>glance member-list --tenant-id <replaceable>PROJECT_ID</replaceable>&nbsp;</screen>
      <para>
       Lists the IDs of private images that members of the specified project
       can access.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>Granting Members Access to a Private Image</term>
     <listitem>
<screen>openstack image add project [--can-share]&nbsp;<replaceable>IMAGE_ID_OR_NAME</replaceable>&nbsp;<replaceable>PROJECT_ID_OR_NAME</replaceable>&nbsp;</screen>
      <para>
       Grants the specified member access to the specified private image.
      </para>
      <para>
       By adding the <option>--can-share</option> option, you can allow the
       members to further share the image.
       <remark>taroth 2012-08-08: however that may work, no idea...</remark>
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>Revoking Member Access to a Private Image</term>
     <listitem>
<screen>openstack image remove project <replaceable>IMAGE_ID_OR_NAME</replaceable>&nbsp;<replaceable>PROJECT_ID_OR_NAME</replaceable>&nbsp;</screen>
      <para>
       Revokes the access of the specified member to the specified private
       image.
      </para>
     </listitem>
    </varlistentry>
   </variablelist>
  </sect1>
 </chapter>
 <chapter xml:id="cha.user.dash.inst">
  <title>Launching Instances from the &cloud; Dashboard</title>
  <para>
   Instances are virtual machines that run inside the cloud. To start an
   instance, a virtual machine image must exist that contains the following
   information: which operating system to use, a user name and password with
   which to log in to the &vmguest;, file storage, etc. The cloud contains a
   pool of such images that have been uploaded to &ostack; &img; and are
   accessible to members of different projects.
  </para>
  <sect1 xml:id="sec.user.dash.inst.launch.params">
   <title>Key Parameters</title>

   <para>
    When starting an instance, specify the following key parameters:
   </para>

   <variablelist>
    <varlistentry>
     <term>Flavor</term>
     <listitem>
      <para>
       In &ostack;, flavors define the compute, memory, and storage capacity of
       <literal>nova</literal> computing instances. To put it simply, a flavor
       is an available hardware configuration for a server. It defines the
       <quote>size</quote> of a virtual server that can be launched.
      </para>
      <para>
       For more details and a list of default flavors available, refer to the
       &cloudadmin;, chapter <citetitle>Dashboard</citetitle> or
       chapter <citetitle>OpenStack command-line clients</citetitle>, section
       <citetitle>Manage Flavors</citetitle>. The guide is available from
       &suse-onlinedoc;.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>Key Pair (optional, but recommended)</term>
     <listitem>
      <para>
       Key Pairs are SSH credentials that are injected into images when they
       are launched. For this to work, the image must contain the
       <systemitem class="resource">cloud-init</systemitem> package.
      </para>
      <para>
       It is recommended to create at least one key pair per project. If you
       already have generated a key pair with an external tool, you can import
       it into &ostack;. The key pair can be used for multiple instances
       belonging to that project.
      </para>
      <para>
       For details, refer to the &clouduser;, chapter <citetitle>OpenStack
       dashboard</citetitle> or chapter <citetitle>OpenStack command-line
       clients</citetitle>, section <citetitle>Configure access and security
       for instances</citetitle>. The guide is available from &suse-onlinedoc;.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>Security Group</term>
     <listitem>
      <para>
       In &productname;, security groups are used to define which incoming
       network traffic should be forwarded to instances. Security groups hold a
       set of firewall policies (security group rules).
      </para>
      <para>
       For details, refer to the &clouduser;, chapter <citetitle>OpenStack
       dashboard</citetitle> or chapter <citetitle>OpenStack command-line
       clients</citetitle>, section <citetitle>Configure access and security
       for instances</citetitle>. The guide is available from &suse-onlinedoc;.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>Network</term>
     <listitem>
      <para>
       Instances can belong to one or multiple networks. By default, each
       instance is given a fixed IP address, belonging to the internal network.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>Boot Source of the Instance</term>
     <listitem>
      <para>
       You can launch instances from the following sources. For details, see the
       &clouduser;, chapter <citetitle>OpenStack dashboard</citetitle> or
       chapter <citetitle>OpenStack command-line clients</citetitle>, section
       <citetitle>Launch and manage instances</citetitle>.
       The guide is available from &suse-onlinedoc;.
      </para>
      <itemizedlist>
       <listitem>
        <para>
         Images that have been uploaded to &productname;.
        </para>
       </listitem>
       <listitem>
        <para>
         Volumes that contain images.
        </para>
       </listitem>
       <listitem>
        <para>
         Instance snapshots.
        </para>
       </listitem>
       <listitem>
        <para>
         Volume snapshots.
        </para>
       </listitem>
      </itemizedlist>
     </listitem>
    </varlistentry>
   </variablelist>
  </sect1>
  <sect1 xml:id="sec.user.dash.inst.launch.vol">
   <title>Launching Instances from Images, Snaphots, or Volumes</title>

   <para>
    For instructions on how to launch instances from images or snapshots, see
    <link xlink:href="http://docs.openstack.org/user-guide/dashboard-launch-instances.html"><citetitle>Launch
    an Instance</citetitle></link>.
   </para>
  </sect1>
  <!--taroth 2016-12-28: the following contents is now also covered in
   http://docs.openstack.org/user-guide/dashboard-launch-instances.html
   (also how to launch an instance from an empty volume), therefore commenting/
   removing this section completely
   <sect1>
   <title>Launching Instances from Volumes</title>-->
 </chapter>
 <chapter xml:id="cha.user.dash.inst.access">
  <title>Configuring Access to the Instances</title>
  <para>
   Access to an instance is mainly influenced by the following parameters:
  </para>
  <variablelist>
   <varlistentry>
    <term>Security Groups and Rules</term>
    <listitem>
     <para>
      In &productname;, security groups are used to define which incoming
      network traffic should be forwarded to instances. Security groups hold a
      set of firewall policies (security group rules).
     </para>
     <para>
      For instructions on how to configure security groups and security group
      rules, see
      <link xlink:href="http://docs.openstack.org/user-guide/configure-access-and-security-for-instances.html">
      <citetitle>Configure access and security for instances</citetitle></link>.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>Key Pairs</term>
    <listitem>
     <para>
      Key Pairs are SSH credentials that are injected into images when they are
      launched. For this to work, the image must contain the
      <systemitem class="resource">cloud-init</systemitem> package.
     </para>
     <para>
      It is recommended to create at least one key pair per project. If you
      already have generated a key pair with an external tool, you can import
      it into &ostack;. The key pair can be used for multiple instances
      belonging to that project.
     </para>
     <para>
      For details on how to create or import keypairs, see
      <link xlink:href="http://docs.openstack.org/user-guide/configure-access-and-security-for-instances.html">
       <citetitle>Configure access and security for instances</citetitle></link>.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>IP Addresses</term>
    <listitem>
     <para>
      Each instance can have two types of IP addresses: private (fixed) IP
      addresses and public (floating) ones. Private IP addresses are used for
      communication between instances, and public ones are used for
      communication with the outside world. When an instance is launched, it is
      automatically assigned private IP addresses in the networks to which it
      is assigned. The private IP stays the same until the instance is
      explicitly terminated. (Rebooting the instance does not have an effect on
      the private IP addresses.)
     </para>
     <para>
      A floating IP is an IP address that can be dynamically added to a virtual
      instance. In &ostack; &netw;, cloud administrators can configure pools of
      floating IP addresses. These pools are represented as external networks.
      Floating IPs are allocated from a subnet that is associated with the
      external network. You can allocate a certain number of floating IPs to a
      project&mdash;the maximum number of floating IP addresses per project is
      defined by the quota. From this set, you can then add a floating IP
      address to an instance of the project.
<!--taroth 2013-07-05: check if to merge with the following, copied from
          http://docs.openstack.org/api/openstack-network/2.0/content/router_ext_concepts.html:
          A floating IP is an IP address on an external network, which is
          associated with a  specific port, and optionally a specific IP
          address, on a private OpenStack Networking network. Therefore a
          floating IP allows access to an instance on a private network from
          an external network. Floating IPs can only be defined on networks
          for which the attribute router:external has been set to True.-->
     </para>
     <para>
      For information on how to assign floating IP addresses to instances, see
      <link xlink:href="http://docs.openstack.org/user-guide/configure-access-and-security-for-instances.html">
       <citetitle>Configure access and security for instances</citetitle></link>.
     </para>
    </listitem>
   </varlistentry>
  </variablelist>
  <sect1 xml:id="sec.user.dash.inst.access.security.rules">
   <title>Security Group Rules</title>

   <para>
    You can adjust rules of the default security group and rules of any other
    security group that has been created. When the rules for a group are
    modified, the new rules are automatically applied to all running instances
    belonging to that security group.
   </para>

   <para>
    Adjust the rules in a security group to allow access to instances via
    different ports and protocols. This is necessary to be able to access
    instances via SSH, to ping them, or to allow UDP traffic (for example, for
    a DNS server running on an instance).
   </para>

   <para>
    Rules in security groups are specified by the following parameters:
   </para>

   <variablelist>
    <varlistentry>
     <term>IP Protocol </term>
     <listitem>
      <para>
       Protocol to which the rule will apply. Choose between TCP (for SSH),
       ICMP (for pings), and UDP.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>Port/Port Range</term>
     <listitem>
      <para>
       For TCP or UDP, define a single port or a port range to open on the
       virtual machine. ICMP does not support ports. In that case, enter values
       that define the codes and types of ICMP traffic to be allowed.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>Source of traffic (<guimenu>Remote</guimenu> in the &cloud; &dash;)</term>
     <listitem>
      <para>
       Decide whether to allow traffic to instances only from IP addresses
       inside the cloud (from other group members) or from
       <emphasis>all</emphasis> IP addresses. Specify either an IP address
       block (in CIDR notation) or a security group as source. Using a security
       group as source will allow any instance in that security group to access
       any other instance.
      </para>
     </listitem>
    </varlistentry>
   </variablelist>

   <para>
    If no further security groups have been created, any instances are
    automatically assigned to the default security group (if not specified
    otherwise). Unless you change the rules for the default group, those
    instances cannot be accessed from any IP addresses outside the cloud.
   </para>

<!--taroth 2012-08-27: IIRC, I read this in upstream docs, but according to
    bmwiedemann's tests, he could not access arbitrary ports of VMs in the same
    security group, therefore commenting-->

<!--Unless you change the rules for the default group, this means that
    those instances are only accessible from IP addresses belonging to other members
    of this group.-->

   <procedure xml:id="pro.cloud.dash.security.rules">
    <title>Configuring Security Group Rules</title>
    <para>
     For quicker configuration, the &dash; provides templates for
     often-used rules that, including rules for well-known protocols on top of TCP
     (such as HTTP or SSH), or rules to allow all ICMP traffic (for pings).
    </para>
    <step>
     <para>
      Log in to &cloud; &dash; and select a project from the drop-down box at
      the top-level row.
     </para>
    </step>
    <step>
     <para>
      Click <menuchoice> <guimenu>Project</guimenu> <guimenu>Compute</guimenu>
      <guimenu>Access &amp; Security</guimenu> </menuchoice>. The view shows
      the following tabs: <guimenu>Security Groups</guimenu>, <guimenu>Key
      Pairs</guimenu>, <guimenu>Floating IPs</guimenu>, and <guimenu>API
      Access</guimenu>.
     </para>
    </step>
    <step>
     <para>
      On the <guimenu>Security Group</guimenu> tab, click <guimenu>Manage
      Rules</guimenu> for the security group you want to modify. This opens the
      <guimenu> Security Group Rules</guimenu> screen that shows the existing
      rules for the group and lets you add or delete rules.
<!-- <remark>taroth 2013-07-25: unfortunately, it does not allow to _edit_ rules, filed
       https://bugs.launchpad.net/horizon/+bug/1210513 for this</remark>-->
     </para>
    </step>
    <step>
     <para>
      Click <guimenu>Add Rule</guimenu> to open a new dialog.
     </para>
     <para>
      From the <guimenu>Rule</guimenu> drop-down box, you can select templates
      for often-used rules, including rules for well-known protocols
      on top of TCP (such as HTTP or SSH), or rules to allow all ICMP traffic
      (for pings). In the following steps, we will focus on the most
      commonly-used rules only:
     </para>
    </step>
    <step>
     <para>
      To enable SSH access to the instances:
     </para>
     <substeps performance="required">
      <step>
       <para>
        Set <guimenu>Rule</guimenu> to <literal>SSH</literal>.
       </para>
      </step>
      <step>
       <para>
        Decide whether to allow traffic to instances only from IP addresses
        inside the cloud (from other group members) or from
        <emphasis>all</emphasis> IP addresses.
       </para>
       <itemizedlist mark="bullet" spacing="normal">
        <listitem>
         <para>
          To enable access from <emphasis>all</emphasis> IP addresses
          (specified as IP subnet in CIDR notation as
          <literal>0.0.0.0/0</literal>), leave the <guimenu>Remote</guimenu>
          and <guimenu>CIDR</guimenu> fields unchanged.
         </para>
        </listitem>
        <listitem>
         <para>
          Alternatively, allow only IP addresses from other security groups to
          access the specified port. In that case, set
          <guimenu>Remote</guimenu> to <literal>Security Group</literal>.
          Select the desired <guimenu>Security Group</guimenu> and
          <guimenu>Ether Type</guimenu>
<!--taroth 2014-01-23: SIC!-->
          (<literal>IPv4</literal> or <literal>IPv6</literal>).
         </para>
        </listitem>
       </itemizedlist>
      </step>
     </substeps>
    </step>
    <step>
     <para>
      To enable pinging the instances:
     </para>
     <substeps performance="required">
      <step>
       <para>
        Set <guimenu>Rule</guimenu> to <literal>ALL ICMP</literal>.
       </para>
      </step>
      <step>
       <para>
        Decide whether to allow traffic to instances only from IP addresses
        inside the cloud (from other group members) or from
        <emphasis>all</emphasis> IP addresses.
       </para>
       <itemizedlist mark="bullet" spacing="normal">
        <listitem>
         <para>
          To enable access from <emphasis>all</emphasis> IP addresses
          (specified as IP subnet in CIDR notation as
          <literal>0.0.0.0/0</literal>), leave the <guimenu>Remote</guimenu>
          and <guimenu>CIDR</guimenu> fields unchanged.
         </para>
        </listitem>
        <listitem>
         <para>
          Alternatively, allow only IP addresses from other security groups to
          access the specified port. In that case, set
          <guimenu>Remote</guimenu> to <literal>Security Group</literal>.
          Select the desired <guimenu>Security Group</guimenu> and
          <guimenu>Ether Type</guimenu>
<!--taroth 2014-01-23: SIC!-->
          (<literal>IPv4</literal> or <literal>IPv6</literal>).
         </para>
        </listitem>
       </itemizedlist>
      </step>
     </substeps>
    </step>
    <step>
     <para>
      To enable access via a UDP port (for example, for syslog):
     </para>
     <substeps performance="required">
      <step>
       <para>
        Set <guimenu>Rule</guimenu> to <literal>Custom UDP</literal>.
       </para>
      </step>
      <step>
       <para>
        Leave the <guimenu>Direction</guimenu> and <guimenu>Open Port</guimenu>
        values untouched.
       </para>
      </step>
      <step>
       <para>
        In the <guimenu>Port</guimenu> text box, enter the value
        <guimenu>514</guimenu>.
       </para>
      </step>
      <step>
       <para>
        Decide whether to allow traffic to instances only from IP addresses
        inside the cloud (from other group members) or from
        <emphasis>all</emphasis> IP addresses.
       </para>
       <itemizedlist mark="bullet" spacing="normal">
        <listitem>
         <para>
          To enable access from <emphasis>all</emphasis> IP addresses
          (specified as IP subnet in CIDR notation as
          <literal>0.0.0.0/0</literal>), leave the <guimenu>Remote</guimenu>
          and <guimenu>CIDR</guimenu> fields unchanged.
         </para>
        </listitem>
        <listitem>
         <para>
          Alternatively, allow only IP addresses from other security groups to
          access the specified port. In that case, set
          <guimenu>Remote</guimenu> to <literal>Security Group</literal>.
          Select the desired <guimenu>Security Group</guimenu> and
          <guimenu>Ether Type</guimenu>
<!--taroth 2014-01-23: SIC!-->
          (<literal>IPv4</literal> or <literal>IPv6</literal>).
         </para>
        </listitem>
       </itemizedlist>
      </step>
     </substeps>
    </step>
   </procedure>
  </sect1>
 </chapter>
 <chapter xml:id="cha.deploy.kubernetes">
  <title>Deploying Kubernetes</title>
  <para>
   By combining &ostack;, Docker, Kubernetes, and Flannel, you get a containers
   solution which works like other OpenStack services. With &o_container;, Docker
   and Kubernetes are made available as first class resources in &ostack;.
  </para>
  <sect1 xml:id="sec.deploy.kubernetes.cli">
   <title>Deploying a Kubernetes Cluster from Command Line</title>
   <para>
    A cluster (formerly <literal>bay</literal>) is the construct in which
    &o_container; launches container orchestration engines.
   </para>
   <itemizedlist>
    <title>Requirements</title>
    <listitem>
     <para>
      The <systemitem class="resource">python-openstackclient</systemitem> is
      installed. After you have sourced an &ostack; RC file, use the command
      line client to upload images from a machine outside of the cloud.
     </para>
    </listitem>
    <listitem>
     <para>
      To run the
      <systemitem class="resource">python-openstackclient</systemitem>: An
      &ostack; RC file containing the credentials for the &ostack; project to
      which you want to upload the images.
     </para>
    </listitem>
    <listitem>
     <para>
      The <systemitem class="resource">python-magnumclient</systemitem> is installed.
     </para>
    </listitem>
            <listitem>
     <para>
    Install the <package>openstack-magnum-k8s-image-x86_64</package> package. This
    package provides a virtual machine image with Kubernetes pre-installed,
    <filename>openstack-magnum-k8s-image.x86_64.qcow2</filename>. &ostack; &o_container;
    uses this image when creating clusters with its <filename>k8s_opensuse_v1</filename> driver.
     </para>
    </listitem>
   </itemizedlist>

  <procedure>
   <step>
    <para>
     In a shell, source the &ostack; RC file for the project that you want to
     upload an image to. For details, refer to
     <link xlink:href="http://docs.openstack.org/user-guide/common/cli_set_environment_variables_using_openstack_rc.html"/>.
    </para>
   </step>
   <step xml:id="step.deploy.k8s.img.create">
    <para>
     List the &o_container; image uploaded to &o_img; using the <command>openstack
     image list | grep  openstack-magnum-k8s-image</command>. If no image
     found, you can create an image for cluster setup as shown below:
    </para>
<screen>openstack image create openstack-magnum-k8s-image \
  --public --disk-format qcow2 \
  --property os_distro='opensuse' \
  --container-format bare \
  --file /srv/tftpboot/files/openstack-magnum-k8s-image/openstack-magnum-k8s-image.x86_64.qcow2</screen>
<!--openstack image create -/-name sles-openstack-magnum-kubernetes \
    -/-visibility public \
    -/-disk-format qcow2 \
    -/-os-distro opensuse \
    -/-container-format bare \
    -/-file ./sles-openstack-magnum-kubernetes.x86_64.qcow2</screen>-->
   </step>
   <step xml:id="step.deploy.k8s.flavor.create">
    <para>
     Create a &o_container; flavor. For example:
    </para>
 <screen>openstack flavor create --public m1.magnum --id 9 --ram 1024 \
   --disk 10 --vcpus 1</screen>
    <!--# openstack flavor create -/-is-public true m1.magnum 9 1024 10 1 -->
    <!--9 ID| 1024 memory_mb | 10 disk | 1 vcpu-->
    <para>
     If you do not have enough resources and RAM on your compute nodes for a
     flavor of this size, create a smaller flavor instead.
    </para>
   </step>
   <step xml:id="step.deploy.k8s.templ.create">
    <para>
     Create a cluster template for Kubernetes. For example:
     <remark>taroth 2016-12-29: the python-openstackclient does not seem to
      handle magnum commands yet, therefore keeping the magnum commands here</remark>
    </para>
<screen>magnum cluster-template-create --name k8s_template \
  --image-id openstack-magnum-k8s-image \
  --keypair-id default \
  --external-network-id floating \
  --dns-nameserver 8.8.8.8 \
  --flavor-id m1.magnum \
  --master-flavor-id m1.magnum \
  --docker-volume-size 5 \
  --network-driver flannel \
  --coe kubernetes \
  --master-lb-enabled</screen>
   </step>
   <step xml:id="step.deploy.k8s.cluster.create">
    <para>
     Create a Kubernetes cluster using the cluster template you have
     created in the step above. For example:
    </para>
<screen>magnum cluster-create --name k8s_cluster --cluster-template k8s_template \
  --master-count 1 --node-count 2</screen>
    <para>
     The resulting cluster will have one master Kubernetes node and two minion
     nodes.
    </para>
   </step>
  </procedure>
  </sect1>
  <sect1 xml:id="sec.deploy.kubernetes.ui">
   <title>Deploying a Kubernetes Cluster from the &dash;</title>
   <para>
    Alternatively, you can deploy a Kubernetes cluster in the &cloud; &dash;
    by creating a cluster template and creating a Kubernetes cluster afterward.
   </para>
   <itemizedlist>
    <title>Requirements</title>
    <listitem>
     <para>
      You have created an image for cluster setup as described in <xref
       linkend="sec.deploy.kubernetes.cli" xrefstyle="select:label"/>, <xref
        linkend="step.deploy.k8s.img.create" xrefstyle="select:label"/>.
     </para>
    </listitem>
    <listitem>
     <para>
      You have created a &o_container; flavor as described in <xref
       linkend="sec.deploy.kubernetes.cli" xrefstyle="select:label"/>, <xref
        linkend="step.deploy.k8s.flavor.create" xrefstyle="select:label"/>.
     </para>
    </listitem>
   </itemizedlist>
   <procedure xml:id="pro.deploy.cluster.templ.ui">
   <title>Creating a Cluster Template in &cloud; &dash;</title>
   <step>
    <para>
     Log in to &cloud; &dash; and select a project from the drop-down box at
     the top-level row.
    </para>
   </step>
   <step>
    <para>
     Click <menuchoice>
      <guimenu>Container Infra</guimenu>
      <guimenu>Cluster Templates</guimenu>
      <guimenu>Create Cluster Template</guimenu>
     </menuchoice>.
    </para>
    <para>
     The <guimenu>Create Cluster Template</guimenu> dialog opens, showing
     the following sections: <guimenu>Info</guimenu>, <guimenu>Node Spec</guimenu>,
     <guimenu>Network</guimenu>, and <guimenu>Labels</guimenu>.
    </para>
   </step>
   <step>
    <para>
     In the <guimenu>Info</guimenu> section:</para>
    <substeps>
     <step>
      <para>
       Enter a name for the cluster template to create.
      </para>
     </step>
     <step>
      <para>
       As <guimenu>Container Orchestration Engine</guimenu>, choose
       <literal>Kubernetes</literal>.
      </para>
     </step>
     <step>
      <para>
       If wanted, activate the following options:
      </para>
      <itemizedlist>
       <listitem>
        <para>
         <guimenu>Public</guimenu>: The cluster template will be visible for all
         users in &ostack;.
        </para>
       </listitem>
       <listitem>
        <para>
         <guimenu>Enable Registry</guimenu>: The cluster can be built with
         Insecure Docker Registry service.
        </para>
       </listitem>
       <listitem>
        <para>
         <guimenu>Disable TLS</guimenu>: Switch off the SSL protocol for the cluster.
        </para>
       </listitem>
      </itemizedlist>
     </step>
    </substeps>
   </step>
   <step>
    <para>
     In the <guimenu>Node Spec</guimenu> section:
    </para>
    <substeps>
     <step>
      <para>
       Choose the <guimenu>Image</guimenu> you have created in <xref
        linkend="sec.deploy.kubernetes.cli" xrefstyle="select:label"/>, <xref
         linkend="step.deploy.k8s.img.create" xrefstyle="select:label"/>.
      </para>
     </step>
     <step>
      <para>
       Choose a <guimenu>Keypair</guimenu>.
      </para>
     </step>
     <step>
      <para>
       Choose the <guimenu>Flavor</guimenu> you have created in <xref
        linkend="sec.deploy.kubernetes.cli" xrefstyle="select:label"/>, <xref
        linkend="step.deploy.k8s.flavor.create" xrefstyle="select:label"/>.
       It will be used for the minion nodes.
      </para>
     </step>
     <step>
      <para>
       Choose the same flavor as <guimenu>Master Flavor</guimenu>.
       It will be used for the master node.
      </para>
     </step>
     <step>
      <para>
       As <guimenu>Volume Driver</guimenu>, choose <literal>Cinder</literal>.
      </para>
     </step>
     <step>
      <para>
       As <guimenu>Docker Storage Driver</guimenu>, choose
       <literal>Device Mapper</literal>.
      </para>
     </step>
     <step>
      <para>
       Specify the <guimenu>Docker Volume Size (GB)</guimenu>. For example:
       <literal>5</literal>
      </para>
     </step>
    </substeps>
   </step>
   <step>
    <para>
     In the <guimenu>Network</guimenu> section:
    </para>
    <substeps>
     <step>
      <para>
       As <guimenu>Network Driver</guimenu>, choose <literal>Flannel</literal>.
      </para>
     </step>
     <step>
      <para>
       Leave the <guimenu>HTTP Proxy</guimenu>, <guimenu>HTTPS Proxy</guimenu>, and
       <guimenu>No Proxy</guimenu> boxes empty or enter the respective addresses
       to use.
      </para>
     </step>
     <step>
      <para>
       As <guimenu>External Network ID</guimenu>, enter <literal>floating</literal>.
       The network <literal>floating</literal> will be used to connect to the cluster
       template you are creating.
      </para>
     </step>
     <step>
      <para>
       Leave the <guimenu>Fixed Network</guimenu> and <guimenu>Fixed Subnet</guimenu>
       boxes empty.
      </para>
     </step>
     <step>
      <para>
       Enter the <guimenu>DNS</guimenu> server to use for this cluster template.
       For example: <literal>8.8.8.8.</literal>
      </para>
     </step>
     <step>
      <para>
       To deploy the cluster with a load balancer service in front for the cluster
       services, activate <guimenu>Master LB</guimenu>.
      </para>
     </step>
     <step>
      <para>
       To assign floating IP addresses to the nodes in the cluster, activate
       <guimenu>Floating IP</guimenu>.
      </para>
     </step>
    </substeps>
   </step>
   <step>
    <para>
     Confirm your changes to create the cluster template.
    </para>
   </step>
   </procedure>

   <procedure xml:id="pro.deploy.cluster.ui">
    <title>Creating a Cluster in &cloud; &dash;</title>
    <para>Based on the cluster template you have created in <xref
    linkend="pro.deploy.cluster.templ.ui"/>, you can now create a Kubernetes
    cluster.</para>
    <step>
     <para>
      Log in to &cloud; &dash; and select a project from the drop-down box at
      the top-level row.
     </para>
    </step>
    <step>
     <para>
      Click <menuchoice>
       <guimenu>Container Infra</guimenu>
       <guimenu>Clusters</guimenu>
       <guimenu>Create Cluster</guimenu>
      </menuchoice>.
     </para>
     <para>
      The <guimenu>Create Cluster</guimenu> dialog opens, showing the following
      sections: <guimenu>Info</guimenu>, <guimenu>Size</guimenu>,
      and <guimenu>Misc</guimenu>.
     </para>
    </step>
    <step>
     <para>
      In the <guimenu>Info</guimenu> section:</para>
       <substeps>
        <step>
         <para>
          Enter a <guimenu>Cluster Name</guimenu>.
         </para>
        </step>
        <step>
         <para>
          From the <guimenu>Cluster Template</guimenu> list, select the template
          you have created in <xref linkend="pro.deploy.cluster.templ.ui"/>.
         </para>
        </step>
       </substeps>
      </step>
      <step>
       <para>
        In the <guimenu>Size</guimenu> section, enter the number of master
        nodes and minion nodes you want the cluster to have. For example:
        <literal>1</literal> and  <literal>2</literal>.
       </para>
      </step>
     <step>
      <para>
       In the <guimenu>Misc</guimenu> section, you can optionally specify a custom
       URL for node discovery and a <guimenu>Timeout</guimenu> for cluster creation,
       if wanted. The default is no timeout.
      </para>
     </step>
    <step>
     <para>
      Confirm your changes to create the cluster.</para>
    </step>
   </procedure>
  </sect1>
  <sect1 xml:id="sec.deploy.kubernetes.without">
   <title>Deploying a Kubernetes Cluster Without Internet Access</title>
   <para>
    In specific scenarios, you may need to deploy a Kubernetes cluster
    without access to Internet. For those cases, you need to set up a custom
    Insecure Docker Registry and use no discovery URL. You can do this either
    from command line (as described below) or from the &cloud; &dash;.
   </para>
   <procedure>
    <step>
     <para>
      In a shell, source the &ostack; RC file for the project that you want to
      upload an image to. For details, refer to
      <link xlink:href="http://docs.openstack.org/user-guide/common/cli_set_environment_variables_using_openstack_rc.html"/>.
     </para>
    </step>
    <step>
<para>
      Create a cluster template as shown in
      <xref linkend="sec.deploy.kubernetes.cli"
      xrefstyle="select:lable"/>,
      <xref linkend="step.deploy.k8s.templ.create"/>, but add the options
      <option>--registry-enabled</option> and <option>--labels</option>. The
      <option>registry_url</option> must include the protocol, e.g.
      http://<replaceable>URL</replaceable>. For example:
     </para>
<screen>magnum cluster-template-create --name k8s_template_reg_enabled \
 [...]
 --registry-enabled
 --labels registry_url=http://<replaceable>192.168.255.10/srv/files</replaceable></screen>
    </step>
    <step>
     <para>
      Create a cluster as shown in <xref linkend="sec.deploy.kubernetes.cli"
      xrefstyle="select:lable"/>, <xref linkend="step.deploy.k8s.cluster.create"/>,
      but with static IP configuration and setting the option
      <option>--discovery-url</option> to <literal>none</literal>. For example:
     </para>
<screen>magnum cluster-create --name k8s_cluster_without \
  --cluster-template k8s_template_reg_enabled \
  [...]
  --discovery-url none</screen>
    </step>
   </procedure>
  </sect1>
 </chapter>
 <xi:include href="suppl_docupdates.xml"/>
</book>
