 <?xml version="1.0"?>
<!DOCTYPE section [
 <!ENTITY % entities SYSTEM "entities.ent"> %entities;
]>
<section xmlns="http://docbook.org/ns/docbook" xmlns:xi="http://www.w3.org/2001/XInclude"
  xmlns:xlink="http://www.w3.org/1999/xlink" xml:id="ts_magnum">
 <title>Magnum 服务故障排除</title>
 <para>
  Magnum 服务的故障情形和故障排除方案。Magnum Service 提供容器编配引擎如 Docker Swarm，Kubernetes 和 Apache Mesos 作为一流资源。您可以使用本指南来帮助解决 Magnum 服务的已知问题和故障。
 </para>
 <section xml:id="idg-all-operations-troubleshooting-ts_magnum-xml-6">
  <title>Magnum 集群无法创建</title>
  <para>
   通常，小尺寸集群需要大约 3-5 分钟才能建立。如果集群建立需要更长时间，您可能需要进行故障排除，而不用等待状态在超时后转为 CREATE_FAILED。
  </para>
  <orderedlist>
   <listitem>
    <para>
     使用 <literal>heat resource-list -n2</literal> 来确认哪个 Heat 堆栈资源卡在 <emphasis role="bold">CREATE_IN_PROGRESS</emphasis> 中。
    </para>
    <note>
     <para>
      主 Heat 堆栈有嵌套堆栈，一个用于 kubemaster，另一个用于 kubeminion。这些堆栈作为 <emphasis>OS::Heat::ResourceGroup</emphasis> 类型的资源（在父堆栈中）和 <emphasis>file:///...</emphasis>  在嵌套堆栈中可见。如果在嵌套堆栈中任何资源保持 <emphasis>CREATE_IN_PROGRESS</emphasis> 状态，则资源的整体状态将为 <emphasis>CREATE_IN_PROGRESS</emphasis>。
     </para>
    </note>
<screen>$ heat resource-list -n2 22385a42-9e15-49d9-a382-f28acef36810
+-------------------------------+--------------------------------------+--------------------------------------+--------------------+----------------------+------------------------------------------------------------------+
| resource_name                 | physical_resource_id                 | resource_type                        | resource_status    | updated_time         | stack_name                                                       |
+-------------------------------+--------------------------------------+--------------------------------------+--------------------+----------------------+------------------------------------------------------------------+
| api_address_floating_switch   | 06b2cc0d-77f9-4633-8d96-f51e2db1faf3 | Magnum::FloatingIPAddressSwitcher    | CREATE_COMPLETE    | 2017-04-10T21:25:10Z | my-cluster-z4aquda2mgpv                                          |
. . .

| fixed_subnet                  | d782bdf2-1324-49db-83a8-6a3e04f48bb9 | OS::Neutron::Subnet                  | CREATE_COMPLETE    | 2017-04-10T21:25:11Z | my-cluster-z4aquda2mgpv                                          |
| kube_masters                  | f0d000aa-d7b1-441a-a32b-17125552d3e0 | OS::Heat::ResourceGroup              | CREATE_IN_PROGRESS | 2017-04-10T21:25:10Z | my-cluster-z4aquda2mgpv                                          |
| 0                             | b1ff8e2c-23dc-490e-ac7e-14e9f419cfb6 | file:///opt/s...ates/kubemaster.yaml | CREATE_IN_PROGRESS | 2017-04-10T21:25:41Z | my-cluster-z4aquda2mgpv-kube_masters-utyggcbucbhb                |
| kube_master                   | 4d96510e-c202-4c62-8157-c0e3dddff6d5 | OS::Nova::Server                     | CREATE_IN_PROGRESS | 2017-04-10T21:25:48Z | my-cluster-z4aquda2mgpv-kube_masters-utyggcbucbhb-0-saafd5k7l7im |
. . .</screen>
   </listitem>
   <listitem>
    <para>
     如果某些原生 OpenStack 资源（如 <emphasis role="bold">OS::Nova::Server</emphasis> 或 <emphasis role="bold">OS::Neutron::Router</emphasis> 上的堆栈创建失败，请对相应的服务进行故障排除。此类错误通常不会导致超时，并且集群会快速变为 <emphasis role="bold">CREATE_FAILED</emphasis> 状态。可以通过 <literal>magnum cluster-show</literal> 命令来检查 Heat 报告的失败的根本原因。
    </para>
   </listitem>
   <listitem>
    <para>
     如果堆栈创建在 OS::Heat::WaitCondition 类型的资源上停止，则 Heat 未从集群 VM 接收有关引导序列完成的通知。找到 <emphasis role="bold">OS::Nova::Server</emphasis> 类型的相应资源，并使用其 <emphasis role="bold">physical_resource_id</emphasis> 获取有关 VM 的信息（其状态应为 <emphasis role="bold">CREATE_COMPLETE</emphasis>）
    </para>
<!-- ERROR -->
<screen>$ nova show 4d96510e-c202-4c62-8157-c0e3dddff6d5
+--------------------------------------+---------------------------------------------------------------------------------------------------------------+
| Property                             | Value                                                                                                         |
+--------------------------------------+---------------------------------------------------------------------------------------------------------------+
| OS-DCF:diskConfig                    | MANUAL                                                                                                        |
| OS-EXT-AZ:availability_zone          | nova                                                                                                          |
| OS-EXT-SRV-ATTR:host                 | comp1                                                                                                         |
| OS-EXT-SRV-ATTR:hypervisor_hostname  | comp1                                                                                                         |
| OS-EXT-SRV-ATTR:instance_name        | instance-00000025                                                                                             |
| OS-EXT-STS:power_state               | 1                                                                                                             |
| OS-EXT-STS:task_state                | -                                                                                                             |
| OS-EXT-STS:vm_state                  | active                                                                                                        |
| OS-SRV-USG:launched_at               | 2017-04-10T22:10:40.000000                                                                                    |
| OS-SRV-USG:terminated_at             | -                                                                                                             |
| accessIPv4                           |                                                                                                               |
| accessIPv6                           |                                                                                                               |
| config_drive                         |                                                                                                               |
| created                              | 2017-04-10T22:09:53Z                                                                                          |
| flavor                               | m1.small (2)                                                                                                  |
| hostId                               | eb101a0293a9c4c3a2d79cee4297ab6969e0f4ddd105f4d207df67d2                                                      |
| id                                   | 4d96510e-c202-4c62-8157-c0e3dddff6d5                                                                          |
| image                                | fedora-atomic-26-20170723.0.x86_64 (4277115a-f254-46c0-9fb0-fffc45d2fd38)                                     |
| key_name                             | testkey                                                                                                       |
| metadata                             | {}                                                                                                            |
| name                                 | my-zaqshggwge-0-sqhpyez4dig7-kube_master-wc4vv7ta42r6                                                         |
| os-extended-volumes:volumes_attached | [{"id": "24012ce2-43dd-42b7-818f-12967cb4eb81"}]                                                              |
| private network                      | 10.0.0.14, 172.31.0.6                                                                                         |
| progress                             | 0                                                                                                             |
| security_groups                      | my-cluster-z7ttt2jvmyqf-secgroup_base-gzcpzsiqkhxx, my-cluster-z7ttt2jvmyqf-secgroup_kube_master-27mzhmkjiv5v |
| status                               | ACTIVE                                                                                                        |
| tenant_id                            | 2f5b83ab49d54aaea4b39f5082301d09                                                                              |
| updated                              | 2017-04-10T22:10:40Z                                                                                          |
| user_id                              | 7eba6d32db154d4790e1d3877f6056fb                                                                              |
+--------------------------------------+---------------------------------------------------------------------------------------------------------------+</screen>
   </listitem>
   <listitem>
    <para>
     使用主 VM 的浮动 IP 登录第一个主节点。使用下面适当的用户名作为您的 VM 类型。登录不应要求密码，因为 VM 应已安装 ssh 公钥。
    </para>
    <informaltable colsep="1" rowsep="1">
     <tgroup cols="2">
      <colspec colname="c1" colnum="1" colwidth="1.0*"/>
      <colspec colname="c2" colnum="2" colwidth="1.0*"/>
      <thead>
       <row>
        <entry align="center">VM 类型</entry>
        <entry align="center">用户名</entry>
       </row>
      </thead>
      <tbody>
       <row>
        <entry>Fedora Atomic 上的 Kubernetes 或 Swarm</entry>
        <entry align="center">fedora</entry>
       </row>
       <row>
        <entry>CoreOS 上的 Kubernetes</entry>
        <entry align="center">core</entry>
       </row>
       <row>
        <entry>Ubuntu 上的 Mesos</entry>
        <entry align="center">ubuntu</entry>
       </row>
      </tbody>
     </tgroup>
    </informaltable>
   </listitem>
   <listitem>
    <para>
     有用的诊断命令
    </para>
    <itemizedlist>
     <listitem>
      <para>
       Fedora Atomic 上的 Kubernetes 集群
      </para>
<screen>sudo journalctl --system
sudo journalctl -u cloud-init.service
sudo journalctl -u etcd.service
sudo journalctl -u docker.service
sudo journalctl -u kube-apiserver.service
sudo journalctl -u kubelet.service
sudo journalctl -u wc-notify.service</screen>
     </listitem>
     <listitem>
      <para>
       CoreOS 上的 Kubernetes 集群
      </para>
<screen>sudo journalctl --system
sudo journalctl -u oem-cloudinit.service
sudo journalctl -u etcd2.service
sudo journalctl -u containerd.service
sudo journalctl -u flanneld.service
sudo journalctl -u docker.service
sudo journalctl -u kubelet.service
sudo journalctl -u wc-notify.service </screen>
     </listitem>
     <listitem>
      <para>
       Fedora Atomic 上的 Swarm 集群
      </para>
<screen>sudo journalctl --system
sudo journalctl -u cloud-init.service
sudo journalctl -u docker.service
sudo journalctl -u swarm-manager.service
sudo journalctl -u wc-notify.service</screen>
     </listitem>
     <listitem>
      <para>
       Ubuntu 上的 Mesos 集群
      </para>
<screen>sudo less /var/log/syslog
sudo less /var/log/cloud-init.log
sudo less /var/log/cloud-init-output.log
sudo less /var/log/os-collect-config.log
sudo less /var/log/marathon.log
sudo less /var/log/mesos-master.log</screen>
     </listitem>
    </itemizedlist>
   </listitem>
  </orderedlist>
 </section>
</section>
