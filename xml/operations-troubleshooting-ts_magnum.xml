<?xml version="1.0"?>
<!DOCTYPE section [
 <!ENTITY % entities SYSTEM "entity-decl.ent"> %entities;
]>
<section xmlns="http://docbook.org/ns/docbook" xmlns:xi="http://www.w3.org/2001/XInclude"
  xmlns:xlink="http://www.w3.org/1999/xlink" xml:id="ts-magnum">
 <title>Troubleshooting Magnum Service</title>
 <para>
  Troubleshooting scenarios with resolutions for the Magnum service. Magnum
  Service provides container orchestration engines such as Docker Swarm,
  Kubernetes, and Apache Mesos available as first class resources. You can use
  this guide to help with known issues and troubleshooting of Magnum services.
 </para>
 <section xml:id="idg-all-operations-troubleshooting-ts-magnum-xml-6">
  <title>Magnum cluster fails to create</title>
  <para>
   Typically, small size clusters need about 3-5 minutes to stand up. If
   cluster stand up takes longer, you may proceed with troubleshooting, not
   waiting for status to turn to CREATE_FAILED after timing out.
  </para>
  <orderedlist>
   <listitem>
    <para>
     Use <literal>heat resource-list <emphasis>STACK-ID</emphasis></literal> to
     identify which Heat stack resource is stuck in
     <emphasis role="bold">CREATE_IN_PROGRESS</emphasis>.
    </para>
    <note>
     <para>
      The main Heat stack has nested stacks, one for kubemaster(s) and one
      for kubeminion(s). These stacks are visible as resources of type
      <emphasis>OS::Heat::ResourceGroup</emphasis> (in parent stack) and
      <emphasis>file:///...</emphasis> in nested stack. If any resource
      remains in <emphasis>CREATE_IN_PROGRESS</emphasis> state within the
      nested stack, the overall state of the resource will be
      <emphasis>CREATE_IN_PROGRESS</emphasis>.
     </para>
    </note>
<screen>$ heat resource-list -n2 22385a42-9e15-49d9-a382-f28acef36810
+-------------------------------+--------------------------------------+--------------------------------------+--------------------+----------------------+------------------------------------------------------------------+
| resource_name                 | physical_resource_id                 | resource_type                        | resource_status    | updated_time         | stack_name                                                       |
+-------------------------------+--------------------------------------+--------------------------------------+--------------------+----------------------+------------------------------------------------------------------+
| api_address_floating_switch   | 06b2cc0d-77f9-4633-8d96-f51e2db1faf3 | Magnum::FloatingIPAddressSwitcher    | CREATE_COMPLETE    | 2017-04-10T21:25:10Z | my-cluster-z4aquda2mgpv                                          |
. . .

| fixed_subnet                  | d782bdf2-1324-49db-83a8-6a3e04f48bb9 | OS::Neutron::Subnet                  | CREATE_COMPLETE    | 2017-04-10T21:25:11Z | my-cluster-z4aquda2mgpv                                          |
| kube_masters                  | f0d000aa-d7b1-441a-a32b-17125552d3e0 | OS::Heat::ResourceGroup              | CREATE_IN_PROGRESS | 2017-04-10T21:25:10Z | my-cluster-z4aquda2mgpv                                          |
| 0                             | b1ff8e2c-23dc-490e-ac7e-14e9f419cfb6 | file:///opt/s...ates/kubemaster.yaml | CREATE_IN_PROGRESS | 2017-04-10T21:25:41Z | my-cluster-z4aquda2mgpv-kube_masters-utyggcbucbhb                |
| kube_master                   | 4d96510e-c202-4c62-8157-c0e3dddff6d5 | OS::Nova::Server                     | CREATE_IN_PROGRESS | 2017-04-10T21:25:48Z | my-cluster-z4aquda2mgpv-kube_masters-utyggcbucbhb-0-saafd5k7l7im |
. . .</screen>
   </listitem>
   <listitem>
    <para>
     If stack creation failed on some native OpenStack resource, like
     <emphasis role="bold"
            >OS::Nova::Server</emphasis> or
     <emphasis role="bold">OS::Neutron::Router</emphasis>, proceed with
     respective service troubleshooting. This type of error usually does not
     cause time out, and cluster turns into status
     <emphasis role="bold">CREATE_FAILED</emphasis> quickly. The underlying
     reason of the failure, reported by Heat, can be checked via the
     <literal>magnum cluster-show</literal> command.
    </para>
   </listitem>
   <listitem>
    <para>
     If stack creation stopped on resource of type OS::Heat::WaitCondition,
     Heat is not receiving notification from cluster VM about bootstrap
     sequence completion. Locate corresponding resource of type
     <emphasis role="bold">OS::Nova::Server</emphasis> and use its
     <emphasis role="bold">physical_resource_id</emphasis> to get information
     about the VM (which should be in status
     <emphasis role="bold">CREATE_COMPLETE</emphasis>)
    </para>
<screen>$ openstack server show 4d96510e-c202-4c62-8157-c0e3dddff6d5
+--------------------------------------+---------------------------------------------------------------------------------------------------------------+
| Property                             | Value                                                                                                         |
+--------------------------------------+---------------------------------------------------------------------------------------------------------------+
| OS-DCF:diskConfig                    | MANUAL                                                                                                        |
| OS-EXT-AZ:availability_zone          | nova                                                                                                          |
| OS-EXT-SRV-ATTR:host                 | comp1                                                                                                         |
| OS-EXT-SRV-ATTR:hypervisor_hostname  | comp1                                                                                                         |
| OS-EXT-SRV-ATTR:instance_name        | instance-00000025                                                                                             |
| OS-EXT-STS:power_state               | 1                                                                                                             |
| OS-EXT-STS:task_state                | -                                                                                                             |
| OS-EXT-STS:vm_state                  | active                                                                                                        |
| OS-SRV-USG:launched_at               | 2017-04-10T22:10:40.000000                                                                                    |
| OS-SRV-USG:terminated_at             | -                                                                                                             |
| accessIPv4                           |                                                                                                               |
| accessIPv6                           |                                                                                                               |
| config_drive                         |                                                                                                               |
| created                              | 2017-04-10T22:09:53Z                                                                                          |
| flavor                               | m1.small (2)                                                                                                  |
| hostId                               | eb101a0293a9c4c3a2d79cee4297ab6969e0f4ddd105f4d207df67d2                                                      |
| id                                   | 4d96510e-c202-4c62-8157-c0e3dddff6d5                                                                          |
| image                                | fedora-atomic-26-20170723.0.x86_64 (4277115a-f254-46c0-9fb0-fffc45d2fd38)                                     |
| key_name                             | testkey                                                                                                       |
| metadata                             | {}                                                                                                            |
| name                                 | my-zaqshggwge-0-sqhpyez4dig7-kube_master-wc4vv7ta42r6                                                         |
| os-extended-volumes:volumes_attached | [{"id": "24012ce2-43dd-42b7-818f-12967cb4eb81"}]                                                              |
| private network                      | 10.0.0.14, 172.31.0.6                                                                                         |
| progress                             | 0                                                                                                             |
| security_groups                      | my-cluster-z7ttt2jvmyqf-secgroup_base-gzcpzsiqkhxx, my-cluster-z7ttt2jvmyqf-secgroup_kube_master-27mzhmkjiv5v |
| status                               | ACTIVE                                                                                                        |
| tenant_id                            | 2f5b83ab49d54aaea4b39f5082301d09                                                                              |
| updated                              | 2017-04-10T22:10:40Z                                                                                          |
| user_id                              | 7eba6d32db154d4790e1d3877f6056fb                                                                              |
+--------------------------------------+---------------------------------------------------------------------------------------------------------------+</screen>
   </listitem>
   <listitem>
    <para>
     Use the floating IP of the master VM to log into first master node. Use
     the appropriate username below for your VM type. Passwords should not be
     required as the VMs should have public ssh key installed.
    </para>
    <informaltable colsep="1" rowsep="1">
     <tgroup cols="2">
      <colspec colname="c1" colnum="1" colwidth="1.0*"/>
      <colspec colname="c2" colnum="2" colwidth="1.0*"/>
      <thead>
       <row>
        <entry align="center">VM Type</entry>
        <entry align="center">Username</entry>
       </row>
      </thead>
      <tbody>
       <row>
        <entry>Kubernetes or Swarm on Fedora Atomic</entry>
        <entry align="center">fedora</entry>
       </row>
       <row>
        <entry>Kubernetes on CoreOS</entry>
        <entry align="center">core</entry>
       </row>
       <row>
        <entry>Mesos on Ubuntu</entry>
        <entry align="center">ubuntu</entry>
       </row>
      </tbody>
     </tgroup>
    </informaltable>
   </listitem>
   <listitem>
    <para>
     Useful dianostic commands
    </para>
    <itemizedlist>
     <listitem>
      <para>
       Kubernetes cluster on Fedora Atomic
      </para>
<screen>sudo journalctl --system
sudo journalctl -u cloud-init.service
sudo journalctl -u etcd.service
sudo journalctl -u docker.service
sudo journalctl -u kube-apiserver.service
sudo journalctl -u kubelet.service
sudo journalctl -u wc-notify.service</screen>
     </listitem>
     <listitem>
      <para>
       Kubernetes cluster on CoreOS
      </para>
<screen>sudo journalctl --system
sudo journalctl -u oem-cloudinit.service
sudo journalctl -u etcd2.service
sudo journalctl -u containerd.service
sudo journalctl -u flanneld.service
sudo journalctl -u docker.service
sudo journalctl -u kubelet.service
sudo journalctl -u wc-notify.service </screen>
     </listitem>
     <listitem>
      <para>
       Swarm cluster on Fedora Atomic
      </para>
<screen>sudo journalctl --system
sudo journalctl -u cloud-init.service
sudo journalctl -u docker.service
sudo journalctl -u swarm-manager.service
sudo journalctl -u wc-notify.service</screen>
     </listitem>
     <listitem>
      <para>
       Mesos cluster on Ubuntu
      </para>
<screen>sudo less /var/log/syslog
sudo less /var/log/cloud-init.log
sudo less /var/log/cloud-init-output.log
sudo less /var/log/os-collect-config.log
sudo less /var/log/marathon.log
sudo less /var/log/mesos-master.log</screen>
     </listitem>
    </itemizedlist>
   </listitem>
  </orderedlist>
 </section>
</section>
