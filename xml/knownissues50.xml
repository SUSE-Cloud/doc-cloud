<?xml version="1.0"?>
<?xml-stylesheet href="urn:x-suse:xslt:profiling:docbook51-profile.xsl"
 type="text/xml"
 title="Profiling step"?>
<!DOCTYPE section [
 <!ENTITY % entities SYSTEM "entities.ent"> %entities;
]>

<section xmlns="http://docbook.org/ns/docbook" xmlns:xi="http://www.w3.org/2001/XInclude" xmlns:xlink="http://www.w3.org/1999/xlink" xml:id="knownissues50">
 <title>Known Issues in this Release</title>
 <section xml:id="BUGZILLA-1068107">
  <title>Upgrading from &kw-hos-phrase; 4.0.4</title>
  <warning>
   <para>
    If your system is currently on version &kw-hos-phrase; 4.0.4 the correct
    upgrade path is to &kw-hos-phrase; 5.0.1
   </para>
  </warning>
 </section>
 <section xml:id="DOCS-4101">
  <title>OpenStack Security Advisory OSSA-2017-001</title>
  <para>
   The OpenStack Security Advisory OSSA-2017-001 which describes CVE-2017-2592
   was released on January 26, 2017. It discloses a security defect which
   causes Keystone tokens to be logged when a Neutron API call encounters
   unexpected error conditions. When this happens, the log entry that contains
   the token is propagated to the centralized logging facility. The result is
   that anyone with access to the centralized logging facility or the local log
   file containing the token could use the token to impersonate the owner of
   the token until the token expires. Tokens expire in 4 hours by default.
   Neutron is the only &kw-hos; service affected by this issue. The error
   conditions which cause this token to leak into the logs are not expected to
   happen frequently. However, customers should be aware of this risk.
   Customers may wish to further limit the access to the centralized logging
   facility and watch for suspicious activity that could indicate usage of a
   stolen Keystone token to impersonate another user. The Keystone token
   timeout value could also be changed but it is not recommended; testing would
   be required to ensure things still work in the customer environment if the
   token timeout value is reduced.
  </para>
 </section>
 <section xml:id="BUGZILLA-1061611">
  <title>&kw-hos; Directory Umask</title>
  <para>
   &kw-hos; directory umask is 0022. Changing this value may cause unstable
   internal communication in the &kw-hos; product and will negatively affect
   upgrade.
  </para>
 </section>
 <section xml:id="BUGZILLA-1060915">
  <title>Adding Top-Level Domains with Designate</title>
  <para>
   In &kw-hos-phrase; and earlier ( with designate V2.0.1 ) when using
   Designate to add Top-Level Domains (that controls who can create zones),
   there is a caching defect. When an entry is made to the tld list
   (<literal>openstack tld create --name abc.net</literal>), in order to take
   effect the designate-central on all the controllers needs to be restarted.
  </para>
<screen>$ sudo systemctl restart designate-central</screen>
 </section>
 <section xml:id="BUGZILLA-1057463">
  <title>Adding New Flavors After Upgrade</title>
  <para>
   After upgrading to &kw-hos-phrase; from &kw-hos-phrase; 4.0 you may
   experience an error when attempting to add new flavors.
  </para>
  <para>
   Example:
  </para>
<screen>$ openstack flavor create --ram 512 --vcpus 1 --disk 0 --ephemeral 0 --swap 0 --public m1.tinystu

Not all flavors have been migrated to the API database (HTTP 409) (Request-ID: req-bdae0a10-e6c3-42d4-9a58-b4438fcc4309)</screen>
  <para>
   This is caused by the time fields associated with the flavor not being
   updated correctly. You can view the time fields by selecting all from the
   instance_types table in the nova mysql database.
  </para>
<screen>$ sudo mysql
$ use nova
$ select * from instance_types;</screen>
  <para>
   The time fields <emphasis role="bold">created_at</emphasis>,
   <emphasis role="bold">updated_at</emphasis> or
   <emphasis role="bold">deleted_at</emphasis> may have invalid dates or not
   all of the fields are NULL.
  </para>
  <para>
   To resolve the new flavors issues the time fields must all be set to NULL.
  </para>
<screen>$ sudo mysql
$ use nova
$ update instance_types set created_at=NULL, updated_at=NULL, deleted_at=NULL;</screen>
  <para>
   Once the tables are updated, exit mysql and run the following command.
  </para>
<screen>$ sudo /opt/stack/service/nova-api/venv/bin/nova-manage --config-dir /opt/stack/service/nova-api/etc/nova db online_data_migrations</screen>
  <para>
   You can now create a new flavors in &kw-hos-phrase;.
  </para>
 </section>
 <section xml:id="DOCS-4188">
  <title>Using VSA as a Cinder Backend</title>
  <para>
   When using VSA as a Cinder backend it is necessary to make some changes to
   the kvm-hypervisor.conf.j2 file with the following steps:
  </para>
  <orderedlist>
   <listitem>
    <para>
     Log into the &clm;
    </para>
   </listitem>
   <listitem>
    <para>
     Edit the
     <literal>~/openstack/my_cloud/config/nova/kvm-hypervisor.conf.j2</literal>
     file.
    </para>
   </listitem>
   <listitem>
    <para>
     Remove the following lines under the libvert section:
    </para>
<screen>volume_use_multipath = True</screen>
   </listitem>
   <listitem>
    <para>
     Commit the configuration to the local git repo
    </para>
<screen>cd ~/openstack/ardana/ansible
git add -A
git commit -m "My config or other commit message"</screen>
   </listitem>
   <listitem>
    <para>
     Run the configuration processor
    </para>
<screen>cd ~/openstack/ardana/ansible
ansible-playbook -i hosts/localhost config-processor-run.yml</screen>
   </listitem>
   <listitem>
    <para>
     Use the playbook below to create a deployment directory
    </para>
<screen>cd ~/openstack/ardana/ansible
ansible-playbook i hosts/localhost ready-deployment.yml</screen>
   </listitem>
   <listitem>
    <para>
     Run the Nova reconfigure playbook
    </para>
<screen>cd ~/scratch/ansible/next/hos/ansible
ansible-playbook -i hosts/verb_hosts nova-reconfigure.yml</screen>
   </listitem>
  </orderedlist>
  <para>
   This step is not needed if Cinder backend is HP 3PAR.
  </para>
  <para>
   When using storage solutions other than 3PAR or VSA, please check with the
   storage provider whether the <emphasis role="bold">volume_use_multipath =
   True</emphasis> line should be removed.
  </para>
 </section>
 <section xml:id="DOCS-4187">
  <title>Vertica Log Rotation</title>
  <para>
   Vertica database logs are not rotated automatically for non-primary cluster
   nodes. As a result, Vertica logs on those nodes may grow continuously,
   eventually filling the filesystem to its capacity. This issue can be patched
   by logging into each node with Vertica installed and run the following
   command.
  </para>
  <note>
   <para>
    Vertica nodes are all listed as members of the FND-VDB group in the
    deployer's
    <literal>~/scratch/ansible/next/hos/ansible/hosts/verb_hosts</literal> file.
   </para>
  </note>
<screen>echo -e '#!/bin/sh
logrotate /opt/vertica/config/logrotate_base.conf' | sudo tee /etc/cron.daily/vertica-logrotate; sudo chmod 755 /etc/cron.daily/vertica-logrotate</screen>
  <para>
   While this step is not necessary on the Vertica cluster's primary node, it
   is also not harmful. This command is idempotent and future-proof with regard
   to upgrading to later &kw-hos; versions.
  </para>
 </section>
 <section xml:id="DOCS-4124">
  <title>VPNaaS does not support SHA-384 and SHA-512</title>
  <para>
   The VPNaaS plugin limits the ipsec and ike auth algorithm to
   <emphasis role="bold">SHA-1</emphasis> and
   <emphasis role="bold">SHA-256</emphasis>. If you attempt to add a new driver
   (for example, a hardware VPN gateway), and the new driver supports
   additional auth algorithm, such as
   <emphasis role="bold">SHA2-384</emphasis>,
   <emphasis role="bold">SHA2-512</emphasis>, it cannot be integrated with the
   current VPNaaS plugin.
  </para>
 </section>
 <section xml:id="unknown">
  <title>SLES - No kdump initial ramdisk found</title>
  <para>
   If you are deploying SLES 12 nodes and you see the following exception
   during the initial boot of those nodes:
  </para>
<screen>No kdump initial ramdisk found....</screen>
  <para>
   No further action is required. Please see:
   <link xlink:href="https://www.suse.com/support/kb/doc.php?id=7018428"/>
   for additional information:
  </para>
   <note>
    <para>
     This exception may also occur by running <literal>journalctl -u
     kdump.service</literal>
    </para>
   </note>
 </section>
 <section xml:id="DOCS-3871">
  <title>Manual Cleaning</title>
  <para>
   Manual Cleaning (Raid Configuration) is not supported if the target disk
   type is SATA SSD.
  </para>
 </section>
 <section xml:id="DOCS-4145">
  <title>Transparent VLAN</title>
  <para>
   In &kw-hos-phrase; Transparent VLAN is not supported.
  </para>
 </section>
 <section xml:id="DOCS-4155">
  <title>Disaster Recovery Panel</title>
  <para>
   Although the Disaster Recovery panel can be viewed when logged in as an
   <emphasis role="bold">admin</emphasis> user, only the
   <emphasis role="bold">hlm_backup</emphasis> user is the supported user of
   this feature. The <emphasis role="bold">admin</emphasis> user is unable to
   view the default backup jobs or create new jobs. Therefore, all backup and
   recovery actions should be performed only when logged in as the
   <emphasis role="bold">hlm_backup</emphasis> user.
  </para>
 </section>
 <section xml:id="DOCS-4148">
  <title>Update ironic-conductor.conf.j2 file to be able to connect to Swift</title>
  <para>
   Ironic conductor uses credentials from the <literal>[swift]</literal>
   section of the config file, and falls back to
   <literal>[keystone_authtoken]</literal> section if no credentials are
   specified in the <literal>[swift]</literal> section.
  </para>
  <para>
   As a work-around, add the following to the
   <literal>/ironic-conductor.conf.j2</literal> file.
  </para>
<screen>auth_type = password
auth_url = {{ ironic_cnd_admin.identity_uri }}/v3
username = {{ ironic_swift.glance_user }}
password = {{ ironic_swift.glance_password}}
project_name = {{ ironic_swift.glance_tenant }}
default_domain_name = default</screen>
 </section>
 <section xml:id="BUGZILLA-1049731">
  <title>Attaching a volume with Device Name in Horizon</title>
  <para>
   In Horizon the screen for attaching a volume to an instance requires a
   <emphasis role="bold">Device Name</emphasis>. This field is editable but
   cannot be empty or it will revert to the default value.
  </para>
  <para>
   To attach a volume to an instance without specifying a Device Name, use the
   command line (CLI).
  </para>
 </section>
 <section xml:id="DOCS-4157">
  <title>Ironic: ILO Firmware</title>
  <para>
   In &kw-hos-phrase; the ILO Firmware version needs to be minimum 2.30 for
   agent_ilo driver.
  </para>
 </section>
 <section xml:id="DOCS-4149">
  <title>Ironic 'shreading' (cleaning) may take a long time depending on the disk size/performance.</title>
  <para>
   During deployment, with <literal>automated_cleaning</literal> set to True
   and when the Nova instance is deleted, it may take an unusually long time to
   finish or may even seem as though it is hung depending on the disk
   performance/size.
  </para>
  <para>
   Example Log msg in the ironic-conductor.log:
  </para>
<screen>2017-05-25 18:08:37.648 40436 DEBUG ironic.drivers.modules.agent_base_vendor
[req-89ff4f08-c13f-49f9-b761-f0e0a5611cd1 - - - - -] Cleaning command status for node
84f57c65-caf5-4a4a-941f-82d27afc93e6 on step {u'priority': 10, u'interface': u'deploy',
u'reboot_requested': False, u'abortable': True, u'step': u'erase_devices'}: None
continue_cleaning /opt/stack/venv/ironic-20170522T163729Z/lib/python2.7/site-packages/ironic/drivers/modules/agent_base_vendor.py:347</screen>
  <para>
   For more information on OpenStack Ironic Node cleaning, see:
   <link xlink:href="https://docs.openstack.org/developer/ironic/deploy/cleaning.html">https://docs.openstack.org/developer/ironic/deploy/cleaning.html</link>
  </para>
 </section>
 <section xml:id="DOCS-4177">
  <title>Ironic Multi-Tenancy Conversion</title>
  <para>
   Ironic Multi-tenancy is only supported as a fresh install of
   &kw-hos-phrase;. An existing Ironic flat environment to multi-tenancy
   environment conversion is not supported.
  </para>
 </section>
 <section xml:id="DOCS-4146">
  <title>Do not enable HEAT Auditing</title>
  <para>
   Audit log is disabled by default and cannot be enabled due to a Keystone
   middleware defect in Newton.
  </para>
 </section>
 <section xml:id="DOCS-4163">
  <title>&opscon; Notification Panel</title>
  <para>
   If there are no Alarm notification methods configured, the &opscon;
   notifications tab will not allow creation of new notification methods. It is
   recommended to not delete the default notification method if no other
   methods have been created.
  </para>
  <para>
   You can work around this issue by creating a new notification method using
   the
   <link xlink:href="https://docs.openstack.org/cli-reference/monasca.html">Monasca
   command-line</link>, after which the UI will function again.
  </para>
 </section>
 <section xml:id="DOCS-4143">
  <title>LBaaS</title>
  <para>
   LBaaS Hot-Plugin is not supported with DPDK.
  </para>
  <para>
   LBaaS <literal>load_balancer_expiry_age</literal> value in the housekeeping
   section of the configuration for octavia-housekeeping service needs to be
   configured appropriately to the default <literal>value=604800</literal>.
  </para>
 </section>
 <section xml:id="DOCS-4185">
  <title>F5 Loadbalancer Support</title>
  <para>
   If you are planning to upgrade from &kw-hos-phrase; 3.0 or
   &kw-hos-phrase; 4.0, please contact F5 and HPE PointNext to determine which
   F5 drivers have been certified for use with &productname;. Loading
   drivers not certified by HPE may result in catastrophic failure of your
   cloud deployment. The last tested versions are 8.0.8 for &kw-hos; 3.x and
   9.0.3 for &kw-hos; 4.x . More information is expected in 4th quarter 2017,
   including the correct drivers for &kw-hos; 5.x.
  </para>
 </section>
 <section xml:id="VNETCORE-2909">
  <title>VMs fail to get an IP after upgrade</title>
  <para>
   When a VMs failed to get an IP after an upgrade, apply the following
   work-around:
  </para>
  <para>
   After running the rabbitmq-disaster-recovery on a controller node, check the
   ownership of <literal>/var/run/neutron/lock/neutron-iptables</literal>. If
   it is not owned by <emphasis role="bold">neutron:neutron</emphasis>, use the
   chown command to change the ownership to
   <emphasis role="bold">neutron:neutron</emphasis>:
  </para>
<screen>sudo chown neutron:neutron /var/run/neutron/lock/neutron-iptables </screen>
 </section>
 <section xml:id="IRONIC-634">
  <title>Persistent failures during the early phase of boot</title>
  <para>
   If you are experiencing intermittent or persistent failures during the early
   phase of boot process for certain classes of machines (e.g. DL360 Gen8 and
   DL160 Gen9), the likely issue is DHCP timeouts due to slow convergence of
   STP.
  </para>
  <para>
   You can work around this issue by enabling rapid spanning tree protocol on
   the switch. If this is not an option due to conservative loop detection
   strategies, rebuild ipxe images with higher DHCP timeouts, or alternatively
   maintain link status to the server NICs during power cycles.
  </para>
 </section>
 <section xml:id="MAG-69">
  <title>Swarm cluster behind proxy fails</title>
  <para>
   Swarm cluster is not support behind a proxy even with -http(s)-proxy
   parameters specified.
  </para>
 </section>
 <section xml:id="VNETCORE-2274">
  <title>Neutron Quota Failures</title>
  <para>
   The options <literal>--security_group</literal> and
   <literal>--security_group_rule</literal> are not valid command options for
   the <literal>neutron quota-update</literal> command. Please use following
   options:
  </para>
  <itemizedlist>
   <listitem>
    <para>
     -security-group
    </para>
   </listitem>
   <listitem>
    <para>
     security-group-rule
    </para>
   </listitem>
  </itemizedlist>
 </section>
 <section xml:id="DOCS-4122">
  <title>Starting/Stopping Swift</title>
  <para>
   The command line (CLI) cmd, <literal>swift-init</literal> is not supported.
   To start/stop Swift use the Ansible playbooks,
   <literal>swift-start.yml</literal> and <literal>swift-stop.yml</literal>.
  </para>
 </section>
 <section xml:id="HNOV-999">
  <title>Live Migrations</title>
  <para>
   Live Migration from non-&hlinux; nodes including SLES nodes to &hlinux; nodes
   are not supported.
  </para>
 </section>
</section>
