<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE chapter [
  <!ENTITY % entities SYSTEM "entity-decl.ent">
  %entities;
]>
<chapter xmlns="http://docbook.org/ns/docbook" xmlns:xi="http://www.w3.org/2001/XInclude" xmlns:xlink="http://www.w3.org/1999/xlink" version="5.0" xml:id="cha-depl-nostack">
  <title>Deploying the Non-&ostack; Components</title>
 <info>
  <dm:docmanager xmlns:dm="urn:x-suse:ns:docmanager">
    <dm:maintainer>fs</dm:maintainer>
    <dm:status>editing</dm:status>
    <dm:deadline/>
    <dm:priority/>
    <dm:translation>no</dm:translation>
    <dm:languages/>
  </dm:docmanager>
 </info>
  <para>
    In addition to &ostack; &barcl;s, &suse; &ostack; Cloud includes several components that can be configured using the appropriate  &crow; &barcl;s.
   </para>
   <sect1 xml:id="sec-depl-nostack-crowbar-tuning">
  <title>Tuning the Crowbar Service</title>
  <para>
    Crowbar is a self-referential &barcl; used for enabling other &barcl;s. By
    creating a &crow; proposal, you can modify the default number of threads
    and workers. This way, you can scale the admin
    server according to the actual usage or the number of available cores of
    the admin node.
  </para>
<figure>
        <title>The &crow; &barcl;: Raw Mode</title>
        <mediaobject>
          <imageobject role="fo">
       <imagedata fileref="crowbar_tuning_raw.png" width="100%" format="png"/>
      </imageobject>
      <imageobject role="html">
       <imagedata fileref="crowbar_tuning_raw.png" width="75%" format="png"/>
      </imageobject>
     </mediaobject>
    </figure>
  <para>
    To change the default settings, create a Crowbar proposal and switch to the
    <guimenu>Raw</guimenu> view. Adjust then the
    <systemitem>workers</systemitem> and <systemitem>threads</systemitem>
    values. The number of threads should be set to the number of available
    cores. The default number of workers should be increased to 3 if the
    graphical interface becomes slow. Save and apply the changes using the
    appropriate buttons.
  </para>
   </sect1>
    <sect1 xml:id="sec-depl-nostack-ntp">
  <title>Configuring the NTP Service</title>
  <para>
    The NTP service is responsible for keeping the clocks in your cloud servers
    in sync. Among other things, synchronized clocks ensure that the
    chef-client works properly. It also makes it easier to read logs from
    different nodes by correlating timestamps in them. The NTP component is deployed on the Administration Server automatically using the default settings. The NTP &barcl; can be used to specify IP addresses of the external NTP servers and assign specific roles to the desired nodes. The following parameter can be configured using the NTP &barcl;:
  </para>
  <variablelist>
    <varlistentry>
      <term>External servers</term>
      <listitem>
	<para>
	  A comma-separated list of IP addresses of external NTP servers.
	</para>
      </listitem>
    </varlistentry>
  </variablelist>
  <para>
    The NTP service consists of two different roles:
  </para>
  <variablelist>
   <varlistentry>
    <term><guimenu>ntp-server</guimenu>
    </term>
    <listitem>
     <para>
      A node that acts as an NTP server for NTP clients in your cloud. There
      can be more than one node with the ntp-server role in your cloud. In
      this scenario, the NTP server nodes can communicate with each other and the specified external servers to
      keep their time in sync.
     </para>
    </listitem>
   </varlistentry>
    <varlistentry>
    <term><guimenu>ntp-client</guimenu>
    </term>
    <listitem>
     <para>
      The <systemitem>ntp-client</systemitem> role can be assigned to any
      node. Nodes with the ntp-client role assigned to them keep their time in
      sync using NTP servers in your cloud.
     </para>
    </listitem>
   </varlistentry>
  </variablelist>
    </sect1>
    <sect1 xml:id="sec-depl-nostack-salt">
  <title>Installing and using Salt</title>
  <para>
    &crow; can setup Salt on the admin node to be able to use
    <command>salt-ssh</command> from the admin node.
  </para>
    <note>
      <para>
        Salt is not replacing &chef;. It can be used in parallel to &chef;
        to automate tasks on the nodes.
      </para>
    </note>
    <note>
      <para>
        Only <command>salt-ssh</command> can currently be used. Installing the
        Salt &barcl; does not setup a full Salt stack with Salt Master and
        Salt Minions.
      </para>
    </note>
    <para>
    <!-- See https://www.suse.com/documentation/sles-12/book_sle_deployment/data/cha_add-ons.html -->
    To be able to apply the Salt proposal, the <link xlink:href="https://www.suse.com/documentation/sles-12/book_sle_deployment/data/cha_add-ons.html">Advanced Systems Management Module</link>
    must be available on the node where the <literal>salt-ssh</literal> role will be applied
    (usually the admin node).
    After the Module is available, the &barcl; can be applied.
  </para>
  <para>
    From the node where the <literal>salt-ssh</literal> role is applied,
    Salt can be tested with:
  </para>
  <screen># salt-ssh '*' test.ping
crowbar:
    True
storage1:
    True
controller:
    True
  </screen>
  <para>
    To list the available nodes visible to <literal>salt-ssh</literal>, do:
  </para>
  <screen># salt-ssh -H
/etc/salt/roster:
    ----------
    controller:
        192.168.192.81
    crowbar:
        192.168.192.10
    storage1:
        192.168.192.82
  </screen>
    </sect1>
  </chapter>
