<?xml version="1.0"?>
<?xml-stylesheet href="urn:x-suse:xslt:profiling:docbook51-profile.xsl"
 type="text/xml"
 title="Profiling step"?>
<!DOCTYPE section [
 <!ENTITY % entities SYSTEM "entities.ent"> %entities;
]>
<section xmlns="http://docbook.org/ns/docbook" xmlns:xi="http://www.w3.org/2001/XInclude" xmlns:xlink="http://www.w3.org/1999/xlink" xml:id="sr-iov">
 <title>SR-IOV and PCI Passthrough Support</title>
 <para>
  &kw-hos; supports both single-root I/O virtualization (SR-IOV) and PCI
  passthrough (PCIPT). Both technologies provide for better network
  performance.
<!-- Commented in DITA original: -->
<!--SR-IOV allows you to bypass ...-->
 </para>
 <para>
  This improves network I/O, decreases latency, and reduces processor overhead.
 </para>
 <section>
  <title>SR-IOV</title>
  <para>
   A PCI-SIG Single Root I/O Virtualization and Sharing (SR-IOV) Ethernet
   interface is a physical PCI Ethernet NIC that implements hardware-based
   virtualization mechanisms to expose multiple virtual network interfaces that
   can be used by one or more virtual machines simultaneously. With SR-IOV
   based NICs, the traditional virtual bridge is no longer required. Each
   SR-IOV port is associated with a virtual function (VF).
  </para>
  <para>
   When compared with a PCI Passthtrough Ethernet interface, an SR-IOV Ethernet
   interface:
  </para>
  <itemizedlist>
   <listitem>
    <para>
     Provides benefits similar to those of a PCI Passthtrough Ethernet
     interface, including lower latency packet processing.
    </para>
   </listitem>
   <listitem>
    <para>
     Scales up more easily in a virtualized environment by providing multiple
     VFs that can be attached to multiple virtual machine interfaces.
    </para>
   </listitem>
   <listitem>
    <para>
     Shares the same limitations, including the lack of support for LAG, QoS,
     ACL, and live migration.
    </para>
   </listitem>
   <listitem>
    <para>
     Has the same requirements regarding the VLAN configuration of the access
     switches.
    </para>
   </listitem>
  </itemizedlist>
  <para>
   The process for configuring SR-IOV includes creating a VLAN provider network
   and subnet, then attaching VMs to that network.
  </para>
  <para>
   With SR-IOV based NICs, the traditional virtual bridge is no longer
   required. Each SR-IOV port is associated with a virtual function (VF)
  </para>
 </section>
 <section>
  <title>PCI passthrough Ethernet interfaces</title>
  <para>
   A passthrough Ethernet interface is a physical PCI Ethernet NIC on a compute
   node to which a virtual machine is granted direct access. PCI passthrough
   allows a VM to have direct access to the hardware without being brokered by
   the hypervisor. This minimizes packet processing delays but at the same time
   demands special operational considerations. For all purposes, a PCI
   passthrough interface behaves as if it were physically attached to the
   virtual machine. Therefore any potential throughput limitations coming from
   the virtualized environment, such as the ones introduced by internal copying
   of data buffers, are eliminated. However, by bypassing the virtualized
   environment, the use of PCI passthrough Ethernet devices introduces several
   restrictions that must be taken into consideration. They include:
  </para>
  <itemizedlist>
   <listitem>
    <para>
     no support for LAG, QoS, ACL, or host interface monitoring
    </para>
   </listitem>
   <listitem>
    <para>
     no support for live migration
    </para>
   </listitem>
   <listitem>
    <para>
     no access to the compute node's OVS switch
    </para>
   </listitem>
  </itemizedlist>
  <para>
   A passthrough interface bypasses the compute node's OVS switch completely,
   and is attached instead directly to the provider network's access switch.
   Therefore, proper routing of traffic to connect the passthrough interface to
   a particular tenant network depends entirely on the VLAN tagging options
   configured on both the passthrough interface and the access port on the
   switch (TOR).
  </para>
  <para>
   The access switch routes incoming traffic based on a VLAN ID, which
   ultimately determines the tenant network to which the traffic belongs. The
   VLAN ID is either explicit, as found in incoming tagged packets, or
   implicit, as defined by the access port's default VLAN ID when the incoming
   packets are untagged. In both cases the access switch must be configured to
   process the proper VLAN ID, which therefore has to be known in advance
  </para>
 </section>
 <section xml:id="_pci_passthrough">
  <title>Leveraging PCI Passthrough</title>
  <para>
   Two parts are necessary to leverage PCI passthrough on a &cloud; 8
   &compnode;: preparing the &compnode;, preparing &o_comp; and &o_img;.
  </para>
  <orderedlist>
   <listitem>
    <para>
     <emphasis role="bold">Preparing the &compnode;</emphasis>
    </para>
    <procedure>
     <step>
      <para>
       There should be no kernel drivers or binaries with direct access to the
       PCI device. If there are kernel modules, they should be blacklisted.
      </para>
      <para>
       For example, it is common to have a <literal>nouveau</literal> driver
       from when the node was installed. This driver is a graphics driver for
       Nvidia-based GPUs. It must be blacklisted as shown in this example.
      </para>
<screen>&prompt.ardana;echo 'blacklist nouveau' &gt;&gt; /etc/modprobe.d/nouveau-default.conf</screen>
      <para>
       The file location and its contents are important; the filename is your
       choice. Other drivers can be blacklisted in the same manner, possibly
       including Nvidia drivers.
      </para>
     </step>
     <step>
      <para>
       On the host, <literal>iommu_groups</literal> should be enabled. To check
       if IOMMU is enabled:
      </para>
<screen>&prompt.root;virt-host-validate
.....
QEMU: Checking if IOMMU is enabled by kernel
: WARN (IOMMU appears to be disabled in kernel. Add intel_iommu=on to kernel cmdline arguments)
.....</screen>
      <para>
       To modify the kernel cmdline as suggested in the warning, edit the file
       <filename>/etc/default/grub</filename> and append
       <literal>intel_iommu=on</literal> to the
       <literal>GRUB_CMDLINE_LINUX_DEFAULT</literal> variable. Then run
       <literal>update-bootloader</literal>.
      </para>
      <para>
       A reboot will be required for <literal>iommu_groups</literal> to be
       enabled.
      </para>
     </step>
     <step>
      <para>
       After the reboot, check that IOMMU is enabled:
      </para>
<screen>&prompt.root;virt-host-validate
.....
QEMU: Checking if IOMMU is enabled by kernel
: PASS
.....</screen>
     </step>
     <step>
      <para>
       Confirm IOMMU groups are available by finding the group associated with
       your PCI device (for example Nvidia GPU):
      </para>
<screen>&prompt.ardana;lspci -nn | grep -i nvidia
08:00.0 VGA compatible controller [0300]: NVIDIA Corporation GT218 [NVS 300] [10de:10d8] (rev a2)
08:00.1 Audio device [0403]: NVIDIA Corporation High Definition Audio Controller [10de:0be3] (rev a1)</screen>
      <para>
       In this example, <literal>08:00.0</literal> and
       <literal>08:00.1</literal> are addresses of the PCI device. The vendorID
       is <literal>10de</literal>. The productIDs are <literal>10d8</literal>
       and <literal>0be3</literal>.
      </para>
     </step>
     <step>
      <para>
       Confirm that the devices are available for passthrough:
      </para>
<screen>&prompt.ardana;ls -ld /sys/kernel/iommu_groups/*/devices/*08:00.?/
drwxr-xr-x 3 root root 0 Feb 14 13:05 /sys/kernel/iommu_groups/20/devices/0000:08:00.0/
drwxr-xr-x 3 root root 0 Feb 19 16:09 /sys/kernel/iommu_groups/20/devices/0000:08:00.1/</screen>
      <note>
       <para>
        With PCI passthrough, only an entire IOMMU group can be passed. Parts
        of the group cannot be passed. In this example, the IOMMU group is
        <literal>20</literal>.
       </para>
      </note>
     </step>
    </procedure>
   </listitem>
   <listitem>
    <para>
     <emphasis role="bold">Preparing &o_comp; and &o_img; for
     passthrough</emphasis>
    </para>
    <para>
     Information about configuring &o_comp; and &o_img; is available in the
     documentation at
     <link
     xlink:href="https://docs.openstack.org/nova/pike/admin/pci-passthrough.html"/>.
     Both <literal>nova-compute</literal> and <literal>nova-scheduler</literal>
     must be configured.
    </para>
   </listitem>
  </orderedlist>
 </section>
 <section>
  <title>Supported Intel 82599 Devices</title>
  <table xml:id="intel-82599-table" colsep="1" rowsep="1">
   <title>Intel 82599 devices supported with SRIOV and PCIPT</title>
   <tgroup cols="3" align="center">
    <colspec colname="c1" colnum="1" colwidth="1*"/>
    <colspec colname="c2" colnum="2" colwidth="1.34*"/>
    <colspec colname="c3" colnum="3" colwidth="2.23*"/>
    <thead>
     <row>
      <entry>Vendor</entry>
      <entry>Device</entry>
      <entry>Title</entry>
     </row>
    </thead>
    <tbody>
     <row>
      <entry>Intel Corporation</entry>
      <entry>10f8</entry>
      <entry>82599 10 Gigabit Dual Port Backplane Connection</entry>
     </row>
     <row>
      <entry>Intel Corporation</entry>
      <entry>10f9</entry>
      <entry>82599 10 Gigabit Dual Port Network Connection</entry>
     </row>
     <row>
      <entry>Intel Corporation</entry>
      <entry>10fb</entry>
      <entry>82599ES 10-Gigabit SFI/SFP+ Network Connection</entry>
     </row>
     <row>
      <entry>Intel Corporation</entry>
      <entry>10fc</entry>
      <entry>82599 10 Gigabit Dual Port Network Connection</entry>
     </row>
    </tbody>
   </tgroup>
  </table>
 </section>
 <section>
  <title>SRIOV PCIPT configuration</title>
  <para>
   If you plan to take advantage of SR-IOV support in &kw-hos; you will need to
   plan in advance to meet the following requirements:
  </para>
  <orderedlist>
   <listitem>
    <para>
     Use one of the supported NIC cards:
    </para>
    <itemizedlist>
     <listitem>
      <para>
       HP Ethernet 10Gb 2-port 560FLR-SFP+ Adapter (Intel Niantic). Product
       part number: 665243-B21 -- Same part number for the following card
       options:
      </para>
      <itemizedlist>
       <listitem>
        <para>
         FlexLOM card
        </para>
       </listitem>
       <listitem>
        <para>
         PCI slot adapter card
        </para>
       </listitem>
      </itemizedlist>
     </listitem>
<!-- Commented in DITA original: -->
<!-- DOCS-4025 <li>Mellanox 10Gb 2-port 546FLR-SFP+ Adapter (MT27520
connect-x3 Pro). Product part number: 779799-B21</li>-->
    </itemizedlist>
   </listitem>
   <listitem>
    <para>
     Identify the NIC ports to be used for PCI Passthrough devices and SRIOV
     devices from each compute node
    </para>
   </listitem>
   <listitem>
    <para>
     Ensure that:
    </para>
    <itemizedlist>
     <listitem>
      <para>
       SRIOV is enabled in the BIOS
      </para>
     </listitem>
     <listitem>
      <para>
       HP Shared memory is disabled in the BIOS on the compute nodes.
      </para>
     </listitem>
     <listitem>
      <para>
       The Intel boot agent is disabled on the compute
       (<xref linkend="bootutil"/> can be used to perform this)
      </para>
     </listitem>
    </itemizedlist>
    <note>
     <para>
      Because of Intel driver limitations, you cannot use a NIC port as an
      SRIOV NIC as well as a physical NIC. Using the physical function to carry
      the normal tenant traffic through the OVS bridge at the same time as
      assigning the VFs from the same NIC device as passthrough to the guest VM
      is not supported.
     </para>
    </note>
   </listitem>
  </orderedlist>
  <para>
   If the above prerequisites are met, then SR-IOV or PCIPT can be reconfigured
   at any time. There is no need to do it at install time.
  </para>
 </section>
 <section>
  <title>Deployment use cases</title>
  <para>
   The following are typical use cases that should cover your particular needs:
  </para>
  <orderedlist>
   <listitem>
    <para>
     A device on the host needs to be enabled for both PCI-passthrough and
     PCI-SRIOV during deployment. At run time Nova decides whether to use
     physical functions or virtual function depending on vnic_type of the port
     used for booting the VM.
    </para>
   </listitem>
   <listitem>
    <para>
     A device on the host needs to be configured only for PCI-passthrough.
    </para>
   </listitem>
   <listitem>
    <para>
     A device on the host needs to be configured only for PCI-SRIOV virtual
     functions.
    </para>
   </listitem>
  </orderedlist>
 </section>
 <section>
  <title>Input model updates</title>
  <para>
   &kw-hos-phrase; provides various options for the user to configure the
   network for tenant VMs. These options have been enhanced to support SRIOV
   and PCIPT.
  </para>
  <para>
   the &lcm; input model changes to support SRIOV and PCIPT are as follows. If
   you were familiar with the configuration settings previously, you will
   notice these changes.
  </para>
  <para>
   <emphasis role="bold">net_interfaces.yml:</emphasis> This file defines the
   interface details of the nodes. In it, the following fields have been added
   under the compute node interface section:
  </para>
  <informaltable colsep="1" rowsep="1">
   <tgroup cols="2">
    <colspec colnum="1" colname="col1"/>
    <colspec colnum="2" colname="col2"/>
    <thead>
     <row>
      <entry>Key</entry>
      <entry>Value</entry>
     </row>
    </thead>
    <tbody>
     <row>
      <entry>sriov_only: </entry>
      <entry>
       <para>
        Indicates that only SR-IOV be enabled on the interface. This should be
        set to true if you want to dedicate the NIC interface to support only
        SR-IOV functionality.
       </para>
      </entry>
     </row>
     <row>
      <entry>pci-pt: </entry>
      <entry>
       <para>
        When this value is set to true, it indicates that PCIPT should be
        enabled on the interface.
       </para>
      </entry>
     </row>
     <row>
      <entry>vf-count: </entry>
      <entry>
       <para>
        Indicates the number of VFs to be configured on a given interface.
       </para>
      </entry>
     </row>
    </tbody>
   </tgroup>
  </informaltable>
  <para>
   In control_plane.yml, under Compute resource neutron-sriov-nic-agent has
   been added as service components
  </para>
  <para>
   under resources:
  </para>
  <informaltable colsep="1" rowsep="1">
   <tgroup cols="2">
    <colspec colnum="1" colname="col1"/>
    <colspec colnum="2" colname="col2"/>
    <thead>
     <row>
      <entry>Key</entry>
      <entry>Value</entry>
     </row>
    </thead>
    <tbody>
     <row>
      <entry>name:</entry>
      <entry> Compute</entry>
     </row>
     <row>
      <entry>resource-prefix:</entry>
      <entry> Comp</entry>
     </row>
     <row>
      <entry>server-role:</entry>
      <entry>COMPUTE-ROLE</entry>
     </row>
     <row>
      <entry>allocation-policy:</entry>
      <entry> Any</entry>
     </row>
     <row>
      <entry>min-count:</entry>
      <entry> 0</entry>
     </row>
     <row>
      <entry>service-components:</entry>
      <entry>ntp-client</entry>
     </row>
     <row>
      <entry></entry>
      <entry>nova-compute</entry>
     </row>
     <row>
      <entry></entry>
      <entry>nova-compute-kvm</entry>
     </row>
     <row>
      <entry></entry>
      <entry>neutron-l3-agent</entry>
     </row>
     <row>
      <entry></entry>
      <entry>neutron-metadata-agent</entry>
     </row>
     <row>
      <entry></entry>
      <entry>neutron-openvswitch-agent</entry>
     </row>
     <row>
      <entry></entry>
      <entry>neutron-lbaasv2-agent</entry>
     </row>
     <row>
      <entry></entry>
      <entry>- neutron-sriov-nic-agent*</entry>
     </row>
    </tbody>
   </tgroup>
  </informaltable>
  <para>
   <emphasis role="bold">nic_device_data.yml:</emphasis> This is the new file
   added with this release to support SRIOV and PCIPT configuration details. It
   contains information about the specifics of a nic, and is found here:
   <literal>~/openstack/ardana/services/osconfig/nic_device_data.yml</literal>.
   The fields in this file are as follows.
  </para>
  <orderedlist>
   <listitem>
    <para>
     <emphasis role="bold">nic-device-types:</emphasis> The nic-device-types
     section contains the following key-value pairs:
    </para>
    <informaltable colsep="1" rowsep="1">
     <tgroup cols="2">
      <colspec colname="c1" colnum="1" colwidth="1*"/>
      <colspec colname="c2" colnum="2" colwidth="5.25*"/>
      <thead>
       <row>
        <entry>Key</entry>
        <entry>Value</entry>
       </row>
      </thead>
      <tbody>
       <row>
        <entry>name:</entry>
        <entry>
         <para>
          The name of the nic-device-types that will be referenced in
          nic_mappings.yml
         </para>
        </entry>
       </row>
       <row>
        <entry>family:</entry>
        <entry>
         <para>
          The name of the nic-device-families to be used with this
          nic_device_type
         </para>
        </entry>
       </row>
       <row>
        <entry>device_id:</entry>
        <entry>
         <para>
          Device ID as specified by the vendor for the particular NIC
         </para>
        </entry>
       </row>
       <row>
        <entry>type:</entry>
        <entry>
         <para>
          The value of this field can be "simple-port" or "multi-port". If a
          single bus address is assigned to more than one nic it will be
          multi-port, else if there is a one-to one mapping between bus address
          and the nic then it will be simple-port.
         </para>
        </entry>
       </row>
      </tbody>
     </tgroup>
    </informaltable>
   </listitem>
   <listitem>
    <para>
     <emphasis role="bold">nic-device-families:</emphasis> The
     nic-device-families section contains the following key-value pairs:
    </para>
    <informaltable colsep="1" rowsep="1">
     <tgroup cols="2">
      <colspec colname="c1" colnum="1" colwidth="1*"/>
      <colspec colname="c2" colnum="2" colwidth="4.13*"/>
      <thead>
       <row>
        <entry>Key</entry>
        <entry>Value</entry>
       </row>
      </thead>
      <tbody>
       <row>
        <entry>name:</entry>
        <entry>
         <para>
          The name of the device family that can be used for reference in
          nic-device-types.
         </para>
        </entry>
       </row>
       <row>
        <entry>vendor-id: </entry>
        <entry>
         <para>
          Vendor ID of the NIC
         </para>
        </entry>
       </row>
       <row>
        <entry>config-script:</entry>
        <entry>
         <para>
          A script file used to create the virtual functions (VF) on the
          Compute node.
         </para>
        </entry>
       </row>
       <row>
        <entry>driver:</entry>
        <entry>
         <para>
          Indicates the NIC driver that needs to be used.
         </para>
        </entry>
       </row>
       <row>
        <entry>vf-count-type:</entry>
        <entry>
         <para>
          This value can be either "port" or "driver".
         </para>
        </entry>
       </row>
       <row>
        <entry>“port”:</entry>
        <entry>
         <para>
          Indicates that the device supports per-port virtual function (VF)
          counts.
         </para>
        </entry>
       </row>
       <row>
        <entry>“driver:”</entry>
        <entry>
         <para>
          Indicates that all ports using the same driver will be configured
          with the same number of VFs, whether or not the interface model
          specifies a vf-count attribute for the port. If two or more ports
          specify different vf-count values, the config processor errors out.
         </para>
        </entry>
       </row>
       <row>
        <entry>Max-vf-count:</entry>
        <entry>
         <para>
          This field indicates the maximum VFs that can be configured on an
          interface as defined by the vendor.
         </para>
        </entry>
       </row>
      </tbody>
     </tgroup>
    </informaltable>
   </listitem>
  </orderedlist>
  <para>
   <emphasis role="bold">control_plane.yml:</emphasis> This file provides the
   information about the services to be run on a particular node. To support
   SR-IOV on a particular compute node, you must run neutron-sriov-nic-agent on
   that node.
  </para>
  <para>
   <emphasis role="bold">Mapping the use cases with various fields in input
   model</emphasis>
  </para>
  <informaltable colsep="1" rowsep="1">
   <tgroup cols="7">
    <colspec colnum="1" colname="col1" colwidth="2.48*"/>
    <colspec colnum="2" colname="col2" colwidth="2.1*"/>
    <colspec colnum="3" colname="col3" colwidth="1.25*"/>
    <colspec colnum="4" colname="col4" colwidth="1*"/>
    <colspec colnum="5" colname="col5" colwidth="1.14*"/>
    <colspec colnum="6" colname="col6" colwidth="2.49*"/>
    <colspec colnum="7" colname="col7" colwidth="2.11*"/>
    <thead>
     <row>
      <entry/>
      <entry>Vf-count</entry>
      <entry>SR-IOV</entry>
      <entry>PCIPT</entry>
      <entry>OVS bridge</entry>
      <entry>Can be NIC bonded</entry>
      <entry>Use case</entry>
     </row>
    </thead>
    <tbody>
     <row>
      <entry>sriov-only: true</entry>
      <entry>Mandatory</entry>
      <entry>Yes</entry>
      <entry>No</entry>
      <entry>No</entry>
      <entry>No</entry>
      <entry>Dedicated to SRIOV</entry>
     </row>
     <row>
      <entry>pci-pt : true</entry>
      <entry>Not Specified</entry>
      <entry>No</entry>
      <entry>Yes</entry>
      <entry>No</entry>
      <entry>No</entry>
      <entry>Dedicated to PCI-PT</entry>
     </row>
     <row>
      <entry>pci-pt : true</entry>
      <entry>Specified</entry>
      <entry>Yes</entry>
      <entry>Yes</entry>
      <entry>No</entry>
      <entry>No</entry>
      <entry>PCI-PT or SRIOV</entry>
     </row>
     <row>
      <entry>pci-pt and sriov-only keywords are not specified</entry>
      <entry>Specified</entry>
      <entry>Yes</entry>
      <entry>No</entry>
      <entry>Yes</entry>
      <entry>No</entry>
      <entry>SRIOV with PF used by host</entry>
     </row>
     <row>
      <entry>pci-pt and sriov-only keywords are not specified</entry>
      <entry>Not Specified</entry>
      <entry>No</entry>
      <entry>No</entry>
      <entry>Yes</entry>
      <entry>Yes</entry>
      <entry>Traditional/Usual use case</entry>
     </row>
    </tbody>
   </tgroup>
  </informaltable>
 </section>
 <section>
  <title>Mappings between nic_mappings.yml and net_interfaces.yml</title>
  <para>
   The following diagram shows which fields in nic_mappings.yml map to
   corresponding fields in net_interfaces.yml:
  </para>
  <informalfigure>
   <mediaobject>
    <imageobject role="fo">
     <imagedata fileref="media-sriov_pcpit.png" width="75%"/>
    </imageobject>
    <imageobject role="html">
     <imagedata fileref="media-sriov_pcpit.png"/>
    </imageobject>
   </mediaobject>
  </informalfigure>
 </section>
 <section>
  <title>Example Use Cases for Intel</title>
  <orderedlist>
   <listitem>
    <para>
     <emphasis role="bold">Nic-device-types and nic-device-families</emphasis>
     with Intel 82559 with ixgbe as the driver.
    </para>
<screen>nic-device-types:
    - name: ''8086:10fb
      family: INTEL-82599
      device-id: '10fb'
      type: simple-port
nic-device-families:
    # Niantic
    - name: INTEL-82599
      vendor-id: '8086'
      config-script: intel-82599.sh
      driver: ixgbe
      vf-count-type: port
      max-vf-count: 63</screen>
   </listitem>
   <listitem>
    <para>
     <emphasis role="bold">net_interfaces.yml</emphasis> for the SRIOV-only use
     case:
    </para>
<screen>- name: COMPUTE-INTERFACES
   - name: hed1
     device:
       name: hed1
       sriov-only: true
       vf-count: 6
     network-groups:
      - GUEST1</screen>
   </listitem>
   <listitem>
    <para>
     <emphasis role="bold">net_interfaces.yml</emphasis> for the PCIPT-only use
     case:
    </para>
<screen>- name: COMPUTE-INTERFACES
   - name: hed1
     device:
       name: hed1
       pci-pt: true
    network-groups:
     - GUEST1</screen>
   </listitem>
   <listitem>
    <para>
     <emphasis role="bold">net_interfaces.yml</emphasis> for the SRIOV and
     PCIPT use case
    </para>
<screen> - name: COMPUTE-INTERFACES
    - name: hed1
      device:
        name: hed1
        pci-pt: true
        vf-count: 6
      network-groups:
      - GUEST1</screen>
   </listitem>
   <listitem>
    <para>
     <emphasis role="bold">net_interfaces.yml</emphasis> for SRIOV and Normal
     Virtio use case
    </para>
<screen>- name: COMPUTE-INTERFACES
   - name: hed1
     device:
        name: hed1
        vf-count: 6
      network-groups:
      - GUEST1</screen>
   </listitem>
   <listitem>
    <para>
     <emphasis role="bold">net_interfaces.yml</emphasis> for PCI-PT
     (<literal>hed1</literal> and <literal>hed4</literal> refer to the DUAL
     ports of the PCI-PT NIC)
    </para>
<screen>    - name: COMPUTE-PCI-INTERFACES
      network-interfaces:
      - name: hed3
        device:
          name: hed3
        network-groups:
          - MANAGEMENT
          - EXTERNAL-VM
        forced-network-groups:
          - EXTERNAL-API
      - name: hed1
        device:
          name: hed1
          pci-pt: true
        network-groups:
          - GUEST
      - name: hed4
        device:
          name: hed4
          pci-pt: true
        network-groups:
          - GUEST</screen>
   </listitem>
  </orderedlist>
 </section>
<!-- Commented in DITA original: -->
<!--  DOCS-4025
    <section>
      <title>Example Use Cases for Mellanox</title>
      <ol>
        <li><b>Nic-device-types and nic-device-families</b> with Mellanox connect-x3 with mlx4 as
          the driver.
          <codeblock>nic-device-families:
    # ConnectX-3 Pro
    - name: MT-27520
      vendor-id: '15b3'
      config-script: connect-x3.sh
      driver: mlx4
      vf-count-type: port
      max-vf-count: 16
nic-device-types:
    - name: '15b3:1007'
      family: MT-27520
      device-id: '1007'
      type: multi-port
nic_mappings.yml
    - name: Mellanox-connect-x3-multi-port
      physical-ports:
        - logical-name: hed1
          type: multi-port
          bus-address: "0000:04:00.0"
          nic-device-type: '15b3:1007'
          port-attributes:
             port-num: 0
        - logical-name: hed2
          type: multi-port
          bus-address: "0000:04:00.0"
          nic-device-type: '15b3:1007'
          port-attributes:
             port-num: 1</codeblock>
        </li>
        <li><b>net_interfaces.yml</b> for the SRIOV on hed2 interface and pci-pt on hed1.
          <codeblock>    - name: Mellanox-connect-x3-COMPUTE-INTERFACES-8
      network-interfaces:
       - name: hed1
         device:
           name: hed1
           pci-pt: true
         network-groups:
           - GUEST2
       - name: hed2
         device:
           name: hed2
           sriov-only: true
           vf-count: 11
         network-groups:
           - GUEST1
          </codeblock>
        </li>
        <li><b>net_interfaces.yml</b> for the SRIOV on hed1 and Normal on hed2.
          <codeblock>    - name: Mellanox-connect-x3-COMPUTE-INTERFACES-8
      network-interfaces:
       - name: hed2
         device:
           name: hed2
         network-groups:
           - GUEST2

       - name: hed1
         device:
           name: hed1
           sriov-only: true
           vf-count: 5
         network-groups:
           - GUEST1</codeblock>
        </li>
      </ol>
      <p><b>Support for Mellanox connect-x3</b></p>
      <p>
        <table frame="all" rowsep="1" colsep="1" id="table_wt2_mgp_xx">
          <title/>
          <tgroup cols="6">
            <colspec colname="c1" colnum="1" colwidth="1.0*"/>
            <colspec colname="c2" colnum="2" colwidth="1.0*"/>
            <colspec colname="c3" colnum="3" colwidth="1.0*"/>
            <colspec colname="c4" colnum="4" colwidth="1.0*"/>
            <colspec colname="c5" colnum="5" colwidth="1.0*"/>
            <colspec colname="c6" colnum="6" colwidth="1.0*"/>
            <thead>
              <row>
                <entry namest="c1" nameend="c6" align="center" valign="middle">Mellanox
                  connect-x3</entry>
              </row>
            </thead>
            <tbody>
              <row>
                <entry namest="c1" nameend="c2" morerows="1"/>
                <entry namest="c3" nameend="c6" align="center">hed 2</entry>
              </row>
              <row>
                <entry>Normal</entry>
                <entry>SR-IOV+Normal</entry>
                <entry>SR-IOV Only</entry>
                <entry>PCI-PT Only</entry>
              </row>
              <row>
                <entry morerows="3" align="center" valign="middle">hed 1</entry>
                <entry>Normal</entry>
                <entry align="center">Yes</entry>
                <entry align="center">Yes *</entry>
                <entry align="center">Yes</entry>
                <entry align="center">No</entry>
              </row>
              <row>
                <entry>SR-IOV+Normal</entry>
                <entry align="center">Yes *</entry>
                <entry align="center">No</entry>
                <entry align="center">No</entry>
                <entry align="center">No</entry>
              </row>
              <row>
                <entry>SR-IOV Only</entry>
                <entry align="center">Yes</entry>
                <entry align="center">No</entry>
                <entry align="center">No</entry>
                <entry align="center">No</entry>
              </row>
              <row>
                <entry>PCI-PT Only</entry>
                <entry align="center">No</entry>
                <entry align="center">No</entry>
                <entry align="center">Yes **</entry>
                <entry align="center">No</entry>
              </row>
            </tbody>
          </tgroup>
        </table>
        <note>
          <p><b>*</b>VMs will be unable to communicate with each other on the same network when booted for SR-IOV and Normal,
            however the VMs will be able to communicate with other networks.
          </p>
          <p><b>**</b> PCI-PT Guest VM can only be booted on first interface For example, if you have eth4 and eth5
            on a single Mellanox card, then eth4 should be used for the PCI-PT interface.
          </p>
        </note>
      </p>
    </section>
-->
 <section>
  <title>Launching Virtual Machines</title>
  <para>
   Provisioning a VM with SR-IOV NIC is a two-step process.
  </para>
  <procedure>
   <step>
    <para>
     Create a Neutron port with <literal>vnic_type = direct</literal>.
    </para>
<screen>&prompt.ardana;neutron port-create $net_id --name sriov_port --binding:vnic_type direct</screen>
   </step>
   <step>
    <para>
     Boot a VM with the created <literal>port-id</literal>.
    </para>
<screen>&prompt.ardana;nova boot --flavor m1.large --image ubuntu_14.04 --nic port-id=$port_id test-sriov</screen>
   </step>
  </procedure>
  <para>
   Provisioning a VM with PCI-PT NIC is a two-step process.
  </para>
  <procedure>
   <step>
    <para>
     Create two &o_netw; ports with <literal>vnic_type =
     direct-physical</literal>.
    </para>
<screen>&prompt.ardana;neutron port-create net1 --name pci-port1 --vnic_type=direct-physical
neutron port-create net1 --name pci-port2  --vnic_type=direct-physical</screen>
   </step>
   <step>
    <para>
     Boot a VM with the created ports.
    </para>
<screen>&prompt.ardana;nova boot --flavor 4 --image opensuse --nic port-id pci-port1-port-id \
--nic port-id pci-port2-port-id vm1-pci-passthrough</screen>
   </step>
  </procedure>
  <para>
   If PCI-PT VM gets stuck (hangs) at boot time when using an Intel NIC, the
   boot agent should be disabled.
  </para>
 </section>
 <section xml:id="bootutil">
  <title>Intel bootutils</title>
  <para>
   When Intel cards are used for PCI-PT, a tenant VM can get stuck at boot
   time. When this happens, you should download Intel bootutils and use it to
   should disable bootagent.
  </para>
  <orderedlist>
   <listitem>
    <para>
     Download Preebot.tar.gz from
     <link xlink:href="https://downloadcenter.intel.com/download/19186/Intel-Ethernet-Connections-Boot-Utility-Preboot-Images-and-EFI-Drivers"/>
    </para>
   </listitem>
   <listitem>
    <para>
     Untar the <literal>Preboot.tar.gz</literal> on the compute node where the
     PCI-PT VM is to be hosted.
    </para>
   </listitem>
   <listitem>
    <para>
     Go to ~/APPS/BootUtil/Linux_x64
    </para>
<screen>cd ~/APPS/BootUtil/Linux_x64</screen>
    <para>
     and run following command
    </para>
<screen>./bootutil64e -BOOTENABLE disable -all</screen>
   </listitem>
   <listitem>
    <para>
     Boot the PCI-PT VM and it should boot without getting stuck.
    </para>
    <note>
     <para>
      Here even though VM console shows VM getting stuck at PXE boot, it is not
      related to BIOS PXE settings.
     </para>
    </note>
   </listitem>
  </orderedlist>
 </section>
 <section>
  <title>Making input model changes and implementing PCI PT and SR-IOV</title>
  <para>
   To implent the configuration you require, log into the &lcm; node and update
   the &lcm; model files to enable SR-IOV or PCIPT following the relevent use
   case explained above. You will need to edit
  </para>
  <itemizedlist>
   <listitem>
    <para>
     net_interfaces.yml
    </para>
   </listitem>
   <listitem>
    <para>
     nic_device_data.yml
    </para>
   </listitem>
   <listitem>
    <para>
     control_plane.yml
    </para>
   </listitem>
  </itemizedlist>
  <para>
   To make the edits,
  </para>
  <orderedlist>
   <listitem>
    <para>
     Check out the site branch of the local git repository and change to the
     correct directory:
    </para>
<screen>&prompt.ardana;git checkout site
&prompt.ardana;cd ~/openstack/my_cloud/definition/data/</screen>
   </listitem>
   <listitem>
    <para>
     Open each file in vim or another editor and make the necessary changes.
     Save each file, then commit to the local git repository:
    </para>
<screen>&prompt.ardana;git add -A
&prompt.ardana;git commit -m "your commit message goes here in quotes"</screen>
   </listitem>
   <listitem>
    <para>
     Here you will have the &lcm; enable your changes by running the necessary
     playbooks:
    </para>
<screen>&prompt.ardana;cd ~/openstack/ardana/ansible
&prompt.ardana;ansible-playbook -i hosts/localhost config-processor-run.yml
&prompt.ardana;ansible-playbook -i hosts/localhost ready-deployment.yml
&prompt.ardana;cd ~/scratch/ansible/next/ardana/ansible
&prompt.ardana;ansible-playbook -i hosts/verb_hosts site.yml</screen>
   </listitem>
  </orderedlist>
  <note>
   <para>
    After running the site.yml playbook above, you must reboot the compute
    nodes that are configured with Intel PCI devices.
   </para>
  </note>
  <note>
   <para>
    When a VM is running on an SRIOV port on a given compute node,
    reconfiguration is not supported.
   </para>
  </note>
  <para>
   You can set the number of virtual functions that must be enabled on a
   compute node at install time. You can update the number of virtual functions
   after deployment. If any VMs have been spawned before you change the number
   of virtual functions, those VMs may lose connectivity. Therefore, it is
   always recommended that if any virtual function is used by any tenant VM,
   you should not reconfigure the virtual functions. Instead, you should
   delete/migrate all the VMs on that NIC before reconfiguring the number of
   virtual functions.
  </para>
 </section>
 <section>
  <title>Limitations</title>
  <itemizedlist>
   <listitem>
    <para>
     Security groups are not applicable for PCI-PT and SRIOV ports.
    </para>
   </listitem>
   <listitem>
    <para>
     Live migration is not supported for VMs with PCI-PT and SRIOV ports.
    </para>
   </listitem>
   <listitem>
    <para>
     Rate limiting (QoS) is not applicable on SRIOV and PCI-PT ports.
    </para>
   </listitem>
   <listitem>
    <para>
     SRIOV/PCIPT is not supported for VxLAN network.
    </para>
   </listitem>
   <listitem>
    <para>
     DVR is not supported with SRIOV/PCIPT.
    </para>
   </listitem>
   <listitem>
    <para>
     For Intel cards, the same NIC cannot be used for both SRIOV and normal VM
     boot.
    </para>
   </listitem>
   <listitem>
    <para>
     Current upstream OpenStack code does not support this hot plugin of
     SRIOV/PCIPT interface using the nova <literal>attach_interface</literal>
     command. See <link xlink:href="https://review.openstack.org/#/c/139910/"/>
     for more information.
    </para>
   </listitem>
   <listitem>
    <para>
     Neutron port-update when admin state is down will not work.
    </para>
   </listitem>
   <listitem>
    <para>
     &slsa; &compnode;s with dual-port PCI-PT NICs, both ports should always be
     passed in the VM. It is not possible to split the dual port and pass
     through just a single port.
    </para>
   </listitem>
  </itemizedlist>
 </section>
 <xi:include href="networking-enabling_pcipt_on_gen9.xml"/>
</section>
