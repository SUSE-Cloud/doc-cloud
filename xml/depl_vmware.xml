<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE appendix
[
  <!ENTITY % entities SYSTEM "entity-decl.ent">
  %entities;
]>
<!-- Converted by suse-upgrade version 1.1 -->
<appendix xmlns="http://docbook.org/ns/docbook" xmlns:xi="http://www.w3.org/2001/XInclude" xmlns:xlink="http://www.w3.org/1999/xlink" version="5.0" xml:id="app-deploy-vmware">
 <title>&vmware; vSphere Installation Instructions</title>
 <info>
<dm:docmanager xmlns:dm="urn:x-suse:ns:docmanager">
    <dm:maintainer>fs</dm:maintainer>
    <dm:status>editing</dm:status>
    <dm:deadline/>
    <dm:priority/>
    <dm:translation>no</dm:translation>
    <dm:languages/>
</dm:docmanager>
</info>
<para>
  &cloud; supports the &o_comp; Compute &vmware; vCenter driver. It
  enables access to advanced features such as vMotion, High Availability,
  and Dynamic Resource Scheduling (DRS). However, &vmware; vSphere is not
  supported <quote>natively</quote> by &cloud;&mdash;it rather
  delegates requests to an existing vCenter. It requires preparations at the
  vCenter and post install adjustments of the &compnode;.
 </para>
 <sect1 xml:id="app-deploy-vmware-requirements">
  <title>Requirements</title>

  <para>
   The following requirements must be met to successfully deploy a
   &o_comp; Compute &vmware; node:
  </para>

  <itemizedlist mark="bullet" spacing="normal">
   <listitem>
    <para>
     &vmware; vSphere vCenter 6.0 or higher
    </para>
   </listitem>
   <listitem>
    <para>
     &vmware; vSphere ESXi nodes 6.0 or higher
    </para>
   </listitem>
   <listitem>
    <para>
     A separate &compnode; that acts as a proxy to vCenter is required.
     Minimum system requirements for this node are:
    </para>
    <simplelist>
     <member>CPU: x86_64 with 2 cores (4 recommended)</member>
     <member>RAM: 2 GB (8 GB recommended)</member>
     <member>Disk space: 4 GB (30 GB recommended)</member>
    </simplelist>
    <para>
     See <xref linkend="app-deploy-vmware-compnode"/> for setup
     instructions.
    </para>
   </listitem>
   <listitem>
    <para>
     &o_netw; must not be deployed with the <literal>openvswitch with
     gre</literal> plug-in, a VLAN setup is required.
    </para>
   </listitem>
   <listitem>
    <para>
     Security groups are only supported when running &vmware; NSX. You need to
     deploy &o_netw; with the <guimenu>vmware</guimenu> plug-in to have
     security group support. This is also a prerequisite for
     <literal>gre</literal> tunnel support.
    </para>
   </listitem>
  </itemizedlist>
 </sect1>
 <sect1 xml:id="app-deploy-vmware-vcenter">
  <title>Preparing the &vmware; vCenter Server</title>

  <para>
   &cloud; requires the &vmware; vCenter server to run version 5.1 or
   better. You need to create a single data center for &cloud; (multiple
   data centers are currently not supported):
  </para>

  <procedure>
   <step>
    <para>
     Log in to the vCenter Server using the vSphere Web Client
    </para>
   </step>
   <step>
    <para>
     Choose <guimenu>Hosts and Clusters</guimenu> and create a single
     <guimenu>Datacenter</guimenu>
    </para>
   </step>
   <step>
    <para>
     Set up a <guimenu>New Cluster</guimenu> which has
     <guimenu>DRS</guimenu> enabled.
    </para>
   </step>
   <step>
    <para>
     Set <guimenu>Automation Level</guimenu> to <literal>Fully
     Automated</literal> and <guimenu>Migration Threshold</guimenu> to
     <literal>Aggressive</literal>.
    </para>
   </step>
   <step>
    <para>
     Create shared storage. Only shared storage is supported and data stores
     must be shared among all hosts in a cluster. It is recommended to
     remove data stores not intended for &ostack; from clusters being
     configured for &ostack;. Multiple data stores can be used
     per cluster.
    </para>
   </step>
   <step>
    <para>
     Create a port group with the same name as the
     <envar>vmware.integration_bridge</envar> value in
     <filename>nova.conf</filename> (default is br-int). All VM NICs are
     attached to this port group for management by the &ostack;
     networking plug-in. Assign the same VLAN ID as for the neutron network.
     On the default network setup this is the same VLAN ID as for the
     <literal>nova_fixed</literal> network. Use
     <menuchoice><guimenu>&yast;</guimenu>
     <guimenu>Miscellaneous</guimenu> <guimenu>Crowbar</guimenu>
     <guimenu>Networks</guimenu></menuchoice> to look up the VLAN ID.
    </para>
   </step>
  </procedure>
 </sect1>
 <sect1 xml:id="app-deploy-vmware-compnode">
  <title>Finishing the &o_comp; Compute &vmware; Node Installation</title>

  <para>
   Deploy &o_comp; as described in <xref linkend="sec-depl-ostack-nova"/>
   on a single &compnode; and fill in the <guimenu>&vmware; vCenter
   Settings</guimenu> attributes:
  </para>

  <variablelist>
   <varlistentry>
    <term><guimenu>vCenter IP Address</guimenu>
    </term>
    <listitem>
     <para>
      IP address of the vCenter server.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><guimenu>vCenter Username</guimenu> / <guimenu>vCenter
     Password</guimenu>
    </term>
    <listitem>
     <para>
      vCenter login credentials.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><guimenu>Cluster Names</guimenu></term>
    <listitem>
     <para>
      A comma-separated list of cluster names you have added on the vCenter
      server.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><guimenu>Regex to match the name of a datastore</guimenu></term>
    <listitem>
     <para>Regular expression to match the name of a data store. If you have
      several data stores, this option allows you to specify the
      data stores to use with &o_comp; &comp;. For example, the value <literal>nas.*</literal> selects
      all data stores that have a name starting with <literal>nas</literal>. If
      this option is omitted, &o_comp; &comp; uses the first data store returned by the vSphere
      API. However, it is recommended not to use this option and to remove data
      stores that are not intended for &ostack; instead.</para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><guimenu>VLAN Interface</guimenu></term>
    <listitem>
     <para>
      The physical interface that is to be used for VLAN networking. The
      default value of <literal>vmnic0</literal> references the first
      available interface (<quote>eth0</quote>). <literal>vmnic1</literal>
      would be the second interface (<quote>eth1</quote>).
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>
     <guimenu>CA file for verifying the vCenter certificate</guimenu>
    </term>
    <listitem>
     <para>
      Absolute path to the vCenter CA certificate.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>
     <guimenu>
      vCenter SSL Certificate is insecure (for instance, self-signed)
     </guimenu>
    </term>
    <listitem>
     <para>
      Default value: <literal>false</literal> (the CA truststore is used for
      verification).  Set this option to <literal>true</literal> when using
      self-signed certificates to disable certificate checks. This setting is
      for testing purposes only and must not be used in production
      environments!
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>
     <guimenu>
      Keymap
     </guimenu>
    </term>
    <listitem>
     <para>
      The default is to use the <literal>en-us</literal> keyboard. Change its
      value according to your needs. This value is no longer configured as a
      keymap option under <literal>[vnc]</literal> in
      <filename>nova.conf</filename> (as for example in
      <filename>/etc/nova/nova.conf.d/100-nova.conf</filename>).
     </para>
    </listitem>
   </varlistentry>
  </variablelist>

  <figure>
   <title>The &o_comp; &barcl;: &vmware; Configuration</title>
   <mediaobject>
    <imageobject role="fo">
     <imagedata fileref="depl_barclamp_nova_vmware.png" width="100%" format="png"/>
    </imageobject>
    <imageobject role="html">
     <imagedata fileref="depl_barclamp_nova_vmware.png" width="75%" format="png"/>
    </imageobject>
   </mediaobject>
  </figure>
 </sect1>
 <sect1 xml:id="app-deploy-vmware-ha">
  <title>Making the &o_comp; Compute &vmware; Node Highly Available</title>

  <para>
   &ostack; does not support deploying multiple &vmware; &compnode;s. As
   a workaround, set up an &vmguest; on the vSphere Cluster, register it
   with &crow; and deploy the
   <literal>nova-compute-vmware</literal> role on this node:
  </para>

  <procedure>
   <step>
    <para>
     Create an &vmguest; on the vSphere Cluster and install &cloudos;.
    </para>
   </step>
   <step>
    <para>
     Configure a network interface in a way that it can access the
     &cloud; admin network.
    </para>
   </step>
   <step>
    <para>
     Enable the High-Availability flag in vCenter for this &vmguest;.
    </para>
   </step>
   <step>
    <para>
     Follow the instructions at
     <xref linkend="sec-depl-inst-nodes-install-external"/> to register the
     &vmguest; with the &admserv; and add it to the pool of nodes
     available for deployment.
    </para>
   </step>
   <step>
    <para>
     Deploy the <literal>nova-compute-vmware</literal> role on the new
     node as described in <xref linkend="sec-depl-ostack-nova"/> and
     <xref linkend="app-deploy-vmware-compnode"/>.
    </para>
   </step>
  </procedure>
 </sect1>
</appendix>
