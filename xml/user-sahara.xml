<?xml version="1.0" encoding="UTF-8" standalone="no"?>
<chapter xml:id="sahara-user-guide" xmlns="http://docbook.org/ns/docbook" xmlns:xlink="http://www.w3.org/1999/xlink" xmlns:xi="http://www.w3.org/2001/XInclude" version="5.1">
    <title>Sahara User Guide</title>
        <section xml:id="getting-started" xml:base="overview">
          <title>Getting Started</title>
          <section xml:id="clusters">
            <title>Clusters</title>
            <para>A cluster deployed by sahara consists of node groups. Node groups vary by their
              role, parameters and number of machines. The picture below illustrates an example of a
              Hadoop cluster consisting of 3 node groups each having a different role (set of
              processes).</para>
            <informalfigure>
              <mediaobject>
                <imageobject role="fo">
                  <imagedata fileref="hadoop-cluster-example.jpg"/>
                </imageobject>
                <imageobject role="html">
                  <imagedata fileref="hadoop-cluster-example.jpg"/>
                </imageobject>
              </mediaobject>
            </informalfigure>
            <para>Node group parameters include Hadoop parameters like <literal>io.sort.mb</literal>
              or <literal>mapred.child.java.opts</literal>, and several infrastructure parameters
              like the flavor for VMs or storage location (ephemeral drive or cinder volume).</para>
            <para>A cluster is characterized by its node groups and its parameters. Like a node
              group, a cluster has data processing framework and infrastructure parameters. An
              example of a cluster-wide Hadoop parameter is <literal>dfs.replication</literal>. For
              infrastructure, an example could be image which will be used to launch cluster
              VMs.</para>
          </section>
        </section>
        <section xml:id="templates">
            <title>Templates</title>
            <para>In order to simplify cluster provisioning sahara employs the concept of templates.
              There are two kinds of templates: node group templates and cluster templates. The
              former is used to create node groups, the latter - clusters. Essentially templates
              have the very same parameters as corresponding entities. Their aim is to remove the
              burden of specifying all of the required parameters each time a user wants to launch a
              cluster.</para>
            <para>In the REST interface, templates have extended functionality. First you can
              specify node-scoped parameters here, they will work as defaults for node groups. Also
              with the REST interface, during cluster creation a user can override template
              parameters for both cluster and node groups.</para>
        </section>
        <section xml:id="provisioning-plugins-1">
            <title>Provisioning Plugins</title>
            <para>A provisioning plugin is a component responsible for provisioning a data
              processing cluster. Each plugin is capable of provisioning a specific data
              processing framework or Hadoop distribution. Also the plugin can install management
              and monitoring tools for a cluster.</para>
            <para>Since framework configuration parameters vary depending on the distribution and
              the version, templates are always plugin and version specific. A template cannot be
              used if the plugin, or framework, versions are different than the ones they were
              created for.</para>
            <para>You may find the list of available plugins on that page: <xref linkend="plugins"/>.</para>
        </section>
        <section xml:id="image-registry">
            <title>Image Registry</title>
            <para>OpenStack starts VMs based on a pre-built image with an installed OS. The image
              requirements for sahara depend on the plugin and data processing framework version.
              Some plugins require just a basic cloud image and will install the framework on the
              VMs from scratch. Some plugins might require images with pre-installed frameworks or
              Hadoop distributions.</para>
            <para>The Sahara Image Registry is a feature which helps filter out images during
              cluster creation. See <xref linkend="registering-an-image-2"/> for details on how to work
              with Image Registry.</para>
        </section>
        <section xml:id="features">
            <title>Features</title>
            <para>Sahara has several interesting features. The full list could be found there: <xref
                linkend="features"/></para>
        </section>
        <section xml:id="quickstart-guide" xml:base="quickstart">
          <title>Quickstart guide</title>
          <section xml:id="launching-a-cluster-via-sahara-cli-commands">
            <title>Launching a cluster via Sahara CLI commands</title>
            <para>This guide will help you setup a vanilla Hadoop cluster using a combination of
              OpenStack command line tools and the sahara
              <link xlink:href="https://docs.openstack.org/sahara/pike/reference/restapi.html">REST API</link>.</para>
            <section xml:id="install-sahara">
              <title>1. Install sahara</title>
              <itemizedlist>
                <listitem>
                  <para>If you want to hack the code follow
                    <link xlink:href="https://docs.openstack.org/sahara/ocata/devref/development.environment.html">Setting Up a Development Environment</link>.</para>
                </listitem>
              </itemizedlist>
              <para>OR</para>
              <itemizedlist>
                <listitem>
                  <para>If you just want to install and use sahara follow the
                    <link xlink:href="https://docs.openstack.org/sahara/pike/install/installation-guide.html">Sahara Installation Guide</link>.</para>
                </listitem>
              </itemizedlist>
            </section>
            <section xml:id="identity-service-configuration">
              <title>2. Identity service configuration</title>
              <para>To use the OpenStack command line tools you should specify environment variables
                with the configuration details for your OpenStack installation. The following
                example assumes that the Identity service is at <literal>127.0.0.1:5000</literal>,
                with a user <literal>admin</literal> in the <literal>admin</literal> project whose
                password is <literal>nova</literal>:</para>
              <screen language="console">$ export OS_AUTH_URL=http://127.0.0.1:5000/v2.0/
$ export OS_PROJECT_NAME=admin
$ export OS_USERNAME=admin
$ export OS_PASSWORD=nova</screen>
            </section>
            <section xml:id="upload-an-image-to-the-image-service">
              <title>3. Upload an image to the Image service</title>
              <para>You will need to upload a virtual machine image to the OpenStack Image service.
                You can download pre-built images with vanilla Apache Hadoop installed, or build the
                images yourself. This guide uses the latest available Ubuntu upstream image,
                referred to as <literal>sahara-vanilla-latest-ubuntu.qcow2</literal> and the latest
                version of vanilla plugin as an example. Sample images are available here:</para>
              <para>
                <link xlink:href="http://sahara-files.mirantis.com/images/upstream/">Sample Images</link>
              </para>
              <itemizedlist>
                <listitem>
                  <para>Download a pre-built image</para>
                </listitem>
              </itemizedlist>
              <para><emphasis role="bold">Note:</emphasis> For the steps below, substitute
                  <literal>&lt;openstack_release&gt;</literal> with the appropriate
                OpenStack release and <literal>&lt;sahara_image&gt;</literal> with the image
                of your choice.</para>
              <screen language="console">$ ssh user@hostname
$ wget http://sahara-files.mirantis.com/images/upstream/&lt;openstack_release&gt;/&lt;sahara_image&gt;.qcow2</screen>
              <para>Upload the image downloaded above into the OpenStack Image service:</para>
              <screen language="console">$ openstack image create sahara-vanilla-latest-ubuntu --disk-format qcow2 \
    --container-format bare --file sahara-vanilla-latest-ubuntu.qcow2
+------------------+--------------------------------------+
| Field            | Value                                |
+------------------+--------------------------------------+
| checksum         | 3da49911332fc46db0c5fb7c197e3a77     |
| container_format | bare                                 |
| created_at       | 2016-02-29T10:15:04.000000           |
| deleted          | False                                |
| deleted_at       | None                                 |
| disk_format      | qcow2                                |
| id               | 71b9eeac-c904-4170-866a-1f833ea614f3 |
| is_public        | False                                |
| min_disk         | 0                                    |
| min_ram          | 0                                    |
| name             | sahara-vanilla-latest-ubuntu         |
| owner            | 057d23cddb864759bfa61d730d444b1f     |
| properties       |                                      |
| protected        | False                                |
| size             | 1181876224                           |
| status           | active                               |
| updated_at       | 2016-02-29T10:15:41.000000           |
| virtual_size     | None                                 |
+------------------+--------------------------------------+</screen>
              <para>OR</para>
              <itemizedlist>
                <listitem>
                  <para>Build the image using: <link xlink:href="https://git.openstack.org/cgit/openstack/sahara-image-elements/tree/diskimage-create/README.rst"
                      >diskimage-builder script</link></para>
                </listitem>
              </itemizedlist>
              <para>Remember the image name or save the image ID. This will be used during the image
                registration with sahara. You can get the image ID using the
                  <literal>openstack</literal> command line tool as follows:</para>
              <screen language="console">$ openstack image list --property name=sahara-vanilla-latest-ubuntu
+--------------------------------------+------------------------------+
| ID                                   | Name                         |
+--------------------------------------+------------------------------+
| 71b9eeac-c904-4170-866a-1f833ea614f3 | sahara-vanilla-latest-ubuntu |
+--------------------------------------+------------------------------+</screen>
            </section>
            <section xml:id="register-the-image-with-the-sahara-image-registry">
              <title>4. Register the image with the sahara image registry</title>
              <para>Now you will begin to interact with sahara by registering the virtual machine
                image in the sahara image registry.</para>
              <para>Register the image with the username <literal>ubuntu</literal>.</para>
              <note>
                <para>The username will vary depending on the source image used, as follows: Ubuntu:
                    <literal>ubuntu</literal> CentOS 7: <literal>centos</literal> CentOS 6:
                    <literal>cloud-user</literal> Fedora: <literal>fedora</literal>.</para>
              </note>
              <screen language="console">$ openstack dataprocessing image register sahara-vanilla-latest-ubuntu \
    --username ubuntu</screen>
              <para>Tag the image to inform sahara about the plugin and the version with which it
                shall be used.</para>
              <note>
                <para>For the steps below and the rest of this guide, substitute
                    <literal>&lt;plugin_version&gt;</literal> with the appropriate version
                  of your plugin.</para>
              </note>
              <screen language="console">$ openstack dataprocessing image tags add sahara-vanilla-latest-ubuntu \
    --tags vanilla &lt;plugin_version&gt;
+-------------+--------------------------------------+
| Field       | Value                                |
+-------------+--------------------------------------+
| Description | None                                 |
| Id          | 71b9eeac-c904-4170-866a-1f833ea614f3 |
| Name        | sahara-vanilla-latest-ubuntu         |
| Status      | ACTIVE                               |
| Tags        | &lt;plugin_version&gt;, vanilla            |
| Username    | ubuntu                               |
+-------------+--------------------------------------+</screen>
            </section>
            <section xml:id="create-node-group-templates-1">
              <title>5. Create node group templates</title>
              <para>Node groups are the building blocks of clusters in sahara. Before you can begin
                provisioning clusters you must define a few node group templates to describe node
                group configurations.</para>
              <para>You can get information about available plugins with the following
                command:</para>
              <screen language="console">$ openstack dataprocessing plugin list</screen>
              <para>Also you can get information about available services for a particular plugin
                with the <literal>plugin show</literal> command. For example:</para>
              <screen language="console">$ openstack dataprocessing plugin show vanilla --plugin-version &lt;plugin_version&gt;
+---------------------+-----------------------------------------------------------------------------------------------------------------------+
| Field               | Value                                                                                                                 |
+---------------------+-----------------------------------------------------------------------------------------------------------------------+
| Description         | The Apache Vanilla plugin provides the ability to launch upstream Vanilla Apache Hadoop cluster without any           |
|                     | management consoles. It can also deploy the Oozie component.                                                          |
| Name                | vanilla                                                                                                               |
| Required image tags | &lt;plugin_version&gt;, vanilla                                                                                             |
| Title               | Vanilla Apache Hadoop                                                                                                 |
|                     |                                                                                                                       |
| Service:            | Available processes:                                                                                                  |
|                     |                                                                                                                       |
| HDFS                | datanode, namenode, secondarynamenode                                                                                 |
| Hadoop              |                                                                                                                       |
| Hive                | hiveserver                                                                                                            |
| JobFlow             | oozie                                                                                                                 |
| Spark               | spark history server                                                                                                  |
| MapReduce           | historyserver                                                                                                         |
| YARN                | nodemanager, resourcemanager                                                                                          |
+---------------------+-----------------------------------------------------------------------------------------------------------------------+</screen>
              <note>
                <para>These commands assume that floating IP addresses are
		being used. For more details on floating IP see
    <link xlink:href="https://docs.openstack.org/sahara/pike/admin/configuration-guide.html#floating-ip-management">Floating IP management</link>.</para>
              </note>
              <para>Create a master node group template with the command:</para>
              <screen language="console">$ openstack dataprocessing node group template create \
    --name vanilla-default-master --plugin vanilla \
    --plugin-version &lt;plugin_version&gt; --processes namenode resourcemanager \
    --flavor 2 --auto-security-group --floating-ip-pool &lt;pool-id&gt;
+---------------------+--------------------------------------+
| Field               | Value                                |
+---------------------+--------------------------------------+
| Auto security group | True                                 |
| Availability zone   | None                                 |
| Flavor id           | 2                                    |
| Floating ip pool    | dbd8d1aa-6e8e-4a35-a77b-966c901464d5 |
| Id                  | 0f066e14-9a73-4379-bbb4-9d9347633e31 |
| Is default          | False                                |
| Is protected        | False                                |
| Is proxy gateway    | False                                |
| Is public           | False                                |
| Name                | vanilla-default-master               |
| Node processes      | namenode, resourcemanager            |
| Plugin name         | vanilla                              |
| Security groups     | None                                 |
| Use autoconfig      | False                                |
| Version             | &lt;plugin_version&gt;                     |
| Volumes per node    | 0                                    |
+---------------------+--------------------------------------+</screen>
              <para>Create a worker node group template with the command:</para>
              <screen language="console">$ openstack dataprocessing node group template create \
    --name vanilla-default-worker --plugin vanilla \
    --plugin-version &lt;plugin_version&gt; --processes datanode nodemanager \
    --flavor 2 --auto-security-group --floating-ip-pool &lt;pool-id&gt;
+---------------------+--------------------------------------+
| Field               | Value                                |
+---------------------+--------------------------------------+
| Auto security group | True                                 |
| Availability zone   | None                                 |
| Flavor id           | 2                                    |
| Floating ip pool    | dbd8d1aa-6e8e-4a35-a77b-966c901464d5 |
| Id                  | 6546bf44-0590-4539-bfcb-99f8e2c11efc |
| Is default          | False                                |
| Is protected        | False                                |
| Is proxy gateway    | False                                |
| Is public           | False                                |
| Name                | vanilla-default-worker               |
| Node processes      | datanode, nodemanager                |
| Plugin name         | vanilla                              |
| Security groups     | None                                 |
| Use autoconfig      | False                                |
| Version             | &lt;plugin_version&gt;                     |
| Volumes per node    | 0                                    |
+---------------------+--------------------------------------+</screen>
              <para>Alternatively, create node group templates from JSON files:</para>
              <para>If your environment does not use floating IPs, omit defining floating IP in the
                template below.</para>
              <para>Sample templates can be found here:</para>
              <para>
                <link xlink:href="https://git.openstack.org/cgit/openstack/sahara/tree/sahara/plugins/default_templates/"
                  >Sample Templates</link>
              </para>
              <para>Create a file named <literal>my_master_template_create.json</literal> with the
                following content:</para>
              <screen language="json">{
    "plugin_name": "vanilla",
    "hadoop_version": "&lt;plugin_version&gt;",
    "node_processes": [
        "namenode",
        "resourcemanager"
    ],
    "name": "vanilla-default-master",
    "floating_ip_pool": "&lt;floating_ip_pool_id&gt;",
    "flavor_id": "2",
    "auto_security_group": true
}</screen>
              <para>Create a file named <literal>my_worker_template_create.json</literal> with the
                following content:</para>
              <screen language="json">{
    "plugin_name": "vanilla",
    "hadoop_version": "&lt;plugin_version&gt;",
    "node_processes": [
        "nodemanager",
        "datanode"
    ],
    "name": "vanilla-default-worker",
    "floating_ip_pool": "&lt;floating_ip_pool_id&gt;",
    "flavor_id": "2",
    "auto_security_group": true
}</screen>
              <para>Use the <literal>openstack</literal> client to upload the node group
                templates:</para>
              <screen language="console">$ openstack dataprocessing node group template create \
    --json my_master_template_create.json
$ openstack dataprocessing node group template create \
    --json my_worker_template_create.json</screen>
              <para>List the available node group templates to ensure that they have been added
                properly:</para>
              <screen language="console">$ openstack dataprocessing node group template list --name vanilla-default
+------------------------+--------------------------------------+-------------+--------------------+
| Name                   | Id                                   | Plugin name | Version            |
+------------------------+--------------------------------------+-------------+--------------------+
| vanilla-default-master | 0f066e14-9a73-4379-bbb4-9d9347633e31 | vanilla     | &lt;plugin_version&gt;   |
| vanilla-default-worker | 6546bf44-0590-4539-bfcb-99f8e2c11efc | vanilla     | &lt;plugin_version&gt;   |
+------------------------+--------------------------------------+-------------+--------------------+</screen>
              <para>Remember the name or save the ID for the master and worker node group templates,
                as they will be used during cluster template creation.</para>
              <para>For example:</para>
              <itemizedlist>
                <listitem>
                  <para>vanilla-default-master:
                      <literal>0f066e14-9a73-4379-bbb4-9d9347633e31</literal></para>
                </listitem>
                <listitem>
                  <para>vanilla-default-worker:
                      <literal>6546bf44-0590-4539-bfcb-99f8e2c11efc</literal></para>
                </listitem>
              </itemizedlist>
            </section>
            <section xml:id="create-a-cluster-template-1">
              <title>6. Create a cluster template</title>
              <para>The last step before provisioning the cluster is to create a template that
                describes the node groups of the cluster.</para>
              <para>Create a cluster template with the command:</para>
              <screen language="console">$ openstack dataprocessing cluster template create \
    --name vanilla-default-cluster \
    --node-groups vanilla-default-master:1 vanilla-default-worker:3

+----------------+----------------------------------------------------+
| Field          | Value                                              |
+----------------+----------------------------------------------------+
| Anti affinity  |                                                    |
| Description    | None                                               |
| Id             | 9d871ebd-88a9-40af-ae3e-d8c8f292401c               |
| Is default     | False                                              |
| Is protected   | False                                              |
| Is public      | False                                              |
| Name           | vanilla-default-cluster                            |
| Node groups    | vanilla-default-master:1, vanilla-default-worker:3 |
| Plugin name    | vanilla                                            |
| Use autoconfig | False                                              |
| Version        | &lt;plugin_version&gt;                                   |
+----------------+----------------------------------------------------+</screen>
              <para>Alternatively, create cluster template from JSON file:</para>
              <para>Create a file named <literal>my_cluster_template_create.json</literal> with the
                following content:</para>
              <screen language="json">{
    "plugin_name": "vanilla",
    "hadoop_version": "&lt;plugin_version&gt;",
    "node_groups": [
        {
            "name": "worker",
            "count": 3,
            "node_group_template_id": "6546bf44-0590-4539-bfcb-99f8e2c11efc"
        },
        {
            "name": "master",
            "count": 1,
            "node_group_template_id": "0f066e14-9a73-4379-bbb4-9d9347633e31"
        }
    ],
    "name": "vanilla-default-cluster",
    "cluster_configs": {}
}</screen>
              <para>Upload the cluster template using the <literal>openstack</literal> command line
                tool:</para>
              <screen language="console">$ openstack dataprocessing cluster template create --json my_cluster_template_create.json</screen>
              <para>Remember the cluster template name or save the cluster template ID for use in
                the cluster provisioning command. The cluster ID can be found in the output of the
                creation command or by listing the cluster templates as follows:</para>
              <screen language="console">$ openstack dataprocessing cluster template list --name vanilla-default
+-------------------------+--------------------------------------+-------------+--------------------+
| Name                    | Id                                   | Plugin name | Version            |
+-------------------------+--------------------------------------+-------------+--------------------+
| vanilla-default-cluster | 9d871ebd-88a9-40af-ae3e-d8c8f292401c | vanilla     | &lt;plugin_version&gt;   |
+-------------------------+--------------------------------------+-------------+--------------------+</screen>
            </section>
            <section xml:id="create-cluster">
              <title>7. Create cluster</title>
              <para>Now you are ready to provision the cluster. This step requires a few pieces of
                information that can be found by querying various OpenStack services.</para>
              <para>Create a cluster with the command:</para>
              <screen language="console">$ openstack dataprocessing cluster create --name my-cluster-1 \
    --cluster-template vanilla-default-cluster --user-keypair my_stack \
    --neutron-network private --image sahara-vanilla-latest-ubuntu

+----------------------------+----------------------------------------------------+
| Field                      | Value                                              |
+----------------------------+----------------------------------------------------+
| Anti affinity              |                                                    |
| Cluster template id        | 9d871ebd-88a9-40af-ae3e-d8c8f292401c               |
| Description                |                                                    |
| Id                         | 1f0dc6f7-6600-495f-8f3a-8ac08cdb3afc               |
| Image                      | 71b9eeac-c904-4170-866a-1f833ea614f3               |
| Is protected               | False                                              |
| Is public                  | False                                              |
| Is transient               | False                                              |
| Name                       | my-cluster-1                                       |
| Neutron management network | fabe9dae-6fbd-47ca-9eb1-1543de325efc               |
| Node groups                | vanilla-default-master:1, vanilla-default-worker:3 |
| Plugin name                | vanilla                                            |
| Status                     | Validating                                         |
| Use autoconfig             | False                                              |
| User keypair id            | my_stack                                           |
| Version                    | &lt;plugin_version&gt;                                   |
+----------------------------+----------------------------------------------------+</screen>
              <para>Alternatively, create a cluster template from a JSON file:</para>
              <para>Create a file named <literal>my_cluster_create.json</literal> with the following
                content:</para>
              <screen language="json">{
    "name": "my-cluster-1",
    "plugin_name": "vanilla",
    "hadoop_version": "&lt;plugin_version&gt;",
    "cluster_template_id" : "9d871ebd-88a9-40af-ae3e-d8c8f292401c",
    "user_keypair_id": "my_stack",
    "default_image_id": "71b9eeac-c904-4170-866a-1f833ea614f3",
    "neutron_management_network": "fabe9dae-6fbd-47ca-9eb1-1543de325efc"
}</screen>
              <para>The parameter <literal>user_keypair_id</literal> with the value
                  <literal>my_stack</literal> is generated by creating a keypair. You can create
                your own keypair in the OpenStack Dashboard, or through the
                  <literal>openstack</literal> command line client as follows:</para>
              <screen language="console">$ openstack keypair create my_stack --public-key $PATH_TO_PUBLIC_KEY</screen>
              <para>If sahara is configured to use neutron for networking, you will also need to
                include the <literal>--neutron-network</literal> argument in the <literal>cluster
                  create</literal> command or the <literal>neutron_management_network</literal>
                parameter in <literal>my_cluster_create.json</literal>. If your environment does not
                use neutron, you should omit these arguments. You can determine the neutron network
                id with the following command:</para>
              <screen language="console">$ openstack network list</screen>
              <para>Create and start the cluster:</para>
              <screen language="console">$ openstack dataprocessing cluster create --json my_cluster_create.json</screen>
              <para>Verify the cluster status by using the <literal>openstack</literal> CLI as follows:</para>
              <screen language="console">$ openstack dataprocessing cluster show my-cluster-1 -c Status
+--------+--------+
| Field  | Value  |
+--------+--------+
| Status | Active |
+--------+--------+</screen>
              <para>The cluster creation operation may take several minutes to complete. During this
                time the status returned from the previous command may show states other than
                  <literal>Active</literal>. A cluster also can be created with the
                  <literal>wait</literal> flag. In that case, the cluster creation command will not
                be finished until the cluster is moved to the <literal>Active</literal>
                state.</para>
            </section>
            <section xml:id="run-a-mapreduce-job-to-check-hadoop-installation">
              <title>8. Run a MapReduce job to check Hadoop installation</title>
              <para>Check that your Hadoop installation is working properly by running an example
                job on the cluster manually.</para>
              <itemizedlist>
                <listitem>
                  <para>Login to the NameNode (usually the master node) via ssh with the ssh-key
                    used above:</para>
                </listitem>
              </itemizedlist>
              <screen language="console">$ ssh -i my_stack.pem ubuntu@&lt;namenode_ip&gt;</screen>
              <itemizedlist>
                <listitem>
                  <para>Switch to the hadoop user:</para>
                </listitem>
              </itemizedlist>
              <screen language="console">$ sudo su hadoop</screen>
              <itemizedlist>
                <listitem>
                  <para>Go to the shared hadoop directory and run the simplest MapReduce
                    example:</para>
                </listitem>
              </itemizedlist>
              <screen language="console">$ cd /opt/hadoop-&lt;plugin_version&gt;/share/hadoop/mapreduce
$ /opt/hadoop-&lt;plugin_version&gt;/bin/hadoop jar hadoop-mapreduce-examples-&lt;plugin_version&gt;.jar pi 10 100</screen>
              <para>Congratulations! Your Hadoop cluster is ready to use, running on your OpenStack
                cloud.</para>
            </section>
          </section>
          <section xml:id="elastic-data-processing-edp-1">
            <title>Elastic Data Processing (EDP)</title>
            <para>Job Binaries are the entities you define and upload the source code (mains and
              libraries) for your job. First, you need to download your binary file or script to
              swift container and register your file in Sahara with the command:</para>
            <screen>(openstack) dataprocessing job binary create --url "swift://integration.sahara/hive.sql"\
  --username username --password password --description "My first job binary" hive-binary</screen>
            <section xml:id="data-sources-1">
              <title>Data Sources</title>
              <para>Data Sources are entities where the input and output from your jobs are housed.
                You can create data sources which are related to Swift, Manila or HDFS. You need to
                set the type of data source (swift, hdfs, manila, maprfs), name and url. The next
                two commands will create input and output data sources in swift.</para>
              <screen>$ openstack dataprocessing data source create --type swift --username admin --password admin \
   --url "swift://integration.sahara/input.txt" input

$ openstack dataprocessing data source create --type swift --username admin --password admin \
   --url "swift://integration.sahara/output.txt" output</screen>
              <para>If you want to create data sources in hdfs, use valid hdfs URLs:</para>
              <screen>$ openstack dataprocessing data source create --type hdfs --url "hdfs://tmp/input.txt" input

$ openstack dataprocessing data source create --type hdfs --url "hdfs://tmp/output.txt" output</screen>
            </section>
            <section xml:id="job-templates-jobs-in-api">
              <title>Job Templates (Jobs in API)</title>
              <para>In this step you need to create a job template. You have to set the type of the
                job template using the <literal>type</literal> parameter. Choose the main library
                using the job binary which was created in the previous step and set a name for the
                job template.</para>
              <para>Example of the command:</para>
              <screen>$ openstack dataprocessing job template create --type Hive \
   --name hive-job-template --main hive-binary</screen>
            </section>
            <section xml:id="jobs-job-executions-in-api">
              <title>Jobs (Job Executions in API)</title>
              <para>This is the last step in our guide. In this step you need to launch your job.
                You need to pass the following arguments:</para>
              <itemizedlist>
                <listitem>
                  <para>The name or ID of input/output data sources for the job.</para>
                </listitem>
                <listitem>
                  <para>The name or ID of the job template.</para>
                </listitem>
                <listitem>
                  <para>The name or ID of the cluster on which to run the job.</para>
                </listitem>
              </itemizedlist>
              <para>For instance:</para>
              <screen>$ openstack dataprocessing job execute --input input --output output \
  --job-template hive-job-template --cluster my-first-cluster</screen>
              <para>You can check status of your job with the command:</para>
              <screen>$ openstack dataprocessing job show &lt;id_of_your_job&gt;</screen>
              <para>Once the job is marked as successful, you can check the output data source. It
                will contain the output data of this job.</para>
            </section>
          </section>
        </section>
        <section xml:id="dashboard-user-guide" xml:base="dashboard-user-guide">
          <title>Sahara (Data Processing) UI User Guide</title>
          <para>This guide assumes that you already have the sahara service and Horizon dashboard up
            and running. Don’t forget to make sure that sahara is registered in Keystone. If you
            require assistance with that, see the <link xlink:href="https://docs.openstack.org/sahara/pike/install/installation-guide-html">installation guide</link>.</para>
          <para>The sections below give a panel by panel overview of setting up clusters and running
            jobs. For a description of using the guided cluster and job tools, look at <xref
              linkend="launching-a-cluster-via-the-cluster-creation-guide"/> and <xref
              linkend="running-a-job-via-the-job-execution-guide"/>.</para>
          <section xml:id="launching-a-cluster-via-the-sahara-ui">
            <title>Launching a cluster via the sahara UI</title>
          </section>
          <section xml:id="registering-an-image-1">
            <title>Registering an Image</title>
            <procedure>
              <step>
                <para>Navigate to the <literal>Project</literal> dashboard, then the <literal>Data Processing</literal> tab, then
                  click on the <literal>Clusters</literal> panel and finally the <literal>Image Registry</literal> tab.</para>
              </step>
              <step>
                <para>From that page, click on the <literal>Register Image</literal> button at the top right.</para>
              </step>
              <step>
                <para>Choose the image that you’d like to register with sahara.</para>
              </step>
              <step>
                <para>Enter the username of the cloud-init user on the image.</para>
              </step>
              <step>
                <para>Choose plugin and version to make the image available only for the intended
                  clusters.</para>
              </step>
              <step>
                <para>Click the <literal>Done</literal> button to finish the registration.</para>
              </step>
            </procedure>
          </section>
          <section xml:id="create-node-group-templates-2">
            <title>Create Node Group Templates</title>
            <procedure>
              <step>
                <para>Navigate to the <literal>Project</literal> dashboard, then the <literal>Data Processing</literal> tab, then
                  click on the <literal>Clusters</literal> panel and then the <literal>Node Group Templates</literal> tab.</para>
              </step>
              <step>
                <para>From that page, click on the <literal>Create Template</literal> button at the top right.</para>
              </step>
              <step>
                <para>Choose your desired plugin name and version from the drop-downs and click
                  <literal>Next</literal>.</para>
              </step>
              <step>
                <para>Give your Node Group Template a name (description is optional).</para>
              </step>
              <step>
                <para>Choose a flavor for this template (based on your CPU/memory/disk needs).</para>
              </step>
              <step>
                <para>Choose the storage location for your instance, this can be either Ephemeral
                  Drive or Cinder Volume. If you choose Cinder Volume, you will need to add
                  additional configuration.</para>
              </step>
              <step>
                <para>Switch to the Node processes tab and choose which processes should be run for
                  all instances that are spawned from this Node Group Template.</para>
              </step>
              <step>
                <para>Click on the <literal>Create</literal> button to finish creating your Node Group
                  Template.</para>
              </step>
            </procedure>
          </section>
          <section xml:id="create-a-cluster-template-2">
            <title>Create a Cluster Template</title>
            <procedure>
              <step>
                <para>Navigate to the <literal>Project</literal> dashboard, then the <literal>Data Processing</literal> tab, then
                  click on the <literal>Clusters</literal> panel and finally the <literal>Cluster Templates</literal> tab.</para>
              </step>
              <step>
                <para>From that page, click on the <literal>Create Template</literal> button at the top right.</para>
              </step>
              <step>
                <para>Choose your desired plugin name and version from the drop-downs and click
                  <literal>Next</literal>.</para>
              </step>
              <step>
                <para>Under the <literal>Details</literal> tab, you must give your template a name.</para>
              </step>
              <step>
                <para>Under the <literal>Node Groups</literal> tab, you should add one or more nodes that can be
                  based on one or more templates.</para>
              </step>
            </procedure>
            <itemizedlist>
              <listitem>
                <para>To do this, start by choosing a Node Group Template from the dropdown and
                  click the <literal>+</literal> button.</para>
              </listitem>
              <listitem>
                <para>You can adjust the number of nodes to be spawned for this node group via the
                  text box or the <literal>-</literal> and <literal>+</literal> buttons.</para>
              </listitem>
              <listitem>
                <para>Repeat these steps if you need nodes from additional node group
                  templates.</para>
              </listitem>
            </itemizedlist>
            <procedure>
              <step>
                <para>Optionally, you can adjust your configuration further by using the <literal>General
                  Parameters</literal>, <literal>HDFS Parameters</literal> and
                  <literal>MapReduce Parameters</literal> tabs.</para>
              </step>
              <step>
                <para>If you have Designate DNS service you can choose the domain name in <literal>DNS</literal> tab
                  for internal and external hostname resolution.</para>
              </step>
              <step>
                <para>Click on the <literal>Create</literal> button to finish creating your Cluster Template.</para>
              </step>
            </procedure>
          </section>
          <section xml:id="launching-a-cluster">
            <title>Launching a Cluster</title>
            <procedure>
              <step>
                <para>Navigate to the <literal>Project</literal> dashboard, then the <literal>Data Processing</literal> tab, then
                  click on the <literal>Clusters</literal> panel and lastly, click on the <literal>Clusters</literal> tab.</para>
              </step>
              <step>
                <para>Click on the <literal>Launch Cluster</literal> button at the top right.</para>
              </step>
              <step>
                <para>Choose your desired plugin name and version from the drop-downs and click
                  <literal>Next</literal>.</para>
              </step>
              <step>
                <para>Give your cluster a name (required).</para>
              </step>
              <step>
                <para>Choose which cluster template should be used for your cluster.</para>
              </step>
              <step>
                <para>Choose the image that should be used for your cluster (if you do not see any
                  options here, see <xref linkend="registering-an-image-1"/> above).</para>
              </step>
              <step>
                <para>Optionally choose a key-pair that can be used to authenticate to your cluster
                  instances.</para>
              </step>
              <step>
                <para>Click on the <literal>Create</literal> button to start your cluster.</para>
              </step>
            </procedure>
            <itemizedlist>
              <listitem>
                <para>Your cluster’s status will display on the Clusters table.</para>
              </listitem>
              <listitem>
                <para>It will likely take several minutes to reach the <literal>Active</literal> state.</para>
              </listitem>
            </itemizedlist>
          </section>
          <section xml:id="scaling-a-cluster">
            <title>Scaling a Cluster</title>
            <procedure>
              <step>
                <para>From the <literal>Data Processing/Clusters</literal> page (<literal>Clusters</literal> tab), click on the <literal>Scale
                  Cluster</literal> button of the row that contains the cluster that you want to scale.</para>
              </step>
              <step>
                <para>You can adjust the numbers of instances for existing <literal>Node Group Templates</literal>.</para>
              </step>
              <step>
                <para>You can also add a new <literal>Node Group Template</literal> and choose a number of instances to
                  launch.</para>
              </step>
            </procedure>
            <itemizedlist>
              <listitem>
                <para>This can be done by selecting your desired Node Group Template from the
                  drop-down and clicking the <literal>+</literal> button.</para>
              </listitem>
              <listitem>
                <para>Your new <literal>Node Group</literal> will appear below and you can adjust the number of
                  instances via the text box or the <literal>+</literal> and <literal>-</literal> buttons.</para>
              </listitem>
            </itemizedlist>
            <procedure>
              <step>
                <para>To confirm the scaling settings and trigger the spawning or deletion of
                  instances, click on <literal>Scale</literal>.</para>
              </step>
            </procedure>
          </section>
          <section xml:id="elastic-data-processing-edp-2">
            <title>Elastic Data Processing (EDP)</title>
          </section>
          <section xml:id="data-sources-2">
            <title>Data Sources</title>
            <para>Data Sources are where the input and output from your jobs are housed.</para>
            <procedure>
              <step>
                <para>From the <literal>Data Processing/Jobs</literal> page (<literal>Data Sources</literal> tab), click on the <literal>Create
                  Data Source</literal> button at the top right.</para>
              </step>
              <step>
                <para>Give your data Source a name.</para>
              </step>
              <step>
                <para>Enter the URL of the data source.</para>
              </step>
            </procedure>
            <itemizedlist>
              <listitem>
                <para>For a swift object, enter &lt;container&gt;/&lt;path&gt; (For example,
                    <emphasis>mycontainer/inputfile</emphasis>) sahara will prepend
                    <emphasis>swift://</emphasis> for you.</para>
              </listitem>
              <listitem>
                <para>For an HDFS object, enter an absolute path, a relative path or a full
                  URL:</para>
                <itemizedlist>
                  <listitem>
                    <para><emphasis>/my/absolute/path</emphasis> indicates an absolute path in the
                      cluster HDFS.</para>
                  </listitem>
                  <listitem>
                    <para><emphasis>my/path</emphasis> indicates the path
                        <emphasis>/user/hadoop/my/path</emphasis> in the cluster HDFS assuming the
                      defined HDFS user is <emphasis>hadoop</emphasis>.</para>
                  </listitem>
                  <listitem>
                    <para><emphasis>hdfs://host:port/path</emphasis> can be used to indicate any
                      HDFS location.</para>
                  </listitem>
                </itemizedlist>
              </listitem>
            </itemizedlist>
            <procedure>
              <step>
                <para>Enter the username and password for the Data Source (also see <xref
                    linkend="additional-notes"/>).</para>
              </step>
              <step>
                <para>Enter an optional description.</para>
              </step>
              <step>
                <para>Click on <literal>Create</literal>.</para>
              </step>
              <step>
                <para>Repeat for additional data sources.</para>
              </step>
            </procedure>
          </section>
          <section xml:id="job-binaries-1">
            <title>Job Binaries</title>
            <para>Job Binaries are where you define and upload the source code (mains and libraries) for
              your job.</para>
            <procedure>
              <step>
                <para>From the <literal>Data Processing/Jobs</literal> (<literal>Job Binaries</literal> tab), click on the <literal>Create Job
                  Binary</literal> button at the top right.</para>
              </step>
              <step>
                <para>Give your Job Binary a name (this can be different than the actual filename).</para>
              </step>
              <step>
                <para>Choose the type of storage for your Job Binary.</para>
              </step>
            </procedure>
            <itemizedlist>
              <listitem>
                <para>For swift, enter the URL of your binary
                  (&lt;container&gt;/&lt;path&gt;) as well as the username and
                  password (also see <xref linkend="additional-notes"/>).</para>
              </listitem>
              <listitem>
                <para>For <literal>Internal database</literal>, you can choose from <literal>Create a script</literal> or <literal>Upload a
                  new file</literal>.</para>
              </listitem>
            </itemizedlist>
            <procedure>
              <step>
                <para>Enter an optional description.</para>
              </step>
              <step>
                <para>Click on <literal>Create</literal>.</para>
              </step>
              <step>
                <para>Repeat for additional Job Binaries.</para>
              </step>
            </procedure>
          </section>
          <section xml:id="job-templates-known-as-jobs-in-the-api">
            <title>Job Templates (Known as “Jobs” in the API)</title>
            <para>Job templates are where you define the type of job you’d like to run as well as
              which “ob Binaries are required.</para>
            <procedure>
              <step>
                <para>From the <literal>Data Processing/Jobs</literal> page (<literal>Job Templates</literal> tab), click on the <literal>Create
                  Job Template</literal> button at the top right.</para>
              </step>
              <step>
                <para>Give your Job Template a name.</para>
              </step>
              <step>
                <para>Choose the type of job you’d like to run.</para>
              </step>
              <step>
                <para>Choose the main binary from the drop-down.</para>
                <itemizedlist>
                  <listitem>
                    <para>This is required for Hive, Pig, and Spark jobs.</para>
                  </listitem>
                  <listitem>
                    <para>Other job types do not use a main binary.</para>
                  </listitem>
                </itemizedlist>
              </step>
              <step>
                <para>Enter an optional description for your Job Template.</para>
              </step>
              <step>
                <para>Click on the <literal>Libs</literal> tab and choose any libraries needed by your job
                  template.</para>
                <itemizedlist>
                  <listitem>
                    <para>MapReduce and Java jobs require at least one library.</para>
                  </listitem>
                  <listitem>
                    <para>Other job types may optionally use libraries.</para>
                  </listitem>
                </itemizedlist>
              </step>
              <step>
                <para>Click <literal>Create</literal>.</para>
              </step>
            </procedure>
          </section>
          <section xml:id="jobs-known-as-job-executions-in-the-api">
            <title>Jobs (Known as Job Executions in the API)</title>
            <para>Jobs are what you get by launching a job template. You can monitor the status of
              your job to see when it has completed its run.</para>
            <procedure>
              <step>
                <para>From the <literal>Data Processing/Jobs</literal> page (<literal>Job Templates</literal> tab), find the row that
                  contains the job template you want to launch and click either <literal>Launch on New
                  Cluster</literal> or <literal>Launch on Existing Cluster</literal> the right side of that row.</para>
              </step>
              <step>
                <para>Choose the cluster (already running–see <xref linkend="launching-a-cluster"/>
                  above) on which you would like the job to run.</para>
              </step>
              <step>
                <para>Choose the input and output data sources (Data sources defined above).</para>
              </step>
              <step>
                <para>If additional configuration is required, click on the <literal>Configure</literal> tab.</para>
              </step>
            </procedure>
            <itemizedlist>
              <listitem>
                <para>Additional configuration properties can be defined by clicking on the <literal>Add</literal>
                  button.</para>
              </listitem>
              <listitem>
                <para>An example configuration entry might be <literal>mapred.mapper.class</literal> for the name and
                  <literal>org.apache.oozie.example.SampleMapper</literal> for the value.</para>
              </listitem>
            </itemizedlist>
            <procedure>
              <step>
                <para>Click <literal>Launch</literal>. To monitor the status of your job, you can navigate to the
                  <literal>Data Processing/Jobs panel</literal> and click on the <literal>Jobs</literal> tab.</para>
              </step>
              <step>
                <para>You can relaunch a Job from the Jobs page by using the <literal>Relaunch on New
                  Cluster</literal> or <literal>Relaunch on Existing Cluster</literal> links.</para>
              </step>
            </procedure>
            <itemizedlist>
              <listitem>
                <para>Relaunching a New Cluster takes you through the forms to start a new cluster
                  before letting you specify input/output Data Sources and job configuration.</para>
              </listitem>
              <listitem>
                <para>Relaunching an Existing Cluster prompts you for input and output data sources as
                  well as allow you to change job configuration before launching the job.</para>
              </listitem>
            </itemizedlist>
          </section>
          <section xml:id="example-jobs">
            <title>Example Jobs</title>
            <para>There are sample jobs located in the sahara repository. In this section, we will
              give a walkthrough on how to run those jobs via the Horizon UI. These steps assume
              that you already have a cluster up and running (in the “Active” state). You may want
              to clone into <link xlink:href="https://git.openstack.org/cgit/openstack/sahara-tests/"/> so that you will
              have all of the source code and inputs stored locally.</para>
            <procedure>
              <step>
                <para>Sample Pig job - <link xlink:href="https://git.openstack.org/cgit/openstack/sahara-tests/tree/sahara_tests/scenario/defaults/edp-examples/edp-pig/cleanup-string/example.pig"
                  /></para>
              </step>
            </procedure>
            <itemizedlist>
              <listitem>
                <para>Load the input data file from <link xlink:href="https://git.openstack.org/cgit/openstack/sahara-tests/tree/sahara_tests/scenario/defaults/edp-examples/edp-pig/cleanup-string/data/input"
                  /> into swift</para>
                <itemizedlist>
                  <listitem>
                    <para>Click on <literal>Project/Object Store/Containers</literal> and create a container with any
                      name (We used “samplecontainer” for our purposes).</para>
                  </listitem>
                  <listitem>
                    <para>Click on <literal>Upload Object</literal> and give the object a name (“piginput” in this
                      case).</para>
                  </listitem>
                </itemizedlist>
              </listitem>
              <listitem>
                <para>Navigate to <literal>Data Processing/Jobs/Data Sources</literal>, Click on <literal>Create Data
                  Source</literal>.</para>
                <itemizedlist>
                  <listitem>
                    <para>Name your data source (“pig-input-ds” in this sample).</para>
                  </listitem>
                  <listitem>
                    <para><literal>Type = Swift, URL samplecontainer/piginput</literal>, fill-in the source
                      username and password fields and click <literal>Create</literal>.</para>
                  </listitem>
                </itemizedlist>
              </listitem>
              <listitem>
                <para>Create another data source to use as output for the job.</para>
                <itemizedlist>
                  <listitem>
                    <para><literal>Name = pig-output-ds</literal>, <literal>Type = Swift</literal>, <literal>URL = samplecontainer/pigoutput</literal>,
                      source username and password, <literal>Create</literal>.</para>
                  </listitem>
                </itemizedlist>
              </listitem>
              <listitem>
                <para>Store your job binaries in the sahara database.</para>
                <itemizedlist>
                  <listitem>
                    <para>Navigate to <literal>Data Processing/Jobs/Job Binaries</literal>, Click on <literal>Create Job
                      Binary</literal>.</para>
                  </listitem>
                  <listitem>
                    <para><literal>Name = example.pig</literal>, <literal>Storage type = Internal database</literal>, click <literal>Browse</literal> and
                      find <literal>example.pig</literal> wherever you checked out the sahara project
                      <literal>&lt;sahara-tests root&gt;/etc/edp-examples/edp-pig/trim-spaces</literal>.</para>
                  </listitem>
                  <listitem>
                    <para>Create another job binary: <literal>Name = edp-pig-udf-stringcleaner.jar</literal>, <literal>Storage
                      type = Internal database</literal>, click <literal>Browse</literal> and find <literal>edp-pig-udf-stringcleaner.jar</literal>
                      wherever you checked out the sahara project <literal>&lt;sahara-tests
                      root&gt;/sahara_tests/scenario/defaults/edp-examples/
                      edp-pig/cleanup-string/</literal>.</para>
                  </listitem>
                </itemizedlist>
              </listitem>
              <listitem>
                <para>Create a Job Template</para>
                <itemizedlist>
                  <listitem>
                    <para>Navigate to Data Processing/Jobs/Job Templates, Click on Create Job
                      Template</para>
                  </listitem>
                  <listitem>
                    <para>Name = pigsample, Job Type = Pig, Choose “example.pig” as the main
                      binary</para>
                  </listitem>
                  <listitem>
                    <para>Click on the “Libs” tab and choose “edp-pig-udf-stringcleaner.jar”, then
                      hit the “Choose” button beneath the dropdown, then click on “Create”</para>
                  </listitem>
                </itemizedlist>
              </listitem>
              <listitem>
                <para>Launch your job</para>
                <itemizedlist>
                  <listitem>
                    <para>To launch your job from the Job Templates page, click on the down arrow at
                      the far right of the screen and choose “Launch on Existing Cluster”</para>
                  </listitem>
                  <listitem>
                    <para>For the input, choose “pig-input-ds”, for output choose “pig-output-ds”.
                      Also choose whichever cluster you’d like to run the job on</para>
                  </listitem>
                  <listitem>
                    <para>For this job, no additional configuration is necessary, so you can just
                      click on “Launch”</para>
                  </listitem>
                  <listitem>
                    <para>You will be taken to the “Jobs” page where you can see your job progress
                      through “PENDING, RUNNING, SUCCEEDED” phases</para>
                  </listitem>
                  <listitem>
                    <para>When your job finishes with “SUCCEEDED”, you can navigate back to Object
                      Store/Containers and browse to the samplecontainer to see your output. It
                      should be in the “pigoutput” folder</para>
                  </listitem>
                </itemizedlist>
              </listitem>
            </itemizedlist>
            <procedure>
              <step>
                <para>Sample Spark job - <link xlink:href="https://git.openstack.org/cgit/openstack/sahara-tests/tree/sahara_tests/scenario/defaults/edp-examples/edp-spark"
                  /> You can clone into <link xlink:href="https://git.openstack.org/cgit/openstack/sahara-tests/"/> for quicker
                  access to the files for this sample job.</para>
              </step>
            </procedure>
            <itemizedlist>
              <listitem>
                <para>Store the Job Binary in the sahara database</para>
                <itemizedlist>
                  <listitem>
                    <para>Navigate to Data Processing/Jobs/Job Binaries, Click on Create Job
                      Binary</para>
                  </listitem>
                  <listitem>
                    <para>Name = sparkexample.jar, Storage type = Internal database, Browse to the
                      location &lt;sahara-tests root&gt;/sahara_tests/scenario/defaults/
                      edp-examples/edp-spark/ and choose spark-wordcount.jar, Click “Create”</para>
                  </listitem>
                </itemizedlist>
              </listitem>
              <listitem>
                <para>Create a Job Template</para>
                <itemizedlist>
                  <listitem>
                    <para>Name = sparkexamplejob, Job Type = Spark, Main binary = Choose
                      sparkexample.jar, Click “Create”</para>
                  </listitem>
                </itemizedlist>
              </listitem>
              <listitem>
                <para>Launch your job</para>
                <itemizedlist>
                  <listitem>
                    <para>To launch your job from the Job Templates page, click on the down arrow at
                      the far right of the screen and choose “Launch on Existing Cluster”</para>
                  </listitem>
                  <listitem>
                    <para>Choose whichever cluster you’d like to run the job on</para>
                  </listitem>
                  <listitem>
                    <para>Click on the “Configure” tab</para>
                  </listitem>
                  <listitem>
                    <para>Set the main class to be: sahara.edp.spark.SparkWordCount</para>
                  </listitem>
                  <listitem>
                    <para>Under Arguments, click Add and fill url for the input file, once more
                      click Add and fill url for the output file.</para>
                  </listitem>
                  <listitem>
                    <para>Click on Launch</para>
                  </listitem>
                  <listitem>
                    <para>You will be taken to the “Jobs” page where you can see your job progress
                      through “PENDING, RUNNING, SUCCEEDED” phases</para>
                  </listitem>
                  <listitem>
                    <para>When your job finishes with “SUCCEEDED”, you can see your results in your
                      output file.</para>
                  </listitem>
                  <listitem>
                    <para>The stdout and stderr files of the command used for executing your job are
                      located at /tmp/spark-edp/&lt;name of job template&gt;/&lt;job
                      id&gt; on Spark master node in case of Spark clusters, or on Spark
                      JobHistory node in other cases like Vanilla, CDH and so on.</para>
                  </listitem>
                </itemizedlist>
              </listitem>
            </itemizedlist>
          </section>
          <section xml:id="additional-notes">
            <title>Additional Notes</title>
            <procedure>
              <step>
                <para>Throughout the sahara UI, you will find that if you try to delete an object
                  that you will not be able to delete it if another object depends on it. An example
                  of this would be trying to delete a Job Template that has an existing Job. In
                  order to be able to delete that job, you would first need to delete any Job
                  Templates that relate to that job.</para>
              </step>
              <step>
                <para>In the examples above, we mention adding your username/password for the swift
                  Data Sources. It should be noted that it is possible to configure sahara such that
                  the username and password credentials are <emphasis>not</emphasis>
                  required. For more information on that, refer to the <link xlink:href="https://docs.openstack.org/sahara/pike/admin/advanced-configuration-guide.html">Sahara Advanced Configuration Guide</link>.</para>
              </step>
            </procedure>
          </section>
          <section xml:id="launching-a-cluster-via-the-cluster-creation-guide">
            <title>Launching a cluster via the Cluster Creation Guide</title>
            <procedure>
              <step>
                <para>Under the Data Processing group, choose “Clusters” and then click on the
                  “Clusters” tab. The “Cluster Creation Guide” button is above that table. Click on
                  it.</para>
              </step>
              <step>
                <para>Click on the “Choose Plugin” button then select the cluster type from the
                  Plugin Name dropdown and choose your target version. When done, click on “Select”
                  to proceed.</para>
              </step>
              <step>
                <para>Click on “Create a Master Node Group Template”. Give your template a name,
                  choose a flavor and choose which processes should run on nodes launched for this
                  node group. The processes chosen here should be things that are more server-like
                  in nature (namenode, oozieserver, spark master, etc). Optionally, you can set
                  other options here such as availability zone, storage, security and process
                  specific parameters. Click on “Create” to proceed.</para>
              </step>
              <step>
                <para>Click on “Create a Worker Node Group Template”. Give your template a name,
                  choose a flavor and choose which processes should run on nodes launched for this
                  node group. Processes chosen here should be more worker-like in nature (datanode,
                  spark slave, task tracker, etc). Optionally, you can set other options here such
                  as availability zone, storage, security and process specific parameters. Click on
                  “Create” to proceed.</para>
              </step>
              <step>
                <para>Click on “Create a Cluster Template”. Give your template a name. Next, click
                  on the “Node Groups” tab and enter the count for each of the node groups (these
                  are pre-populated from steps 3 and 4). It would be common to have 1 for the
                  “master” node group type and some larger number of “worker” instances depending on
                  you desired cluster size. Optionally, you can also set additional parameters for
                  cluster-wide settings via the other tabs on this page. Click on “Create” to
                  proceed.</para>
              </step>
              <step>
                <para>Click on “Launch a Cluster”. Give your cluster a name and choose the image
                  that you want to use for all instances in your cluster. The cluster template that
                  you created in step 5 is already pre-populated. If you want ssh access to the
                  instances of your cluster, select a keypair from the dropdown. Click on “Launch”
                  to proceed. You will be taken to the Clusters panel where you can see your cluster
                  progress toward the Active state.</para>
              </step>
            </procedure>
          </section>
          <section xml:id="running-a-job-via-the-job-execution-guide">
            <title>Running a job via the Job Execution Guide</title>
            <procedure>
              <step>
                <para>Under the Data Processing group, choose “Jobs” and then click on the “Jobs”
                  tab. The “Job Execution Guide” button is above that table. Click on it.</para>
              </step>
              <step>
                <para>Click on “Select type” and choose the type of job that you want to run.</para>
              </step>
              <step>
                <para>If your job requires input/output data sources, you will have the option to
                  create them via the “Create a Data Source” button (Note: This button will not be
                  shown for job types that do not require data sources). Give your data source a
                  name and choose the type. If you have chosen swift, you may also enter the
                  username and password. Enter the URL for your data source. For more details on
                  what the URL should look like, see <xref linkend="data-sources-1"/>.</para>
              </step>
              <step>
                <para>Click on “Create a job template”. Give your job template a name. Depending on
                  the type of job that you’ve chosen, you may need to select your main binary and/or
                  additional libraries (available from the “Libs” tab). If you have not yet uploaded
                  the files to run your program, you can add them via the “+” icon next to the
                  “Choose a main binary” select box.</para>
              </step>
              <step>
                <para>Click on “Launch job”. Choose the active cluster where you want to run you
                  job. Optionally, you can click on the “Configure” tab and provide any required
                  configuration, arguments or parameters for your job. Click on “Launch” to execute
                  your job. You will be taken to the Jobs tab where you can monitor the state of
                  your job as it progresses.</para>
              </step>
            </procedure>
          </section>
        </section>
        <section xml:id="features-overview" xml:base="features">
          <title>Features Overview</title>
          <para>This page highlights some of the most prominent features available in sahara. The
            guidance provided here is primarily focused on the runtime aspects of
            sahara. For discussions about configuring the sahara
            server processes see the <link xlink:href="https://docs.openstack.org/sahara/pike/admin/configuration-guide.html">Sahara Configuration Guide</link> and
            <link xlink:href="https://docs.openstack.org/sahara/pike/admin/advanced-configuration-guide.html">Sahara Advanced Configuration Guide</link></para>
          <section xml:id="anti-affinity">
            <title>Anti-affinity</title>
            <para>One of the problems with running data processing applications on OpenStack is the
              inability to control where an instance is actually running. It is not always possible
              to ensure that two new virtual machines are started on different physical machines. As
              a result, any replication within the cluster is not reliable because all replicas may
              be co-located on one physical machine. To remedy this, sahara provides the
              anti-affinity feature to explicitly command all instances of the specified processes
              to spawn on different Compute nodes. This is especially useful for Hadoop data node
              processes to increase HDFS replica reliability.</para>
            <para>Starting with the Juno release, sahara can create server groups with the
                <literal>anti-affinity</literal> policy to enable this feature. Sahara creates one
              server group per cluster and assigns all instances with affected processes to this
              server group. Refer to the <link xlink:href="https://docs.openstack.org/nova/latest/">Nova documentation</link> on how
              server groups work.</para>
            <para>This feature is supported by all plugins out of the box, and can be enabled during
              the cluster template creation.</para>
          </section>
          <section xml:id="block-storage-support">
            <title>Block Storage support</title>
            <para>OpenStack Block Storage (cinder) can be used as an alternative for ephemeral
              drives on instances. Using Block Storage volumes increases the reliability of data
              which is important for HDFS services.</para>
            <para>A user can set how many volumes will be attached to each instance in a node group
              and the size of each volume. All volumes are attached during cluster creation and
              scaling operations.</para>
            <para>If volumes are used for the HDFS storage it’s important to make sure that the
              linear read-write operations as well as IOpS level are high enough to handle the
              workload. Volumes placed on the same compute host provide a higher level of
              performance.</para>
            <para>In some cases cinder volumes can be backed by a distributed storage like Ceph. In
              this type of installation it’s important to make sure that the network latency and
              speed do not become a blocker for HDFS. Distributed storage solutions usually provide
              their own replication mechanism. HDFS replication should be disabled so that it does
              not generate redundant traffic across the cloud.</para>
          </section>
          <section xml:id="cluster-scaling">
            <title>Cluster scaling</title>
            <para>Cluster scaling allows users to change the number of running instances in a
              cluster without needing to recreate the cluster. Users may increase or decrease the
              number of instances in node groups or add new node groups to existing clusters. If a
              cluster fails to scale properly, all changes will be rolled back.</para>
          </section>
          <section xml:id="data-locality">
            <title>Data locality</title>
            <para>For optimal performance, it is best for data processing applications to work on
              data local to the same rack, OpenStack Compute node, or virtual machine. Hadoop
              supports a data locality feature and can schedule jobs to task tracker nodes that are
              local for the input stream. In this manner the task tracker nodes can communicate
              directly with the local data nodes.</para>
            <para>Sahara supports topology configuration for HDFS and Object
	    Storage data source.s For more information on configuring this option see the <link xlink:href="https://docs.openstack.org/sahara/pike/admin/advanced-configuration-guide.html#data-locality-configuration">Data-locality configuration</link>.</para>
          </section>
          <section xml:id="volume-to-instance-locality">
            <title>Volume-to-instance locality</title>
            <para>Having an instance and an attached volume on the same physical host can be very
              helpful in order to achieve high-performance disk I/O operations. To achieve this,
              sahara provides access to the Block Storage volume instance locality
              functionality.</para>
            <para>For more information on using volume instance locality with sahara, see
              <link xlink:href="https://docs.openstack.org/sahara/pike/admin/advanced-configuration-guide.html#volume-instance-locality-configuration">Volume instance locality configuration</link>.</para>
          </section>
          <section xml:id="distributed-mode">
            <title>Distributed Mode</title>
            <para>The Installation Guide
              The Installation Guide <link xlink:href="https://docs.openstack.org/sahara/pike/install/installation-guide.html">Sahara Installation Guide</link>
                suggests launching sahara in
              distributed mode with <literal>sahara-api</literal> and
                <literal>sahara-engine</literal> processes potentially running on several machines
              simultaneously. Running in distributed mode allows sahara to offload intensive tasks
              to the engine processes while keeping the API process free to handle requests.</para>
            <para>For an expanded discussion of configuring sahara to run in distributed mode
              see <link xlink:href="https://docs.openstack.org/sahara/pike/admin/advanced-configuration-guide.html#distributed-mode-configuration">Distributed mode configuration</link>.</para>
          </section>
          <section xml:id="hadoop-hdfs-and-yarn-high-availability">
            <title>Hadoop HDFS and YARN High Availability</title>
            <para>Currently HDFS and YARN HA are supported with the HDP 2.4 plugin and CDH 5.7
              plugins.</para>
            <para>Hadoop HDFS and YARN High Availability provide an architecture to ensure that HDFS
              or YARN will continue to work in the result of an active namenode or resourcemanager
              failure. They use 2 namenodes and 2 resourcemanagers in an active/passive state to
              provide this availability.</para>
            <para>In the HDP 2.4 plugin, the feature can be enabled through dashboard in the Cluster
              Template creation form. High availability is achieved by using a set of journalnodes,
              Zookeeper servers, and ZooKeeper Failover Controllers (ZKFC), as well as additional
              configuration changes to HDFS and other services that use HDFS.</para>
            <para>In the CDH 5.7 plugin, HA for HDFS and YARN is enabled through adding several
              HDFS_JOURNALNODE roles in the node group templates of cluster template. The HDFS HA is
              enabled when HDFS_JOURNALNODE roles are added and the roles setup meets below
              requirements:</para>
            <itemizedlist>
              <listitem>
                <para>HDFS_JOURNALNODE number is odd, and at least 3.</para>
              </listitem>
              <listitem>
                <para>Zookeeper is enabled.</para>
              </listitem>
              <listitem>
                <para>NameNode and SecondaryNameNode are on different physical hosts by setting
                  anti-affinity.</para>
              </listitem>
              <listitem>
                <para>Cluster has both ResourceManager and StandByResourceManager.</para>
              </listitem>
            </itemizedlist>
            <para>In this case, the original SecondaryNameNode node will be used as the Standby
              NameNode.</para>
          </section>
          <section xml:id="networking-support">
            <title>Networking support</title>
            <para>Sahara supports both the nova-network and neutron implementations of OpenStack
              Networking. By default sahara is configured to behave as if the nova-network
              implementation is available. For OpenStack installations that are using the neutron
              projectsee <link xlink:href="https://docs.openstack.org/sahara/pike/admin/configuration-guide.html#neutron-nova-network">Networking configuration</link>.</para>
          </section>
          <section xml:id="object-storage-support">
            <title>Object Storage support</title>
            <para>Sahara can use OpenStack Object Storage (swift) to store job binaries and data
              sources utilized by its job executions and clusters. In order to leverage this support
              within Hadoop, including using Object Storage for data sources for EDP, Hadoop
              requires the application of a patch. For additional information about enabling this
              support, including patching Hadoop and configuring sahara, refer to the
              <link xlink:href="https://docs.openstack.org/sahara/pike/user/hadoop-swift.html">Swift Integration</link>.</para>
          </section>
          <section xml:id="shared-filesystem-support">
            <title>Shared Filesystem support</title>
            <para>Sahara can also use NFS shares through the OpenStack Shared Filesystem service
              (manila) to store job binaries and data sources. See <link xlink:href="https://docs.openstack.org/sahara/pike/user/edp.html">Elastic Data Processing (EDP)</link>
              for more information on this feature.</para>
          </section>
          <section xml:id="orchestration-support">
            <title>Orchestration support</title>
            <para>Sahara may use the <link xlink:href="https://wiki.openstack.org/wiki/Heat">OpenStack Orchestration engine</link>
              (heat) to provision nodes for clusters. For more information about enabling
              Orchestration usage in sahara see <link xlink:href="https://docs.openstack.org/sahara/pike/admin/configuration-guide.html#orchestration-configuration">Orchestration Configuration</link>.</para>
          </section>
          <section xml:id="dns-support">
            <title>DNS support</title>
            <para>Sahara can resolve hostnames of cluster instances by using DNS. For this Sahara
              uses designate. For additional details see
              <link xlink:href="https://docs.openstack.org/sahara/pike/admin/advanced-configuration-guide.html">Sahara Advanced Configuration Guide</link>.</para>
          </section>
          <section xml:id="kerberos-support">
            <title>Kerberos support</title>
            <para>You can protect your HDP or CDH cluster using MIT Kerberos security. To get more
              details about this, see the documentation for the appropriate plugin.</para>
          </section>
          <section xml:id="plugin-capabilities">
            <title>Plugin Capabilities</title>
            <para>The following table provides a plugin capability matrix:</para>
            <informaltable>
              <tgroup cols="5">
                <colspec colname="c0" colwidth="26"/>
                <colspec colname="c1" colwidth="9"/>
                <colspec colname="c2" colwidth="10"/>
                <colspec colname="c3" colwidth="10"/>
                <colspec colname="c4" colwidth="7"/>
                <thead>
                  <row>
                    <entry>
                      <para>Feature/Plugin</para>
                    </entry>
                    <entry>
                      <para>Vanilla</para>
                    </entry>
                    <entry>
                      <para>HDP</para>
                    </entry>
                    <entry>
                      <para>Cloudera</para>
                    </entry>
                    <entry>
                      <para>Spark</para>
                    </entry>
                  </row>
                </thead>
                <tbody>
                  <row>
                    <entry>
                      <para>Nova and Neutron network</para>
                    </entry>
                    <entry>
                      <para>x</para>
                    </entry>
                    <entry>
                      <para>x</para>
                    </entry>
                    <entry>
                      <para>x</para>
                    </entry>
                    <entry>
                      <para>x</para>
                    </entry>
                  </row>
                  <row>
                    <entry>
                      <para>Cluster Scaling</para>
                    </entry>
                    <entry>
                      <para>x</para>
                    </entry>
                    <entry>
                      <para>x</para>
                    </entry>
                    <entry>
                      <para>x</para>
                    </entry>
                    <entry>
                      <para>x</para>
                    </entry>
                  </row>
                  <row>
                    <entry>
                      <para>Swift Integration</para>
                    </entry>
                    <entry>
                      <para>x</para>
                    </entry>
                    <entry>
                      <para>x</para>
                    </entry>
                    <entry>
                      <para>x</para>
                    </entry>
                    <entry>
                      <para>x</para>
                    </entry>
                  </row>
                  <row>
                    <entry>
                      <para>Cinder Support</para>
                    </entry>
                    <entry>
                      <para>x</para>
                    </entry>
                    <entry>
                      <para>x</para>
                    </entry>
                    <entry>
                      <para>x</para>
                    </entry>
                    <entry>
                      <para>x</para>
                    </entry>
                  </row>
                  <row>
                    <entry>
                      <para>Data Locality</para>
                    </entry>
                    <entry>
                      <para>x</para>
                    </entry>
                    <entry>
                      <para>x</para>
                    </entry>
                    <entry>
                      <para>x</para>
                    </entry>
                    <entry>
                      <para>x</para>
                    </entry>
                  </row>
                  <row>
                    <entry>
                      <para>DNS</para>
                    </entry>
                    <entry>
                      <para>x</para>
                    </entry>
                    <entry>
                      <para>x</para>
                    </entry>
                    <entry>
                      <para>x</para>
                    </entry>
                    <entry>
                      <para>x</para>
                    </entry>
                  </row>
                  <row>
                    <entry>
                      <para>Kerberos</para>
                    </entry>
                    <entry>
                      <para>-</para>
                    </entry>
                    <entry>
                      <para>x</para>
                    </entry>
                    <entry>
                      <para>x</para>
                    </entry>
                    <entry>
                      <para>-</para>
                    </entry>
                  </row>
                  <row>
                    <entry>
                      <para>HDFS HA</para>
                    </entry>
                    <entry>
                      <para>-</para>
                    </entry>
                    <entry>
                      <para>x</para>
                    </entry>
                    <entry>
                      <para>x</para>
                    </entry>
                    <entry>
                      <para>-</para>
                    </entry>
                  </row>
                  <row>
                    <entry>
                      <para>EDP</para>
                    </entry>
                    <entry>
                      <para>x</para>
                    </entry>
                    <entry>
                      <para>x</para>
                    </entry>
                    <entry>
                      <para>x</para>
                    </entry>
                    <entry>
                      <para>x</para>
                    </entry>
                  </row>
                </tbody>
              </tgroup>
            </informaltable>
          </section>
          <section xml:id="security-group-management">
            <title>Security group management</title>
            <para>Security groups are sets of IP filter rules that are applied to an instance’s
              networking. They are project specified, and project members can edit the default rules
              for their group and add new rules sets. All projects have a “default” security group,
              which is applied to instances that have no other security group defined. Unless
              changed, this security group denies all incoming traffic.</para>
            <para>Sahara allows you to control which security groups will be used for created
              instances. This can be done by providing the <literal>security_groups</literal>
              parameter for the node group or node group template. The default for this option is an
              empty list, which will result in the default project security group being used for the
              instances.</para>
            <para>Sahara may also create a security group for instances in the node group
              automatically. This security group will only contain open ports for required instance
              processes and the sahara engine. This option is useful for development and for when
              your installation is secured from outside environments. For production environments we
              recommend controlling the security group policy manually.</para>
          </section>
          <section xml:id="shared-and-protected-resources-support">
            <title>Shared and protected resources support</title>
            <para>Sahara allows you to create resources that can be shared across projects and
              protected from modifications.</para>
            <para>To provide this feature all sahara objects that can be accessed through REST API
              have <literal>is_public</literal> and <literal>is_protected</literal> boolean fields.
              They can be initially created with enabled <literal>is_public</literal> and
                <literal>is_protected</literal> parameters or these parameters can be updated after
              creation. Both fields are set to <literal>False</literal> by default.</para>
            <para>If some object has its <literal>is_public</literal> field set to
                <literal>True</literal>, it means that it’s visible not only from the project in
              which it was created, but from any other projects too.</para>
            <para>If some object has its <literal>is_protected</literal> field set to
                <literal>True</literal>, it means that it can not be modified (updated, scaled,
              canceled or deleted) unless this field is set to <literal>False</literal>.</para>
            <para>Public objects created in one project can be used from other projects (for
              example, a cluster can be created from a public cluster template which is created in
              another project), but modification operations are possible only from the project in
              which object was created.</para>
          </section>
          <section xml:id="data-source-placeholders-support">
            <title>Data source placeholders support</title>
            <para>Sahara supports special strings that can be used in data source URLs. These
              strings will be replaced with appropriate values during job execution which allows the
              use of the same data source as an output multiple times.</para>
            <para>There are 2 types of string currently supported:</para>
            <itemizedlist>
              <listitem>
                <para><literal>%JOB_EXEC_ID%</literal> - this string will be replaced with the job
                  execution ID.</para>
              </listitem>
              <listitem>
                <para><literal>%RANDSTR(len)%</literal> - this string will be replaced with random
                  string of lowercase letters of length <literal>len</literal>.
                    <literal>len</literal> must be less than 1024.</para>
              </listitem>
            </itemizedlist>
            <para>After placeholders are replaced, the real URLs are stored in the
                <literal>data_source_urls</literal> field of the job execution object. This is used
              later to find objects created by a particular job run.</para>
          </section>
        </section>
        <section xml:id="registering-an-image-2">
          <title>Registering an Image</title>
          <para>Sahara deploys a cluster of machines using images stored in Glance.</para>
          <para>Each plugin has its own requirements on the image contents (see specific plugin
            documentation for details). Two general requirements for an image are to have the
            cloud-init and the ssh-server packages installed.</para>
          <para>Sahara requires the images to be registered in the Sahara Image Registry. A
            registered image must have two properties set:</para>
          <itemizedlist>
            <listitem>
              <para>username - a name of the default cloud-init user.</para>
            </listitem>
            <listitem>
              <para>tags - certain tags mark image to be suitable for certain plugins. The tags
                depend on the plugin used, you can find required tags in the plugin’s
                documentations.</para>
            </listitem>
          </itemizedlist>
          <para>The default username specified for these images is different for each
            distribution:</para>
          <informaltable>
            <tgroup cols="2">
              <colspec colname="c0" colwidth="14"/>
              <colspec colname="c1" colwidth="12"/>
              <thead>
                <row>
                  <entry>
                    <para>OS</para>
                  </entry>
                  <entry>
                    <para>username</para>
                  </entry>
                </row>
              </thead>
              <tbody>
                <row>
                  <entry>
                    <para>Ubuntu 12,14</para>
                  </entry>
                  <entry>
                    <para>ubuntu</para>
                  </entry>
                </row>
                <row>
                  <entry>
                    <para>Fedora</para>
                  </entry>
                  <entry>
                    <para>fedora</para>
                  </entry>
                </row>
                <row>
                  <entry>
                    <para>CentOS 6.x</para>
                  </entry>
                  <entry>
                    <para>cloud-user</para>
                  </entry>
                </row>
                <row>
                  <entry>
                    <para>CentOS 7.x</para>
                  </entry>
                  <entry>
                    <para>centos</para>
                  </entry>
                </row>
              </tbody>
            </tgroup>
          </informaltable>
        </section>
        <section xml:id="sahara-cluster-statuses-overview" xml:base="statuses">
          <title>Sahara Cluster Statuses Overview</title>
          <para>All Sahara Cluster operations are performed in multiple steps. A Cluster object has
            a <literal>Status</literal> attribute which changes when Sahara finishes one step of
            operations and starts another one. Also a Cluster object has a <literal>Status
              description</literal> attribute which changes whenever Cluster errors occur.</para>
          <variablelist>
            <varlistentry>
              <term>Sahara supports three types of Cluster operations:</term>
              <listitem>
                <itemizedlist>
                  <listitem>
                    <para>Create a new Cluster</para>
                  </listitem>
                  <listitem>
                    <para>Scale/Shrink an existing Cluster</para>
                  </listitem>
                  <listitem>
                    <para>Delete an existing Cluster</para>
                  </listitem>
                </itemizedlist>
              </listitem>
            </varlistentry>
          </variablelist>
          <section xml:id="creating-a-new-cluster">
            <title>Creating a new Cluster</title>
            <section xml:id="validating">
              <title>1. Validating</title>
              <para>Before performing any operations with OpenStack environment, Sahara validates
                user input.</para>
              <variablelist>
                <varlistentry>
                  <term>There are two types of validations, that are done:</term>
                  <listitem>
                    <itemizedlist>
                      <listitem>
                        <para>Check that a request contains all necessary fields and that the
                          request does not violate any constraints like unique naming, etc.</para>
                      </listitem>
                      <listitem>
                        <para>Plugin check (optional). The provisioning Plugin may also perform any
                          specific checks like a Cluster topology validation check.</para>
                      </listitem>
                    </itemizedlist>
                  </listitem>
                </varlistentry>
              </variablelist>
              <para>If any of the validations fails during creating, the Cluster object will still
                be kept in the database with an <literal>Error</literal> status. If any validations
                fails during scaling the <literal>Active</literal> Cluster, it will be kept with an
                  <literal>Active</literal> status. In both cases status description will contain
                error messages about the reasons of failure.</para>
            </section>
            <section xml:id="infraupdating">
              <title>2. InfraUpdating</title>
              <para>This status means that the Provisioning plugin is performing some infrastructure
                updates.</para>
            </section>
            <section xml:id="spawning">
              <title>3. Spawning</title>
              <variablelist>
                <varlistentry>
                  <term>Sahara sends requests to OpenStack for all resources to be created:</term>
                  <listitem>
                    <itemizedlist>
                      <listitem>
                        <para>VMs</para>
                      </listitem>
                      <listitem>
                        <para>Volumes</para>
                      </listitem>
                      <listitem>
                        <para>Floating IPs (if Sahara is configured to use Floating IPs)</para>
                      </listitem>
                    </itemizedlist>
                  </listitem>
                </varlistentry>
              </variablelist>
              <para>It takes some time for OpenStack to schedule all the required VMs and Volumes,
                so sahara will wait until all of the VMs are in an <literal>Active</literal>
                state.</para>
            </section>
            <section xml:id="waiting">
              <title>4. Waiting</title>
              <para>Sahara waits while VMs’ operating systems boot up and all internal
                infrastructure components like networks and volumes are attached and ready to
                use.</para>
            </section>
            <section xml:id="preparing">
              <title>5. Preparing</title>
              <para>Sahara prepares a Cluster for starting. This step includes generating the
                  <literal>/etc/hosts</literal> file or changing <literal>/etc/resolv.conf</literal>
                file (if you use Designate service), so that all instances can access each other by
                a hostname. Also Sahara updates the <literal>authorized_keys</literal> file on each
                VM, so that VMs can communicate without passwords.</para>
            </section>
            <section xml:id="configuring">
              <title>6. Configuring</title>
              <para>Sahara pushes service configurations to VMs. Both XML and JSON based
                configurations and environmental variables are set on this step.</para>
            </section>
            <section xml:id="starting">
              <title>7. Starting</title>
              <para>Sahara is starting Hadoop services on Cluster’s VMs.</para>
            </section>
            <section xml:id="active">
              <title>8. Active</title>
              <para>Active status means that a Cluster has started successfully and is ready to run
                EDP Jobs.</para>
            </section>
          </section>
          <section xml:id="scaling-shrinking-an-existing-cluster">
            <title>Scaling/Shrinking an existing Cluster</title>
            <section xml:id="id1-sahara">
              <title>1. Validating</title>
              <para>Sahara checks the scale/shrink request for validity. The Plugin method called
                for performing Plugin specific checks is different from the validation method in
                creation.</para>
            </section>
            <section xml:id="scaling">
              <title>2. Scaling</title>
              <para>Sahara performs database operations updating all affected existing Node Groups
                and creating new ones to join the existing Node Groups.</para>
            </section>
            <section xml:id="adding-instances">
              <title>3. Adding Instances</title>
              <para>Status is similar to <literal>Spawning</literal> in Cluster creation. Sahara
                adds required amount of VMs to the existing Node Groups and creates new Node
                Groups.</para>
            </section>
            <section xml:id="id2-sahara">
              <title>4. Configuring</title>
              <para>Status is similar to <literal>Configuring</literal> in Cluster creation. New
                instances are being configured in the same manner as already existing ones. The VMs
                in the existing Cluster are also updated with a new <literal>/etc/hosts</literal>
                file or <literal>/etc/resolv.conf</literal> file.</para>
            </section>
            <section xml:id="decommissioning">
              <title>5. Decommissioning</title>
              <para>Sahara stops Hadoop services on VMs that will be deleted from a Cluster.
                Decommissioning a Data Node may take some time because Hadoop rearranges data
                replicas around the Cluster, so that no data will be lost after that Data Node is
                deleted.</para>
            </section>
            <section xml:id="deleting-instances">
              <title>6. Deleting Instances</title>
              <variablelist>
                <varlistentry>
                  <term>Sahara sends requests to OpenStack to release unneeded resources:</term>
                  <listitem>
                    <itemizedlist>
                      <listitem>
                        <para>VMs</para>
                      </listitem>
                      <listitem>
                        <para>Volumes</para>
                      </listitem>
                      <listitem>
                        <para>Floating IPs (if they are used)</para>
                      </listitem>
                    </itemizedlist>
                  </listitem>
                </varlistentry>
              </variablelist>
            </section>
            <section xml:id="id3-sahara">
              <title>7. Active</title>
              <para>The same <literal>Active</literal> status as after Cluster creation.</para>
            </section>
          </section>
          <section xml:id="deleting-an-existing-cluster">
            <title>Deleting an existing Cluster</title>
            <section xml:id="deleting">
              <title>1. Deleting</title>
              <para>The only step, that releases all Cluster’s resources and removes it from the
                database.</para>
            </section>
          </section>
          <section xml:id="error-state">
            <title>Error State</title>
            <para>If the Cluster creation fails, the Cluster will enter the <literal>Error</literal>
              state. This status means the Cluster may not be able to perform any operations
              normally. This cluster will stay in the database until it is manually deleted. The
              reason for failure may be found in the sahara logs. Also, the status description will
              contain information about the error.</para>
            <para>If an error occurs during the <literal>Adding Instances</literal> operation,
              Sahara will first try to rollback this operation. If a rollback is impossible or fails
              itself, then the Cluster will also go into an <literal>Error</literal> state. If a
              rollback was successful, Cluster will get into an <literal>Active</literal> state and
              status description will contain a short message about the reason of <literal>Adding
                Instances</literal> failure.</para>
          </section>
        </section>
        <section xml:id="how-to-run-a-sahara-cluster-on-bare-metal-servers"
          xml:base="sahara-on-ironic">
          <title>How to run a Sahara cluster on bare metal servers</title>
          <para>Hadoop clusters are designed to store and analyze extremely large amounts of
            unstructured data in distributed computing environments. Sahara enables you to boot
            Hadoop clusters in both virtual and bare metal environments. When Booting Hadoop
            clusters with Sahara on bare metal servers, you benefit from the bare metal performance
            with self-service resource provisioning.</para>
          <procedure>
            <step>
              <para>Create a new OpenStack environment using Devstack as described in the <link
                  xlink:href="https://docs.openstack.org/devstack/latest/">Devstack Guide</link></para>
            </step>
            <step>
              <para>Install Ironic as described in the <link xlink:href="https://docs.openstack.org/ironic/pike/install/index.html">Ironic
                  Installation Guide</link>.</para>
            </step>
            <step>
              <para>Install Sahara as described in the <link xlink:href="https://docs.openstack.org/sahara/pike/install/index.html">Sahara Installation
                Guide</link>.</para>
            </step>
            <step>
              <para>Build the Sahara image and prepare it for uploading to Glance:</para>
              <itemizedlist>
                <listitem>
                  <para>Build an image for Sahara plugin with the <literal>-b</literal> flag. Use
                    sahara image elements when building the image. See <link xlink:href="https://git.openstack.org/cgit/openstack/sahara-image-elements">Sahara Image Elements</link>.</para>
                </listitem>
                <listitem>
                  <para>Convert the qcow2 image format to the raw format. For example:</para>
                </listitem>
              </itemizedlist>
            </step>
          </procedure>
          <screen language="console">$ qemu-img convert -O raw image-converted.qcow image-converted-from-qcow2.raw</screen>
          <itemizedlist>
            <listitem>
              <para>Mount the raw image to the system.</para>
            </listitem>
            <listitem>
              <para><literal>chroot</literal> to the mounted directory and remove the installed
                grub.</para>
            </listitem>
            <listitem>
              <para>Build grub2 from sources and install to <literal>/usr/sbin</literal>.</para>
            </listitem>
            <listitem>
              <para>In <literal>/etc/sysconfig/selinux</literal>, disable selinux
                  <literal>SELINUX=disabled</literal></para>
            </listitem>
            <listitem>
              <para>In the configuration file, set <literal>onboot=yes</literal> and
                  <literal>BOOTPROTO=dhcp</literal> for every interface.</para>
            </listitem>
            <listitem>
              <para>Add the configuration files for all interfaces in the
                  <literal>/etc/sysconfig/network-scripts</literal> directory.</para>
            </listitem>
          </itemizedlist>
          <procedure>
            <step>
              <para>Upload the Sahara disk image to Glance, and register it in the Sahara Image
                Registry. Referencing its separate kernel and initramfs images.</para>
            </step>
            <step>
              <para>Configure the bare metal network for the Sahara cluster nodes:</para>
              <itemizedlist>
                <listitem>
                  <para>Add bare metal servers to your environment manually referencing their IPMI
                    addresses (Ironic does not detect servers), for Ironic to manage the servers
                    power and network. For example:</para>
                </listitem>
              </itemizedlist>
            </step>
          </procedure>
          <screen language="bash">$ ironic node-create -d pxe_ipmitool \
$ -i ipmi_address=$IP_ADDRESS \
$ -i ipmi_username=$USERNAME \
$ -i ipmi_password=$PASSWORD \
$ -i pxe_deploy_kernel=$deploy.kernel.id \
$ -i pxe_deploy_ramdisk=$deploy.ramfs.id

$ ironic port-create -n $NODE_ID -a "$MAC_eth1"</screen>
          <itemizedlist>
            <listitem>
              <para>Add the hardware information:</para>
            </listitem>
          </itemizedlist>
          <screen language="bash">$ ironic node-update $NODE_ID add properties/cpus=$CPU \
$ properties/memory_mb=$RAM properties/local_gb=$ROOT_GB \
$ properties/cpu_arch='x86_64'</screen>
          <procedure>
            <step>
              <para>Add a special flavor for the bare metal instances with an arch meta parameter to
                match the virtual architecture of the server’s CPU with the metal one. For
                example:</para>
            </step>
          </procedure>
          <screen language="bash">$ nova flavor-create baremetal auto $RAM $DISK_GB $CPU
$ nova flavor-key baremetal set cpu_arch=x86_64</screen>
          <section xml:id="note">
            <title>Note:</title>
            <para>The vCPU ad vRAM parameters (x86_64 in the example) will not be applied because
              the operating system has access to the real CPU cores and RAM. Only the root disk
              parameter is applied, and Ironic will resize the root disk partition. Ironic supports
              only a flat network topology for the bare metal provisioning, you must use Neutron to
              configure it.</para>
            <procedure>
              <step>
                <para>Launch your Sahara cluster on Ironic from the cluster template:</para>
                <itemizedlist>
                  <listitem>
                    <para>Log in to Horizon.</para>
                  </listitem>
                  <listitem>
                    <variablelist>
                      <varlistentry>
                        <term>Go to Data Processing &gt; Node Group Templates.</term>
                        <listitem>
                          <itemizedlist>
                            <listitem>
                              <para>Find the templates that belong to the plugin you would like to
                                use</para>
                            </listitem>
                            <listitem>
                              <para>Update those templates to use ‘bare metal’ flavor instead of the
                                default one</para>
                            </listitem>
                          </itemizedlist>
                        </listitem>
                      </varlistentry>
                    </variablelist>
                  </listitem>
                  <listitem>
                    <para>Go to Data Processing &gt; Cluster Templates.</para>
                  </listitem>
                  <listitem>
                    <para>Click Launch Cluster.</para>
                  </listitem>
                  <listitem>
                    <variablelist>
                      <varlistentry>
                        <term>On the Launch Cluster dialog:</term>
                        <listitem>
                          <itemizedlist>
                            <listitem>
                              <para>Specify the bare metal network for cluster nodes</para>
                            </listitem>
                          </itemizedlist>
                        </listitem>
                      </varlistentry>
                    </variablelist>
                  </listitem>
                </itemizedlist>
              </step>
            </procedure>
            <para>The cluster provisioning time is slower compared to the cluster provisioning of
              the same size that runs on VMs. Ironic does real hardware reports which is time
              consuming, and the whole root disk is filled from <literal>/dev/zero</literal> for
              security reasons.</para>
            <section xml:id="known-limitations">
              <title>Known limitations:</title>
              <itemizedlist>
                <listitem>
                  <para>Security groups are not applied.</para>
                </listitem>
                <listitem>
                  <para>When booting a nova instance with a bare metal flavor, the user can not
                    provide a pre-created neutron port to <literal>nova boot</literal> command.
                      <link xlink:href="https://bugs.launchpad.net/nova/+bug/1544195">LP1544195</link>.</para>
                </listitem>
                <listitem>
                  <para>Nodes are not isolated by projects.</para>
                </listitem>
                <listitem>
                  <para>VM to Bare Metal network routing is not allowed.</para>
                </listitem>
                <listitem>
                  <para>The user has to specify the count of ironic nodes before Devstack deploys an
                    OpenStack.</para>
                </listitem>
                <listitem>
                  <para>The user cannot use the same image for several ironic node types. For
                    example, if there are 3 ironic node types, the user has to create 3 images and 3
                    flavors.</para>
                </listitem>
                <listitem>
                  <para>Multiple interfaces on a single node are not supported. Devstack configures
                    only one interface.</para>
                </listitem>
              </itemizedlist>
            </section>
          </section>
      </section>
      <section xml:id="plugins">
        <title>Plugins</title>
        <section xml:id="provisioning-plugins-2" xml:base="provisioning-plugins-2">
          <title>Provisioning Plugins</title>
          <para>This page lists all available provisioning plugins. In general a plugin enables
            sahara to deploy a specific data processing framework (for example, Hadoop) or
            distribution, and allows configuration of topology and management/monitoring
            tools.</para>
          <itemizedlist>
            <listitem>
              <para><link xlink:href="https://docs.openstack.org/sahara/pike/user/vanilla-plugin.html">Vanilla Plugin</link> - deploys Vanilla Apache Hadoop</para>
            </listitem>
            <listitem>
              <para><link xlink:href="https://docs.openstack.org/sahara/pike/user/ambari-plugin.html">Ambari Plugin</link> - deploys Hortonworks Data Platform</para>
            </listitem>
            <listitem>
              <para><link xlink:href="https://docs.openstack.org/sahara/pike/user/spark-plugin.html">Spark Plugin</link> - deploys Apache Spark with Cloudera HDFS</para>
            </listitem>
            <listitem>
              <para><link xlink:href="https://docs.openstack.org/sahara/pike/user/mapr-plugin.html">MapR Distribution Plugin</link> - deploys MapR plugin with MapR File System</para>
            </listitem>
            <listitem>
              <para><link xlink:href="https://docs.openstack.org/sahara/pike/user/cdh-plugin.html">Cloudera Plugin</link> - deploys Cloudera Hadoop</para>
            </listitem>
          </itemizedlist>
          <section xml:id="managing-plugins">
            <title>Managing plugins</title>
            <para>Since the Newton release a project admin can configure plugins by specifying
              additional values for plugin’s labels.</para>
            <para>To disable a plugin (Vanilla Apache Hadoop, for example), the admin can run the
              following command:</para>
            <screen language="console">cat update_configs.json
{
    "plugin_labels": {
        "enabled": {
            "status": true
        }
    }
}
openstack dataprocessing plugin update vanilla update_configs.json</screen>
            <para>Additionally, specific versions can be disabled by the following command:</para>
            <screen language="console">cat update_configs.json
{
    "version_labels": {
        "2.7.1": {
            "enabled": {
                "status": true
            }
        }
    }
}
openstack dataprocessing plugin update vanilla update_configs.json</screen>
            <para>Finally, to see all labels of a specific plugin and to see the current status of
              the plugin (is it stable or not, deprecation status) the following command can be
              executed from the CLI:</para>
            <screen language="console">openstack dataprocessing plugin show vanilla</screen>
            <para>The same actions are available from UI respectively.</para>
          </section>
        </section>
        <section xml:id="vanilla-plugin" xml:base="vanilla-plugin">
          <title>Vanilla Plugin</title>
          <para>The vanilla plugin is a reference implementation which allows users to operate a
            cluster with Apache Hadoop.</para>
          <para>Since the Newton release Spark is integrated into the Vanilla plugin so you can
            launch Spark jobs on a Vanilla cluster.</para>
          <para>For cluster provisioning prepared images should be used. They already have Apache
            Hadoop 2.7.1 installed.</para>
          <para>You may build images by yourself using the Image builder <xref linkend="vanilla-imagebuilder"/>  or you
            could download prepared images from <link xlink:href="http://sahara-files.mirantis.com/images/upstream"></link></para>
          <para>Vanilla plugin requires an image to be tagged in Sahara Image Registry with two
            tags: ‘vanilla’ and ‘&lt;hadoop version&gt;’ (e.g. ‘2.7.1’).</para>
          <para>The default username specified for these images is different for each
            distribution:</para>
          <informaltable>
            <tgroup cols="2">
              <colspec colname="c0" colwidth="14"/>
              <colspec colname="c1" colwidth="12"/>
              <thead>
                <row>
                  <entry>
                    <para>OS</para>
                  </entry>
                  <entry>
                    <para>username</para>
                  </entry>
                </row>
              </thead>
              <tbody>
                <row>
                  <entry>
                    <para>Ubuntu 14</para>
                  </entry>
                  <entry>
                    <para>ubuntu</para>
                  </entry>
                </row>
                <row>
                  <entry>
                    <para>Fedora 20</para>
                  </entry>
                  <entry>
                    <para>fedora</para>
                  </entry>
                </row>
                <row>
                  <entry>
                    <para>CentOS 6</para>
                  </entry>
                  <entry>
                    <para>cloud-user</para>
                  </entry>
                </row>
                <row>
                  <entry>
                    <para>CentOS 7</para>
                  </entry>
                  <entry>
                    <para>centos</para>
                  </entry>
                </row>
              </tbody>
            </tgroup>
          </informaltable>
          <section xml:id="cluster-validation-1">
            <title>Cluster Validation</title>
            <para>When user creates or scales a Hadoop cluster using a Vanilla plugin, the cluster
              topology requested by user is verified for consistency.</para>
            <para>Currently there are the following limitations in cluster topology for Vanilla
              plugin:</para>
            <para>For Vanilla Hadoop version 2.x.x:</para>
            <itemizedlist>
              <listitem>
                <para>Cluster must contain exactly one namenode</para>
              </listitem>
              <listitem>
                <para>Cluster can contain at most one resourcemanager</para>
              </listitem>
              <listitem>
                <para>Cluster can contain at most one secondary namenode</para>
              </listitem>
              <listitem>
                <para>Cluster can contain at most one historyserver</para>
              </listitem>
              <listitem>
                <para>Cluster can contain at most one oozie and this process is also required for
                  EDP</para>
              </listitem>
              <listitem>
                <para>Cluster can’t contain oozie without resourcemanager and without
                  historyserver</para>
              </listitem>
              <listitem>
                <para>Cluster can’t have nodemanager nodes if it doesn’t have resourcemanager</para>
              </listitem>
              <listitem>
                <para>Cluster can have at most one hiveserver node.</para>
              </listitem>
              <listitem>
                <para>Cluster can have at most one spark history server and this process is also
                  required for Spark EDP (Spark is available since the Newton release).</para>
              </listitem>
            </itemizedlist>
          </section>
        </section>
        <section xml:id="ambari-plugin" xml:base="ambari-plugin">
          <title>Ambari Plugin</title>
          <para>The Ambari sahara plugin provides a way to provision clusters with Hortonworks Data
            Platform on OpenStack using templates in a single click and in an easily repeatable
            fashion. The sahara controller serves as the glue between Hadoop and OpenStack. The
            Ambari plugin mediates between the sahara controller and Apache Ambari in order to
            deploy and configure Hadoop on OpenStack. Core to the HDP Plugin is Apache Ambari which
            is used as the orchestrator for deploying HDP on OpenStack. The Ambari plugin uses
            Ambari Blueprints for cluster provisioning.</para>
          <section xml:id="apache-ambari-blueprints">
            <title>Apache Ambari Blueprints</title>
            <para>Apache Ambari Blueprints is a portable document definition, which provides a
              complete definition for an Apache Hadoop cluster, including cluster topology,
              components, services and their configurations. Ambari Blueprints can be consumed by
              the Ambari plugin to instantiate a Hadoop cluster on OpenStack. The benefits of this
              approach is that it allows for Hadoop clusters to be configured and deployed using an
              Ambari native format that can be used with as well as outside of OpenStack allowing
              for clusters to be re-instantiated in a variety of environments.</para>
          </section>
          <section xml:id="images-1">
            <title>Images</title>
            <para>The sahara Ambari plugin is using minimal (operating system only) images.</para>
            <para>For more information about Ambari images, refer to <link
                xlink:href="https://git.openstack.org/cgi/openstack/sahara-image-elements"></link>.</para>
            <para>You could download well tested and up-to-date prepared images from <link
                xlink:href="http://sahara-files.mirantis.com/images/upstream/"></link></para>
            <para>HDP plugin requires an image to be tagged in sahara Image Registry with two tags:
              ‘ambari’ and ‘&lt;plugin version&gt;’ (e.g. ‘2.5’).</para>
            <para>Also in the Image Registry you will need to specify username for an image. The
              username specified should be ‘cloud-user’ in case of CentOS 6.x image, ‘centos’ for
              CentOS 7 images and ‘ubuntu’ for Ubuntu images.</para>
          </section>
          <section xml:id="high-availability-for-hdfs-and-yarn">
            <title>High Availability for HDFS and YARN</title>
            <para>High Availability (Using the Quorum Journal Manager) can be deployed automatically
              with the Ambari plugin. You can deploy High Available cluster through UI by selecting
                <literal>NameNode HA</literal> and/or <literal>ResourceManager HA</literal> options
              in general configs of cluster template.</para>
            <para>The NameNode High Availability is deployed using 2 NameNodes, one active and one
              standby. The NameNodes use a set of JournalNodes and Zookepeer Servers to ensure the
              necessary synchronization. In case of ResourceManager HA 2 ResourceManagers should be
              enabled in addition.</para>
            <para>A typical Highly available Ambari cluster uses 2 separate NameNodes, 2 separate
              ResourceManagers and at least 3 JournalNodes and at least 3 Zookeeper Servers.</para>
          </section>
          <section xml:id="hdp-version-support">
            <title>HDP Version Support</title>
            <para>The HDP plugin currently supports deployment of HDP 2.3, 2.4 and 2.5.</para>
          </section>
          <section xml:id="cluster-validation-2">
            <title>Cluster Validation</title>
            <para>Prior to Hadoop cluster creation, the HDP plugin will perform the following
              validation checks to ensure a successful Hadoop deployment:</para>
            <itemizedlist>
              <listitem>
                <para>Ensure the existence of Ambari Server process in the cluster;</para>
              </listitem>
              <listitem>
                <para>Ensure the existence of a NameNode, Zookeeper, ResourceManagers processes
                  HistoryServer and App TimeLine Server in the cluster</para>
              </listitem>
            </itemizedlist>
          </section>
          <section xml:id="enabling-kerberos-security-for-cluster-1">
            <title>Enabling Kerberos security for cluster</title>
            <para>If you want to protect your clusters using MIT Kerberos security you have to
              complete a few steps below.</para>
            <itemizedlist>
              <listitem>
                <para>If you would like to create a cluster protected by Kerberos security you just
                  need to enable Kerberos by checkbox in the <literal>General Parameters</literal>
                  section of the cluster configuration. If you prefer to use the OpenStack CLI for
                  cluster creation, you have to put the data below in the
                    <literal>cluster_configs</literal> section:</para>
                <screen language="console">"cluster_configs": {
  "Enable Kerberos Security": true,
}</screen>
                <para>Sahara in this case will correctly prepare KDC server and will create
                  principals along with keytabs to enable authentication for Hadoop services.</para>
              </listitem>
              <listitem>
                <para>Ensure that you have the latest hadoop-openstack jar file distributed on your
                  cluster nodes. You can download one at
                    <literal>http://tarballs.openstack.org/sahara/dist/</literal></para>
              </listitem>
              <listitem>
                <para>Sahara will create principals along with keytabs for system users like
                    <literal>oozie</literal>, <literal>hdfs</literal> and <literal>spark</literal>
                  so that you will not have to perform additional auth operations to execute your
                  jobs on top of the cluster.</para>
              </listitem>
            </itemizedlist>
          </section>
        </section>
        <section xml:id="spark-plugin" xml:base="spark-plugin">
          <title>Spark Plugin</title>
          <para>The Spark plugin for sahara provides a way to provision Apache Spark clusters on
            OpenStack in a single click and in an easily repeatable fashion.</para>
          <para>Currently Spark is installed in standalone mode, with no YARN or Mesos
            support.</para>
          <section xml:id="images-2">
            <title>Images</title>
            <para>For cluster provisioning, prepared images should be used. The Spark plugin has
              been developed and tested with the images generated by sahara-image-elements:</para>
            <itemizedlist>
              <listitem>
                <para>
                  <link xlink:href="https://git.openstack.org/cgit/openstack/sahara-image-elements"></link>
                </para>
              </listitem>
            </itemizedlist>
            <para>The latest Ubuntu images generated by sahara-image-elements have Cloudera CDH
              5.4.0 HDFS and Apache Spark installed. A prepared image for Spark can be found at the
              following location:</para>
            <itemizedlist>
              <listitem>
                <para>
                  <link xlink:href="http://sahara-files.mirantis.com/images/upstream/"></link>
                </para>
              </listitem>
            </itemizedlist>
            <para>The Spark plugin requires an image to be tagged in the sahara image registry with
              two tags: ‘spark’ and ‘&lt;Spark version&gt;’ (e.g. ‘1.6.0’).</para>
            <para>Also you should specify the username of the default cloud-user used in the image.
              For the images available at the URLs listed above and for all the ones generated with
              the DIB it is <literal>ubuntu</literal>.</para>
            <para>Note that the Spark cluster is deployed using the scripts available in the Spark
              distribution, which allow the user to start all services (master and slaves), stop all
              services and so on. As such (and as opposed to CDH HDFS daemons), Spark is not
              deployed as a standard Ubuntu service and if the virtual machines are rebooted, Spark
              will not be restarted.</para>
          </section>
          <section xml:id="spark-configuration">
            <title>Spark configuration</title>
            <para>Spark needs few parameters to work and has sensible defaults. If needed they can
              be changed when creating the sahara cluster template. No node group options are
              available.</para>
            <para>Once the cluster is ready, connect with ssh to the master using the
                <literal>ubuntu</literal> user and the appropriate ssh key. Spark is installed in
                <literal>/opt/spark</literal> and should be completely configured and ready to start
              executing jobs. At the bottom of the cluster information page from the OpenStack
              dashboard, a link to the Spark web interface is provided.</para>
          </section>
          <section xml:id="cluster-validation-3">
            <title>Cluster Validation</title>
            <para>When a user creates an Hadoop cluster using the Spark plugin, the cluster topology
              requested by user is verified for consistency.</para>
            <para>Currently there are the following limitations in cluster topology for the Spark
              plugin:</para>
            <itemizedlist>
              <listitem>
                <para>Cluster must contain exactly one HDFS namenode</para>
              </listitem>
              <listitem>
                <para>Cluster must contain exactly one Spark master</para>
              </listitem>
              <listitem>
                <para>Cluster must contain at least one Spark slave</para>
              </listitem>
              <listitem>
                <para>Cluster must contain at least one HDFS datanode</para>
              </listitem>
            </itemizedlist>
            <para>The tested configuration co-locates the NameNode with the master and a DataNode
              with each slave to maximize data locality.</para>
          </section>
        </section>
        <section xml:id="cloudera-plugin" xml:base="cdh-plugin">
          <title>Cloudera Plugin</title>
          <para>The Cloudera plugin is a Sahara plugin which allows the user to deploy and operate a
            cluster with Cloudera Manager.</para>
          <para>The Cloudera plugin is enabled in Sahara by default. You can manually modify the
            Sahara configuration file (default /etc/sahara/sahara.conf) to explicitly enable or
            disable it in “plugins” line.</para>
          <para>You need to build images using the CDH image builder <xref linkend="cdh-imagebuilder"/> to produce images
            used to provision cluster or you could download prepared images from <link
              xlink:href="http://sahara-files.mirantis.com/images/upstream/"></link> They already have
            Cloudera Express installed (version 5.0.0, 5.3.0, 5.4.0, 5.5.0, 5.7.x and 5.9.x).</para>
          <para>The cloudera plugin requires an image to be tagged in Sahara Image Registry with two
            tags: ‘cdh’ and ‘&lt;cloudera version&gt;’ (e.g. ‘5’, ‘5.3.0’, ‘5.4.0’, ‘5.5.0’,
            ‘5.7.0’, ‘5.9.0’ or ‘5.9.1’, here ‘5’ stands for ‘5.0.0’).</para>
          <para>The default username specified for these images is different for each
            distribution:</para>
          <para>for 5.0.0, 5.3.0 and 5.4.0 version:</para>
          <informaltable>
            <tgroup cols="2">
              <colspec colname="c0" colwidth="14"/>
              <colspec colname="c1" colwidth="12"/>
              <thead>
                <row>
                  <entry>
                    <para>OS</para>
                  </entry>
                  <entry>
                    <para>username</para>
                  </entry>
                </row>
              </thead>
              <tbody>
                <row>
                  <entry>
                    <para>Ubuntu 12.04</para>
                  </entry>
                  <entry>
                    <para>ubuntu</para>
                  </entry>
                </row>
                <row>
                  <entry>
                    <para>CentOS 6.6</para>
                  </entry>
                  <entry>
                    <para>cloud-user</para>
                  </entry>
                </row>
              </tbody>
            </tgroup>
          </informaltable>
          <para>for 5.5.0 and higher versions:</para>
          <informaltable>
            <tgroup cols="2">
              <colspec colname="c0" colwidth="14"/>
              <colspec colname="c1" colwidth="12"/>
              <thead>
                <row>
                  <entry>
                    <para>OS</para>
                  </entry>
                  <entry>
                    <para>username</para>
                  </entry>
                </row>
              </thead>
              <tbody>
                <row>
                  <entry>
                    <para>Ubuntu 14.04</para>
                  </entry>
                  <entry>
                    <para>ubuntu</para>
                  </entry>
                </row>
                <row>
                  <entry>
                    <para>CentOS 6.6</para>
                  </entry>
                  <entry>
                    <para>cloud-user</para>
                  </entry>
                </row>
                <row>
                  <entry>
                    <para>CentOS 7</para>
                  </entry>
                  <entry>
                    <para>centos</para>
                  </entry>
                </row>
              </tbody>
            </tgroup>
          </informaltable>
          <section xml:id="services-supported">
            <title>Services Supported</title>
            <para>Currently below services are supported in both versions of Cloudera plugin: HDFS,
              Oozie, YARN, Spark, Zookeeper, Hive, Hue, HBase. 5.3.0 version of Cloudera Plugin also
              supported following services: Impala, Flume, Solr, Sqoop, and Key-value Store Indexer.
              In version 5.4.0 KMS service support was added based on version 5.3.0. Kafka 2.0.2 was
              added for CDH 5.5 and higher.</para>
            <note>
              <para>Sentry service is enabled in Cloudera plugin. However, as we do not enable
                Kerberos authentication in the cluster for CDH version &lt; 5.5 (which is
                required for Sentry functionality) then using Sentry service will not really take
                any effect, and other services depending on Sentry will not do any authentication
                too.</para>
            </note>
          </section>
          <section xml:id="high-availability-support">
            <title>High Availability Support</title>
            <para>Currently HDFS NameNode High Availability is supported beginning with Cloudera
              5.4.0 version. You can refer to <xref linkend="features"/> for the detail info.</para>
            <para>YARN ResourceManager High Availability is supported beginning with Cloudera 5.4.0
              version. This feature adds redundancy in the form of an Active/Standby ResourceManager
              pair to avoid the failure of single RM. Upon failover, the Standby RM become Active so
              that the applications can resume from their last check-pointed state.</para>
          </section>
          <section xml:id="cluster-validation-4">
            <title>Cluster Validation</title>
            <para>When the user performs an operation on the cluster using a Cloudera plugin, the
              cluster topology requested by the user is verified for consistency.</para>
            <para>The following limitations are required in the cluster topology for all cloudera
              plugin versions:</para>
            <itemizedlist>
              <listitem>
                <para>Cluster must contain exactly one manager.</para>
              </listitem>
              <listitem>
                <para>Cluster must contain exactly one namenode.</para>
              </listitem>
              <listitem>
                <para>Cluster must contain exactly one secondarynamenode.</para>
              </listitem>
              <listitem>
                <para>Cluster must contain at least <literal>dfs_replication</literal>
                  datanodes.</para>
              </listitem>
              <listitem>
                <para>Cluster can contain at most one resourcemanager and this process is also
                  required by nodemanager.</para>
              </listitem>
              <listitem>
                <para>Cluster can contain at most one jobhistory and this process is also required
                  for resourcemanager.</para>
              </listitem>
              <listitem>
                <para>Cluster can contain at most one oozie and this process is also required for
                  EDP.</para>
              </listitem>
              <listitem>
                <para>Cluster can’t contain oozie without datanode.</para>
              </listitem>
              <listitem>
                <para>Cluster can’t contain oozie without nodemanager.</para>
              </listitem>
              <listitem>
                <para>Cluster can’t contain oozie without jobhistory.</para>
              </listitem>
              <listitem>
                <para>Cluster can’t contain hive on the cluster without the following services:
                  metastore, hive server, webcat and resourcemanager.</para>
              </listitem>
              <listitem>
                <para>Cluster can contain at most one hue server.</para>
              </listitem>
              <listitem>
                <para>Cluster can’t contain hue server without hive service and oozie.</para>
              </listitem>
              <listitem>
                <para>Cluster can contain at most one spark history server.</para>
              </listitem>
              <listitem>
                <para>Cluster can’t contain spark history server without resourcemanager.</para>
              </listitem>
              <listitem>
                <para>Cluster can’t contain hbase master service without at least one zookeeper and
                  at least one hbase regionserver.</para>
              </listitem>
              <listitem>
                <para>Cluster can’t contain hbase regionserver without at least one hbase
                  maser.</para>
              </listitem>
            </itemizedlist>
            <para>In case of 5.3.0, 5.4.0, 5.5.0, 5.7.x or 5.9.x version of Cloudera Plugin there
              are few extra limitations in the cluster topology:</para>
            <itemizedlist>
              <listitem>
                <para>Cluster can’t contain flume without at least one datanode.</para>
              </listitem>
              <listitem>
                <para>Cluster can contain at most one sentry server service.</para>
              </listitem>
              <listitem>
                <para>Cluster can’t contain sentry server service without at least one zookeeper and
                  at least one datanode.</para>
              </listitem>
              <listitem>
                <para>Cluster can’t contain solr server without at least one zookeeper and at least
                  one datanode.</para>
              </listitem>
              <listitem>
                <para>Cluster can contain at most one sqoop server.</para>
              </listitem>
              <listitem>
                <para>Cluster can’t contain sqoop server without at least one datanode, nodemanager
                  and jobhistory.</para>
              </listitem>
              <listitem>
                <para>Cluster can’t contain hbase indexer without at least one datanode, zookeeper,
                  solr server and hbase master.</para>
              </listitem>
              <listitem>
                <para>Cluster can contain at most one impala catalog server.</para>
              </listitem>
              <listitem>
                <para>Cluster can contain at most one impala statestore.</para>
              </listitem>
              <listitem>
                <para>Cluster can’t contain impala catalogserver without impala statestore, at least
                  one impalad service, at least one datanode, and metastore.</para>
              </listitem>
              <listitem>
                <para>If using Impala, the daemons must be installed on every datanode.</para>
              </listitem>
            </itemizedlist>
            <para>In case of version 5.5.0, 5.7.x or 5.9.x of Cloudera Plugin additional services in
              the cluster topology are available:</para>
            <itemizedlist>
              <listitem>
                <para>Cluster can have the kafka service and several kafka brokers.</para>
              </listitem>
            </itemizedlist>
          </section>
          <section xml:id="enabling-kerberos-security-for-cluster-2">
            <title>Enabling Kerberos security for cluster</title>
            <para>If you want to protect your clusters using MIT Kerberos security you have to
              complete a few steps below.</para>
            <itemizedlist>
              <listitem>
                <para>If you would like to create a cluster protected by Kerberos security you just
                  need to enable Kerberos by checkbox in the <literal>General Parameters</literal>
                  section of the cluster configuration. If you prefer to use the OpenStack CLI for
                  cluster creation, you have to put the data below in the
                    <literal>cluster_configs</literal> section:</para>
                <screen language="console">"cluster_configs": {
  "Enable Kerberos Security": true,
}</screen>
                <para>Sahara in this case will correctly prepare KDC server and will create
                  principals along with keytabs to enable authentication for Hadoop services.</para>
              </listitem>
              <listitem>
                <para>Ensure that you have the latest hadoop-openstack jar file distributed on your
                  cluster nodes. You can download one at
                    <literal>http://tarballs.openstack.org/sahara/dist/</literal></para>
              </listitem>
              <listitem>
                <para>Sahara will create principals along with keytabs for system users like
                    <literal>hdfs</literal> and <literal>spark</literal> so that you will not have
                  to perform additional auth operations to execute your jobs on top of the
                  cluster.</para>
              </listitem>
            </itemizedlist>
          </section>
        </section>
        <section xml:id="mapr-distribution-plugin" xml:base="mapr-plugin">
          <title>MapR Distribution Plugin</title>
          <para>The MapR Sahara plugin allows to provision MapR clusters on OpenStack in an easy way
            and do it, quickly, conveniently and simply.</para>
          <section xml:id="operation">
            <title>Operation</title>
            <para>The MapR Plugin performs the following four primary functions during cluster
              creation:</para>
            <procedure>
              <step>
                <para>MapR components deployment - the plugin manages the deployment of the required
                  software to the target VMs</para>
              </step>
              <step>
                <para>Services Installation - MapR services are installed according to provided
                  roles list</para>
              </step>
              <step>
                <para>Services Configuration - the plugin combines default settings with user
                  provided settings</para>
              </step>
              <step>
                <para>Services Start - the plugin starts appropriate services according to specified
                  roles</para>
              </step>
            </procedure>
          </section>
          <section xml:id="images-3">
            <title>Images</title>
            <para>The Sahara MapR plugin can make use of either minimal (operating system only)
              images or pre-populated MapR images. The base requirement for both is that the image
              is cloud-init enabled and contains a supported operating system (see <link
                xlink:href="http://maprdocs.mapr.com/home/InteropMatrix/r_os_matrix.html"></link>).</para>
            <para>The advantage of a pre-populated image is that provisioning time is reduced, as
              packages do not need to be downloaded which make up the majority of the time spent in
              the provisioning cycle. In addition, provisioning large clusters will put a burden on
              the network as packages for all nodes need to be downloaded from the package
              repository.</para>
            <para>For more information about MapR images, refer to <link xlink:href="https://git.openstack.org/cgit/openstack/sahara-image-elements"></link>.</para>
            <para>There are VM images provided for use with the MapR Plugin, that can also be built
              using the tools available in sahara-image-elements: <link xlink:href="https://s3-us-west-2.amazonaws.com/sahara-images/index.html"></link>.</para>
            <para>MapR plugin needs an image to be tagged in Sahara Image Registry with two tags:
              ‘mapr’ and ‘&lt;MapR version&gt;’ (e.g. ‘5.2.0.mrv2’).</para>
            <para>The default username specified for these images is different for each
              distribution:</para>
            <informaltable>
              <tgroup cols="2">
                <colspec colname="c0" colwidth="14"/>
                <colspec colname="c1" colwidth="12"/>
                <thead>
                  <row>
                    <entry>
                      <para>OS</para>
                    </entry>
                    <entry>
                      <para>username</para>
                    </entry>
                  </row>
                </thead>
                <tbody>
                  <row>
                    <entry>
                      <para>Ubuntu 14</para>
                    </entry>
                    <entry>
                      <para>ubuntu</para>
                    </entry>
                  </row>
                  <row>
                    <entry>
                      <para>CentOS 6</para>
                    </entry>
                    <entry>
                      <para>cloud-user</para>
                    </entry>
                  </row>
                  <row>
                    <entry>
                      <para>CentOS 7</para>
                    </entry>
                    <entry>
                      <para>centos</para>
                    </entry>
                  </row>
                </tbody>
              </tgroup>
            </informaltable>
          </section>
          <section xml:id="hadoop-version-support">
            <title>Hadoop Version Support</title>
            <para>The MapR plugin currently supports Hadoop 2.7.0 (5.2.0.mrv2).</para>
          </section>
          <section xml:id="cluster-validation-5">
            <title>Cluster Validation</title>
            <para>When the user creates or scales a Hadoop cluster using a mapr plugin, the cluster
              topology requested by the user is verified for consistency.</para>
            <para>Every MapR cluster must contain:</para>
            <itemizedlist>
              <listitem>
                <para>at least 1 <emphasis>CLDB</emphasis> process</para>
              </listitem>
              <listitem>
                <para>exactly 1 <emphasis>Webserver</emphasis> process</para>
              </listitem>
              <listitem>
                <para>odd number of <emphasis>ZooKeeper</emphasis> processes but not less than
                  1</para>
              </listitem>
              <listitem>
                <para><emphasis>FileServer</emphasis> process on every node</para>
              </listitem>
              <listitem>
                <para>at least 1 ephemeral drive (then you need to specify the ephemeral drive in
                  the flavor not on the node group template creation) or 1 Cinder volume per
                  instance</para>
              </listitem>
            </itemizedlist>
            <para>Every Hadoop cluster must contain exactly 1 <emphasis>Oozie</emphasis>
              process</para>
            <para>Every MapReduce v1 cluster must contain:</para>
            <itemizedlist>
              <listitem>
                <para>at least 1 <emphasis>JobTracker</emphasis> process</para>
              </listitem>
              <listitem>
                <para>at least 1 <emphasis>TaskTracker</emphasis> process</para>
              </listitem>
            </itemizedlist>
            <para>Every MapReduce v2 cluster must contain:</para>
            <itemizedlist>
              <listitem>
                <para>exactly 1 <emphasis>ResourceManager</emphasis> process</para>
              </listitem>
              <listitem>
                <para>exactly 1 <emphasis>HistoryServer</emphasis> process</para>
              </listitem>
              <listitem>
                <para>at least 1 <emphasis>NodeManager</emphasis> process</para>
              </listitem>
            </itemizedlist>
            <para>Every Spark cluster must contain:</para>
            <itemizedlist>
              <listitem>
                <para>exactly 1 <emphasis>Spark Master</emphasis> process</para>
              </listitem>
              <listitem>
                <para>exactly 1 <emphasis>Spark HistoryServer</emphasis> process</para>
              </listitem>
              <listitem>
                <para>at least 1 <emphasis>Spark Slave</emphasis> (worker) process</para>
              </listitem>
            </itemizedlist>
            <para>HBase service is considered valid if:</para>
            <itemizedlist>
              <listitem>
                <para>cluster has at least 1 <emphasis>HBase-Master</emphasis> process</para>
              </listitem>
              <listitem>
                <para>cluster has at least 1 <emphasis>HBase-RegionServer</emphasis> process</para>
              </listitem>
            </itemizedlist>
            <para>Hive service is considered valid if:</para>
            <itemizedlist>
              <listitem>
                <para>cluster has exactly 1 <emphasis>HiveMetastore</emphasis> process</para>
              </listitem>
              <listitem>
                <para>cluster has exactly 1 <emphasis>HiveServer2</emphasis> process</para>
              </listitem>
            </itemizedlist>
            <para>Hue service is considered valid if:</para>
            <itemizedlist>
              <listitem>
                <para>cluster has exactly 1 <emphasis>Hue</emphasis> process</para>
              </listitem>
              <listitem>
                <para><emphasis>Hue</emphasis> process resides on the same node as
                    <emphasis>HttpFS</emphasis> process</para>
              </listitem>
            </itemizedlist>
            <para>HttpFS service is considered valid if cluster has exactly 1
                <emphasis>HttpFS</emphasis> process</para>
            <para>Sqoop service is considered valid if cluster has exactly 1
                <emphasis>Sqoop2-Server</emphasis> process</para>
          </section>
          <section xml:id="the-mapr-plugin">
            <title>The MapR Plugin</title>
            <para>For more information, contact MapR.</para>
          </section>
        </section>
      </section>
      <section xml:id="elastic-data-processing">
        <title>Elastic Data Processing</title>
        <section xml:id="elastic-data-processing-edp-3" xml:base="edp">
          <title>Elastic Data Processing (EDP)</title>
          <section xml:id="overview">
            <title>Overview</title>
            <para>Sahara’s Elastic Data Processing facility or <emphasis>EDP</emphasis> allows the
              execution of jobs on clusters created from sahara. EDP supports:</para>
            <itemizedlist>
              <listitem>
                <para>Hive, Pig, MapReduce, MapReduce.Streaming, Java, and Shell job types on Hadoop
                  clusters</para>
              </listitem>
              <listitem>
                <para>Spark jobs on Spark standalone clusters, MapR (v5.0.0 - v5.2.0) clusters,
                  Vanilla clusters (v2.7.1) and CDH clusters (v5.3.0 or higher).</para>
              </listitem>
              <listitem>
                <para>storage of job binaries in the OpenStack Object Storage service (swift), the
                  OpenStack Shared file systems service (manila), or sahara’s own database</para>
              </listitem>
              <listitem>
                <para>access to input and output data sources in</para>
                <itemizedlist>
                  <listitem>
                    <para>HDFS for all job types</para>
                  </listitem>
                  <listitem>
                    <para>swift for all types excluding Hive</para>
                  </listitem>
                  <listitem>
                    <para>manila (NFS shares only) for all types excluding Pig</para>
                  </listitem>
                </itemizedlist>
              </listitem>
              <listitem>
                <para>configuration of jobs at submission time</para>
              </listitem>
              <listitem>
                <para>execution of jobs on existing clusters or transient clusters</para>
              </listitem>
            </itemizedlist>
          </section>
          <section xml:id="interfaces">
            <title>Interfaces</title>
            <para>The EDP features can be used from the sahara web UI which is described in the
                <xref linkend="dashboard-user-guide"/>.</para>
            <para>The EDP features also can be used directly by a client through the <link
                xlink:href="http://developer.openstack.org/api-ref/data-processing/">REST
              api</link></para>
          </section>
          <section xml:id="edp-concepts">
            <title>EDP Concepts</title>
            <para>Sahara EDP uses a collection of simple objects to define and execute jobs. These
              objects are stored in the sahara database when they are created, allowing them to be
              reused. This modular approach with database persistence allows code and data to be
              reused across multiple jobs.</para>
            <para>The essential components of a job are:</para>
            <itemizedlist>
              <listitem>
                <para>executable code to run</para>
              </listitem>
              <listitem>
                <para>input and output data paths, as needed for the job</para>
              </listitem>
              <listitem>
                <para>any additional configuration values needed for the job run</para>
              </listitem>
            </itemizedlist>
            <para>These components are supplied through the objects described below.</para>
            <section xml:id="job-binaries-2">
              <title>Job Binaries</title>
              <para>A <emphasis>Job Binary</emphasis> object stores a URL to a single script or Jar
                file and any credentials needed to retrieve the file. The file itself may be stored
                in the sahara internal database (but it is deprecated now), in swift, or in
                manila.</para>
              <para><emphasis role="bold">deprecated:</emphasis> Files in the sahara database are
                stored as raw bytes in a <emphasis>Job Binary Internal</emphasis> object. This
                object’s sole purpose is to store a file for later retrieval. No extra credentials
                need to be supplied for files stored internally.</para>
              <para>Sahara requires credentials (username and password) to access files stored in
                swift unless swift proxy users are configured as described in the configuration
                guide <xref linkend="additional-notes"/>. The
                swift service must be running in the same OpenStack installation referenced by
                sahara.</para>
              <para>To reference a binary file stored in manila, create the job binary with the URL
                  <literal>manila://{share_id}/{path}</literal>. This assumes that you have already
                stored that file in the appropriate path on the share. The share will be
                automatically mounted to any cluster nodes which require access to the file, if it
                is not mounted already.</para>
              <para>There is a configurable limit on the size of a single job binary that may be
                retrieved by sahara. This limit is 5MB and may be set with the
                  <emphasis>job_binary_max_KB</emphasis> setting in the
                  <literal>sahara.conf</literal> configuration file.</para>
            </section>
            <section xml:id="jobs">
              <title>Jobs</title>
              <para>A <emphasis>Job</emphasis> object specifies the type of the job and lists all of
                the individual Job Binary objects that are required for execution. An individual Job
                Binary may be referenced by multiple Jobs. A Job object specifies a main binary
                and/or supporting libraries depending on its type:</para>
              <informaltable>
                <tgroup cols="3">
                  <colspec colname="c0" colwidth="25"/>
                  <colspec colname="c1" colwidth="13"/>
                  <colspec colname="c2" colwidth="11"/>
                  <thead>
                    <row>
                      <entry>
                        <para>Job type</para>
                      </entry>
                      <entry>
                        <para>Main binary</para>
                      </entry>
                      <entry>
                        <para>Libraries</para>
                      </entry>
                    </row>
                  </thead>
                  <tbody>
                    <row>
                      <entry>
                        <para>
                          <literal>Hive</literal>
                        </para>
                      </entry>
                      <entry>
                        <para>required</para>
                      </entry>
                      <entry>
                        <para>optional</para>
                      </entry>
                    </row>
                    <row>
                      <entry>
                        <para>
                          <literal>Pig</literal>
                        </para>
                      </entry>
                      <entry>
                        <para>required</para>
                      </entry>
                      <entry>
                        <para>optional</para>
                      </entry>
                    </row>
                    <row>
                      <entry>
                        <para>
                          <literal>MapReduce</literal>
                        </para>
                      </entry>
                      <entry>
                        <para>not used</para>
                      </entry>
                      <entry>
                        <para>required</para>
                      </entry>
                    </row>
                    <row>
                      <entry>
                        <para>
                          <literal>MapReduce.Streaming</literal>
                        </para>
                      </entry>
                      <entry>
                        <para>not used</para>
                      </entry>
                      <entry>
                        <para>optional</para>
                      </entry>
                    </row>
                    <row>
                      <entry>
                        <para>
                          <literal>Java</literal>
                        </para>
                      </entry>
                      <entry>
                        <para>not used</para>
                      </entry>
                      <entry>
                        <para>required</para>
                      </entry>
                    </row>
                    <row>
                      <entry>
                        <para>
                          <literal>Shell</literal>
                        </para>
                      </entry>
                      <entry>
                        <para>required</para>
                      </entry>
                      <entry>
                        <para>optional</para>
                      </entry>
                    </row>
                    <row>
                      <entry>
                        <para>
                          <literal>Spark</literal>
                        </para>
                      </entry>
                      <entry>
                        <para>required</para>
                      </entry>
                      <entry>
                        <para>optional</para>
                      </entry>
                    </row>
                    <row>
                      <entry>
                        <para>
                          <literal>Storm</literal>
                        </para>
                      </entry>
                      <entry>
                        <para>required</para>
                      </entry>
                      <entry>
                        <para>not used</para>
                      </entry>
                    </row>
                    <row>
                      <entry>
                        <para>
                          <literal>Storm Pyelus</literal>
                        </para>
                      </entry>
                      <entry>
                        <para>required</para>
                      </entry>
                      <entry>
                        <para>not used</para>
                      </entry>
                    </row>
                  </tbody>
                </tgroup>
              </informaltable>
            </section>
            <section xml:id="data-sources-3">
              <title>Data Sources</title>
              <para>A <emphasis>Data Source</emphasis> object stores a URL which designates the
                location of input or output data and any credentials needed to access the
                location.</para>
              <para>Sahara supports data sources in swift. The swift service must be running in the
                same OpenStack installation referenced by sahara.</para>
              <para>Sahara also supports data sources in HDFS. Any HDFS instance running on a sahara
                cluster in the same OpenStack installation is accessible without manual
                configuration. Other instances of HDFS may be used as well provided that the URL is
                resolvable from the node executing the job.</para>
              <para>Sahara supports data sources in manila as well. To reference a path on an NFS
                share as a data source, create the data source with the URL
                  <literal>manila://{share_id}/{path}</literal>. As in the case of job binaries, the
                specified share will be automatically mounted to your cluster’s nodes as needed to
                access the data source.</para>
              <para>Some job types require the use of data source objects to specify input and
                output when a job is launched. For example, when running a Pig job the UI will
                prompt the user for input and output data source objects.</para>
              <para>Other job types like Java or Spark do not require the user to specify data
                sources. For these job types, data paths are passed as arguments. For convenience,
                sahara allows data source objects to be referenced by name or id. The section <xref
                  linkend="using-data-source-references-as-arguments"/> gives further
                details.</para>
            </section>
            <section xml:id="job-execution">
              <title>Job Execution</title>
              <para>Job objects must be <emphasis>launched</emphasis> or
                  <emphasis>executed</emphasis> in order for them to run on the cluster. During job
                launch, a user specifies execution details including data sources, configuration
                values, and program arguments. The relevant details will vary by job type. The
                launch will create a <emphasis>Job Execution</emphasis> object in sahara which is
                used to monitor and manage the job.</para>
              <para>To execute Hadoop jobs, sahara generates an Oozie workflow and submits it to the
                Oozie server running on the cluster. Familiarity with Oozie is not necessary for
                using sahara but it may be beneficial to the user. A link to the Oozie web console
                can be found in the sahara web UI in the cluster details.</para>
              <para>For Spark jobs, sahara uses the <emphasis>spark-submit</emphasis> shell script
                and executes the Spark job from the master node in case of Spark cluster and from
                the Spark Job History server in other cases. Logs of spark jobs run by sahara can be
                found on this node under the <emphasis>/tmp/spark-edp</emphasis> directory.</para>
            </section>
          </section>
          <section xml:id="edp-workflow">
            <title>General Workflow</title>
            <para>The general workflow for defining and executing a job in sahara is essentially the
              same whether using the web UI or the REST API.</para>
            <procedure>
              <step>
                <para>Launch a cluster from sahara if there is not one already available</para>
              </step>
              <step>
                <para>Create all of the Job Binaries needed to run the job, stored in the sahara
                  database, in swift, or in manila</para>
                <itemizedlist>
                  <listitem>
                    <para>When using the REST API and internal storage of job binaries, the Job
                      Binary Internal objects must be created first</para>
                  </listitem>
                  <listitem>
                    <para>Once the Job Binary Internal objects are created, Job Binary objects may
                      be created which refer to them by URL</para>
                  </listitem>
                </itemizedlist>
              </step>
              <step>
                <para>Create a Job object which references the Job Binaries created in step 2</para>
              </step>
              <step>
                <para>Create an input Data Source which points to the data you wish to
                  process</para>
              </step>
              <step>
                <para>Create an output Data Source which points to the location for output
                  data</para>
              </step>
              <step>
                <para>Create a Job Execution object specifying the cluster and Job object plus
                  relevant data sources, configuration values, and program arguments</para>
                <itemizedlist>
                  <listitem>
                    <para>When using the web UI this is done with the <guimenu>Launch On Existing
                        Cluster</guimenu> or <guimenu>Launch on New Cluster</guimenu> buttons on the
                      Jobs tab</para>
                  </listitem>
                  <listitem>
                    <para>When using the REST API this is done via the
                        <emphasis>/jobs/&lt;job_id&gt;/execute</emphasis> method</para>
                  </listitem>
                </itemizedlist>
              </step>
            </procedure>
            <para>The workflow is simpler when using existing objects. For example, to construct a
              new job which uses existing binaries and input data a user may only need to perform
              steps 3, 5, and 6 above. Of course, to repeat the same job multiple times a user would
              need only step 6.</para>
            <section xml:id="specifying-configuration-values-parameters-and-arguments">
              <title>Specifying Configuration Values, Parameters, and Arguments</title>
              <para>Jobs can be configured at launch. The job type determines the kinds of values
                that may be set:</para>
              <informaltable>
                <tgroup cols="4">
                  <colspec colname="c0" colwidth="26"/>
                  <colspec colname="c1" colwidth="15"/>
                  <colspec colname="c2" colwidth="12"/>
                  <colspec colname="c3" colwidth="11"/>
                  <thead>
                    <row>
                      <entry>
                        <para>Job type</para>
                      </entry>
                      <entry>
                        <para>Configuration Values</para>
                      </entry>
                      <entry>
                        <para>Parameters</para>
                      </entry>
                      <entry>
                        <para>Arguments</para>
                      </entry>
                    </row>
                  </thead>
                  <tbody>
                    <row>
                      <entry>
                        <para>
                          <literal>Hive</literal>
                        </para>
                      </entry>
                      <entry>
                        <para>Yes</para>
                      </entry>
                      <entry>
                        <para>Yes</para>
                      </entry>
                      <entry>
                        <para>No</para>
                      </entry>
                    </row>
                    <row>
                      <entry>
                        <para>
                          <literal>Pig</literal>
                        </para>
                      </entry>
                      <entry>
                        <para>Yes</para>
                      </entry>
                      <entry>
                        <para>Yes</para>
                      </entry>
                      <entry>
                        <para>Yes</para>
                      </entry>
                    </row>
                    <row>
                      <entry>
                        <para>
                          <literal>MapReduce</literal>
                        </para>
                      </entry>
                      <entry>
                        <para>Yes</para>
                      </entry>
                      <entry>
                        <para>No</para>
                      </entry>
                      <entry>
                        <para>No</para>
                      </entry>
                    </row>
                    <row>
                      <entry>
                        <para>
                          <literal>MapReduce.Streaming</literal>
                        </para>
                      </entry>
                      <entry>
                        <para>Yes</para>
                      </entry>
                      <entry>
                        <para>No</para>
                      </entry>
                      <entry>
                        <para>No</para>
                      </entry>
                    </row>
                    <row>
                      <entry>
                        <para>
                          <literal>Java</literal>
                        </para>
                      </entry>
                      <entry>
                        <para>Yes</para>
                      </entry>
                      <entry>
                        <para>No</para>
                      </entry>
                      <entry>
                        <para>Yes</para>
                      </entry>
                    </row>
                    <row>
                      <entry>
                        <para>
                          <literal>Shell</literal>
                        </para>
                      </entry>
                      <entry>
                        <para>Yes</para>
                      </entry>
                      <entry>
                        <para>Yes</para>
                      </entry>
                      <entry>
                        <para>Yes</para>
                      </entry>
                    </row>
                    <row>
                      <entry>
                        <para>
                          <literal>Spark</literal>
                        </para>
                      </entry>
                      <entry>
                        <para>Yes</para>
                      </entry>
                      <entry>
                        <para>No</para>
                      </entry>
                      <entry>
                        <para>Yes</para>
                      </entry>
                    </row>
                    <row>
                      <entry>
                        <para>
                          <literal>Storm</literal>
                        </para>
                      </entry>
                      <entry>
                        <para>Yes</para>
                      </entry>
                      <entry>
                        <para>No</para>
                      </entry>
                      <entry>
                        <para>Yes</para>
                      </entry>
                    </row>
                    <row>
                      <entry>
                        <para>
                          <literal>Storm Pyelus</literal>
                        </para>
                      </entry>
                      <entry>
                        <para>Yes</para>
                      </entry>
                      <entry>
                        <para>No</para>
                      </entry>
                      <entry>
                        <para>Yes</para>
                      </entry>
                    </row>
                  </tbody>
                </tgroup>
              </informaltable>
              <itemizedlist>
                <listitem>
                  <para><emphasis>Configuration values</emphasis> are key/value pairs.</para>
                  <itemizedlist>
                    <listitem>
                      <para>The EDP configuration values have names beginning with
                          <emphasis>edp.</emphasis> and are consumed by sahara</para>
                    </listitem>
                    <listitem>
                      <para>Other configuration values may be read at runtime by Hadoop jobs</para>
                    </listitem>
                    <listitem>
                      <para>Currently additional configuration values are not available to Spark
                        jobs at runtime</para>
                    </listitem>
                  </itemizedlist>
                </listitem>
                <listitem>
                  <para><emphasis>Parameters</emphasis> are key/value pairs. They supply values for
                    the Hive and Pig parameter substitution mechanisms. In Shell jobs, they are
                    passed as environment variables.</para>
                </listitem>
                <listitem>
                  <para><emphasis>Arguments</emphasis> are strings passed as command line arguments
                    to a shell or main program</para>
                </listitem>
              </itemizedlist>
              <para>These values can be set on the <guimenu>Configure</guimenu> tab during job
                launch through the web UI or through the <emphasis>job_configs</emphasis> parameter
                when using the <emphasis>/jobs/&lt;job_id&gt;/execute</emphasis> REST
                method.</para>
              <para>In some cases sahara generates configuration values or parameters automatically.
                Values set explicitly by the user during launch will override those generated by
                sahara.</para>
            </section>
            <section xml:id="using-data-source-references-as-arguments">
              <title>Using Data Source References as Arguments</title>
              <para>Sometimes it’s necessary or desirable to pass a data path as an argument to a
                job. In these cases, a user may simply type out the path as an argument when
                launching a job. If the path requires credentials, the user can manually add the
                credentials as configuration values. However, if a data source object has been
                created that contains the desired path and credentials there is no need to specify
                this information manually.</para>
              <para>As a convenience, sahara allows data source objects to be referenced by name or
                id in arguments, configuration values, or parameters. When the job is executed,
                sahara will replace the reference with the path stored in the data source object and
                will add any necessary credentials to the job configuration. Referencing an existing
                data source object is much faster than adding this information by hand. This is
                particularly useful for job types like Java or Spark that do not use data source
                objects directly.</para>
              <para>There are two job configuration parameters that enable data source references.
                They may be used with any job type and are set on the
                  <literal>Configuration</literal> tab when the job is launched:</para>
              <itemizedlist>
                <listitem>
                  <para><literal>edp.substitute_data_source_for_name</literal> (default <emphasis
                      role="bold">False</emphasis>) If set to <emphasis role="bold">True</emphasis>,
                    causes sahara to look for data source object name references in configuration
                    values, arguments, and parameters when a job is launched. Name references have
                    the form <emphasis role="bold"
                    >datasource://name_of_the_object</emphasis>.</para>
                  <para>For example, assume a user has a WordCount application that takes an input
                    path as an argument. If there is a data source object named <emphasis
                      role="bold">my_input</emphasis>, a user may simply set the <emphasis
                      role="bold">edp.substitute_data_source_for_name</emphasis> configuration
                    parameter to <emphasis role="bold">True</emphasis> and add <emphasis role="bold"
                      >datasource://my_input</emphasis> as an argument when launching the
                    job.</para>
                </listitem>
                <listitem>
                  <para><literal>edp.substitute_data_source_for_uuid</literal> (default <emphasis
                      role="bold">False</emphasis>) If set to <emphasis role="bold">True</emphasis>,
                    causes sahara to look for data source object ids in configuration values,
                    arguments, and parameters when a job is launched. A data source object id is a
                    uuid, so they are unique. The id of a data source object is available through
                    the UI or the sahara command line client. A user may simply use the id as a
                    value.</para>
                </listitem>
              </itemizedlist>
            </section>
            <section xml:id="creating-an-interface-for-your-job">
              <title>Creating an Interface for Your Job</title>
              <para>In order to better document your job for cluster operators (or for yourself in
                the future), sahara allows the addition of an interface (or method signature) to
                your job template. A sample interface for the Teragen Hadoop example might
                be:</para>
              <informaltable>
                <tgroup cols="6">
                  <colspec colname="c0" colwidth="9"/>
                  <colspec colname="c1" colwidth="9"/>
                  <colspec colname="c2" colwidth="11"/>
                  <colspec colname="c3" colwidth="13"/>
                  <colspec colname="c4" colwidth="10"/>
                  <colspec colname="c5" colwidth="20"/>
                  <thead>
                    <row>
                      <entry>
                        <para>Name</para>
                      </entry>
                      <entry>
                        <para>Mapping Type</para>
                      </entry>
                      <entry>
                        <para>Location</para>
                      </entry>
                      <entry>
                        <para>Value Type</para>
                      </entry>
                      <entry>
                        <para>Required</para>
                      </entry>
                      <entry>
                        <para>Default</para>
                      </entry>
                    </row>
                  </thead>
                  <tbody>
                    <row>
                      <entry>
                        <para>Example Class</para>
                      </entry>
                      <entry>
                        <para>args</para>
                      </entry>
                      <entry>
                        <para>0</para>
                      </entry>
                      <entry>
                        <para>string</para>
                      </entry>
                      <entry>
                        <para>false</para>
                      </entry>
                      <entry>
                        <para>teragen</para>
                      </entry>
                    </row>
                    <row>
                      <entry>
                        <para>Rows</para>
                      </entry>
                      <entry>
                        <para>args</para>
                      </entry>
                      <entry>
                        <para>1</para>
                      </entry>
                      <entry>
                        <para>number</para>
                      </entry>
                      <entry>
                        <para>true</para>
                      </entry>
                      <entry>
                        <para>unset</para>
                      </entry>
                    </row>
                    <row>
                      <entry>
                        <para>Output Path</para>
                      </entry>
                      <entry>
                        <para>args</para>
                      </entry>
                      <entry>
                        <para>2</para>
                      </entry>
                      <entry>
                        <para>data_source</para>
                      </entry>
                      <entry>
                        <para>false</para>
                      </entry>
                      <entry>
                        <para>hdfs://ip:port/path</para>
                      </entry>
                    </row>
                    <row>
                      <entry>
                        <para>Mapper Count</para>
                      </entry>
                      <entry>
                        <para>configs</para>
                      </entry>
                      <entry>
                        <para>mapred. map.tasks</para>
                      </entry>
                      <entry>
                        <para>number</para>
                      </entry>
                      <entry>
                        <para>false</para>
                      </entry>
                      <entry>
                        <para>unset</para>
                      </entry>
                    </row>
                  </tbody>
                </tgroup>
              </informaltable>
              <para>A “Description” field may also be added to each interface argument.</para>
              <para>To create such an interface via the REST API, provide an “interface” argument,
                the value of which consists of a list of JSON objects, as below:</para>
              <screen language="json">[
    {
        "name": "Example Class",
        "description": "Indicates which example job class should be used.",
        "mapping_type": "args",
        "location": "0",
        "value_type": "string",
        "required": false,
        "default": "teragen"
    },
]</screen>
              <para>Creating this interface would allow you to specify a configuration for any
                execution of the job template by passing an “interface” map similar to:</para>
              <screen language="json">{
    "Rows": "1000000",
    "Mapper Count": "3",
    "Output Path": "hdfs://mycluster:8020/user/myuser/teragen-output"
}</screen>
              <para>The specified arguments would be automatically placed into the args, configs,
                and params for the job, according to the mapping type and location fields of each
                interface argument. The final <literal>job_configs</literal> map would be:</para>
              <screen language="json">{
    "job_configs": {
        "configs":
            {
                "mapred.map.tasks": "3"
            },
        "args":
            [
                "teragen",
                "1000000",
                "hdfs://mycluster:8020/user/myuser/teragen-output"
            ]
    }
}</screen>
              <para>Rules for specifying an interface are as follows:</para>
              <itemizedlist>
                <listitem>
                  <para>Mapping Type must be one of <literal>configs</literal>,
                      <literal>params</literal>, or <literal>args</literal>. Only types supported
                    for your job type are allowed (see above.)</para>
                </listitem>
                <listitem>
                  <para>Location must be a string for <literal>configs</literal> and
                      <literal>params</literal>, and an integer for <literal>args</literal>. The set
                    of <literal>args</literal> locations must be an unbroken series of integers
                    starting from 0.</para>
                </listitem>
                <listitem>
                  <para>Value Type must be one of <literal>string</literal>,
                      <literal>number</literal>, or <literal>data_source</literal>. Data sources may
                    be passed as UUIDs or as valid paths (see above.) All values should be sent as
                    JSON strings. (Note that booleans and null values are serialized differently in
                    different languages. Specify them as a string representation of the
                    appropriate constants for your data processing engine.)</para>
                </listitem>
                <listitem>
                  <para><literal>args</literal> that are not required must be given a default
                    value.</para>
                </listitem>
              </itemizedlist>
              <para>The additional one-time complexity of specifying an interface on your template
                allows a simpler repeated execution path, and also allows us to generate a
                customized form for your job in the Horizon UI. This may be particularly useful in
                cases in which an operator who is not a data processing job developer will be
                running and administering the jobs.</para>
            </section>
            <section xml:id="generation-of-swift-properties-for-data-sources-4">
              <title>Generation of Swift Properties for Data Sources</title>
              <para>If swift proxy users are not configured (see
                <link xlink:href="https://docs.openstack.org/sahara/queens/admin/advanced-configuration-guide.html">Sahara Advanced Configuration Guide</link>.)
                and a job is run with data source objects containing swift paths, sahara will
                automatically generate swift username and password configuration values based on the
                credentials in the data sources. If the input and output data sources are both in
                swift, it is expected that they specify the same credentials.</para>
              <para>The swift credentials may be set explicitly with the following configuration
                values:</para>
              <informaltable>
                <tgroup cols="1">
                  <colspec colname="c0" colwidth="36"/>
                  <thead>
                    <row>
                      <entry>
                        <para>Name</para>
                      </entry>
                    </row>
                  </thead>
                  <tbody>
                    <row>
                      <entry>
                        <para>fs.swift.service.sahara.username</para>
                      </entry>
                    </row>
                    <row>
                      <entry>
                        <para>fs.swift.service.sahara.password</para>
                      </entry>
                    </row>
                  </tbody>
                </tgroup>
              </informaltable>
              <para>Setting the swift credentials explicitly is required when passing literal swift
                paths as arguments instead of using data source references. When possible, use data
                source references as described in <xref
                  linkend="using-data-source-references-as-arguments"/>.</para>
            </section>
            <section xml:id="additional-details-for-hive-jobs">
              <title>Additional Details for Hive jobs</title>
              <para>Sahara will automatically generate values for the <literal>INPUT</literal> and
                  <literal>OUTPUT</literal> parameters required by Hive based on the specified data
                sources.</para>
            </section>
            <section xml:id="additional-details-for-pig-jobs">
              <title>Additional Details for Pig jobs</title>
              <para>Sahara will automatically generate values for the <literal>INPUT</literal> and
                  <literal>OUTPUT</literal> parameters required by Pig based on the specified data
                sources.</para>
              <para>For Pig jobs, <literal>arguments</literal> should be thought of as command line
                arguments separated by spaces and passed to the <literal>pig</literal> shell.</para>
              <para><literal>Parameters</literal> are a shorthand and are actually translated to the
                arguments <literal>-param name=value</literal></para>
            </section>
            <section xml:id="additional-details-for-mapreduce-jobs">
              <title>Additional Details for MapReduce jobs</title>
              <para>
                <emphasis role="bold">Important!</emphasis>
              </para>
              <para>If the job type is MapReduce, the mapper and reducer classes
                  <emphasis>must</emphasis> be specified as configuration values.</para>
              <para>Note that the UI will not prompt the user for these required values; they must
                be added manually with the <literal>Configure</literal> tab.</para>
              <para>Make sure to add these values with the correct names:</para>
              <informaltable>
                <tgroup cols="2">
                  <colspec colname="c0" colwidth="29"/>
                  <colspec colname="c1" colwidth="40"/>
                  <thead>
                    <row>
                      <entry>
                        <para>Name</para>
                      </entry>
                      <entry>
                        <para>Example Value</para>
                      </entry>
                    </row>
                  </thead>
                  <tbody>
                    <row>
                      <entry>
                        <para>mapred.mapper.new-api</para>
                      </entry>
                      <entry>
                        <para>true</para>
                      </entry>
                    </row>
                    <row>
                      <entry>
                        <para>mapred.reducer.new-api</para>
                      </entry>
                      <entry>
                        <para>true</para>
                      </entry>
                    </row>
                    <row>
                      <entry>
                        <para>mapreduce.job.map.class</para>
                      </entry>
                      <entry>
                        <para>org.apache.oozie.example.SampleMapper</para>
                      </entry>
                    </row>
                    <row>
                      <entry>
                        <para>mapreduce.job.reduce.class</para>
                      </entry>
                      <entry>
                        <para>org.apache.oozie.example.SampleReducer</para>
                      </entry>
                    </row>
                  </tbody>
                </tgroup>
              </informaltable>
            </section>
            <section xml:id="additional-details-for-mapreduce-streaming-jobs">
              <title>Additional Details for MapReduce.Streaming jobs</title>
              <para>
                <emphasis role="bold">Important!</emphasis>
              </para>
              <para>If the job type is MapReduce.Streaming, the streaming mapper and reducer classes
                  <emphasis>must</emphasis> be specified.</para>
              <para>In this case, the UI <emphasis>will</emphasis> prompt the user to enter mapper
                and reducer values on the form and will take care of adding them to the job
                configuration with the appropriate names. If using the python client, however, be
                certain to add these values to the job configuration manually with the correct
                names:</para>
              <informaltable>
                <tgroup cols="2">
                  <colspec colname="c0" colwidth="25"/>
                  <colspec colname="c1" colwidth="15"/>
                  <thead>
                    <row>
                      <entry>
                        <para>Name</para>
                      </entry>
                      <entry>
                        <para>Example Value</para>
                      </entry>
                    </row>
                  </thead>
                  <tbody>
                    <row>
                      <entry>
                        <para>edp.streaming.mapper</para>
                      </entry>
                      <entry>
                        <para>/bin/cat</para>
                      </entry>
                    </row>
                    <row>
                      <entry>
                        <para>edp.streaming.reducer</para>
                      </entry>
                      <entry>
                        <para>/usr/bin/wc</para>
                      </entry>
                    </row>
                  </tbody>
                </tgroup>
              </informaltable>
            </section>
            <section xml:id="additional-details-for-java-jobs">
              <title>Additional Details for Java jobs</title>
              <para>Data Source objects are not used directly with Java job types. Instead, any
                input or output paths must be specified as arguments at job launch either explicitly
                or by reference as described in <xref
                  linkend="using-data-source-references-as-arguments"/>. Using data source
                references is the recommended way to pass paths to Java jobs.</para>
              <para>If configuration values are specified, they must be added to the job’s Hadoop
                configuration at runtime. There are two methods of doing this. The simplest way is
                to use the <emphasis role="bold">edp.java.adapt_for_oozie</emphasis> option
                described below. The other method is to use the code from <link
                  xmlns:xl="http://www.w3.org/1999/xlink"
                  xl:href="https://git.openstack.org/cgit/openstack/sahara-tests/tree/sahara_tests/scenario/defaults/edp-examples/edp-java/README.rst"
                  >this example</link> to explicitly load the values.</para>
              <para>The following special configuration values are read by sahara and affect how
                Java jobs are run:</para>
              <itemizedlist>
                <listitem>
                  <para><literal>edp.java.main_class</literal> (required) Specifies the full name of
                    the class containing <literal>main(String[] args)</literal></para>
                  <para>A Java job will execute the <emphasis role="bold">main</emphasis> method of
                    the specified main class. Any arguments set during job launch will be passed to
                    the program through the <emphasis role="bold">args</emphasis> array.</para>
                </listitem>
                <listitem>
                  <para><literal>oozie.libpath</literal> (optional) Specifies configuration values
                    for the Oozie share libs, these libs can be shared by different workflows</para>
                </listitem>
                <listitem>
                  <para><literal>edp.java.java_opts</literal> (optional) Specifies configuration
                    values for the JVM</para>
                </listitem>
                <listitem>
                  <para><literal>edp.java.adapt_for_oozie</literal> (optional) Specifies that sahara
                    should perform special handling of configuration values and exit conditions. The
                    default is <emphasis role="bold">False</emphasis>.</para>
                  <para>If this configuration value is set to <emphasis role="bold">True</emphasis>,
                    sahara will modify the job’s Hadoop configuration before invoking the specified
                      <emphasis role="bold">main</emphasis> method. Any configuration values
                    specified during job launch (excluding those beginning with <emphasis
                      role="bold">edp.</emphasis>) will be automatically set in the job’s Hadoop
                    configuration and will be available through standard methods.</para>
                  <para>Secondly, setting this option to <emphasis role="bold">True</emphasis>
                    ensures that Oozie will handle program exit conditions correctly.</para>
                </listitem>
              </itemizedlist>
              <para>At this time, the following special configuration value only applies when
                running jobs on a cluster generated by the Cloudera plugin with the <emphasis
                  role="bold">Enable Hbase Common Lib</emphasis> cluster config set to <emphasis
                  role="bold">True</emphasis> (the default value):</para>
              <itemizedlist>
                <listitem>
                  <para><literal>edp.hbase_common_lib</literal> (optional) Specifies that a common
                    Hbase lib generated by sahara in HDFS be added to the <emphasis role="bold"
                      >oozie.libpath</emphasis>. This for use when an Hbase application is driven
                    from a Java job. Default is <emphasis role="bold">False</emphasis>.</para>
                </listitem>
              </itemizedlist>
              <para>The <emphasis role="bold">edp-wordcount</emphasis> example bundled with sahara
                shows how to use configuration values, arguments, and swift data paths in a Java job
                type. Note that the example does not use the <emphasis role="bold"
                  >edp.java.adapt_for_oozie</emphasis> option but includes the code to load the
                configuration values explicitly.</para>
            </section>
            <section xml:id="additional-details-for-shell-jobs">
              <title>Additional Details for Shell jobs</title>
              <para>A shell job will execute the script specified as <literal>main</literal>, and
                will place any files specified as <literal>libs</literal> in the same working
                directory (on both the filesystem and in HDFS). Command line arguments may be passed
                to the script through the <literal>args</literal> array, and any
                  <literal>params</literal> values will be passed as environment variables.</para>
              <para>Data Source objects are not used directly with Shell job types but data source
                references may be used as described in <xref
                  linkend="using-data-source-references-as-arguments"/>.</para>
              <para>The <emphasis role="bold">edp-shell</emphasis> example bundled with sahara
                contains a script which will output the executing user to a file specified by the
                first command line argument.</para>
            </section>
            <section xml:id="additional-details-for-spark-jobs">
              <title>Additional Details for Spark jobs</title>
              <para>Data Source objects are not used directly with Spark job types. Instead, any
                input or output paths must be specified as arguments at job launch either explicitly
                or by reference as described in <xref
                  linkend="using-data-source-references-as-arguments"/>. Using data source
                references is the recommended way to pass paths to Spark jobs.</para>
              <para>Spark jobs use some special configuration values:</para>
              <itemizedlist>
                <listitem>
                  <para><literal>edp.java.main_class</literal> (required) Specifies the full name of
                    the class containing the Java or Scala main method:</para>
                  <itemizedlist>
                    <listitem>
                      <para><literal>main(String[] args)</literal> for Java</para>
                    </listitem>
                    <listitem>
                      <para><literal>main(args: Array[String]</literal> for Scala</para>
                    </listitem>
                  </itemizedlist>
                  <para>A Spark job will execute the <emphasis role="bold">main</emphasis> method of
                    the specified main class. Any arguments set during job launch will be passed to
                    the program through the <emphasis role="bold">args</emphasis> array.</para>
                </listitem>
                <listitem>
                  <para><literal>edp.spark.adapt_for_swift</literal> (optional) If set to <emphasis
                      role="bold">True</emphasis>, instructs sahara to modify the job’s Hadoop
                    configuration so that swift paths may be accessed. Without this configuration
                    value, swift paths will not be accessible to Spark jobs. The default is
                      <emphasis role="bold">False</emphasis>.</para>
                </listitem>
                <listitem>
                  <para><literal>edp.spark.driver.classpath</literal> (optional) If set to empty
                    string sahara will use default classpath for the cluster during job execution.
                    Otherwise this will override default value for the cluster for particular job
                    execution.</para>
                </listitem>
              </itemizedlist>
              <para>The <emphasis role="bold">edp-spark</emphasis> example bundled with sahara
                contains a Spark program for estimating Pi.</para>
            </section>
          </section>
          <section xml:id="special-sahara-urls">
            <title>Special Sahara URLs</title>
            <para>Sahara uses custom URLs to refer to objects stored in swift, in manila, or in the
              sahara internal database. These URLs are not meant to be used outside of
              sahara.</para>
            <para>Sahara swift URLs passed to running jobs as input or output sources include a
              “.sahara” suffix on the container, for example:</para>
            <para>
              <literal>swift://container.sahara/object</literal>
            </para>
            <para>You may notice these swift URLs in job logs, however, you do not need to add the
              suffix to the containers yourself. sahara will add the suffix if necessary, so when
              using the UI or the python client you may write the above URL simply as:</para>
            <para>
              <literal>swift://container/object</literal>
            </para>
            <para>Sahara internal database URLs have the form:</para>
            <para>
              <literal>internal-db://sahara-generated-uuid</literal>
            </para>
            <para>This indicates a file object in the sahara database which has the given uuid as a
              key.</para>
            <para>Manila NFS filesystem reference URLS take the form:</para>
            <para>
              <literal>manila://share-uuid/path</literal>
            </para>
            <para>This format should be used when referring to a job binary or a data source stored
              in a manila NFS share.</para>
          </section>
        </section>
        <section xml:id="edp-requirements" xml:base="edp">
          <title>EDP Requirements</title>
          <para>The OpenStack installation and the cluster launched from sahara must meet the
            following minimum requirements in order for EDP to function:</para>
          <section xml:id="openstack-services">
            <title>OpenStack Services</title>
            <para>When a Hadoop job is executed, binaries are first uploaded to a cluster node and
              then moved from the node local filesystem to HDFS. Therefore, there must be an
              instance of HDFS available to the nodes in the sahara cluster.</para>
            <para>If the swift service <emphasis>is not</emphasis> running in the OpenStack
              installation:</para>
            <itemizedlist>
              <listitem>
                <para>Job binaries may only be stored in the sahara internal database</para>
              </listitem>
              <listitem>
                <para>Data sources require a long-running HDFS</para>
              </listitem>
            </itemizedlist>
            <para>If the swift service <emphasis>is</emphasis> running in the OpenStack
              installation:</para>
            <itemizedlist>
              <listitem>
                <para>Job binaries may be stored in swift or the sahara internal database</para>
              </listitem>
              <listitem>
                <para>Data sources may be in swift or a long-running HDFS</para>
              </listitem>
            </itemizedlist>
          </section>
          <section xml:id="cluster-processes">
            <title>Cluster Processes</title>
            <para>Requirements for EDP support depend on the EDP job type and plugin used for the
              cluster. For example a Vanilla sahara cluster must run at least one instance of these
              processes to support EDP:</para>
            <itemizedlist>
              <listitem>
                <para>For Hadoop version 1:</para>
                <itemizedlist>
                  <listitem>
                    <para>jobtracker</para>
                  </listitem>
                  <listitem>
                    <para>namenode</para>
                  </listitem>
                  <listitem>
                    <para>oozie</para>
                  </listitem>
                  <listitem>
                    <para>tasktracker</para>
                  </listitem>
                  <listitem>
                    <para>datanode</para>
                  </listitem>
                </itemizedlist>
              </listitem>
              <listitem>
                <para>For Hadoop version 2:</para>
                <itemizedlist>
                  <listitem>
                    <para>namenode</para>
                  </listitem>
                  <listitem>
                    <para>datanode</para>
                  </listitem>
                  <listitem>
                    <para>resourcemanager</para>
                  </listitem>
                  <listitem>
                    <para>nodemanager</para>
                  </listitem>
                  <listitem>
                    <para>historyserver</para>
                  </listitem>
                  <listitem>
                    <para>oozie</para>
                  </listitem>
                  <listitem>
                    <para>spark history server</para>
                  </listitem>
                </itemizedlist>
              </listitem>
            </itemizedlist>
          </section>
        </section>
        <section xml:id="edp-technical-considerations" xml:base="edp">
          <title>EDP Technical Considerations</title>
          <para>There are several things in EDP which require attention in order to work properly.
            They are listed on this page.</para>
          <section xml:id="transient-clusters">
            <title>Transient Clusters</title>
            <para>EDP allows running jobs on transient clusters. In this case the cluster is created
              specifically for the job and is shut down automatically once the job is
              finished.</para>
            <para>Two config parameters control the behaviour of periodic clusters:</para>
            <itemizedlist>
              <listitem>
                <para>periodic_enable - if set to ‘false’, sahara will do nothing to a transient
                  cluster once the job it was created for is completed. If it is set to ‘true’, then
                  the behaviour depends on the value of the next parameter.</para>
              </listitem>
              <listitem>
                <para>use_identity_api_v3 - set it to ‘false’ if your OpenStack installation does
                  not provide keystone API v3. In that case sahara will not terminate unneeded
                  clusters. Instead it will set their state to ‘AwaitingTermination’ meaning that
                  they could be manually deleted by a user. If the parameter is set to ‘true’,
                  sahara will itself terminate the cluster. The limitation is caused by lack of
                  ‘trusts’ feature in Keystone API older than v3.</para>
              </listitem>
            </itemizedlist>
            <para>If both parameters are set to ‘true’, sahara works with transient clusters in the
              following manner:</para>
            <procedure>
              <step>
                <para>When a user requests for a job to be executed on a transient cluster, sahara
                  creates such a cluster.</para>
              </step>
              <step>
                <para>Sahara drops the user’s credentials once the cluster is created but prior to
                  that it creates a trust allowing it to operate with the cluster instances in the
                  future without user credentials.</para>
              </step>
              <step>
                <para>Once a cluster is not needed, sahara terminates its instances using the stored
                  trust. sahara drops the trust after that.</para>
              </step>
            </procedure>
          </section>
        </section>
      </section>
      <section xml:id="guest-images">
        <title>Guest Images</title>
        <section xml:id="requirements-for-guests" xml:base="guest-requirements">
          <title>Requirements for Guests</title>
          <para>Sahara manages guests of various platforms (for example Ubuntu, Fedora, RHEL, and
            CentOS) with various versions of the Hadoop ecosystem projects installed. There are
            common requirements for all guests, and additional requirements based on the plugin that
            is used for cluster deployment.</para>
          <section xml:id="common-requirements">
            <title>Common Requirements</title>
            <itemizedlist>
              <listitem>
                <para>The operating system must be Linux</para>
              </listitem>
              <listitem>
                <para>cloud-init must be installed</para>
              </listitem>
              <listitem>
                <para>ssh-server must be installed</para>
                <itemizedlist>
                  <listitem>
                    <para>if a firewall is active it must allow connections on port 22 to enable
                      ssh</para>
                  </listitem>
                </itemizedlist>
              </listitem>
            </itemizedlist>
          </section>
          <section xml:id="vanilla-plugin-requirements">
            <title>Vanilla Plugin Requirements</title>
            <para>If the Vanilla Plugin is used for cluster deployment the guest is required to
              have</para>
            <itemizedlist>
              <listitem>
                <para>ssh-client installed</para>
              </listitem>
              <listitem>
                <para>Java (version &gt;= 6)</para>
              </listitem>
              <listitem>
                <para>Apache Hadoop installed</para>
              </listitem>
              <listitem>
                <para>‘hadoop’ user created</para>
              </listitem>
            </itemizedlist>
                <para>See <xref linkend="hadoop-swift"/> for information on using Swift with your sahara
              cluster (for EDP support Swift integration is currently required).</para>
            <para>To support EDP, the following components must also be installed on the
              guest:</para>
            <itemizedlist>
              <listitem>
                <para>Oozie version 4 or higher</para>
              </listitem>
              <listitem>
                <para>mysql</para>
              </listitem>
              <listitem>
                <para>hive</para>
              </listitem>
            </itemizedlist>
              <para>See <xref linkend="vanilla-imagebuilder"/> for instructions on building images for
              this plugin.</para>
          </section>
          <section xml:id="hortonworks-plugin-requirements">
            <title>Hortonworks Plugin Requirements</title>
            <para>This plugin does not have any additional requirements. Currently, only the CentOS
              Linux and Ubuntu distributions are supported but other distributions will be supported
              in the future. To speed up provisioning, the HDP packages can be pre-installed on the
              image used. The packages’ versions depend on the HDP version being used.</para>
          </section>
          <section xml:id="cloudera-plugin-requirements">
            <title>Cloudera Plugin Requirements</title>
            <para>Cloudera Plugin does not have any additional requirements, just build a CDH image
              to deploy the cluster.</para>
            <para>See <xref linkend="cdh-imagebuilder"/> for instructions on building images for
              this plugin.</para>
          </section>
        </section>
        <section xml:id="hadoop-swift" xml:base="hadoop-swift">
          <title>Swift Integration</title>
          <para>Hadoop and Swift integration are the essential continuation of the Hadoop/OpenStack
            marriage. The key component to making this marriage work is the Hadoop Swift filesystem
            implementation. Although this implementation has been merged into the upstream Hadoop
            project, Sahara maintains a version with the most current features enabled.</para>
          <itemizedlist>
            <listitem>
              <para>The original Hadoop patch can be found at <link
                  xlink:href="https://issues.apache.org/jira/browse/HADOOP-8545"/></para>
            </listitem>
            <listitem>
              <para>The most current Sahara maintained version of this patch can be found in the
                  <link xlink:href="https://git.openstack.org/cgit/openstack/sahara-extra">Sahara Extra
                  repository</link></para>
            </listitem>
            <listitem>
              <para>The latest compiled version of the jar for this component can be downloaded from
                  <link xlink:href="http://tarballs.openstack.org/sahara/dist/hadoop-openstack/master/"
                /></para>
            </listitem>
          </itemizedlist>
          <para>Now the latest version of this jar (which uses Keystone API v3) is used in the
            plugins’ images automatically during build of these images. But for Ambari plugin we
            need to explicitly put this jar into /opt directory of the base image <emphasis
              role="bold">before</emphasis> cluster launching.</para>
          <section xml:id="hadoop-patching">
            <title>Hadoop patching</title>
            <para>You may build the jar file yourself by choosing the latest patch from the Sahara
              Extra repository and using Maven to build with the pom.xml file provided. Or you may
              get the latest jar pre-built at <link xlink:href="http://tarballs.openstack.org/sahara/dist/hadoop-openstack/master/"
              /></para>
            <para>You will need to put this file into the hadoop libraries (e.g.
              /usr/lib/share/hadoop/lib, it depends on the plugin which you use) on each
              ResourceManager and NodeManager node (for Hadoop 2.x) in the cluster.</para>
          </section>
          <section xml:id="hadoop-configurations">
            <title>Hadoop configurations</title>
            <para>In general, when Sahara runs a job on a cluster it will handle configuring the
              Hadoop installation. In cases where a user might require more in-depth configuration
              all the data is set in the <literal>core-site.xml</literal> file on the cluster
              instances using this template:</para>
            <screen language="xml">&lt;property&gt;
    &lt;name&gt;${name} + ${config}&lt;/name&gt;
    &lt;value&gt;${value}&lt;/value&gt;
    &lt;description&gt;${not mandatory description}&lt;/description&gt;
&lt;/property&gt;</screen>
            <para>There are two types of configs here:</para>
            <procedure>
              <step>
                <para>General. The <literal>${name}</literal> in this case equals to
                    <literal>fs.swift</literal>. Here is the list of
                  <literal>${config}</literal>:</para>
                <itemizedlist>
                  <listitem>
                    <para><literal>.impl</literal> - Swift FileSystem implementation. The ${value}
                      is
                      <literal>org.apache.hadoop.fs.swift.snative.SwiftNativeFileSystem</literal></para>
                  </listitem>
                  <listitem>
                    <para><literal>.connect.timeout</literal> - timeout for all connections by
                      default: 15000</para>
                  </listitem>
                  <listitem>
                    <para><literal>.socket.timeout</literal> - how long the connection waits for
                      responses from servers. by default: 60000</para>
                  </listitem>
                  <listitem>
                    <para><literal>.connect.retry.count</literal> - connection retry count for all
                      connections. by default: 3</para>
                  </listitem>
                  <listitem>
                    <para><literal>.connect.throttle.delay</literal> - delay in millis between bulk
                      (delete, rename, copy operations). by default: 0</para>
                  </listitem>
                  <listitem>
                    <para><literal>.blocksize</literal> - blocksize for filesystem. By default:
                      32Mb</para>
                  </listitem>
                  <listitem>
                    <para><literal>.partsize</literal> - the partition size for uploads. By default:
                      4608*1024Kb</para>
                  </listitem>
                  <listitem>
                    <para><literal>.requestsize</literal> - request size for reads in KB. By
                      default: 64Kb</para>
                  </listitem>
                </itemizedlist>
              </step>
              <step>
                <para>Provider-specific. The patch for Hadoop supports different cloud providers.
                  The <literal>${name}</literal> in this case equals to
                    <literal>fs.swift.service.${provider}</literal>.</para>
                <para>Here is the list of <literal>${config}</literal>:</para>
                <itemizedlist>
                  <listitem>
                    <para><literal>.auth.url</literal> - authorization URL</para>
                  </listitem>
                  <listitem>
                    <para><literal>.auth.endpoint.prefix</literal> - prefix for the service url,
                      e.g. <literal>/AUTH_</literal></para>
                  </listitem>
                  <listitem>
                    <para><literal>.tenant</literal> - project name</para>
                  </listitem>
                  <listitem>
                    <para>
                      <literal>.username</literal>
                    </para>
                  </listitem>
                  <listitem>
                    <para>
                      <literal>.password</literal>
                    </para>
                  </listitem>
                  <listitem>
                    <para><literal>.domain.name</literal> - Domains can be used to specify users who
                      are not in the project specified.</para>
                  </listitem>
                  <listitem>
                    <para><literal>.domain.id</literal> - You can also specify domain using
                      id.</para>
                  </listitem>
                  <listitem>
                    <para><literal>.trust.id</literal> - Trusts are optionally used to scope the
                      authentication tokens of the supplied user.</para>
                  </listitem>
                  <listitem>
                    <para>
                      <literal>.http.port</literal>
                    </para>
                  </listitem>
                  <listitem>
                    <para>
                      <literal>.https.port</literal>
                    </para>
                  </listitem>
                  <listitem>
                    <para><literal>.region</literal> - Swift region is used when cloud has more than
                      one Swift installation. If region param is not set first region from Keystone
                      endpoint list will be chosen. If region param not found exception will be
                      thrown.</para>
                  </listitem>
                  <listitem>
                    <para><literal>.location-aware</literal> - turn On location awareness. Is false
                      by default</para>
                  </listitem>
                  <listitem>
                    <para>
                      <literal>.apikey</literal>
                    </para>
                  </listitem>
                  <listitem>
                    <para>
                      <literal>.public</literal>
                    </para>
                  </listitem>
                </itemizedlist>
              </step>
            </procedure>
          </section>
          <section xml:id="example">
            <title>Example</title>
            <para>For this example it is assumed that you have setup a Hadoop instance with a valid
              configuration and the Swift filesystem component. Furthermore there is assumed to be a
              Swift container named <literal>integration</literal> holding an object named
                <literal>temp</literal>, as well as a Keystone user named <literal>admin</literal>
              with a password of <literal>swordfish</literal>.</para>
            <para>The following example illustrates how to copy an object to a new location in the
              same container. We will use Hadoop’s <literal>distcp</literal> command (<link
                xlink:href="http://hadoop.apache.org/docs/r0.19.0/distcp.html"/>) to accomplish the
              copy. Note that the service provider for our Swift access is
              <literal>sahara</literal>, and that we will not need to specify the project of our
              Swift container as it will be provided in the Hadoop configuration.</para>
            <para>Swift paths are expressed in Hadoop according to the following template:
                <literal>swift://${container}.${provider}/${object}</literal>. For our example
              source this will appear as <literal>swift://integration.sahara/temp</literal>.</para>
            <para>Let’s run the job:</para>
            <screen language="console">$ hadoop distcp -D fs.swift.service.sahara.username=admin \
 -D fs.swift.service.sahara.password=swordfish \
 swift://integration.sahara/temp swift://integration.sahara/temp1</screen>
            <para>After that just confirm that <literal>temp1</literal> has been created in our
                <literal>integration</literal> container.</para>
          </section>
          <section xml:id="limitations">
            <title>Limitations</title>
            <para><emphasis role="bold">Note:</emphasis> Container names should be
              a valid URI.</para>
          </section>
        </section>
        <section xml:id="vanilla-imagebuilder" xml:base="vanilla-imagebuilder">
          <title>Building Images for Vanilla Plugin</title>
          <para>In this document you will find instruction on how to build Ubuntu, Fedora, and
            CentOS images with Apache Hadoop version 2.x.x.</para>
          <para>As of now the vanilla plugin works with images with pre-installed versions of Apache
            Hadoop. To simplify the task of building such images we use <link
              xlink:href="https://git.openstack.org/cgit/openstack/diskimage-builder">Disk Image
              Builder</link>.</para>
          <para><literal>Disk Image Builder</literal> builds disk images using elements. An element
            is a particular set of code that alters how the image is built, or runs within the
            chroot to prepare the image.</para>
          <para>Elements for building vanilla images are stored in the <link
              xlink:href="https://git.openstack.org/cgit/openstack/sahara-image-elements">Sahara image
              elements repository</link></para>
          <note>
            <para>Sahara requires images with cloud-init package installed:</para>
            <itemizedlist>
              <listitem>
                <para>
                  <link xlink:href="http://pkgs.fedoraproject.org/cgit/cloud-init.git/">For Fedora</link>
                </para>
              </listitem>
              <listitem>
                <para>
                  <link xlink:href="http://packages.ubuntu.com/trusty/cloud-init">For Ubuntu 14</link>
                </para>
              </listitem>
              <listitem>
                <para>
                  <link xlink:href="http://mirror.centos.org/centos/6/extras/x86_64/Packages/cloud-init-0.7.5-10.el6.centos.2.x86_64.rpm"
                    >For CentOS 6</link>
                </para>
              </listitem>
              <listitem>
                <para>
                  <link xlink:href="http://mirror.centos.org/centos/7/extras/x86_64/Packages/cloud-init-0.7.5-10.el7.centos.1.x86_64.rpm"
                    >For CentOS 7</link>
                </para>
              </listitem>
            </itemizedlist>
          </note>
          <para>To create vanilla images follow these steps:</para>
          <procedure>
            <step>
              <para>Clone repository “<link xlink:href="https://git.openstack.org/cgit/openstack/sahara-image-elements"/>”
                locally.</para>
            </step>
            <step>
              <para>Use tox to build images.</para>
              <para>You can run the command below in sahara-image-elements directory to build
                images. By default this script will attempt to create cloud images for all versions
                of supported plugins and all operating systems (subset of Ubuntu, Fedora, and CentOS
                depending on plugin).</para>
              <screen language="console">tox -e venv -- sahara-image-create -u</screen>
              <para>If you want to build Vanilla 2.7.1 image with centos 7 just execute:</para>
              <screen language="console">tox -e venv -- sahara-image-create -p vanilla -v 2.7.1 -i centos7</screen>
              <para>Tox will create a virtualenv and install required python packages in it, clone
                the repositories “<link xlink:href="https://git.openstack.org/cgit/openstack/diskimage-builder"/>” and “<link
                  xlink:href="https://git.openstack.org/cgit/openstack/sahara-image-elements"/>” and
                export necessary parameters.</para>
              <itemizedlist>
                <listitem>
                  <para><literal>DIB_HADOOP_VERSION</literal> - version of Hadoop to install</para>
                </listitem>
                <listitem>
                  <para><literal>JAVA_DOWNLOAD_URL</literal> - download link for JDK (tarball or
                    bin)</para>
                </listitem>
                <listitem>
                  <para><literal>OOZIE_DOWNLOAD_URL</literal> - download link for OOZIE (we have
                    built Oozie libs here:
                      <literal>http://sahara-files.mirantis.com/oozie-4.2.0-hadoop-2.7.1.tar.gz</literal>)</para>
                </listitem>
                <listitem>
                  <para><literal>SPARK_DOWNLOAD_URL</literal> - download link for Spark</para>
                </listitem>
                <listitem>
                  <para><literal>HIVE_VERSION</literal> - version of Hive to install (currently
                    supports only 0.11.0)</para>
                </listitem>
                <listitem>
                  <para>
                    <literal>ubuntu_image_name</literal>
                  </para>
                </listitem>
                <listitem>
                  <para>
                    <literal>fedora_image_name</literal>
                  </para>
                </listitem>
                <listitem>
                  <para><literal>DIB_IMAGE_SIZE</literal> - parameter that specifies a volume of
                    hard disk of instance. You need to specify it only for Fedora because Fedora
                    doesn’t use all available volume</para>
                </listitem>
                <listitem>
                  <para><literal>DIB_COMMIT_ID</literal> - latest commit id of diskimage-builder
                    project</para>
                </listitem>
                <listitem>
                  <para><literal>SAHARA_ELEMENTS_COMMIT_ID</literal> - latest commit id of
                    sahara-image-elements project</para>
                </listitem>
              </itemizedlist>
              <variablelist>
                <varlistentry>
                  <term>NOTE: If you don’t want to use default values, you should set your
                    values</term>
                  <listitem>
                    <para>of parameters.</para>
                  </listitem>
                </varlistentry>
              </variablelist>
              <para>Then it will create required cloud images using image elements that install all
                the necessary packages and configure them. You will find created images in the
                parent directory.</para>
            </step>
          </procedure>
          <note>
            <para>Disk Image Builder will generate QCOW2 images, used with the default OpenStack
              Qemu/KVM hypervisors. If your OpenStack uses a different hypervisor, the generated
              image should be converted to an appropriate format.</para>
            <para>VMware Nova backend requires VMDK image format. You may use qemu-img utility to
              convert a QCOW2 image to VMDK.</para>
            <screen language="console">qemu-img convert -O vmdk &lt;original_image&gt;.qcow2 &lt;converted_image&gt;.vmdk</screen>
          </note>
          <para>For finer control of diskimage-create.sh see the <link
              xmlns:xl="http://www.w3.org/1999/xlink"
              xl:href="https://git.openstack.org/cgit/openstack/sahara-image-elements/tree/diskimage-create/README.rst"
              >official documentation</link></para>
        </section>
        <section xml:id="cdh-imagebuilder" xml:base="cdh-imagebuilder">
          <title>Building Images for Cloudera Plugin</title>
          <para>In this document you will find instructions on how to build Ubuntu and CentOS images
            with Cloudera Express (now only versions {5.0.0, 5.3.0 5.4.0, 5.5.0, 5.7.x, 5.9.x} are
            supported).</para>
          <para>To simplify the task of building such images we use <link
              xmlns:xl="http://www.w3.org/1999/xlink"
              xl:href="https://git.openstack.org/cgit/openstack/diskimage-builder">Disk Image
              Builder</link>.</para>
          <para><literal>Disk Image Builder</literal> builds disk images using elements. An element
            is a particular set of code that alters how the image is built, or runs within the
            chroot to prepare the image.</para>
          <para>Elements for building Cloudera images are stored in <link
              xlink:href="https://git.openstack.org/cgit/openstack/sahara-image-elements">Sahara extra
              repository</link></para>
          <note>
            <para>Sahara requires images with cloud-init package installed:</para>
            <itemizedlist>
              <listitem>
                <para>
                  <link xlink:href="http://mirror.centos.org/centos/6/extras/x86_64/Packages/cloud-init-0.7.5-10.el6.centos.2.x86_64.rpm"
                    >For CentOS 6</link>
                </para>
              </listitem>
              <listitem>
                <para>
                  <link xlink:href="http://mirror.centos.org/centos/7/extras/x86_64/Packages/cloud-init-0.7.5-10.el7.centos.1.x86_64.rpm"
                    >For CentOS 7</link>
                </para>
              </listitem>
              <listitem>
                <para>
                  <link xlink:href="http://packages.ubuntu.com/trusty/cloud-init">For Ubuntu 14</link>
                </para>
              </listitem>
            </itemizedlist>
          </note>
          <para>To create cloudera images follow these steps:</para>
          <procedure>
            <step>
              <para>Clone repository “<link xlink:href="https://git.openstack.org/cgit/openstack/sahara-image-elements"/>”
                locally.</para>
            </step>
            <step>
              <para>Use tox to build images.</para>
              <para>You can run “tox -e venv – sahara-image-create” command in sahara-image-elements
                directory to build images. By default this script will attempt to create cloud
                images for all versions of supported plugins and all operating systems (subset of
                Ubuntu, Fedora, and CentOS depending on plugin). To only create Cloudera images, you
                should use the “-p cloudera” parameter in the command line. If you want to create
                the image only for a specific operating system, you should use the “-i
                ubuntu|centos|centos7” parameter to assign the operating system (the cloudera plugin
                only supports Ubuntu and Centos). If you want to create the image only for a
                specific Cloudera version, you should use the “-v 5.0|5.3|5.4|5.5|5.7|5.9” parameter
                to assign the version. Note that Centos 7 can only be used with CDH 5.5 and higher.
                Below is an example to create Cloudera images for both Ubuntu and CentOS with
                Cloudera Express 5.5.0 version.</para>
              <screen language="console">tox -e venv -- sahara-image-create -p cloudera -v 5.5</screen>
              <para>If you want to create only an Ubuntu image, you may use following example for
                that:</para>
              <screen language="console">tox -e venv -- sahara-image-create -p cloudera -i ubuntu -v 5.5</screen>
              <para>For CDH 5.7 and higher we support minor versions. If you want to build a minor
                version just export DIB_CDH_MINOR_VERSION before sahara-image-create launch,
                e.g.:</para>
              <screen language="console">export DIB_CDH_MINOR_VERSION=5.7.1</screen>
              <para>NOTE: If you don’t want to use default values, you should explicitly set the
                values of your required parameters.</para>
              <para>The script will create required cloud images using image elements that install
                all the necessary packages and configure them. You will find the created images in
                the parent directory.</para>
            </step>
          </procedure>
          <note>
            <para>Disk Image Builder will generate QCOW2 images, used with the default OpenStack
              Qemu/KVM hypervisors. If your OpenStack uses a different hypervisor, the generated
              image should be converted to an appropriate format.</para>
            <para>The VMware Nova backend requires the VMDK image format. You may use qemu-img
              utility to convert a QCOW2 image to VMDK.</para>
            <screen language="console">qemu-img convert -O vmdk &lt;original_image&gt;.qcow2 &lt;converted_image&gt;.vmdk</screen>
          </note>
          <para>For finer control of diskimage-create.sh see the <link xlink:href="https://git.openstack.org/cgit/openstack/sahara-image-elements/tree/diskimage-create/README.rst"
              >official documentation</link></para>
        </section>
      </section>
  </chapter>
