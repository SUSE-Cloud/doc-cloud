<?xml version="1.0"?>
<?xml-stylesheet href="urn:x-suse:xslt:profiling:docbook51-profile.xsl"
 type="text/xml"
 title="Profiling step"?>
<!DOCTYPE section [
 <!ENTITY % entities SYSTEM "entities.ent"> %entities;
]>
<section xmlns="http://docbook.org/ns/docbook" xmlns:xi="http://www.w3.org/2001/XInclude" xmlns:xlink="http://www.w3.org/1999/xlink" xml:id="replace_shared_lm">
 <title>Replacing a Shared &lcm;/Controller Node</title>
 <para>
  If the controller node you need to replace was also being used as your
  &lcm; then use these steps below. If this is not a shared
  controller then skip to the next section.
 </para>
 <para>
  If the controller node you need to replace was also being used as your
  &lcm; then use these steps below. If this is not a shared
  controller then skip to the next section.
 </para>
 <procedure>
  <step>
   <para>
    Ensuring that you use the same version of &kw-hos; that you previously had
    loaded on your &lcm;, you will need to download and install the
    lifecycle management software using the instructions from the installation
    guide:
   </para>
   <orderedlist>
    <listitem>
     <para>
      <xref linkend="sec.kvm.setup_deployer"/>
     </para>
    </listitem>
    <listitem>
     <para>
      To restore your data, see <xref linkend="pit_lifecyclemanager_recovery"/>
     </para>
    </listitem>
   </orderedlist>
  </step>
  <step>
   <para>
    Update your cloud model (<literal>servers.yml</literal>) with the new
    <literal>mac-addr</literal>, <literal>ilo-ip</literal>,
    <literal>ilo-password</literal>, or <literal>ilo-user</literal> fields
    where these have changed. Do not change the <literal>id</literal>,
    <literal>ip-addr</literal>, <literal>role</literal>, or
    <literal>server-group</literal> settings. (Please follow the procedure for
    updating your cloud model in the git repo)
   </para>
  </step>
  <step>
   <para>
    Get the <emphasis role="bold">servers.yml</emphasis> file stored in git:
   </para>
<screen>cd ~/openstack/my_cloud/definition/data
git checkout site</screen>
   <para>
    then change, as necessary, the <literal>mac-addr</literal>,
    <literal>ilo-ip</literal>, <literal>ilo-password</literal>, and
    <literal>ilo-user</literal> fields of this existing controller node. Save
    and commit the change
   </para>
<screen>git commit -a -m "repaired node X"</screen>
  </step>
  <step>
   <para>
    Run the configuration processor as follows:
   </para>
<screen>cd ~/openstack/ardana/ansible
ansible-playbook -i hosts/localhost config-processor-run.yml</screen>
   <para>
    Then run ready-deployment:
   </para>
<screen>cd ~/openstack/ardana/ansible
ansible-playbook -i hosts/localhost ready-deployment.yml</screen>
  </step>
  <step>
   <para>
    Once that is complete, copy the Cobbler images to the correct location:
   </para>
<screen>mkdir /srv/www/cobbler/ks_mirror/hlinux-cattleprod
cp /opt/hlm_packager/hos-3.0.0/hlinux_venv/hlm/hlinux/dists/cattleprod/main/installer-amd64/current/images/netboot/debian-installer/amd64/linux /srv/www/cobbler/ks_mirror/hlinux-cattleprod
cp /opt/hlm_packager/hos-3.0.0/hlinux_venv/hlm/hlinux/dists/cattleprod/main/installer-amd64/current/images/netboot/debian-installer/amd64/initrd.gz /srv/www/cobbler/ks_mirror/hlinux-cattleprod</screen>
  </step>
  <step>
   <para>
    Deploy Cobbler:
   </para>
<screen>cd ~/openstack/ardana/ansible
ansible-playbook -i hosts/localhost cobbler-deploy.yml</screen>
  </step>
  <step>
   <para>
    Delete the haproxy user:
   </para>
<screen>deluser haproxy</screen>
  </step>
  <step>
   <para>
    Install the software on your new &lcm;/controller node with
    these three playbooks:
   </para>
<screen>cd ~/scratch/ansible/next/hos/ansible
ansible-playbook -i hosts/verb_hosts monasca-rebuild-pretasks.yml
ansible-playbook -i hosts/verb_hosts osconfig-run.yml -e rebuild=True --limit=&lt;controller-hostname&gt;
ansible-playbook -i hosts/verb_hosts hlm-deploy.yml -e rebuild=True --limit=&lt;controller-hostname&gt;,&lt;first-proxy-hostname&gt;</screen>
  </step>
  <step>
   <para>
    During the replacement of the node there will be alarms that show up during
    the process. If those do not clear after the node is back up and healthy,
    restart the threshold engine by running the following playbooks:
   </para>
<screen>cd ~/scratch/ansible/next/hos/ansible
ansible-playbook -i hosts/verb_hosts monasca-stop.yaml --tags thresh
ansible-playbook -i hosts/verb_hosts monasca-start.yaml --tags thresh</screen>
  </step>
 </procedure>
</section>
