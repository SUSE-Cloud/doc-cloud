<?xml version="1.0" encoding="UTF-8" standalone="no"?>
<section xmlns="http://docbook.org/ns/docbook" xmlns:xlink="http://www.w3.org/1999/xlink" xmlns:xi="http://www.w3.org/2001/XInclude" version="5.1" xml:id="config-dvr-snat-ha-ovs">
  <title>Distributed Virtual Routing with VRRP</title>
  <para><xref linkend="deploy-ovs-ha-dvr"/> supports augmentation
            using Virtual Router Redundancy Protocol (VRRP). Using this configuration,
            virtual routers support both the <literal>--distributed</literal> and <literal>--ha</literal> options.</para>
  <para>Similar to legacy HA routers, DVR/SNAT HA routers provide a quick fail over of
            the SNAT service to a backup DVR/SNAT router on an l3-agent running on a
            different node.</para>
  <para>SNAT high availability is implemented in a manner similar to the
            <xref linkend="deploy-lb-ha-vrrp"/> and <xref linkend="deploy-ovs-ha-vrrp"/> examples where
            <literal>keepalived</literal> uses VRRP to provide quick failover of SNAT services.</para>
  <para>During normal operation, the master router periodically transmits <emphasis>heartbeat</emphasis>
            packets over a hidden project network that connects all HA routers for a
            particular project.</para>
  <para>If the DVR/SNAT backup router stops receiving these packets, it assumes failure
            of the master DVR/SNAT router and promotes itself to master router by
            configuring IP addresses on the interfaces in the <literal>snat</literal> namespace. In
            environments with more than one backup router, the rules of VRRP are followed
            to select a new master router.</para>
  <warning>
    <para>There is a known bug with <literal>keepalived</literal> v1.2.15 and earlier which can
                cause packet loss when <literal>max_l3_agents_per_router</literal> is set to 3 or more.
                Therefore, we recommend that you upgrade to <literal>keepalived</literal> v1.2.16
                or greater when using this feature.</para>
  </warning>
  <note>
    <para>Experimental feature or incomplete documentation.</para>
  </note>
  <section>
    <title>Configuration example</title>
    <para>The basic deployment model consists of one controller node, two or more network
                nodes, and multiple computes nodes.</para>
    <section>
      <title>Controller node configuration</title>
      <procedure>
        <step>
          <para>Add the following to <literal>/etc/neutron/neutron.conf</literal>:</para>
          <screen language="ini">[DEFAULT]
core_plugin = ml2
service_plugins = router
allow_overlapping_ips = True
router_distributed = True
l3_ha = True
l3_ha_net_cidr = 169.254.192.0/18
max_l3_agents_per_router = 3</screen>
          <para>When the <literal>router_distributed = True</literal> flag is configured, routers created
                            by all users are distributed. Without it, only privileged users can create
                            distributed routers by using <literal>--distributed True</literal>.</para>
          <para>Similarly, when the <literal>l3_ha = True</literal> flag is configured, routers created
                            by all users default to HA.</para>
          <para>It follows that with these two flags set to <literal>True</literal> in the configuration
                            file, routers created by all users will default to distributed HA routers
                            (DVR HA).</para>
          <para>The same can explicitly be accomplished by a user with administrative
                            credentials setting the flags in the <command>neutron router-create</command>
                            command:</para>
          <screen language="console">$ neutron router-create name-of-router --distributed=True --ha=True</screen>
          <note>
            <para>The <emphasis>max_l3_agents_per_router</emphasis> determine the number of backup
                                DVR/SNAT routers which  will be instantiated.</para>
          </note>
        </step>
        <step>
          <para>Add the following to <literal>/etc/neutron/plugins/ml2/ml2_conf.ini</literal>:</para>
          <screen language="ini">[ml2]
type_drivers = flat,vxlan
tenant_network_types = vxlan
mechanism_drivers = openvswitch,l2population
extension_drivers = port_security

[ml2_type_flat]
flat_networks = external

[ml2_type_vxlan]
vni_ranges = MIN_VXLAN_ID:MAX_VXLAN_ID</screen>
          <para>Replace <literal>MIN_VXLAN_ID</literal> and <literal>MAX_VXLAN_ID</literal> with  VXLAN ID minimum and
                            maximum values suitable for your environment.</para>
          <note>
            <para>The first value in the <literal>tenant_network_types</literal> option becomes the
                                default project network type when a regular user creates a network.</para>
          </note>
        </step>
      </procedure>
    </section>
    <section>
      <title>Network nodes</title>
      <procedure>
        <step>
          <para>Configure the Open vSwitch agent. Add the following to
                            <literal>/etc/neutron/plugins/ml2/ml2_conf.ini</literal>:</para>
          <screen language="ini">[ovs]
local_ip = TUNNEL_INTERFACE_IP_ADDRESS
bridge_mappings = external:br-ex

[agent]
enable_distributed_routing = True
tunnel_types = vxlan
l2_population = True</screen>
          <para>Replace <literal>TUNNEL_INTERFACE_IP_ADDRESS</literal> with the IP address of the interface
                            that handles VXLAN project networks.</para>
        </step>
        <step>
          <para>Configure the L3 agent. Add the following to <literal>/etc/neutron/l3_agent.ini</literal>:</para>
          <screen language="ini">[DEFAULT]
ha_vrrp_auth_password = password
interface_driver = openvswitch
external_network_bridge =
agent_mode = dvr_snat</screen>
          <note>
            <para>The <literal>external_network_bridge</literal> option intentionally contains
                                no value.</para>
          </note>
        </step>
      </procedure>
    </section>
    <section>
      <title>Compute nodes</title>
      <procedure>
        <step>
          <para>Configure the Open vSwitch agent. Add the following to
                            <literal>/etc/neutron/plugins/ml2/ml2_conf.ini</literal>:</para>
          <screen language="ini"><?dbsuse-fo font-size="8pt"?>[ovs]
local_ip = TUNNEL_INTERFACE_IP_ADDRESS
bridge_mappings = external:br-ex

[agent]
enable_distributed_routing = True
tunnel_types = vxlan
l2_population = True

[securitygroup]
firewall_driver = neutron.agent.linux.iptables_firewall.OVSHybridIptablesFirewallDriver</screen>
        </step>
        <step>
          <para>Configure the L3 agent. Add the following to <literal>/etc/neutron/l3_agent.ini</literal>:</para>
          <screen language="ini">[DEFAULT]
interface_driver = openvswitch
external_network_bridge =
agent_mode = dvr</screen>
          <para>Replace <literal>TUNNEL_INTERFACE_IP_ADDRESS</literal> with the IP address of the interface
                            that handles VXLAN project networks.</para>
        </step>
      </procedure>
    </section>
    <section>
      <title>Keepalived VRRP health check</title>
      <para>The health of your <literal>keepalived</literal> instances can be automatically monitored via
                    a bash script that verifies connectivity to all available and configured
                    gateway addresses. In the event that connectivity is lost, the master router
                    is rescheduled to another node.</para>
      <para>If all routers lose connectivity simultaneously, the process of selecting a
                    new master router will be repeated in a round-robin fashion until one or more
                    routers have their connectivity restored.</para>
      <para>To enable this feature, edit the <literal>l3_agent.ini</literal> file:</para>
      <screen language="ini">ha_vrrp_health_check_interval = 30</screen>
      <para>Where <literal>ha_vrrp_health_check_interval</literal> indicates how often in seconds the
                    health check should run. The default value is <literal>0</literal>, which indicates that the
                    check should not run at all.</para>
    </section>
  </section>
  <section>
    <title>Known limitations</title>
    <itemizedlist>
      <listitem>
        <para>Migrating a router from distributed only, HA only, or legacy to distributed
                        HA is not supported at this time. The router must be created as distributed
                        HA.
                        The reverse direction is also not supported. You cannot reconfigure a
                        distributed HA router to be only distributed, only HA, or legacy.</para>
      </listitem>
      <listitem>
        <para>There are certain scenarios where l2pop and distributed HA routers do not
                        interact in an expected manner. These situations are the same that affect HA
                        only routers and l2pop.</para>
      </listitem>
    </itemizedlist>
  </section>
</section>
