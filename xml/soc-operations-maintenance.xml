<?xml version="1.0"?>
<?xml-stylesheet href="urn:x-suse:xslt:profiling:docbook51-profile.xsl"
 type="text/xml"
 title="Profiling step"?>
<!DOCTYPE chapter [
 <!ENTITY % entities SYSTEM "entity-decl.ent"> %entities;
]>

<chapter xmlns="http://docbook.org/ns/docbook" xmlns:xi="http://www.w3.org/2001/XInclude" xmlns:xlink="http://www.w3.org/1999/xlink" version="5.1" xml:id="sec.maintenance">
 <title>Maintenance</title>

 <section xml:id="sec.nodes-update">
  <title>Keeping the Nodes Up-To-Date</title>
  <para>
   Keeping the nodes in &productname; up-to-date requires an appropriate
   setup of the update and pool repositories and the deployment of
   either the <guimenu>Updater</guimenu> &barcl; or the &susemgr;
   &barcl;. For details, see
   <xref linkend="sec.depl.adm_conf.repos.scc"/>, <xref
   linkend="sec.depl.inst.nodes.post.updater"/>, and
   <xref linkend="sec.depl.inst.nodes.post.manager"/>.
  </para>
  <para>
   If one of those &barcl;s is deployed, patches are installed on the
   nodes. Patches that do not require a reboot will not cause a service
   interruption. If a patch (for example, a kernel update) requires a reboot
   after the installation, services running on the machine that is rebooted
   will not be available within &cloud;.  Therefore, we strongly recommend
   installing those patches during a maintenance window.
  </para>
  <note>
   <title>No Maintenance Mode</title>
   <para>
    As of &productname; 8, it is not possible to put
    &cloud; into <quote>Maintenance Mode</quote>.
   </para>
  </note>
  <variablelist>
   <title>Consequences when Rebooting Nodes</title>
   <varlistentry>
    <term>&admserv;</term>
    <listitem>
     <para>
      While the &admserv; is offline, it is not possible to deploy new
      nodes. However, rebooting the &admserv; has no effect on starting
      &vmguest;s or on &vmguest;s already running.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>&contrnode;s</term>
    <listitem>
     <para>
      The consequences a reboot of a &contrnode; depend on the
      services running on that node:
     </para>
     <formalpara>
      <title>Database, &o_ident;, RabbitMQ, &o_img;, &o_comp;:</title>
      <para>
       No new &vmguest;s can be started.
      </para>
     </formalpara>
     <formalpara>
      <title>&o_objstore;:</title>
      <para>
       No object storage data is available. If &o_img; uses
       &o_objstore;, it will not be possible to start new &vmguest;s.
      </para>
     </formalpara>
     <formalpara>
      <title>&o_blockstore;, &ceph;:</title>
      <para>
       No block storage data is available.
      </para>
     </formalpara>
     <formalpara>
      <title>&o_netw;:</title>
      <para>
       No new &vmguest;s can be started. On running &vmguest;s the
       network will be unavailable.
      </para>
     </formalpara>
     <formalpara>
      <title>&o_dash;</title>
      <para>
       &o_dash; will be unavailable. Starting and managing &vmguest;s
       can be done with the command line tools.
      </para>
     </formalpara>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>&compnode;s</term>
    <listitem>
     <para>
      Whenever a &compnode; is rebooted, all &vmguest;s running on
      that particular node will be shut down and must be manually restarted.
      Therefore it is recommended to <quote>evacuate</quote> the node by
      migrating &vmguest;s to another node, before rebooting it.
     </para>
    </listitem>
   </varlistentry>
  </variablelist>
 </section>

 <section xml:id="sec.service-orders">
  <title>Service Order on &cloud; Start-up or Shutdown</title>
  <para>
   In case you need to restart your complete &cloud; (after a complete shut
   down or a power outage), ensure that the external &ceph; cluster is started,
   available and healthy.
   Then start nodes and services in the order documented
   below.
  </para>
  <orderedlist>
   <title>Service Order on Start-up</title>
   <listitem>
    <para>
     &contrnode;/Cluster on which the Database is deployed
    </para>
   </listitem>
   <listitem>
    <para>
     &contrnode;/Cluster on which RabbitMQ is deployed
    </para>
   </listitem>
   <listitem>
    <para>
     &contrnode;/Cluster on which &o_ident; is deployed
    </para>
   </listitem>
   <listitem>
    <para>
     For &o_objstore;:
    </para>
    <orderedlist>
     <listitem>
      <para>
       &stornode; on which the <literal>swift-storage</literal> role is deployed
      </para>
     </listitem>
     <listitem>
      <para>
       &stornode; on which the <literal>swift-proxy</literal> role is deployed
      </para>
     </listitem>
    </orderedlist>
   </listitem>
   <listitem>
    <para>
     Any remaining &contrnode;/Cluster. The following additional rules apply:
    </para>
    <itemizedlist>
     <listitem>
      <para>
       The &contrnode;/Cluster on which the <literal>neutron-server</literal>
       role is deployed needs to be started before starting the node/cluster
       on which the <literal>neutron-l3</literal> role is deployed.
      </para>
     </listitem>
     <listitem>
      <para>
       The &contrnode;/Cluster on which the <literal>nova-controller</literal>
       role is deployed needs to be started before starting the node/cluster
       on which &o_orch; is deployed.
      </para>
     </listitem>
    </itemizedlist>
   </listitem>
   <listitem>
    <para>
     &compnode;s
    </para>
   </listitem>
  </orderedlist>
  <para>
   If multiple roles are deployed on a single &contrnode;, the services are
   automatically started in the correct order on that node. If you have more
   than one node with multiple roles, make sure they are
   started as closely as possible to the order listed above.
  </para>
  <para>
   To shut down &cloud;, terminate nodes and services in the order documented
   below (which is the reverse of the start-up order).
  </para>
  <orderedlist>
   <title>Service Order on Shutdown</title>
   <listitem>
    <para>
     &compnode;s
    </para>
   </listitem>
   <listitem>
    <para>
     &contrnode;/Cluster on which &o_orch; is deployed
    </para>
   </listitem>
   <listitem>
    <para>
     &contrnode;/Cluster on which the <literal>nova-controller</literal>
     role is deployed
    </para>
   </listitem>
   <listitem>
    <para>
     &contrnode;/Cluster on which the <literal>neutron-l3</literal>
     role is deployed
    </para>
   </listitem>
   <listitem>
    <para>
     All &contrnode;(s)/Cluster(s) on which neither of the following services
     is deployed: Database, RabbitMQ, and &o_ident;.
    </para>
   </listitem>
   <listitem>
    <para>
     For &o_objstore;:
    </para>
    <orderedlist>
     <listitem>
      <para>
       &stornode; on which the <literal>swift-proxy</literal> role is
       deployed
      </para>
     </listitem>
     <listitem>
      <para>
       &stornode; on which the <literal>swift-storage</literal> role is
       deployed
      </para>
     </listitem>
    </orderedlist>
   </listitem>
   <listitem>
    <para>
     &contrnode;/Cluster on which &o_ident; is deployed
    </para>
   </listitem>
   <listitem>
    <para>
     &contrnode;/Cluster on which RabbitMQ is deployed
    </para>
   </listitem>
   <listitem>
    <para>
     &contrnode;/Cluster on which the Database is deployed
    </para>
   </listitem>
   <listitem>
    <para>
     If required, gracefully shut down an external &ceph; cluster
    </para>
   </listitem>
  </orderedlist>
 </section>

 <section xml:id="sec.upgrade-8-9">
  <title>Upgrading from &productname; 8 to &productname; 9</title>
  <para>
   Upgrading from &productname; 8 to &productname; 9 can be done either via a
   &wi; or from the command line. &productname; supports a non-disruptive
   upgrade which provides a fully-functional &cloud; operation during most of
   the upgrade procedure, if your installation meets the requirements at <xref
   linkend="il.upgrade-8-9.non-disruptive"/>.
  </para>
  <para>
   If the requirements for a non-disruptive upgrade are not met, the
   upgrade procedure will be done in normal mode. When
   live-migration is set up, &vmguest;s will be migrated to another node
   before the respective &compnode; is updated to ensure continuous
   operation.
 </para>
  <important>
    <title>STONITH and &admserv;</title>
    <para>
      Make sure that the STONITH mechanism in your cloud does not rely on the
      state of the &admserv; (for example, no SBD devices are located there,
      and IPMI is not using the network connection relying on the
      &admserv;). Otherwise, this may affect the clusters when the &admserv; is
      rebooted during the upgrade procedure.
     </para>
    </important>

  <section xml:id="sec.upgrade-8-9.requirements">
   <title>Requirements</title>
   <para>
    When starting the upgrade process, several checks are performed to
    determine whether the &cloud; is in an upgradeable state and whether a
    non-disruptive update would be supported:
   </para>
   <itemizedlist>
    <title>General Upgrade Requirements</title>
    <listitem>
     <para>
      All nodes need to have the latest &productname; 8 updates <emphasis
      role="bold">and</emphasis> the latest &slsa; 12 SP3 updates installed. If
      this is not the case, refer to <xref
      linkend="sec.depl.inst.nodes.post.updater"/> for instructions on how to
      update.
     </para>
    </listitem>
    <listitem>
     <para>
      All allocated nodes need to be turned on and have to be in state
      <quote>ready</quote>.
     </para>
    </listitem>
    <listitem>
     <para>
      All &barcl; proposals need to have been successfully deployed. If a
      proposal is in state <quote>failed</quote>, the upgrade procedure will
      refuse to start. Fix the issue or&mdash;if possible&mdash;remove the
      proposal.
     </para>
    </listitem>
    <listitem>
     <para>
      If the &pacemaker; &barcl; is deployed, all clusters
      need to be in a healthy state.
     </para>
    </listitem>
    <listitem>
     <para> The upgrade will not start when &ceph; is deployed via &crow;. Only
     external &ceph; is supported. Documentation for &ses; is available at
     <link
      xlink:href="https://www.suse.com/documentation/suse-enterprise-storage-5"/>.
     </para>
    </listitem>
    <listitem>
     <para>
      At this time, upgrade is only possible if the <literal>SQL
      Engine</literal> in the <literal>Database</literal> &barcl; is set to
      <guimenu>&mariadb;</guimenu>. Migration from &postgres;-based setups will
      be provided in a future maintenance update.
     </para>
    </listitem>
    <listitem>
     <para>
      The following repositories need to be available on a server that is
      accessible from the &admserv;. The HA repositories are only needed if you
      have an &hasetup;. It is recommended to use the same server that also
      hosts the respective repositories of the current version.
     </para>
     <simplelist>
      <member vendor="suse-crow"><literal>&cloud_repo;-Pool</literal></member>
      <member vendor="suse-crow"><literal>&cloud_repo;-Update</literal></member>
      <member><literal>&sle_repo;-Pool</literal></member>
      <member><literal>&sle_repo;-Update</literal></member>
      <member>
       <literal>&sleha_repo;-Pool</literal> (for &hasetup;s only)
      </member>
      <member>
       <literal>&sleha_repo;-Update</literal> (for &hasetup;s only)
      </member>
     </simplelist>
     <important>
      <para>
      Do not add repositories to the &cloud; repository configuration. This
      needs to be done during the upgrade procedure.
      </para>
     </important>
    </listitem>
    <listitem>
     <para>
       A non-disruptive upgrade is not supported if &o_blockstore; has been
       deployed with the <literal>raw devices</literal> or <literal>local
       file</literal> back-end. In this case, you have to perform a regular
       upgrade, or change the &o_blockstore; back-end for a non-disruptive
       upgrade.
     </para>
    </listitem>
    <listitem>
     <para>
       If &ses; is currently deployed using &crow;, migrate it to an
       external cluster. You may want to upgrade &ses;, refer to
       <link xlink:href="https://www.suse.com/documentation/suse-enterprise-storage-5/book_storage_deployment/data/ceph_upgrade_4to5crowbar.html"/>.
     </para>
    </listitem>
   </itemizedlist>
   <itemizedlist xml:id="il.upgrade-8-9.non-disruptive">
    <title>Non-Disruptive Upgrade Requirements</title>
    <listitem>
     <para>
      All &contrnode;s need to be set up highly available.
     </para>
    </listitem>
    <listitem>
     <para>
      A non-disruptive upgrade is not supported if the &o_blockstore;
      has been deployed with the <literal>raw devices</literal> or
      <literal>local file</literal> back-end. In this case, you have to perform
      a regular upgrade, or change the &o_blockstore; back-end for a
      non-disruptive upgrade.
     </para>
    </listitem>
    <listitem>
     <para>
      A non-disruptive upgrade is prevented if the
      <literal>cinder-volume</literal> service is placed on &compnode;. For a
      non-disruptive upgrade, <literal>cinder-volume</literal> should either be
      HA-enabled or placed on non-compute nodes.
     </para>
    </listitem>
    <listitem>
     <para>
      A non-disruptive upgrade is prevented if <literal>manila-share</literal>
      service is placed on a &compnode;. For more information, see <xref
      linkend="sec.depl.ostack.manila"/>.
     </para>
    </listitem>
    <listitem>
     <para>
      Live-migration support needs to be configured and enabled for the
      &compnode;s. The amount of free resources (CPU and RAM) on the
      &compnode;s needs to be sufficient to evacuate the nodes one by one.
     </para>
    </listitem>
    <listitem>
     <para>
       In case of a non-disruptive upgrade, &o_img; must be configured as a
       shared storage if the <guimenu>Default Storage
       Store</guimenu> value in the &o_img; is set to <literal>File</literal>.
     </para>
    </listitem>
    <listitem>
     <para>
       For a non-disruptive upgrade, only KVM-based &compnode;s with
       the <literal>nova-computer-kvm</literal> role are allowed in &productname; 7.
     </para>
    </listitem>
    <listitem>
     <para>
       Non-disruptive upgrade is limited to the following cluster
       configurations:
     </para>
       <itemizedlist>
       <listitem>
         <para>
           Single cluster that has all supported controller roles on it
         </para>
       </listitem>
       <listitem>
         <para>
          Two clusters where one only has
          <systemitem>neutron-network</systemitem> and the other one has the
          rest of the controller roles.
         </para>
       </listitem>
       <listitem>
         <para>
          Two clusters where one only has
          <systemitem>neutron-server</systemitem> plus
          <systemitem>neutron-network</systemitem> and the other one has the
          rest of the controller roles.
         </para>
       </listitem>
       <listitem>
         <para>
           Two clusters, where one cluster runs the database and RabbitMQ
         </para>
       </listitem>
       <listitem>
         <para>
           Three clusters, where one cluster runs database and RabbitMQ,
           another cluster runs APIs, and the third cluster has the
           <systemitem>neutron-network</systemitem> role.
         </para>
       </listitem>
       </itemizedlist>
       <para>
       If your cluster configuration is not supported by the non-disruptive
       upgrade procedure, you can still perform a normal upgrade.
     </para>
    </listitem>
   </itemizedlist>
  </section>
  <section xml:id="sec.upgrade-web-ui">
   <title>Upgrading Using the Web Interface</title>
   <para>
    The &wi; features a wizard that guides you through the upgrade
    procedure.
   </para>
   <note>
    <title>Canceling the Upgrade</title>
    <para>
     You can cancel the upgrade process by clicking <guimenu>Cancel
     Upgrade</guimenu>. The upgrade operation can only be canceled
     before the &admserv; upgrade is started. When the upgrade has been
     canceled, the nodes return to the ready state. However, any user
     modifications must be undone manually. This includes reverting repository
     configuration.
    </para>
   </note>
   <procedure>
    <step>
     <para>
      To start the upgrade procedure, open the &crow; &wi; on the &admserv; and choose <menuchoice>
      <guimenu>Utilities</guimenu> <guimenu>Upgrade</guimenu>
      </menuchoice>. Alternatively, point the browser directly to the upgrade
      wizard on the &admserv;, for example
      <literal>http://192.168.124.10/upgrade/</literal>.
     </para>
    </step>
    <step>
     <para>
      On the first screen of the &wi; you will run preliminary checks, get
      information about the upgrade mode and start the upgrade process.
     </para>
     <informalfigure>
      <mediaobject>
       <imageobject role="fo">
        <imagedata fileref="depl_upgrade7-8_prepare.png" width="75%"/>
       </imageobject>
       <imageobject role="html">
        <imagedata fileref="depl_upgrade7-8_prepare.png"/>
       </imageobject>
      </mediaobject>
     </informalfigure>
    </step>
    <step>
     <para>
      Perform the preliminary checks to determine whether the upgrade
      requirements are met by clicking <guimenu>Check</guimenu> in
      <literal>Preliminary Checks</literal>.
     </para>
     <para>
      The &wi; displays the progress of the checks. Make sure all checks are
      passed (you should see a green marker next to each check). If errors
      occur, fix them and run the <guimenu>Check</guimenu> again. Do not
      proceed until all checks are passed.
     </para>
    </step>
    <step>
     <para>
      When all checks in the previous step have passed, <literal>Upgrade
      Mode</literal> shows the result of the upgrade analysis. It will indicate
      whether the upgrade procedure will continue in non-disruptive or in
      normal mode.
     </para>
    </step>
    <step>
     <para>
      To start the upgrade process, click <guimenu>Begin Upgrade</guimenu>.
     </para>
    </step>
    <step>
     <para>
      While the upgrade of the &admserv; is prepared, the upgrade wizard
      prompts you to <guimenu>Download the Backup of the
      &admserv;</guimenu>. When the backup is done, move it to a safe place. If
      something goes wrong during the upgrade procedure of the &admserv;, you
      can restore the original state from this backup using the
      <command>crowbarctl backup restore
      <replaceable>NAME</replaceable></command> command.
     </para>
    </step>
    <step>
     <para>
      Check that the repositories required for upgrading the &admserv; are
      available and updated. To do this, click the <guimenu>Check</guimenu>
      button. If the checks fail, add the software repositories as described in
      <xref linkend="cha.depl.repo_conf" />. Run the
      checks again, and click <guimenu>Next</guimenu>.
     </para>
     <informalfigure>
      <mediaobject>
       <imageobject role="fo">
        <imagedata fileref="depl_upgrade7-8_repocheck-admin.png" width="75%"/>
       </imageobject>
       <imageobject role="html">
        <imagedata fileref="depl_upgrade7-8_repocheck-admin.png"/>
       </imageobject>
      </mediaobject>
     </informalfigure>
    </step>

    <step>
      <para>
        Click <guimenu>Upgrade Administration Server</guimenu> to upgrade and
        reboot the admin node. Note that this operation may take a while. When
        the &admserv; has been updated, click <guimenu>Next</guimenu>.
      </para>
     <informalfigure>
      <mediaobject>
       <imageobject role="fo">
        <imagedata fileref="depl_upgrade7-8_admin.png" width="75%"/>
       </imageobject>
       <imageobject role="html">
        <imagedata fileref="depl_upgrade7-8_admin.png"/>
       </imageobject>
      </mediaobject>
     </informalfigure>
    </step>
    <step>
     <para>
      Check that the repositories required for upgrading all nodes are
      available and updated.  To do this click the <guimenu>Check</guimenu>
      button. If the check fails, add the software repositories as described in
      <xref linkend="cha.depl.repo_conf" />. Run the
      checks again, and click <guimenu>Next</guimenu>.
     </para>
     <informalfigure>
      <mediaobject>
       <imageobject role="fo">
        <imagedata fileref="depl_upgrade7-8_repocheck-nodes.png" width="75%"/>
       </imageobject>
       <imageobject role="html">
        <imagedata fileref="depl_upgrade7-8_repocheck-nodes.png"/>
       </imageobject>
      </mediaobject>
     </informalfigure>
    </step>
    <step>
     <para>
      Stop the &ostack; services. Before you proceed, be aware that no changes
      can be made to your cloud during and after stopping the services. The
      &ostack; API will not be available until the upgrade process is
      completed. When you are ready, click <guimenu>Stop
      Services</guimenu>. Wait until the services are stopped and click
      <guimenu>Next</guimenu>.
      </para>
    </step>
    <step>
      <para>
        Before upgrading the nodes, the wizard prompts you to <guimenu>Back up
        OpenStack Database</guimenu>. The &mariadb; database backup will be
        stored on the &admserv;. It can be used to restore the database in case
        something goes wrong during the upgrade. To back up the database, click
        <guimenu>Create Backup</guimenu>. When the backup operation is
        finished, click <guimenu>Next</guimenu>.
      </para>
    </step>
    <step>
      <para>
        Start the upgrade by clicking <guimenu>Upgrade Nodes</guimenu>. The
        number of nodes determines how long the upgrade process will take. When
        the upgrade is completed, press <guimenu>Finish</guimenu> to return to
        the Dashboard.
      </para>
     <informalfigure>
      <mediaobject>
       <imageobject role="fo">
        <imagedata fileref="depl_upgrade7-8_finished.png" width="75%"/>
       </imageobject>
       <imageobject role="html">
        <imagedata fileref="depl_upgrade7-8_finished.png"/>
       </imageobject>
      </mediaobject>
     </informalfigure>
    </step>
   </procedure>
   <note>
   <para>
    With this first maintenance update, only systems already using &mariadb; as
    their &ostack; database will be able to upgrade.  In a future maintenance
    update, there will be a way to migrate from &postgres; to &mariadb; so
    &postgres; users will be able to upgrade.
   </para>
   </note>
   <note>
    <title>Dealing with Errors</title>
    <para>
     If an error occurs during the upgrade process, the wizard displays a
     message with a description of the error and a possible solution. After
     fixing the error, re-run the step where the error occurred.
    </para>
   </note>
  </section>
  <section xml:id="sec.upgrade-command-line">
   <title>Upgrading from the Command Line</title>
   <para>
    The upgrade procedure on the command line is performed by using the program
    <command>crowbarctl</command>. For general help, run <command>crowbarctl
    help</command>. To get help on a certain subcommand, run
    <command>crowbarctl <replaceable>COMMAND</replaceable> help</command>.
   </para>
   <para>
    To review the process of the upgrade procedure, you may call
    <command>crowbarctl upgrade status</command> at any time. Steps may have
    three states: <literal>pending</literal>, <literal>running</literal>, and
    <literal>passed</literal>.
   </para>
   <procedure>
    <step>
     <para>
      To start the upgrade procedure from the command line, log in to the
      &admserv; as &rootuser;.
     </para>
    </step>
    <step>
     <para>
      Perform the preliminary checks to determine whether the upgrade
      requirements are met:
     </para>
     <screen>&prompt.root;crowbarctl upgrade prechecks</screen>
     <para>
      The command's result is shown in a table. Make sure the column
      <guimenu>Errors</guimenu> does not contain any entries. If there are
      errors, fix them and restart the <command>precheck</command> command
      afterwards. Do not proceed before all checks are passed.
     </para>
<screen>&prompt.root;crowbarctl upgrade prechecks
+-------------------------------+--------+----------+--------+------+
| Check ID                      | Passed | Required | Errors | Help |
+-------------------------------+--------+----------+--------+------+
| network_checks                | true   | true     |        |      |
| cloud_healthy                 | true   | true     |        |      |
| maintenance_updates_installed | true   | true     |        |      |
| compute_status                | true   | false    |        |      |
| ha_configured                 | true   | false    |        |      |
| clusters_healthy              | true   | true     |        |      |
+-------------------------------+--------+----------+--------+------+</screen>
     <para>
      Depending on the outcome of the checks, it is automatically decided
      whether the upgrade procedure will continue in non-disruptive or in
      normal mode.
     </para>
     <tip>
      <title>Forcing Normal Mode Upgrade</title>
      <para>
       The non-disruptive update will take longer than an upgrade in normal
       mode, because it performs certain tasks in parallel which are done
       sequentially during the non-disruptive upgrade. Live-migrating guests to
       other &compnode;s during the non-disruptive
       upgrade takes additional time.
      </para>
      <para>
       Therefore, if a non-disruptive upgrade is not a requirement for you, you
       may want to switch to the normal upgrade mode, even if your setup
       supports the non-disruptive method. To force the normal upgrade mode,
       run:
      </para>
      <screen>&prompt.root;crowbarctl upgrade mode normal</screen>
      <para>
       To query the current upgrade mode run:
      </para>
      <screen>&prompt.root;crowbarctl upgrade mode</screen>
      <para>
       To switch back to the non-disruptive mode run:
      </para>
      <screen>&prompt.root;crowbarctl upgrade mode non_disruptive</screen>
      <para>
       It is possible to call this command at any time during the upgrade
       process until the <literal>services</literal> step is started. After
       that point the upgrade mode can no longer be changed.
      </para>
     </tip>
    </step>
    <step>
     <para>
      Prepare the nodes by transitioning them into the <quote>upgrade</quote>
      state and stopping the chef daemon:
     </para>
     <screen>&prompt.root;crowbarctl upgrade prepare</screen>
     <para>
      Depending of the size of your &cloud; deployment, this step may take
      some time. Use the command <command>crowbarctl upgrade status</command>
      to monitor the status of the process named
      <literal>steps.prepare.status</literal>. It needs to be in state
      <literal>passed</literal> before you proceed:
     </para>
     <screen>&prompt.root;crowbarctl upgrade status
+--------------------------------+----------------+
| Status                         | Value          |
+--------------------------------+----------------+
| current_step                   | backup_crowbar |
| current_substep                |                |
| current_substep_status         |                |
| current_nodes                  |                |
| current_node_action            |                |
| remaining_nodes                |                |
| upgraded_nodes                 |                |
| crowbar_backup                 |                |
| openstack_backup               |                |
| suggested_upgrade_mode         | non_disruptive |
| selected_upgrade_mode          |                |
| compute_nodes_postponed        | false          |
| steps.prechecks.status         | passed         |
| steps.prepare.status           | passed         |
| steps.backup_crowbar.status    | pending        |
| steps.repocheck_crowbar.status | pending        |
| steps.admin.status             | pending        |
| steps.repocheck_nodes.status   | pending        |
| steps.services.status          | pending        |
| steps.backup_openstack.status  | pending        |
| steps.nodes.status             | pending        |
+--------------------------------+----------------+</screen>
    </step>
    <step>
     <para>
      Create a backup of the existing &admserv; installation. In case something
      goes wrong during the upgrade procedure of the &admserv; you can restore
      the original state from this backup with the command <command>crowbarctl
      backup restore <replaceable>NAME</replaceable></command>
     </para>
     <screen>&prompt.root;crowbarctl upgrade backup crowbar</screen>
     <para>
      To list all existing backups including the one you have just created, run
      the following command:
     </para>
     <screen>&prompt.root;crowbarctl backup list
+----------------------------+--------------------------+--------+---------+
| Name                       | Created                  | Size   | Version |
+----------------------------+--------------------------+--------+---------+
| crowbar_upgrade_1534864741 | 2018-08-21T15:19:03.138Z | 219 KB | 4.0     |
+----------------------------+--------------------------+--------+---------+</screen>
    </step>
    <step>
     <para>
      This step prepares the upgrade of the &admserv; by checking the
      availability of the update and pool repositories for &productname;
      &productnumber; and &cloudos;. Run the following command:
     </para>
     <screen>&prompt.root;crowbarctl upgrade repocheck crowbar
+---------------------+----------------------------------------+
| Status              | Value                                  |
+---------------------+----------------------------------------+
| os.available        | false                                  |
| os.repos            | SLES12-SP4-Pool                        |
|                     | SLES12-SP4-Updates                     |
| openstack.available | false                                  |
| openstack.repos     | SUSE-OpenStack-Cloud-Crowbar-9-Pool    |
|                     | SUSE-OpenStack-Cloud-Crowbar-9-Updates |
+---------------------+----------------------------------------+</screen>
     <para>
      All four required repositories are reported as missing, because they have
      not yet been added to the &crow; configuration. To add them to the
      &admserv; proceed as follows.
     </para>
     <para>
      Note that this step is for setting up the repositories for the &admserv;,
      not for the nodes in &cloud; (this will be done in a subsequent step).
     </para>
     <substeps>
      <step>
       <para>
        Start <command>yast repositories</command> and proceed with
        <guimenu>Continue</guimenu>. Replace the repositories
        <literal>SLES12-SP3-Pool</literal> and
        <literal>SLES12-SP3-Updates</literal> with the respective SP4
        repositories.
       </para>
       <para>
        If you prefer to use zypper over &yast;, you may alternatively make the
        change using <command>zypper mr</command>.
       </para>
      </step>
      <step>
       <para>
        Next, replace the <literal>SUSE-OpenStack-Cloud-Crowbar-8</literal>
        update and pool repositories with the respective &productname; 9 versions.
       </para>
      </step>
      <step>
       <para>
        Check for other (custom) repositories. All &slsa; SP3 repositories need
        to be replaced with the respective &slsa; SP4 version. In case no SP3
        version exists, disable the repository&mdash;the respective packages
        from that repository will be deleted during the upgrade.
       </para>
      </step>
     </substeps>
     <para>
      Once the repository configuration on the &admserv; has been updated, run
      the command to check the repositories again. If the configuration is
      correct, the result should look like the following:
     </para>
     <screen>&prompt.root;crowbarctl upgrade repocheck crowbar
+---------------------+----------------------------------------+
| Status              | Value                                  |
+---------------------+----------------------------------------+
| os.available        | true                                   |
| os.repos            | SLES12-SP4-Pool                        |
|                     | SLES12-SP4-Updates                     |
| openstack.available | true                                   |
| openstack.repos     | SUSE-OpenStack-Cloud-Crowbar-9-Pool    |
|                     | SUSE-OpenStack-Cloud-Crowbar-9-Updates |
+---------------------+----------------------------------------+</screen>
    </step>
    <step>
     <para>
      Now that the repositories are available, the &admserv; itself will be
      upgraded. The update will run in the background using <command>zypper
      dup</command>. Once all packages have been upgraded, the &admserv; will
      be rebooted and you will be logged out. To start the upgrade run:
     </para>
     <screen>&prompt.root;crowbarctl upgrade admin</screen>
    </step>
    <step>
     <para>
      After the &admserv; has been successfully updated, the &contrnode;s and
      &compnode;s will be upgraded. At first the availability of the
      repositories used to provide packages for the &cloud; nodes is tested.
     </para>
     <note>
       <title>Correct Metadata in the PTF Repository</title>
       <para>
         When adding new repositories to the nodes, make sure that the new PTF
         repository also contains correct metadata (even if it is empty). To do
         this, run the <command>createrepo-cloud-ptf</command> command.
       </para>
     </note>
     <para>
      Note that the configuration for these repositories differs from the one
      for the &admserv; that was already done in a previous step. In this step
      the repository locations are made available to &crow; rather than to
      libzypp on the &admserv;. To check the repository configuration run the
      following command:
     </para>
     <screen>&prompt.root;crowbarctl upgrade repocheck nodes
+---------------------------------+----------------------------------------+
| Status                          | Value                                  |
+---------------------------------+----------------------------------------+
| ha.available                    | false                                  |
| ha.repos                        | SLES12-SP4-HA-Pool                     |
|                                 | SLES12-SP4-HA-Updates                  |
| ha.errors.x86_64.missing        | SLES12-SP4-HA-Pool                     |
|                                 | SLES12-SP4-HA- Updates                 |
| os.available                    | false                                  |
| os.repos                        | SLES12-SP4-Pool                        |
|                                 | SLES12-SP4-Updates                     |
| os.errors.x86_64.missing        | SLES12-SP4-Pool                        |
|                                 | SLES12-SP4-Updates                     |
| openstack.available             | false                                  |
| openstack.repos                 | SUSE-OpenStack-Cloud-Crowbar-9-Pool    |
|                                 | SUSE-OpenStack-Cloud-Crowbar-9-Updates |
| openstack.errors.x86_64.missing | SUSE-OpenStack-Cloud-Crowbar-9-Pool    |
|                                 | SUSE-OpenStack-Cloud-Crowbar-9-Updates |
+---------------------------------+----------------------------------------+</screen>
     <para>
      To update the locations for the listed repositories, start <command>yast
      crowbar</command> and proceed as described in <xref
      linkend="sec.depl.adm_inst.crowbar.repos"/>.
     </para>
     <para>
      Once the repository configuration for &crow; has been updated, run the
      command to check the repositories again to determine, whether the current
      configuration is correct.
     </para>
     <screen>&prompt.root;crowbarctl upgrade repocheck nodes
+---------------------+----------------------------------------+
| Status              | Value                                  |
+---------------------+----------------------------------------+
| ha.available        | true                                   |
| ha.repos            | SLE12-SP4-HA-Pool                      |
|                     | SLE12-SP4-HA-Updates                   |
| os.available        | true                                   |
| os.repos            | SLES12-SP4-Pool                        |
|                     | SLES12-SP4-Updates                     |
| openstack.available | true                                   |
| openstack.repos     | SUSE-OpenStack-Cloud-Crowbar-9-Pool    |
|                     | SUSE-OpenStack-Cloud-Crowbar-9-Updates |
+---------------------+----------------------------------------+</screen>

     <important>
      <title>Shut Down Running &vmguest;s in Normal Mode</title>
      <para>
       If the upgrade is done in normal mode (prechecks compute_status and
       ha_configured have not been passed), you need to shut down all running
       &vmguest;s now.
      </para>
     </important>
     <important>
      <title>Product Media Repository Copies</title>
      <para>
       To PXE boot new nodes, an additional &cloudos; repository&mdash;a copy
       of the installation system&mdash; is required. Although not required
       during the upgrade procedure, it is recommended to set up this directory
       now. Refer to <xref linkend="sec.depl.adm_conf.repos.product"/> for
       details. If you had also copied the &productname; 8 installation media
       (optional), you may also want to provide the &productname;
       9 the same way.
      </para>
      <para>
       Once the upgrade procedure has been successfully finished, you may
       delete the previous copies of the installation media in
       <filename>/srv/tftpboot/suse-12.4/x86_64/install</filename> and
       <filename>/srv/tftpboot/suse-12.4/x86_64/repos/Cloud</filename>.
      </para>
     </important>
    </step>
    <step>
     <para>
      To ensure the status of the nodes does not change during the upgrade
      process, the majority of the &ostack; services will be stopped on the
      nodes. As a result, the &ostack; API will no longer be
      accessible. The &vmguest;s, however, will continue to run and will also
      be accessible. Run the following command:
     </para>
     <screen>&prompt.root;crowbarctl upgrade services</screen>
     <para>
      This step takes a while to finish. Monitor the process by running
      <command>crowbarctl upgrade status</command>. Do not proceed before
      <literal>steps.services.status</literal> is set to
      <literal>passed</literal>.
     </para>
    </step>
    <step>
     <para>
      The last step before upgrading the nodes is to make a backup of the
      &ostack; &postgres; database. The database dump will be stored on the
      &admserv; and can be used to restore the database in case something goes
      wrong during the upgrade.
     </para>
     <screen>&prompt.root;crowbarctl upgrade backup openstack</screen>
    </step>
    <step>
     <para>
      The final step of the upgrade procedure is upgrading the
      nodes.  To start the process, enter:
     </para>
     <screen>&prompt.root;crowbarctl upgrade nodes all</screen>
     <para>
      The upgrade process runs in the background and can be queried with
      <command>crowbarctl upgrade status</command>. Depending on the size of
      your &cloud; it may take several hours, especially when performing a
      non-disruptive update. In that case, the &compnode;s are updated
      one-by-one after &vmguest;s have been live-migrated to other nodes.
     </para>
     <para>
      Instead of upgrading all nodes you may also upgrade
      the &contrnode;s first and individual &compnode;s afterwards. Refer to
      <command>crowbarctl upgrade nodes --help</command> for details. If you
      choose this approach, you can use the <command>crowbarctl upgrade
      status</command> command to monitor the upgrade process. The output of
      this command contains the following entries:
     </para>
      <variablelist>
        <varlistentry>
          <term>
            current_node_action
          </term>
        <listitem>
          <para>
            The current action applied to the node.
          </para>
        </listitem>
        </varlistentry>
        <varlistentry>
          <term>
            current_substep
          </term>
        <listitem>
          <para>
            Shows the current substep of the node upgrade step. For example,
            for the <command>crowbarctl upgrade nodes controllers</command>,
            the <literal>current_substep</literal> entry displays the
            <literal>controller_nodes</literal> status when upgrading controllers.
          </para>
        </listitem>
        </varlistentry>
      </variablelist>
     <para>
       After the controllers have been upgraded, the
       <literal>steps.nodes.status</literal> entry in the output displays the
       <literal>running</literal> status. Check then the status of the
       <literal>current_substep_status</literal> entry. If it displays
       <literal>finished</literal>, you can move to the next step of upgrading
       the &compnode;s.
     </para>
     <para>
      <emphasis role="bold">Postponing the Upgrade</emphasis>
     </para>
     <para>
      It is possible to stop the upgrade of compute nodes and postpone their
      upgrade with the command:
     </para>
     <screen>&prompt.root;crowbarctl upgrade nodes postpone</screen>
     <para>
      After the upgrade of compute nodes is postponed, you can go to &crow;
      &wi;, check the configuration. You can also apply some changes, provided
      they do not affect the &compnode;s. During the postponed upgrade, all
      &ostack; services should be up and running. &compnode;s are still
      running old version of services.
     </para>
     <para>
      To resume the upgrade, issue the command:
     </para>
     <screen>&prompt.root;crowbarctl upgrade nodes resume</screen>
     <para>
      And finish the upgrade with either <command>crowbarctl upgrade nodes
      all</command> or upgrade nodes one node by one with <command>crowbarctl
      upgrade nodes <replaceable>NODE_NAME</replaceable></command>.
     </para>
     <para>
       When upgrading individual &compnode;s using the <command>crowbarctl
       upgrade nodes</command> <replaceable>NODE_NAME</replaceable> command, the
       <literal>current_substep_status</literal> entry changes to
       <literal>node_finished</literal> when the upgrade of a single node is
       done. After all nodes have been upgraded, the
       <literal>current_substep_status</literal> entry displays <literal>finished</literal>.
     </para>
    </step>
   </procedure>
   <note>
    <title>Dealing with Errors</title>
    <para>
     If an error occurs during the upgrade process, the output of the
     <command>crowbarctl upgrade status</command> provides a detailed
     description of the failure. In most cases, both the output and the error
     message offer enough information for fixing the issue. When the problem has
     been solved, run the previously-issued upgrade command to resume the
     upgrade process.
    </para>
   </note>
  </section>
 </section>

 <section xml:id="sec.recover-comp-node-failure">
  <title>Recovering from &compnode; Failure</title>
  <para>
   The following procedure assumes that there is at least one &compnode;
   already running. Otherwise, see
   <xref linkend="sec.bootstrap-compute-plane"/>.
  </para>
  <procedure xml:id="pro.recover.compute.node.failure">
   <title>Procedure for Recovering from &compnode; Failure</title>
   <step xml:id="st.compnode.failed.reason">
    <para>
     If the &compnode; failed, it should have been fenced. Verify that this is
     the case. Otherwise, check <filename>/var/log/pacemaker.log</filename> on
     the &dc; to determine why the &compnode; was not fenced.
     The most likely reason is a problem with STONITH devices.
    </para>
   </step>
   <step>
    <para>
     Determine the cause of the &compnode;'s failure.
    </para>
   </step>
   <step>
    <para>
     Rectify the root cause.
    </para>
   </step>
   <step>
    <para>
     Boot the &compnode; again.
    </para>
   </step>
   <step>
    <para>
     Check whether the <systemitem>crowbar_join</systemitem> script ran
     successfully on the &compnode;. If this is not the case, check the log
     files to find out the reason. Refer to
     <xref linkend="sec.deploy.logs.crownodes"/> to find the exact
     location of the log file.
    </para>
   </step>
   <step>
    <para>
     If the <systemitem>chef-client</systemitem> agent triggered by
     <systemitem>crowbar_join</systemitem> succeeded, confirm that the
     <systemitem>pacemaker_remote</systemitem> service is up and running.
    </para>
   </step>
   <step>
    <para>
     Check whether the remote node is registered and considered healthy by the
     core cluster. If this is not the case check
     <filename>/var/log/pacemaker.log</filename> on the &dc;
     to determine the cause. There should be a remote primitive running on the
     core cluster (active/passive). This primitive is responsible for
     establishing a TCP connection to the
     <systemitem>pacemaker_remote</systemitem> service on port 3121 of the
     &compnode;. Ensure that nothing is preventing this particular TCP
     connection from being established (for example, problems with NICs,
     switches, firewalls etc.). One way to do this is to run the following
     commands:
    </para>
<screen>&prompt.user;lsof -i tcp:3121
&prompt.user;tcpdump tcp port 3121
</screen>
   </step>
   <step>
    <para>
     If &pacemaker; can communicate with the remote node, it should start the
     <systemitem>nova-compute</systemitem> service on it as part of the cloned
     group <literal>cl-g-nova-compute</literal> using the NovaCompute OCF
     resource agent. This cloned group will block startup of
     <systemitem>nova-evacuate</systemitem> until at least one clone is
     started.
    </para>
    <para>
     A necessary, related but different procedure is described in
     <xref linkend="sec.bootstrap-compute-plane"/>.
    </para>
   </step>
   <step>
    <para>
     It may happen that <systemitem>NovaCompute</systemitem> has been launched
     correctly on the &compnode; by <systemitem>lrmd</systemitem>, but the
     <systemitem>openstack-nova-compute</systemitem> service is still not
     running. This usually happens when <systemitem>nova-evacuate</systemitem>
     did not run correctly.
    </para>
    <para>
     If <systemitem>nova-evacuate</systemitem> is not
     running on one of the core cluster nodes, make sure that the service is
     marked as started (<literal>target-role="Started"</literal>). If this is
     the case, then your cloud does not have any &compnode;s already running as
     assumed by this procedure.
    </para>
    <para>
     If <systemitem>nova-evacuate</systemitem> is started but it is
     failing, check the &pacemaker; logs to determine the cause.
    </para>
    <para>
     If <systemitem>nova-evacuate</systemitem> is started and
     functioning correctly, it should call &o_comp;'s
     <literal>evacuate</literal> API to release resources used by the
     &compnode; and resurrect elsewhere any VMs that died when it failed.
    </para>
   </step>
   <step>
    <para>
     If <systemitem>openstack-nova-compute</systemitem> is running, but VMs are
     not booted on the node, check that the service is not disabled or forced
     down using the <command>openstack compute service list</command>
     command. In case the service is disabled, run the <command>openstack
     compute service set â€“enable
     <replaceable>SERVICE_ID</replaceable></command> command. If the service is
     forced down, run the following commands:
    </para>
<screen>&prompt.user;fence_nova_param () {
    key="$1"
    cibadmin -Q -A "//primitive[@id='fence-nova']//nvpair[@name='$key']" | \
    sed -n '/.*value="/{s///;s/".*//;p}'
}
&prompt.user;fence_compute \
    --auth-url=`fence_nova_param auth-url` \
    --endpoint-type=`fence_nova_param endpoint-type` \
    --tenant-name=`fence_nova_param tenant-name` \
    --domain=`fence_nova_param domain` \
    --username=`fence_nova_param login` \
    --password=`fence_nova_param passwd` \
    -n <replaceable>COMPUTE_HOSTNAME</replaceable> \
    --action=on
</screen>
   </step>
  </procedure>
  <para>
   The above steps should be performed automatically after the node is
   booted. If that does not happen, try the following debugging techniques.
  </para>
  <para>
   Check the <literal>evacuate</literal> attribute for the &compnode; in the
   &pacemaker; cluster's <systemitem>attrd</systemitem> service using the
   command:
  </para>
<screen>&prompt.user;attrd_updater -p -n evacuate -N <replaceable>NODE</replaceable></screen>
  <para>
   Possible results are the following:
  </para>
  <itemizedlist>
   <listitem>
    <para>
     The attribute is not set. Refer to
     <xref linkend="st.compnode.failed.reason"/> in
     <xref linkend="pro.recover.compute.node.failure"/>.
    </para>
   </listitem>
   <listitem>
    <para>
     The attribute is set to <literal>yes</literal>. This means that the
     &compnode; was fenced, but <systemitem>nova-evacuate</systemitem> never
     initiated the recovery procedure by calling &o_comp;'s evacuate API.
    </para>
   </listitem>
   <listitem>
    <para>
     The attribute contains a time stamp, in which case the recovery procedure
     was initiated at the time indicated by the time stamp, but has not
     completed yet.
    </para>
   </listitem>
   <listitem>
    <para>
     If the attribute is set to <literal>no</literal>, the recovery procedure
     recovered successfully and the cloud is ready for the &compnode; to
     rejoin.
    </para>
   </listitem>
  </itemizedlist>
  <para>
   If the attribute is stuck with the wrong value, it can be set to
   <literal>no</literal> using the command:
  </para>
<screen>&prompt.user;attrd_updater -n evacuate -U no -N <replaceable>NODE</replaceable></screen>
  <para>
   After standard fencing has been performed, fence agent
   <systemitem>fence_compute</systemitem> should activate the secondary
   fencing device (<literal>fence-nova</literal>). It does this by setting
   the attribute to <literal>yes</literal> to mark the node as needing
   recovery. The agent also calls &o_comp;'s
   <systemitem>force_down</systemitem> API to notify it that the host is down.
   You should be able to see this in
   <filename>/var/log/nova/fence_compute.log</filename> on the node in the core
   cluster that was running the <systemitem>fence-nova</systemitem> agent at
   the time of fencing. During the recovery, <literal>fence_compute</literal>
   tells &o_comp; that the host is up and running again.
  </para>
 </section>
 <section xml:id="sec.bootstrap-compute-plane">
  <title>Bootstrapping the Compute Plane</title>
  <para>
   If the whole compute plane is down, it is not always obvious how to boot it
   up, because it can be subject to deadlock if evacuate attributes are set on
   every &compnode;. In this case, manual intervention is
   required. Specifically, the operator must manually choose one or more
   &compnode;s to bootstrap the compute plane, and then run the
   <command>attrd_updater -n evacuate -U no -N <replaceable>NODE</replaceable></command>
   command for each
   of those &compnode;s to indicate that they do not require the resurrection
   process and can have their <literal>nova-compute</literal> start up straight
   away. Once these &compnode;s are up, this breaks the deadlock allowing
   <literal>nova-evacuate</literal> to start. This way, any other nodes that
   require resurrection can be processed automatically. If no resurrection is
   desired anywhere in the cloud, then the attributes should be set to
   <literal>no</literal> for all nodes.
  </para>
  <important>
   <para>
    If &compnode;s are started too long after the
    <literal>remote-*</literal> resources are started on the control plane,
    they are liable to fencing. This should be avoided.
   </para>
  </important>
 </section>
 <section xml:id="sec.bootstrap-galera-cluster-with-missing-node">
   <!-- https://bugzilla.suse.com/show_bug.cgi?id=1132853 -->
  <title>Bootstrapping the &mariadb_cluster; with &pacemaker; when a node is missing</title>
  <para>
    &pacemaker; does not promote a node to master until it received from all
    nodes the latest <glossterm linkend="gloss.galera_sequence_number">Sequence number (seqno)</glossterm>.
    That is a problem when one node of the &mariadb_cluster; is down (eg. due
    to hardware or network problems) because the <glossterm linkend="gloss.galera_sequence_number">Sequence number</glossterm>
    can not be received from the unavailable node.
    To recover a &mariadb_cluster; manual steps are needed to
    select a bootstrap node for &mariadb_cluster; and to promote that node
    with &pacemaker;.
  </para>
  <important>
    <para>
      Selecting the correct bootstrap node (depending on the highest
      <glossterm linkend="gloss.galera_sequence_number">Sequence number (seqno)</glossterm>)
      is important. If the wrong node is selected data loss is possible.
   </para>
  </important>
  <procedure>
    <step>
      <para>
        To find out which node has the latest Sequence number, call the following
        command on all &mariadb_cluster; nodes and select the node with the
        highest Sequence number.
      </para>
      <screen language="bash">mysqld_safe --wsrep-recover
      tail -5 /var/log/mysql/mysqld.log
      ...
      [Note] WSREP: Recovered position: 7a477edc-757d-11e9-a01a-d218e7381711:2490</screen>
      <para>
        At the end of <filename>/var/log/mysql/mysqld.log</filename> the Sequence
        number is written (in this example, the sequence number is 2490).
        After all Sequence numbers are collected from all nodes, the node with
        the highest Sequence number is selected for bootstrap node.
        In this example, the node with the highest Sequence number is called
        <literal>node1</literal>.
      </para>
    </step>
    <step>
      <para>
        Temporarily mark the galera &pacemaker; resource as unmanaged:
      </para>
      <screen language="bash">
        crm resource unmanage galera
      </screen>
    </step>
    <step>
      <para>
        Mark the node as bootstrap node (call the following commands from the
        bootstrap node which is <literal>node1</literal> in this example):
      </para>
      <screen language="bash">
        crm_attribute -N node1 -l reboot --name galera-bootstrap -v true
        crm_attribute -N node1 -l reboot --name master-galera -v 100
      </screen>
    </step>
    <step>
      <para>
        Promote the bootstrap node:
      </para>
      <screen language="bash">
        crm_resource --force-promote -r galera -V
      </screen>
    </step>
    <step>
      <para>
        Redetect the current state of the galera resource:
      </para>
      <screen language="bash">
        crm resource cleanup galera
      </screen>
    </step>
    <step>
      <para>
         Return the control to &pacemaker;:
      </para>
      <screen language="bash">
        crm resource manage galera
        crm resource start galera
      </screen>
    </step>
  </procedure>
  <para>
    The &mariadb_cluster; is now running and &pacemaker; is handling the cluster.
  </para>
 </section>
 <section>
  <title>Updating &mariadb; with Galera</title>
  <para>
   When using &pacemaker;, updating &mariadb; with Galera must be done
   manually. &crow; will not install updates automatically. In particular, this
   situation applies to upgrades to &mariadb; 10.2.17 or higher from &mariadb;
   10.2.16 or earlier. See <link
   xlink:href="https://mariadb.com/kb/en/library/mariadb-10222-release-notes/">MariaDB
   10.2.22 Release Notes - Notable Changes</link>.
  </para>
  <para>
   Using the &pacemaker; GUI, update &mariadb; with the following procedure:
  </para>
  <procedure>
   <step>
    <para>
     Put the cluster into maintenance mode. Detailed information about the
     &pacemaker; GUI and its operation is available in the <link
     xlink:href="https://www.suse.com/documentation/sle_ha/singlehtml/book_sleha/book_sleha.html#cha.ha.configuration.gui">&sle;
     High Availability documentation</link>.
    </para>
   </step>
   <step>
    <para>
     Perform a rolling upgrade to &mariadb; following the instructions at <link
     xlink:href="https://mariadb.com/kb/en/library/upgrading-between-minor-versions-with-galera-cluster/">Upgrading
     Between Minor Versions with Galera Cluster</link>.
    </para>
    <para>
     The process involves the following steps:
    </para>
    <substeps>
     <step>
      <para>
       Stop &mariadb;
      </para>
     </step>
     <step>
      <para>
       Uninstall the old versions of &mariadb; and the Galera wsrep provider
      </para>
     </step>
     <step>
      <para>
       Install the new versions of &mariadb; and the Galera wsrep provider
      </para>
     </step>
     <step>
      <para>
       Change configuration options if necessary
      </para>
     </step>
     <step>
      <para>
       Start &mariadb;
      </para>
     </step>
     <step>
      <para>
       Run <command>mysql_upgrade</command> with the
       <literal>--skip-write-binlog</literal> option
      </para>
     </step>
    </substeps>
   </step>
     <step>
      <para>
       Each node must upgraded individually so that the cluster is always
       operational.
    </para>
   </step>
   <step>
    <para>
     Using the &pacemaker; GUI, take the cluster out of maintenance mode.
    </para>
   </step>
  </procedure>
  <para>
   Using the CLI, update &mariadb; with the following procedure:
  </para>
  <procedure>
   <step>
    <para>
     Mark Galera as unmanaged:
    </para>
    <screen>crm resource unmanage galera</screen>
    <para>
     Or put the whole cluster into maintenance mode:
    </para>
    <screen>crm configure property maintenance-mode=true</screen>
   </step>
   <step>
    <para>
     Pick a node other than the one currently targeted by the load balancer and
     stop &mariadb; on that node:
    </para>
    <screen>crm_resource --wait --force-demote -r galera -V</screen>
   </step>
   <step>
    <para>
     Perform updates with the following steps:
    </para>
    <substeps>
     <step>
      <para>
       Uninstall the old versions of &mariadb; and the Galera wsrep provider.
      </para>
     </step>
     <step>
      <para>
       Install the new versions of &mariadb; and the Galera wsrep
       provider. Select the appropriate instructions at <link
       xlink:href="https://mariadb.com/kb/en/library/installing-mariadb-with-zypper/">Installing
       MariaDB with zypper</link>.
      </para>
     </step>
     <step>
      <para>
       Change configuration options if necessary.
      </para>
     </step>
    </substeps>
   </step>
     <step>
      <para>
       Start &mariadb; on the node.
      </para>
      <screen>crm_resource --wait --force-promote -r galera -V</screen>
     </step>
     <step>
      <para>
       Run <command>mysql_upgrade</command> with the
       <literal>--skip-write-binlog</literal> option.
      </para>
     </step>
     <step>
      <para>
       On the other nodes, repeat the process detailed above: stop &mariadb;,
       perform updates, start &mariadb;, run <command>mysql_upgrade</command>.
      </para>
     </step>
     <step>
      <para>
       Mark Galera as managed:
      </para>
      <screen>crm resource manage galera</screen>
      <para>
       Or take the cluster out of maintenance mode.
      </para>
     </step>
  </procedure>
 </section>
</chapter>
