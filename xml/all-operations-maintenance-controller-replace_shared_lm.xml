<?xml version="1.0"?>
<!DOCTYPE section [
 <!ENTITY % entities SYSTEM "entities.ent"> %entities;
]>
<section xmlns="http://docbook.org/ns/docbook" xmlns:xi="http://www.w3.org/2001/XInclude" xmlns:xlink="http://www.w3.org/1999/xlink" xml:id="replace_shared_lm"><title>Replacing a Shared Lifecycle Manager/Controller
    Node</title><abstract><para><para>If the controller node you need to replace was also
      being used as your lifecycle manager then use these steps below. If this is not a shared
      controller then skip to the next section.</para></para>
</abstract>
    <para>If the controller node you need to replace was also being used as your lifecycle manager then
      use these steps below. If this is not a shared controller then skip to the next section.</para>

    <orderedlist>
      <listitem><para>Ensuring that you use the same version of &kw-hos; that you previously
        had loaded on your lifecycle manager, you will need to download and install the lifecycle
        management software using the instructions from the installation guide: </para>
<orderedlist>
          <listitem/>
        </orderedlist></listitem>
      <listitem><para>Then you will want to restore your data using the <citetitle>FIXME: broken external xref</citetitle>
        instructions.</para>
</listitem>
      <listitem><para> Update your cloud model (<literal>servers.yml</literal>) with the new
          <literal>mac-addr</literal>, <literal>ilo-ip</literal>, <literal>ilo-password</literal>, or
          <literal>ilo-user</literal> fields where these have changed. Do not change the
          <literal>id</literal>, <literal>ip-addr</literal>, <literal>role</literal>, or
          <literal>server-group</literal> settings. (Please follow the procedure for updating your
        cloud model in the git repo)</para>
</listitem>
      <listitem><para> Get the <emphasis role="bold">servers.yml</emphasis> file stored in git:
        </para>
<screen>cd ~/helion/my_cloud/definition/data
git checkout site</screen><para> then change,
        as necessary, the <literal>mac-addr</literal>, <literal>ilo-ip</literal>,
          <literal>ilo-password</literal>, and <literal>ilo-user</literal> fields of this existing
        controller node. Save and commit the change
        </para>
<screen>git commit -a -m "repaired node X"</screen></listitem>
      <listitem><para>Run the configuration processor as follows:
        </para>
<screen>cd ~/helion/hos/ansible
ansible-playbook -i hosts/localhost config-processor-run.yml</screen><para>
        Then run ready-deployment:
        </para>
<screen>cd ~/helion/hos/ansible
ansible-playbook -i hosts/localhost ready-deployment.yml</screen></listitem>
      <listitem><para>Once that is complete, copy the Cobbler images to the correct location:
        </para>
<screen>mkdir /srv/www/cobbler/ks_mirror/hlinux-cattleprod
cp /opt/hlm_packager/hos-3.0.0/hlinux_venv/hlm/hlinux/dists/cattleprod/main/installer-amd64/current/images/netboot/debian-installer/amd64/linux /srv/www/cobbler/ks_mirror/hlinux-cattleprod
cp /opt/hlm_packager/hos-3.0.0/hlinux_venv/hlm/hlinux/dists/cattleprod/main/installer-amd64/current/images/netboot/debian-installer/amd64/initrd.gz /srv/www/cobbler/ks_mirror/hlinux-cattleprod</screen></listitem>
      <listitem><para>Deploy Cobbler:
          </para>
<screen>cd ~/helion/hos/ansible
ansible-playbook -i hosts/localhost cobbler-deploy.yml</screen></listitem>
      <listitem><para>Delete the haproxy user: </para>
<screen>deluser haproxy</screen></listitem>
      <listitem><para>Install the software on your new lifecycle manager/controller node with these three
        playbooks:
        </para>
<screen>cd ~/scratch/ansible/next/hos/ansible
ansible-playbook -i hosts/verb_hosts monasca-rebuild-pretasks.yml
ansible-playbook -i hosts/verb_hosts osconfig-run.yml -e rebuild=True --limit=&lt;controller-hostname&gt;
ansible-playbook -i hosts/verb_hosts hlm-deploy.yml -e rebuild=True --limit=&lt;controller-hostname&gt;,&lt;first-proxy-hostname&gt;</screen></listitem>
      <listitem><para>During the replacement of the node there will be alarms that show up during the process.
        If those do not clear after the node is back up and healthy, restart the threshold engine by
        running the following
        playbooks:</para>
<screen>cd ~/scratch/ansible/next/hos/ansible
ansible-playbook -i hosts/verb_hosts monasca-stop.yaml --tags thresh
ansible-playbook -i hosts/verb_hosts monasca-start.yaml --tags thresh</screen></listitem>
    </orderedlist>
  </section>
