<?xml version="1.0"?>
<!DOCTYPE section [
 <!ENTITY % entities SYSTEM "entities.ent"> %entities;
]>
<section xml:id="sec.install_kvm_xpointer"
 xmlns="http://docbook.org/ns/docbook" version="5.1"
 xmlns:xi="http://www.w3.org/2001/XInclude"
 xmlns:xlink="http://www.w3.org/1999/xlink">
 <title>XPointer File for SOC/HOS KVM Installation</title>
<!--
 IMPORTANT: Working With This File

 If you MAKE STRUCTURAL CHANGES to this file, do not forget to
 UPDATE ALL FILES THAT EMBED SECTIONS from this file.

 Add new sections only at the end.
 To find all files embedding sections of this file, do:
 $ grep -m1 "href=[\"']installation-kvm_xpointer.xml[\"']" xml/*
 (-m1 means you only get the line with first XInclude within each file.)
 -->
<!-- To include this section: <xi:include xpointer="element(/1/2)" href="installation-kvm_xpointer.xml"/> -->
 <section>
  <title>Important Notes</title>
  <itemizedlist>
   <listitem>
    <para>
     If you are looking for information about when to use the GUI installer and
     when to use the CLI, see the <xref linkend="install_overview"/>.
    </para>
   </listitem>
   <listitem>
    <para>
     Review the <xref linkend="min_hardware"/> that we have listed.
    </para>
   </listitem>
   <listitem>
    <para>
     Review the release notes to make yourself aware of any known issues and
     limitations.
    </para>
   </listitem>
   <listitem>
    <para>
     The installation process can occur in different phases. For example, you
     can install the control plane only and then add Compute nodes afterwards
     if you would like.
    </para>
   </listitem>
<!-- FIXME: Comment from DITA original: "consider including steps to
   achieve this" -->
   <listitem>
    <para>
     If you run into issues during installation, we have put together a list of
     <xref linkend="troubleshooting_installation"/> you can reference.
    </para>
   </listitem>
   <listitem>
    <para>
     Make sure all disks on the system(s) are wiped before you begin the
     install. (For Swift, refer to <xref linkend="topic_d1s_hht_tt"/>.)
    </para>
   </listitem>
   <listitem>
    <para>
     There is no requirement to have a dedicated network for OS-install and
     system deployment, this can be shared with the management network. More
     information can be found in <xref linkend="example_configurations"/>.
    </para>
   </listitem>
   <listitem>
    <para>
     You may see the terms deployer and &lcm; used interchangeably. These are
     referring to the same nodes in your environment.
    </para>
   </listitem>
   <listitem>
    <para>
     When running the Ansible playbook in this installation guide, if a runbook
     fails you will see in the error response to use the
     <literal>--limit</literal> switch when retrying a playbook. This should be
     avoided. You can simply re-run any playbook without this switch.
    </para>
   </listitem>
   <listitem>
    <para>
     DVR is not supported with ESX compute.
    </para>
   </listitem>
   <listitem>
    <para>
     When you attach a Cinder volume to the VM running on the ESXi host, the
     volume will not get detected automatically. Make sure to set the image
     metadata <emphasis role="bold">vmware_adaptertype=lsiLogicsas</emphasis>
     for image before launching the instance. This will help to discover the
     volume change appropriately.
    </para>
   </listitem>
  </itemizedlist>
 </section>
<!-- To include this section: <xi:include xpointer="element(/1/3)" href="installation-kvm_xpointer.xml"/> -->
 <section>
  <title>Before You Start</title>
  <procedure>
   <step>
    <para>
     Review the <xref linkend="preinstall_checklist"/> about recommended
     pre-installation tasks.
    </para>
   </step>
   <step>
    <para>
     Prepare your servers. The &lcm; must be accessible either directly or via
     <literal>ssh</literal>, and have &sles; 12 SP3 installed. All nodes must
     be accessible to the &lcm;.
    </para>
    <substeps>
     <step>
      <para>
       Add the repository as <literal>cloud-install-repo</literal>:
      </para>
<screen vendor="suse">
sudo zypper ar -G "iso:/?iso=SUSE-OPENSTACK-CLOUD-8-x86_64-GM-DVD1.iso" \
  cloud-install-repo</screen>
<screen vendor="hpe">
sudo zypper ar -G "iso:/?iso=HPE-HELION-OPENSTACK-CLOUD-8-x86_64-GM-DVD1.iso" \
  cloud-install-repo</screen>
     </step>
     <step>
      <para>
       Refresh your repositories:
      </para>
<screen>sudo zypper ref</screen>
     </step>
     <step>
      <para>
       Install &productname; patterns:
      </para>
<screen>sudo zypper -n in patterns-cloud-ardana</screen>
     </step>
     <step>
      <para>
       Set the password for the user
       <systemitem class="username">ardana</systemitem>:
      </para>
<screen>sudo passwd ardana</screen>
     </step>
     <step>
      <para>
       Become the user <systemitem class="username">ardana</systemitem>:
      </para>
<screen>su - ardana</screen>
     </step>
     <step>
      <para>
       Place a copy of the &sles; 12 SP3 <filename>.iso</filename> in the
       <systemitem class="username">ardana</systemitem> home directory,
       <filename>var/lib/ardana</filename>, and rename it to
       <filename>sles12sp3.iso</filename>.
      </para>
     </step>
     <step>
      <important>
       <para>
        The &install_ui; must run continuously without stopping for
        authentication at any step. When using the &install_ui; it is required
        to launch the &lcm; with the following command:
       </para>
<screen>ARDANA_INIT_AUTO=1 /usr/bin/ardana-init</screen>
      </important>
      <para>
       If you are not using the &install_ui;, running the <command>ARDANA_INIT_AUTO=1</command>
       command is optional. You can also run <command>ardana-init</command>to launch the &lcm;.
       You will be prompted to enter an optional SSH passphrase, which is used
       to protect the key used by Ansible when connecting to its client nodes.
        If you do not want to use a passphrase, press <keycap>enter</keycap> at the prompt.
      </para>
      <para>
       If you have protected the SSH key with a passphrase, you can avoid
       having to enter the passphrase on every attempt by Ansible to connect to
       its client nodes with the following commands:
      </para>
<screen>
eval $(ssh-agent)
ssh-add ~/.ssh/id_rsa
</screen>
     </step>
    </substeps>
   </step>
  </procedure>
  <para>
   Since &cloud; is an extension to &sls;, installing it during the &sls;
   installation is the easiest and recommended way to set up the &admserv;. If
   you choose this approach, you can skip substeps <literal>a</literal> through
   <literal>c</literal>. To get access to the extension selection dialog, you
   need to register &cloudos; during the installation as described in
   <link
   xlink:href="&suse-onlinedoc;/suse-openstack-cloud-7/book_cloud_deploy/data/sec_depl_adm_inst_online_update.html">Registration
   and Online Updates</link>.
  </para>
  <para>
   After a successful registration, the &cloudos; installation continues with
   the <guimenu>Extension &amp; Module Selection</guimenu>. Choose
   <guimenu>&productname; &productnumber;</guimenu> and provide the
   registration key you obtained by purchasing &productname;. The registration
   and the extension installation require an Internet connection or a local
   &smt;.
  </para>
  <para>
   Alternatively, install the &productname; after the &cloudos; installation
   via <menuchoice><guimenu>&yast;</guimenu> <guimenu>Software</guimenu>
   <guimenu>Add-On Products</guimenu></menuchoice>. For details, refer to the
   section
   <link
   xlink:href="https://www.suse.com/documentation/sles-12/book_sle_deployment/data/sec_add-ons_extensions.html">Installing
   Modules and Extensions from Online Channels</link> of the &cloudos;
   <citetitle>Deployment Guide</citetitle>.
  </para>
 </section>
<!-- To include this section: <xi:include xpointer="element(/1/4)" href="installation-kvm_xpointer.xml"/> -->
 <section>
  <title>Setting Up the &lcm;</title>
  <section>
   <title>Installing the &lcm;</title>
   <para>
    The &lcm; will contain the installation scripts and configuration files to
    deploy your cloud. You can set up the &lcm; on a dedicated node or you do
    so on your first controller node. The default choice is to use the first
    controller node as the &lcm;.
   </para>
   <procedure>
    <step>
     <para>
      Sign in and download the product and signature files at the link below:
     </para>
     <substeps>
      <step>
       <para>
        <link xlink:href="http://www.hpe.com/software/entitlements">Software
        Entitlement Portal</link>
       </para>
      </step>
     </substeps>
    </step>
    <step>
     <para>
      You can verify the download was complete via the signature verification
      process outlined
      <link xlink:href="https://h20392.www2.hpe.com/portal/swdepot/displayProductInfo.do?productNumber=HPLinuxCodeSigning"/>.
     </para>
    </step>
    <step>
     <para>
      Boot your &lcm; from the ISO contained in the download.
     </para>
    </step>
    <step>
     <para>
      Enter <literal>install</literal> (all lower-case, exactly as spelled out
      here) to start installation.
     </para>
    </step>
    <step>
     <para>
      Select the language. Note that only the English language selection is
      currently supported.
     </para>
    </step>
    <step>
     <para>
      Select the location.
     </para>
    </step>
    <step>
     <para>
      Select the keyboard layout.
     </para>
    </step>
    <step>
     <para>
      Select the primary network interface, if prompted:
     </para>
     <substeps>
      <step>
       <para>
        Assign IP address, subnet mask, and default gateway
       </para>
      </step>
     </substeps>
    </step>
    <step>
     <para>
      Create new account:
     </para>
     <substeps>
      <step>
       <para>
        Enter a username.
       </para>
      </step>
      <step>
       <para>
        Enter a password.
       </para>
      </step>
      <step>
       <para>
        Enter time zone.
       </para>
      </step>
     </substeps>
    </step>
   </procedure>
   <para>
    Once the initial installation is finished, complete the &lcm; setup with
    these steps:
   </para>
   <procedure>
    <step>
     <para>
      Ensure your &lcm; has a valid DNS nameserver specified in
      <literal>/etc/resolv.conf</literal>.
     </para>
    </step>
    <step>
     <para>
      Set the environment variable LC_ALL:
     </para>
<screen>export LC_ALL=C</screen>
     <note>
      <para>
       This can be added to <filename>~/.bashrc</filename> or
       <filename>/etc/bash.bashrc</filename>.
      </para>
     </note>
    </step>
   </procedure>
   <para>
<!-- FIXME: This procedure was written for hLinux and needs to be fixed to
   work for SLES - sknorr, 2018-02-03 -->
    At the end of this section you should have a node set up with &slsa; on it.
   </para>
  </section>
 </section>
<!-- To include this section: <xi:include xpointer="element(/1/5)" href="installation-kvm_xpointer.xml"/> -->
 <section>
  <title>Configuring Your Environment</title>
  <para>
   During the configuration phase of the installation you will be making
   modifications to the example configuration input files to match your cloud
   environment. You should use the <xref linkend="example_configurations"/>
   documentation for detailed information on how to do this. There is also a
   <filename>README.md</filename> file included in each of the example
   directories on the &lcm; that has useful information about the models.
  </para>
  <para>
   In the steps below we show how to set up the directory structure with the
   example input files as well as use the optional encryption methods for your
   sensitive data.
  </para>
  <procedure>
   <step>
    <para>
     Setup your configuration files, as follows:
    </para>
    <substeps>
     <step>
      <para>
       Copy the example configuration files into the required setup directory
       and edit them to contain the details of your environment.
      </para>
      <para>
       For example, if you want to use the &productname; Mid-scale KVM model,
       you can use this command to copy the files to your cloud definition
       directory:
      </para>
<screen>cp -r ~/openstack/examples/mid-scale-kvm/*
~/openstack/my_cloud/definition/</screen>
      <para>
       If you want to use the &productname; Entry-scale KVM model, you can use
       this command to copy the files to your cloud definition directory:
      </para>
<screen>cp -r ~/openstack/examples/entry-scale-kvm/*
~/openstack/my_cloud/definition/</screen>
     </step>
     <step>
      <para>
       Begin inputting your environment information into the configuration
       files in the <filename>~/openstack/my_cloud/definition</filename>
       directory.
      </para>
     </step>
    </substeps>
   </step>
<!-- To include this listitem: <xi:include xpointer="element(/1/5/4/2)" href="installation-kvm_xpointer.xml"/> -->
   <step>
<!-- xml:id="ardanaencrypt" -->
    <para>
     [OPTIONAL] - You can use the <literal>ardanaencrypt.py</literal> script to
     encrypt your iLo passwords. This script uses OpenSSL.
    </para>
    <substeps>
     <step>
      <para>
       Change to the Ansible directory:
      </para>
<screen>cd ~/openstack/ardana/ansible</screen>
     </step>
     <step>
      <para>
       Put the encryption key into the following environment variable:
      </para>
<screen>export ARDANA_USER_PASSWORD_ENCRYPT_KEY=&lt;encryption key&gt;</screen>
     </step>
     <step>
      <para>
       Run the python script below and follow the instructions. Enter a
       password that you want to encrypt.
      </para>
<screen>./ardanaencrypt.py</screen>
     </step>
     <step>
      <para>
       Take the string generated and place it in the
       <literal>ilo-password</literal> field in your
       <filename>~/openstack/my_cloud/definition/data/servers.yml</filename>
       file, remembering to enclose it in quotes.
      </para>
     </step>
     <step>
      <para>
       Repeat the above for each server.
      </para>
      <note>
       <para>
        Before you run any playbooks, remember that you need to export the
        encryption key in the following environment variable: <literal>export
        ARDANA_USER_PASSWORD_ENCRYPT_KEY=&lt;encryption key&gt;</literal>
       </para>
      </note>
     </step>
    </substeps>
   </step>
<!-- To include this listitem: <xi:include xpointer="element(/1/5/4/3)" href="installation-kvm_xpointer.xml"/> -->
   <step>
<!-- xml:id="commit" -->
    <para>
     Commit your configuration to the local git repo
     (<xref linkend="using_git"/>), as follows:
    </para>
<screen>cd ~/openstack/ardana/ansible
git add -A
git commit -m "My config or other commit message"</screen>
    <important>
     <para>
      This step needs to be repeated any time you make changes to your
      configuration files before you move onto the following steps. See
      <xref linkend="using_git"/> for more information.
     </para>
    </important>
   </step>
  </procedure>
 </section>
<!-- To include this section: <xi:include xpointer="element(/1/6)" href="installation-kvm_xpointer.xml"/> -->
 <section>
  <title>Provisioning Your Baremetal Nodes</title>
  <para>
   To provision the baremetal nodes in your cloud deployment you can either use
   the automated operating system installation process provided by &kw-hos; or
   you can use the 3rd party installation tooling of your choice. We will
   outline both methods below:
  </para>
  <section>
   <title>Using 3rd Party Baremetal Installers</title>
   <para>
    If you do not wish to use the automated operating system installation
    tooling included with &kw-hos-version; then the requirements that have to
    be met using the installation tooling of your choice are:
   </para>
   <itemizedlist>
    <listitem>
     <para>
      The operating system must be installed via the &hlinux; ISO provided on
      the <link xlink:href="http://www.hpe.com/software/entitlements">Software
      Entitlement Portal</link>.
     </para>
    </listitem>
    <listitem>
     <para>
      Each node must have SSH keys in place that allows the same user from the
      &lcm; node who will be doing the deployment to SSH to each node without a
      password.
     </para>
    </listitem>
    <listitem>
     <para>
      Passwordless sudo needs to be enabled for the user.
     </para>
    </listitem>
    <listitem>
     <para>
      There should be a LVM logical volume as <literal>/root</literal> on each
      node.
     </para>
    </listitem>
    <listitem>
     <para>
      If the LVM volume group name for the volume group holding the "root" LVM
      logical volume is ardana-vg then it will align with the disk input models
      in the examples.
     </para>
    </listitem>
    <listitem>
     <para>
      <phrase>Ensure that <literal>openssh-server</literal>,
      <literal>python</literal>, <literal>python-apt</literal>, and
      <literal>rsync</literal> are installed.</phrase>
     </para>
    </listitem>
   </itemizedlist>
   <para>
    If you chose this method for installing your baremetal hardware, skip
    forward to the
<!-- FIXME: this seems to be the wrong reference <xref linkend="install_kvm"/> - sknorr, 2018-01-19 -->
    step.
   </para>
  </section>
  <section>
   <title>Using the Automated Operating System Installation Provided by &productname;</title>
   <para>
    If you would like to use the automated operating system installation tools
    provided by &productname;, complete the steps below.
   </para>
   <section>
    <title>Deploying Cobbler</title>
    <para>
     This phase of the install process takes the baremetal information that was
     provided in <literal>servers.yml</literal> and installs the Cobbler
     provisioning tool and loads this information into Cobbler. This sets each
     node to <literal>netboot-enabled: true</literal> in Cobbler. Each node
     will be automatically marked as <literal>netboot-enabled: false</literal>
     when it completes its operating system install successfully. Even if the
     node tries to PXE boot subsequently, Cobbler will not serve it. This is
     deliberate so that you can't reimage a live node by accident.
    </para>
    <para>
     The <literal>cobbler-deploy.yml</literal> playbook prompts for a password
     - this is the password that will be encrypted and stored in Cobbler, which
     is associated with the user running the command on the &lcm;, that you
     will use to log in to the nodes via their consoles after install. The
     username is the same as the user set up in the initial dialogue when
     installing the &lcm; from the ISO, and is the same user that is running
     the cobbler-deploy play.
    </para>
    <procedure>
     <step>
      <para>
       Run the following playbook which confirms that there is iLo connectivity
       for each of your nodes so that they are accessible to be re-imaged in a
       later step:
      </para>
<screen>cd ~/openstack/ardana/ansible
ansible-playbook -i hosts/localhost bm-power-status.yml</screen>
     </step>
     <step>
      <para>
       Run the following playbook to deploy Cobbler:
      </para>
<screen>cd ~/openstack/ardana/ansible
ansible-playbook -i hosts/localhost cobbler-deploy.yml</screen>
     </step>
    </procedure>
   </section>
   <section>
    <title>Imaging the Nodes</title>
    <para>
     This phase of the install process goes through a number of distinct steps:
    </para>
    <procedure>
     <step>
      <para>
       Powers down the nodes to be installed
      </para>
     </step>
     <step>
      <para>
       Sets the nodes hardware boot order so that the first option is a network
       boot.
      </para>
     </step>
     <step>
      <para>
       Powers on the nodes. (The nodes will then boot from the network and be
       installed using infrastructure set up in the previous phase)
      </para>
     </step>
     <step>
      <para>
       Waits for the nodes to power themselves down (this indicates a success
       install). This can take some time.
      </para>
     </step>
     <step>
      <para>
       Sets the boot order to hard disk and powers on the nodes.
      </para>
     </step>
     <step>
      <para>
       Waits for the nodes to be ssh-able and verifies that they have the
       signature expected.
      </para>
     </step>
    </procedure>
    <para>
     The reimaging command is:
    </para>
<screen>
&prompt.root;cd ~/openstack/ardana/ansible
&prompt.root;ansible-playbook -i hosts/localhost bm-reimage.yml \
  [-e nodelist=node1,node2,node3]</screen>
    <para>
     If a nodelist is not specified then the set of nodes in cobbler with
     <literal>netboot-enabled: True</literal> is selected. The playbook pauses
     at the start to give you a chance to review the set of nodes that it is
     targeting and to confirm that it is correct.
    </para>
    <para>
     You can use the command below which will list all of your nodes with the
     <literal>netboot-enabled: True</literal> flag set:
    </para>
<screen>sudo cobbler system find --netboot-enabled=1</screen>
   </section>
  </section>
 </section>
<!-- To include this section: <xi:include xpointer="element(/1/7)" href="installation-kvm_xpointer.xml"/> -->
 <section>
  <title>Running the Configuration Processor</title>
  <para>
   Once you have your configuration files setup you need to run the
   configuration processor to complete your configuration.
  </para>
  <para>
   When you run the configuration processor you will be prompted for two
   passwords. Enter the first password to make the configuration processor
   encrypt its sensitive data, which consists of the random inter-service
   passwords that it generates and the ansible <literal>group_vars</literal>
   and <literal>host_vars</literal> that it produces for subsequent deploy
   runs. You will need this password for subsequent ansible deploy and
   configuration processor runs. If you wish to change an encryption password
   that you have already used when running the configuration processor then
   enter the new password at the second prompt, otherwise just press
   <keycap function="enter"/> to bypass this.
  </para>
  <para>
   Run the configuration processor with this command:
  </para>
<screen>cd ~/openstack/ardana/ansible
ansible-playbook -i hosts/localhost config-processor-run.yml</screen>
  <para>
   For automated installation (for example CI), you can specify the required
   passwords on the ansible command line. For example, the command below will
   disable encryption by the configuration processor:
  </para>
<screen>ansible-playbook -i hosts/localhost config-processor-run.yml \
  -e encrypt="" -e rekey=""</screen>
  <para>
   If you receive an error during this step, there is probably an issue with
   one or more of your configuration files. Verify that all information in each
   of your configuration files is correct for your environment. Then commit
   those changes to Git using the instructions in the previous section before
   re-running the configuration processor again.
  </para>
  <para>
   For any troubleshooting information regarding these steps, see
   <xref linkend="sec.trouble-config_processor"/>.
  </para>
 </section>
<!-- To include this section: <xi:include xpointer="element(/1/8)" href="installation-kvm_xpointer.xml"/>-->
 <section>
  <title>Configuring TLS</title>
  <important>
   <para>
    This section is optional, but recommended, for a &kw-hos; installation.
   </para>
  </important>
  <para>
   After you run the configuration processor the first time, the IP addresses
   for your environment will be generated and populated in the
   <filename>~/openstack/my_cloud/info/address_info.yml</filename> file. At
   this point, consider whether to configure TLS and set up an SSL certificate
   for your environment. Please read <xref linkend="tls30"/> before proceeding
   for how to achieve this.
  </para>
 </section>
<!-- To include this section: <xi:include xpointer="element(/1/9)" href="installation-kvm_xpointer.xml"/> -->
 <section>
  <title>Deploying the Cloud</title>
  <procedure>
   <step>
    <para>
     Use the playbook below to create a deployment directory:
    </para>
<screen>&prompt.root;cd ~/openstack/ardana/ansible
&prompt.root;ansible-playbook -i hosts/localhost ready-deployment.yml</screen>
   </step>
   <step>
    <para>
     [OPTIONAL] - Run the <literal>wipe_disks.yml</literal> playbook to ensure
     all of your partitions on your nodes are completely wiped before
     continuing with the installation. If you are using fresh machines this
     step may not be necessary.
    </para>
<screen>cd ~/scratch/ansible/next/ardana/ansible
ansible-playbook -i hosts/verb_hosts wipe_disks.yml</screen>
    <para>
     If you have used an encryption password when running the configuration
     processor use the command below and enter the encryption password when
     prompted:
    </para>
<screen>ansible-playbook -i hosts/verb_hosts wipe_disks.yml --ask-vault-pass</screen>
   </step>
   <step>
    <para>
     Run the <literal>site.yml</literal> playbook below:
    </para>
<screen>cd ~/scratch/ansible/next/ardana/ansible
ansible-playbook -i hosts/verb_hosts site.yml</screen>
    <para>
     If you have used an encryption password when running the configuration
     processor use the command below and enter the encryption password when
     prompted:
    </para>
<screen>ansible-playbook -i hosts/verb_hosts site.yml --ask-vault-pass </screen>
    <note>
     <para>
      The step above runs <literal>osconfig</literal> to configure the cloud
      and <literal>ardana-deploy</literal> to deploy the cloud. Therefore, this
      step may run for a while, perhaps 45 minutes or more, depending on the
      number of nodes in your environment.
     </para>
    </note>
   </step>
   <step>
    <para>
     Verify that the network is working correctly. Ping each IP in the
     <literal>/etc/hosts</literal> file from one of the controller nodes.
    </para>
   </step>
  </procedure>
  <para>
   For any troubleshooting information regarding these steps, see
   <xref linkend="sec.trouble-deploy_cloud"/>.
  </para>
 </section>
<!-- To include this section: <xi:include xpointer="element(/1/10)" href="installation-kvm_xpointer.xml"/> -->
 <section>
  <title>Configuring a Block Storage Backend (Optional)</title>
  <para>
   &kw-hos; supports multiple block storage backend options. You can use one or
   more of these for setting up multiple block storage backends. Multiple
   volume types are also supported.
  </para>
  <para>
   Regardless of whether you have a single or multiple block storage backends
   defined in your <filename>cinder.conf.j2</filename> file then you can create
   one or more volume types using the specific attributes associated with the
   backend. For more information, see <xref linkend="config_3par"/>.
  </para>
 </section>
<!-- To include this section: <xi:include xpointer="element(/1/11)" href="installation-kvm_xpointer.xml"/> -->
 <section>
  <title>Post-Installation Verification and Administration</title>
  <para>
   We recommend verifying the installation using the instructions in
   <xref linkend="cloud_verification"/>.
  </para>
  <para>
   There are also a list of other common post-installation administrative tasks
   listed in the <xref linkend="postinstall_checklist"/> list.
  </para>
 </section>
</section>
