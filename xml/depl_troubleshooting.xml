<?xml version="1.0" encoding="utf-8"?>
<?xml-stylesheet
 href="urn:x-daps:xslt:profiling:novdoc-profile.xsl"
 type="text/xml"
 title="Profiling step"?>
<!DOCTYPE chapter PUBLIC "-//Novell//DTD NovDoc XML V1.0//EN" "novdocx.dtd"
[
  <!ENTITY % NOVDOC.DEACTIVATE.IDREF "IGNORE">
  <!ENTITY % entities SYSTEM "entity-decl.ent">
  %entities;
]>
<chapter id="cha.depl.trouble">
 <title>Troubleshooting and Support</title>
  <para>
   Find solutions for the most common pitfalls and technical details on how to
   create a support request for &productname; here.
  </para>
  
 <sect1 id="sec.depl.trouble.faq">
  <title>FAQ</title>

  <para>
   If your problem is not mentioned here, checking the log files on either the
   &admserv; or the &ostack; nodes may help. A list of log files is available
   at <xref linkend="cha.deploy.logs"/>.
  </para>

  <qandaset defaultlabel="qanda">
   <qandadiv id="sec.depl.trouble.faq.admin">
    <title>Admin Node Deployment</title>
    <qandaentry>
     <question>
      <para>
       What to do when <command>install-suse-cloud</command> fails?
      </para>
     </question>
     <answer>
      <para>
       Check the script's log file at
       <filename>/var/log/crowbar/install.log</filename> for error messages.
      </para>
     </answer>
    </qandaentry>
    <qandaentry>
     <question>
      <para>
<!-- bnc #782337 -->
       What to do if <command>install-suse-cloud</command> fails while
       deploying the IPMI/BMC network?
      </para>
     </question>
     <answer>
      <para>
       As of &productname; &productnumber;, it is assumed that each machine
       can be accessed directly via IPMI/BMC. However, this is not the case on
       certain blade hardware, where several nodes are accessed via a common
       adapter. Such a hardware setup causes an error on deploying the
       IPMI/BMC network. You need to disable the IPMI deployment running the
       following command:
      </para>
<screen>/opt/dell/bin/json-edit -r -a "attributes.ipmi.bmc_enable" \
-v "false" /opt/dell/chef/data_bags/crowbar/bc-template-ipmi.json</screen>
      <para>
       Re-run <command>install-suse-cloud</command> after having disabled
       the IPMI deployment.
      </para>
     </answer>
    </qandaentry>
    <qandaentry>
     <question>
      <para>
       Why am I not able to reach the &admserv; from outside the admin
       network via the bastion network?
      </para>
     </question>
     <answer>
      <para>
       If <command>route <option>-n</option></command> shows no gateway for
       the bastion network, make sure the value for the bastion network's
       <literal>"router_pref":</literal> entry in
       <filename>/etc/crowbar/network.json</filename> is set to a
       <emphasis>lower</emphasis> value than the
       <literal>"router_pref":</literal> entry for the admin network.
      </para>
      <para>
       If the router preference is set correctly, <command>route
       <option>-n</option></command> shows a gateway for the bastion
       network. In case the &admserv; is still not accessible via its admin
       network address (for example, <systemitem
       class="ipaddress">192.168.124.10</systemitem>), you need to disable
       route verification (<literal>rp_filter</literal>). Do so by running the
       following command on the &admserv;:
      </para>
      <screen>echo 0 > /proc/sys/net/ipv4/conf/all/rp_filter</screen>
      <para>
       If this setting solves the problem, make it permanent by editing
       <filename>/etc/sysctl.conf</filename> and setting the value for
       <literal>net.ipv4.conf.all.rp_filter</literal> to <literal>0</literal>.
      </para>
     </answer>
    </qandaentry>
    <qandaentry>
     <question>
      <para>
       Can I change the host name of the &admserv;?
      </para>
     </question>
     <answer>
      <para>
       No, after you have run <command>install-suse-cloud</command> you
       cannot change the host name anymore. Services like &crow;, &chef;, and
       the RabbitMQ will fail when having changed the host name.
      </para>
     </answer>
    </qandaentry>
    <qandaentry>
     <question>
      <para>
       What to do when browsing the &chef; Web UI gives a <literal>Tampered
       with cookie</literal> error?
      </para>
     </question>
     <answer>
      <para>
       You probably have an old cookie in your browser from a previous
       &chef; installation on the same IP. Remove the cookie named
       <literal>_chef_server_session_id</literal> and try again.
      </para>
     </answer>
    </qandaentry>
    <qandaentry id="q.depl.trouble.faq.admin.custom_repos">
     <question>
      <para>
       How to make custom software repositories from an external server (for
       example a remote SMT or SUSE Manager server) available for the nodes?
      </para>
     </question>
     <answer>
      <para>
       Custom repositories need to be added using the &yast; Crowbar module:
      </para>
      <procedure>
       <step>
	<para>
	 Start the &yast; Crowbar module and switch to the
	 <guimenu>Repositories</guimenu> tab: <menuchoice>
	 <guimenu>&yast;</guimenu> <guimenu>Miscellaneous</guimenu>
	 <guimenu>Crowbar</guimenu>
	 <guimenu>Repositoroes</guimenu></menuchoice>. 
	</para>
       </step>
       <step>
	<para>
	 Choose <guimenu>Add Repositories</guimenu>
	</para>
       </step>
       <step>
	<para>
	 Enter the following data:
	</para>
	<variablelist>
	 <varlistentry>
	  <term><guimenu>Name</guimenu></term>
	  <listitem>
	   <para>
	    A unique name to identify the repository.
	   </para>
	  </listitem>
	 </varlistentry>
	 <varlistentry>
	  <term><guimenu>URL</guimenu></term>
	  <listitem>
	   <para>
	    Link to the repository. The following example shows a URL to the
	    SLES11-SP3-Updates repository on an external &smt; server.
	   </para>
	   <screen>http://<replaceable>smt.&exampledomain;</replaceable>/repo/$RCE/SLES11-SP3-Updates/x86_64/</screen>
	  </listitem>
	 </varlistentry>
	 <varlistentry>
	  <term><guimenu>Ask On Error</guimenu></term>
	  <listitem>
	   <para>
	    Access errors to a repository are silently ignored by default. To
	    ensure that you get notified of these errors, set the <literal>Ask
	    On Error</literal> flag.
	   </para>
	  </listitem>
	 </varlistentry>
	 <varlistentry>
	  <term><guimenu>Target Platform</guimenu></term>
	  <listitem>
	   <para>
	    Select the &slsa; version for which the repository should be
	    added. When selecting <guimenu>Common for All</guimenu>, the
	    repository will be applied to nodes running &slsa; 11 or &slsa;
	    12. Choose this setting for all &cloud; repositories. If selecting
	    <guimenu>SLES 11 SP3</guimenu> or <guimenu>SLES 12</guimenu>, the
	    repository will only be added to nodes running the respective
	    &slsa; version. Chooses this setting for &slsa; update or pool
	    repositories.
	   </para>
	  </listitem>
	 </varlistentry>
	</variablelist>
       </step>
       <step>
	<para>
	 Save your settings selecting <guimenu>OK</guimenu>.
	</para>
       </step>
      </procedure>
     </answer>
    </qandaentry>
   </qandadiv>
   <qandadiv id="sec.depl.trouble.faq.ostack">
    <title>&ostack; Node Deployment</title>
    <qandaentry id="var.depl.trouble.faq.ostack.login">
     <question>
      <para>
       How can I log in to a node as &rootuser;?
      </para>
     </question>
     <answer>
      <para>
       By default you cannot directly log in to a node as &rootuser;,
       because the nodes were set up without a &rootuser; password. You can
       only log in via SSH from the &admserv;. You should be able to log in
       to a node with
       <command>ssh&nbsp;root@<replaceable>NAME</replaceable></command>
       where <replaceable>NAME</replaceable> is the name (alias) of the
       node.
      </para>
      <para>
       If name resolution does not work, go to the &crow; Web interface and
       open the <guimenu>Node Dashboard</guimenu>. Click the name of the
       node and look for its <guimenu>admin (eth0)</guimenu> <guimenu>IP
       Address</guimenu>. Log in to that IP address via SSH as user
       &rootuser;.
      </para>
     </answer>
    </qandaentry>
<!-- fs 2013-10-14: Needs clarification
    <qandaentry>
     <question>
      <para>
       How to change the default disk used for operating system installation?
      </para>
     </question>
     <answer>
      <para/>
     </answer>
    </qandaentry>
-->
    <qandaentry>
     <question>
      <para>
       What to do if a node refuses to boot or boots into a previous
       installation?
      </para>
     </question>
     <answer>
      <para>
       Make sure to change the boot order in the BIOS of the node, so that
       the first boot option is to boot from the network/boot using PXE.
      </para>
     </answer>
    </qandaentry>
    <qandaentry>
     <question>
      <para>
       What to do if a node hangs during hardware discovery after the very
       first PXE boot into the <quote>SLEShammer</quote> image?
      </para>
     </question>
     <answer>
      <para>
       The &rootuser; login is enabled at a very early state in discovery
       mode, so chances are high that you can log in for debugging purposes
       as described in <xref linkend="var.depl.trouble.faq.ostack.login"/>.
       If logging in as &rootuser; does not work, you need to set the
       &rootuser; password manually:
      </para>
      <orderedlist>
       <listitem>
        <para>
         Create a directory on the &admserv; named
         <filename>/updates/discovering-pre</filename>
        </para>
<screen>mkdir /updates/discovering-pre</screen>
       </listitem>
       <listitem>
        <para>
         Create a hook script <filename>setpw.hook</filename> in the
         directory created in the previous step:
        </para>
<screen>cat > /updates/discovering-pre/setpw.hook &lt;&lt;EOF
#!/bin/sh
echo "linux" | passwd --stdin root
EOF</screen>
       </listitem>
       <listitem>
        <para>
         Make the script executable:
        </para>
<screen>chmod a+x  /updates/discovering-pre/setpw.hook</screen>
       </listitem>
      </orderedlist>
     </answer>
    </qandaentry>
    <qandaentry>
     <question>
      <para>
       What to do when a deployed node fails to boot using PXE with the following
       error message: <literal>Could not find kernel image:
       ../suse-11.3/install/boot/x86_64/loader/linux</literal>?
      </para>
     </question>
     <answer>
      <para>
       The installation repository at
       <filename>/srv/tftpboot/suse-11.3/install</filename> on the &admserv;
       has not been set up correctly to contain the &sls; 11 SP3
       installation media. Review the instructions at
       <xref linkend="sec.depl.adm_conf.repos.product"/>.
      </para>
     </answer>
    </qandaentry>
    <qandaentry>
     <question>
      <para>
       Why does my deployed node hang at <literal>Unpacking
       initramfs</literal> during PXE boot?
      </para>
     </question>
     <answer>
      <para>
       The node probably does not have enough RAM. You need at least 2 GB
       RAM.
      </para>
     </answer>
    </qandaentry>
    <qandaentry>
     <question>
      <para>
       What to do if a node hangs at <literal>Executing AutoYast script:
       /var/adm/autoinstall/init.d/crowbar_join</literal> after the
       installation has been finished?
      </para>
     </question>
     <answer>
      <para>
       Be patient&mdash;the &ay; script may take a while to finish. If it
       really hangs, log in to the node as &rootuser; (see
       <xref
       linkend="var.depl.trouble.faq.ostack.login"/> for
       details). Check the log files at
       <filename>/var/log/crowbar/crowbar_join/*</filename> for errors.
      </para>
      <para>
       If the node is in a state where login in from the &admserv; is not
       possible, you need to create a &rootuser; password for it as described
       in <xref linkend=" var.depl.inst.nodes.prep.root_login"/>.  Now
       re-install the node by going to the node on the &crow; Web interface
       and clicking <guimenu>Reinstall</guimenu>. After having been
       re-installed, the node will hang again, but now you can log in and
       check the log files to find the cause.
      </para>
     </answer>
    </qandaentry>
    <qandaentry>
     <question>
      <para>
       Where to find more information when applying a &barcl; proposal
       fails?
      </para>
     </question>
     <answer>
      <para>
       Check the &chef; client log files on the &admserv; located at
       <filename>/var/log/crowbar/chef-client/d*.log</filename>. Further
       information is available from the &chef; client log files located on the
       node(s) affected by the proposal
       (<filename>/var/log/chef/client.log</filename>), and from the
       log files of the service that failed to be deployed. Additional
       information may be gained from the &crow; Web UI log files on the
       &admserv;. For a list of log file locations refer to
       <xref linkend="cha.deploy.logs"/>.
      </para>
     </answer>
    </qandaentry>
    <qandaentry>
     <question>
      <para>
       I have installed a new hard disk on a node that was already deployed.
       Why is it ignored by &crow;?
      </para>
     </question>
     <answer>
      <para>
       When adding a new hard disk to a node that has already been deployed,
       it can take up to 15 minutes before the new disk is detected.
      </para>
     </answer>
    </qandaentry>
   </qandadiv>
   <qandadiv id="sec.depl.trouble.faq.misc">
    <title>Miscellaneous</title>
    <qandaentry id="sec.depl.trouble.faq.misc.nova">
     <question>
      <para>
       How to change the <literal>nova</literal> default configuration? 
      </para>
     </question>
     <answer>
      <para>
       To change the <literal>nova</literal> default configuration, the
       respective &chef; cookbook file needs to be adjusted. This files is
       stored on the &admserv; at
       <filename>/opt/dell/chef/cookbooks/nova/templates/default/nova.erb</filename>. To
       activate changes to these files, the following command needs to be
       executed:
      </para>
      <screen>barclamp_install --rpm nova</screen>
     </answer>
    </qandaentry>
    <qandaentry id="sec.depl.trouble.faq.misc.keymap">
     <question>
      <para>
       How to change the default keymap for &vmguest;s? 
      </para>
     </question>
     <answer>
      <para>
       By default all &vmguest;s launched from &cloud; are configured to use
       the <literal>en-US</literal> keyboard. Proceed as follows to change
       this default:
      </para>
      <procedure>
       <step>
	<para>
	 Open
	 <filename>/opt/dell/chef/cookbooks/nova/templates/default/nova.conf.erb</filename>
	 on the &admserv; in an editor and search for the keyword
	 <literal>vnc_keymap</literal>. Change its value according to your
	 needs.  
	</para>
       </step>
       <step>
	<para>
	 Run the following command on the &admserv;:
	</para>
	<screen>barclamp_install --rpm nova</screen>
       </step>
      </procedure>
     </answer>
    </qandaentry>
   </qandadiv>
  </qandaset>
 </sect1>
 <sect1 id="sec.depl.trouble.support">
  <title>Support</title>

  <para>
   <remark condition="clarity">
    2012-09-07 - fs: Do we have some official text on cloud and support we
    could use here?
   </remark>
   Before contacting support to help you with a problem on &cloud;, it is
   strongly recommended that you gather as much information about your
   system and the problem as possible. For this purpose, &productname; ships
   with a tool called <command>supportconfig</command>. It gathers system
   information such as the current kernel version being used, the hardware,
   RPM database, partitions, and other items.
   <command>supportconfig</command> also collects the most important log
   files, making it easier for the supporters to identify and solve your
   problem.
  </para>

  <para>
   It is recommended to always run <command>supportconfig</command> on the
   &admserv; and on the &contrnode;(s). If a &compnode; or a &stornode;
   is part of the problem, run <command>supportconfig</command> on the
   affected node as well. For details on how to run
   <command>supportconfig</command>, see
   <ulink
   url="http://www.suse.com/documentation/sles11/book_sle_admin/data/cha_adm_support.html"/>.
  </para>
  <sect2 id="sec.depl.trouble.support.ptf">
   <title>
    Applying PTFs (Program Temporary Fixes) Provided by the &suse; L3 Support
   </title>
   <para>
    Under certain circumstances, the &suse; support may provide temporary
    fixes, the so-called PTFs, to customers with an L3 support contract. These
    PTFs are provided as RPM packages. To make them available on all nodes in
    &cloud;, proceed as follows.
   </para>
   <procedure>
    <step>
     <para>
      Download the packages from the location provided by the &suse; L3
      Support to a temporary location on the &admserv;. Packages may be
      provided for &slsa; 11 SP3 and/or for &slsa; 12. Be sure to not mix
      these packages! 
     </para>
    </step>
    <step>
     <para>
      Move the packages from the temporary download location to the following
      directories on the &admserv;:
     </para>
     <variablelist>
      <varlistentry>
       <term>
	<quote>noarch</quote> packages (<filename>*.noarch.rpm</filename>) for
	&slsa; 11 SP3:
       </term>
       <listitem>
	<para>
	 <filename>/srv/tftpboot/suse-11.3/repos/Cloud-PTF/rpm/noarch</filename>
	</para>
       </listitem>
      </varlistentry>
      <varlistentry>
       <term>
	<quote>86_64</quote> packages (<filename>*.x86_64.rpm</filename>) for
	&slsa; 11 SP3:
       </term>
       <listitem>
	<para>
	 <filename>/srv/tftpboot/suse-11.3/repos/Cloud-PTF/rpm/x86_64</filename>
	</para>
       </listitem>
      </varlistentry>
      <varlistentry>
       <term>
	<quote>noarch</quote> packages (<filename>*.noarch.rpm</filename>) for
	&slsa; 12:
       </term>
       <listitem>
	<para>
	 <filename>/srv/tftpboot/suse-12.0/repos/Cloud-PTF/rpm/noarch</filename>
	</para>
       </listitem>
      </varlistentry>
      <varlistentry>
       <term>
	<quote>86_64</quote> packages (<filename>*.x86_64.rpm</filename>) for
	&slsa; 12:
       </term>
       <listitem>
	<para>
	 <filename>/srv/tftpboot/suse-12.0/repos/Cloud-PTF/rpm/x86_64</filename>
	</para>
       </listitem>
      </varlistentry>
     </variablelist>
    </step>
    <step>
     <para>
      Create or update the repository metadata:
     </para>
     <screen>createrepo-cloud-ptf</screen>
    </step>
    <step>
     <para>
      The repositories are now set up and are available for all nodes in &cloud;
      except for the &admserv;. In case the PTF also contains packages to be
      installed on the &admserv;, you need to make the repository available on
      the &admserv; as well:
     </para>
     <screen>zypper ar -f /srv/tftpboot/suse-11.3/repos/Cloud-PTF Cloud-PTF</screen>
     
     <important>
      <title>Only use &slsa; 11 SP3 repositories on the &admserv;</title>
      <para>
       The &admserv; is installed with &slsa; 11 SP3, therefore we only need
       to add the &slsa; 11 SP3 repository in this step. Do not add the &slsa;
       12 repository here.
      </para>
     </important>
    </step>
    <step>
     <para>
      To deploy the updates, proceed as described in <xref
      linkend="sec.depl.inst.nodes.post.updater"/>. Alternatively, run
      <command>zypper up</command> manually on each node.
     </para>
    </step>
   </procedure>
  </sect2>
 </sect1>
</chapter>
