<?xml version="1.0"?>
<?xml-stylesheet href="urn:x-suse:xslt:profiling:docbook51-profile.xsl"
 type="text/xml"
 title="Profiling step"?>
<!DOCTYPE section [
 <!ENTITY % entities SYSTEM "entities.ent"> %entities;
]>
<section xmlns="http://docbook.org/ns/docbook" xmlns:xi="http://www.w3.org/2001/XInclude" xmlns:xlink="http://www.w3.org/1999/xlink" xml:id="sec.esx.remove-cluster">
 <title>Removing a Cluster from the Compute Resource Pool</title>
 <section xml:id="idg-all-esx-remove_existing_cluster_compute_resource_pool-xml-6">
  <title>Prerequisites</title>
  <para>
   Write down the Hostname and ESXi configuration IP addresses of OVSvAPP VMs
   of that ESX cluster before deleting the VMs. These IP address and Hostname
   will be used to cleanup Monasca alarm definitions.
  </para>
  <para>
   Perform the following steps:
  </para>
  <orderedlist>
   <listitem>
    <para>
     Login to vSphere client.
    </para>
   </listitem>
   <listitem>
    <para>
     Select the ovsvapp node running on each ESXi host and click
     <emphasis role="bold">Summary</emphasis> tab as shown in the following
     example.
    </para>
    <informalfigure>
    <mediaobject>
      <imageobject role="fo">
       <imagedata fileref="media-esx-esx_hostname.png" width="75%" format="PNG"/>
      </imageobject>
      <imageobject role="html">
       <imagedata fileref="media-esx-esx_hostname.png"/>
      </imageobject>
     </mediaobject>
    </informalfigure>
    <para>
     Similarly you can retrieve the compute-proxy node information.
    </para>
    <informalfigure>
     <mediaobject>
      <imageobject role="fo">
       <imagedata fileref="media-esx-esx_cluster2.png" width="75%" format="PNG"/>
      </imageobject>
      <imageobject role="html">
       <imagedata fileref="media-esx-esx_cluster2.png"/>
      </imageobject>
     </mediaobject>
    </informalfigure>
   </listitem>
  </orderedlist>
 </section>
 <section>
  <title>Removing an existing cluster from the compute resource pool</title>
  <para>
   Perform the following steps to remove an existing cluster from the compute
   resource pool.
  </para>
  <orderedlist>
   <listitem>
    <para>
     Run the following command to check for the instances launched in that
     cluster:
    </para>
<screen># nova list --host &lt;hostname&gt;
+--------------------------------------+------+--------+------------+-------------+------------------+
| ID                                   | Name | Status | Task State | Power State | Networks         |
+--------------------------------------+------+--------+------------+-------------+------------------+
| 80e54965-758b-425e-901b-9ea756576331 | VM1  | ACTIVE | -          | Running     | private=10.0.0.2 |
+--------------------------------------+------+--------+------------+-------------+------------------+</screen>
    <para>
     where:
    </para>
    <itemizedlist>
     <listitem>
      <para>
       <emphasis role="bold">hostname</emphasis>: Specifies hostname of the
       compute proxy present in that cluster.
      </para>
<!-- Comment from DITA oril: -->
<!--<b>Is of the form &lt;*esx-comp000#-mgmt> (how to get hostname??)</b>-->
     </listitem>
    </itemizedlist>
   </listitem>
   <listitem>
    <para>
     Delete all instances spawned in that cluster:
    </para>
<screen># nova delete &lt;server&gt; [&lt;server ...&gt;]</screen>
    <para>
     where:
    </para>
    <itemizedlist>
     <listitem>
      <para>
       <emphasis role="bold">server</emphasis>: Specifies the name or ID of
       server (s)
      </para>
     </listitem>
    </itemizedlist>
    <para>
     OR
    </para>
    <para>
     Migrate all instances spawned in that cluster.
    </para>
<screen># nova migrate &lt;server&gt;</screen>
   </listitem>
   <listitem>
    <para>
     Run the following playbooks for stop the Compute (Nova) and Networking
     (Neutron) services:
    </para>
<screen>ansible-playbook -i hosts/verb_hosts nova-stop --limit &lt;hostname&gt;;
ansible-playbook -i hosts/verb_hosts neutron-stop --limit &lt;hostname&gt;;</screen>
    <para>
     where:
    </para>
    <itemizedlist xml:id="ul_hll_qrh_rt">
     <listitem>
      <para>
       <emphasis role="bold">hostname</emphasis>: Specifies hostname of the
       compute proxy present in that cluster.
      </para>
     </listitem>
    </itemizedlist>
   </listitem>
  </orderedlist>
 </section>
 <section>
  <title>Cleanup Monasca Agent for OVSvAPP Service</title>
  <para>
   Perform the following procedure to cleanup Monasca agents for ovsvapp-agent
   service.
  </para>
  <orderedlist xml:id="idg-all-esx-remove_existing_cluster_compute_resource_pool-xml-14">
   <listitem>
    <para>
     If Monasca-API is installed on different node, copy the
     <literal>service.orsc</literal> from &lcm; to Monasca API
     server.
    </para>
<screen>scp service.orsc $USER@ardana-cp1-mtrmon-m1-mgmt:</screen>
   </listitem>
   <listitem>
    <para>
     SSH to Monasca API server. You must SSH to each Monasca API server for
     cleanup.
    </para>
    <para>
     For example:
    </para>
<screen>ssh ardana-cp1-mtrmon-m1-mgmt</screen>
   </listitem>
   <listitem>
    <para>
     Edit <literal>/etc/monasca/agent/conf.d/host_alive.yaml</literal> file to
     remove the reference to the OVSvAPP you removed. This requires
     <command>sudo</command> access.
    </para>
<screen>sudo vi /etc/monasca/agent/conf.d/host_alive.yaml</screen>
    <para>
     A sample of <literal>host_alive.yaml</literal>:
    </para>
<screen>- alive_test: ping
  built_by: HostAlive
  host_name: esx-cp1-esx-ovsvapp0001-mgmt
  name: esx-cp1-esx-ovsvapp0001-mgmt ping
  target_hostname: esx-cp1-esx-ovsvapp0001-mgmt </screen>
    <para>
     where <replaceable>HOST_NAME</replaceable> and
     <replaceable>TARGET_HOSTNAME</replaceable> is mentioned at the DNS name
     field at the vSphere client. (Refer to
     <xref linkend="idg-all-esx-remove_existing_cluster_compute_resource_pool-xml-6"/>).
    </para>
   </listitem>
   <listitem>
    <para>
     After removing the reference on each of the Monasca API servers, restart
     the monasca-agent on each of those servers by executing the following
     command.
    </para>
<screen>sudo service monasca-agent restart</screen>
   </listitem>
   <listitem>
    <para>
     With the OVSvAPP references removed and the monasca-agent restarted, you
     can delete the corresponding alarm to complete the cleanup process. We
     recommend using the Monasca CLI which is installed on each of your Monasca
     API servers by default. Execute the following command from the Monasca API
     server (for example: <literal>ardana-cp1-mtrmon-mX-mgmt</literal>).
    </para>
<screen>monasca alarm-list --metric-name host_alive_status --metric-dimensions hostname=&lt;ovsvapp deleted&gt;</screen>
    <para>
     For example: You can execute the following command to get the alarm ID, if
     the OVSvAPP appears as a preceding example.
    </para>
<screen>monasca alarm-list --metric-name host_alive_status --metric-dimensions hostname=MCP-VCP-cpesx-esx-ovsvapp0001-mgmt
+--------------------------------------+--------------------------------------+-----------------------+-------------------+-------------------------------------------+----------+-------+-----------------+------+--------------------------+--------------------------+--------------------------+
| id                                   | alarm_definition_id                  | alarm_definition_name | metric_name       | metric_dimensions                         | severity | state | lifecycle_state | link | state_updated_timestamp  | updated_timestamp        | created_timestamp        |
+--------------------------------------+--------------------------------------+-----------------------+-------------------+-------------------------------------------+----------+-------+-----------------+------+--------------------------+--------------------------+--------------------------+
| cfc6bfa4-2485-4319-b1e5-0107886f4270 | cca96c53-a927-4b0a-9bf3-cb21d28216f3 | Host Status           | host_alive_status | service: system                           | HIGH     | OK    | None            | None | 2016-10-27T06:33:04.256Z | 2016-10-27T06:33:04.256Z | 2016-10-23T13:41:57.258Z |
|                                      |                                      |                       |                   | cloud_name: entry-scale-kvm-esx-mml       |          |       |                 |      |                          |                          |                          |
|                                      |                                      |                       |                   | test_type: ping                           |          |       |                 |      |                          |                          |                          |
|                                      |                                      |                       |                   | hostname: ardana-cp1-esx-ovsvapp0001-mgmt |          |       |                 |      |                          |                          |                          |
|                                      |                                      |                       |                   | control_plane: control-plane-1            |          |       |                 |      |                          |                          |                          |
|                                      |                                      |                       |                   | cluster: mtrmon                           |          |       |                 |      |                          |                          |                          |
|                                      |                                      |                       |                   | observer_host: ardana-cp1-mtrmon-m1-mgmt  |          |       |                 |      |                          |                          |                          |
|                                      |                                      |                       | host_alive_status | service: system                           |          |       |                 |      |                          |                          |                          |
|                                      |                                      |                       |                   | cloud_name: entry-scale-kvm-esx-mml       |          |       |                 |      |                          |                          |                          |
|                                      |                                      |                       |                   | test_type: ping                           |          |       |                 |      |                          |                          |                          |
|                                      |                                      |                       |                   | hostname: ardana-cp1-esx-ovsvapp0001-mgmt |          |       |                 |      |                          |                          |                          |
|                                      |                                      |                       |                   | control_plane: control-plane-1            |          |       |                 |      |                          |                          |                          |
|                                      |                                      |                       |                   | cluster: mtrmon                           |          |       |                 |      |                          |                          |                          |
|                                      |                                      |                       |                   | observer_host: ardana-cp1-mtrmon-m3-mgmt  |          |       |                 |      |                          |                          |                          |
|                                      |                                      |                       | host_alive_status | service: system                           |          |       |                 |      |                          |                          |                          |
|                                      |                                      |                       |                   | cloud_name: entry-scale-kvm-esx-mml       |          |       |                 |      |                          |                          |                          |
|                                      |                                      |                       |                   | test_type: ping                           |          |       |                 |      |                          |                          |                          |
|                                      |                                      |                       |                   | hostname: ardana-cp1-esx-ovsvapp0001-mgmt |          |       |                 |      |                          |                          |                          |
|                                      |                                      |                       |                   | control_plane: control-plane-1            |          |       |                 |      |                          |                          |                          |
|                                      |                                      |                       |                   | cluster: mtrmon                           |          |       |                 |      |                          |                          |                          |
|                                      |                                      |                       |                   | observer_host: ardana-cp1-mtrmon-m2-mgmt  |          |       |                 |      |                          |                          |                          |
+--------------------------------------+--------------------------------------+-----------------------+-------------------+-------------------------------------------+----------+-------+-----------------+------+--------------------------+--------------------------+--------------------------+</screen>
   </listitem>
   <listitem>
    <para>
     Delete the Monasca alaram.
    </para>
<screen>monasca alarm-delete &lt;alarm ID&gt;</screen>
    <para>
     For example:
    </para>
<screen>monasca alarm-delete cfc6bfa4-2485-4319-b1e5-0107886f4270Successfully deleted alarm </screen>
    <para>
     After deleting the alarms and updating the monasca-agent configuration,
     those alarms will be removed from the Opsconsole UI. You can login to
     Opconsole and view the status.
    </para>
   </listitem>
  </orderedlist>
 </section>
 <section>
  <title>Removing the Compute Proxy from Monitoring</title>
  <para>
   Once you have removed the Compute proxy, the alarms against them will still
   trigger. Therefore to resolve this, you must perform the following steps.
  </para>
  <orderedlist>
   <listitem>
    <para>
     SSH to Monasca API server. You must SSH to each Monasca API server for
     cleanup.
    </para>
    <para>
     For example:
    </para>
<screen>ssh ardana-cp1-mtrmon-m1-mgmt</screen>
   </listitem>
   <listitem>
    <para>
     Edit <literal>/etc/monasca/agent/conf.d/host_alive.yaml</literal> file to
     remove the reference to the Compute proxy you removed. This requires
     <command>sudo</command> access.
    </para>
<screen>sudo vi /etc/monasca/agent/conf.d/host_alive.yaml</screen>
    <para>
     A sample of <literal>host_alive.yaml</literal> file.
    </para>
<screen>- alive_test: ping
  built_by: HostAlive
  host_name: MCP-VCP-cpesx-esx-comp0001-mgmt
  name: MCP-VCP-cpesx-esx-comp0001-mgmt ping</screen>
   </listitem>
   <listitem>
    <para>
     Once you have removed the references on each of your Monasca API servers,
     execute the following command to restart the monasca-agent on each of
     those servers.
    </para>
<screen>sudo service monasca-agent restart</screen>
   </listitem>
   <listitem>
    <para>
     With the Compute proxy references removed and the monasca-agent restarted,
     delete the corresponding alarm to complete this process. complete the
     cleanup process. We recommend using the Monasca CLI which is installed on
     each of your Monasca API servers by default.
    </para>
<screen>monasca alarm-list --metric-dimensions hostname= &lt;compute node deleted&gt;</screen>
    <para>
     For example: You can execute the following command to get the alarm ID, if
     the Compute proxy appears as a preceding example.
    </para>
<screen>monasca alarm-list --metric-dimensions hostname=ardana-cp1-comp0001-mgmt</screen>
   </listitem>
   <listitem>
    <para>
     Delete the Monasca alarm
    </para>
<screen>monasca alarm-delete &lt;alarm ID&gt;</screen>
   </listitem>
  </orderedlist>
 </section>
 <section xml:id="sec.esx.clean_monasca">
  <title>Cleaning the &o_monitor; Alarms Related to ESX Proxy and vCenter Cluster</title>
  <para>
   Perform the following procedure:
  </para>
  <procedure>
   <step xml:id="st.esx.clean_monasca.alarm">
    <para>
     Using the ESX proxy hostname, execute the following command to list all
     alarms.
    </para>
<screen>monasca alarm-list --metric-dimensions hostname=<replaceable>COMPUTE_NODE_DELETED</replaceable></screen>
    <para>
     where <replaceable>COMPUTE_NODE_DELETED</replaceable> - hostname is
     taken from the vSphere client (refer to
     <xref linkend="idg-all-esx-remove_existing_cluster_compute_resource_pool-xml-6"/>).
    </para>
    <note>
     <para>
      Ensure to make a note of all the alarm IDs that is displayed after
      executing the preceding command.
     </para>
    </note>
    <para>
     For example, the compute proxy hostname is
     <literal>MCP-VCP-cpesx-esx-comp0001-mgmt</literal>.
    </para>
<screen>monasca alarm-list --metric-dimensions hostname=MCP-VCP-cpesx-esx-comp0001-mgmt
stack@R28N6340-701-cp1-c1-m1-mgmt:~$ monasca alarm-list --metric-dimensions hostname=R28N6340-701-cp1-esx-comp0001-mgmt
+--------------------------------------+--------------------------------------+------------------------+------------------------+--------------------------------------------------+----------+-------+-----------------+------+--------------------------+--------------------------+--------------------------+
| id                                   | alarm_definition_id                  | alarm_definition_name  | metric_name            | metric_dimensions                                | severity | state | lifecycle_state | link | state_updated_timestamp  | updated_timestamp        | created_timestamp        |
+--------------------------------------+--------------------------------------+------------------------+------------------------+--------------------------------------------------+----------+-------+-----------------+------+--------------------------+--------------------------+--------------------------+
| 02342bcb-da81-40db-a262-09539523c482 | 3e302297-0a36-4f0e-a1bd-03402b937a4e | HTTP Status            | http_status            | service: compute                                 | HIGH     | OK    | None            | None | 2016-11-11T06:58:11.717Z | 2016-11-11T06:58:11.717Z | 2016-11-10T08:55:45.136Z |
|                                      |                                      |                        |                        | cloud_name: entry-scale-esx-kvm                  |          |       |                 |      |                          |                          |                          |
|                                      |                                      |                        |                        | url: https://10.244.209.9:8774                   |          |       |                 |      |                          |                          |                          |
|                                      |                                      |                        |                        | hostname: R28N6340-701-cp1-esx-comp0001-mgmt     |          |       |                 |      |                          |                          |                          |
|                                      |                                      |                        |                        | component: nova-api                              |          |       |                 |      |                          |                          |                          |
|                                      |                                      |                        |                        | control_plane: control-plane-1                   |          |       |                 |      |                          |                          |                          |
|                                      |                                      |                        |                        | cluster: esx-compute                             |          |       |                 |      |                          |                          |                          |
| 04cb36ce-0c7c-4b4c-9ebc-c4011e2f6c0a | 15c593de-fa54-4803-bd71-afab95b980a4 | Disk Usage             | disk.space_used_perc   | mount_point: /proc/sys/fs/binfmt_misc            | HIGH     | OK    | None            | None | 2016-11-10T08:52:52.886Z | 2016-11-10T08:52:52.886Z | 2016-11-10T08:51:29.197Z |
|                                      |                                      |                        |                        | service: system                                  |          |       |                 |      |                          |                          |                          |
|                                      |                                      |                        |                        | cloud_name: entry-scale-esx-kvm                  |          |       |                 |      |                          |                          |                          |
|                                      |                                      |                        |                        | hostname: R28N6340-701-cp1-esx-comp0001-mgmt     |          |       |                 |      |                          |                          |                          |
|                                      |                                      |                        |                        | control_plane: control-plane-1                   |          |       |                 |      |                          |                          |                          |
|                                      |                                      |                        |                        | cluster: esx-compute                             |          |       |                 |      |                          |                          |                          |
|                                      |                                      |                        |                        | device: systemd-1                                |          |       |                 |      |                          |                          |                          |
+--------------------------------------+--------------------------------------+------------------------+------------------------+--------------------------------------------------+----------+-------+-----------------+------+--------------------------+--------------------------+--------------------------+</screen>
   </step>
   <step>
    <para>
     Delete the alarm using the alarm IDs.
    </para>
<screen>monasca alarm-delete &lt;alarm ID&gt;</screen>
    <para>
     This step has to be performed for all alarm IDs listed from the preceding
     step (<xref linkend="st.esx.clean_monasca.alarm"/>).
    </para>
    <para>
     For Example:
    </para>
<screen>monasca alarm-delete 1cc219b1-ce4d-476b-80c2-0cafa53e1a12</screen>
   </step>
  </procedure>
 </section>
 <section>
  <title>[Optional] Force Deactivate (if there is any irreversible error during deactivation)</title>
  <para>
   Cleans-up the OVSvApp and Compute Proxy VM's and updates database.
  </para>
<screen>eon resource-deactivate --force true &lt;RESOURCE_ID&gt;</screen>
  <important>
   <para>
    If there is any error while cleaning the OVSvApp and Compute Proxy VM's,
    the exceptions are ignored and cluster are set to the imported state. If
    the cluster is not moved back to the imported state, the cluster cannot be
    activated again. In this case, you need to manually clean-up the VM's.
   </para>
   <para>
    Executing <literal>force deactivation of resource</literal> command does not clean up the
    input model. You must perform the following steps to clean up the input
    model:
   </para>
   <orderedlist>
    <listitem>
     <para>
      Login to the &lcm;.
     </para>
    </listitem>
    <listitem>
     <para>
      Remove the entries manually from
      <literal>~/openstack/my_cloud/definition/data/servers.yml</literal> and
      <literal>~/openstack/my_cloud/definition/data/passthrough.yml</literal>
      files.
     </para>
    </listitem>
    <listitem>
     <para>
      Commit your changes
     </para>
<screen>cd /home/stack/openstack/ardana/ansible
git add -A
git commit -m "commit message&gt;"</screen>
    </listitem>
    <listitem>
     <para>
      Run the configuration processor
     </para>
<screen>cd ~/openstack/ardana/ansible
ansible-playbook -i hosts/localhost config-processor-run.yml -e remove_deleted_servers="y" -e free_unused_addresses="y"</screen>
    </listitem>
    <listitem>
     <para>
      Run ready deployment
     </para>
<screen>ansible-playbook -i hosts/localhost ready-deployment.yml</screen>
    </listitem>
   </orderedlist>
  </important>
 </section>
</section>
