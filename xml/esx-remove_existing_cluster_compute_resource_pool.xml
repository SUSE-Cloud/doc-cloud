<?xml version="1.0"?>
<?xml-stylesheet href="urn:x-suse:xslt:profiling:docbook51-profile.xsl"
 type="text/xml"
 title="Profiling step"?>
<!DOCTYPE section [
 <!ENTITY % entities SYSTEM "entities.ent"> %entities;
]>
<section xmlns="http://docbook.org/ns/docbook" xmlns:xi="http://www.w3.org/2001/XInclude" xmlns:xlink="http://www.w3.org/1999/xlink" xml:id="sec.esx.remove-cluster">
 <title>Removing a Cluster from the Compute Resource Pool</title>
 <section xml:id="idg-all-esx-remove_existing_cluster_compute_resource_pool-xml-6">
  <title>Prerequisites</title>
  <para>
   Write down the Hostname and ESXi configuration IP addresses of OVSvAPP VMs
   of that ESX cluster before deleting the VMs. These IP address and Hostname
   will be used to cleanup Monasca alarm definitions.
  </para>
  <para>
   Perform the following steps:
  </para>
  <orderedlist>
   <listitem>
    <para>
     Login to vSphere client.
    </para>
   </listitem>
   <listitem>
    <para>
     Select the ovsvapp node running on each ESXi host and click
     <emphasis role="bold">Summary</emphasis> tab as shown in the following
     example.
    </para>
    <informalfigure>
    <mediaobject>
      <imageobject role="fo">
       <imagedata fileref="media-esx-esx_hostname.png" width="75%" format="PNG"/>
      </imageobject>
      <imageobject role="html">
       <imagedata fileref="media-esx-esx_hostname.png"/>
      </imageobject>
     </mediaobject>
    </informalfigure>
    <para>
     Similarly you can retrieve the compute-proxy node information.
    </para>
    <informalfigure>
     <mediaobject>
      <imageobject role="fo">
       <imagedata fileref="media-esx-esx_cluster2.png" width="75%" format="PNG"/>
      </imageobject>
      <imageobject role="html">
       <imagedata fileref="media-esx-esx_cluster2.png"/>
      </imageobject>
     </mediaobject>
    </informalfigure>
   </listitem>
  </orderedlist>
 </section>
 <section>
  <title>Removing an existing cluster from the compute resource pool</title>
  <para>
   Perform the following steps to remove an existing cluster from the compute
   resource pool.
  </para>
  <orderedlist>
   <listitem>
    <para>
     Run the following command to check for the instances launched in that
     cluster:
    </para>
<screen># nova list --host &lt;hostname&gt;
+--------------------------------------+------+--------+------------+-------------+------------------+
| ID                                   | Name | Status | Task State | Power State | Networks         |
+--------------------------------------+------+--------+------------+-------------+------------------+
| 80e54965-758b-425e-901b-9ea756576331 | VM1  | ACTIVE | -          | Running     | private=10.0.0.2 |
+--------------------------------------+------+--------+------------+-------------+------------------+</screen>
    <para>
     where:
    </para>
    <itemizedlist>
     <listitem>
      <para>
       <emphasis role="bold">hostname</emphasis>: Specifies hostname of the
       compute proxy present in that cluster.
      </para>
<!-- Comment from DITA oril: -->
<!--<b>Is of the form &lt;*esx-comp000#-mgmt> (how to get hostname??)</b>-->
     </listitem>
    </itemizedlist>
   </listitem>
   <listitem>
    <para>
     Delete all instances spawned in that cluster:
    </para>
<screen># nova delete &lt;server&gt; [&lt;server ...&gt;]</screen>
    <para>
     where:
    </para>
    <itemizedlist>
     <listitem>
      <para>
       <emphasis role="bold">server</emphasis>: Specifies the name or ID of
       server (s)
      </para>
     </listitem>
    </itemizedlist>
    <para>
     OR
    </para>
    <para>
     Migrate all instances spawned in that cluster.
    </para>
<screen># nova migrate &lt;server&gt;</screen>
   </listitem>
   <listitem>
    <para>
     Run the following playbooks for stop the Compute (Nova) and Networking
     (Neutron) services:
    </para>
<screen>ansible-playbook -i hosts/verb_hosts nova-stop --limit &lt;hostname&gt;;
ansible-playbook -i hosts/verb_hosts neutron-stop --limit &lt;hostname&gt;;</screen>
    <para>
     where:
    </para>
    <itemizedlist xml:id="ul_hll_qrh_rt">
     <listitem>
      <para>
       <emphasis role="bold">hostname</emphasis>: Specifies hostname of the
       compute proxy present in that cluster.
      </para>
     </listitem>
    </itemizedlist>
   </listitem>
  </orderedlist>
 </section>
 <section>
  <title>Cleanup Monasca Agent for OVSvAPP Service</title>
  <para>
   Perform the following procedure to cleanup Monasca agents for ovsvapp-agent
   service.
  </para>
  <orderedlist xml:id="idg-all-esx-remove_existing_cluster_compute_resource_pool-xml-14">
   <listitem>
    <para>
     If Monasca-API is installed on different node, copy the
     <literal>service.orsc</literal> from &clm; to Monasca API
     server.
    </para>
<screen>scp service.orsc $USER@ardana-cp1-mtrmon-m1-mgmt:</screen>
   </listitem>
   <listitem>
    <para>
     SSH to Monasca API server. You must SSH to each Monasca API server for
     cleanup.
    </para>
    <para>
     For example:
    </para>
<screen>ssh ardana-cp1-mtrmon-m1-mgmt</screen>
   </listitem>
   <listitem>
    <para>
     Edit <literal>/etc/monasca/agent/conf.d/host_alive.yaml</literal> file to
     remove the reference to the OVSvAPP you removed. This requires
     <command>sudo</command> access.
    </para>
<screen>sudo vi /etc/monasca/agent/conf.d/host_alive.yaml</screen>
    <para>
     A sample of <literal>host_alive.yaml</literal>:
    </para>
<screen>- alive_test: ping
  built_by: HostAlive
  host_name: esx-cp1-esx-ovsvapp0001-mgmt
  name: esx-cp1-esx-ovsvapp0001-mgmt ping
  target_hostname: esx-cp1-esx-ovsvapp0001-mgmt </screen>
    <para>
     where <replaceable>HOST_NAME</replaceable> and
     <replaceable>TARGET_HOSTNAME</replaceable> is mentioned at the DNS name
     field at the vSphere client. (Refer to
     <xref linkend="idg-all-esx-remove_existing_cluster_compute_resource_pool-xml-6"/>).
    </para>
   </listitem>
   <listitem>
    <para>
     After removing the reference on each of the Monasca API servers, restart
     the monasca-agent on each of those servers by executing the following
     command.
    </para>
    <screen>&prompt.sudo;service openstack-monasca-agent restart</screen>
   </listitem>
   <listitem>
    <para>
     With the OVSvAPP references removed and the monasca-agent restarted, you
     can delete the corresponding alarm to complete the cleanup process. We
     recommend using the Monasca CLI which is installed on each of your Monasca
     API servers by default. Execute the following command from the Monasca API
     server (for example: <literal>ardana-cp1-mtrmon-mX-mgmt</literal>).
    </para>
<screen>monasca alarm-list --metric-name host_alive_status --metric-dimensions hostname=&lt;ovsvapp deleted&gt;</screen>
    <para>
     For example: You can execute the following command to get the alarm ID, if
     the OVSvAPP appears as a preceding example.
    </para>
<screen>monasca alarm-list --metric-name host_alive_status --metric-dimensions hostname=MCP-VCP-cpesx-esx-ovsvapp0001-mgmt
+--------------------------------------+--------------------------------------+-----------------------+-------------------+-------------------------------------------+----------+-------+-----------------+------+--------------------------+--------------------------+--------------------------+
| id                                   | alarm_definition_id                  | alarm_definition_name | metric_name       | metric_dimensions                         | severity | state | lifecycle_state | link | state_updated_timestamp  | updated_timestamp        | created_timestamp        |
+--------------------------------------+--------------------------------------+-----------------------+-------------------+-------------------------------------------+----------+-------+-----------------+------+--------------------------+--------------------------+--------------------------+
| cfc6bfa4-2485-4319-b1e5-0107886f4270 | cca96c53-a927-4b0a-9bf3-cb21d28216f3 | Host Status           | host_alive_status | service: system                           | HIGH     | OK    | None            | None | 2016-10-27T06:33:04.256Z | 2016-10-27T06:33:04.256Z | 2016-10-23T13:41:57.258Z |
|                                      |                                      |                       |                   | cloud_name: entry-scale-kvm-esx-mml       |          |       |                 |      |                          |                          |                          |
|                                      |                                      |                       |                   | test_type: ping                           |          |       |                 |      |                          |                          |                          |
|                                      |                                      |                       |                   | hostname: ardana-cp1-esx-ovsvapp0001-mgmt |          |       |                 |      |                          |                          |                          |
|                                      |                                      |                       |                   | control_plane: control-plane-1            |          |       |                 |      |                          |                          |                          |
|                                      |                                      |                       |                   | cluster: mtrmon                           |          |       |                 |      |                          |                          |                          |
|                                      |                                      |                       |                   | observer_host: ardana-cp1-mtrmon-m1-mgmt  |          |       |                 |      |                          |                          |                          |
|                                      |                                      |                       | host_alive_status | service: system                           |          |       |                 |      |                          |                          |                          |
|                                      |                                      |                       |                   | cloud_name: entry-scale-kvm-esx-mml       |          |       |                 |      |                          |                          |                          |
|                                      |                                      |                       |                   | test_type: ping                           |          |       |                 |      |                          |                          |                          |
|                                      |                                      |                       |                   | hostname: ardana-cp1-esx-ovsvapp0001-mgmt |          |       |                 |      |                          |                          |                          |
|                                      |                                      |                       |                   | control_plane: control-plane-1            |          |       |                 |      |                          |                          |                          |
|                                      |                                      |                       |                   | cluster: mtrmon                           |          |       |                 |      |                          |                          |                          |
|                                      |                                      |                       |                   | observer_host: ardana-cp1-mtrmon-m3-mgmt  |          |       |                 |      |                          |                          |                          |
|                                      |                                      |                       | host_alive_status | service: system                           |          |       |                 |      |                          |                          |                          |
|                                      |                                      |                       |                   | cloud_name: entry-scale-kvm-esx-mml       |          |       |                 |      |                          |                          |                          |
|                                      |                                      |                       |                   | test_type: ping                           |          |       |                 |      |                          |                          |                          |
|                                      |                                      |                       |                   | hostname: ardana-cp1-esx-ovsvapp0001-mgmt |          |       |                 |      |                          |                          |                          |
|                                      |                                      |                       |                   | control_plane: control-plane-1            |          |       |                 |      |                          |                          |                          |
|                                      |                                      |                       |                   | cluster: mtrmon                           |          |       |                 |      |                          |                          |                          |
|                                      |                                      |                       |                   | observer_host: ardana-cp1-mtrmon-m2-mgmt  |          |       |                 |      |                          |                          |                          |
+--------------------------------------+--------------------------------------+-----------------------+-------------------+-------------------------------------------+----------+-------+-----------------+------+--------------------------+--------------------------+--------------------------+</screen>
   </listitem>
   <listitem>
    <para>
     Delete the Monasca alaram.
    </para>
<screen>monasca alarm-delete &lt;alarm ID&gt;</screen>
    <para>
     For example:
    </para>
<screen>monasca alarm-delete cfc6bfa4-2485-4319-b1e5-0107886f4270Successfully deleted alarm </screen>
    <para>
     After deleting the alarms and updating the monasca-agent configuration,
     those alarms will be removed from the &opscon; UI. You can login to
     &opscon; and view the status.
    </para>
   </listitem>
  </orderedlist>
 </section>
 <section>
  <title>Removing the Compute Proxy from Monitoring</title>
  <para>
   Once you have removed the Compute proxy, the alarms against them will still
   trigger. Therefore to resolve this, you must perform the following steps.
  </para>
  <orderedlist>
   <listitem>
    <para>
     SSH to Monasca API server. You must SSH to each Monasca API server for
     cleanup.
    </para>
    <para>
     For example:
    </para>
<screen>ssh ardana-cp1-mtrmon-m1-mgmt</screen>
   </listitem>
   <listitem>
    <para>
     Edit <literal>/etc/monasca/agent/conf.d/host_alive.yaml</literal> file to
     remove the reference to the Compute proxy you removed. This requires
     <command>sudo</command> access.
    </para>
<screen>sudo vi /etc/monasca/agent/conf.d/host_alive.yaml</screen>
    <para>
     A sample of <literal>host_alive.yaml</literal> file.
    </para>
<screen>- alive_test: ping
  built_by: HostAlive
  host_name: MCP-VCP-cpesx-esx-comp0001-mgmt
  name: MCP-VCP-cpesx-esx-comp0001-mgmt ping</screen>
   </listitem>
   <listitem>
    <para>
     Once you have removed the references on each of your Monasca API servers,
     execute the following command to restart the monasca-agent on each of
     those servers.
    </para>
<screen>&prompt.sudo;service openstack-monasca-agent restart</screen>
   </listitem>
   <listitem>
    <para>
     With the Compute proxy references removed and the monasca-agent restarted,
     delete the corresponding alarm to complete this process. complete the
     cleanup process. We recommend using the Monasca CLI which is installed on
     each of your Monasca API servers by default.
    </para>
<screen>monasca alarm-list --metric-dimensions hostname= &lt;compute node deleted&gt;</screen>
    <para>
     For example: You can execute the following command to get the alarm ID, if
     the Compute proxy appears as a preceding example.
    </para>
<screen>monasca alarm-list --metric-dimensions hostname=ardana-cp1-comp0001-mgmt</screen>
   </listitem>
   <listitem>
    <para>
     Delete the Monasca alarm
    </para>
<screen>monasca alarm-delete &lt;alarm ID&gt;</screen>
   </listitem>
  </orderedlist>
 </section>
 <section xml:id="sec.esx.clean_monasca">
  <title>Cleaning the &o_monitor; Alarms Related to ESX Proxy and vCenter Cluster</title>
  <para>
   Perform the following procedure:
  </para>
  <procedure>
   <step xml:id="st.esx.clean_monasca.alarm">
    <para>
     Using the ESX proxy hostname, execute the following command to list all
     alarms.
    </para>
<screen>monasca alarm-list --metric-dimensions hostname=<replaceable>COMPUTE_NODE_DELETED</replaceable></screen>
    <para>
     where <replaceable>COMPUTE_NODE_DELETED</replaceable> - hostname is
     taken from the vSphere client (refer to
     <xref linkend="idg-all-esx-remove_existing_cluster_compute_resource_pool-xml-6"/>).
    </para>
    <note>
     <para>
      Ensure to make a note of all the alarm IDs that is displayed after
      executing the preceding command.
     </para>
    </note>
    <para>
     For example, the compute proxy hostname is
     <literal>MCP-VCP-cpesx-esx-comp0001-mgmt</literal>.
    </para>
<screen>monasca alarm-list --metric-dimensions hostname=MCP-VCP-cpesx-esx-comp0001-mgmt
ardana@R28N6340-701-cp1-c1-m1-mgmt:~$ monasca alarm-list --metric-dimensions hostname=R28N6340-701-cp1-esx-comp0001-mgmt
+--------------------------------------+--------------------------------------+------------------------+------------------------+--------------------------------------------------+----------+-------+-----------------+------+--------------------------+--------------------------+--------------------------+
| id                                   | alarm_definition_id                  | alarm_definition_name  | metric_name            | metric_dimensions                                | severity | state | lifecycle_state | link | state_updated_timestamp  | updated_timestamp        | created_timestamp        |
+--------------------------------------+--------------------------------------+------------------------+------------------------+--------------------------------------------------+----------+-------+-----------------+------+--------------------------+--------------------------+--------------------------+
| 02342bcb-da81-40db-a262-09539523c482 | 3e302297-0a36-4f0e-a1bd-03402b937a4e | HTTP Status            | http_status            | service: compute                                 | HIGH     | OK    | None            | None | 2016-11-11T06:58:11.717Z | 2016-11-11T06:58:11.717Z | 2016-11-10T08:55:45.136Z |
|                                      |                                      |                        |                        | cloud_name: entry-scale-esx-kvm                  |          |       |                 |      |                          |                          |                          |
|                                      |                                      |                        |                        | url: https://10.244.209.9:8774                   |          |       |                 |      |                          |                          |                          |
|                                      |                                      |                        |                        | hostname: R28N6340-701-cp1-esx-comp0001-mgmt     |          |       |                 |      |                          |                          |                          |
|                                      |                                      |                        |                        | component: nova-api                              |          |       |                 |      |                          |                          |                          |
|                                      |                                      |                        |                        | control_plane: control-plane-1                   |          |       |                 |      |                          |                          |                          |
|                                      |                                      |                        |                        | cluster: esx-compute                             |          |       |                 |      |                          |                          |                          |
| 04cb36ce-0c7c-4b4c-9ebc-c4011e2f6c0a | 15c593de-fa54-4803-bd71-afab95b980a4 | Disk Usage             | disk.space_used_perc   | mount_point: /proc/sys/fs/binfmt_misc            | HIGH     | OK    | None            | None | 2016-11-10T08:52:52.886Z | 2016-11-10T08:52:52.886Z | 2016-11-10T08:51:29.197Z |
|                                      |                                      |                        |                        | service: system                                  |          |       |                 |      |                          |                          |                          |
|                                      |                                      |                        |                        | cloud_name: entry-scale-esx-kvm                  |          |       |                 |      |                          |                          |                          |
|                                      |                                      |                        |                        | hostname: R28N6340-701-cp1-esx-comp0001-mgmt     |          |       |                 |      |                          |                          |                          |
|                                      |                                      |                        |                        | control_plane: control-plane-1                   |          |       |                 |      |                          |                          |                          |
|                                      |                                      |                        |                        | cluster: esx-compute                             |          |       |                 |      |                          |                          |                          |
|                                      |                                      |                        |                        | device: systemd-1                                |          |       |                 |      |                          |                          |                          |
+--------------------------------------+--------------------------------------+------------------------+------------------------+--------------------------------------------------+----------+-------+-----------------+------+--------------------------+--------------------------+--------------------------+</screen>
   </step>
   <step>
    <para>
     Delete the alarm using the alarm IDs.
    </para>
<screen>monasca alarm-delete &lt;alarm ID&gt;</screen>
    <para>
     This step has to be performed for all alarm IDs listed from the preceding
     step (<xref linkend="st.esx.clean_monasca.alarm"/>).
    </para>
    <para>
     For Example:
    </para>
<screen>monasca alarm-delete 1cc219b1-ce4d-476b-80c2-0cafa53e1a12</screen>
   </step>
  </procedure>
 </section>
</section>
