<?xml version="1.0"?>
<?xml-stylesheet href="urn:x-suse:xslt:profiling:docbook51-profile.xsl"
 type="text/xml"
 title="Profiling step"?>

<!DOCTYPE chapter [
 <!ENTITY % entities SYSTEM "entity-decl.ent"> %entities;
 <!ENTITY % entitydecl SYSTEM "entity-decl.ent"> %entitydecl;
]>
<chapter
 xmlns="http://docbook.org/ns/docbook"
 xmlns:xi="http://www.w3.org/2001/XInclude"
 xmlns:xlink="http://www.w3.org/1999/xlink" version="5.1" xml:id="ccp-operations">

<title>Administration and Operations Guide</title>
 <info>
  <productname>&productname;</productname>
  <productnumber>10</productnumber>
 </info>
<para>
  This section has information on the administration and operation of
  Containerized SUSE Openstack Cloud.
</para>
<section xml:id="using-run.sh">
  <title>Using <literal>run.sh</literal></title>
  <para>
    The primary means for running deployment, update, and cleanup actions in
    Containerized SUSE OpenStack Cloud is <literal>run.sh</literal>, a bash script
    that acts as a convenient wrapper around Ansible playbook execution.  All
    of the commands below should be run from the root of the socok8s directory.
  </para>
  <section xml:id="deployment-actions">
    <title>Deployment Actions</title>
    <para>
      The <literal>run.sh deploy</literal> command:
    </para>
    <itemizedlist>
      <listitem>
        <para>
          Performs all necessary setup actions
        </para>
      </listitem>
      <listitem>
        <para>
          Deploys all Airship UCP components and OpenStack services
        </para>
      </listitem>
      <listitem>
        <para>
          Configures the inventory, <filename>extravars</filename> file, and
          appropriate environment variables as described in the
          <xref linkend="sec.deployment-index"/>
        </para>
      </listitem>
    </itemizedlist>
    <screen>./run.sh deploy</screen>
    <para>
      It may be desirable to redeploy only OpenStack services while
      leaving all Airship components in the UCP untouched. In these use
      cases, run:
    </para>
    <screen>./run.sh update_openstack</screen>
  </section>
  <section xml:id="cleanup-actions">
    <title>Cleanup Actions</title>
    <para>
      In addition to deployment, <literal>run.sh</literal> can be used to
      perform environment cleanup actions.
    </para>
    <para>
      To clean up the deployment and remove Containerized SUSE OpenStack
      Cloud entirely, run the following command in the root of the socok8s
      directory:
    </para>
    <screen>./run.sh remove_deployment</screen>
    <para>
      This will delete all Helm releases, all Kubernetes resources in
      the <literal>ucp</literal> and <literal>openstack</literal>
      namespaces, and all persistent volumes that were provisioned for
      use in the deployment. After this operation is complete, only the
      original Kubernetes services deployed by the SUSE CaaS Platform
      will remain.
    </para>
  </section>
  <section xml:id="tempest-testing">
    <title>Testing</title>
    <para>
      The <literal>run.sh</literal> script also has an option to deploy and run
      OpenStack Tempest tests. To begin testing, review <xref
      linkend="sec.verify-deployment"/> and then run the following command:
    </para>
    <screen>./run.sh test</screen>
    <note>
     <para>
      Please read the <xref linkend="sec.deployment-index"/> for more
      information about configuring and running OpenStack Tempest tests
      in Containerized SUSE OpenStack Cloud.
     </para>
    </note>
  </section>
</section>
<section xml:id="scaling-inout">
  <title>Scaling In/Scaling Out</title>
  <section xml:id="adding-or-removing-compute-nodes">
    <title>Adding or removing compute nodes</title>
    <para>
      To add a compute node, the node must be running SUSE CaaS Platform v3.0,
      has been accepted into the cluster and bootstrapped using the Velum
      dashboard. After the node is bootstrapped, add its host details to the
      <literal>airship-openstack-compute-workers</literal> group in your
      inventory in <filename>${WORKSPACE}/inventory/hosts.yaml</filename>. Run
      the following command from the root of the socok8s directory:
    </para>
    <screen>./run.sh add_openstack_compute</screen>
    <note>
     <para>
      Multiple new compute nodes can be added to the inventory at the
      same time.
     </para>
     <para>
       It can take a few minutes for the new host to initialize and show
       in the OpenStack hypervisor list.
     </para>
    </note>
    <para>
      To remove a compute node, run the following command from the root
      of the socok8s directory:
    </para>
    <screen>./run.sh remove_openstack_compute ${NODE_HOSTNAME}</screen>
    <note>
     <para>
      <literal>NODE_HOSTNAME</literal> must be same as host name in ansible
      inventory.
     </para>
     <para>
      Compute nodes must be removed individually. When the node has been
      successfully removed, the host details must be manually removed
      from <literal>airship-openstack-compute-workers</literal> group in the
      inventory.
     </para>
    </note>
  </section>
  <section xml:id="control-plane-horizontal-scaling">
    <title>Control plane horizontal scaling</title>
    <para>
      Containerized SUSE OpenStack Cloud provides two built-in scale profiles:
    </para>
    <itemizedlist>
      <listitem>
        <para>
          <emphasis role="strong">minimal</emphasis>, the default
          profile, deploys a single Pod for each service
        </para>
      </listitem>
      <listitem>
        <para>
          <emphasis role="strong">ha</emphasis> deploys a minimum of two
          Pods for each service. Three or more Pods are suggested for
          services that will be heavily utilized or require a quorum
        </para>
      </listitem>
    </itemizedlist>
    <para>
      Change scale profiles by adding a <literal>scale_profile</literal> key to
      <filename>${WORKSPACE}/env/extravars</filename> and specifying a profile
      value:
    </para>
    <screen>scale_profile: ha</screen>
    <para>
      The built-in profiles are defined in
      <filename>playbooks/roles/airship-deploy-ucp/files/profiles</filename>
      and can be modified to suit custom use cases. Additional profiles can be
      created and added to this directory following the file naming convention
      in that directory.
    </para>
    <para>
      We recommend using at least three controller nodes for a highly
      available control plane for both Airship and OpenStack services.
      To add new controller nodes, the nodes must:
    </para>
    <itemizedlist>
      <listitem>
        <para>
          be running SUSE CaaS Platform v3.0
        </para>
      </listitem>
      <listitem>
        <para>
          have been accepted into the cluster
        </para>
      </listitem>
      <listitem>
        <para>
          be bootstrapped using the Velum dashboard.
        </para>
      </listitem>
    </itemizedlist>
    <para>
      After the nodes are bootstrapped, add the host entries to the
      <literal>airship-ucp-workers</literal>,
      <literal>airship-openstack-control-workers</literal>,
      <literal>airship-openstack-l3-agent-workers</literal>, and
      <literal>airship-kube-system-workers</literal> groups in your Ansible
      inventory in <filename>${WORKSPACE}/inventory/hosts.yaml</filename>.
    </para>
    <para>
      To apply the changes, run the following command from the root of
      the socok8s directory:
    </para>
    <screen>./run.sh deploy</screen>
  </section>
</section>
<section xml:id="updates">
  <title>Updates</title>
  <para>
    Containerized SUSE OpenStack Cloud is delivered as an RPM package.  Generally it
    can be updated by updating the RPM package to the latest version and
    redeploying with the necessary steps in the <xref
    linkend="sec.deployment-index"/>. This is the typical update path and will
    incorporate all recent changes. It will also automatically update component
    chart and image versions.
  </para>
  <para>
    It is also possible to update services and components directly using
    the procedures below.
  </para>
  <section xml:id="updating-openstack-version">
    <title>Updating OpenStack Version</title>
    <para>
      To make a global change to the OpenStack version used by all component
      images, create a key in <filename>${WORKSPACE}/env/extravars</filename>
      called <literal>suse_openstack_image_version</literal> and set it to the
      desired value. For example, to use the <literal>stein</literal> version,
      add the following line to the <filename>extravars</filename> file:
    </para>
    <screen>suse_openstack_image_version: &quot;stein&quot;</screen>
    <para>
      It is also possible to update an individual image or subset of images to
      a different version rather than making a global change.  To do this, it
      is necessary to manually edit the <filename>versions.yaml</filename> file
      located in <filename>socok8s/site/soc/software/config/</filename>. Locate
      the images to be changed in the <literal>images</literal> section of the
      file and modify the line to include the desired version. For example, to
      use the <literal>stein</literal> version for the
      <literal>heat_api</literal> image, change the following line in
      <filename>versions.yaml</filename> from
    </para>
    <screen>heat_api: &quot;{{ suse_osh_registry_location }}/openstackhelm/heat:{{ suse_openstack_image_version }}&quot;</screen>
    <para>
      to
    </para>
    <screen>heat_api: &quot;{{ suse_osh_registry_location }}/openstackhelm/heat:stein&quot;</screen>
  </section>
  <section>
   <title>Updating OpenStack Service Configuration</title>
   <para>
    Certain use cases may require the addition or modification of OpenStack
    service configuration parameters. To update the configuration for a
    particular service, parameters can be added or modified in the
    <literal>conf</literal> section of the chart for that service. For example,
    to change the logging level of the Keystone service to
    <literal>debug</literal>, locate the <literal>conf</literal> section of the
    Keystone chart located at
    <filename>socok8s/site/soc/software/charts/osh/openstack-keystone/keystone.yaml</filename>
    and add the following lines, beginning with the <literal>logging</literal>
    key:
   </para>
   <screen>conf:
  logging:
    logger_root:
      level: DEBUG
    logger_keystone:
      level: DEBUG</screen>
   <note>
    <para>
     Information about the supported configuration parameters for each service
     can generally be found in the <link
     xlink:href="https://docs.openstack.org/rocky/configuration/index.html">OpenStack
     Configuration Guides</link> for each release. Determining the correct keys
     and values to include in the chart for each service may require examining
     the values.yaml file for OpenStack Helm. In the Keystone logging example
     above, the names and proper locations for the logging keys were determined
     by reviewing the <literal>logging</literal> section in
     <filename>/opt/openstack/openstack-helm/keystone/values.yaml</filename>,
     then copying those keys to
     <filename>socok8s/site/soc/software/charts/osh/openstack-keystone/keystone.yaml</filename>
     and providing the desired values.
    </para>
   </note>
   <para>
    When the desired parameters have been added to each chart requiring
    changes, the configuration updates can be applied by changing to the root
    of the socok8s directory and running:
   </para>
   <screen>./run.sh update_openstack</screen>
  </section>
  <section xml:id="updating-individual-images-and-helm-charts">
    <title>Updating Individual Images and Helm Charts</title>
    <para>
      The <filename>versions.yaml</filename> file can also be used for more
      advanced update configurations such as using a specific image or Helm
      chart source version.
    </para>
    <note>
     <para>
      Changing the image registry location from its default value or
      using a custom or non-default image will void any product support
      by SUSE.
     </para>
    </note>
    <para>
      To specify the use of an updated or customized image, locate the
      appropriate image name in
      <filename>socok8s/site/soc/software/config/versions.yaml</filename> and
      modify the line to include the desired image location and tag. For
      example, to use a new heat_api image, modify its entry with the new image
      location:
    </para>
    <screen>
heat_api: &quot;registry_location/image_directory/image_name:tag&quot;
</screen>
    <para>
      Similarly, the <filename>versions.yaml</filename> file can be used to
      retrieve a specific version of any Helm chart being deployed. To do so,
      it is necessary to provide a repository location, type, and a reference.
      The reference can be a branch, commit ID, or a reference in the
      repository and will default to <literal>master</literal> if not
      specified. As an example, to use a specific version of the Helm chart for
      Heat, add the following information to the <literal>osh</literal> section
      under <literal>charts</literal>:
    </para>
    <screen>heat:
location: https://git.openstack.org/openstack/openstack-helm
reference: ${REFERENCE}
subpath: heat
type: git</screen>
    <note>
     <para>
      When specifying a particular version of a Helm chart, it may be
      necessary to first create the appropriate subsection under
      <literal>charts</literal>. Airship components such as Deckhand and
      Shipyard belong under <literal>ucp</literal>, OpenStack services belong
      under <literal>osh</literal>, and infrastructure components belong under
      <literal>osh_infra</literal>.
     </para>
    </note>
  </section>
 </section>
 <section>
  <title>Reboot Compute Host</title>
  <para>
   Before rebooting compute host, shut down all Nova VMs from that compute
   host. After reboot the compute host, it is possible that the pods will come
   up out of order.  If this happens, you might see indications of the Nova VMs
   not getting an IP address.  To address this problem, run the following
   commands:
  </para>
  <screen>kubectl get pods -o wide | grep ovs-agent | grep <replaceable>COMPUTE_NAME</replaceable>
kubectl delete pod -n openstack
  <replaceable>OVS-AGENT_POD_NAME</replaceable></screen>
  <para>
   This should restart the Neutron OVS agent pod and reconfigure the vxlan
   tunnel network configuration.
  </para>
 </section>
<section xml:id="troubleshooting">
  <title>Troubleshooting</title>
  <section xml:id="viewing-shipyard-logs">
    <title>Viewing Shipyard Logs</title>
    <para>
      The deployment of OpenStack components in Containerized SUSE
      OpenStack Cloud is directed by Shipyard, the Airship platform's directed
      acyclic graph (DAG) controller, so Shipyard is one of the best
      places to begin troubleshooting deployment problems. The Shipyard
      CLI client authenticates with Keystone, so the following
      environment variables must be set before running any commands:
    </para>
    <screen>export OS_USERNAME=shipyard
export OS_PASSWORD=$(kubectl get secret -n ucp shipyard-keystone-user \
-o json | jq -r '.data.OS_PASSWORD' | base64 -d)</screen>
    <note>
     <para>
      The Shipyard user's password can be obtained from the contents of
      <literal>${WORKSPACE}/secrets/ucp_shipyard_keystone_password</literal>.
     </para>
    </note>
    <para>
      The following commands are run from the
      <filename>/opt/airship/shipyard/tools</filename> directory. If no
      Shipyard image is found when the first command is executed, it is
      downloaded automatically.
    </para>
    <para>
      To view the status of all Shipyard actions, run:
    </para>
    <screen>./shipyard.sh get actions</screen>
    <para>
      Example output:
    </para>
    <screen>Name                   Action                                   Lifecycle        Execution Time             Step Succ/Fail/Oth        Footnotes
update_software        action/01D9ZSVG70XS9ZMF4Z6QFF32A6        Complete         2019-05-03T21:33:27        13/0/1                    (1)
update_software        action/01DAB3ETP69MGN7XHVVRHNPVCR        Failed           2019-05-08T06:52:58        7/0/7                     (2)
</screen>
    <para>
      To view the status of the individual steps of a particular action,
      copy its action ID and run the following command:
    </para>
    <screen>./shipyard.sh describe action/01DAB3ETP69MGN7XHVVRHNPVCR</screen>
    <para>
      Example output:
    </para>
    <screen>Name:                  update_software
Action:                action/01DAB3ETP69MGN7XHVVRHNPVCR
Lifecycle:             Failed
Parameters:            {}
Datetime:              2019-05-08 06:52:55.366919+00:00
Dag Status:            failed
Context Marker:        18993f2c-1cfa-4d42-9320-3fbd70e75c21
User:                  shipyard

Steps                                                                Index        State            Footnotes
step/01DAB3ETP69MGN7XHVVRHNPVCR/action_xcom                          1            success
step/01DAB3ETP69MGN7XHVVRHNPVCR/dag_concurrency_check                2            success
step/01DAB3ETP69MGN7XHVVRHNPVCR/deployment_configuration             3            success
step/01DAB3ETP69MGN7XHVVRHNPVCR/validate_site_design                 4            success
step/01DAB3ETP69MGN7XHVVRHNPVCR/armada_build                         5            failed
step/01DAB3ETP69MGN7XHVVRHNPVCR/decide_airflow_upgrade               6            None
step/01DAB3ETP69MGN7XHVVRHNPVCR/armada_get_status                    7            success
step/01DAB3ETP69MGN7XHVVRHNPVCR/armada_post_apply                    8            upstream_failed
step/01DAB3ETP69MGN7XHVVRHNPVCR/skip_upgrade_airflow                 9            upstream_failed
step/01DAB3ETP69MGN7XHVVRHNPVCR/upgrade_airflow                      10           None
step/01DAB3ETP69MGN7XHVVRHNPVCR/deckhand_validate_site_design        11           success
step/01DAB3ETP69MGN7XHVVRHNPVCR/armada_validate_site_design          12           upstream_failed
step/01DAB3ETP69MGN7XHVVRHNPVCR/armada_get_releases                  13           failed
step/01DAB3ETP69MGN7XHVVRHNPVCR/create_action_tag                    14           None
</screen>
    <para>
      To view the logs from a particular step such as
      <literal>armada_build</literal>, which has failed in the above example,
      run:
    </para>
    <screen>./shipyard.sh logs step/01DAB3ETP69MGN7XHVVRHNPVCR/armada_build</screen>
  </section>
  <section xml:id="viewing-logs-from-kubernetes-pods">
    <title>Viewing Logs From Kubernetes Pods</title>
    <para>
      To view the logs from any Pod in the Running or Completed state,
      run
    </para>
    <screen>kubectl logs -n ${NAMESPACE} ${POD_NAME}</screen>
    <para>
      To view logs from a specific container within a Pod in the Running
      or Completed state, run:
    </para>
    <screen>kubectl logs -n ${NAMESPACE} ${POD_NAME} -c ${CONTAINER_NAME}</screen>
    <para>
      If logs cannot be retrieved due to the Pod entering the
      <literal>Error</literal> or <literal>CrashLoopBackoff</literal> state, it
      may be necessary to use the <literal>-p</literal> option to retrieve logs from the previous
      instance:
    </para>
    <screen>kubectl logs -n ${NAMESPACE} ${POD_NAME} -p</screen>
  </section>
  <section xml:id="recover-controller-host-node">
    <title>Recover Controller Host Node</title>
    <para>
      If deployment fails with the error of controller host not reachable (
      has entered maintenance mode )
    </para>
    <para>
      Go to maintenance mode on controller host and run following
      commands:
    </para>
    <screen>
mounted_snapshot=$(mount | grep snapshot | gawk  'match($6, /ro.*@\/.snapshots\/(.*)\/snapshot/ , arr1 ) { print arr1[1] }')

btrfs property set -ts /.snapshots/$mounted_snapshot/snapshot ro false

mount -o remount, rw /

mkdir /var/lib/neutron

btrfs property set -ts /.snapshots/$mounted_snapshot/snapshot ro true

reboot
</screen>
  </section>
  <section xml:id="recover-compute-host-node">
    <title>Recover Compute Host Node</title>
    <para>
      If deployment failed with the error of compute host not reachable (
      has entered maintenance mode )
    </para>
    <para>
      Go to maintenance mode on compute host and run following commands:
    </para>
    <screen>
mounted_snapshot=$(mount | grep snapshot | gawk  'match($6, /ro.*@\/.snapshots\/(.*)\/snapshot/ , arr1 ) { print arr1[1] }')

btrfs property set -ts /.snapshots/$mounted_snapshot/snapshot ro false

mount -o remount, rw /

mkdir /var/lib/libvirt
mkdir /var/lib/nova
mkdir /var/lib/openstack-helm
mkdir /var/lib/neutron

btrfs property set -ts /.snapshots/$mounted_snapshot/snapshot ro true

reboot
</screen>
  </section>
</section>
<section xml:id="recovering-from-node-failure">
  <title>Recovering from Node Failure</title>
  <para>
    Kubernetes clusters are generally able to recover from node failures
    by performing a number of self-healing actions, but it may be
    necessary to manually intervene occasionally. Recovery actions vary
    depending on the type of failure. Some common scenarios and their
    solutions are outlined below.
  </para>
  <section xml:id="pod-status-of-nodelost-or-unknown">
    <title>Pod Status of NodeLost or Unknown</title>
    <para>
      If a large number of Pods show a status of <literal>NodeLost</literal> or
      <literal>Unknown</literal>, first determine which nodes may be causing
      the problem by running:
    </para>
    <screen>kubectl get nodes</screen>
    <para>
      If any of the nodes show a status of <literal>NotReady</literal> but they
      still respond to ping and can be accessed via SSH, it may be that either
      the kubelet or docker service has stopped running. This can be confirmed
      by checking the <literal>Conditions</literal> section for the message
      <literal>Kubelet has stopped posting node status</literal> after running:
    </para>
    <screen>kubectl describe node ${NODE_NAME}</screen>
      <para>
        Log into the affected nodes and check the status of these
        services by running:
      </para>
    <screen>systemctl status kubelet
systemctl status docker</screen>
    <para>
      If either service has stopped, start it by running:
    </para>
    <screen>systemctl start ${SERVICE_NAME}</screen>
    <note>
     <para>
      The kubelet service requires Docker to be running. If both
      services are stopped, Docker should be restarted first.
     </para>
    </note>
    <para>
      These services should start automatically each time a node boots up and
      should be running at all times. If either service has stopped, examine
      the system logs to determine the root cause of the failure. This can be
      done by using the <command>journalctl</command> command:
    </para>
    <screen>journalctl -u kubelet</screen>
  </section>
  <section xml:id="frequent-pod-evictions">
    <title>Frequent Pod Evictions</title>
    <para>
      If Pods are frequently being evicted from a particular node, it
      may be a sign that the node is unhealthy and requires maintenance.
      Check that node's conditions and events by running:
    </para>
    <screen>kubectl describe node <replaceable>NODE_NAME</replaceable></screen>
    <para>
      If the cause of the Pod evictions is determined to be resource
      exhaustion, such as <literal>NodeHasDiskPressure</literal> or
      <literal>NodeHasMemoryPressure</literal>, it may be necessary to remove
      the node from the cluster temporarily to perform maintenance. To
      gracefully remove all Pods from the affected node and mark them as not
      schedulable, run:
    </para>
    <screen>kubectl drain <replaceable>NODE_NAME</replaceable></screen>
    <para>
      After maintenance work is complete, the node can be brought back
      into the cluster by running:
    </para>
    <screen>kubectl uncordon <replaceable>NODE_NAME</replaceable></screen>
    <para>
      which will allow normal Pod scheduling operations to resume. If the node
      was decommissioned permanently while offline and a new node was brought
      into the CaaSP cluster as a replacement, it is not necessary to run the
      <command>uncordon</command> command. A new schedulable resource will be
      created automatically.
    </para>
  </section>
</section>
<section xml:id="kubernetesoperations">
  <title>Kubernetes Operations</title>
  <para>
    Kubernetes has documentation for
    <link xlink:href="https://kubernetes.io/docs/tasks/debug-application-cluster/troubleshooting//">troubleshooting
    typical problems with applications and clusters</link>.
  </para>
</section>
<section xml:id="tips_and_tricks">
  <title>Tips and Tricks</title>
  <section xml:id="display-all-images-used-by-a-component">
    <title>Display all images used by a component</title>
    <para>
      Using Neutron as an example:
    </para>
    <screen>kubectl get pods -n openstack -l application=neutron -o \
jsonpath=&quot;{.items[*].spec.containers[*].image}&quot;|tr -s '[[:space:]]' '\n' \
| sort | uniq -c
</screen>
  </section>
  <section xml:id="remove-dangling-docker-images">
    <title>Remove Dangling Docker images</title>
    <para>
      This is useful after building local images:
    </para>
    <screen>docker rmi $(docker images -f &quot;dangling=true&quot; -q)</screen>
  </section>
  <section xml:id="setting-the-default-context">
    <title>Setting the Default Context</title>
    <para>
      To avoid having to pass <literal>-n openstack</literal> repeatedly:
    </para>
    <screen>kubectl config set-context $(kubectl config current-context) --namespace=openstack</screen>
  </section>
</section>
</chapter>
