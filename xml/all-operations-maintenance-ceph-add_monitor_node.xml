<?xml version="1.0"?>
<!DOCTYPE section [
 <!ENTITY % entities SYSTEM "entities.ent"> %entities;
]>
<section xmlns="http://docbook.org/ns/docbook" xmlns:xi="http://www.w3.org/2001/XInclude" xmlns:xlink="http://www.w3.org/1999/xlink" xml:id="add_monitor_node"><title>&kw-hos-tm; &kw-hos-version-50;: Adding a Ceph Monitor Node</title><abstract><para><para>During your initial installation and deployment, if
      you defined a dedicated cluster for the Ceph monitor component then these steps can be used to
      add additional Ceph monitor nodes.</para><para>During your initial installation and deployment, if you defined separate hosts for the Ceph
      monitor component then these steps can be used to add additional Ceph monitor nodes.</para><para>If you did not define separate Ceph monitor nodes in your deployment then these steps will
      not work for you. For details on how to define a dedicated Ceph monitor cluster, see <xref linkend="topic_ah5_4yx_qt"/>.</para><para>The steps involved are:</para></para>
</abstract>
    <para><para xml:id="idg-all-operations-maintenance-ceph-add_monitor_node-xml-4"><phrase><phrase/></phrase></para></para>


    <sidebar xml:id="idg-all-operations-maintenance-ceph-add_monitor_node-xml-6"><title>Prerequisites</title><para>You should have one or more baremetal servers that meet the minimum hardware requirements
        for a Ceph monitor node which are documented in the <xref linkend="min_hardware"/>.</para>
</sidebar>
    <sidebar xml:id="idg-all-operations-maintenance-ceph-add_monitor_node-xml-7"><title>Notes</title><para>It is recommended to maintain an odd number of Ceph monitors because the Paxos protocol is
        being used by Ceph monitors to form a consensus. Although clusters can be deployed with a
        single monitor, we strongly recommend to deploy three monitors on independent nodes for
        production usage to avoid a single point of failure of the monitor service.</para>
<para>For this reason, it is recommended to increase the number of monitors two at a time if a
        valid need for more than three exists.</para>
</sidebar>
    <sidebar xml:id="idg-all-operations-maintenance-ceph-add_monitor_node-xml-8"><title>Adding a Ceph Monitor Node</title><para>These steps will show you how to add the new Ceph monitor node to your
          <literal>servers.yml</literal> file and then run the playbooks that update your cloud
        configuration. You will run these playbooks from the lifecycle manager.</para>
<orderedlist>
        <listitem><para>Log in to your lifecycle manager.</para>
</listitem>
        <listitem><para>Checkout the <literal>site</literal> branch of your local git so you can begin to make the
          necessary edits:
          </para>
<screen>cd ~/helion/my_cloud/definition/data
git checkout site</screen></listitem>
        <listitem><para>In the same directory, edit your <literal>servers.yml</literal> file to add the details
          for your new Ceph monitor nodes. Copy and paste the section for an existing server having
          the <literal>MON-ROLE</literal> role and edit the values to match the new server being
            added.</para>
<para>Here is an example:</para>
<screen>
- id: ceph-mon4
  ip-addr: 10.13.111.141
  server-group: RACK1
  role: CEP-MON-ROLE
  nic-mapping: MY-4PORT-SERVER
  mac-addr: "f0:92:1c:05:69:10"
  ilo-ip: 10.12.8.217
  ilo-password: password
  ilo-user: admin</screen><important><para>You will need to verify that the <literal>ip-addr</literal> value you
            choose for this node does not conflict with any other IP address in your cloud
            environment. You can confirm this by checking the
              <literal>~/helion/my_cloud/info/address_info.yml</literal> file on your lifecycle
            manager.</para>
</important></listitem>
        <listitem><para>In your <literal>control_plane.yml</literal> file you will need to check the values for
            <literal>member-count</literal>, <literal>min-count</literal>, and
            <literal>max-count</literal>. If you specified them, ensure that they match up with your
          new total node count. So for example, if you had previously specified
            <literal>member-count: 3</literal> and are adding a fourth Ceph monitor node, you will
          need to change that value to <literal>member-count: 4</literal>.</para>
</listitem>
        <listitem><para>Commit the changes to git:
          </para>
<screen>git commit -a -m "Add new Ceph monitor node &lt;name&gt;"</screen></listitem>
        <listitem><para>Run the configuration processor:
          </para>
<screen>cd ~/helion/hos/ansible
ansible-playbook -i hosts/localhost config-processor-run.yml</screen></listitem>
        <listitem><para>Update your deployment directory:
          </para>
<screen>cd ~/helion/hos/ansible
ansible-playbook -i hosts/localhost ready-deployment.yml</screen></listitem>
        <listitem><para>Add the new node into Cobbler:
          </para>
<screen>cd ~/helion/hos/ansible
ansible-playbook -i hosts/localhost cobbler-deploy.yml</screen></listitem>
        <listitem><para>Then you can image the node: </para>
<screen>cd ~/helion/hos/ansible
ansible-playbook -i hosts/localhost bm-reimage.yml -e nodelist=&lt;node name&gt;</screen><para>Before proceeding, you may want to take a look at
              <emphasis role="bold">~/helion/my_cloud/info/server_info.yml</emphasis> to see if the assignment of the node you
            have added is what you expect. It may not be, as nodes will not be numbered
            consecutively if any have previously been removed. This is to prevent loss of data; the
            config processor retains data about removed nodes and keeps their ID numbers from being
            reallocated. See the Persisted Server Allocations section in  for information on how this works.</para>
</listitem>
        <listitem><para>Run the following series of playbooks complete the deployment of your additional Ceph
          monitor node(s) to your Ceph cluster: </para>
<screen>cd ~/scratch/ansible/next/hos/ansible
ansible-playbook -i hosts/verb_hosts site.yml --tags "generate_hosts_file"
ansible-playbook -i hosts/verb_hosts site.yml --limit &lt;hostname&gt;</screen><para>The hostname of the newly added node can be found in the list generated from the output
            of the following command:</para>
<screen>grep hostname ~/helion/my_cloud/info/server_info.yml</screen></listitem>
      </orderedlist></sidebar>
    <sidebar xml:id="idg-all-operations-maintenance-ceph-add_monitor_node-xml-9"><title>Verifying the New Ceph Monitor Node</title><para>From the lifecycle manager, run the following command:</para>
<screen >ceph --cluster ceph -f json quorum_status</screen><para>If the Ceph cluster name is different than the default (i.e. <emphasis role="bold">ceph</emphasis>), you need to
        modify the above command accordingly.</para>
<para>The newly added Ceph monitor's hostname appears in the <literal>quorum_names</literal>
        section of the output of above command.</para>
</sidebar>
    <sidebar xml:id="idg-all-operations-maintenance-ceph-add_monitor_node-xml-10"><title>Adding a New Ceph Monitor Node to Monitoring</title><para>If you want to add a new Ceph monitor node to the monitoring service checks, there is an
        additional playbook that must be run to ensure this happens:</para>
<screen >cd ~/scratch/ansible/next/hos/ansible
ansible-playbook -i hosts/verb_hosts monasca-deploy.yml --tags active_ping_checks</screen></sidebar>
  </section>
