<?xml version="1.0"?>
<?xml-stylesheet href="urn:x-suse:xslt:profiling:docbook51-profile.xsl"
 type="text/xml"
 title="Profiling step"?>
<!DOCTYPE section [
 <!ENTITY % entities SYSTEM "entity-decl.ent"> %entities;
]>

<section xmlns="http://docbook.org/ns/docbook" xmlns:xi="http://www.w3.org/2001/XInclude" xmlns:xlink="http://www.w3.org/1999/xlink" xml:id="release-notes-501">
 <title>&productnamereg; 5.0.1: Release Notes</title>
 <para>
  An overview of the features contained within version 5.0.1, including known
  issues and workarounds for this release.
 </para>
 <section>
  <title>Fixed in this release</title>
  <para>
   <emphasis role="bold">&productnamereg; 5.0.1: supports SLES12 SP3 for Compute
   Nodes</emphasis>
  </para>
  <para>
   SLES12 SP2 is supported for compute nodes in &productnamereg; 5.0. &productnamereg;
   5.0.1: includes support for the more recently released SLES12 SP3.
  </para>
  <important>
   <para>
    &productnamereg; 5.0.1: does not support SLES12 SP2 compute nodes. If you intend
    to upgrade to &productnamereg; 5.0.1: you will be required to upgrade your
    compute nodes to SP3. See steps below for upgrading SLES compute nodes from
    SP2 to SP3.
   </para>
  </important>
  <para>
   <emphasis role="bold">Using the &lcm; to Deploy SLES Compute
   Nodes</emphasis>
  </para>
  <para>
   The method used for deploying SLES compute nodes using Cobbler on the
   &lcm; uses legacy BIOS.
  </para>
  <note>
   <para>
    UEFI and Secure boot are currently not supported on SLES compute.
   </para>
  </note>
  <para>
   Deploying legacy BIOS SLES compute nodes
  </para>
  <para>
   The installation process for SLES nodes is almost identical to that of HPE
   Linux nodes as described in the topic for Installation for
   <xref linkend="install_kvm"/>. The key differences are:
  </para>
  <itemizedlist>
   <listitem>
    <para>
     The standard SLES ISO (SLE-12-SP3-Server-DVD-x86_64-GM-DVD1.iso) must be
     accessible via <literal>/home/stack/sles12sp3.iso</literal>. Rename the
     ISO or create a symbolic link.
    </para>
<screen>mv SLE-12-SP3-Server-DVD-x86_64-GM-DVD1.iso /home/stack/sles12sp3.iso</screen>
   </listitem>
   <listitem>
    <para>
     The contents of the SLES SDK ISO (SLE-12-SP3-SDK-DVD-x86_64-GM-DVD1.iso)
     must be mounted or copied to
     <literal>/opt/hlm_packager/hlm/sles12/zypper/SDK/</literal>. If you choose
     to mount the ISO, we recommend creating an <literal>/etc/fstab</literal>
     entry to ensure the ISO is mounted after a reboot.
    </para>
    <itemizedlist>
     <listitem>
      <para>
       If mounting the .iso on loopback
      </para>
<screen>sudo mount -o loop /home/stack/sles12.iso /opt/hlm_packager/hlm/sles12/zypper/SDK/</screen>
     </listitem>
     <listitem>
      <para>
       Entry for mounting the .iso on loopback in /etc/fstab
      </para>
<screen>/home/stack/sles12.iso    /opt/hlm_packager/hlm/sles12/zypper/SDK/    iso9660    loop    0    0</screen>
     </listitem>
    </itemizedlist>
   </listitem>
   <listitem>
    <para>
     You must identify the node(s) on which you want to install SLES, by adding
     the key/value pair distro-id: <literal>sles12sp3-x86_64</literal> to
     server details in <literal>servers.yml</literal>. You will also need to
     update <literal>net_interfaces.yml</literal>,
     <literal>server_roles.yml</literal>, <literal>disk_compute.yml</literal>
     and <literal>control_plane.yml</literal>. For more information on
     configuration of the Input Model for SLES, see SLES Compute Model.
    </para>
   </listitem>
   <listitem>
    <para>
     &productname; playbooks currently do not take care of SDK, so it needs to be
     added manually. The following command needs to be run on every SLES
     compute node:
    </para>
<screen>deployer_ip=192.168.10.254
zypper addrepo --no-gpg-checks --refresh http://$deployer_ip:79/hlm/sles12/zypper/SDK SLES-SDK</screen>
   </listitem>
  </itemizedlist>
  <para>
   <emphasis role="bold">Upgrading &product; with SLES Compute SP2 to
   SP3</emphasis>
  </para>
  <para>
   You can upgrade your SLES compute nodes from SP2 to SP3 by following these
   steps.
  </para>
  <orderedlist>
   <listitem>
    <para>
     You must be running &product; with SLES12 SP2 Compute
    </para>
   </listitem>
   <listitem>
    <para>
     Upgrade the Compute OS from SP2 to SP3 by following these steps.
    </para>
    <itemizedlist>
     <listitem>
      <para>
       On the deployer, unmount the sp2 content, example:
      </para>
<screen>sudo umount /opt/hlm_packager/hlm/sles12/zypper/OS/
sudo umount /opt/hlm_packager/hlm/sles12/zypper/SDK/</screen>
     </listitem>
     <listitem>
      <para>
       Copy or mount the contents of SLES SDK ISO, example:
      </para>
<screen>sudo mount -o loop SLE-12-SP3-SDK-DVD-x86_64-GM-DVD1.iso /opt/hlm_packager/hlm/sles12/zypper/SDK/
sudo mount -o loop SLE-12-SP3-Server-DVD-x86_64-GM-DVD1.iso /opt/hlm_packager/hlm/sles12/zypper/OS/ </screen>
     </listitem>
     <listitem>
      <para>
       On sles12sp2 compute, remove old repositories if applicable
      </para>
<screen>sudo zypper removerepo SLES-SDK
sudo zypper removerepo SLES12-SP2-12.2-0</screen>
     </listitem>
     <listitem>
      <para>
       On sles12sp2 compute, clean up old repositories if applicable
      </para>
<screen>sudo zypper clean</screen>
     </listitem>
     <listitem>
      <para>
       Add updated repositories (Update the value of
       <literal>deployer_ip</literal> as necessary)
      </para>
<screen>deployer_ip=192.168.10.254
zypper addrepo --no-gpg-checks --refresh http://$deployer_ip:79/hlm/sles12/zypper/OS SLES-OS
zypper addrepo --no-gpg-checks --refresh http://$deployer_ip:79/hlm/sles12/zypper/SDK SLES-SDK</screen>
     </listitem>
     <listitem>
      <para>
       Run zypper to refresh
      </para>
<screen>sudo <literal>zypper</literal> refresh</screen>
     </listitem>
     <listitem>
      <para>
       Run <literal>zypper</literal> to upgrade:
      </para>
<screen>sudo zypper up</screen>
     </listitem>
    </itemizedlist>
    <note>
     <para>
      After OS upgrade from SP2 to SP3, the administrator will need to run
      the following command. If the only way to reach the SLES Compute node
      is from the IPMI console, the command may be run from there.
      <command>sudo systemctl enable openvswitch</command>If the openvswitch
      is not running after the upgrade, the administrator will need to run
      the command: <command>sudo systemctl start openvswitch</command>
     </para>
    </note>
   </listitem>
   <listitem>
    <para>
     Mount the 5.0.1 ISO image
    </para>
<screen>sudo mount -o loop HelionOpenStack-5.0.iso /media/cdrom</screen>
   </listitem>
   <listitem>
    <para>
     Unpack the following tarball:
    </para>
<screen>cd ~
tar fxv /media/cdrom/hos/hos-5.0.0-20161014T020249Z.tar</screen>
   </listitem>
   <listitem>
    <para>
     Run the included initialization script to update the deployer:
    </para>
<screen>~/hos-5.0.1/hos-init.bash</screen>
   </listitem>
   <listitem>
    <para>
     Run the configuration processor:
    </para>
<screen>cd ~/openstack/ardana/ansible
ansible-playbook -i hosts/localhost config-processor-run.yml</screen>
   </listitem>
   <listitem>
    <para>
     Update the deployment directory:
    </para>
<screen>cd ~/openstack/ardana/ansible
ansible-playbook -i hosts/localhost ready-deployment.yml</screen>
   </listitem>
   <listitem>
    <para>
     Run site.yml
    </para>
<screen>cd ~/scratch/ansible/next/ardana/ansible
ansible-playbook -i hosts/verb_hosts site.yml</screen>
   </listitem>
   <listitem>
    <para>
     If Ceph is configured, run hlm-cloud-configure.yml
    </para>
<screen>cd ~/scratch/ansible/next/ardana/ansible
ansible-playbook -i hosts/verb_hosts hlm-cloud-configure.yml</screen>
   </listitem>
   <listitem>
    <para>
     Reboot all
    </para>
   </listitem>
  </orderedlist>
  <para>
   <emphasis role="bold">HPE Linux Changes</emphasis>
  </para>
  <para>
   &productname; 5.0.1 includes a new version of the &slsa; operating
   system which has fixes for CVEs known to us and deemed to be issues not
   mitigated by architecture at the time of ISO creation.
  </para>
  <para>
   <emphasis role="bold">MTU Bug for LBaaS</emphasis>
  </para>
  <para>
   Contains fix for OpenStack bug
   <link xlink:href="https://review.openstack.org/#/c/407617/">https://review.openstack.org/#/c/407617/</link>
  </para>
  <para>
   LBaaSv2 sets up a VIF without specifying its MTU. Therefore, the VIF always
   gets the default MTU of 1500. When attaching the load balancer to a
   VXLAN-backed project (tenant) network, which by default has a MTU of 1450,
   this leads to packet dropping.
  </para>
  <para>
   Removed hardcoded value which prevented LBaaSv2 from using MTU specified by
   the network object.
  </para>
  <para>
   <emphasis role="bold">Magnum Fedora Atomic for &product; must use
   version fedora-25</emphasis>
  </para>
  <para>
   You can find more information in
   <xref linkend="deploying_kubernetes_fedora_atomic"/>.
  </para>
  <para>
   <emphasis role="bold">3PAR Fix</emphasis>
  </para>
  <para>
   Fixed an issue with the 3PAR driver to ensure that CHAP credentials are used
   correctly after compute host is rebooted.
  </para>
  <para>
   <emphasis role="bold">Adding Compute Nodes with Ceph already
   configured</emphasis>
  </para>
  <para>
   An additional step is needed to add compute nodes to a cloud where Ceph has
   already been configured.
  </para>
  <itemizedlist>
   <listitem>
    <para>
     Complete the compute host deployment using site.yml as noted in in the
     &product; documentation.
    </para>
   </listitem>
   <listitem>
    <para>
     Then run the following command.
    </para>
<screen>cd ~/scratch/ansible/next/ardana/ansible/ansible-playbook -i hosts/verb_hosts ceph-client-prepare.yml</screen>
   </listitem>
  </itemizedlist>
  <para>
   This applies to hLinux, SLES, RHEL and RHEV computes.
  </para>
 </section>
<!---->
</section>
