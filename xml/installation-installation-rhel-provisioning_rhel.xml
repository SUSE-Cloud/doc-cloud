<?xml version="1.0"?>
<!DOCTYPE section [
 <!ENTITY % entities SYSTEM "entities.ent"> %entities;
]>
<section xml:id="provisioning_rhel"
 xmlns="http://docbook.org/ns/docbook" version="5.1"
 xmlns:xi="http://www.w3.org/2001/XInclude"
 xmlns:xlink="http://www.w3.org/1999/xlink">
 <title>Provisioning &rhla; Yourself</title>
 <para>
  This section outlines how to manually provision a &rhla; node, so
  that it can be added to a new or existing cloud created with &productname;.
 </para>
 <section xml:id="sec.provision-rhel.install">
  <title>Installing &rhla; 7.3</title>
  <para>
   Install &rhla; 7.3 using the standard ISO
   (<filename>RHEL-7.3-20151030.0-Server-x86_64-dvd1.iso</filename>)
  </para>
 </section>
 <section xml:id="sec.provision-rhel.assign-ip">
  <title>Assigning a Static IP</title>
  <procedure>
   <step>
    <para>
     Use the <literal>ip addr</literal> command to find out what network
     devices are on your system:
    </para>
<screen>1: lo: &lt;LOOPBACK,UP,LOWER_UP&gt; mtu 65536 qdisc noqueue state UNKNOWN
    link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00
    inet 127.0.0.1/8 scope host lo
       valid_lft forever preferred_lft forever
    inet6 ::1/128 scope host
       valid_lft forever preferred_lft forever
2: <emphasis role="bold">eno1</emphasis>: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc mq state UP qlen 1000
    link/ether <emphasis role="bold">f0:92:1c:05:89:70</emphasis> brd ff:ff:ff:ff:ff:ff
    inet 10.13.111.178/26 brd 10.13.111.191 scope global eno1
       valid_lft forever preferred_lft forever
    inet6 fe80::f292:1cff:fe05:8970/64 scope link
       valid_lft forever preferred_lft forever
3: eno2: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc mq state UP qlen 1000
    link/ether f0:92:1c:05:89:74 brd ff:ff:ff:ff:ff:ff</screen>
   </step>
   <step>
    <para>
     Identify the entry that matches the MAC address of your server and edit
     the corresponding configuration file in
     <filename>/etc/sysconfig/network-scripts</filename>:
    </para>
<screen>vi /etc/sysconfig/network-scripts/<emphasis role="bold">ifcfg-eno1</emphasis></screen>
   </step>
   <step>
    <para>
     Edit the <literal>IPADDR</literal> and <literal>NETMASK</literal> values
     to match your environment. Note that the <literal>IPADDR</literal> is used
     in the corresponding stanza in <literal>servers.yml</literal>. You may
     also need to set <literal>BOOTPROTO</literal> to <literal>none</literal>.
    </para>
<screen>TYPE=Ethernet
<emphasis role="bold">BOOTPROTO=none</emphasis>
DEFROUTE=yes
PEERDNS=yes
PEERROUTES=yes
IPV4_FAILURE_FATAL=no
IPV6INIT=yes
IPV6_AUTOCONF=yes
IPV6_DEFROUTE=yes
IPV6_PEERDNS=yes
IPV6_PEERROUTES=yes
IPV6_FAILURE_FATAL=no
NAME=eno1
UUID=36060f7a-12da-469b-a1da-ee730a3b1d7c
DEVICE=eno1
ONBOOT=yes
<emphasis role="bold">NETMASK=255.255.255.192</emphasis>
<emphasis role="bold">IPADDR=10.13.111.14</emphasis></screen>
   </step>
   <step performance="optional">
    <para>
     Reboot your &rhla; node and ensure that it can be accessed from
     the &lcm;.
    </para>
   </step>
  </procedure>
 </section>
 <section xml:id="sec.provision-rhel.user">
  <title>Adding The User <systemitem class="username">stack</systemitem> and Their Home Directory</title>
<screen>useradd -m stack
passwd stack</screen>
 </section>
 <section>
  <title>Allow User <systemitem class="username">stack</systemitem> to Use <command>sudo</command> Without Password</title>
  <para>
   There are a number of different ways to achieve this. Here is one
   possibility using the pre-existing
   <systemitem class="groupname">wheel</systemitem> group.
  </para>
  <procedure>
   <step>
    <para>
     Add the user <systemitem class="username">stack</systemitem> to the
     <systemitem class="groupname">wheel</systemitem> group.
    </para>
<screen>usermod -aG wheel stack</screen>
   </step>
   <step>
    <para>
     Run the command <command>visudo</command>.
    </para>
   </step>
   <step>
    <para>
     Uncomment the line specifying <literal>NOPASSWD: ALL</literal> for the
     <systemitem class="groupname">wheel</systemitem> group.
    </para>
<screen>## Allows people in group wheel to run all commands
%wheel  ALL=(ALL)       ALL

## Same thing without a password
<emphasis role="bold">%wheel ALL=(ALL)       NOPASSWD: ALL</emphasis></screen>
   </step>
   <step>
    <para>
     To facilitate using SSH from the deployer and running a command via
     <command>sudo</command>, comment the lines for
     <literal>requiretty</literal> and
     <literal>!visiblepw</literal>
    </para>
<screen>#
# Disable "ssh hostname sudo &lt;cmd&gt;", because it will show the password in clear.
#         You have to run "ssh -t hostname sudo &lt;cmd&gt;".
#
<emphasis role="bold">#</emphasis>Defaults    requiretty

#
# Refuse to run if unable to disable echo on the tty. This setting should also be
# changed in order to be able to use sudo without a tty. See requiretty above.
#
<emphasis role="bold">#</emphasis>Defaults   !visiblepw</screen>
   </step>
  </procedure>
 </section>

 <section xml:id="sec.provision-deployer-for-rhel">
  <title>Setting Up a Deployer for RHEL compute </title>
  <para>
  Before you install the software required for &rhel; &compnode;s, perform
  the following tasks.
 </para>
 <procedure>
  <step>
   <para>
    Configure the &obs; &centos; software repository on the deployer node and
    on the &rhel; &compnode;s using the following URL:
    <link xlink:href="https://download.opensuse.org/repositories/systemsmanagement:/Ardana:/8:/CentOS/CentOS_7.3"/>.
   </para>
  </step>
  <step>
   <para>
    &ostack; service and its components are installed on &rhel; &compnode;s via
    virtualenv (venv). So you need to install the following venv packages from
    the &obs; &centos; repository on the deployer node:
   </para>
   <itemizedlist>
    <listitem>
     <para>
      <package>venv-openstack-nova-rhel-x86_64</package>
     </para>
    </listitem>
    <listitem>
     <para>
      <package>venv-openstack-neutron-rhel-x86_64</package>
     </para>
    </listitem>
    <listitem>
     <para>
      <package>venv-openstack-freezer-rhel-x86_64</package>
     </para>
    </listitem>
    <listitem>
     <para>
      <package>venv-openstack-monasca_agent-rhel-x86_64</package>
     </para>
    </listitem>
   </itemizedlist>
   <para>
    On the deployer node, the &obs; &centos; repository needs to be configured
    with lower priority than &sles; repositories. This ensures that packages
    present in both repos are preferably installed from the &cloudos;
    repository.
   </para>
<screen><?dbsuse-fo font-size="0.6em"?>
zypper ar --priority 100 \
  https://download.opensuse.org/repositories/systemsmanagement:/Ardana:/8:/CentOS/CentOS_7.3/systemsmanagement:Ardana:8:CentOS.repo</screen>
   <para>
    Review and accept the certificate for the new repository.
   </para>
<screen>zypper ref</screen>
   <para>
    venv RPM packages can be specifically installed from the &obs; &centos;
    repository:
   </para>
<screen>zypper install --from systemsmanagement_Ardana_8_CentOS \
  venv-openstack-nova-rhel-x86_64</screen>
   <para>
    Run the following command to add the venv packages to package indexes.
   </para>
<screen>/usr/bin/create_index --dir=/opt/ardana_packager/ardana-8/rhel_venv/x86_64</screen>
  </step>
  <step>
   <para>
    On &rhel; &compnode;s, configure the &obs; &centos; repository to download
    specific RPM packages required by cloud tooling, &ostack; &o_comp; and
    &o_netw; &comp; components. The following RPM packages are required from
    the &obs; &centos; repository on the &compnode;:
   </para>
   <itemizedlist>
    <listitem>
     <para>
      <package>python-ardana-packager</package>
     </para>
    </listitem>
    <listitem>
     <para>
      <package>openvswitch</package>
     </para>
    </listitem>
    <listitem>
     <para>
      <package>python-PyYAML</package>
     </para>
    </listitem>
    <listitem>
     <para>
      <package>python-Beaver</package>
     </para>
    </listitem>
    <listitem>
     <para>
      <package>python-monascaclient</package>
     </para>
    </listitem>
    <listitem>
     <para>
      <package>openstack-freezer-api</package>
     </para>
    </listitem>
    <listitem>
     <para>
      <package>openstack-freezer-agent</package>
     </para>
    </listitem>
    <listitem>
     <para>
      <package>openstack-freezer-scheduler</package>
     </para>
    </listitem>
    <listitem>
     <para>
      <package>python-python-logstash</package>
     </para>
    </listitem>
   </itemizedlist>
  </step>
  <step>
   <para>
    Installing the &ostack-current; release &o_comp; &comp; version requires
    <package>qemu-kvm</package> version 2.1.0 or higher. It is recommended to
    install the required packages via &rhel; subscription to ensure continuous
    updates. Alternatively, these packages can be installed manually from
    <link xlink:href="http://vault.centos.org/7.3.1611/virt/x86_64/"/>. This
    includes the following packages:
   </para>
   <itemizedlist>
    <listitem>
     <para>
      <package>qemu-img-ev-2.6.0-28.el7.10.1.x86_64.rpm</package>
     </para>
    </listitem>
    <listitem>
     <para>
      <package>qemu-kvm-common-ev-2.6.0-28.el7.10.1.x86_64.rpm</package>
     </para>
    </listitem>
    <listitem>
     <para>
      <package>qemu-kvm-ev-2.6.0-28.el7.10.1.x86_64.rpm</package>
     </para>
    </listitem>
    <listitem>
     <para>
      <package>qemu-kvm-tools-ev-2.6.0-28.el7.10.1.x86_64.rpm</package>
     </para>
    </listitem>
   </itemizedlist>
  </step>
 </procedure>
 </section>


 <section xml:id="sec.provision-rhel.yum-iso">
  <title>Setting Up a Yum Repository from a &rhla; ISO</title>
  <para>
   You need to set up a Yum repository, either external or local, containing a
   &rhla; distribution supported by &productname;. This repository must mirror
   the entire product repository including the
   <literal>ResilientStorage</literal> and <literal>HighAvailability</literal>
   add-ons. To create this repository, follow these steps:
  </para>
  <procedure>
   <step>
    <para>
     Mount the &rhla; ISO and expand it:
    </para>
<screen>mkdir /tmp/localrhel
mount -o loop rhel7.iso /mnt
cd /mnt
tar cvf - . | (cd /tmp/localrhel ; tar xvf -)
cd /
umount /mnt</screen>
   </step>
   <step>
    <para>
     Create a repository file named
     <filename>/etc/yum.repos.d/localrhel.repo</filename>
     with the following contents:
    </para>
<screen>[localrhel]
name=localrhel
baseurl=file:///tmp/localrhel
enabled=1
gpgcheck=0

[localrhel-1]
name=localrhel-1
baseurl=file:///tmp/localrhel/addons/ResilientStorage
enabled=1
gpgcheck=0

[localrhel-2]
name=localrhel-2
baseurl=file:///tmp/localrhel/addons/HighAvailability
enabled=1
gpgcheck=0</screen>
   </step>
   <step>
    <para>
     Run:
    </para>
<screen>yum clean all</screen>
   </step>
  </procedure>
 </section>
 <section xml:id="sec.provision-rhel.yum-ardana">
  <title>Setting Up a Yum Repository With Packages Related to &lcm;</title>
  <para>
   You must configure a Yum repository with contents related to &lcm; on each
   &rhla; compute node. This repository was originally created for &centos;
   which means the packages are compatible with &rhla;.
  </para>
  <procedure>
   <step>
    <para>
     Configure the repository with low priority.
    </para>
    <para>
     Create a repository file
     <filename>/etc/yum.repos.d/centos_c8_ardana.repo</filename>
     with the following contents:
    </para>
<screen>
<?dbsuse-fo font-size="0.6em"?>
[centos_c8_ardana]
name=centos_c8_ardana
enabled=1
autorefresh=0
baseurl=https://download.opensuse.org/repositories/systemsmanagement:/Ardana:/8:/CentOS/CentOS_7.3/
type=rpm-md
priority=9999
gpgcheck=0
</screen>
   </step>
   <step>
    <para>
     &lcm; playbooks will install the necessary RPM packages from the &centos;
     repository while executing various service plays.
    </para>
   </step>
   <step>
    <para>
     To install the &ostack-current; version of <literal>nova-compute</literal>,
     there is a requirement to have a <literal>qemu-kvm</literal> version
     greater than 2.1.0.
    </para>
    <para>
     The RPM packages needed for <literal>nova-compute</literal> must be
     provided before executing <filename>site.yml</filename>. It is
     expected that a &rhla; subscription will be used to provide these RPM
     packages so that they get continuous updates.
    </para>
    <para>
     The <literal>qemu-kvm</literal> packages can also be can be downloaded
     from
     <link xlink:href="http://vault.centos.org/7.3.1611/virt/x86_64/kvm-common/"/>.
     The RPM packages below have been tested successfully:
    </para>
    <itemizedlist>
     <listitem>
      <para>
       <filename>qemu-img-ev-2.6.0-28.el7.10.1.x86_64.rpm</filename>
      </para>
     </listitem>
     <listitem>
      <para>
       <filename>qemu-kvm-common-ev-2.6.0-28.el7.10.1.x86_64.rpm</filename>
      </para>
     </listitem>
     <listitem>
      <para>
       <filename>qemu-kvm-ev-2.6.0-28.el7.10.1.x86_64.rpm</filename>
      </para>
     </listitem>
     <listitem>
      <para>
       <filename>qemu-kvm-tools-ev-2.6.0-28.el7.10.1.x86_64.rpm</filename>
      </para>
     </listitem>
    </itemizedlist>
   </step>
   <step>
    <para>
     When using &centos; <literal>qemu-kvm</literal> RPM packages on &rhla;
     compute nodes, SELinux policy updates are needed for
     <literal>nova-compute</literal> to work. SELinux policy updates can be
     enabled by setting a flag in the default values of
     <literal>nova-compute</literal> plays. These flag values need to be set in
     the &lcm; <emphasis>before</emphasis>
     <literal>nova-compute</literal> plays are executed on &rhla; nodes.
    </para>
    <substeps>
     <step>
      <para>
       On the &lcm;:
      </para>
<screen>cd ~/openstack/ardana/ansible/</screen>
     </step>
     <step>
      <para>
       Edit the <literal>nova-compute</literal> file with the flag defined in
       Ansible defaults:
      </para>
<screen>vi roles/NOV-CMP-KVM/defaults/main.yml</screen>
     </step>
     <step>
      <para>
       Set the flag to <literal>true</literal>:
      </para>
<screen>nova_rhel_compute_apply_selinux_policy_updates: true</screen>
     </step>
     <step>
      <para>
       Save the file.
      </para>
     </step>
     <step>
      <para>
       Commit the change to Git:
      </para>
<screen>
git add -A
git commit --allow-empty -m "Enable SELinux policy updates for compute nodes"
</screen>
     </step>
     <step>
      <para>
       Copy the change to scratch (work) area:
      </para>
<screen>ansible-playbook -i hosts/localhost ready-deployment.yml</screen>
     </step>
    </substeps>
   </step>
  </procedure>
  <para>
   Proceed with configuring &rhla; compute nodes.
  </para>
 </section>
 <xi:include xpointer="element(/1/9)" href="installation-kvm_xpointer.xml"/>
 <section xml:id="sec.provision-rhel.package">
  <title>Adding Required Packages</title>
  <para>
   As documented in <xref linkend="sec.kvm.provision"/>, you will need to add
   some extra packages that are required. Ensure that
   <package>openssh-server</package>, <package>python</package>, and
   <package>rsync</package> are installed.
  </para>
 </section>
 <section xml:id="sec.provision-rhel.ssh">
  <title>Setting Up Passwordless SSH Access</title>
  <para>
   After you have started your installation using the &lcm;, or if you are
   adding a &rhla; node to an existing cloud, you need to copy the deployer
   public key to the &rhla; node. One way of doing this is to copy the
   <filename>/home/stack/.ssh/authorized_keys</filename> from another node in
   the cloud to the same location on the &rhla; node. If you are installing a
   new cloud, this file will be available on the nodes after running the
   <filename>bm-reimage.yml</filename> playbook.
  </para>
  <important>
   <para>
    Ensure that there is global read access to the file
    <filename>/home/stack/.ssh/authorized_keys</filename>.
   </para>
  </important>
  <para>
   Now test passwordless SSH from the deployer and check your ability to
   remotely execute sudo commands:
  </para>
<screen>ssh stack@<replaceable>IP_OF_RHEL_NODE</replaceable> "sudo tail -5 /var/log/messages"</screen>
 </section>
</section>
