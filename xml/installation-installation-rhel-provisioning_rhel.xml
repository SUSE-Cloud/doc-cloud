<?xml version="1.0"?>
<!DOCTYPE section [
 <!ENTITY % entities SYSTEM "entities.ent"> %entities;
]>
<section xml:id="provisioning_rhel"
 xmlns="http://docbook.org/ns/docbook" version="5.1"
 xmlns:xi="http://www.w3.org/2001/XInclude"
 xmlns:xlink="http://www.w3.org/1999/xlink">
 <title>Provisioning &rhla; Yourself</title>
 <para>
  This article outlines the steps needed to manually provision a &rhla; node so
  that it can be added to a new or existing &kw-hos-phrase; cloud.
 </para>
 <section>
  <title>Install &rhla; 7.2</title>
  <para>
   Install &rhla; 7.2 using the standard ISO
   (RHEL-7.2-20151030.0-Server-x86_64-dvd1.iso)
  </para>
 </section>
 <section>
  <title>Assign a static IP</title>
  <procedure>
   <step>
    <para>
     Use the <literal>ip addr</literal> command to find out what network
     devices are on your system:
    </para>
<screen>1: lo: &lt;LOOPBACK,UP,LOWER_UP&gt; mtu 65536 qdisc noqueue state UNKNOWN
    link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00
    inet 127.0.0.1/8 scope host lo
       valid_lft forever preferred_lft forever
    inet6 ::1/128 scope host
       valid_lft forever preferred_lft forever
2: <emphasis role="bold">eno1</emphasis>: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc mq state UP qlen 1000
    link/ether <emphasis role="bold">f0:92:1c:05:89:70</emphasis> brd ff:ff:ff:ff:ff:ff
    inet 10.13.111.178/26 brd 10.13.111.191 scope global eno1
       valid_lft forever preferred_lft forever
    inet6 fe80::f292:1cff:fe05:8970/64 scope link
       valid_lft forever preferred_lft forever
3: eno2: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc mq state UP qlen 1000
    link/ether f0:92:1c:05:89:74 brd ff:ff:ff:ff:ff:ff</screen>
   </step>
   <step>
    <para>
     Identify the one that matches the MAC address of your server and edit the
     corresponding config file in
     <literal>/etc/sysconfig/network-scripts</literal>.
    </para>
<screen>vi /etc/sysconfig/network-scripts/<emphasis role="bold">ifcfg-eno1</emphasis> </screen>
   </step>
   <step>
    <para>
     Edit the <literal>IPADDR</literal> and <literal>NETMASK</literal> values
     to match your environment. Note that the <literal>IPADDR</literal> is used
     in the corresponding stanza in <literal>servers.yml</literal>. You may
     also need to set <literal>BOOTPROTO</literal> to <literal>none</literal>.
    </para>
<screen>TYPE=Ethernet
<emphasis role="bold">BOOTPROTO=none</emphasis>
DEFROUTE=yes
PEERDNS=yes
PEERROUTES=yes
IPV4_FAILURE_FATAL=no
IPV6INIT=yes
IPV6_AUTOCONF=yes
IPV6_DEFROUTE=yes
IPV6_PEERDNS=yes
IPV6_PEERROUTES=yes
IPV6_FAILURE_FATAL=no
NAME=eno1
UUID=36060f7a-12da-469b-a1da-ee730a3b1d7c
DEVICE=eno1
ONBOOT=yes
<emphasis role="bold">NETMASK=255.255.255.192</emphasis>
<emphasis role="bold">IPADDR=10.13.111.14</emphasis></screen>
   </step>
   <step>
    <para>
     [OPTIONAL] Reboot your &rhla; node and ensure that it can be accessed from
     the &lcm;.
    </para>
   </step>
  </procedure>
 </section>
 <section>
  <title>Add <systemitem class="username">stack</systemitem> user and home directory</title>
<screen>useradd -m stack
passwd stack</screen>
 </section>
 <section>
<!-- FIXME: "(to) sudo" as a verb?? - sknorr, 2018-01-25 -->
  <title>Allow user <systemitem class="username">stack</systemitem> to use <command>sudo</command> without password</title>
  <para>
   There are a number of different ways to achieve this. Here is one
   possibility using the pre-existing
   <systemitem class="groupname">wheel</systemitem> group.
  </para>
  <procedure>
   <step>
    <para>
     Add the <literal>stack</literal> user to the <literal>wheel</literal>
     group.
    </para>
<screen>usermod -aG wheel stack</screen>
   </step>
   <step>
    <para>
     Run the <literal>visudo</literal> command
    </para>
   </step>
   <step>
    <para>
     Uncomment the line specifying <literal>NOPASSWD: ALL</literal> for the
     <literal>wheel</literal> group.
    </para>
<screen>## Allows people in group wheel to run all commands
%wheel  ALL=(ALL)       ALL

## Same thing without a password
<emphasis role="bold">%wheel ALL=(ALL)       NOPASSWD: ALL</emphasis></screen>
   </step>
   <step>
    <para>
     To facilitate using ssh from the deployer and running a command via sudo,
     comment out the lines for <literal>requiretty</literal> and
     <literal>!visiblepw</literal>
    </para>
<screen>#
# Disable "ssh hostname sudo &lt;cmd&gt;", because it will show the password in clear.
#         You have to run "ssh -t hostname sudo &lt;cmd&gt;".
#
<emphasis role="bold">#</emphasis>Defaults    requiretty

#
# Refuse to run if unable to disable echo on the tty. This setting should also be
# changed in order to be able to use sudo without a tty. See requiretty above.
#
<emphasis role="bold">#</emphasis>Defaults   !visiblepw</screen>
   </step>
  </procedure>
 </section>
 <section>
  <title>Set up a Yum Repository</title>
  <para>
   You need to set up a Yum repository, either external or local, containing a
   &kw-hos; supported &rhla; distribution. You must have a full repository
   including ResilientStorage and HighAvailability addons. One possible method
   for setting up a local repository is outlined in this section.
  </para>
  <procedure>
   <step>
    <para>
     Mount the &rhla; ISO and expand it
    </para>
<screen>mkdir /tmp/localrhel
mount -o loop rhel7.iso /mnt
cd /mnt
tar cvf - . | (cd /tmp/localrhel ; tar xvf -)
cd /
umount /mnt</screen>
   </step>
   <step>
    <para>
     Create a repo file <filename>/etc/yum.repos.d/localrhel.repo</filename>
    </para>
<screen>[localrhel]
name=localrhel
baseurl=file:///tmp/localrhel
enabled=1
gpgcheck=0

[localrhel-1]
name=localrhel-1
baseurl=file:///tmp/localrhel/addons/ResilientStorage
enabled=1
gpgcheck=0

[localrhel-2]
name=localrhel-2
baseurl=file:///tmp/localrhel/addons/HighAvailability
enabled=1
gpgcheck=0</screen>
   </step>
   <step>
    <para>
     Run <literal> yum clean all</literal>.
    </para>
   </step>
  </procedure>
 </section>
 <section>
  <title>Set up a CentOS Yum Repository</title>
  <para>
   You must configure a CentOS software repository on each &rhla; compute node.
  </para>
  <procedure>
   <step>
    <para>
     Configure CentOS repository with low priority.
    </para>
    <para>
     Create a repo file
     <filename>/etc/yum.repos.d/centos_c8_ardana.repo</filename>.
    </para>
<screen>
[centos_c8_ardana]
name=centos_c8_ardana
enabled=1
autorefresh=0
baseurl=https://download.opensuse.org/repositories/systemsmanagement:/Ardana:/8:/CentOS/CentOS_7.3/
type=rpm-md
priority=9999
gpgcheck=0
</screen>
   </step>
   <step>
    <para>
     Ardana playbooks will install the necessary RPM packages from the CentOS
     repository while executing various service plays.
    </para>
   </step>
   <step>
    <para>
     To install the OpenStack Pike version of <literal>nova-compute</literal>,
     there is a requirement to have a <literal>qemu-kvm</literal> version
     greater than 2.1.0.
    </para>
    <para>
     The RPM packages needed for <literal>nova-compute</literal> must be
     provided before <filename>site.yml</filename> execution is started. It is
     expected that a &rhla; subscription will be used to provide these RPM
     packages so that they get continuous updates.
    </para>
    <para>
     The <literal>qemu-kvm</literal> packages can also be can be downloaded
     from
     <link xlink:href="http://vault.centos.org/7.3.1611/virt/x86_64/kvm-common/"/>.
     The RPM packages below have been tested successfully:
    </para>
    <itemizedlist>
     <listitem>
      <para>
       <filename>qemu-img-ev-2.6.0-28.el7.10.1.x86_64.rpm</filename>
      </para>
     </listitem>
     <listitem>
      <para>
       <filename>qemu-kvm-common-ev-2.6.0-28.el7.10.1.x86_64.rpm</filename>
      </para>
     </listitem>
     <listitem>
      <para>
       <filename>qemu-kvm-ev-2.6.0-28.el7.10.1.x86_64.rpm</filename>
      </para>
     </listitem>
     <listitem>
      <para>
       <filename>qemu-kvm-tools-ev-2.6.0-28.el7.10.1.x86_64.rpm</filename>
      </para>
     </listitem>
    </itemizedlist>
   </step>
   <step>
    <para>
     When using CentOS <literal>qemu-kvm</literal> RPM packages on &rhla;
     compute nodes, SELinux policy updates are needed for
     <literal>nova-compute</literal> to work. SELinux policy updates can be
     enabled by setting a flag in the default values of
     <literal>nova-compute</literal> plays. These flag values need to be set in
     the &lcm; <emphasis role="bold">before</emphasis>
     <literal>nova-compute</literal> plays are executed on &rhla; nodes.
    </para>
    <substeps>
     <step>
      <para>
       On the &lcm;:
      </para>
<screen>cd ~/openstack/ardana/ansible/</screen>
     </step>
     <step>
      <para>
       Edit the <literal>nova-compute</literal> file with the flag defined in
       ansible defaults.
      </para>
<screen>vi roles/NOV-CMP-KVM/defaults/main.yml</screen>
     </step>
     <step>
      <para>
       Set flag to <literal>true</literal>.
      </para>
<screen>nova_rhel_compute_apply_selinux_policy_updates: true</screen>
     </step>
     <step>
      <para>
       Save the file.
      </para>
     </step>
     <step>
      <para>
       Commit the change in git.
      </para>
<screen>
git add -A
git commit --allow-empty -m "Enabled selinux policy updates for compute nodes"
</screen>
     </step>
     <step>
      <para>
       Copy the change to scratch (work) area.
      </para>
<screen>ansible-playbook -i hosts/localhost ready-deployment.yml</screen>
     </step>
    </substeps>
   </step>
  </procedure>
  <para>
   Proceed with configuring &rhla; compute nodes.
  </para>
  <xi:include xpointer="element(/1/9)" href="installation-kvm_xpointer.xml"/>
 </section>
 <section>
  <title>Add Required Packages</title>
  <para>
   As documented in <xref linkend="sec.kvm.provision"/>, you will need to add
   some extra packages that are required. Ensure that
   <package>openssh-server</package>, <package>python</package>, and
   <package>rsync</package> are installed.
  </para>
 </section>
 <section>
  <title>Set Up Passwordless SSH Access</title>
  <para>
   Once you have started your installation using the &lcm;, or if you are
   adding a &rhla; node to an existing cloud, you need to copy the deployer
   public key to the &rhla; node. One way of doing this is to copy the
   <filename>/home/stack/.ssh/authorized_keys</filename> from another node in
   the cloud to the same location on the &rhla; node. If you are installing a
   new cloud, this file will be available on the nodes after running the
   <filename>bm-reimage.yml</filename> playbook.
  </para>
  <important>
   <para>
    Ensure that there is global read access to the file
    <filename>/home/stack/.ssh/authorized_keys</filename>.
   </para>
  </important>
  <para>
   Now test passwordless ssh from the deployer and check your ability to
   remotely execute sudo commands:
  </para>
<screen>ssh stack@<replaceable>IP_OF_RHEL_NODE</replaceable> "sudo tail -5 /var/log/messages"</screen>
 </section>
</section>
