<?xml version="1.0"?>
<?xml-stylesheet href="urn:x-suse:xslt:profiling:docbook51-profile.xsl"
 type="text/xml"
 title="Profiling step"?>
<!DOCTYPE section [
 <!ENTITY % entities SYSTEM "entity-decl.ent"> %entities;
]>
<section xmlns="http://docbook.org/ns/docbook" xmlns:xi="http://www.w3.org/2001/XInclude" xmlns:xlink="http://www.w3.org/1999/xlink" xml:id="add_sles_compute">
 <title>Adding a &slsa; Compute Node</title>
 <para>
  Adding a &slsa; compute node allows you to add additional capacity for more
  virtual machines.
 </para>
 <para>
  You may have a need to add additional &slsa; compute hosts for more virtual
  machine capacity or another purpose and these steps will help you achieve
  this.
 </para>
 <para>
  There are two methods you can use to add &slsa; compute hosts to your
  environment:
 </para>
 <orderedlist>
  <listitem>
   <para>
    Adding &slsa; pre-installed compute hosts. This method does not require the
    &slsa; ISO be on the &lcm; to complete.
   </para>
  </listitem>
  <listitem>
   <para>
    Using the provided Ansible playbooks and Cobbler, &slsa; will be installed on
    your new compute hosts. This method requires that you provided a &cloudos;
    ISO during the initial installation of your cloud, following the
    instructions at <xref linkend="sles_overview"/>.
   </para>
   <para>
    If you want to use the provided Ansible playbooks and Cobbler to setup and
    configure your &slsa; hosts and you did not have the &cloudos; ISO on your
    &lcm; during your initial installation then ensure you look at
    the note at the top of that section before proceeding.
   </para>
  </listitem>
 </orderedlist>
 <section xml:id="idg-all-operations-maintenance-compute-add_sles_compute-xml-5">
  <title>Prerequisites</title>
  <para>
   You need to ensure your input model files are properly setup for &slsa;
   compute host clusters. This must be done during the installation process of
   your cloud and is discussed further at <xref linkend="install_sles"/> and
   <xref linkend="sles_compute_model"/>.
  </para>
 </section>
 <section xml:id="idg-all-operations-maintenance-compute-add_sles_compute-xml-6">
  <title>Adding a &slsa; compute node</title>
  <para>
   <emphasis role="bold">Adding pre-installed &slsa; compute hosts</emphasis>
  </para>
  <para>
   This method requires that you have &cloudos; pre-installed on the
   baremetal host prior to beginning these steps.
  </para>
  <orderedlist>
   <listitem>
    <para>
     Ensure you have &cloudos; pre-installed on your baremetal host.
    </para>
   </listitem>
   <listitem>
    <para>
     Log in to the &lcm;.
    </para>
   </listitem>
   <listitem>
    <para>
     Edit your <literal>~/openstack/my_cloud/definition/data/servers.yml</literal>
     file to include the details about your new compute host(s).
    </para>
    <para>
     For example, if you already had a cluster of three &slsa; compute hosts
     using the <literal>SLES-COMPUTE-ROLE</literal> role and needed to add a
     fourth one you would add your details to the bottom of the file in the
     format. Note that we left out the IPMI details because they will not be
     needed since you pre-installed the &slsa; OS on your host(s).
    </para>
<screen>- id: compute4
  ip-addr: 192.168.102.70
  role: SLES-COMPUTE-ROLE
  server-group: RACK1  </screen>
    <para>
     You can find detailed descriptions of these fields in
     <xref linkend="configobj_servers"/>. Ensure that you use the same role for
     any new &slsa; hosts you are adding as you specified on your existing &slsa;
     hosts.
    </para>
    <important>
     <para>
      You will need to verify that the <literal>ip-addr</literal> value you
      choose for this host does not conflict with any other IP address in your
      cloud environment. You can confirm this by checking the
      <literal>~/openstack/my_cloud/info/address_info.yml</literal> file on your
      &lcm;.
     </para>
    </important>
   </listitem>
   <listitem>
    <para>
     In your
     <literal>~/openstack/my_cloud/definition/data/control_plane.yml</literal>
     file you will need to check the values for
     <literal>member-count</literal>, <literal>min-count</literal>, and
     <literal>max-count</literal>. If you specified them, ensure that they
     match up with your new total node count. For example, if you had
     previously specified <literal>member-count: 3</literal> and are adding a
     fourth compute node, you will need to change that value to
     <literal>member-count: 4</literal>.
    </para>
    <para>
     See for <xref linkend="configobj_controlplane"/> more details.
    </para>
   </listitem>
   <listitem>
    <para>
     Commit the changes to git:
    </para>
<screen>git add -A
git commit -a -m "Add node &lt;name&gt;"</screen>
   </listitem>
   <listitem>
    <para>
     Run the configuration processor and resolve any errors that are indicated:
    </para>
<screen>cd ~/openstack/ardana/ansible
ansible-playbook -i hosts/localhost config-processor-run.yml</screen>
   </listitem>
   <listitem>
    <para>
     Update your deployment directory:
    </para>
<screen>cd ~/openstack/ardana/ansible
ansible-playbook -i hosts/localhost ready-deployment.yml</screen>
    <para>
     Before proceeding, you may want to take a look at
     <emphasis role="bold">info/server_info.yml</emphasis> to see if the
     assignment of the node you have added is what you expect. It may not be,
     as nodes will not be numbered consecutively if any have previously been
     removed. This is to prevent loss of data; the config processor retains
     data about removed nodes and keeps their ID numbers from being
     reallocated. See <xref linkend="persistedserverallocations"/> for
     information on how this works.
    </para>
   </listitem>
   <listitem>
    <para>
     [OPTIONAL] - Run the <literal>wipe_disks.yml</literal> playbook to ensure
     all of your partitions on your nodes are completely wiped prior to
     continuing with the installation.
    </para>
    <note>
     <para>
      The <filename>wipe_disks.yml</filename> playbook is only meant to be run
      on systems immediately after running
      <filename>bm-reimage.yml</filename>. If used for any other case, it may
      not wipe all of the expected partitions.
     </para>
    </note>
    <para>
     The location of <literal>hostname</literal> is
     <filename>~/scratch/ansible/next/ardana/ansible/hosts</filename>.
    </para>
<screen>cd ~/scratch/ansible/next/ardana/ansible/
ansible-playbook -i hosts/verb_hosts wipe_disks.yml --limit &lt;hostname&gt;</screen>
   </listitem>
   <listitem>
    <para>
     Complete the compute host deployment with this playbook:
    </para>
<screen>cd ~/scratch/ansible/next/ardana/ansible/
ansible-playbook -i hosts/verb_hosts site.yml --tag "generate_hosts_file"
ansible-playbook -i hosts/verb_hosts site.yml --limit &lt;hostname&gt;</screen>
   </listitem>
  </orderedlist>
  <para>
   <emphasis role="bold">Adding &slsa; compute hosts with Ansible playbooks and
   Cobbler</emphasis>
  </para>
  <para>
   These steps will show you how to add the new &slsa; compute host to your
   <literal>servers.yml</literal> file and then run the playbooks that update
   your cloud configuration. You will run these playbooks from the lifecycle
   manager.
  </para>
  <para>
   If you did not have the &cloudos; ISO available on your &lcm;
   during your initial installation, it must be installed before proceeding
   further. Instructions can be found in <xref linkend="install_sles_compute"/>.
  </para>
  <para>
   When you are prepared to continue, use these steps:
  </para>
  <procedure>
   <step>
    <para>
     Log in to your &lcm;.
    </para>
   </step>
   <step>
    <para>
     Checkout the <literal>site</literal> branch of your local git so you can
     begin to make the necessary edits:
    </para>
<screen>cd ~/openstack/my_cloud/definition/data
git checkout site</screen>
   </step>
   <step>
    <para>
     Edit your <literal>~/openstack/my_cloud/definition/data/servers.yml</literal>
     file to include the details about your new compute host(s).
    </para>
    <para>
     For example, if you already had a cluster of three &slsa; compute hosts
     using the <literal>SLES-COMPUTE-ROLE</literal> role and needed to add a
     fourth one you would add your details to the bottom of the file in this
     format:
    </para>
<screen>- id: compute4
  ip-addr: 192.168.102.70
  role: SLES-COMPUTE-ROLE
  server-group: RACK1
  mac-addr: e8:39:35:21:32:4e
  ilo-ip: 10.1.192.36
  ilo-password: password
  ilo-user: admin
  distro-id: sles12sp4-x86_64</screen>
    <para>
     You can find detailed descriptions of these fields in
     <xref linkend="configobj_servers"/>. Ensure that you use the same role for
     any new &slsa; hosts you are adding as you specified on your existing &slsa;
     hosts.
    </para>
    <important>
     <para>
      You will need to verify that the <literal>ip-addr</literal> value you
      choose for this host does not conflict with any other IP address in your
      cloud environment. You can confirm this by checking the
      <literal>~/openstack/my_cloud/info/address_info.yml</literal> file on your
      &lcm;.
     </para>
    </important>
   </step>
   <step>
    <para>
     In your
     <literal>~/openstack/my_cloud/definition/data/control_plane.yml</literal>
     file you will need to check the values for
     <literal>member-count</literal>, <literal>min-count</literal>, and
     <literal>max-count</literal>. If you specified them, ensure that they
     match up with your new total node count. For example, if you had
     previously specified <literal>member-count: 3</literal> and are adding a
     fourth compute node, you will need to change that value to
     <literal>member-count: 4</literal>.
    </para>
    <para>
     See <xref linkend="configobj_controlplane"/> for more details.
    </para>
   </step>
   <step>
    <para>
     Commit the changes to git:
    </para>
<screen>git add -A
git commit -a -m "Add node &lt;name&gt;"</screen>
   </step>
   <step>
    <para>
     Run the configuration processor and resolve any errors that are indicated:
    </para>
<screen>cd ~/openstack/ardana/ansible
ansible-playbook -i hosts/localhost config-processor-run.yml</screen>
   </step>
   <step>
    <para>
     Add the new node into Cobbler:
    </para>
<screen>cd ~/openstack/ardana/ansible
ansible-playbook -i hosts/localhost cobbler-deploy.yml</screen>
   </step>
   <step>
    <para>
     Run the following playbook, ensuring that you specify only your UEFI
     &slsa; nodes using the nodelist. This playbook will reconfigure Cobbler
     for the nodes listed.
    </para>
    <screen>&prompt.ardana;cd ~/scratch/ansible/next/ardana/ansible
&prompt.ardana;ansible-playbook prepare-sles-grub2.yml -e nodelist=node1[,node2,node3]</screen>
   </step>
   <step>
    <para>
     Then you can image the node:
    </para>
<screen>cd ~/openstack/ardana/ansible
ansible-playbook -i hosts/localhost bm-reimage.yml -e nodelist=&lt;node name&gt;</screen>
     <note>
      <para>
       If you do not know the <literal>&lt;node name&gt;</literal>, you can
       get it by using <command>sudo cobbler system list</command>.
      </para>
     </note>
    <para>
     Before proceeding, you may want to take a look at
     <emphasis role="bold">info/server_info.yml</emphasis> to see if the
     assignment of the node you have added is what you expect. It may not be,
     as nodes will not be numbered consecutively if any have previously been
     removed. This is to prevent loss of data; the config processor retains
     data about removed nodes and keeps their ID numbers from being
     reallocated. See <xref linkend="persistedserverallocations"/> for
     information on how this works.
    </para>
   </step>
   <step>
    <para>
     Update your deployment directory:
    </para>
<screen>cd ~/openstack/ardana/ansible
ansible-playbook -i hosts/localhost ready-deployment.yml</screen>
   </step>
   <step>
    <para>
     [OPTIONAL] - Run the <literal>wipe_disks.yml</literal> playbook to ensure
     all of your partitions on your hosts are completely wiped prior to
     continuing with the installation. The <filename>wipe_disks.yml</filename>
     playbook is only meant to be run on systems immediately after running
     <filename>bm-reimage.yml</filename>. If used for any other case, it may
     not wipe all of the expected partitions.
    </para>
<screen>cd ~/scratch/ansible/next/ardana/ansible/
ansible-playbook -i hosts/verb_hosts wipe_disks.yml --limit &lt;hostname&gt;</screen>
    <note>
     <para>
      You can obtain the <literal>&lt;hostname&gt;</literal> from the file
      <filename>~/scratch/ansible/next/ardana/ansible/hosts/verb_hosts</filename>.
     </para>
    </note>
   </step>
   <step>
    <para>
     You should verify that the netmask, bootproto, and other necessary
     settings are correct and if they are not then re-do them. See
     <xref linkend="install_sles_compute"/> for details.
    </para>
   </step>
   <step>
    <para>
     Complete the compute host deployment with these playbooks. For the last
     one, ensure you specify the compute hosts you are added with the
     <literal>--limit</literal> switch:
    </para>
<screen>cd ~/scratch/ansible/next/ardana/ansible/
ansible-playbook -i hosts/verb_hosts site.yml --tag "generate_hosts_file"
ansible-playbook -i hosts/verb_hosts site.yml --limit &lt;hostname&gt;</screen>
   </step>
  </procedure>
 </section>
 <section xml:id="idg-all-operations-maintenance-compute-add_sles_compute-xml-8">
  <title>Adding a new &slsa; compute node to monitoring</title>
  <para>
   If you want to add a new Compute node to the monitoring service checks,
   there is an additional playbook that must be run to ensure this happens:
  </para>
<screen>cd ~/scratch/ansible/next/ardana/ansible
ansible-playbook -i hosts/verb_hosts monasca-deploy.yml --tags "active_ping_checks"</screen>
 </section>
</section>
