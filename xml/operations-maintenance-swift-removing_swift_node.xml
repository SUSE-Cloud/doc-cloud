<?xml version="1.0"?>
<?xml-stylesheet href="urn:x-suse:xslt:profiling:docbook51-profile.xsl"
 type="text/xml"
 title="Profiling step"?>
<!DOCTYPE section [
 <!ENTITY % entities SYSTEM "entities.ent"> %entities;
]>
<section xmlns="http://docbook.org/ns/docbook" xmlns:xi="http://www.w3.org/2001/XInclude" xmlns:xlink="http://www.w3.org/1999/xlink" xml:id="remove_swift_node">
 <title>Removing a Swift Node</title>
 <para>
  Removal process for both Swift Object and PAC nodes.
 </para>
 <para>
  You can use this process when you want to remove one or more Swift nodes
  permanently. This process applies to both Swift Proxy, Account, Container
  (PAC) nodes and Swift Object nodes.
 </para>
 <section xml:id="idg-all-operations-maintenance-swift-removing_swift_node-xml-6">
  <title>Setting the Pass-through Attributes</title>
  <para>
   This process will remove the Swift node's drives from the rings and move it
   to the remaining nodes in your cluster.
  </para>
  <orderedlist>
   <listitem>
    <para>
     Log in to the &lcm;.
    </para>
   </listitem>
   <listitem>
    <para>
     Ensure that the weight-step attribute is set. See
     <xref linkend="swift_weight_att"/> for more details.
    </para>
   </listitem>
   <listitem>
    <para>
     Add the pass-through definition to your input model, specifying the server
     ID (as opposed to the server name). It is easiest to include in your
     <literal>~/openstack/my_cloud/definition/data/servers.yml</literal> file
     since your server IDs are already listed in that file. For more
     information about pass-through, see <xref linkend="passthrough"/>.
    </para>
    <para>
     Here is the general format:
    </para>
<screen>
pass-through:
  servers:
    - id: &lt;server-id&gt;
      data:
          &lt;subsystem&gt;:
                &lt;subsystem-attributes&gt;</screen>
    <para>
     Here is an example:
    </para>
<screen>
---
  product:
    version: 2

  <emphasis role="bold">pass-through:
    servers:
      - id: ccn-0001
        data:
          swift:
            drain: yes</emphasis></screen>
    <para>
     By setting this pass-through attribute, you indicate that the system
     should reduce the weight of the server's drives. The weight reduction is
     determined by the weight-step attribute as described in the previous step.
     This process is known as "draining", where you remove the Swift data from
     the node in preparation for removing the node.
    </para>
   </listitem>
   <listitem>
    <para>
     Commit your configuration to the local Git repository (see
     <xref linkend="using_git"/>), as follows:
    </para>
<screen>cd ~/openstack/ardana/ansible
git add -A
git commit -m "My config or other commit message"</screen>
   </listitem>
   <listitem>
    <para>
     Run the configuration processor:
    </para>
<screen>cd ~/openstack/ardana/ansible
ansible-playbook -i hosts/localhost config-processor-run.yml</screen>
   </listitem>
   <listitem>
    <para>
     Use the playbook to create a deployment directory:
    </para>
<screen>cd ~/openstack/ardana/ansible
ansible-playbook -i hosts/localhost ready-deployment.yml</screen>
   </listitem>
   <listitem>
    <para>
     Run the Swift deploy playbook to perform the first ring rebuild. This will
     remove some of the partitions from all drives on the node you are
     removing:
    </para>
<screen>cd ~/scratch/ansible/next/ardana/ansible
ansible-playbook -i hosts/verb_hosts swift-deploy.yml</screen>
   </listitem>
   <listitem>
    <para>
     Wait until the replication has completed. For further details, see
     <xref linkend="topic_ohx_j1t_4t"/>
    </para>
   </listitem>
   <listitem>
    <para>
     Determine whether all of the partitions have been removed from all drives
     on the Swift node you are removing. You can do this by SSH'ing into the
     first account server node and using these commands:
    </para>
<screen>cd /etc/swiftlm/cloud1/cp1/builder_dir/
sudo swift-ring-builder &lt;ring_name&gt;.builder</screen>
    <para>
     For example, if the node you are removing was part of the object-o ring
     the command would be:
    </para>
<screen>sudo swift-ring-builder object-0.builder</screen>
    <para>
     Check the output. You will need to know the IP address of the server being
     drained. In the example below, the number of partitions of the drives on
     192.168.245.3 has reached zero for the object-0 ring:
    </para>
<screen>$ cd /etc/swiftlm/cloud1/cp1/builder_dir/
$ sudo swift-ring-builder object-0.builder
account.builder, build version 6
4096 partitions, 3.000000 replicas, 1 regions, 1 zones, 6 devices, 0.00 balance, 0.00 dispersion
The minimum number of hours before a partition can be reassigned is 16
The overload factor is 0.00% (0.000000)
Devices:    id  region  zone      ip address  port  replication ip  replication port      name weight partitions balance meta
             0       1     1   192.168.245.3  6002   192.168.245.3              6002     disk0   0.00          0   -0.00 padawan-ccp-c1-m1:disk0:/dev/sdc
             1       1     1   192.168.245.3  6002   192.168.245.3              6002     disk1   0.00          0   -0.00 padawan-ccp-c1-m1:disk1:/dev/sdd
             2       1     1   192.168.245.4  6002   192.168.245.4              6002     disk0  18.63       2048   -0.00 padawan-ccp-c1-m2:disk0:/dev/sdc
             3       1     1   192.168.245.4  6002   192.168.245.4              6002     disk1  18.63       2048   -0.00 padawan-ccp-c1-m2:disk1:/dev/sdd
             4       1     1   192.168.245.5  6002   192.168.245.5              6002     disk0  18.63       2048   -0.00 padawan-ccp-c1-m3:disk0:/dev/sdc
             5       1     1   192.168.245.5  6002   192.168.245.5              6002     disk1  18.63       2048   -0.00 padawan-ccp-c1-m3:disk1:/dev/sdd</screen>
   </listitem>
   <listitem>
    <para>
     If the number of partitions is zero for the server on all rings, you can
     move to the next step, otherwise continue the ring rebalance cycle by
     repeating steps 7-9 until the weight has reached zero.
    </para>
   </listitem>
   <listitem>
    <para>
     If the number of partitions is zero for the server on all rings, you can
     remove the Swift nodes' drives from all rings. Edit the pass-through data
     you created in step #3 and set the <literal>remove</literal> attribute as
     shown in this example:
    </para>
<screen>---
  product:
    version: 2

  pass-through:
    servers:
      - id: ccn-0001
        data:
          swift:
            <emphasis role="bold">remove: yes</emphasis></screen>
   </listitem>
   <listitem>
    <para>
     Commit your configuration to the local Git repository (see
     <xref linkend="using_git"/>), as follows:
    </para>
<screen>cd ~/openstack/ardana/ansible
git add -A
git commit -m "My config or other commit message"</screen>
   </listitem>
   <listitem>
    <para>
     Run the configuration processor:
    </para>
<screen>cd ~/openstack/ardana/ansible
ansible-playbook -i hosts/localhost config-processor-run.yml</screen>
   </listitem>
   <listitem>
    <para>
     Update your deployment directory:
    </para>
<screen>cd ~/openstack/ardana/ansible
ansible-playbook -i hosts/localhost ready-deployment.yml</screen>
   </listitem>
   <listitem>
    <para>
     Run the Swift deploy playbook to rebuild the rings by removing the server:
    </para>
<screen>cd ~/scratch/ansible/next/ardana/ansible
ansible-playbook -i hosts/verb_hosts swift-deploy.yml</screen>
   </listitem>
   <listitem>
    <para>
     At this stage, the server has been removed from the rings and the data
     that was originally stored on the server has been replicated in a balanced
     way to the other servers in the system. You can proceed to the next phase.
    </para>
   </listitem>
  </orderedlist>
 </section>
 <section xml:id="sec.swift.disable-node">
  <title>To Disable Swift on a Node</title>
  <para>
   The next phase in this process will disable the Swift service on the node.
   In this example, <emphasis role="bold">swobj4</emphasis> is the node being
   removed from Swift.
  </para>
  <orderedlist>
   <listitem>
    <para>
     Log in to the &lcm;.
    </para>
   </listitem>
   <listitem>
    <para>
     Stop Swift services on the node using the
     <literal>swift-stop.yml</literal> playbook:
    </para>
<screen>cd ~/scratch/ansible/next/ardana/ansible
ansible-playbook -i hosts/verb_hosts swift-stop.yml --limit <emphasis role="bold">&lt;hostname&gt;</emphasis></screen>
    <note>
     <para>
      When using the <literal>--limit</literal> argument, you must specify the
      full hostname (for example: <emphasis>ardana-cp1-swobj0004</emphasis>) or
      use the wild card <literal>*</literal> (for example,
      <emphasis>*swobj4*</emphasis>).
     </para>
    </note>
    <para>
     The following example uses the <literal>swift-stop.yml</literal> playbook
     to stop Swift services on
     <emphasis role="bold">ardana-cp1-swobj0004</emphasis>:
    </para>
<screen>cd ~/scratch/ansible/next/ardana/ansible
ansible-playbook -i hosts/verb_hosts swift-stop.yml --limit <emphasis role="bold">ardana-cp1-swobj0004</emphasis></screen>
   </listitem>
   <listitem>
    <para>
     Remove the configuration files.
    </para>
<screen>ssh ardana-cp1-swobj4-mgmt sudo rm -R /etc/swift</screen>
    <note>
     <para>
      Do not run any other playbooks until you have finished the process
      described in <xref linkend="sec.swift.remove-node-model"/>. Otherwise,
      these playbooks may recreate <filename>/etc/swift</filename> and
      restart Swift on <emphasis>swobj4</emphasis>. If you accidentally run a
      playbook, repeat the process in <xref linkend="sec.swift.disable-node"/>.
     </para>
    </note>
   </listitem>
  </orderedlist>
 </section>
 <section xml:id="sec.swift.remove-node-model">
  <title>To Remove a Node from the Input Model</title>
  <para>
   Use the following steps to finish the process of removing the Swift node.
  </para>
  <orderedlist>
   <listitem>
    <para>
     Log in to the &lcm;.
    </para>
   </listitem>
   <listitem>
    <para>
     Edit the <literal>~/openstack/my_cloud/definition/data/servers.yml</literal>
     file and remove the entry for the node
     (<emphasis role="bold">swobj4</emphasis> in this example).
    </para>
   </listitem>
   <listitem>
    <para>
     If this was a SWPAC node, reduce the member-count attribute by 1 in the
     <literal>~/openstack/my_cloud/definition/data/control_plane.yml</literal>
     file. For SWOBJ nodes, no such action is needed.
    </para>
   </listitem>
   <listitem>
    <para>
     Commit your configuration to the local Git repository (see
     <xref linkend="using_git"/>), as follows:
    </para>
<screen>cd ~/openstack/ardana/ansible
git add -A
git commit -m "My config or other commit message"</screen>
   </listitem>
   <listitem>
    <para>
     Run the configuration processor:
    </para>
<screen>cd ~/openstack/ardana/ansible
ansible-playbook -i hosts/localhost config-processor-run.yml</screen>
    <para>
     You may want to use the <literal>remove_deleted_servers</literal> and
     <literal>free_unused_addresses</literal> switches to free up the resources
     when running the configuration processor. For more information, see
     <xref linkend="persisteddata"/>.
    </para>
<screen>ansible-playbook -i hosts/localhost config-processor-run.yml -e remove_deleted_servers="y" -e free_unused_addresses="y"</screen>
   </listitem>
   <listitem>
    <para>
     Update your deployment directory:
    </para>
<screen>cd ~/openstack/ardana/ansible
ansible-playbook -i hosts/localhost ready-deployment.yml</screen>
   </listitem>
   <listitem>
    <para>
     Validate the changes you have made to the configuration files using the
     playbook below before proceeding further:
    </para>
<screen>cd ~/scratch/ansible/next/ardana/ansible
ansible-playbook -i hosts/verb_hosts swift-compare-model-rings.yml --limit SWF*</screen>
    <para>
     If any errors occur, correct them in your configuration files and repeat
     steps 3-5 again until no more errors occur before going to the next step.
    </para>
    <para>
     For more details on how to interpret and resolve errors, see
     <xref linkend="sec.input-swift.error"/>
    </para>
   </listitem>
   <listitem>
    <para>
     Remove the node from Cobbler:
    </para>
<screen>sudo cobbler system remove --name=swobj4</screen>
   </listitem>
   <listitem>
    <para>
     Run the Cobbler deploy playbook:
    </para>
<screen>cd ~/openstack/ardana/ansible
ansible-playbook -i hosts/localhost cobbler-deploy.yml</screen>
   </listitem>
   <listitem>
    <para>
     The final step will depend on what type of Swift node you are removing.
    </para>
    <para>
     If the node was a SWPAC node, run the <literal>ardana-deploy.yml</literal>
     playbook:
    </para>
<screen>cd ~/scratch/ansible/next/ardana/ansible
ansible-playbook -i hosts/verb_hosts ardana-deploy.yml</screen>
    <para>
     If the node was a SWOBJ node, run the <literal>swift-deploy.yml</literal>
     playbook:
    </para>
<screen>cd ~/scratch/ansible/next/ardana/ansible
ansible-playbook -i hosts/verb_hosts swift-deploy.yml</screen>
   </listitem>
   <listitem>
    <para>
     Wait until replication has finished. For more details, see
     <xref linkend="topic_ohx_j1t_4t"/>.
    </para>
   </listitem>
   <listitem>
    <para>
     You may need to continue to rebalance the rings. For instructions, see
     <emphasis role="bold">Final Rebalance Phase </emphasis> at
     <xref linkend="change_swift_rings"/>.
    </para>
   </listitem>
  </orderedlist>
 </section>
 <section xml:id="idg-all-operations-maintenance-swift-removing_swift_node-xml-9">
  <title>Remove the Swift Node from Monitoring</title>
  <para>
   Once you have removed the Swift node(s), the alarms against them will
   trigger so there are additional steps to take to resolve this issue.
  </para>
  <para>
   You will want to SSH to each of the Monasca API servers and edit the
   <literal>/etc/monasca/agent/conf.d/host_alive.yaml</literal> file to remove
   references to the Swift node(s) you removed. This will require
   <literal>sudo</literal> access.
  </para>
  <para>
   Once you have removed the references on each of your Monasca API servers you
   then need to restart the monasca-agent on each of those servers with this
   command:
  </para>
<screen>&prompt.sudo;service openstack-monasca-agent restart</screen>
  <para>
   With the Swift node references removed and the monasca-agent restarted, you
   can then delete the corresponding alarm to finish this process. To do so we
   recommend using the Monasca CLI which should be installed on each of your
   Monasca API servers by default:
  </para>
<screen>monasca alarm-list --metric-dimensions hostname=&lt;swift node deleted&gt;</screen>
  <para>
   You can then delete the alarm with this command:
  </para>
<screen>monasca alarm-delete &lt;alarm ID&gt;</screen>
 </section>
</section>
