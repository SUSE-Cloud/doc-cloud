<?xml version="1.0"?>
<!DOCTYPE section [
 <!ENTITY % entities SYSTEM "entity-decl.ent"> %entities;
]>
<section xmlns="http://docbook.org/ns/docbook" xmlns:xi="http://www.w3.org/2001/XInclude"
  xmlns:xlink="http://www.w3.org/1999/xlink" xml:id="other_alarmdefinitions">
 <title>Other Services Alarms</title>
 <para>
  These alarms show under the Other Services section of the &productname;
  &opscon;.
 </para>
 <informaltable colsep="1" rowsep="1">
  <tgroup cols="5">
   <colspec colname="c1" colnum="1"/>
   <colspec colname="c2" colnum="2"/>
   <colspec colname="c3" colnum="3"/>
   <colspec colname="c4" colnum="4"/>
   <colspec colname="c5" colnum="5"/>
   <thead>
    <row>
     <entry>Service</entry>
     <entry>Alarm Name</entry>
     <entry>Description</entry>
     <entry>Likely Cause</entry>
     <entry>Mitigation Tasks to Perform</entry>
    </row>
   </thead>
   <tbody>
    <row>
     <entry morerows="2">apache</entry>
     <entry>Apache Status</entry>
     <entry>Alarms on failure to reach the Apache status endpoint.</entry>
     <entry/>
     <entry/>
    </row>
    <row>
     <entry>Process Check</entry>
     <entry>
      <para>
       Alarms when the specified process is not running.
      </para>
<screen>process_name = apache2</screen>
     </entry>
     <entry/>
     <entry>
      <para>
       If the Apache process goes down, connect to the affected node via SSH
       and restart it with this command:
      </para>
<screen>sudo systemctl restart apache2</screen>
     </entry>
    </row>
    <row>
     <entry>Apache Idle Worker Count</entry>
     <entry>Alarms when there are no idle workers in the Apache server.</entry>
     <entry/>
     <entry/>
    </row>
    <row>
     <entry morerows="2">backup</entry>
     <entry>Process Check</entry>
     <entry>
      <para>
       Alarms when the specified process is not running.
      </para>
<screen>process_name = freezer-scheduler</screen>
     </entry>
     <entry>Process crashed.</entry>
     <entry>Restart the process on the affected node. Review the associated logs.</entry>
    </row>
    <row>
     <entry>HTTP Status</entry>
     <entry>
      <para>
       Alarms when the specified HTTP endpoint is down or not reachable.
      </para>
<screen>process_name = freezer-api</screen>
     </entry>
     <entry/>
     <entry/>
    </row>
    <row>
     <entry>Service Log Directory Size</entry>
     <entry>Service log directory consuming more disk than its quota.</entry>
     <entry>The service log directory, as indicated by the <literal>path</literal> dimension,
            is over the 2.5 GB quota.</entry>
     <entry>Find the service that is consuming too much disk space. Look at the logs. If
              <literal>DEBUG</literal> log entries exist, set the logging level to
              <literal>INFO</literal>. If the logs are repeatedly logging an error message, do what
            is needed to resolve the error. If old log files exist, configure log rotate to remove
            them. You could also choose to remove old log files by hand after backing them up if
            needed.</entry>
    </row>
<!---->
    <row>
     <entry>haproxy</entry>
     <entry>Process Check</entry>
     <entry>
      <para>
       Alarms when the specified process is not running.
      </para>
<screen>process_name = haproxy</screen>
     </entry>
     <entry>HA Proxy is not running on this machine.</entry>
     <entry>
      <para>
       Restart the process on the affected node using these steps:
      </para>
      <orderedlist xml:id="ol_krq_lzp_mx">
       <listitem>
        <para>
         Log in to the &clm;.
        </para>
       </listitem>
       <listitem>
        <para>
         Use this playbook against the affected node:
        </para>
<screen>cd ~/scratch/ansible/next/ardana/ansible
ansible-playbook -i hosts/verb_hosts FND-CLU-start.yml --limit &lt;hostname&gt;</screen>
       </listitem>
      </orderedlist>
      <para>
       Review the associated logs.
      </para>
     </entry>
    </row>
<!---->
    <row>
     <entry>ardana-ux-services</entry>
     <entry>HTTP Status</entry>
     <entry>Alarms when the specified HTTP endpoint is down or not reachable.</entry>
     <entry/>
     <entry/>
    </row>
<!---->
    <row>
     <entry morerows="3">key-manager</entry>
     <entry>Process Check</entry>
     <entry>
      <para>
       Alarms when the specified process is not running.
      </para>
<screen>process_name = barbican-api</screen>
     </entry>
     <entry>Process has crashed.</entry>
     <entry>
      <para>
       Restart the process on the affected node using these steps:
      </para>
      <orderedlist xml:id="ol_mrq_lzp_mx">
       <listitem>
        <para>
         Log in to the &clm;.
        </para>
       </listitem>
       <listitem>
        <para>
         Use the Barbican start playbook against the affected node:
        </para>
<screen>cd ~/scratch/ansible/next/ardana/ansible
ansible-playbook -i hosts/verb_hosts barbican-start.yml --limit &lt;hostname&gt;</screen>
       </listitem>
      </orderedlist>
     </entry>
    </row>
    <row>
     <entry>HTTP Status</entry>
     <entry>
      <para>
       Alarms when the specified HTTP endpoint is down or not reachable.
      </para>
<screen>component = barbican-api
api_endpoint = public or internal</screen>
     </entry>
     <entry>The endpoint is not responsive, it may be down.</entry>
     <entry>
      <para>
       For the HTTP Status alarms for the public and internal endpoints,
       restart the process on the affected node using these steps:
      </para>
      <orderedlist xml:id="ol_nrq_lzp_mx">
       <listitem>
        <para>
         Log in to the &clm;.
        </para>
       </listitem>
       <listitem>
        <para>
         Stop the barbican service:
        </para>
<screen>cd ~/scratch/ansible/next/ardana/ansible
ansible-playbook -i hosts/verb_hosts barbican-stop.yml --limit &lt;hostname&gt;</screen>
       </listitem>
       <listitem>
        <para>
         Restart the barbican service back up:
        </para>
<screen>cd ~/scratch/ansible/next/ardana/ansible
ansible-playbook -i hosts/verb_hosts barbican-start.yml --limit &lt;hostname&gt;</screen>
       </listitem>
      </orderedlist>
      <para>
       Examine the logs <literal>/var/log/barbican/</literal> for possible
       error messages.
      </para>
     </entry>
    </row>
    <row>
     <entry>HTTP Status</entry>
     <entry>
      <para>
       Alarms when the specified HTTP endpoint is down or not reachable.
      </para>
<screen>component = barbican-api
monitored_host_type = vip</screen>
     </entry>
     <entry>The Barbican API on the admin virtual IP is down.</entry>
     <entry>This alarm is verifying access to the Barbican API via the virtual IP address
            (HAProxy). If this check is failing but the other two HTTP Status alarms for the
            key-manager service are not then the issue is likely with HAProxy so you should view the
            alarms for that service. If the other two HTTP Status alarms are alerting as well then
            restart Barbican using the steps listed.</entry>
    </row>
    <row>
     <entry>Service Log Directory Size</entry>
     <entry>Service log directory consuming more disk than its quota.</entry>
     <entry>The service log directory, as indicated by the <literal>path</literal> dimension,
            is over the 2.5 GB quota.</entry>
     <entry>Find the service that is consuming too much disk space. Look at the logs. If
              <literal>DEBUG</literal> log entries exist, set the logging level to
              <literal>INFO</literal>. If the logs are repeatedly logging an error message, do what
            is needed to resolve the error. If old log files exist, configure log rotate to remove
            them. You could also choose to remove old log files by hand after backing them up if
            needed.</entry>
    </row>
<!---->
    <row>
     <entry morerows="1">mysql</entry>
     <entry>MySQL Slow Query Rate</entry>
     <entry>Alarms when the slow query rate is high.</entry>
     <entry>The system load is too high.</entry>
     <entry>This could be an indication of near capacity limits or an exposed bad query. First,
            check overall system load and then investigate MySQL details.</entry>
    </row>
    <row>
     <entry>Process Check</entry>
     <entry>
      <para>
       Alarms when the specified process is not running.
      </para>
<screen>process_name = mysqld</screen>
     </entry>
     <entry>MySQL crashed.</entry>
     <entry>Restart MySQL on the affected node.</entry>
    </row>
<!---->
    <row>
     <entry morerows="2">octavia</entry>
     <entry>Process Check</entry>
     <entry>
      <para>
       Alarms when the specified process is not running. There are individual
       alarms for each of these processes:
      </para>
      <itemizedlist xml:id="ul_orq_lzp_mx">
       <listitem>
        <para>
         octavia-worker
        </para>
       </listitem>
       <listitem>
        <para>
         octavia-housekeeping
        </para>
       </listitem>
       <listitem>
        <para>
         octavia-api
        </para>
       </listitem>
       <listitem>
        <para>
         octavia-health-manager
        </para>
       </listitem>
      </itemizedlist>
     </entry>
     <entry>The process has crashed.</entry>
     <entry>
      <para>
       Restart the process on the affected node using these steps:
      </para>
      <orderedlist xml:id="ol_prq_lzp_mx">
       <listitem>
        <para>
         Log in to the &clm;.
        </para>
       </listitem>
       <listitem>
        <para>
         Use the Octavia start playbook against the affected node:
        </para>
<screen>cd ~/scratch/ansible/next/ardana/ansible
ansible-playbook -i hosts/verb_hosts octavia-start.yml --limit &lt;hostname&gt;</screen>
       </listitem>
      </orderedlist>
     </entry>
    </row>
    <row>
     <entry>HTTP Status</entry>
     <entry>Alarms when the specified HTTP endpoint is down or not reachable.</entry>
     <entry>The <literal>octavia-api </literal>process could be down or you could be
            experiencing an issue with either haproxy or another network related issue.</entry>
     <entry>
      <para>
       If the <literal>octavia-api</literal> process is down, restart it on
       the affected node using these steps:
      </para>
      <orderedlist xml:id="ol_qrq_lzp_mx">
       <listitem>
        <para>
         Log in to the &clm;.
        </para>
       </listitem>
       <listitem>
        <para>
         Use the Octavia start playbook against the affected node:
        </para>
<screen>cd ~/scratch/ansible/next/ardana/ansible
ansible-playbook -i hosts/verb_hosts octavia-start.yml --limit &lt;hostname&gt;</screen>
       </listitem>
      </orderedlist>
      <para>
       If it is not the <literal>octavia-process</literal> that is the issue,
       then check if there is an issue with <literal>haproxy</literal> or
       possibly a network issue and troubleshoot accordingly.
      </para>
     </entry>
    </row>
    <row>
     <entry>Service Log Directory Size</entry>
     <entry>Service log directory consuming more disk than its quota.</entry>
     <entry>The service log directory, as indicated by the <literal>path</literal> dimension,
            is over the 2.5 GB quota.</entry>
     <entry>Find the service that is consuming too much disk space. Look at the logs. If
              <literal>DEBUG</literal> log entries exist, set the logging level to
              <literal>INFO</literal>. If the logs are repeatedly logging an error message, do what
            is needed to resolve the error. If old log files exist, configure log rotate to remove
            them. You could also choose to remove old log files by hand after backing them up if
            needed.</entry>
    </row>
<!---->
    <row>
     <entry morerows="2">orchestration</entry>
     <entry>Process Check</entry>
     <entry>
      <para>
       Alarms when the specified process is not running. There are individual
       alarms for each of these processes:
      </para>
      <itemizedlist xml:id="ul_kvs_yjq_rv">
       <listitem>
        <para>
         heat-api
        </para>
       </listitem>
       <listitem>
        <para>
         heat-api-cfn
        </para>
       </listitem>
       <listitem>
        <para>
         heat-api-cloudwatch
        </para>
       </listitem>
       <listitem>
        <para>
         heat-engine
        </para>
       </listitem>
      </itemizedlist>
      <para>
       heat-api process check on each node
      </para>
     </entry>
     <entry>Process crashed.</entry>
     <entry>
      <para>
       Restart the process with these steps:
      </para>
      <orderedlist xml:id="ol_rrq_lzp_mx">
       <listitem>
        <para>
         Log in to the &clm;.
        </para>
       </listitem>
       <listitem>
        <para>
         Stop all the Heat processes:
        </para>
<screen>cd ~/scratch/ansible/next/ardana/ansible
ansible-playbook -i hosts/verb_hosts heat-start.yml</screen>
       </listitem>
       <listitem>
        <para>
         Start the Heat processes back up:
        </para>
<screen>cd ~/scratch/ansible/next/ardana/ansible
ansible-playbook -i hosts/verb_hosts heat-start.yml</screen>
       </listitem>
      </orderedlist>
      <para>
       Review the relevant log at the following locations on the affected node:
      </para>
<screen>/var/log/heat/heat-api.log
/var/log/heat/heat-cfn.log
/var/log/heat/heat-cloudwatch.log
/var/log/heat/heat-engine.log</screen>
     </entry>
    </row>
    <row>
     <entry>HTTP Status</entry>
     <entry>
      <para>
       Alarms when the specified HTTP endpoint is down or not reachable.
      </para>
      <itemizedlist xml:id="ul_ztx_1kq_rv">
       <listitem>
        <para>
         heat-api
        </para>
       </listitem>
       <listitem>
        <para>
         heat-api-cfn
        </para>
       </listitem>
       <listitem>
        <para>
         heat-api-cloudwatch
        </para>
       </listitem>
      </itemizedlist>
     </entry>
     <entry/>
     <entry>
      <para>
       Restart the Heat service with these steps:
      </para>
      <orderedlist xml:id="ol_srq_lzp_mx">
       <listitem>
        <para>
         Log in to the &clm;.
        </para>
       </listitem>
       <listitem>
        <para>
         Stop all the Heat processes:
        </para>
<screen>cd ~/scratch/ansible/next/ardana/ansible
ansible-playbook -i hosts/verb_hosts heat-start.yml</screen>
       </listitem>
       <listitem>
        <para>
         Start the Heat processes back up:
        </para>
<screen>cd ~/scratch/ansible/next/ardana/ansible
ansible-playbook -i hosts/verb_hosts heat-start.yml</screen>
       </listitem>
      </orderedlist>
      <para>
       Review the relevant log at the following locations on the affected node:
      </para>
<screen>/var/log/heat/heat-api.log
/var/log/heat/heat-cfn.log
/var/log/heat/heat-cloudwatch.log</screen>
     </entry>
    </row>
    <row>
     <entry>Service Log Directory Size</entry>
     <entry>Service log directory consuming more disk than its quota.</entry>
     <entry>The service log directory, as indicated by the <literal>path</literal> dimension,
            is over the 2.5 GB quota.</entry>
     <entry>Find the service that is consuming too much disk space. Look at the logs. If
              <literal>DEBUG</literal> log entries exist, set the logging level to
              <literal>INFO</literal>. If the logs are repeatedly logging an error message, do what
            is needed to resolve the error. If old log files exist, configure log rotate to remove
            them. You could also choose to remove old log files by hand after backing them up if
            needed.</entry>
    </row>
<!---->
    <row>
     <entry>OVSvApp-ServiceVM</entry>
     <entry>Process Check</entry>
     <entry>
      <para>
       Alarms when the specified process is not running.
      </para>
<screen>process_name = ovs-vswitchd
process_name = neutron-ovsvapp-agent
process_name = ovsdb-server</screen>
     </entry>
     <entry>Process has crashed.</entry>
     <entry>Restart process on affected node. Review logs.</entry>
    </row>
<!---->
    <row>
     <entry>rabbitmq</entry>
     <entry>Process Check</entry>
     <entry>
      <para>
       Alarms when the specified process is not running.
      </para>
<screen>process_name = rabbitmq
process_name = epmd</screen>
     </entry>
     <entry>Process has crashed.</entry>
     <entry>Restart process on affected node. Review logs.</entry>
    </row>
    <row>
     <entry>spark</entry>
     <entry>Process Check</entry>
     <entry>
      <para>
       Alarms when the specified process is not running.
      </para>
<screen>process_name = org.apache.spark.deploy.master.Master
process_name = org.apache.spark.deploy.worker.Worker</screen>
     </entry>
     <entry>Process has crashed.</entry>
     <entry>Restart process on affected node. Review logs.</entry>
    </row>
<!---->
    <row>
     <entry morerows="1">web-ui</entry>
     <entry>HTTP Status</entry>
     <entry>Alarms when the specified HTTP endpoint is down or not reachable.</entry>
     <entry>Apache is not running or there is a misconfiguration.</entry>
     <entry>Check that Apache is running; investigate Horizon logs.</entry>
    </row>
    <row>
     <entry>Service Log Directory Size</entry>
     <entry>Service log directory consuming more disk than its quota.</entry>
     <entry>The service log directory, as indicated by the <literal>path</literal> dimension,
            is over the 2.5 GB quota.</entry>
     <entry>Find the service that is consuming too much disk space. Look at the logs. If
              <literal>DEBUG</literal> log entries exist, set the logging level to
              <literal>INFO</literal>. If the logs are repeatedly logging an error message, do what
            is needed to resolve the error. If old log files exist, configure log rotate to remove
            them. You could also choose to remove old log files by hand after backing them up if
            needed.</entry>
    </row>
<!---->
    <row>
     <entry morerows="1">zookeeper</entry>
     <entry>Process Check</entry>
     <entry>
      <para>
       Alarms when the specified process is not running.
      </para>
<screen>process_name = org.apache.zookeeper.server</screen>
     </entry>
     <entry>Process crashed.</entry>
     <entry>Restart the process on the affected node. Review the associated logs.</entry>
    </row>
    <row>
     <entry>ZooKeeper Latency</entry>
     <entry>Alarms when the ZooKeeper latency is high.</entry>
     <entry>Heavy system load.</entry>
     <entry>Check the individual system as well as activity across the entire service.</entry>
    </row>
   </tbody>
  </tgroup>
 </informaltable>
</section>
