<?xml version="1.0"?>
<?xml-stylesheet href="urn:x-suse:xslt:profiling:docbook51-profile.xsl"
 type="text/xml"
 title="Profiling step"?>
<!DOCTYPE section [
 <!ENTITY % entities SYSTEM "entities.ent"> %entities;
]>
<section xmlns="http://docbook.org/ns/docbook" xmlns:xi="http://www.w3.org/2001/XInclude" xmlns:xlink="http://www.w3.org/1999/xlink" xml:id="sec.esx.remove-esxi-host">
 <title>Removing an ESXi Host from a Cluster</title>
 <para>
  This topic describes how to remove an existing ESXi host from a cluster and
  clean up of services for OVSvAPP VM.
 </para>
 <note>
  <para>
   Before performing this procedure, wait until VCenter migrates all the tenant
   VMs to other active hosts in that same cluster.
  </para>
 </note>
 <section xml:id="sec.esxi-host.pre">
  <title>Prerequisite</title>
  <para>
   Write down the Hostname and ESXi configuration IP addresses of OVSvAPP VMs of
   that ESX cluster before deleting the VMs. These IP address and Hostname will
   be used to clean up Monasca alarm definitions.
  </para>
  <orderedlist>
   <listitem>
    <para>
     Login to vSphere client.
    </para>
   </listitem>
   <listitem>
    <para>
     Select the ovsvapp node running on the ESXi host and click
     <emphasis role="bold">Summary</emphasis> tab.
    </para>
   </listitem>
  </orderedlist>
 </section>
 <section>
  <title>Procedure</title>
  <orderedlist>
   <listitem>
    <para>
     Right-click and put the host in the maintenance mode. This will
     automatically migrate all the tenant VMs except OVSvApp.
    </para>
    <informalfigure>
     <mediaobject>
      <imageobject role="fo">
       <imagedata fileref="media-esx-eon_maintenance.png" width="75%" format="PNG"/>
      </imageobject>
      <imageobject role="html">
       <imagedata fileref="media-esx-eon_maintenance.png"/>
      </imageobject>
     </mediaobject>
    </informalfigure>
   </listitem>
   <listitem>
    <para>
     Cancel the maintenance mode task.
    </para>
    <!-- Comment from DITA original: -->
    <!-- TODO Missing Image <image
    href="../../media/esx/eon_cancel_maintenance%20mode.png" id="image_lpd_qfx_st"/>-->
   </listitem>
   <listitem>
    <para>
     Right-click the <emphasis role="bold">ovsvapp VM (IP Address)</emphasis>
     node, select <emphasis role="bold">Power</emphasis>, and then click
     <emphasis role="bold">Power Off</emphasis>.
    </para>
    <informalfigure>
     <mediaobject>
      <imageobject role="fo">
       <imagedata fileref="media-esx-eon_poweroff_ovsvapp.png" width="75%" format="PNG"/>
      </imageobject>
      <imageobject role="html">
       <imagedata fileref="media-esx-eon_poweroff_ovsvapp.png"/>
      </imageobject>
     </mediaobject>
    </informalfigure>
   </listitem>
   <listitem>
    <para>
     Right-click the node and then click <emphasis role="bold">Delete from
     Disk</emphasis>.
    </para>
    <informalfigure>
     <mediaobject>
      <imageobject role="fo">
       <imagedata fileref="media-esx-eon_delete_ovsvapp.png" width="75%" format="PNG"/>
      </imageobject>
      <imageobject role="html">
       <imagedata fileref="media-esx-eon_delete_ovsvapp.png"/>
      </imageobject>
     </mediaobject>
    </informalfigure>
   </listitem>
   <listitem>
    <para>
     Right-click the <emphasis role="bold">Host</emphasis>, and then click
     <emphasis role="bold">Enter Maintenance Mode</emphasis>.
    </para>
   </listitem>
   <listitem>
    <para>
     Disconnect the VM. Right-click the VM, and then click
     <emphasis role="bold">Disconnect</emphasis>.
    </para>
    <informalfigure>
     <mediaobject>
      <imageobject role="fo">
       <imagedata fileref="media-esx-eon_disconnect_maintenance.png" width="75%" format="PNG"/>
      </imageobject>
      <imageobject role="html">
       <imagedata fileref="media-esx-eon_disconnect_maintenance.png"/>
      </imageobject>
     </mediaobject>
    </informalfigure>
   </listitem>
  </orderedlist>
  <para>
   The ESXi node is removed from the vCenter.
  </para>
 </section>
 <section>
  <title>Clean up Neutron Agent for OVSvAPP Service</title>
  <para>
   After removing ESXi node from a vCenter, perform the following procedure to
   clean up neutron agents for ovsvapp-agent service.
  </para>
  <orderedlist>
   <listitem>
    <para>
     Login to &lcm;.
    </para>
   </listitem>
   <listitem>
    <para>
     Source the credentials.
    </para>
<screen>source service.osrc</screen>
   </listitem>
   <listitem>
    <para>
     Execute the following command.
    </para>
<screen>neutron agent-list | grep &lt;OVSvapp hostname&gt;</screen>
    <para>
     For example:
    </para>
<screen>neutron agent-list | grep MCP-VCP-cpesx-esx-ovsvapp0001-mgmt
| 92ca8ada-d89b-43f9-b941-3e0cd2b51e49 | OVSvApp Agent      | MCP-VCP-cpesx-esx-ovsvapp0001-mgmt |                   | :-)   | True           | ovsvapp-agent             | </screen>
   </listitem>
   <listitem>
    <para>
     Delete the OVSvAPP agent.
    </para>
<screen>neutron agent-delete &lt;Agent -ID&gt;</screen>
    <para>
     For example:
    </para>
<screen>neutron agent-delete 92ca8ada-d89b-43f9-b941-3e0cd2b51e49</screen>
   </listitem>
  </orderedlist>
  <para>
   If you have more than one host, perform the preceding procedure for all the
   hosts.
  </para>
 </section>
 <section>
  <title>Clean up Monasca Agent for OVSvAPP Service</title>
  <para>
   Perform the following procedure to clean up Monasca agents for ovsvapp-agent
   service.
  </para>
  <orderedlist>
   <listitem>
    <para>
     If Monasca-API is installed on different node, copy the
     <literal>service.orsc</literal> from &lcm; to Monasca API
     server.
    </para>
<screen>scp service.orsc $USER@ardana-cp1-mtrmon-m1-mgmt:</screen>
   </listitem>
   <listitem>
    <para>
     SSH to Monasca API server. You must SSH to each Monasca API server for
     cleanup.
    </para>
    <para>
     For example:
    </para>
<screen>ssh ardana-cp1-mtrmon-m1-mgmt</screen>
   </listitem>
   <listitem>
    <para>
     Edit <literal>/etc/monasca/agent/conf.d/host_alive.yaml</literal> file to
     remove the reference to the OVSvAPP you removed. This requires
     <command>sudo</command> access.
    </para>
<screen>sudo vi /etc/monasca/agent/conf.d/host_alive.yaml</screen>
    <para>
     A sample of <literal>host_alive.yaml</literal>:
    </para>
<screen>- alive_test: ping
  built_by: HostAlive
  host_name: MCP-VCP-cpesx-esx-ovsvapp0001-mgmt
  name: MCP-VCP-cpesx-esx-ovsvapp0001-mgmt ping
  target_hostname: MCP-VCP-cpesx-esx-ovsvapp0001-mgmt </screen>
    <para>
     where <literal>host_name</literal> and
     <literal>target_hostname</literal> are mentioned at the DNS name field at
     the vSphere client.
     (Refer to <xref linkend="sec.esxi-host.pre"/>).
    </para>
   </listitem>
   <listitem>
    <para>
     After removing the reference on each of the Monasca API servers, restart
     the monasca-agent on each of those servers by executing the following
     command.
    </para>
<screen>&prompt.sudo;service openstack-monasca-agent restart</screen>
   </listitem>
   <listitem>
    <para>
     With the OVSvAPP references removed and the monasca-agent restarted, you
     can delete the corresponding alarm to complete the cleanup process. We
     recommend using the Monasca CLI which is installed on each of your Monasca
     API servers by default. Execute the following command from the Monasca API
     server (for example:
     <literal>ardana-cp1-mtrmon-mX-mgmt</literal>).
    </para>
<screen>monasca alarm-list --metric-name host_alive_status --metric-dimensions hostname=&lt;ovsvapp deleted&gt;</screen>
    <para>
     For example: You can execute the following command to get the alarm ID, if
     the OVSvAPP appears as a preceding example.
    </para>
<screen>monasca alarm-list --metric-name host_alive_status --metric-dimensions hostname=MCP-VCP-cpesx-esx-ovsvapp0001-mgmt
+--------------------------------------+--------------------------------------+-----------------------+-------------------+-------------------------------------------+----------+-------+-----------------+------+--------------------------+--------------------------+--------------------------+
| id                                   | alarm_definition_id                  | alarm_definition_name | metric_name       | metric_dimensions                         | severity | state | lifecycle_state | link | state_updated_timestamp  | updated_timestamp        | created_timestamp        |
+--------------------------------------+--------------------------------------+-----------------------+-------------------+-------------------------------------------+----------+-------+-----------------+------+--------------------------+--------------------------+--------------------------+
| cfc6bfa4-2485-4319-b1e5-0107886f4270 | cca96c53-a927-4b0a-9bf3-cb21d28216f3 | Host Status           | host_alive_status | service: system                           | HIGH     | OK    | None            | None | 2016-10-27T06:33:04.256Z | 2016-10-27T06:33:04.256Z | 2016-10-23T13:41:57.258Z |
|                                      |                                      |                       |                   | cloud_name: entry-scale-kvm-esx-mml       |          |       |                 |      |                          |                          |                          |
|                                      |                                      |                       |                   | test_type: ping                           |          |       |                 |      |                          |                          |                          |
|                                      |                                      |                       |                   | hostname: ardana-cp1-esx-ovsvapp0001-mgmt |          |       |                 |      |                          |                          |                          |
|                                      |                                      |                       |                   | control_plane: control-plane-1            |          |       |                 |      |                          |                          |                          |
|                                      |                                      |                       |                   | cluster: mtrmon                           |          |       |                 |      |                          |                          |                          |
|                                      |                                      |                       |                   | observer_host: ardana-cp1-mtrmon-m1-mgmt  |          |       |                 |      |                          |                          |                          |
|                                      |                                      |                       | host_alive_status | service: system                           |          |       |                 |      |                          |                          |                          |
|                                      |                                      |                       |                   | cloud_name: entry-scale-kvm-esx-mml       |          |       |                 |      |                          |                          |                          |
|                                      |                                      |                       |                   | test_type: ping                           |          |       |                 |      |                          |                          |                          |
|                                      |                                      |                       |                   | hostname: ardana-cp1-esx-ovsvapp0001-mgmt |          |       |                 |      |                          |                          |                          |
|                                      |                                      |                       |                   | control_plane: control-plane-1            |          |       |                 |      |                          |                          |                          |
|                                      |                                      |                       |                   | cluster: mtrmon                           |          |       |                 |      |                          |                          |                          |
|                                      |                                      |                       |                   | observer_host: ardana-cp1-mtrmon-m3-mgmt  |          |       |                 |      |                          |                          |                          |
|                                      |                                      |                       | host_alive_status | service: system                           |          |       |                 |      |                          |                          |                          |
|                                      |                                      |                       |                   | cloud_name: entry-scale-kvm-esx-mml       |          |       |                 |      |                          |                          |                          |
|                                      |                                      |                       |                   | test_type: ping                           |          |       |                 |      |                          |                          |                          |
|                                      |                                      |                       |                   | hostname: ardana-cp1-esx-ovsvapp0001-mgmt |          |       |                 |      |                          |                          |                          |
|                                      |                                      |                       |                   | control_plane: control-plane-1            |          |       |                 |      |                          |                          |                          |
|                                      |                                      |                       |                   | cluster: mtrmon                           |          |       |                 |      |                          |                          |                          |
|                                      |                                      |                       |                   | observer_host: ardana-cp1-mtrmon-m2-mgmt  |          |       |                 |      |                          |                          |                          |
+--------------------------------------+--------------------------------------+-----------------------+-------------------+-------------------------------------------+----------+-------+-----------------+------+--------------------------+--------------------------+--------------------------+</screen>
   </listitem>
   <listitem>
    <para>
     Delete the Monasca alaram.
    </para>
<screen>monasca alarm-delete &lt;alarm ID&gt;</screen>
    <para>
     For example:
    </para>
<screen>monasca alarm-delete cfc6bfa4-2485-4319-b1e5-0107886f4270Successfully deleted alarm </screen>
    <para>
     After deleting the alarms and updating the monasca-agent configuration,
     those alarms will be removed from the &opscon; UI. You can login to
     &opscon; and view the status.
    </para>
   </listitem>
  </orderedlist>
 </section>
 <section>
  <title>Clean up the entries of OVSvAPP VM from /etc/host</title>
  <para>
   Perform the following procedure to clean up the entries of OVSvAPP VM from
   <literal>/etc/host</literal>.
  </para>
  <orderedlist>
   <listitem>
    <para>
     Login to &lcm;.
    </para>
   </listitem>
   <listitem>
    <para>
     Edit <literal>/etc/host</literal>.
    </para>
<screen>vi /etc/host</screen>
    <para>
     For example: <literal>MCP-VCP-cpesx-esx-ovsvapp0001-mgmt</literal> VM is
     present in the <literal>/etc/host</literal>.
    </para>
<screen>192.168.86.17    MCP-VCP-cpesx-esx-ovsvapp0001-mgmt</screen>
   </listitem>
   <listitem>
    <para>
     Delete the OVSvAPP entries from <literal>/etc/host</literal>.
    </para>
   </listitem>
  </orderedlist>
 </section>
 <section>
  <title>Remove the OVSVAPP VM from the servers.yml and pass_through.yml files and run the Configuration Processor</title>
  <para>
   Complete these steps from the &lcm; to remove the OVSvAPP VM:
  </para>
  <orderedlist>
   <listitem>
    <para>
     Log in to the &lcm;
    </para>
   </listitem>
   <listitem>
    <para>
     Edit <literal>servers.yml</literal> file to remove references to the
     OVSvAPP VM(s) you want to remove:
    </para>
<screen>~/openstack/my_cloud/definition/data/servers.yml</screen>
    <para>
     For example:
    </para>
<screen>- ip-addr:192.168.86.17
  server-group: AZ1    role:
  OVSVAPP-ROLE    id:
  6afaa903398c8fc6425e4d066edf4da1a0f04388</screen>
   </listitem>
   <listitem>
    <para>
     Edit <literal>~/openstack/my_cloud/definition/data/pass_through.yml</literal>
     file to remove the OVSvAPP VM references using the server-id above section
     to find the references.
    </para>
<screen>- data:
  vmware:
  vcenter_cluster: Clust1
  cluster_dvs_mapping: 'DC1/host/Clust1:TRUNK-DVS-Clust1'
  esx_hostname: MCP-VCP-cpesx-esx-ovsvapp0001-mgmt
  vcenter_id: 0997E2ED9-5E4F-49EA-97E6-E2706345BAB2
id: 6afaa903398c8fc6425e4d066edf4da1a0f04388
</screen>
   </listitem>
   <listitem>
    <para>
     Commit the changes to git:
    </para>
<screen>git commit -a -m "Remove ESXi host &lt;name&gt;"</screen>
   </listitem>
   <listitem>
    <para>
     Run the configuration processor. You may want to use the
     <literal>remove_deleted_servers</literal> and
     <literal>free_unused_addresses</literal> switches to free up the resources
     when running the configuration processor. See
     <xref linkend="persisteddata"/> for more details.
    </para>
<screen>cd ~/openstack/ardana/ansible
ansible-playbook -i hosts/localhost config-processor-run.yml -e remove_deleted_servers="y" -e free_unused_addresses="y"</screen>
   </listitem>
   <listitem>
    <para>
     Update your deployment directory:
    </para>
<screen>cd ~/openstack/ardana/ansible
ansible-playbook -i hosts/localhost ready-deployment.yml</screen>
   </listitem>
  </orderedlist>
 </section>
 <section>
  <title>Remove Distributed Resource Scheduler (DRS) Rules</title>
  <para>
   Perform the following procedure to remove DRS rules, which is added by
   OVSvAPP installer to ensure that OVSvAPP does not get migrated to other
   hosts.
  </para>
  <orderedlist>
   <listitem>
    <para>
     Login to vCenter.
    </para>
   </listitem>
   <listitem>
    <para>
     Right click on cluster and select <emphasis role="bold">Edit
     settings</emphasis>.
    </para>
    <informalfigure>
     <mediaobject>
      <imageobject role="fo">
       <imagedata fileref="media-esx-drs-rule1.png" width="75%" format="PNG"/>
      </imageobject>
      <imageobject role="html">
       <imagedata fileref="media-esx-drs-rule1.png"/>
      </imageobject>
     </mediaobject>
    </informalfigure>
    <para>
     A cluster settings page appears.
    </para>
   </listitem>
   <listitem>
    <para>
     Click <emphasis role="bold">DRS Groups Manager</emphasis> on the left hand
     side of the pop-up box. Select the group which is created for deleted
     OVSvAPP and click Remove.
    </para>
    <informalfigure>
     <mediaobject>
      <imageobject role="fo">
       <imagedata fileref="media-esx-drs-group2.png" width="75%" format="PNG"/>
      </imageobject>
      <imageobject role="html">
       <imagedata fileref="media-esx-drs-group2.png"/>
      </imageobject>
     </mediaobject>
    </informalfigure>
   </listitem>
   <listitem>
    <para>
     Click <emphasis role="bold">Rules</emphasis> on the left hand side of the
     pop-up box and select the checkbox for deleted OVSvAPP and click
     <emphasis role="bold">Remove</emphasis>.
    </para>
    <informalfigure>
     <mediaobject>
      <imageobject role="fo">
       <imagedata fileref="media-esx-rules3.png" width="75%" format="PNG"/>
      </imageobject>
      <imageobject role="html">
       <imagedata fileref="media-esx-rules3.png"/>
      </imageobject>
     </mediaobject>
    </informalfigure>
   </listitem>
   <listitem>
    <para>
     Click <emphasis role="bold">OK</emphasis>.
    </para>
   </listitem>
  </orderedlist>
 </section>
</section>
