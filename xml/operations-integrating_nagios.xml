<?xml version="1.0"?>
<?xml-stylesheet href="urn:x-suse:xslt:profiling:docbook51-profile.xsl"
 type="text/xml"
 title="Profiling step"?>
<!DOCTYPE section [
 <!ENTITY % entities SYSTEM "entities.ent"> %entities;
]>
<section xmlns="http://docbook.org/ns/docbook" xmlns:xi="http://www.w3.org/2001/XInclude" xmlns:xlink="http://www.w3.org/1999/xlink" xml:id="nagios_integration">
 <title>Nagios Integration</title>
 <para>
  &kw-hos; cloud operators that are using Nagios or Icinga-based monitoring
  systems may wish to integrate them with the built-in monitoring
  infrastructure of &productname;. Integrating with the existing
  monitoring processes and procedures will reduce support overhead and avoid
  duplication. This document describes the different approaches that can be
  taken to create a well-integrated monitoring dashboard using both
  technologies.
 </para>
 <note>
  <para>
   This document refers to Nagios but the proposals will work equally well with
   Icinga, Icinga2, or other Nagios clone monitoring systems.
  </para>
 </note>
 <section xml:id="idg-all-operations-integrating_nagios-xml-5">
  <title>&kw-hos; monitoring and reporting</title>
  <para>
   &kw-hos; comes with a monitoring engine (&o_monitor;) and a separate management
   dashboard (&opscon;). &o_monitor; is extremely scalable, designed to
   cope with the constant change in monitoring sources and services found in a
   cloud environment. Monitoring agents running on hosts (physical and virtual)
   submit data to the &o_monitor; message bus via a RESTful API. Threshold and
   notification engines then trigger alarms when predefined thresholds are
   passed. Notification methods are flexible and extensible. Typical examples
   of notification methods would be emails generated or creating alarms in
   PagerDuty.
  </para>
  <para>
   While extensible, &o_monitor; is largely focused on monitoring cloud
   infrastructures rather than traditional environments such as server
   hardware, network links, switches, etc. For more details about the
   monitoring service, see <xref linkend="mon"/>.
  </para>
  <para>
   The &opscon; (Ops Console) provides cloud administrators a clear
   web interfaces to view alarm status, management alarm workflow, and
   configure alarms and thresholds. For more details about the Ops Console, see
   <xref linkend="opsconsole"/>.
  </para>
 </section>
 <section xml:id="nagios">
  <title>Nagios monitoring and reporting</title>
  <para>
   Nagios is an industry leading open source monitoring service with extensive
   plugins and agents. Nagios checks are either run directly from the
   monitoring server or run on a remote host via an agent and with results
   submitted back to the monitoring server. While Nagios has proven extremely
   flexible and scalable, it requires significant explicit configuration. Using
   Nagios to monitor guest virtual machines becomes more challenging because
   virtual machines can be ephemeral which means new virtual machines are
   created and destroyed regularly. Configuration automation (Chef, Puppet,
   Ansible etc) can create a more dynamic Nagios setup but they still require
   the Nagios service to be restarted every time a new host is added.
  </para>
  <para>
   A key benefit of Nagios style monitoring is that it allows for &kw-hos; to
   be monitored externally, from a user or service perspective. For example,
   checks can be created to monitor availability of all the API endpoints from
   external locations or even to create and destroy instances to ensure the
   entire system is working as expected.
  </para>
 </section>
 <section xml:id="idg-all-operations-integrating_nagios-xml-7">
  <title>Adding &o_monitor;</title>
  <para>
   Many private cloud operators already have existing monitoring solutions such
   as Nagios and Icinga. We recommend that you extend your existing solutions
   into &o_monitor; or forward &o_monitor; alerts to your existing solution to maximize
   coverage and reduce risk.
  </para>
 </section>
 <section xml:id="integration_approaches">
  <title>Integration Approaches</title>
  <para>
   Integration between Nagios and &o_monitor; can occur at two levels, at the
   individual check level or at the management interfaces. Both options are
   discussed in the following sections.
  </para>
  <para>
   <emphasis role="bold">Running Nagios-style checks in the &o_monitor;
   agents</emphasis>
  </para>
<!--
  <informalfigure>
   <mediaobject>
    <imageobject role="fo">
     <imagedata fileref="media-hos.docs-integrations-nagios-nagios_int1.png" width="75%" format="PNG"/>
    </imageobject>
    <imageobject role="html">
     <imagedata fileref="media-hos.docs-integrations-nagios-nagios_int1.png"/>
    </imageobject>
   </mediaobject>
  </informalfigure> -->
  <para>
   The &o_monitor; agent is installed on all &kw-hos; servers and includes the
   ability to execute Nagios-style plugins as well as its own plugin scripts.
   For this configuration check, plugins need to be installed on the required
   server then added to the &o_monitor; configuration under
   <literal>/etc/monasca/agent/conf.d</literal>. Care should be taken as
   plugins that take a long time (greater than 10 seconds) to run can result in
   the &o_monitor; agent failing to run its own checks in the allotted time and
   therefore stopping all client monitoring. Issues have been seen with
   hardware monitoring plugins that can take greater than 30 seconds and any
   plugins relying on name resolution when DNS services are not available.
   Details on the required &o_monitor; configuration can be found at
   <link xlink:href="https://github.com/openstack/monasca-agent/blob/master/docs/Plugins.md#nagios-wrapper"/>.
  </para>
  <para>
   Use Case:
  </para>
  <itemizedlist>
   <listitem>
    <para>
     Local host checking. As an operator I want to run a local monitoring check
     on my host to check physical hardware. Check status and alert management
     will be based around the &opscon;, not Nagios.
    </para>
   </listitem>
  </itemizedlist>
  <para>
   Limitation
  </para>
  <itemizedlist>
   <listitem>
    <para>
     As mentioned earlier, care should be taken to ensure checks do not
     introduce load or delays in the &o_monitor; agent check cycle. Additionally,
     depending on the operating system the node is running, plugins or
     dependencies may not be available.
    </para>
   </listitem>
  </itemizedlist>
  <para>
   <emphasis role="bold">Using Nagios as a central dashboard</emphasis>
  </para>
<!--
  <informalfigure>
   <mediaobject>
    <imageobject role="fo">
     <imagedata fileref="media-hos.docs-integrations-nagios-nagios_int2.png" width="75%" format="PNG"/>
    </imageobject>
    <imageobject role="html">
     <imagedata fileref="media-hos.docs-integrations-nagios-nagios_int2.png"/>
    </imageobject>
   </mediaobject>
  </informalfigure> -->
  <para>
   It is possible to create a Nagios-style plugin that will query the &o_monitor;
   API endpoint for an alarm status to create Nagios alerts and alarms based on
   &o_monitor; alarms and filters. &o_monitor; alarms appear in Nagios using two
   approaches, one listing checks by service and the other listing checks by
   physical host.
  </para>
<!--
  <informalfigure>
   <mediaobject>
    <imageobject role="fo">
     <imagedata fileref="media-hos.docs-integrations-nagios-nagios.png" width="75%" format="PNG"/>
    </imageobject>
    <imageobject role="html">
     <imagedata fileref="media-hos.docs-integrations-nagios-nagios.png"/>
    </imageobject>
   </mediaobject>
  </informalfigure> -->
  <para>
   In the top section of the Nagios-style plugin, services can be created under
   a dummy host, <literal>monasca_endpoint</literal>. Each service retrieves
   all alarms based on defined dimensions. For example the
   <literal>ardana-compute</literal> check will return all alarms with the
   compute (Nova) dimension.
  </para>
  <para>
   In the bottom section, the physical servers making up the &kw-hos;
   cluster can be defined and checks can be run. For example, one could check
   the server hardware from the Nagios server using a third party plugin and the
   another could retrieve all monasca alarms related to that host.
  </para>
  <para>
   To build this configuration, a custom Nagios plugin
   (Please see example plugin at:
   <link xlink:href="https://github.com/openstack/python-monascaclient/tree/stable/pike/examples"/>)
   was created with the following options:
  </para>
<screen>check_monasca –c <replaceable>CREDENTIALS</replaceable> -d <replaceable>DIMENSION</replaceable> -v <replaceable>VALUE</replaceable></screen>
  <para>
   Examples:
  </para>
  <para>
   To check alarms on <literal>test-ccp-comp001-mgmt</literal> you would use:
  </para>
<screen>check_monasca –c service.osrc –d hostname –v test-ccp-comp001-mgmt</screen>
  <para>
   To check all Network related alarms, you would use:
  </para>
<screen>check_monasca –c service.osrc –d service –v networking</screen>
  <para>
   Use Cases:
  </para>
  <itemizedlist>
   <listitem>
    <para>
     Multiple clouds, integrating &kw-hos; monitoring with existing monitoring
     capabilities or viewing &o_monitor; alerts in Nagios, fully integrating
     &o_monitor; alarms with Nagios alarms and workflow.
    </para>
   </listitem>
   <listitem>
    <para>
     In a predominantly Nagios or Icinga-based monitoring environment,
     &o_monitor; alarm status can be integrated into existing processes and
     workflows. This approach works best for checks associated with physical
     servers running the &kw-hos; services.
    </para>
   </listitem>
   <listitem>
    <para>
     With multiple &kw-hos; clusters, all of their alarms can be consolidated
     into a single view, the current version of &opscon; is for a single
     cluster only.
    </para>
   </listitem>
  </itemizedlist>
  <para>
   Limitations
  </para>
  <itemizedlist>
   <listitem>
    <para>
     Nagios has a more traditional configuration model that requires checks to
     belong to predefined services and hosts, this is not well suited in highly
     dynamic cloud environments where the lifespan of virtual instances can be
     very short. One possible solution is with Icinga2 which has an API
     available to dynamically add host and service definitions, the check
     plugin could be extended to create alarm definitions dynamically as they
     occur.
    </para>
    <para>
     The key disadvantage is that multiple alarms can appear as a single
     service. For example, suppose there are 3 warnings against one service.
     If the operator acknowledges this alarm and subsequently a 4th warning
     alarm occurs, it would not generate an alert and could get missed.
    </para>
    <para>
     Care has to be taken that alarms are not missed. If the defined checks
     are only looking for checks in an ALARM status they will not report
     undetermined checks that might indicate other issues.
    </para>
   </listitem>
  </itemizedlist>
  <para>
   <emphasis role="bold">Using &opscon; as central dashboard</emphasis>
  </para>
<!--
  <informalfigure>
   <mediaobject>
    <imageobject role="fo">
     <imagedata fileref="media-hos.docs-integrations-nagios-nagios_int3.png" width="75%" format="PNG"/>
    </imageobject>
    <imageobject role="html">
     <imagedata fileref="media-hos.docs-integrations-nagios-nagios_int3.png"/>
    </imageobject>
   </mediaobject>
  </informalfigure> -->
  <para>
   Nagios has the ability to run custom scripts in response to events. It is
   therefore possible to write a plugin to update &o_monitor; whenever a Nagios
   alert occurs. The &opscon; could then be used as a central reporting
   dashboard for both &o_monitor; and Nagios alarms. The external Nagios alarms can
   have their own check dimension and could be displayed as a separate group in
   the &opscon;.
  </para>
  <para>
   Use Cases
  </para>
  <itemizedlist>
   <listitem>
    <para>
     Using &opscon; the central monitoring tool.
    </para>
   </listitem>
  </itemizedlist>
  <para>
   Limitations
  </para>
  <itemizedlist>
   <listitem>
    <para>
     The alarm could not be acknowledged from the &opscon; so Nagios could
     send repetitive notifications unless configured to take this into account.
    </para>
   </listitem>
  </itemizedlist>
  <para>
   <emphasis role="bold">&kw-hos;-specific Nagios Plugins</emphasis>
  </para>
  <para>
   Several OpenStack plugin packages exist (see
   <link xlink:href="https://launchpad.net/ubuntu/+source/nagios-plugins-openstack"/>)
   that are useful to run from external sources to ensure the overall system is
   working as expected. &o_monitor; requires some OpenStack components to be working
   in order to work at all. For example, if &o_ident; were unavailable,
   &o_monitor; could not authenticate client or console requests. An external
   service check could highlight this.
  </para>
 </section>
 <section xml:id="issues">
  <title>Common integration issues</title>
  <para>
   <emphasis role="bold">Alarm status differences</emphasis>
  </para>
  <para>
   &o_monitor; and Nagios treat alarms and status in different ways and for the two
   systems to talk there needs to be a mapping between them. The following
   table details the alarm parameters available for each:
  </para>
  <informaltable colsep="1" rowsep="1">
   <tgroup cols="4">
    <colspec colname="c1" colnum="1"/>
    <colspec colname="c2" colnum="2"/>
    <colspec colname="c3" colnum="3"/>
    <colspec colname="c4" colnum="4"/>
    <thead>
     <row>
      <entry>System</entry>
      <entry>Status</entry>
      <entry>Severity</entry>
      <entry>Details</entry>
     </row>
    </thead>
    <tbody>
     <row>
      <entry morerows="3">Nagios</entry>
      <entry>OK</entry>
      <entry/>
      <entry>Plugin returned OK with given thresholds</entry>
     </row>
     <row>
      <entry>WARNING</entry>
      <entry/>
      <entry>Plugin returned WARNING based on thresholds</entry>
     </row>
     <row>
      <entry>CRITICAL</entry>
      <entry/>
      <entry>Plugin returned CRITICAL alarm</entry>
     </row>
     <row>
      <entry>UNKNOWN</entry>
      <entry/>
      <entry>Plugin failed</entry>
     </row>
     <row>
      <entry morerows="4">&o_monitor;</entry>
      <entry>OK</entry>
      <entry/>
      <entry>No alarm triggered</entry>
     </row>
     <row>
      <entry>ALARM</entry>
      <entry>LOW</entry>
      <entry>Alarm state, LOW impact</entry>
     </row>
     <row>
      <entry>ALARM</entry>
      <entry>MEDIUM</entry>
      <entry>Alarm state, MEDIUM impact</entry>
     </row>
     <row>
      <entry>ALARM</entry>
      <entry>HIGH</entry>
      <entry>Alarm state, HIGH impact</entry>
     </row>
     <row>
      <entry>UNDETERMINED</entry>
      <entry/>
      <entry>No metrics received</entry>
     </row>
    </tbody>
   </tgroup>
  </informaltable>
  <para>
   In the plugin described here, the mapping was created with this flow:
  </para>
<screen>Monasca OK -&gt; Nagios OK
Monasca ALARM ( LOW or MEDIUM ) -&gt; Nagios Warning
Monasca ALARM ( HIGH ) -&gt; Nagios Critical</screen>
  <para>
   <emphasis role="bold">Alarm workflow differences</emphasis>
  </para>
  <para>
   In both, system alarms can be acknowledged in the dashboards to indicate
   they are being worked on (or ignored). Not all the scenarios above will
   provide the same level of workflow integration.
  </para>
 </section>
</section>
