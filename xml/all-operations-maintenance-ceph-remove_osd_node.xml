<?xml version="1.0"?>
<!DOCTYPE section [
 <!ENTITY % entities SYSTEM "entities.ent"> %entities;
]>
<!---->
<section xmlns="http://docbook.org/ns/docbook" xmlns:xi="http://www.w3.org/2001/XInclude" xmlns:xlink="http://www.w3.org/1999/xlink" xml:id="remove_osd_node"><title>&kw-hos-tm; &kw-hos-version-50;: Removing a Ceph OSD Node</title><abstract><para><para>Removing a Ceph Object Storage Daemon (OSD) node
            allows you to remove capacity.</para>Perform the following steps to remove a Ceph
        Object Storage Daemon (OSD) node from a cluster:</para>
</abstract><!---->


        <sidebar xml:id="idg-all-operations-maintenance-ceph-remove_osd_node-xml-6"><title>Removing a Ceph OSD Node</title><orderedlist>
                    <listitem><para>Log in to the OSD node that you wish to remove.</para>
</listitem>
                    <listitem><para>Use the following command to retrieve and make note of the OSD numbers on
                        the OSD node to be removed:</para>
<screen>ceph osd tree</screen><para>A sample of
                        OSD node is as
                            follows:</para>
<screen>$ ceph osd tree
# id    weight  type name       up/down reweight
-1      6       root default
-2      3               host ceph-ccp-ceph0003-mgmt
0       1                       osd.0   up      1
3       1                       osd.3   up      1
6       1                       osd.6   up      1
<emphasis role="bold">-4      3               host ceph-ccp-ceph0001-mgmt
1       1                       osd.1   up      1
4       1                       osd.4   up      1
7       1                       osd.7   up      1</emphasis>
-3      3               host ceph-ccp-ceph0002-mgmt
2       1                       osd.2   up      1
5       1                       osd.5   up      1
8       1                       osd.8   up      1</screen><para>For
                            example, to remove the OSD node <literal>ceph-ccp-ceph0001-mgmt</literal>,
                            the OSD numbers 1, 4, and 7 are relevant.</para>
</listitem>
                    <listitem><para>Log in to the lifecycle manager.</para>
</listitem>
                    <listitem><para>Run the following command to stop the OSDs running on the OSD node to be removed:</para>
<screen>cd ~/scratch/ansible/next/hos/ansible
ansible-playbook -i hosts/verb_hosts ceph-stop.yml --limit &lt;osd_node_to_remove&gt;</screen><para>Replace the <literal>&lt;osd_node_to_remove&gt;</literal> string with the
                            hostname of the node that you wish to remove. For example
                                <literal>'ceph-ccp-ceph0001-mgmt'</literal>.</para>
</listitem>
                    <listitem><para> Log back in to the OSD node you are removing and run the following command
                        for each OSD number (obtained from step #2
                            above):</para>
<screen>ceph osd out &lt;osd-number&gt; --cluster &lt;ceph_cluster&gt;
ceph auth del osd.&lt;osd-number&gt; --cluster &lt;ceph_cluster&gt;
ceph osd crush remove osd.&lt;osd-number&gt; --cluster &lt;ceph_cluster&gt;
ceph osd rm &lt;osd-number&gt; --cluster &lt;ceph_cluster&gt;</screen></listitem>
                    <listitem><para>From the same OSD node, remove the OSD host from the Ceph CRUSH map by
                        executing the following
                            command:</para>
<screen>ceph osd crush remove &lt;host-name&gt;</screen></listitem>
                    <listitem><para>Log in to the lifecycle manager and remove the OSD host fentry rom the input
                        model in the <literal>~/helion/my_cloud/definition/data/servers.yml</literal>
                        file.</para>
</listitem>
                    <listitem><para>Check your
                            <literal>~/helion/my_cloud/definition/data/control_plane.yml</literal>
                        file and ensure that the <literal>member-count</literal>,
                            <literal>min-count</literal>, and <literal>max-count</literal> values (if
                        used) represent the number of OSD nodes that will remain after the node is
                        removed.</para>
</listitem>
                    <listitem><para>Commit your configuration to the <xref linkend="using_git"/>, as
                        follows:
                        </para>
<screen>cd ~/helion/hos/ansible
git add -A
git commit -m "removing OSD node"</screen></listitem>
                    <listitem><para>Run the configuration processor:
                        </para>
<screen>cd ~/helion/hos/ansible
ansible-playbook -i hosts/localhost config-processor-run.yml</screen></listitem>
                    <listitem><para>Update your deployment directory:
                        </para>
<screen>cd ~/helion/hos/ansible
ansible-playbook -i hosts/localhost ready-deployment.yml</screen></listitem>
                    <listitem><para>Remove the OSD node from Cobbler: </para>
<screen>sudo cobbler system remove --name=&lt;node&gt;</screen></listitem>
                    <listitem><para>Run the Cobbler deploy playbook:
                        </para>
<screen>cd ~/helion/hos/ansible
ansible-playbook -i hosts/localhost cobbler-deploy.yml</screen></listitem>
                    <listitem><para>Log in to the OSD node and unmount the data and journal disks. Also, remove
                        the mount instructions for the same disks from the
                            <literal>/etc/fstab</literal> file.</para>
</listitem>
                    <listitem><para>On the OSD node, remove the OSD service
                            directory:</para>
<screen>sudo rm -rf /var/lib/ceph/osd/ceph-{osd-number}</screen><para>Example:</para>
<screen>sudo rm -rf /var/lib/ceph/osd/ceph-1</screen>
</listitem>
                </orderedlist>
</sidebar>

        <sidebar xml:id="idg-all-operations-maintenance-ceph-remove_osd_node-xml-7"><title>Remove the Ceph OSD Node from Monitoring</title><para>Once a Ceph OSD node has been removed, any related monitoring alarms will trigger.
                Take the following additional steps to take to resolve this issue.</para>
<para>You will want to SSH to each of the Monasca API servers and edit the
                    <literal>/etc/monasca/agent/conf.d/host_alive.yaml</literal> file and remove
                references to the Ceph OSD node(s) you removed. This will require
                    <literal>sudo</literal> access.</para>
<para>Once you have removed the references on each of your Monasca API servers you then
                need to restart the monasca-agent on each of those servers with the following
                command:</para>
<screen>sudo service monasca-agent restart</screen><para>With the Ceph OSD node references removed from your Monasca API servers and the
                corrensponding monasca-agents restarted, you can then delete the corresponding
                alarms to finish this process. To do so, we recommend using the Monasca CLI, which
                should be installed by default on each of your Monasca API servers:</para>
<screen>monasca alarm-list --metric-dimensions hostname=&lt;ceph osd node deleted&gt;</screen><para>You can then delete the alarm with this command:</para>
<screen>monasca alarm-delete &lt;alarm ID&gt;</screen></sidebar>

    </section>
