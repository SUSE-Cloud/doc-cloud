<?xml version="1.0"?>
<!DOCTYPE section [
 <!ENTITY % entities SYSTEM "entities.ent"> %entities;
]>
<section xmlns="http://docbook.org/ns/docbook" xmlns:xi="http://www.w3.org/2001/XInclude"
  xmlns:xlink="http://www.w3.org/1999/xlink" xml:id="hw_support_diskcalc" version="5.1">
 <title>Disk Calculator</title>
 <para>
  This topic provides guidance on how to estimate the amount of disk space
  required for a compute-centric &kw-hos; deployment. To accurately estimate
  the disk space needed, it is important to understand how &productname;
  utilizes resources. Although there are a variety of factors, including the
  number of compute nodes, a large portion of the utilization is driven by
  operational tools, such as monitoring, metering, and logging.
 </para>
 <important>
  <para>
   The disk calculator does not accurately estimate a Swift-centric deployment
   at this time. For more information on Swift, see
   <xref linkend="rec_min_swift"/>.
  </para>
 </important>
 <para>
  The usage of disk space by operational tools can be estimated from the
  following parameters:
 </para>
 <itemizedlist>
  <listitem>
   <para>
    <emphasis role="bold">Number of compute nodes</emphasis> +
    <emphasis role="bold">Number of VMs running on each compute
    node</emphasis>
   </para>
  </listitem>
  <listitem>
   <para>
    <emphasis role="bold">Number of services being monitored or
    metered</emphasis> + <emphasis role="bold">Amount of logs
    created</emphasis>
   </para>
  </listitem>
  <listitem>
   <para>
    <emphasis role="bold">Retention periods for operational data</emphasis>
    (for Elastic Search, Vertica/InfluxDB, and Kafka)
   </para>
  </listitem>
 </itemizedlist>
 <important>
  <para>
   If you also enable auditing, follow the steps in
   <xref linkend="disk_calc_audit_adj"/> to enter additional input parameters.
  </para>
 </important>
 <para>
  <emphasis role="bold">Disk Estimation Process</emphasis>
 </para>
 <para>
  &kw-hos; provides entry scale and scale-out models for deployment. This disk
  estimation tool, currently in a spreadsheet form, helps you decide which
  disk model to start from as well as what customizations you need to meet
  your deployment requirements. The disk estimation process also provides
  default settings and minimum values for the parameters that drive disk size.
 </para>
 <important>
  <para>
   Kafka is the queuing system used to process metering monitoring and logging
   (MML) data. Kakfa stores the queued data on disk, so the disk space
   available will have a large impact on the amount of data the MML systems
   can process. Providing less than the minimum disk space for Kakfa will
   result in loss of MML data and can affect other components on the control
   plane. The default for Kafka is 1 hour which is 17 GB.
  </para>
 </important>
 <para>
  <emphasis role="bold">To estimate the disk sizes required for your
  deployment:</emphasis>
 </para>
 <procedure>
  <step>
   <para>
    <xref linkend="disk_calc_input"/>
   </para>
  </step>
  <step>
   <para>
    If you also enable auditing, follow the steps in
    <xref linkend="disk_calc_audit_adj"/> to enter additional
    input parameters.
   </para>
  </step>
  <step>
   <para>
    <xref linkend="disk_calc_select"/>
   </para>
  </step>
  <step>
   <para>
    <xref linkend="disk_calc_match"/> Match the selected deployment to a disk
    model example.
   </para>
  </step>
 </procedure>
 <section xml:id="disk_calc_input">
  <title>Enter Input Parameters</title>
  <para>
   The Disk Calculator spreadsheet automatically displays the minimum
   requirements for the components that define disk size. You can replace the
   default values with either the number you have to work with or the number
   that you want to support.
  </para>
  <important>
   <para>
    If you want to enable audit logging, follow the steps in the
    <xref linkend="disk_calc_audit_adj"/> section to enter additional
    input parameters.
   </para>
  </important>
  <informaltable colsep="1" rowsep="1">
   <tgroup cols="3">
    <colspec colname="c1" colnum="1"/>
    <colspec colname="c2" colnum="2"/>
    <colspec colname="c3" colnum="3"/>
    <thead>
     <row>
      <entry>Input Parameter</entry>
      <entry>Default</entry>
      <entry>Minimum</entry>
     </row>
    </thead>
    <tbody>
     <row>
      <entry>System Memory</entry>
      <entry>64 GB</entry>
      <entry>64 GB</entry>
     </row>
     <row>
      <entry>Compute Nodes</entry>
      <entry>100</entry>
      <entry>100</entry>
     </row>
     <row>
      <entry>VMs per Compute Node</entry>
      <entry>40</entry>
      <entry>40</entry>
     </row>
     <row>
      <entry>Component: Vertica</entry>
      <entry>45 days retention period</entry>
      <entry>30 days</entry>
     </row>
     <row>
      <entry>Component: Logging</entry>
      <entry>
       <para>
        22 services covered
       </para>
       <para>
        7 days retention period
       </para>
      </entry>
      <entry>7 days retention period</entry>
     </row>
     <row>
      <entry>Component: Kafka (message queue)</entry>
      <entry>0.17 of an hour retention period</entry>
      <entry>0.042 of an hour retention period</entry>
     </row>
     <row>
      <entry>Component: Elastic Search (log storage)</entry>
      <entry>7 days retention period</entry>
      <entry>7 days retention period</entry>
     </row>
     <row>
      <entry>Component: Audit</entry>
      <entry>0 days retention period</entry>
      <entry>0 days retention period</entry>
     </row>
    </tbody>
   </tgroup>
  </informaltable>
  <para>
   The following diagram shows the input parameters in the spreadsheet.
  </para>
  <informalfigure>
   <mediaobject>
    <imageobject role="fo">
     <imagedata fileref="media-DiskCalc1.png" width="75%" format="PNG"/>
    </imageobject>
    <imageobject role="html">
     <imagedata fileref="media-DiskCalc1.png"/>
    </imageobject>
   </mediaobject>
  </informalfigure>
  <para>
   To provide the parameters required to estimate disk size:
  </para>
  <procedure>
   <step>
    <para>
     <!-- FIXME: <link xlink:href="../../media/hos.docs/diskcalc40.zip">-->
     Open the disk calculator spreadsheet.
    </para>
   </step>
   <step>
    <para>
     At the bottom of the spreadsheet, click on the <emphasis role="bold">Draft
     Sizing Tool4</emphasis> tab.
    </para>
   </step>
   <step>
    <para>
     To set the server RAM size, replace the default value in the
     <emphasis role="bold">System Memory</emphasis> field.
    </para>
   </step>
   <step>
    <para>
     To set the number of compute nodes, replace the default in the
     <emphasis role="bold">Compute Nodes</emphasis> field.
    </para>
   </step>
   <step>
    <para>
     To set the average number of virtual machines per compute node, replace
     the default in the <emphasis role="bold">VMs per Compute Node</emphasis>
     field.
    </para>
   </step>
   <step>
    <para>
     To set the number of days you want the metering and logging files
     retained, replace the default in the <emphasis role="bold">Vertica
     Retention Period</emphasis> field.
    </para>
   </step>
   <step>
    <para>
     To set logging values, replace the default in <emphasis role="bold">Number
     of Services Covered</emphasis> and <emphasis role="bold">Retention
     Period</emphasis>.
    </para>
    <important>
     <para>
      If you enable additional logging of services than those set by default,
      then you must increase the number in the <emphasis role="bold">Logging
      Number of Services</emphasis> Field.
     </para>
    </important>
   </step>
   <step>
    <para>
     To set a value for Kafka messages to be retained, replace the default in
     the <emphasis role="bold">Kafka Retention Period</emphasis>
     field.
    </para>
   </step>
   <step>
    <para>
     To set a value for Elastic Search log file retention, replace the default
     in the <emphasis role="bold">Elastic Search Retention Period</emphasis>
     field.
    </para>
   </step>
   <step>
    <para>
     To set a value for Audit logging file retention, replace the default in
     the <emphasis role="bold">Audit Retention Period</emphasis>
     field.
    </para>
   </step>
  </procedure>
 </section>
 <section xml:id="disk_calc_audit_adj">
  <title>Audit Logging Adjustment</title>
  <para>
   If you want to enable audit logging, you must enter additional input
   parameters to ensure there is enough room to retain the audit logs. The
   following diagram shows the parameters you need to specify in the Disk
   Calculator spreadsheet.
  </para>
  <informalfigure>
   <mediaobject>
    <imageobject role="fo">
     <imagedata fileref="media-DiskCalc2.png" width="75%" format="PNG"/>
    </imageobject>
    <imageobject role="html">
     <imagedata fileref="media-DiskCalc2.png"/>
    </imageobject>
   </mediaobject>
  </informalfigure>
  <para>
   To add audit logging to disk size calculations:
  </para>
  <procedure>
   <step>
    <para>
     Determine which services you have enabled to collect audit logging
     information. This is part of &lcm; configuration.
    </para>
   </step>
   <step>
    <para>
     Enter the number of Audit Enabled services on cluster.  Auditing is
     disabled by default, so these values will initially be 0.  If audit
     logging is enabled, initial suggested values would be 9 for API/Core
     Services, 1 for Networking, 1 for Swift, and 2 for MMLB.
    </para>
    <important>
     <para>
      If you enable logging for services beyond the defaults, you must change
      the <emphasis role="bold">Number of Services on a Cluster</emphasis>
      field in the spreadsheet. It is recommended that you increase the total
      services covered as well as increment the number on the appropriate
      cluster. For example, if you enable Apache logs on the core services,
      then the total would increase to 23 and the api/core services entry would
      change from 13 to 14.
     </para>
    </important>
   </step>
   <step>
    <para>
     To include Glance image space in your estimation, determine the size of
     the images that will be cached.
    </para>
   </step>
   <step>
    <para>
     Enter the total size needed to store Glance images in the
     <emphasis role="bold">/var/lib/glance/work_dir</emphasis>
     field.
    </para>
   </step>
  </procedure>
 </section>
 <section xml:id="disk_calc_select">
  <title>Select the Deployment Model</title>
  <para>
   To decide which architecture will meet all of your requirements, use the
   values given in the Disk Calculator spreadsheet. Keeping in mind the rough
   scale you expect to target as well as any need to separate services, choose
   an Entry Scale, Entry Scale MML, or Mid Scale deployment. Once you have
   chosen a deployment you can match it to the sample disk models in the
   <xref linkend="disk_calc_match"/> section. The following diagram shows the
   deployment options that are recommended if you use the default values in the
   Disk Calculator spreadsheet.
  </para>
  <figure xml:id="image_cmk_lxc_wv">
   <title>Deployment options diagram</title>
   <mediaobject>
    <imageobject role="fo">
     <imagedata fileref="media-DiskCalc3.png" width="75%" format="PNG"/>
    </imageobject>
    <imageobject role="html">
     <imagedata fileref="media-DiskCalc3.png"/>
    </imageobject>
   </mediaobject>
  </figure>
  <para>
   For example, in the above diagram, if you wanted to choose an Entry Scale
   MML deployment, the calculator recommends the following disk sizes:
  </para>
  <itemizedlist>
   <listitem>
    <para>
     216GB for API/Core Service
    </para>
   </listitem>
   <listitem>
    <para>
     216GB for Neutron (networking)
    </para>
   </listitem>
   <listitem>
    <para>
     216GB for Swift (storage)
    </para>
   </listitem>
   <listitem>
    <para>
     573GB for MMLB
    </para>
   </listitem>
   <listitem>
    <para>
     252GB for &mariadb;/RabbitMQ
    </para>
   </listitem>
  </itemizedlist>
 </section>
 <section xml:id="disk_calc_match">
  <title>Match to a Disk Model</title>
  <para>
   For each of the entry-scale and scale-out cloud models, there is a set of
   associated disk models that can be used as the basis for your deployment.
   These models provide examples of potential parameters for operational tools
   and are expected to be used as the starting point for actual deployments.
   Since each deployment can vary greatly, the disk calculator spreadsheet
   provides a way to create the basic disk model and customize it to fit the
   specific parameters your deployment. Once you have estimated disk sizes and
   chosen a deployment architecture, you can choose which example disk
   partitioning file to use from the tables below. Keep in mind if you are
   enabling more options than are listed in the Disk Calculator, or if you want
   to plan for growth, you will need to manually adjust paramters as necessary.
  </para>
  <para>
   Disk models are provided for each deployment option based on the expected
   size of the disk available to the control plane nodes. The available space
   is then partitioned by percentage to be allocated to each of the required
   volumes on the control plane. Each of the disk models is targeted at a
   specific set of parameters which can be found in the following tables:
  </para>
  <itemizedlist>
   <listitem>
    <para>
     <xref linkend="disk_calc_entry"/> for Entry Scale servers: 600 GB, 1 TB
    </para>
   </listitem>
   <listitem>
    <para>
     <xref linkend="disk_calc_Mid"/> for Mid Scale/Entry Scale MML servers: 600
     GB, 2 TB, 4.5 TB
    </para>
   </listitem>
  </itemizedlist>
 </section>
 <section xml:id="disk_calc_entry">
  <title>Entry Scale Disk Models</title>
  <para>
   These models include a single cluster of control plane nodes and all
   services.
  </para>
  <table xml:id="table_disk_matchE6" colsep="1" rowsep="1">
   <title>600GB Entry Scale</title>
   <tgroup cols="2">
    <colspec colname="c1" colnum="1" colwidth="1*"/>
    <colspec colname="c2" colnum="2" colwidth="2*"/>
    <thead>
     <row>
      <entry>Component</entry>
      <entry>Parameters</entry>
     </row>
    </thead>
    <tbody>
     <row>
      <entry>compute nodes</entry>
      <entry>
       <para>
        100
       </para>
       <para>
        This model provides lower than recommended retention and should only be
        used for POC deployments.
       </para>
      </entry>
     </row>
    </tbody>
   </tgroup>
  </table>
  <table xml:id="table_disk_matchE1" colsep="1" rowsep="1">
   <title>1TB Entry Scale</title>
   <tgroup cols="2">
    <colspec colname="c1" colnum="1" colwidth="1*"/>
    <colspec colname="c2" colnum="2" colwidth="2*"/>
    <thead>
     <row>
      <entry>Component</entry>
      <entry>Parameters</entry>
     </row>
    </thead>
    <tbody>
     <row>
      <entry>compute nodes</entry>
      <entry>100</entry>
     </row>
     <row>
      <entry>
       <para>
        local logging
       </para>
       <para>
        var/log
       </para>
      </entry>
      <entry>7 day retention</entry>
     </row>
     <row>
      <entry>
       <para>
        metering/monitoring
       </para>
       <para>
        /var/vertica
       </para>
      </entry>
      <entry>45 day retention</entry>
     </row>
     <row>
      <entry>
       <para>
        centralized logging
       </para>
       <para>
        /var/lib/elasticsearch
       </para>
      </entry>
      <entry>7 day retention</entry>
     </row>
     <row>
      <entry>
       <para>
        Kafka Message Queue
       </para>
       <para>
        /var/kafka
       </para>
      </entry>
      <entry>4 hour retention</entry>
     </row>
    </tbody>
   </tgroup>
  </table>
 </section>
 <section xml:id="disk_calc_Mid">
  <title>MML Disk Models</title>
  <para>
   These mid-scale and entry-scale MML models include separate control plane
   nodes for core services, metering/monitoring/logging, and
   &mariadb;/RabbitMQ. Optionally you can also separate out Swift (storage) and
   Neutron (networking). MML servers are the ones that will need modification
   based on the scale and operational parameters.
  </para>
  <table xml:id="table_disk_matchM6" colsep="1" rowsep="1">
   <title>600GB MML Server</title>
   <tgroup cols="2">
    <colspec colname="c1" colnum="1" colwidth="1*"/>
    <colspec colname="c2" colnum="2" colwidth="2*"/>
    <thead>
     <row>
      <entry>Component</entry>
      <entry>Parameters</entry>
     </row>
    </thead>
    <tbody>
     <row>
      <entry>compute nodes</entry>
      <entry>100</entry>
     </row>
     <row>
      <entry>
       <para>
        local logging
       </para>
       <para>
        var/log
       </para>
      </entry>
      <entry>7 day retention</entry>
     </row>
     <row>
      <entry>
       <para>
        metering/monitoring
       </para>
       <para>
        /var/vertica
       </para>
      </entry>
      <entry>
       <para>
        30 day retention (45 days is the default minimum)
       </para>
      </entry>
     </row>
     <row>
      <entry>
       <para>
        centralized logging
       </para>
       <para>
        /var/lib/elasticsearch
       </para>
      </entry>
      <entry>7 day retention</entry>
     </row>
     <row>
      <entry>
       <para>
        Kafka Message Queue
       </para>
       <para>
        /var/kafka
       </para>
      </entry>
      <entry>4 hour retention</entry>
     </row>
    </tbody>
   </tgroup>
  </table>
  <table xml:id="table_disk_matchM2T" colsep="1" rowsep="1">
   <title>2TB MML Server</title>
   <tgroup cols="2">
    <colspec colname="c1" colnum="1" colwidth="1*"/>
    <colspec colname="c2" colnum="2" colwidth="2*"/>
    <thead>
     <row>
      <entry>Component</entry>
      <entry>Parameters</entry>
     </row>
    </thead>
    <tbody>
     <row>
      <entry>compute nodes</entry>
      <entry>200</entry>
     </row>
     <row>
      <entry>
       <para>
        local logging
       </para>
       <para>
        var/log
       </para>
      </entry>
      <entry>7 day retention</entry>
     </row>
     <row>
      <entry>
       <para>
        metering/monitoring
       </para>
       <para>
        /var/vertica
       </para>
      </entry>
      <entry>45 day retention</entry>
     </row>
     <row>
      <entry>
       <para>
        centralized logging
       </para>
       <para>
        /var/lib/elasticsearch
       </para>
      </entry>
      <entry>7 day retention</entry>
     </row>
     <row>
      <entry>
       <para>
        Kafka Message Queue
       </para>
       <para>
        /var/kafka
       </para>
      </entry>
      <entry>12 hour retention</entry>
     </row>
    </tbody>
   </tgroup>
  </table>
  <table xml:id="table_disk_matchM4T" colsep="1" rowsep="1">
   <title>4.5TB MML Server</title>
   <tgroup cols="2">
    <colspec colname="c1" colnum="1" colwidth="1*"/>
    <colspec colname="c2" colnum="2" colwidth="2*"/>
    <thead>
     <row>
      <entry>Component</entry>
      <entry>Parameters</entry>
     </row>
    </thead>
    <tbody>
     <row>
      <entry>compute nodes</entry>
      <entry>200</entry>
     </row>
     <row>
      <entry>
       <para>
        local logging
       </para>
       <para>
        var/log
       </para>
      </entry>
      <entry>7 day retention</entry>
     </row>
     <row>
      <entry>
       <para>
        metering/monitoring
       </para>
       <para>
        /var/vertica
       </para>
      </entry>
      <entry>45 day retention</entry>
     </row>
     <row>
      <entry>
       <para>
        centralized logging
       </para>
       <para>
        /var/lib/elasticsearch
       </para>
      </entry>
      <entry>45 day retention</entry>
     </row>
     <row>
      <entry>
       <para>
        Kafka Message Queue
       </para>
       <para>
        /var/kafka
       </para>
      </entry>
      <entry>12 hour retention</entry>
     </row>
    </tbody>
   </tgroup>
  </table>
 </section>
 <section xml:id="disk_cinder">
  <title>Notes about disk sizing for Cinder bootable volumes</title>
  <para>
   When creating your disk model for nodes that will have the cinder volume
   role make sure that there is sufficient disk space allocated for a temporary
   space for image conversion if you will be creating bootable volumes.
  </para>
  <para>
   By default, Cinder uses <filename>/var/lib/cinder</filename> for image
   conversion and this will be on the root filesystem unless it is explicitly
   separated. You can ensure there is enough space by ensuring that the root
   file system is sufficiently large, or by creating a logical volume mounted
   at <filename>/var/lib/cinder</filename> in the disk model when installing
   the system.
  </para>
  <para>
   If you have post-installation issues with creating bootable volumes, see the
   <citetitle>FIXME: broken external xref</citetitle> documentation for steps
   to resolve these issues.
  </para>
 </section>
</section>
