<?xml version="1.0"?>
<?xml-stylesheet href="urn:x-suse:xslt:profiling:docbook51-profile.xsl"
 type="text/xml"
 title="Profiling step"?>
<!DOCTYPE section [
 <!ENTITY % entities SYSTEM "entity-decl.ent"> %entities;
]>
<section xmlns="http://docbook.org/ns/docbook" xmlns:xi="http://www.w3.org/2001/XInclude" xmlns:xlink="http://www.w3.org/1999/xlink" xml:id="lifecyclemanager_recovery">
 <title>&clm; Disaster Recovery</title>
 <para>
  In this scenario everything is still running (controller nodes and compute
  nodes) but you have lost either a dedicated &clm; or a shared
  &clm;/controller node.
 </para>
 <para>
  To ensure that you use the same version of &productname; that you previously had
  loaded on your &clm;, you will need to download and install the
  lifecycle management software using the instructions from the
  <xref linkend="sec.depl.adm_inst.add_on"/> before proceeding further.
 </para>
 <section>
  <title>Restore from a &swift; backup</title>
  <procedure>
   <step>
    <para>
     Log in to the &clm;.
    </para>
   </step>
   <step>
    <para>
     Access one of the other controller or compute nodes in your environment to
     perform the following steps:
    </para>
    <substeps>
     <step>
      <para>
       Retrieve the <filename>/var/lib/ardana/backup.osrc</filename> file and
       copy it to the <filename>/var/lib/ardana/</filename> directory on the
       &clm;.
      </para>
     </step>
     <step>
      <para>
       Copy all the files in the <literal>/var/lib/ca-certificates</literal>
       directory to the same directory on the &clm;.
      </para>
     </step>
     <step>
      <para>
       Retrieve the <literal>/etc/hosts</literal> file and replace the one
       found on the &clm;.
      </para>
     </step>
    </substeps>
   </step>
   <step>
    <para>
     Log back in to the &clm;.
    </para>
   </step>
   <step>
    <para>
     Update your ca-certificates:
    </para>
<screen>sudo update-ca-certificates</screen>
   </step>
   <step>
    <para>
     Edit the <literal>/etc/hosts</literal> file, ensuring you edit the
     127.0.0.1 line so it points to <literal>ardana</literal>:
    </para>
<screen>127.0.0.1       localhost ardana
::1             localhost ip6-localhost ip6-loopback
ff02::1         ip6-allnodes
ff02::2         ip6-allrouters</screen>
   </step>
  <step>
   <para>
    Retrieve the &clm; backups that were created with <xref
    linkend="clm_data_backup"/>. There are multiple backups; directories are
    handled differently than files.
    </para>
   </step>
   <step>
    <para>
     Create temporary directories and extract the TAR archives for each of
     the seven locations.
    </para>
    <screen>&prompt.ardana;sudo tar -z --incremental --extract --ignore-zeros
    --warning=none --overwrite --directory
    <replaceable>RESTORE_TARGET</replaceable> -f
    <replaceable>BACKUP_TARGET</replaceable>.tar.gz</screen>
    <para>
     For example, with a directory such as
     <replaceable>BACKUP_TARGET</replaceable>=<filename>/etc/ssh/</filename>
    </para>
    <screen>&prompt.ardana;sudo tar -z --incremental --extract --ignore-zeros \
--warning=none --overwrite --directory /etc/ssh/ -f ssh.tar.gz</screen>
    <para>
     With a file such as <replaceable>BACKUP_TARGET</replaceable>=/etc/passwd
    </para>
    <screen>&prompt.ardana;sudo tar -z --incremental --extract --ignore-zeros --warning=none --overwrite --directory /etc/ -f passwd.tar.gz</screen>
   </step>
   <step>
    <para>
     When the job completes, the previous &clm; contents should be
     restored to your home directory:
    </para>
<screen>&prompt.ardana;cd ~
&prompt.ardana;ls</screen>
   </step>
   <step>
    <para>
     If you are using Cobbler, restore your Cobbler configuration with these
     steps:
    </para>
    <substeps>
     <step>
      <para>
       Remove the following files:
      </para>
<screen>sudo rm -rf /var/lib/cobbler
sudo rm -rf /srv/www/cobbler</screen>
     </step>
     <step>
      <para>
       Deploy Cobbler:
      </para>
<screen>&prompt.ardana;cd ~/openstack/ardana/ansible
&prompt.ardana;ansible-playbook -i hosts/localhost cobbler-deploy.yml</screen>
     </step>
     <step>
      <para>
       Set the <literal>netboot-enabled</literal> flag for each of your nodes
       with this command:
      </para>
<screen>for h in $(sudo cobbler system list)
do
  sudo cobbler system edit --name=$h --netboot-enabled=0
done</screen>
     </step>
    </substeps>
   </step>
   <step>
    <para>
     Update your deployment directory:
    </para>
<screen>&prompt.ardana;cd ~/openstack/ardana/ansible
&prompt.ardana;ansible-playbook -i hosts/localhost ready_deployment.yml</screen>
   </step>
   <step>
    <para>
     If you are using a dedicated &clm;, follow these steps:
    </para>
    <substeps>
     <step>
      <para>
       re-run the deployment to ensure the &clm; is in the correct
       state:
      </para>
<screen>&prompt.ardana;cd ~/scratch/ansible/next/ardana/ansible
&prompt.ardana;ansible-playbook -i hosts/verb_hosts site.yml --limit localhost</screen>
     </step>
    </substeps>
   </step>
   <step>
    <para>
     If you are using a shared &clm;/controller, follow these
     steps:
    </para>
    <substeps>
     <step>
      <para>
       If the node is also a &clm; hypervisor, run the following commands to
       recreate the virtual machines that were lost:
      </para>
<screen>&prompt.ardana;cd ~/scratch/ansible/next/ardana/ansible
&prompt.ardana;ansible-playbook -i hosts/verb_hosts ardana-hypervisor-setup.yml --limit &lt;this node&gt;</screen>
     </step>
     <step>
      <para>
       If the node that was lost (or one of the VMs that it hosts) was a member
       of the RabbitMQ cluster then you need to remove the record of the old
       node, by running the following command <emphasis role="bold">on any one
       of the other cluster members</emphasis>. In this example the nodes are
       called <literal>cloud-cp1-rmq-mysql-m*-mgmt</literal> but you need to
       use the correct names for your system, which you can find in
       <literal>/etc/hosts</literal>:
      </para>
<screen>&prompt.ardana;cd ~/scratch/ansible/next/ardana/ansible
&prompt.ardana;ssh cloud-cp1-rmq-mysql-m3-mgmt sudo rabbitmqctl forget_cluster_node \
rabbit@cloud-cp1-rmq-mysql-m1-mgmt</screen>
     </step>
     <step>
      <para>
       Run the <literal>site.yml</literal> against the complete cloud to
       reinstall and rebuild the services that were lost. If you replaced one
       of the RabbitMQ cluster members then you will need to add the
       <literal>-e</literal> flag shown below, to nominate a new master node
       for the cluster, otherwise you can omit it.
      </para>
<screen>&prompt.ardana;cd ~/scratch/ansible/next/ardana/ansible
&prompt.ardana;ansible-playbook -i hosts/verb_hosts site.yml -e \
rabbit_primary_hostname=cloud-cp1-rmq-mysql-m3</screen>
     </step>
    </substeps>
   </step>
  </procedure>
 </section>
 <section>
  <title>Restore from an SSH backup</title>
  <procedure>
   <step>
    <para>
     Log in to the &clm;.
    </para>
   </step>
  <step>
   <para>
    Retrieve (with <command>scp</command>) the &clm; backups that were created
    with <xref linkend="clm_data_backup"/>. There are multiple backups;
    directories are handled differently than files.
    </para>
   </step>
   <step>
    <para>
     Create temporary directories and extract the TAR archives for each of
     the seven locations.
    </para>
    <screen>&prompt.ardana;sudo tar -z --incremental --extract --ignore-zeros
    --warning=none --overwrite --directory
    <replaceable>RESTORE_TARGET</replaceable> -f
    <replaceable>BACKUP_TARGET</replaceable>.tar.gz</screen>
    <para>
     For example, with a directory such as
     <replaceable>BACKUP_TARGET</replaceable>=<filename>/etc/ssh/</filename>
    </para>
    <screen>&prompt.ardana;sudo tar -z --incremental --extract --ignore-zeros \
--warning=none --overwrite --directory /etc/ssh/ -f ssh.tar.gz</screen>
    <para>
     With a file such as <replaceable>BACKUP_TARGET</replaceable>=/etc/passwd
    </para>
    <screen>&prompt.ardana;sudo tar -z --incremental --extract --ignore-zeros --warning=none --overwrite --directory /etc/ -f passwd.tar.gz</screen>
   </step>
   <step>
    <para>
     Update your deployment directory:
    </para>
<screen>&prompt.ardana;cd ~/openstack/ardana/ansible
&prompt.ardana;ansible-playbook -i hosts/localhost ready_deployment.yml</screen>
   </step>
   <step>
    <para>
     When the &clm; is restored, re-run the deployment to ensure
     the &clm; is in the correct state:
    </para>
<screen>&prompt.ardana;cd ~/scratch/ansible/next/ardana/ansible
&prompt.ardana;ansible-playbook -i hosts/verb_hosts site.yml --limit localhost</screen>
   </step>
  </procedure>
 </section>
</section>
