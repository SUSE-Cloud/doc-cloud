<?xml version="1.0"?>
<?xml-stylesheet href="urn:x-suse:xslt:profiling:docbook51-profile.xsl"
 type="text/xml"
 title="Profiling step"?>
<!DOCTYPE section [
 <!ENTITY % entities SYSTEM "entities.ent"> %entities;
]>
<section xmlns="http://docbook.org/ns/docbook" xmlns:xi="http://www.w3.org/2001/XInclude" xmlns:xlink="http://www.w3.org/1999/xlink" xml:id="lifecyclemanager_recovery">
 <title>&lcm; Disaster Recovery</title>
 <para>
  In this scenario everything is still running (controller nodes and compute
  nodes) but you have lost either a dedicated &lcm; or a shared
  &lcm;/controller node.
 </para>
 <para>
  To ensure that you use the same version of &kw-hos; that you previously had
  loaded on your &lcm;, you will need to download and install the
  lifecycle management software using the instructions from the
  <xref linkend="sec.depl.adm_inst.add_on"/> before proceeding further.
 </para>
 <section>
  <title>Restore from a &swift; backup</title>
  <procedure>
   <step>
    <para>
     Log in to the &lcm;.
    </para>
   </step>
   <step>
    <para>
     Install the freezer-agent using the following playbook:
    </para>
<screen>&prompt.ardana;cd ~/openstack/ardana/ansible/
&prompt.ardana;ansible-playbook -i hosts/localhost _deployer_restore_helper.yml</screen>
   </step>
   <step>
    <para>
     Access one of the other controller or compute nodes in your environment to
     perform the following steps:
    </para>
    <substeps>
     <step>
      <para>
       Retrieve the <filename>/var/lib/ardana/backup.osrc</filename> file and
       copy it to the <filename>/var/lib/ardana/</filename> directory on the
       &lcm;.
      </para>
     </step>
     <step>
      <para>
       Copy all the files in the
       <literal>/opt/stack/service/freezer-api/etc/</literal> directory to
       the same directory on the &lcm;.
      </para>
     </step>
     <step>
      <para>
       Copy all the files in the <literal>/var/lib/ca-certificates</literal>
       directory to the same directory on the &lcm;.
      </para>
     </step>
     <step>
      <para>
       Retrieve the <literal>/etc/hosts</literal> file and replace the one
       found on the &lcm;.
      </para>
     </step>
    </substeps>
   </step>
   <step>
    <para>
     Log back in to the &lcm;.
    </para>
   </step>
   <step>
    <para>
     Edit the value for <literal>client_id</literal> in the following file to
     contain the hostname of your &lcm;:
    </para>
<screen>/opt/stack/service/freezer-api/etc/freezer-api.conf</screen>
   </step>
   <step>
    <para>
     Update your ca-certificates:
    </para>
<screen>sudo update-ca-certificates</screen>
   </step>
   <step>
    <para>
     Edit the <literal>/etc/hosts</literal> file, ensuring you edit the
     127.0.0.1 line so it points to <literal>ardana</literal>:
    </para>
<screen>127.0.0.1       localhost ardana
::1             localhost ip6-localhost ip6-loopback
ff02::1         ip6-allnodes
ff02::2         ip6-allrouters</screen>
   </step>
   <step>
    <para>
     On the &lcm;, source the backup user credentials:
    </para>
<screen>&prompt.ardana;source ~/backup.osrc</screen>
   </step>
   <step>
    <para>
     Find the <literal>Client ID</literal>
     (<literal>ardana002-cp1-c0-m1-mgmt</literal>) for the host name as done in
     previous procedures (see <xref linkend="restore_swift_ssh_bu"/>).
    </para>
<screen><?dbsuse-fo font-size="0.65em"?>&prompt.ardana;freezer client-list
+-----------------------------+----------------------------------+-----------------------------+-------------+
| Client ID                   | uuid                             | hostname                    | description |
+-----------------------------+----------------------------------+-----------------------------+-------------+
| ardana002-cp1-comp0001-mgmt | f4d9cfe0725145fb91aaf95c80831dd6 | ardana002-cp1-comp0001-mgmt |             |
| ardana002-cp1-comp0002-mgmt | 55c93eb7d609467a8287f175a2275219 | ardana002-cp1-comp0002-mgmt |             |
| ardana002-cp1-c0-m1-mgmt    | 50d26318e81a408e97d1b6639b9404b2 | ardana002-cp1-c0-m1-mgmt    |             |
| ardana002-cp1-c1-m1-mgmt    | 78fe921473914bf6a802ad360c09d35b | ardana002-cp1-c1-m1-mgmt    |             |
| ardana002-cp1-c1-m2-mgmt    | b2e9a4305c4b4272acf044e3f89d327f | ardana002-cp1-c1-m2-mgmt    |             |
| ardana002-cp1-c1-m3-mgmt    | a3ceb80b8212425687dd11a92c8bc48e | ardana002-cp1-c1-m3-mgmt    |             |
+-----------------------------+----------------------------------+-----------------------------+-------------+</screen>
    <para>
     In this example, the <literal>hostname</literal> and the <literal>Client
     ID</literal> are the same: <literal>ardana002-cp1-c0-m1-mgmt</literal>.
    </para>
   </step>
   <step>
    <para>
     List the Freezer jobs
    </para>
<screen>&prompt.ardana;freezer job-list -C <replaceable>CLIENT ID</replaceable></screen>
    <para>
     Using the example in the previous step:
    </para>
<screen>&prompt.ardana;freezer job-list -C ardana002-cp1-c0-m1-mgmt</screen>
   </step>
   <step>
    <para>
     Get the id of the job corresponding to <literal>Ardana Default: deployer
     backup to Swift</literal>. Stop that job so the freezer scheduler does not
     begin making backups when started.
    </para>
<screen>&prompt.ardana;freezer job-stop <replaceable>JOB-ID</replaceable></screen>
    <para>
     If it is present, also stop the &lcm;'s SSH backup.
    </para>
   </step>
   <step>
    <para>
     Start the freezer scheduler:
    </para>
<screen>sudo systemctl start openstack-freezer-scheduler</screen>
   </step>
   <step>
    <para>
     Get the id of the job corresponding to <literal>Ardana Default: deployer
     restore from Swift</literal> and launch that job:
    </para>
<screen>&prompt.ardana;freezer job-start <replaceable>JOB-ID</replaceable></screen>
    <para>
     This will take some time. You can follow the progress by running
     <command>tail -f /var/log/freezer/freezer-scheduler.log</command>. Wait
     until the restore job is finished before doing the next step.
    </para>
   </step>
   <step>
    <para>
     When the job completes, the previous &lcm; contents should be
     restored to your home directory:
    </para>
<screen>&prompt.ardana;cd ~
&prompt.ardana;ls</screen>
   </step>
   <step>
    <para>
     If you are using Cobbler, restore your Cobbler configuration with these
     steps:
    </para>
    <substeps>
     <step>
      <para>
       Remove the following files:
      </para>
<screen>sudo rm -rf /var/lib/cobbler
sudo rm -rf /srv/www/cobbler</screen>
     </step>
     <step>
      <para>
       Deploy Cobbler:
      </para>
<screen>&prompt.ardana;cd ~/openstack/ardana/ansible
&prompt.ardana;ansible-playbook -i hosts/localhost cobbler-deploy.yml</screen>
     </step>
     <step>
      <para>
       Set the <literal>netboot-enabled</literal> flag for each of your nodes
       with this command:
      </para>
<screen>for h in $(sudo cobbler system list)
do
  sudo cobbler system edit --name=$h --netboot-enabled=0
done</screen>
     </step>
    </substeps>
   </step>
   <step>
    <para>
     Update your deployment directory:
    </para>
<screen>&prompt.ardana;cd ~/openstack/ardana/ansible
&prompt.ardana;ansible-playbook -i hosts/localhost ready_deployment.yml</screen>
   </step>
   <step>
    <para>
     If you are using a dedicated &lcm;, follow these steps:
    </para>
    <substeps>
     <step>
      <para>
       re-run the deployment to ensure the &lcm; is in the correct
       state:
      </para>
<screen>&prompt.ardana;cd ~/scratch/ansible/next/ardana/ansible
&prompt.ardana;ansible-playbook -i hosts/verb_hosts site.yml --limit localhost</screen>
     </step>
    </substeps>
   </step>
   <step>
    <para>
     If you are using a shared &lcm;/controller, follow these
     steps:
    </para>
    <substeps>
     <step>
      <para>
       If the node is also a &lcm; hypervisor, run the following commands to
       recreate the virtual machines that were lost:
      </para>
<screen>&prompt.ardana;cd ~/scratch/ansible/next/ardana/ansible
&prompt.ardana;ansible-playbook -i hosts/verb_hosts ardana-hypervisor-setup.yml --limit &lt;this node&gt;</screen>
     </step>
     <step>
      <para>
       If the node that was lost (or one of the VMs that it hosts) was a member
       of the RabbitMQ cluster then you need to remove the record of the old
       node, by running the following command <emphasis role="bold">on any one
       of the other cluster members</emphasis>. In this example the nodes are
       called <literal>cloud-cp1-rmq-mysql-m*-mgmt</literal> but you need to
       use the correct names for your system, which you can find in
       <literal>/etc/hosts</literal>:
      </para>
<screen>&prompt.ardana;cd ~/scratch/ansible/next/ardana/ansible
&prompt.ardana;ssh cloud-cp1-rmq-mysql-m3-mgmt sudo rabbitmqctl forget_cluster_node rabbit@cloud-cp1-rmq-mysql-m1-mgmt</screen>
     </step>
     <step>
      <para>
       Run the <literal>site.yml</literal> against the complete cloud to
       reinstall and rebuild the services that were lost. If you replaced one
       of the RabbitMQ cluster members then you will need to add the
       <literal>-e</literal> flag shown below, to nominate a new master node
       for the cluster, otherwise you can omit it.
      </para>
<screen>&prompt.ardana;cd ~/scratch/ansible/next/ardana/ansible
&prompt.ardana;ansible-playbook -i hosts/verb_hosts site.yml -e \
rabbit_primary_hostname=cloud-cp1-rmq-mysql-m3</screen>
     </step>
    </substeps>
   </step>
  </procedure>
 </section>
 <section>
  <title>Restore from an SSH backup</title>
  <procedure>
   <step>
    <para>
     On the &lcm;, edit the following file so it contains the same
     information as it did previously:
    </para>
<screen>&prompt.ardana;~/openstack/my_cloud/config/freezer/ssh_credentials.yml</screen>
   </step>
   <step>
    <para>
     On the &lcm;, copy the following files, change directories,
     and run the playbook _deployer_restore_helper.yml:
    </para>
<screen>&prompt.ardana;cp -r ~/hp-ci/openstack/* ~/openstack/my_cloud/definition/
&prompt.ardana;cd ~/openstack/ardana/ansible/
&prompt.ardana;ansible-playbook -i hosts/localhost _deployer_restore_helper.yml</screen>
   </step>
   <step>
    <para>
     Perform the restore. First become root and change directories:
    </para>
<screen>sudo su
&prompt.root;cd /root/deployer_restore_helper/</screen>
   </step>
   <step>
    <para>
     Execute the restore job:
    </para>
<screen>&prompt.ardana;./deployer_restore_script.sh</screen>
   </step>
   <step>
    <para>
     Update your deployment directory:
    </para>
<screen>&prompt.ardana;cd ~/openstack/ardana/ansible
&prompt.ardana;ansible-playbook -i hosts/localhost ready_deployment.yml</screen>
   </step>
   <step>
    <para>
     When the &lcm; is restored, re-run the deployment to ensure
     the &lcm; is in the correct state:
    </para>
<screen>&prompt.ardana;cd ~/scratch/ansible/next/ardana/ansible
&prompt.ardana;ansible-playbook -i hosts/verb_hosts site.yml --limit localhost</screen>
   </step>
  </procedure>
 </section>
</section>
