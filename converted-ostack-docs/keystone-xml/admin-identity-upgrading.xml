<?xml version="1.0" encoding="UTF-8" standalone="no"?>
<section xmlns="http://docbook.org/ns/docbook" xmlns:xlink="http://www.w3.org/1999/xlink" xmlns:xi="http://www.w3.org/2001/XInclude" version="5.1">
  <title>Upgrading Keystone</title>
  <para>As of the Newton release, keystone supports two different approaches to
            upgrading across releases. The traditional approach requires a significant
            outage to be scheduled for the entire duration of the upgrade process. The more
            modern approach results in zero downtime, but is more complicated due to a
            longer upgrade procedure.</para>
  <note>
    <para>The details of these steps are entirely dependent on the details of your
                specific deployment, such as your chosen application server and database
                management system. Use it only as a guide when implementing your own
                upgrade process.</para>
  </note>
  <section>
    <title>Before you begin</title>
    <para>Plan your upgrade:</para>
    <itemizedlist>
      <listitem>
        <para>Read and ensure you understand the <link xlink:href="https://docs.openstack.org/releasenotes/keystone/">release notes</link> for the next release.</para>
      </listitem>
      <listitem>
        <para>Resolve any outstanding deprecation warnings in your logs. Some deprecation
                        cycles are as short as a single release, so it’s possible to break a
                        deployment if you leave <emphasis>any</emphasis> outstanding warnings. It might be a good idea
                        to re-read the release notes for the previous release (or two!).</para>
      </listitem>
      <listitem>
        <para>Prepare your new configuration files, including <literal>keystone.conf</literal>,
                        <literal>logging.conf</literal>, <literal>policy.json</literal>, <literal>keystone-paste.ini</literal>, and anything else
                        in <literal>/etc/keystone/</literal>, by customizing the corresponding files from the next
                        release.</para>
      </listitem>
    </itemizedlist>
  </section>
  <section xml:id="upgrading-with-downtime">
    <title>Upgrading with downtime</title>
    <para>This is a high-level description of our upgrade strategy built around
                <literal>keystone-manage db_sync</literal>. It assumes that you are willing to have downtime
                of your control plane during the upgrade process and presents minimal risk.
                With keystone unavailable, no other OpenStack services will be able to
                authenticate requests, effectively preventing the rest of the control plane
                from functioning normally.</para>
    <procedure>
      <step>
        <para>Stop all keystone processes. Otherwise, you’ll risk multiple releases of
                        keystone trying to write to the database at the same time, which may result
                        in data being inconsistently written and read.</para>
      </step>
      <step>
        <para>Make a backup of your database. Keystone does not support downgrading the
                        database, so restoring from a full backup is your only option for recovery
                        in the event of an upgrade failure.</para>
      </step>
      <step>
        <para>Upgrade all keystone nodes to the next release.</para>
      </step>
      <step>
        <para>Update your configuration files (<literal>/etc/keystone/</literal>) with those
                        corresponding from the latest release.</para>
      </step>
      <step>
        <para>Run <literal>keystone-manage db_sync</literal> from any single node to upgrade both the
                        database schema and run any corresponding database migrations.</para>
      </step>
      <step>
        <para>(<emphasis>New in Newton</emphasis>) Run <literal>keystone-manage doctor</literal> to diagnose symptoms of
                        common deployment issues and receive instructions for resolving them.</para>
      </step>
      <step>
        <para>Start all keystone processes.</para>
      </step>
    </procedure>
  </section>
  <section>
    <title>Upgrading with minimal downtime</title>
    <para>If you run a multi-node keystone cluster that uses a replicated database, like
                a Galera cluster, it is possible to upgrade with minimal downtime. This method
                also optimizes recovery time from a failed upgrade. This section assumes
                familiarity with the base case (<xref linkend="upgrading-with-downtime"/>) outlined above.
                In these steps the nodes will be divided into <literal>first</literal> and <literal>other</literal> nodes.</para>
    <procedure>
      <step>
        <para>Backup your database. There is no way to rollback the upgrade of keystone
                        and this is your worst-case fallback option.</para>
      </step>
      <step>
        <para>Disable keystone on all nodes but the <literal>first</literal> node. This can be done via a
                        variety of mechanisms that will depend on the deployment. If you are unable
                        to disable a service or place a service into maintenance mode in your load
                        balancer, you can stop the keystone processes.</para>
      </step>
      <step>
        <para>Stop the database service on one of the <literal>other</literal> nodes in the cluster. This
                        will isolate the old dataset on a single node in the cluster. In the event
                        of a failed update this data can be used to rebuild the cluster without
                        having to restore from backup.</para>
      </step>
      <step>
        <para>Update the configuration files on the <literal>first</literal> node.</para>
      </step>
      <step>
        <para>Upgrade keystone on the <literal>first</literal> node. keystone is now down for your cloud.</para>
      </step>
      <step>
        <para>Run <literal>keystone-manage db_sync</literal> on the <literal>first</literal> node. As soon as this
                        finishes, keystone is now working again on a single node in the cluster.</para>
      </step>
      <step>
        <para>keystone is now upgraded on a single node. Your load balancers will be
                        sending all traffic to this single node. This is your chance to run
                        ensure keystone up and running, and not broken. If keystone is broken, see
                        the <xref linkend="rollback-after-a-failed-upgrade"/> section below.</para>
      </step>
      <step>
        <para>Once you have verified that keystone is up and running, begin the upgrade on
                        the <literal>other</literal> nodes. This entails updating configuration files and upgrading
                        the code. The <literal>db_sync</literal> does not need to be run again.</para>
      </step>
      <step>
        <para>On the node where you stopped the database service, be sure to restart
                        it and ensure that it properly rejoins the cluster.</para>
      </step>
    </procedure>
    <para>Using this model, the outage window is minimized because the only time
                when your cluster is totally offline is between loading the newer version
                of keystone and running the <literal>db_sync</literal> command. Typically the outage with
                this method can be measured in tens of seconds especially if automation is
                used.</para>
    <section xml:id="rollback-after-a-failed-upgrade">
      <title>Rollback after a failed upgrade</title>
      <para>If the upgrade fails, only a single node has been affected. This makes the
                    recovery simpler and quicker. If issues are not discovered until the entire
                    cluster is upgraded, a full shutdown and restore from backup will be required.
                    That will take much longer than just fixing a single node with an old copy of
                    the database still available. This process will be dependent on your
                    architecture and it is highly recommended that you’ve practiced this in a
                    development environment before trying to use it for the first time.</para>
      <procedure>
        <step>
          <para>Isolate the bad node. Shutdown keystone and the database services
                            on the upgraded “bad” node.</para>
        </step>
        <step>
          <para>Bootstrap the database cluster from the node holding the old data.
                            This may require wiping the data first on any nodes who are not
                            holding old data.</para>
        </step>
        <step>
          <para>Enable keystone on the old nodes in your load balancer or if
                            the processes were stopped, restart them.</para>
        </step>
        <step>
          <para>Validate that keystone is working.</para>
        </step>
        <step>
          <para>Downgrade the code and config files on the bad node.</para>
        </step>
      </procedure>
      <para>This process should be doable in a matter of minutes and will minimize cloud
                    downtime if it is required.</para>
    </section>
  </section>
  <section>
    <title>Upgrading without downtime</title>
    <para>This is a high-level description of our upgrade strategy built around
                additional options in <literal>keystone-manage db_sync</literal>. Although it is much more
                complex than the upgrade process described above, it assumes that you are not
                willing to have downtime of your control plane during the upgrade process. With
                this upgrade process, end users will still be able to authenticate to receive
                tokens normally, and other OpenStack services will still be able to
                authenticate requests normally.</para>
    <procedure>
      <step>
        <para>Make a backup of your database. keystone does not support downgrading the
                        database, so restoring from a full backup is your only option for recovery
                        in the event of an upgrade failure.</para>
      </step>
      <step>
        <para>Stop the keystone processes on the first node (or really, any arbitrary
                        node). This node will serve to orchestrate database upgrades.</para>
      </step>
      <step>
        <para>Upgrade your first node to the next release, but do not start any keystone
                        processes.</para>
      </step>
      <step>
        <para>Update your configuration files on the first node (<literal>/etc/keystone/</literal>) with
                        those corresponding to the latest release.</para>
      </step>
      <step>
        <para>(<emphasis>New in Newton</emphasis>) Run <literal>keystone-manage doctor</literal> on the first node to
                        diagnose symptoms of common deployment issues and receive instructions for
                        resolving them.</para>
      </step>
      <step>
        <para>(<emphasis>New in Newton</emphasis>) Run <literal>keystone-manage db_sync --expand</literal> on the first node
                        to expand the database schema to a superset of what both the previous and
                        next release can utilize, and create triggers to facilitate the live
                        migration process.</para>
        <warning>
          <para>For MySQL, using the <literal>keystone-manage db_sync --expand</literal> command requires
                            that you either grant your keystone user <literal>SUPER</literal> privileges, or run
                            <literal>set global log_bin_trust_function_creators=1;</literal> in mysql beforehand.</para>
        </warning>
        <para>At this point, new columns and tables may exist in the database, but will
                        <emphasis>not</emphasis> all be populated in such a way that the next release will be able to
                        function normally.</para>
        <para>As the previous release continues to write to the old schema, database
                        triggers will live migrate the data to the new schema so it can be read by
                        the next release.</para>
      </step>
      <step>
        <para>(<emphasis>New in Newton</emphasis>) Run <literal>keystone-manage db_sync --migrate</literal> on the first
                        node to forcefully perform data migrations. This process will migrate all
                        data from the old schema to the new schema while the previous release
                        continues to operate normally.</para>
        <para>When this process completes, all data will be available in both the new
                        schema and the old schema, so both the previous release and the next release
                        will be capable of operating normally.</para>
      </step>
      <step>
        <para>Update your configuration files (<literal>/etc/keystone/</literal>) on all nodes (except
                        the first node, which you’ve already done) with those corresponding to the
                        latest release.</para>
      </step>
      <step>
        <para>Upgrade all keystone nodes to the next release, and restart them one at a
                        time. During this step, you’ll have a mix of releases operating side by
                        side, both writing to the database.</para>
        <para>As the next release begins writing to the new schema, database triggers will
                        also migrate the data to the old schema, keeping both data schemas in sync.</para>
      </step>
      <step>
        <para>(<emphasis>New in Newton</emphasis>) Run <literal>keystone-manage db_sync --contract</literal> to remove the
                        old schema and all data migration triggers.</para>
        <para>When this process completes, the database will no longer be able to support
                        the previous release.</para>
      </step>
    </procedure>
    <section>
      <title>Using db_sync check</title>
      <para>(<emphasis>New in Pike</emphasis>) In order to check the current state of your rolling upgrades,
                    you may run the command <literal>keystone-manage db_sync --check</literal>. This will inform
                    you of any outstanding actions you have left to take as well as any possible
                    upgrades you can make from your current version. Here are a list of possible
                    return codes.</para>
      <itemizedlist>
        <listitem>
          <para>A return code of <literal>0</literal> means you are currently up to date with the latest
                            migration script version and all <literal>db_sync</literal> commands are complete.</para>
        </listitem>
        <listitem>
          <para>A return code of <literal>1</literal> generally means something serious is wrong with your
                            database and operator intervention will be required.</para>
        </listitem>
        <listitem>
          <para>A return code of <literal>2</literal> means that an upgrade from your current database
                            version is available, your database is not currently under version control,
                            or the database is already under control. Your first step is to run
                            <literal>keystone-manage db_sync --expand</literal>.</para>
        </listitem>
        <listitem>
          <para>A return code of <literal>3</literal> means that the expansion stage is complete, and the
                            next step is to run <literal>keystone-manage db_sync --migrate</literal>.</para>
        </listitem>
        <listitem>
          <para>A return code of <literal>4</literal> means that the expansion and data migration stages are
                            complete, and the next step is to run <literal>keystone-manage db_sync --contract</literal>.</para>
        </listitem>
      </itemizedlist>
    </section>
  </section>
</section>
