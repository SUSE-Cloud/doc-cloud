<?xml version="1.0" encoding="UTF-8" standalone="no"?>
<section xmlns="http://docbook.org/ns/docbook" xmlns:xlink="http://www.w3.org/1999/xlink" xmlns:xi="http://www.w3.org/2001/XInclude" version="5.1">
  <title>Administrator’s Guide</title>
  <section>
    <title>Defining Storage Policies</title>
    <para>Defining your Storage Policies is very easy to do with Swift.  It is important
                that the administrator understand the concepts behind Storage Policies
                before actually creating and using them in order to get the most benefit out
                of the feature and, more importantly, to avoid having to make unnecessary changes
                once a set of policies have been deployed to a cluster.</para>
    <para>It is highly recommended that the reader fully read and comprehend
                 before proceeding with administration of
                policies.  Plan carefully and it is suggested that experimentation be
                done first on a non-production cluster to be certain that the desired
                configuration meets the needs of the users.  See <xref linkend="upgrade-policy"/>
                before planning the upgrade of your existing deployment.</para>
    <para>Following is a high level view of the very few steps it takes to configure
                policies once you have decided what you want to do:</para>
    <procedure>
      <step>
        <para>Define your policies in <literal>/etc/swift/swift.conf</literal></para>
      </step>
      <step>
        <para>Create the corresponding object rings</para>
      </step>
      <step>
        <para>Communicate the names of the Storage Policies to cluster users</para>
      </step>
    </procedure>
    <para>For a specific example that takes you through these steps, please see
                </para>
  </section>
  <section>
    <title>Managing the Rings</title>
    <para>You may build the storage rings on any server with the appropriate
                version of Swift installed.  Once built or changed (rebalanced), you
                must distribute the rings to all the servers in the cluster.  Storage
                rings contain information about all the Swift storage partitions and
                how they are distributed between the different nodes and disks.</para>
    <para>Swift 1.6.0 is the last version to use a Python pickle format.
                Subsequent versions use a different serialization format.  <emphasis role="bold">Rings
                    generated by Swift versions 1.6.0 and earlier may be read by any
                    version, but rings generated after 1.6.0 may only be read by Swift
                    versions greater than 1.6.0.</emphasis>  So when upgrading from version 1.6.0 or
                earlier to a version greater than 1.6.0, either upgrade Swift on your
                ring building server <emphasis role="bold">last</emphasis> after all Swift nodes have been successfully
                upgraded, or refrain from generating rings until all Swift nodes have
                been successfully upgraded.</para>
    <para>If you need to downgrade from a version of Swift greater than 1.6.0 to
                a version less than or equal to 1.6.0, first downgrade your ring-building
                server, generate new rings, push them out, then continue with the rest
                of the downgrade.</para>
    <para>For more information see .</para>
    <para>Removing a device from the ring:</para>
    <screen>swift-ring-builder &lt;builder-file&gt; remove &lt;ip_address&gt;/&lt;device_name&gt;</screen>
    <para>Removing a server from the ring:</para>
    <screen>swift-ring-builder &lt;builder-file&gt; remove &lt;ip_address&gt;</screen>
    <para>Adding devices to the ring:</para>
    <para>See <xref linkend="ring-preparing"/></para>
    <para>See what devices for a server are in the ring:</para>
    <screen>swift-ring-builder &lt;builder-file&gt; search &lt;ip_address&gt;</screen>
    <para>Once you are done with all changes to the ring, the changes need to be
                “committed”:</para>
    <screen>swift-ring-builder &lt;builder-file&gt; rebalance</screen>
    <para>Once the new rings are built, they should be pushed out to all the servers
                in the cluster.</para>
    <para>Optionally, if invoked as ‘swift-ring-builder-safe’ the directory containing
                the specified builder file will be locked (via a .lock file in the parent
                directory). This provides a basic safe guard against multiple instances
                of the swift-ring-builder (or other utilities that observe this lock) from
                attempting to write to or read the builder/ring files while operations are in
                progress. This can be useful in environments where ring management has been
                automated but the operator still needs to interact with the rings manually.</para>
    <para>If the ring builder is not producing the balances that you are
                expecting, you can gain visibility into what it’s doing with the
                <literal>--debug</literal> flag.:</para>
    <screen>swift-ring-builder &lt;builder-file&gt; rebalance --debug</screen>
    <para>This produces a great deal of output that is mostly useful if you are
                either (a) attempting to fix the ring builder, or (b) filing a bug
                against the ring builder.</para>
    <para>You may notice in the rebalance output a ‘dispersion’ number. What this
                number means is explained in <xref linkend="ring-dispersion"/> but in essence
                is the percentage of partitions in the ring that have too many replicas
                within a particular failure domain. You can ask ‘swift-ring-builder’ what
                the dispersion is with:</para>
    <screen>swift-ring-builder &lt;builder-file&gt; dispersion</screen>
    <para>This will give you the percentage again, if you want a detailed view of
                the dispersion simply add a <literal>--verbose</literal>:</para>
    <screen>swift-ring-builder &lt;builder-file&gt; dispersion --verbose</screen>
    <para>This will not only display the percentage but will also display a dispersion
                table that lists partition dispersion by tier. You can use this table to figure
                out were you need to add capacity or to help tune an <xref linkend="ring-overload"/> value.</para>
    <para>Now let’s take an example with 1 region, 3 zones and 4 devices. Each device has
                the same weight, and the <literal>dispersion --verbose</literal> might show the following:</para>
    <screen>Dispersion is 50.000000, Balance is 0.000000, Overload is 0.00%
Required overload is 33.333333%
Worst tier is 50.000000 (r1z3)
--------------------------------------------------------------------------
Tier                           Parts      %    Max     0     1     2     3
--------------------------------------------------------------------------
r1                               256   0.00      3     0     0     0   256
r1z1                             192   0.00      1    64   192     0     0
r1z1-127.0.0.1                   192   0.00      1    64   192     0     0
r1z1-127.0.0.1/sda               192   0.00      1    64   192     0     0
r1z2                             192   0.00      1    64   192     0     0
r1z2-127.0.0.2                   192   0.00      1    64   192     0     0
r1z2-127.0.0.2/sda               192   0.00      1    64   192     0     0
r1z3                             256  50.00      1     0   128   128     0
r1z3-127.0.0.3                   256  50.00      1     0   128   128     0
r1z3-127.0.0.3/sda               192   0.00      1    64   192     0     0
r1z3-127.0.0.3/sdb               192   0.00      1    64   192     0     0</screen>
    <para>The first line reports that there are 256 partitions with 3 copies in region 1;
                and this is an expected output in this case (single region with 3 replicas) as
                reported by the “Max” value.</para>
    <para>However, there is some imbalance in the cluster, more precisely in zone 3. The
                “Max” reports a maximum of 1 copy in this zone; however 50.00% of the partitions
                are storing 2 replicas in this zone (which is somewhat expected, because there
                are more disks in this zone).</para>
    <para>You can now either add more capacity to the other zones, decrease the total
                weight in zone 3 or set the overload to a value <literal>greater than</literal> 33.333333% -
                only as much overload as needed will be used.</para>
  </section>
  <section>
    <title>Scripting Ring Creation</title>
    <para>You can create scripts to create the account and container rings and rebalance. Here’s an example script for the Account ring. Use similar commands to create a make-container-ring.sh script on the proxy server node.</para>
    <procedure>
      <step>
        <para>Create a script file called make-account-ring.sh on the proxy
                        server node with the following content:</para>
        <screen><?dbsuse-fo font-size="8pt"?>#!/bin/bash
cd /etc/swift
rm -f account.builder account.ring.gz backups/account.builder backups/account.ring.gz
swift-ring-builder account.builder create 18 3 1
swift-ring-builder account.builder add r1z1-&lt;account-server-1&gt;:6202/sdb1 1
swift-ring-builder account.builder add r1z2-&lt;account-server-2&gt;:6202/sdb1 1
swift-ring-builder account.builder rebalance</screen>
        <para>You need to replace the values of &lt;account-server-1&gt;,
                        &lt;account-server-2&gt;, etc. with the IP addresses of the account
                        servers used in your setup. You can have as many account servers as
                        you need. All account servers are assumed to be listening on port
                        6202, and have a storage device called “sdb1” (this is a directory
                        name created under /drives when we setup the account server). The
                        “z1”, “z2”, etc. designate zones, and you can choose whether you
                        put devices in the same or different zones. The “r1” designates
                        the region, with different regions specified as “r1”, “r2”, etc.</para>
      </step>
      <step>
        <para>Make the script file executable and run it to create the account ring file:</para>
        <screen>chmod +x make-account-ring.sh
sudo ./make-account-ring.sh</screen>
      </step>
      <step>
        <para>Copy the resulting ring file /etc/swift/account.ring.gz to all the
                        account server nodes in your Swift environment, and put them in the
                        /etc/swift directory on these nodes. Make sure that every time you
                        change the account ring configuration, you copy the resulting ring
                        file to all the account nodes.</para>
      </step>
    </procedure>
  </section>
  <section>
    <title>Handling System Updates</title>
    <para>It is recommended that system updates and reboots are done a zone at a time.
                This allows the update to happen, and for the Swift cluster to stay available
                and responsive to requests.  It is also advisable when updating a zone, let
                it run for a while before updating the other zones to make sure the update
                doesn’t have any adverse effects.</para>
  </section>
  <section>
    <title>Handling Drive Failure</title>
    <para>In the event that a drive has failed, the first step is to make sure the drive
                is unmounted.  This will make it easier for Swift to work around the failure
                until it has been resolved.  If the drive is going to be replaced immediately,
                then it is just best to replace the drive, format it, remount it, and let
                replication fill it up.</para>
    <para>After the drive is unmounted, make sure the mount point is owned by root
                (root:root 755). This ensures that rsync will not try to replicate into the
                root drive once the failed drive is unmounted.</para>
    <para>If the drive can’t be replaced immediately, then it is best to leave it
                unmounted, and set the device weight to 0. This will allow all the
                replicas that were on that drive to be replicated elsewhere until the drive
                is replaced. Once the drive is replaced, the device weight can be increased
                again. Setting the device weight to 0 instead of removing the drive from the
                ring gives Swift the chance to replicate data from the failing disk too (in case
                it is still possible to read some of the data).</para>
    <para>Setting the device weight to 0 (or removing a failed drive from the ring) has
                another benefit: all partitions that were stored on the failed drive are
                distributed over the remaining disks in the cluster, and each disk only needs to
                store a few new partitions. This is much faster compared to replicating all
                partitions to a single, new disk. It decreases the time to recover from a
                degraded number of replicas significantly, and becomes more and more important
                with bigger disks.</para>
  </section>
  <section>
    <title>Handling Server Failure</title>
    <para>If a server is having hardware issues, it is a good idea to make sure the
                Swift services are not running.  This will allow Swift to work around the
                failure while you troubleshoot.</para>
    <para>If the server just needs a reboot, or a small amount of work that should
                only last a couple of hours, then it is probably best to let Swift work
                around the failure and get the machine fixed and back online.  When the
                machine comes back online, replication will make sure that anything that is
                missing during the downtime will get updated.</para>
    <para>If the server has more serious issues, then it is probably best to remove
                all of the server’s devices from the ring.  Once the server has been repaired
                and is back online, the server’s devices can be added back into the ring.
                It is important that the devices are reformatted before putting them back
                into the ring as it is likely to be responsible for a different set of
                partitions than before.</para>
  </section>
  <section>
    <title>Detecting Failed Drives</title>
    <para>It has been our experience that when a drive is about to fail, error messages
                will spew into <literal>/var/log/kern.log</literal>.  There is a script called
                <literal>swift-drive-audit</literal> that can be run via cron to watch for bad drives.  If
                errors are detected, it will unmount the bad drive, so that Swift can
                work around it.  The script takes a configuration file with the following
                settings:</para>
    <para>
      <literal>[drive-audit]</literal>
    </para>
    <informaltable>
      <tgroup cols="3">
        <colspec colname="c1" colwidth="24.0*"/>
        <colspec colname="c2" colwidth="18.7*"/>
        <colspec colname="c3" colwidth="57.3*"/>
        <tbody>
          <row>
            <entry>
              <para>Option</para>
            </entry>
            <entry>
              <para>Default</para>
            </entry>
            <entry>
              <para>Description</para>
            </entry>
          </row>
          <row>
            <entry>
              <para>user</para>
            </entry>
            <entry>
              <para>swift</para>
            </entry>
            <entry>
              <para>Drop privileges to this user for non-root
                                    tasks</para>
            </entry>
          </row>
          <row>
            <entry>
              <para>log_facility</para>
            </entry>
            <entry>
              <para>LOG_LOCAL0</para>
            </entry>
            <entry>
              <para>Syslog log facility</para>
            </entry>
          </row>
          <row>
            <entry>
              <para>log_level</para>
            </entry>
            <entry>
              <para>INFO</para>
            </entry>
            <entry>
              <para>Log level</para>
            </entry>
          </row>
          <row>
            <entry>
              <para>device_dir</para>
            </entry>
            <entry>
              <para>/srv/node</para>
            </entry>
            <entry>
              <para>Directory devices are mounted under</para>
            </entry>
          </row>
          <row>
            <entry>
              <para>minutes</para>
            </entry>
            <entry>
              <para>60</para>
            </entry>
            <entry>
              <para>Number of minutes to look back in
                                    <literal>/var/log/kern.log</literal></para>
            </entry>
          </row>
          <row>
            <entry>
              <para>error_limit</para>
            </entry>
            <entry>
              <para>1</para>
            </entry>
            <entry>
              <para>Number of errors to find before a device
                                    is unmounted</para>
            </entry>
          </row>
          <row>
            <entry>
              <para>log_file_pattern</para>
            </entry>
            <entry>
              <para>/var/log/kern*</para>
            </entry>
            <entry>
              <para>Location of the log file with globbing
                                    pattern to check against device errors</para>
            </entry>
          </row>
          <row>
            <entry>
              <para>regex_pattern_X</para>
            </entry>
            <entry>
              <para>(see below)</para>
            </entry>
            <entry>
              <para>Regular expression patterns to be used to
                                    locate device blocks with errors in the
                                    log file</para>
            </entry>
          </row>
        </tbody>
      </tgroup>
    </informaltable>
    <para>The default regex pattern used to locate device blocks with errors are
                <literal>berrorb.*b(sd[a-z]{1,2}d?)b</literal> and <literal>b(sd[a-z]{1,2}d?)b.*berrorb</literal>.
                One is able to overwrite the default above by providing new expressions
                using the format <literal>regex_pattern_X = regex_expression</literal>, where <literal>X</literal> is a number.</para>
    <para>This script has been tested on Ubuntu 10.04 and Ubuntu 12.04, so if you are
                using a different distro or OS, some care should be taken before using in production.</para>
  </section>
  <section>
    <title>Preventing Disk Full Scenarios</title>
    <para>Prevent disk full scenarios by ensuring that the <literal>proxy-server</literal> blocks PUT
                requests and rsync prevents replication to the specific drives.</para>
    <para>You can prevent <literal>proxy-server</literal> PUT requests to low space disks by ensuring
                <literal>fallocate_reserve</literal> is set in the <literal>object-server.conf</literal>. By default,
                <literal>fallocate_reserve</literal> is set to 1%. This blocks PUT requests that leave the
                free disk space below 1% of the disk.</para>
    <para>In order to prevent rsync replication to specific drives, firstly
                setup <literal>rsync_module</literal> per disk in your <literal>object-replicator</literal>.
                Set this in <literal>object-server.conf</literal>:</para>
    <screen>[object-replicator]
rsync_module = {replication_ip}::object_{device}</screen>
    <para>Set the individual drives in <literal>rsync.conf</literal>. For example:</para>
    <screen>[object_sda]
max connections = 4
lock file = /var/lock/object_sda.lock

[object_sdb]
max connections = 4
lock file = /var/lock/object_sdb.lock</screen>
    <para>Finally, monitor the disk space of each disk and adjust the rsync
                <literal>max connections</literal> per drive to <literal>-1</literal>. We recommend utilising your existing
                monitoring solution to achieve this. The following is an example script:</para>
    <screen language="python">#!/usr/bin/env python
import os
import errno

RESERVE = 500 * 2 ** 20  # 500 MiB

DEVICES = '/srv/node1'

path_template = '/etc/rsync.d/disable_%s.conf'
config_template = '''
[object_%s]
max connections = -1
'''

def disable_rsync(device):
    with open(path_template % device, 'w') as f:
        f.write(config_template.lstrip() % device)


def enable_rsync(device):
    try:
        os.unlink(path_template % device)
    except OSError as e:
        # ignore file does not exist
        if e.errno != errno.ENOENT:
            raise


for device in os.listdir(DEVICES):
    path = os.path.join(DEVICES, device)
    st = os.statvfs(path)
    free = st.f_bavail * st.f_frsize
    if free &lt; RESERVE:
        disable_rsync(device)
    else:
        enable_rsync(device)</screen>
    <para>For the above script to work, ensure <literal>/etc/rsync.d/</literal> conf files are
                included, by specifying <literal>&amp;include</literal> in your <literal>rsync.conf</literal> file:</para>
    <screen>&amp;include /etc/rsync.d</screen>
    <para>Use this in conjunction with a cron job to periodically run the script, for example:</para>
    <screen># /etc/cron.d/devicecheck
* * * * * root /some/path/to/disable_rsync.py</screen>
  </section>
  <section xml:id="dispersion-report">
    <title>Dispersion Report</title>
    <para>There is a swift-dispersion-report tool for measuring overall cluster health.
                This is accomplished by checking if a set of deliberately distributed
                containers and objects are currently in their proper places within the cluster.</para>
    <para>For instance, a common deployment has three replicas of each object. The health
                of that object can be measured by checking if each replica is in its proper
                place. If only 2 of the 3 is in place the object’s heath can be said to be at
                66.66%, where 100% would be perfect.</para>
    <para>A single object’s health, especially an older object, usually reflects the
                health of that entire partition the object is in. If we make enough objects on
                a distinct percentage of the partitions in the cluster, we can get a pretty
                valid estimate of the overall cluster health. In practice, about 1% partition
                coverage seems to balance well between accuracy and the amount of time it takes
                to gather results.</para>
    <para>The first thing that needs to be done to provide this health value is create a
                new account solely for this usage. Next, we need to place the containers and
                objects throughout the system so that they are on distinct partitions. The
                swift-dispersion-populate tool does this by making up random container and
                object names until they fall on distinct partitions. Last, and repeatedly for
                the life of the cluster, we need to run the swift-dispersion-report tool to
                check the health of each of these containers and objects.</para>
    <para>These tools need direct access to the entire cluster and to the ring files
                (installing them on a proxy server will probably do). Both
                swift-dispersion-populate and swift-dispersion-report use the same
                configuration file, /etc/swift/dispersion.conf. Example conf file:</para>
    <screen>[dispersion]
auth_url = http://localhost:8080/auth/v1.0
auth_user = test:tester
auth_key = testing
endpoint_type = internalURL</screen>
    <para>There are also options for the conf file for specifying the dispersion coverage
                (defaults to 1%), retries, concurrency, etc. though usually the defaults are
                fine. If you want to use keystone v3 for authentication there are options like
                auth_version, user_domain_name, project_domain_name and project_name.</para>
    <para>Once the configuration is in place, run <literal>swift-dispersion-populate</literal> to populate
                the containers and objects throughout the cluster.</para>
    <para>Now that those containers and objects are in place, you can run
                <literal>swift-dispersion-report</literal> to get a dispersion report, or the overall health of
                the cluster. Here is an example of a cluster in perfect health:</para>
    <screen>$ swift-dispersion-report
Queried 2621 containers for dispersion reporting, 19s, 0 retries
100.00% of container copies found (7863 of 7863)
Sample represents 1.00% of the container partition space

Queried 2619 objects for dispersion reporting, 7s, 0 retries
100.00% of object copies found (7857 of 7857)
Sample represents 1.00% of the object partition space</screen>
    <para>Now I’ll deliberately double the weight of a device in the object ring (with
                replication turned off) and rerun the dispersion report to show what impact
                that has:</para>
    <screen>$ swift-ring-builder object.builder set_weight d0 200
$ swift-ring-builder object.builder rebalance
...
$ swift-dispersion-report
Queried 2621 containers for dispersion reporting, 8s, 0 retries
100.00% of container copies found (7863 of 7863)
Sample represents 1.00% of the container partition space

Queried 2619 objects for dispersion reporting, 7s, 0 retries
There were 1763 partitions missing one copy.
77.56% of object copies found (6094 of 7857)
Sample represents 1.00% of the object partition space</screen>
    <para>You can see the health of the objects in the cluster has gone down
                significantly. Of course, I only have four devices in this test environment, in
                a production environment with many many devices the impact of one device change
                is much less. Next, I’ll run the replicators to get everything put back into
                place and then rerun the dispersion report:</para>
    <screen>... start object replicators and monitor logs until they're caught up ...
$ swift-dispersion-report
Queried 2621 containers for dispersion reporting, 17s, 0 retries
100.00% of container copies found (7863 of 7863)
Sample represents 1.00% of the container partition space

Queried 2619 objects for dispersion reporting, 7s, 0 retries
100.00% of object copies found (7857 of 7857)
Sample represents 1.00% of the object partition space</screen>
    <para>You can also run the report for only containers or objects:</para>
    <screen>$ swift-dispersion-report --container-only
Queried 2621 containers for dispersion reporting, 17s, 0 retries
100.00% of container copies found (7863 of 7863)
Sample represents 1.00% of the container partition space

$ swift-dispersion-report --object-only
Queried 2619 objects for dispersion reporting, 7s, 0 retries
100.00% of object copies found (7857 of 7857)
Sample represents 1.00% of the object partition space</screen>
    <para>Alternatively, the dispersion report can also be output in JSON format. This
                allows it to be more easily consumed by third party utilities:</para>
    <screen><?dbsuse-fo font-size="8pt"?>$ swift-dispersion-report -j
{"object": {"retries:": 0, "missing_two": 0, "copies_found": 7863, "missing_one": 0, "copies_expected": 7863, "pct_found": 100.0, "overlapping": 0, "missing_all": 0}, "container": {"retries:": 0, "missing_two": 0, "copies_found": 12534, "missing_one": 0, "copies_expected": 12534, "pct_found": 100.0, "overlapping": 15, "missing_all": 0}}</screen>
    <para>Note that you may select which storage policy to use by setting the option
                ‘–policy-name silver’ or ‘-P silver’ (silver is the example policy name here).
                If no policy is specified, the default will be used per the swift.conf file.
                When you specify a policy the containers created also include the policy index,
                thus even when running a container_only report, you will need to specify the
                policy not using the default.</para>
  </section>
  <section>
    <title>Geographically Distributed Swift Considerations</title>
    <para>Swift provides two features that may be used to distribute replicas of objects
                across multiple geographically distributed data-centers: with
                 object replicas may be dispersed across devices
                from different data-centers by using <literal>regions</literal> in ring device descriptors; with
                 objects may be copied between independent Swift
                clusters in each data-center. The operation and configuration of each are
                described in their respective documentation. The following points should be
                considered when selecting the feature that is most appropriate for a particular
                use case:</para>
    <procedure>
      <step>
        <para>Global Clusters allows the distribution of object replicas across
                        data-centers to be controlled by the cluster operator on per-policy basis,
                        since the distribution is determined by the assignment of devices from
                        each data-center in each policy’s ring file. With Container Sync the end
                        user controls the distribution of objects across clusters on a
                        per-container basis.</para>
      </step>
      <step>
        <para>Global Clusters requires an operator to coordinate ring deployments across
                        multiple data-centers. Container Sync allows for independent management of
                        separate Swift clusters in each data-center, and for existing Swift
                        clusters to be used as peers in Container Sync relationships without
                        deploying new policies/rings.</para>
      </step>
      <step>
        <para>Global Clusters seamlessly supports features that may rely on
                        cross-container operations such as large objects and versioned writes.
                        Container Sync requires the end user to ensure that all required
                        containers are sync’d for these features to work in all data-centers.</para>
      </step>
      <step>
        <para>Global Clusters makes objects available for GET or HEAD requests in both
                        data-centers even if a replica of the object has not yet been
                        asynchronously migrated between data-centers, by forwarding requests
                        between data-centers. Container Sync is unable to serve requests for an
                        object in a particular data-center until the asynchronous sync process has
                        copied the object to that data-center.</para>
      </step>
      <step>
        <para>Global Clusters may require less storage capacity than Container Sync to
                        achieve equivalent durability of objects in each data-center. Global
                        Clusters can restore replicas that are lost or corrupted in one
                        data-center using replicas from other data-centers. Container Sync
                        requires each data-center to independently manage the durability of
                        objects, which may result in each data-center storing more replicas than
                        with Global Clusters.</para>
      </step>
      <step>
        <para>Global Clusters execute all account/container metadata updates
                        synchronously to account/container replicas in all data-centers, which may
                        incur delays when making updates across WANs. Container Sync only copies
                        objects between data-centers and all Swift internal traffic is
                        confined to each data-center.</para>
      </step>
      <step>
        <para>Global Clusters does not yet guarantee the availability of objects stored
                        in Erasure Coded policies when one data-center is offline. With Container
                        Sync the availability of objects in each data-center is independent of the
                        state of other data-centers once objects have been synced. Container Sync
                        also allows objects to be stored using different policy types in different
                        data-centers.</para>
      </step>
    </procedure>
    <section>
      <title>Checking handoff partition distribution</title>
      <para>You can check if handoff partitions are piling up on a server by
                    comparing the expected number of partitions with the actual number on
                    your disks. First get the number of partitions that are currently
                    assigned to a server using the <literal>dispersion</literal> command from
                    <literal>swift-ring-builder</literal>:</para>
      <screen>swift-ring-builder sample.builder dispersion --verbose
Dispersion is 0.000000, Balance is 0.000000, Overload is 0.00%
Required overload is 0.000000%
--------------------------------------------------------------------------
Tier                           Parts      %    Max     0     1     2     3
--------------------------------------------------------------------------
r1                              8192   0.00      2     0     0  8192     0
r1z1                            4096   0.00      1  4096  4096     0     0
r1z1-172.16.10.1                4096   0.00      1  4096  4096     0     0
r1z1-172.16.10.1/sda1           4096   0.00      1  4096  4096     0     0
r1z2                            4096   0.00      1  4096  4096     0     0
r1z2-172.16.10.2                4096   0.00      1  4096  4096     0     0
r1z2-172.16.10.2/sda1           4096   0.00      1  4096  4096     0     0
r1z3                            4096   0.00      1  4096  4096     0     0
r1z3-172.16.10.3                4096   0.00      1  4096  4096     0     0
r1z3-172.16.10.3/sda1           4096   0.00      1  4096  4096     0     0
r1z4                            4096   0.00      1  4096  4096     0     0
r1z4-172.16.20.4                4096   0.00      1  4096  4096     0     0
r1z4-172.16.20.4/sda1           4096   0.00      1  4096  4096     0     0
r2                              8192   0.00      2     0  8192     0     0
r2z1                            4096   0.00      1  4096  4096     0     0
r2z1-172.16.20.1                4096   0.00      1  4096  4096     0     0
r2z1-172.16.20.1/sda1           4096   0.00      1  4096  4096     0     0
r2z2                            4096   0.00      1  4096  4096     0     0
r2z2-172.16.20.2                4096   0.00      1  4096  4096     0     0
r2z2-172.16.20.2/sda1           4096   0.00      1  4096  4096     0     0</screen>
      <para>As you can see from the output, each server should store 4096 partitions, and
                    each region should store 8192 partitions. This example used a partition power
                    of 13 and 3 replicas.</para>
      <para>With write_affinity enabled it is expected to have a higher number of
                    partitions on disk compared to the value reported by the
                    swift-ring-builder dispersion command. The number of additional (handoff)
                    partitions in region r1 depends on your cluster size, the amount
                    of incoming data as well as the replication speed.</para>
      <para>Let’s use the example from above with 6 nodes in 2 regions, and write_affinity
                    configured to write to region r1 first. <literal>swift-ring-builder</literal> reported that
                    each node should store 4096 partitions:</para>
      <screen>Expected partitions for region r2:                                      8192
Handoffs stored across 4 nodes in region r1:                 8192 / 4 = 2048
Maximum number of partitions on each server in region r1: 2048 + 4096 = 6144</screen>
      <para>Worst case is that handoff partitions in region 1 are populated with new
                    object replicas faster than replication is able to move them to region 2.
                    In that case you will see ~ 6144 partitions per
                    server in region r1. Your actual number should be lower and
                    between 4096 and 6144 partitions (preferably on the lower side).</para>
      <para>Now count the number of object partitions on a given server in region 1,
                    for example on 172.16.10.1.  Note that the pathnames might be
                    different; <literal>/srv/node/</literal> is the default mount location, and <literal>objects</literal>
                    applies only to storage policy 0 (storage policy 1 would use
                    <literal>objects-1</literal> and so on):</para>
      <screen>find -L /srv/node/ -maxdepth 3 -type d -wholename "*objects/*" | wc -l</screen>
      <para>If this number is always on the upper end of the expected partition
                    number range (4096 to 6144) or increasing you should check your
                    replication speed and maybe even disable write_affinity.
                    Please refer to the next section how to collect metrics from Swift, and
                    especially <xref linkend="recon-replication"/> how to check replication
                    stats.</para>
    </section>
  </section>
  <section xml:id="cluster-telemetry-and-monitoring">
    <title>Cluster Telemetry and Monitoring</title>
    <para>Various metrics and telemetry can be obtained from the account, container, and
                object servers using the recon server middleware and the swift-recon cli. To do
                so update your account, container, or object servers pipelines to include recon
                and add the associated filter config.</para>
    <para>object-server.conf sample:</para>
    <screen>[pipeline:main]
pipeline = recon object-server

[filter:recon]
use = egg:swift#recon
recon_cache_path = /var/cache/swift</screen>
    <para>container-server.conf sample:</para>
    <screen>[pipeline:main]
pipeline = recon container-server

[filter:recon]
use = egg:swift#recon
recon_cache_path = /var/cache/swift</screen>
    <para>account-server.conf sample:</para>
    <screen>[pipeline:main]
pipeline = recon account-server

[filter:recon]
use = egg:swift#recon
recon_cache_path = /var/cache/swift</screen>
    <para>The recon_cache_path simply sets the directory where stats for a few items will
                be stored. Depending on the method of deployment you may need to create this
                directory manually and ensure that Swift has read/write access.</para>
    <para>Finally, if you also wish to track asynchronous pending on your object
                servers you will need to setup a cronjob to run the swift-recon-cron script
                periodically on your object servers:</para>
    <screen>*/5 * * * * swift /usr/bin/swift-recon-cron /etc/swift/object-server.conf</screen>
    <para>Once the recon middleware is enabled, a GET request for
                “/recon/&lt;metric&gt;” to the backend object server will return a
                JSON-formatted response:</para>
    <screen>fhines@ubuntu:~$ curl -i http://localhost:6030/recon/async
HTTP/1.1 200 OK
Content-Type: application/json
Content-Length: 20
Date: Tue, 18 Oct 2011 21:03:01 GMT

{"async_pending": 0}</screen>
    <para>Note that the default port for the object server is 6200, except on a
                Swift All-In-One installation, which uses 6010, 6020, 6030, and 6040.</para>
    <para>The following metrics and telemetry are currently exposed:</para>
    <informaltable>
      <tgroup cols="2">
        <colspec colname="c1" colwidth="22.1*"/>
        <colspec colname="c2" colwidth="77.9*"/>
        <tbody>
          <row>
            <entry>
              <para>Request URI</para>
            </entry>
            <entry>
              <para>Description</para>
            </entry>
          </row>
          <row>
            <entry>
              <para>/recon/load</para>
            </entry>
            <entry>
              <para>returns 1,5, and 15 minute load average</para>
            </entry>
          </row>
          <row>
            <entry>
              <para>/recon/mem</para>
            </entry>
            <entry>
              <para>returns /proc/meminfo</para>
            </entry>
          </row>
          <row>
            <entry>
              <para>/recon/mounted</para>
            </entry>
            <entry>
              <para>returns <emphasis>ALL</emphasis> currently mounted filesystems</para>
            </entry>
          </row>
          <row>
            <entry>
              <para>/recon/unmounted</para>
            </entry>
            <entry>
              <para>returns all unmounted drives if mount_check = True</para>
            </entry>
          </row>
          <row>
            <entry>
              <para>/recon/diskusage</para>
            </entry>
            <entry>
              <para>returns disk utilization for storage devices</para>
            </entry>
          </row>
          <row>
            <entry>
              <para>/recon/driveaudit</para>
            </entry>
            <entry>
              <para>returns # of drive audit errors</para>
            </entry>
          </row>
          <row>
            <entry>
              <para>/recon/ringmd5</para>
            </entry>
            <entry>
              <para>returns object/container/account ring md5sums</para>
            </entry>
          </row>
          <row>
            <entry>
              <para>/recon/swiftconfmd5</para>
            </entry>
            <entry>
              <para>returns swift.conf md5sum</para>
            </entry>
          </row>
          <row>
            <entry>
              <para>/recon/quarantined</para>
            </entry>
            <entry>
              <para>returns # of quarantined objects/accounts/containers</para>
            </entry>
          </row>
          <row>
            <entry>
              <para>/recon/sockstat</para>
            </entry>
            <entry>
              <para>returns consumable info from /proc/net/sockstat|6</para>
            </entry>
          </row>
          <row>
            <entry>
              <para>/recon/devices</para>
            </entry>
            <entry>
              <para>returns list of devices and devices dir i.e. /srv/node</para>
            </entry>
          </row>
          <row>
            <entry>
              <para>/recon/async</para>
            </entry>
            <entry>
              <para>returns count of async pending</para>
            </entry>
          </row>
          <row>
            <entry>
              <para>/recon/replication</para>
            </entry>
            <entry>
              <para>returns object replication info (for backward compatibility)</para>
            </entry>
          </row>
          <row>
            <entry>
              <para>/recon/replication/&lt;type&gt;</para>
            </entry>
            <entry>
              <para>returns replication info for given type (account, container, object)</para>
            </entry>
          </row>
          <row>
            <entry>
              <para>/recon/auditor/&lt;type&gt;</para>
            </entry>
            <entry>
              <para>returns auditor stats on last reported scan for given type (account, container, object)</para>
            </entry>
          </row>
          <row>
            <entry>
              <para>/recon/updater/&lt;type&gt;</para>
            </entry>
            <entry>
              <para>returns last updater sweep times for given type (container, object)</para>
            </entry>
          </row>
          <row>
            <entry>
              <para>/recon/expirer/object</para>
            </entry>
            <entry>
              <para>returns time elapsed and number of objects deleted during last object expirer sweep</para>
            </entry>
          </row>
          <row>
            <entry>
              <para>/recon/version</para>
            </entry>
            <entry>
              <para>returns Swift version</para>
            </entry>
          </row>
          <row>
            <entry>
              <para>/recon/time</para>
            </entry>
            <entry>
              <para>returns node time</para>
            </entry>
          </row>
        </tbody>
      </tgroup>
    </informaltable>
    <para>Note that ‘object_replication_last’ and ‘object_replication_time’ in object
                replication info are considered to be transitional and will be removed in
                the subsequent releases. Use ‘replication_last’ and ‘replication_time’ instead.</para>
    <para>This information can also be queried via the swift-recon command line utility:</para>
    <screen>fhines@ubuntu:~$ swift-recon -h
Usage:
        usage: swift-recon &lt;server_type&gt; [-v] [--suppress] [-a] [-r] [-u] [-d]
        [-l] [-T] [--md5] [--auditor] [--updater] [--expirer] [--sockstat]

        &lt;server_type&gt;   account|container|object
        Defaults to object server.

        ex: swift-recon container -l --auditor


Options:
  -h, --help            show this help message and exit
  -v, --verbose         Print verbose info
  --suppress            Suppress most connection related errors
  -a, --async           Get async stats
  -r, --replication     Get replication stats
  --auditor             Get auditor stats
  --updater             Get updater stats
  --expirer             Get expirer stats
  -u, --unmounted       Check cluster for unmounted devices
  -d, --diskusage       Get disk usage stats
  -l, --loadstats       Get cluster load average stats
  -q, --quarantined     Get cluster quarantine stats
  --md5                 Get md5sum of servers ring and compare to local copy
  --sockstat            Get cluster socket usage stats
  -T, --time            Check time synchronization
  --all                 Perform all checks. Equal to
                        -arudlqT --md5 --sockstat --auditor --updater
                        --expirer --driveaudit --validate-servers
  -z ZONE, --zone=ZONE  Only query servers in specified zone
  -t SECONDS, --timeout=SECONDS
                        Time to wait for a response from a server
  --swiftdir=SWIFTDIR   Default = /etc/swift</screen>
    <para>For example, to obtain container replication info from all hosts in zone “3”:</para>
    <screen>fhines@ubuntu:~$ swift-recon container -r --zone 3
===============================================================================
--&gt; Starting reconnaissance on 1 hosts
===============================================================================
[2012-04-02 02:45:48] Checking on replication
[failure] low: 0.000, high: 0.000, avg: 0.000, reported: 1
[success] low: 486.000, high: 486.000, avg: 486.000, reported: 1
[replication_time] low: 20.853, high: 20.853, avg: 20.853, reported: 1
[attempted] low: 243.000, high: 243.000, avg: 243.000, reported: 1</screen>
  </section>
  <section>
    <title>Reporting Metrics to StatsD</title>
    <para>If you have a <link xlink:href="http://codeascraft.etsy.com/2011/02/15/measure-anything-measure-everything/">StatsD</link> server running, Swift may be configured to send it
                real-time operational metrics.  To enable this, set the following
                configuration entries (see the sample configuration files):</para>
    <screen>log_statsd_host = localhost
log_statsd_port = 8125
log_statsd_default_sample_rate = 1.0
log_statsd_sample_rate_factor = 1.0
log_statsd_metric_prefix =                [empty-string]</screen>
    <para>If <literal>log_statsd_host</literal> is not set, this feature is disabled.  The default values
                for the other settings are given above.  The <literal>log_statsd_host</literal> can be a
                hostname, an IPv4 address, or an IPv6 address (not surrounded with brackets, as
                this is unnecessary since the port is specified separately).  If a hostname
                resolves to an IPv4 address, an IPv4 socket will be used to send StatsD UDP
                packets, even if the hostname would also resolve to an IPv6 address.</para>
    <para>The sample rate is a real number between 0 and 1 which defines the
                probability of sending a sample for any given event or timing measurement.
                This sample rate is sent with each sample to StatsD and used to
                multiply the value.  For example, with a sample rate of 0.5, StatsD will
                multiply that counter’s value by 2 when flushing the metric to an upstream
                monitoring system (<link xlink:href="http://graphite.wikidot.com/">Graphite</link>, <link xlink:href="http://ganglia.sourceforge.net/">Ganglia</link>, etc.).</para>
    <para>Some relatively high-frequency metrics have a default sample rate less than
                one.  If you want to override the default sample rate for all metrics whose
                default sample rate is not specified in the Swift source, you may set
                <literal>log_statsd_default_sample_rate</literal> to a value less than one.  This is NOT
                recommended (see next paragraph).  A better way to reduce StatsD load is to
                adjust <literal>log_statsd_sample_rate_factor</literal> to a value less than one.  The
                <literal>log_statsd_sample_rate_factor</literal> is multiplied to any sample rate (either the
                global default or one specified by the actual metric logging call in the Swift
                source) prior to handling.  In other words, this one tunable can lower the
                frequency of all StatsD logging by a proportional amount.</para>
    <para>To get the best data, start with the default <literal>log_statsd_default_sample_rate</literal>
                and <literal>log_statsd_sample_rate_factor</literal> values of 1 and only lower
                <literal>log_statsd_sample_rate_factor</literal> if needed.  The
                <literal>log_statsd_default_sample_rate</literal> should not be used and remains for backward
                compatibility only.</para>
    <para>The metric prefix will be prepended to every metric sent to the StatsD server
                For example, with:</para>
    <screen>log_statsd_metric_prefix = proxy01</screen>
    <para>the metric <literal>proxy-server.errors</literal> would be sent to StatsD as
                <literal>proxy01.proxy-server.errors</literal>.  This is useful for differentiating different
                servers when sending statistics to a central StatsD server.  If you run a local
                StatsD server per node, you could configure a per-node metrics prefix there and
                leave <literal>log_statsd_metric_prefix</literal> blank.</para>
    <para>Note that metrics reported to StatsD are counters or timing data (which are
                sent in units of milliseconds).  StatsD usually expands timing data out to min,
                max, avg, count, and 90th percentile per timing metric, but the details of
                this behavior will depend on the configuration of your StatsD server.  Some
                important “gauge” metrics may still need to be collected using another method.
                For example, the <literal>object-server.async_pendings</literal> StatsD metric counts the generation
                of async_pendings in real-time, but will not tell you the current number of
                async_pending container updates on disk at any point in time.</para>
    <para>Note also that the set of metrics collected, their names, and their semantics
                are not locked down and will change over time.</para>
    <para>Metrics for <literal>account-auditor</literal>:</para>
    <informaltable>
      <tgroup cols="2">
        <colspec colname="c1" colwidth="31.3*"/>
        <colspec colname="c2" colwidth="68.7*"/>
        <tbody>
          <row>
            <entry>
              <para>Metric Name</para>
            </entry>
            <entry>
              <para>Description</para>
            </entry>
          </row>
          <row>
            <entry>
              <para>
                <literal>account-auditor.errors</literal>
              </para>
            </entry>
            <entry>
              <para>Count of audit runs (across all account databases) which
                                    caught an Exception.</para>
            </entry>
          </row>
          <row>
            <entry>
              <para>
                <literal>account-auditor.passes</literal>
              </para>
            </entry>
            <entry>
              <para>Count of individual account databases which passed audit.</para>
            </entry>
          </row>
          <row>
            <entry>
              <para>
                <literal>account-auditor.failures</literal>
              </para>
            </entry>
            <entry>
              <para>Count of individual account databases which failed audit.</para>
            </entry>
          </row>
          <row>
            <entry>
              <para>
                <literal>account-auditor.timing</literal>
              </para>
            </entry>
            <entry>
              <para>Timing data for individual account database audits.</para>
            </entry>
          </row>
        </tbody>
      </tgroup>
    </informaltable>
    <para>Metrics for <literal>account-reaper</literal>:</para>
    <informaltable>
      <tgroup cols="2">
        <colspec colname="c1" colwidth="46.9*"/>
        <colspec colname="c2" colwidth="53.1*"/>
        <tbody>
          <row>
            <entry>
              <para>Metric Name</para>
            </entry>
            <entry>
              <para>Description</para>
            </entry>
          </row>
          <row>
            <entry>
              <para>
                <literal>account-reaper.errors</literal>
              </para>
            </entry>
            <entry>
              <para>Count of devices failing the mount check.</para>
            </entry>
          </row>
          <row>
            <entry>
              <para>
                <literal>account-reaper.timing</literal>
              </para>
            </entry>
            <entry>
              <para>Timing data for each reap_account() call.</para>
            </entry>
          </row>
          <row>
            <entry>
              <para>
                <literal>account-reaper.return_codes.X</literal>
              </para>
            </entry>
            <entry>
              <para>Count of HTTP return codes from various operations
                                    (e.g. object listing, container deletion, etc.). The
                                    value for X is the first digit of the return code
                                    (2 for 201, 4 for 404, etc.).</para>
            </entry>
          </row>
          <row>
            <entry>
              <para>
                <literal>account-reaper.containers_failures</literal>
              </para>
            </entry>
            <entry>
              <para>Count of failures to delete a container.</para>
            </entry>
          </row>
          <row>
            <entry>
              <para>
                <literal>account-reaper.containers_deleted</literal>
              </para>
            </entry>
            <entry>
              <para>Count of containers successfully deleted.</para>
            </entry>
          </row>
          <row>
            <entry>
              <para>
                <literal>account-reaper.containers_remaining</literal>
              </para>
            </entry>
            <entry>
              <para>Count of containers which failed to delete with
                                    zero successes.</para>
            </entry>
          </row>
          <row>
            <entry>
              <para>
                <literal>account-reaper.containers_possibly_remaining</literal>
              </para>
            </entry>
            <entry>
              <para>Count of containers which failed to delete with
                                    at least one success.</para>
            </entry>
          </row>
          <row>
            <entry>
              <para>
                <literal>account-reaper.objects_failures</literal>
              </para>
            </entry>
            <entry>
              <para>Count of failures to delete an object.</para>
            </entry>
          </row>
          <row>
            <entry>
              <para>
                <literal>account-reaper.objects_deleted</literal>
              </para>
            </entry>
            <entry>
              <para>Count of objects successfully deleted.</para>
            </entry>
          </row>
          <row>
            <entry>
              <para>
                <literal>account-reaper.objects_remaining</literal>
              </para>
            </entry>
            <entry>
              <para>Count of objects which failed to delete with zero
                                    successes.</para>
            </entry>
          </row>
          <row>
            <entry>
              <para>
                <literal>account-reaper.objects_possibly_remaining</literal>
              </para>
            </entry>
            <entry>
              <para>Count of objects which failed to delete with at
                                    least one success.</para>
            </entry>
          </row>
        </tbody>
      </tgroup>
    </informaltable>
    <para>Metrics for <literal>account-server</literal> (“Not Found” is not considered an error and requests
                which increment <literal>errors</literal> are not included in the timing data):</para>
    <informaltable>
      <tgroup cols="2">
        <colspec colname="c1" colwidth="42.1*"/>
        <colspec colname="c2" colwidth="57.9*"/>
        <tbody>
          <row>
            <entry>
              <para>Metric Name</para>
            </entry>
            <entry>
              <para>Description</para>
            </entry>
          </row>
          <row>
            <entry>
              <para>
                <literal>account-server.DELETE.errors.timing</literal>
              </para>
            </entry>
            <entry>
              <para>Timing data for each DELETE request resulting in an
                                    error: bad request, not mounted, missing timestamp.</para>
            </entry>
          </row>
          <row>
            <entry>
              <para>
                <literal>account-server.DELETE.timing</literal>
              </para>
            </entry>
            <entry>
              <para>Timing data for each DELETE request not resulting in
                                    an error.</para>
            </entry>
          </row>
          <row>
            <entry>
              <para>
                <literal>account-server.PUT.errors.timing</literal>
              </para>
            </entry>
            <entry>
              <para>Timing data for each PUT request resulting in an error:
                                    bad request, not mounted, conflict, recently-deleted.</para>
            </entry>
          </row>
          <row>
            <entry>
              <para>
                <literal>account-server.PUT.timing</literal>
              </para>
            </entry>
            <entry>
              <para>Timing data for each PUT request not resulting in an
                                    error.</para>
            </entry>
          </row>
          <row>
            <entry>
              <para>
                <literal>account-server.HEAD.errors.timing</literal>
              </para>
            </entry>
            <entry>
              <para>Timing data for each HEAD request resulting in an
                                    error: bad request, not mounted.</para>
            </entry>
          </row>
          <row>
            <entry>
              <para>
                <literal>account-server.HEAD.timing</literal>
              </para>
            </entry>
            <entry>
              <para>Timing data for each HEAD request not resulting in
                                    an error.</para>
            </entry>
          </row>
          <row>
            <entry>
              <para>
                <literal>account-server.GET.errors.timing</literal>
              </para>
            </entry>
            <entry>
              <para>Timing data for each GET request resulting in an
                                    error: bad request, not mounted, bad delimiter,
                                    account listing limit too high, bad accept header.</para>
            </entry>
          </row>
          <row>
            <entry>
              <para>
                <literal>account-server.GET.timing</literal>
              </para>
            </entry>
            <entry>
              <para>Timing data for each GET request not resulting in
                                    an error.</para>
            </entry>
          </row>
          <row>
            <entry>
              <para>
                <literal>account-server.REPLICATE.errors.timing</literal>
              </para>
            </entry>
            <entry>
              <para>Timing data for each REPLICATE request resulting in an
                                    error: bad request, not mounted.</para>
            </entry>
          </row>
          <row>
            <entry>
              <para>
                <literal>account-server.REPLICATE.timing</literal>
              </para>
            </entry>
            <entry>
              <para>Timing data for each REPLICATE request not resulting
                                    in an error.</para>
            </entry>
          </row>
          <row>
            <entry>
              <para>
                <literal>account-server.POST.errors.timing</literal>
              </para>
            </entry>
            <entry>
              <para>Timing data for each POST request resulting in an
                                    error: bad request, bad or missing timestamp, not
                                    mounted.</para>
            </entry>
          </row>
          <row>
            <entry>
              <para>
                <literal>account-server.POST.timing</literal>
              </para>
            </entry>
            <entry>
              <para>Timing data for each POST request not resulting in
                                    an error.</para>
            </entry>
          </row>
        </tbody>
      </tgroup>
    </informaltable>
    <para>Metrics for <literal>account-replicator</literal>:</para>
    <informaltable>
      <tgroup cols="2">
        <colspec colname="c1" colwidth="41.6*"/>
        <colspec colname="c2" colwidth="58.4*"/>
        <tbody>
          <row>
            <entry>
              <para>Metric Name</para>
            </entry>
            <entry>
              <para>Description</para>
            </entry>
          </row>
          <row>
            <entry>
              <para>
                <literal>account-replicator.diffs</literal>
              </para>
            </entry>
            <entry>
              <para>Count of syncs handled by sending differing rows.</para>
            </entry>
          </row>
          <row>
            <entry>
              <para>
                <literal>account-replicator.diff_caps</literal>
              </para>
            </entry>
            <entry>
              <para>Count of “diffs” operations which failed because
                                    “max_diffs” was hit.</para>
            </entry>
          </row>
          <row>
            <entry>
              <para>
                <literal>account-replicator.no_changes</literal>
              </para>
            </entry>
            <entry>
              <para>Count of accounts found to be in sync.</para>
            </entry>
          </row>
          <row>
            <entry>
              <para>
                <literal>account-replicator.hashmatches</literal>
              </para>
            </entry>
            <entry>
              <para>Count of accounts found to be in sync via hash
                                    comparison (<literal>broker.merge_syncs</literal> was called).</para>
            </entry>
          </row>
          <row>
            <entry>
              <para>
                <literal>account-replicator.rsyncs</literal>
              </para>
            </entry>
            <entry>
              <para>Count of completely missing accounts which were sent
                                    via rsync.</para>
            </entry>
          </row>
          <row>
            <entry>
              <para>
                <literal>account-replicator.remote_merges</literal>
              </para>
            </entry>
            <entry>
              <para>Count of syncs handled by sending entire database
                                    via rsync.</para>
            </entry>
          </row>
          <row>
            <entry>
              <para>
                <literal>account-replicator.attempts</literal>
              </para>
            </entry>
            <entry>
              <para>Count of database replication attempts.</para>
            </entry>
          </row>
          <row>
            <entry>
              <para>
                <literal>account-replicator.failures</literal>
              </para>
            </entry>
            <entry>
              <para>Count of database replication attempts which failed
                                    due to corruption (quarantined) or inability to read
                                    as well as attempts to individual nodes which
                                    failed.</para>
            </entry>
          </row>
          <row>
            <entry>
              <para>
                <literal>account-replicator.removes.&lt;device&gt;</literal>
              </para>
            </entry>
            <entry>
              <para>Count of databases on &lt;device&gt; deleted because the
                                    delete_timestamp was greater than the put_timestamp
                                    and the database had no rows or because it was
                                    successfully sync’ed to other locations and doesn’t
                                    belong here anymore.</para>
            </entry>
          </row>
          <row>
            <entry>
              <para>
                <literal>account-replicator.successes</literal>
              </para>
            </entry>
            <entry>
              <para>Count of replication attempts to an individual node
                                    which were successful.</para>
            </entry>
          </row>
          <row>
            <entry>
              <para>
                <literal>account-replicator.timing</literal>
              </para>
            </entry>
            <entry>
              <para>Timing data for each database replication attempt
                                    not resulting in a failure.</para>
            </entry>
          </row>
        </tbody>
      </tgroup>
    </informaltable>
    <para>Metrics for <literal>container-auditor</literal>:</para>
    <informaltable>
      <tgroup cols="2">
        <colspec colname="c1" colwidth="35.0*"/>
        <colspec colname="c2" colwidth="65.0*"/>
        <tbody>
          <row>
            <entry>
              <para>Metric Name</para>
            </entry>
            <entry>
              <para>Description</para>
            </entry>
          </row>
          <row>
            <entry>
              <para>
                <literal>container-auditor.errors</literal>
              </para>
            </entry>
            <entry>
              <para>Incremented when an Exception is caught in an audit
                                    pass (only once per pass, max).</para>
            </entry>
          </row>
          <row>
            <entry>
              <para>
                <literal>container-auditor.passes</literal>
              </para>
            </entry>
            <entry>
              <para>Count of individual containers passing an audit.</para>
            </entry>
          </row>
          <row>
            <entry>
              <para>
                <literal>container-auditor.failures</literal>
              </para>
            </entry>
            <entry>
              <para>Count of individual containers failing an audit.</para>
            </entry>
          </row>
          <row>
            <entry>
              <para>
                <literal>container-auditor.timing</literal>
              </para>
            </entry>
            <entry>
              <para>Timing data for each container audit.</para>
            </entry>
          </row>
        </tbody>
      </tgroup>
    </informaltable>
    <para>Metrics for <literal>container-replicator</literal>:</para>
    <informaltable>
      <tgroup cols="2">
        <colspec colname="c1" colwidth="41.9*"/>
        <colspec colname="c2" colwidth="58.1*"/>
        <tbody>
          <row>
            <entry>
              <para>Metric Name</para>
            </entry>
            <entry>
              <para>Description</para>
            </entry>
          </row>
          <row>
            <entry>
              <para>
                <literal>container-replicator.diffs</literal>
              </para>
            </entry>
            <entry>
              <para>Count of syncs handled by sending differing rows.</para>
            </entry>
          </row>
          <row>
            <entry>
              <para>
                <literal>container-replicator.diff_caps</literal>
              </para>
            </entry>
            <entry>
              <para>Count of “diffs” operations which failed because
                                    “max_diffs” was hit.</para>
            </entry>
          </row>
          <row>
            <entry>
              <para>
                <literal>container-replicator.no_changes</literal>
              </para>
            </entry>
            <entry>
              <para>Count of containers found to be in sync.</para>
            </entry>
          </row>
          <row>
            <entry>
              <para>
                <literal>container-replicator.hashmatches</literal>
              </para>
            </entry>
            <entry>
              <para>Count of containers found to be in sync via hash
                                    comparison (<literal>broker.merge_syncs</literal> was called).</para>
            </entry>
          </row>
          <row>
            <entry>
              <para>
                <literal>container-replicator.rsyncs</literal>
              </para>
            </entry>
            <entry>
              <para>Count of completely missing containers where were sent
                                    via rsync.</para>
            </entry>
          </row>
          <row>
            <entry>
              <para>
                <literal>container-replicator.remote_merges</literal>
              </para>
            </entry>
            <entry>
              <para>Count of syncs handled by sending entire database
                                    via rsync.</para>
            </entry>
          </row>
          <row>
            <entry>
              <para>
                <literal>container-replicator.attempts</literal>
              </para>
            </entry>
            <entry>
              <para>Count of database replication attempts.</para>
            </entry>
          </row>
          <row>
            <entry>
              <para>
                <literal>container-replicator.failures</literal>
              </para>
            </entry>
            <entry>
              <para>Count of database replication attempts which failed
                                    due to corruption (quarantined) or inability to read
                                    as well as attempts to individual nodes which
                                    failed.</para>
            </entry>
          </row>
          <row>
            <entry>
              <para>
                <literal>container-replicator.removes.&lt;device&gt;</literal>
              </para>
            </entry>
            <entry>
              <para>Count of databases deleted on &lt;device&gt; because the
                                    delete_timestamp was greater than the put_timestamp
                                    and the database had no rows or because it was
                                    successfully sync’ed to other locations and doesn’t
                                    belong here anymore.</para>
            </entry>
          </row>
          <row>
            <entry>
              <para>
                <literal>container-replicator.successes</literal>
              </para>
            </entry>
            <entry>
              <para>Count of replication attempts to an individual node
                                    which were successful.</para>
            </entry>
          </row>
          <row>
            <entry>
              <para>
                <literal>container-replicator.timing</literal>
              </para>
            </entry>
            <entry>
              <para>Timing data for each database replication attempt
                                    not resulting in a failure.</para>
            </entry>
          </row>
        </tbody>
      </tgroup>
    </informaltable>
    <para>Metrics for <literal>container-server</literal> (“Not Found” is not considered an error and requests
                which increment <literal>errors</literal> are not included in the timing data):</para>
    <informaltable>
      <tgroup cols="2">
        <colspec colname="c1" colwidth="44.7*"/>
        <colspec colname="c2" colwidth="55.3*"/>
        <tbody>
          <row>
            <entry>
              <para>Metric Name</para>
            </entry>
            <entry>
              <para>Description</para>
            </entry>
          </row>
          <row>
            <entry>
              <para>
                <literal>container-server.DELETE.errors.timing</literal>
              </para>
            </entry>
            <entry>
              <para>Timing data for DELETE request errors: bad request,
                                    not mounted, missing timestamp, conflict.</para>
            </entry>
          </row>
          <row>
            <entry>
              <para>
                <literal>container-server.DELETE.timing</literal>
              </para>
            </entry>
            <entry>
              <para>Timing data for each DELETE request not resulting in
                                    an error.</para>
            </entry>
          </row>
          <row>
            <entry>
              <para>
                <literal>container-server.PUT.errors.timing</literal>
              </para>
            </entry>
            <entry>
              <para>Timing data for PUT request errors: bad request,
                                    missing timestamp, not mounted, conflict.</para>
            </entry>
          </row>
          <row>
            <entry>
              <para>
                <literal>container-server.PUT.timing</literal>
              </para>
            </entry>
            <entry>
              <para>Timing data for each PUT request not resulting in an
                                    error.</para>
            </entry>
          </row>
          <row>
            <entry>
              <para>
                <literal>container-server.HEAD.errors.timing</literal>
              </para>
            </entry>
            <entry>
              <para>Timing data for HEAD request errors: bad request,
                                    not mounted.</para>
            </entry>
          </row>
          <row>
            <entry>
              <para>
                <literal>container-server.HEAD.timing</literal>
              </para>
            </entry>
            <entry>
              <para>Timing data for each HEAD request not resulting in
                                    an error.</para>
            </entry>
          </row>
          <row>
            <entry>
              <para>
                <literal>container-server.GET.errors.timing</literal>
              </para>
            </entry>
            <entry>
              <para>Timing data for GET request errors: bad request,
                                    not mounted, parameters not utf8, bad accept header.</para>
            </entry>
          </row>
          <row>
            <entry>
              <para>
                <literal>container-server.GET.timing</literal>
              </para>
            </entry>
            <entry>
              <para>Timing data for each GET request not resulting in
                                    an error.</para>
            </entry>
          </row>
          <row>
            <entry>
              <para>
                <literal>container-server.REPLICATE.errors.timing</literal>
              </para>
            </entry>
            <entry>
              <para>Timing data for REPLICATE request errors: bad
                                    request, not mounted.</para>
            </entry>
          </row>
          <row>
            <entry>
              <para>
                <literal>container-server.REPLICATE.timing</literal>
              </para>
            </entry>
            <entry>
              <para>Timing data for each REPLICATE request not resulting
                                    in an error.</para>
            </entry>
          </row>
          <row>
            <entry>
              <para>
                <literal>container-server.POST.errors.timing</literal>
              </para>
            </entry>
            <entry>
              <para>Timing data for POST request errors: bad request,
                                    bad x-container-sync-to, not mounted.</para>
            </entry>
          </row>
          <row>
            <entry>
              <para>
                <literal>container-server.POST.timing</literal>
              </para>
            </entry>
            <entry>
              <para>Timing data for each POST request not resulting in
                                    an error.</para>
            </entry>
          </row>
        </tbody>
      </tgroup>
    </informaltable>
    <para>Metrics for <literal>container-sync</literal>:</para>
    <informaltable>
      <tgroup cols="2">
        <colspec colname="c1" colwidth="37.3*"/>
        <colspec colname="c2" colwidth="62.7*"/>
        <tbody>
          <row>
            <entry>
              <para>Metric Name</para>
            </entry>
            <entry>
              <para>Description</para>
            </entry>
          </row>
          <row>
            <entry>
              <para>
                <literal>container-sync.skips</literal>
              </para>
            </entry>
            <entry>
              <para>Count of containers skipped because they don’t have
                                    sync’ing enabled.</para>
            </entry>
          </row>
          <row>
            <entry>
              <para>
                <literal>container-sync.failures</literal>
              </para>
            </entry>
            <entry>
              <para>Count of failures sync’ing of individual containers.</para>
            </entry>
          </row>
          <row>
            <entry>
              <para>
                <literal>container-sync.syncs</literal>
              </para>
            </entry>
            <entry>
              <para>Count of individual containers sync’ed successfully.</para>
            </entry>
          </row>
          <row>
            <entry>
              <para>
                <literal>container-sync.deletes</literal>
              </para>
            </entry>
            <entry>
              <para>Count of container database rows sync’ed by
                                    deletion.</para>
            </entry>
          </row>
          <row>
            <entry>
              <para>
                <literal>container-sync.deletes.timing</literal>
              </para>
            </entry>
            <entry>
              <para>Timing data for each container database row
                                    synchronization via deletion.</para>
            </entry>
          </row>
          <row>
            <entry>
              <para>
                <literal>container-sync.puts</literal>
              </para>
            </entry>
            <entry>
              <para>Count of container database rows sync’ed by Putting.</para>
            </entry>
          </row>
          <row>
            <entry>
              <para>
                <literal>container-sync.puts.timing</literal>
              </para>
            </entry>
            <entry>
              <para>Timing data for each container database row
                                    synchronization via Putting.</para>
            </entry>
          </row>
        </tbody>
      </tgroup>
    </informaltable>
    <para>Metrics for <literal>container-updater</literal>:</para>
    <informaltable>
      <tgroup cols="2">
        <colspec colname="c1" colwidth="36.6*"/>
        <colspec colname="c2" colwidth="63.4*"/>
        <tbody>
          <row>
            <entry>
              <para>Metric Name</para>
            </entry>
            <entry>
              <para>Description</para>
            </entry>
          </row>
          <row>
            <entry>
              <para>
                <literal>container-updater.successes</literal>
              </para>
            </entry>
            <entry>
              <para>Count of containers which successfully updated their
                                    account.</para>
            </entry>
          </row>
          <row>
            <entry>
              <para>
                <literal>container-updater.failures</literal>
              </para>
            </entry>
            <entry>
              <para>Count of containers which failed to update their
                                    account.</para>
            </entry>
          </row>
          <row>
            <entry>
              <para>
                <literal>container-updater.no_changes</literal>
              </para>
            </entry>
            <entry>
              <para>Count of containers which didn’t need to update
                                    their account.</para>
            </entry>
          </row>
          <row>
            <entry>
              <para>
                <literal>container-updater.timing</literal>
              </para>
            </entry>
            <entry>
              <para>Timing data for processing a container; only
                                    includes timing for containers which needed to
                                    update their accounts (i.e. “successes” and
                                    “failures” but not “no_changes”).</para>
            </entry>
          </row>
        </tbody>
      </tgroup>
    </informaltable>
    <para>Metrics for <literal>object-auditor</literal>:</para>
    <informaltable>
      <tgroup cols="2">
        <colspec colname="c1" colwidth="35.0*"/>
        <colspec colname="c2" colwidth="65.0*"/>
        <tbody>
          <row>
            <entry>
              <para>Metric Name</para>
            </entry>
            <entry>
              <para>Description</para>
            </entry>
          </row>
          <row>
            <entry>
              <para>
                <literal>object-auditor.quarantines</literal>
              </para>
            </entry>
            <entry>
              <para>Count of objects failing audit and quarantined.</para>
            </entry>
          </row>
          <row>
            <entry>
              <para>
                <literal>object-auditor.errors</literal>
              </para>
            </entry>
            <entry>
              <para>Count of errors encountered while auditing objects.</para>
            </entry>
          </row>
          <row>
            <entry>
              <para>
                <literal>object-auditor.timing</literal>
              </para>
            </entry>
            <entry>
              <para>Timing data for each object audit (does not include
                                    any rate-limiting sleep time for
                                    max_files_per_second, but does include rate-limiting
                                    sleep time for max_bytes_per_second).</para>
            </entry>
          </row>
        </tbody>
      </tgroup>
    </informaltable>
    <para>Metrics for <literal>object-expirer</literal>:</para>
    <informaltable>
      <tgroup cols="2">
        <colspec colname="c1" colwidth="31.6*"/>
        <colspec colname="c2" colwidth="68.4*"/>
        <tbody>
          <row>
            <entry>
              <para>Metric Name</para>
            </entry>
            <entry>
              <para>Description</para>
            </entry>
          </row>
          <row>
            <entry>
              <para>
                <literal>object-expirer.objects</literal>
              </para>
            </entry>
            <entry>
              <para>Count of objects expired.</para>
            </entry>
          </row>
          <row>
            <entry>
              <para>
                <literal>object-expirer.errors</literal>
              </para>
            </entry>
            <entry>
              <para>Count of errors encountered while attempting to
                                    expire an object.</para>
            </entry>
          </row>
          <row>
            <entry>
              <para>
                <literal>object-expirer.timing</literal>
              </para>
            </entry>
            <entry>
              <para>Timing data for each object expiration attempt,
                                    including ones resulting in an error.</para>
            </entry>
          </row>
        </tbody>
      </tgroup>
    </informaltable>
    <para>Metrics for <literal>object-reconstructor</literal>:</para>
    <informaltable>
      <tgroup cols="2">
        <colspec colname="c1" colwidth="50.0*"/>
        <colspec colname="c2" colwidth="50.0*"/>
        <tbody>
          <row>
            <entry>
              <para>Metric Name</para>
            </entry>
            <entry>
              <para>Description</para>
            </entry>
          </row>
          <row>
            <entry>
              <para>
                <literal>object-reconstructor.partition.delete.count.&lt;device&gt;</literal>
              </para>
            </entry>
            <entry>
              <para>A count of partitions on &lt;device&gt; which were
                                    reconstructed and synced to another node because they
                                    didn’t belong on this node. This metric is tracked
                                    per-device to allow for “quiescence detection” for
                                    object reconstruction activity on each device.</para>
            </entry>
          </row>
          <row>
            <entry>
              <para>
                <literal>object-reconstructor.partition.delete.timing</literal>
              </para>
            </entry>
            <entry>
              <para>Timing data for partitions reconstructed and synced to
                                    another node because they didn’t belong on this node.
                                    This metric is not tracked per device.</para>
            </entry>
          </row>
          <row>
            <entry>
              <para>
                <literal>object-reconstructor.partition.update.count.&lt;device&gt;</literal>
              </para>
            </entry>
            <entry>
              <para>A count of partitions on &lt;device&gt; which were
                                    reconstructed and synced to another node, but also
                                    belong on this node. As with delete.count, this metric
                                    is tracked per-device.</para>
            </entry>
          </row>
          <row>
            <entry>
              <para>
                <literal>object-reconstructor.partition.update.timing</literal>
              </para>
            </entry>
            <entry>
              <para>Timing data for partitions reconstructed which also
                                    belong on this node. This metric is not tracked
                                    per-device.</para>
            </entry>
          </row>
          <row>
            <entry>
              <para>
                <literal>object-reconstructor.suffix.hashes</literal>
              </para>
            </entry>
            <entry>
              <para>Count of suffix directories whose hash (of filenames)
                                    was recalculated.</para>
            </entry>
          </row>
          <row>
            <entry>
              <para>
                <literal>object-reconstructor.suffix.syncs</literal>
              </para>
            </entry>
            <entry>
              <para>Count of suffix directories reconstructed with ssync.</para>
            </entry>
          </row>
        </tbody>
      </tgroup>
    </informaltable>
    <para>Metrics for <literal>object-replicator</literal>:</para>
    <informaltable>
      <tgroup cols="2">
        <colspec colname="c1" colwidth="49.0*"/>
        <colspec colname="c2" colwidth="51.0*"/>
        <tbody>
          <row>
            <entry>
              <para>Metric Name</para>
            </entry>
            <entry>
              <para>Description</para>
            </entry>
          </row>
          <row>
            <entry>
              <para>
                <literal>object-replicator.partition.delete.count.&lt;device&gt;</literal>
              </para>
            </entry>
            <entry>
              <para>A count of partitions on &lt;device&gt; which were
                                    replicated to another node because they didn’t
                                    belong on this node.  This metric is tracked
                                    per-device to allow for “quiescence detection” for
                                    object replication activity on each device.</para>
            </entry>
          </row>
          <row>
            <entry>
              <para>
                <literal>object-replicator.partition.delete.timing</literal>
              </para>
            </entry>
            <entry>
              <para>Timing data for partitions replicated to another
                                    node because they didn’t belong on this node.  This
                                    metric is not tracked per device.</para>
            </entry>
          </row>
          <row>
            <entry>
              <para>
                <literal>object-replicator.partition.update.count.&lt;device&gt;</literal>
              </para>
            </entry>
            <entry>
              <para>A count of partitions on &lt;device&gt; which were
                                    replicated to another node, but also belong on this
                                    node.  As with delete.count, this metric is tracked
                                    per-device.</para>
            </entry>
          </row>
          <row>
            <entry>
              <para>
                <literal>object-replicator.partition.update.timing</literal>
              </para>
            </entry>
            <entry>
              <para>Timing data for partitions replicated which also
                                    belong on this node.  This metric is not tracked
                                    per-device.</para>
            </entry>
          </row>
          <row>
            <entry>
              <para>
                <literal>object-replicator.suffix.hashes</literal>
              </para>
            </entry>
            <entry>
              <para>Count of suffix directories whose hash (of filenames)
                                    was recalculated.</para>
            </entry>
          </row>
          <row>
            <entry>
              <para>
                <literal>object-replicator.suffix.syncs</literal>
              </para>
            </entry>
            <entry>
              <para>Count of suffix directories replicated with rsync.</para>
            </entry>
          </row>
        </tbody>
      </tgroup>
    </informaltable>
    <para>Metrics for <literal>object-server</literal>:</para>
    <informaltable>
      <tgroup cols="2">
        <colspec colname="c1" colwidth="42.9*"/>
        <colspec colname="c2" colwidth="57.1*"/>
        <tbody>
          <row>
            <entry>
              <para>Metric Name</para>
            </entry>
            <entry>
              <para>Description</para>
            </entry>
          </row>
          <row>
            <entry>
              <para>
                <literal>object-server.quarantines</literal>
              </para>
            </entry>
            <entry>
              <para>Count of objects (files) found bad and moved to
                                    quarantine.</para>
            </entry>
          </row>
          <row>
            <entry>
              <para>
                <literal>object-server.async_pendings</literal>
              </para>
            </entry>
            <entry>
              <para>Count of container updates saved as async_pendings
                                    (may result from PUT or DELETE requests).</para>
            </entry>
          </row>
          <row>
            <entry>
              <para>
                <literal>object-server.POST.errors.timing</literal>
              </para>
            </entry>
            <entry>
              <para>Timing data for POST request errors: bad request,
                                    missing timestamp, delete-at in past, not mounted.</para>
            </entry>
          </row>
          <row>
            <entry>
              <para>
                <literal>object-server.POST.timing</literal>
              </para>
            </entry>
            <entry>
              <para>Timing data for each POST request not resulting in
                                    an error.</para>
            </entry>
          </row>
          <row>
            <entry>
              <para>
                <literal>object-server.PUT.errors.timing</literal>
              </para>
            </entry>
            <entry>
              <para>Timing data for PUT request errors: bad request,
                                    not mounted, missing timestamp, object creation
                                    constraint violation, delete-at in past.</para>
            </entry>
          </row>
          <row>
            <entry>
              <para>
                <literal>object-server.PUT.timeouts</literal>
              </para>
            </entry>
            <entry>
              <para>Count of object PUTs which exceeded max_upload_time.</para>
            </entry>
          </row>
          <row>
            <entry>
              <para>
                <literal>object-server.PUT.timing</literal>
              </para>
            </entry>
            <entry>
              <para>Timing data for each PUT request not resulting in an
                                    error.</para>
            </entry>
          </row>
          <row>
            <entry>
              <para>
                <literal>object-server.PUT.&lt;device&gt;.timing</literal>
              </para>
            </entry>
            <entry>
              <para>Timing data per kB transferred (ms/kB) for each
                                    non-zero-byte PUT request on each device.
                                    Monitoring problematic devices, higher is bad.</para>
            </entry>
          </row>
          <row>
            <entry>
              <para>
                <literal>object-server.GET.errors.timing</literal>
              </para>
            </entry>
            <entry>
              <para>Timing data for GET request errors: bad request,
                                    not mounted, header timestamps before the epoch,
                                    precondition failed.
                                    File errors resulting in a quarantine are not
                                    counted here.</para>
            </entry>
          </row>
          <row>
            <entry>
              <para>
                <literal>object-server.GET.timing</literal>
              </para>
            </entry>
            <entry>
              <para>Timing data for each GET request not resulting in an
                                    error.  Includes requests which couldn’t find the
                                    object (including disk errors resulting in file
                                    quarantine).</para>
            </entry>
          </row>
          <row>
            <entry>
              <para>
                <literal>object-server.HEAD.errors.timing</literal>
              </para>
            </entry>
            <entry>
              <para>Timing data for HEAD request errors: bad request,
                                    not mounted.</para>
            </entry>
          </row>
          <row>
            <entry>
              <para>
                <literal>object-server.HEAD.timing</literal>
              </para>
            </entry>
            <entry>
              <para>Timing data for each HEAD request not resulting in
                                    an error.  Includes requests which couldn’t find the
                                    object (including disk errors resulting in file
                                    quarantine).</para>
            </entry>
          </row>
          <row>
            <entry>
              <para>
                <literal>object-server.DELETE.errors.timing</literal>
              </para>
            </entry>
            <entry>
              <para>Timing data for DELETE request errors: bad request,
                                    missing timestamp, not mounted, precondition
                                    failed.  Includes requests which couldn’t find or
                                    match the object.</para>
            </entry>
          </row>
          <row>
            <entry>
              <para>
                <literal>object-server.DELETE.timing</literal>
              </para>
            </entry>
            <entry>
              <para>Timing data for each DELETE request not resulting
                                    in an error.</para>
            </entry>
          </row>
          <row>
            <entry>
              <para>
                <literal>object-server.REPLICATE.errors.timing</literal>
              </para>
            </entry>
            <entry>
              <para>Timing data for REPLICATE request errors: bad
                                    request, not mounted.</para>
            </entry>
          </row>
          <row>
            <entry>
              <para>
                <literal>object-server.REPLICATE.timing</literal>
              </para>
            </entry>
            <entry>
              <para>Timing data for each REPLICATE request not resulting
                                    in an error.</para>
            </entry>
          </row>
        </tbody>
      </tgroup>
    </informaltable>
    <para>Metrics for <literal>object-updater</literal>:</para>
    <informaltable>
      <tgroup cols="2">
        <colspec colname="c1" colwidth="35.0*"/>
        <colspec colname="c2" colwidth="65.0*"/>
        <tbody>
          <row>
            <entry>
              <para>Metric Name</para>
            </entry>
            <entry>
              <para>Description</para>
            </entry>
          </row>
          <row>
            <entry>
              <para>
                <literal>object-updater.errors</literal>
              </para>
            </entry>
            <entry>
              <para>Count of drives not mounted or async_pending files
                                    with an unexpected name.</para>
            </entry>
          </row>
          <row>
            <entry>
              <para>
                <literal>object-updater.timing</literal>
              </para>
            </entry>
            <entry>
              <para>Timing data for object sweeps to flush async_pending
                                    container updates.  Does not include object sweeps
                                    which did not find an existing async_pending storage
                                    directory.</para>
            </entry>
          </row>
          <row>
            <entry>
              <para>
                <literal>object-updater.quarantines</literal>
              </para>
            </entry>
            <entry>
              <para>Count of async_pending container updates which were
                                    corrupted and moved to quarantine.</para>
            </entry>
          </row>
          <row>
            <entry>
              <para>
                <literal>object-updater.successes</literal>
              </para>
            </entry>
            <entry>
              <para>Count of successful container updates.</para>
            </entry>
          </row>
          <row>
            <entry>
              <para>
                <literal>object-updater.failures</literal>
              </para>
            </entry>
            <entry>
              <para>Count of failed container updates.</para>
            </entry>
          </row>
          <row>
            <entry>
              <para>
                <literal>object-updater.unlinks</literal>
              </para>
            </entry>
            <entry>
              <para>Count of async_pending files unlinked. An
                                    async_pending file is unlinked either when it is
                                    successfully processed or when the replicator sees
                                    that there is a newer async_pending file for the
                                    same object.</para>
            </entry>
          </row>
        </tbody>
      </tgroup>
    </informaltable>
    <para>Metrics for <literal>proxy-server</literal> (in the table, <literal>&lt;type&gt;</literal> is the proxy-server
                controller responsible for the request and will be one of “account”,
                “container”, or “object”):</para>
    <informaltable>
      <tgroup cols="2">
        <colspec colname="c1" colwidth="43.0*"/>
        <colspec colname="c2" colwidth="57.0*"/>
        <tbody>
          <row>
            <entry>
              <para>Metric Name</para>
            </entry>
            <entry>
              <para>Description</para>
            </entry>
          </row>
          <row>
            <entry>
              <para>
                <literal>proxy-server.errors</literal>
              </para>
            </entry>
            <entry>
              <para>Count of errors encountered while serving requests
                                    before the controller type is determined.  Includes
                                    invalid Content-Length, errors finding the internal
                                    controller to handle the request, invalid utf8, and
                                    bad URLs.</para>
            </entry>
          </row>
          <row>
            <entry>
              <para>
                <literal>proxy-server.&lt;type&gt;.handoff_count</literal>
              </para>
            </entry>
            <entry>
              <para>Count of node hand-offs; only tracked if log_handoffs
                                    is set in the proxy-server config.</para>
            </entry>
          </row>
          <row>
            <entry>
              <para>
                <literal>proxy-server.&lt;type&gt;.handoff_all_count</literal>
              </para>
            </entry>
            <entry>
              <para>Count of times <emphasis>only</emphasis> hand-off locations were
                                    utilized; only tracked if log_handoffs is set in the
                                    proxy-server config.</para>
            </entry>
          </row>
          <row>
            <entry>
              <para>
                <literal>proxy-server.&lt;type&gt;.client_timeouts</literal>
              </para>
            </entry>
            <entry>
              <para>Count of client timeouts (client did not read within
                                    <literal>client_timeout</literal> seconds during a GET or did not
                                    supply data within <literal>client_timeout</literal> seconds during
                                    a PUT).</para>
            </entry>
          </row>
          <row>
            <entry>
              <para>
                <literal>proxy-server.&lt;type&gt;.client_disconnects</literal>
              </para>
            </entry>
            <entry>
              <para>Count of detected client disconnects during PUT
                                    operations (does NOT include caught Exceptions in
                                    the proxy-server which caused a client disconnect).</para>
            </entry>
          </row>
        </tbody>
      </tgroup>
    </informaltable>
    <para>Metrics for <literal>proxy-logging</literal> middleware (in the table, <literal>&lt;type&gt;</literal> is either the
                proxy-server controller responsible for the request: “account”, “container”,
                “object”, or the string “SOS” if the request came from the <link xlink:href="https://github.com/dpgoetz/sos">Swift Origin Server</link>
                middleware.  The <literal>&lt;verb&gt;</literal> portion will be one of “GET”, “HEAD”, “POST”, “PUT”,
                “DELETE”, “COPY”, “OPTIONS”, or “BAD_METHOD”.  The list of valid HTTP methods
                is configurable via the <literal>log_statsd_valid_http_methods</literal> config variable and
                the default setting yields the above behavior):</para>
    <informaltable>
      <tgroup cols="2">
        <colspec colname="c1" colwidth="54.2*"/>
        <colspec colname="c2" colwidth="45.8*"/>
        <tbody>
          <row>
            <entry>
              <para>Metric Name</para>
            </entry>
            <entry>
              <para>Description</para>
            </entry>
          </row>
          <row>
            <entry>
              <para>
                <literal>proxy-server.&lt;type&gt;.&lt;verb&gt;.&lt;status&gt;.timing</literal>
              </para>
            </entry>
            <entry>
              <para>Timing data for requests, start to finish.
                                    The &lt;status&gt; portion is the numeric HTTP
                                    status code for the request (e.g.  “200” or
                                    “404”).</para>
            </entry>
          </row>
          <row>
            <entry>
              <para>
                <literal>proxy-server.&lt;type&gt;.GET.&lt;status&gt;.first-byte.timing</literal>
              </para>
            </entry>
            <entry>
              <para>Timing data up to completion of sending the
                                    response headers (only for GET requests).
                                    &lt;status&gt; and &lt;type&gt; are as for the main
                                    timing metric.</para>
            </entry>
          </row>
          <row>
            <entry>
              <para>
                <literal>proxy-server.&lt;type&gt;.&lt;verb&gt;.&lt;status&gt;.xfer</literal>
              </para>
            </entry>
            <entry>
              <para>This counter metric is the sum of bytes
                                    transferred in (from clients) and out (to
                                    clients) for requests.  The &lt;type&gt;, &lt;verb&gt;,
                                    and &lt;status&gt; portions of the metric are just
                                    like the main timing metric.</para>
            </entry>
          </row>
        </tbody>
      </tgroup>
    </informaltable>
    <para>The <literal>proxy-logging</literal> middleware also groups these metrics by policy.  The
                <literal>&lt;policy-index&gt;</literal> portion represents a policy index):</para>
    <informaltable>
      <tgroup cols="2">
        <colspec colname="c1" colwidth="66.7*"/>
        <colspec colname="c2" colwidth="33.3*"/>
        <tbody>
          <row>
            <entry>
              <para>Metric Name</para>
            </entry>
            <entry>
              <para>Description</para>
            </entry>
          </row>
          <row>
            <entry>
              <para>
                <literal>proxy-server.object.policy.&lt;policy-index&gt;.&lt;verb&gt;.&lt;status&gt;.timing</literal>
              </para>
            </entry>
            <entry>
              <para>Timing data for requests, aggregated
                                    by policy index.</para>
            </entry>
          </row>
          <row>
            <entry>
              <para>
                <literal>proxy-server.object.policy.&lt;policy-index&gt;.GET.&lt;status&gt;.first-byte.timing</literal>
              </para>
            </entry>
            <entry>
              <para>Timing data up to completion of
                                    sending the response headers,
                                    aggregated by policy index.</para>
            </entry>
          </row>
          <row>
            <entry>
              <para>
                <literal>proxy-server.object.policy.&lt;policy-index&gt;.&lt;verb&gt;.&lt;status&gt;.xfer</literal>
              </para>
            </entry>
            <entry>
              <para>Sum of bytes transferred in and out,
                                    aggregated by policy index.</para>
            </entry>
          </row>
        </tbody>
      </tgroup>
    </informaltable>
    <para>Metrics for <literal>tempauth</literal> middleware (in the table, <literal>&lt;reseller_prefix&gt;</literal> represents
                the actual configured reseller_prefix or “<literal>NONE</literal>” if the reseller_prefix is the
                empty string):</para>
    <informaltable>
      <tgroup cols="2">
        <colspec colname="c1" colwidth="44.1*"/>
        <colspec colname="c2" colwidth="55.9*"/>
        <tbody>
          <row>
            <entry>
              <para>Metric Name</para>
            </entry>
            <entry>
              <para>Description</para>
            </entry>
          </row>
          <row>
            <entry>
              <para>
                <literal>tempauth.&lt;reseller_prefix&gt;.unauthorized</literal>
              </para>
            </entry>
            <entry>
              <para>Count of regular requests which were denied with
                                    HTTPUnauthorized.</para>
            </entry>
          </row>
          <row>
            <entry>
              <para>
                <literal>tempauth.&lt;reseller_prefix&gt;.forbidden</literal>
              </para>
            </entry>
            <entry>
              <para>Count of regular requests which were denied with
                                    HTTPForbidden.</para>
            </entry>
          </row>
          <row>
            <entry>
              <para>
                <literal>tempauth.&lt;reseller_prefix&gt;.token_denied</literal>
              </para>
            </entry>
            <entry>
              <para>Count of token requests which were denied.</para>
            </entry>
          </row>
          <row>
            <entry>
              <para>
                <literal>tempauth.&lt;reseller_prefix&gt;.errors</literal>
              </para>
            </entry>
            <entry>
              <para>Count of errors.</para>
            </entry>
          </row>
        </tbody>
      </tgroup>
    </informaltable>
  </section>
  <section>
    <title>Debugging Tips and Tools</title>
    <para>When a request is made to Swift, it is given a unique transaction id.  This
                id should be in every log line that has to do with that request.  This can
                be useful when looking at all the services that are hit by a single request.</para>
    <para>If you need to know where a specific account, container or object is in the
                cluster, <literal>swift-get-nodes</literal> will show the location where each replica should be.</para>
    <para>If you are looking at an object on the server and need more info,
                <literal>swift-object-info</literal> will display the account, container, replica locations
                and metadata of the object.</para>
    <para>If you are looking at a container on the server and need more info,
                <literal>swift-container-info</literal> will display all the information like the account,
                container, replica locations and metadata of the container.</para>
    <para>If you are looking at an account on the server and need more info,
                <literal>swift-account-info</literal> will display the account, replica locations
                and metadata of the account.</para>
    <para>If you want to audit the data for an account, <literal>swift-account-audit</literal> can be
                used to crawl the account, checking that all containers and objects can be
                found.</para>
  </section>
  <section>
    <title>Managing Services</title>
    <para>Swift services are generally managed with <literal>swift-init</literal>. the general usage is
                <literal>swift-init &lt;service&gt; &lt;command&gt;</literal>, where service is the Swift service to
                manage (for example object, container, account, proxy) and command is one of:</para>
    <informaltable>
      <tgroup cols="2">
        <colspec colname="c1" colwidth="17.5*"/>
        <colspec colname="c2" colwidth="82.5*"/>
        <tbody>
          <row>
            <entry>
              <para>Command</para>
            </entry>
            <entry>
              <para>Description</para>
            </entry>
          </row>
          <row>
            <entry>
              <para>start</para>
            </entry>
            <entry>
              <para>Start the service</para>
            </entry>
          </row>
          <row>
            <entry>
              <para>stop</para>
            </entry>
            <entry>
              <para>Stop the service</para>
            </entry>
          </row>
          <row>
            <entry>
              <para>restart</para>
            </entry>
            <entry>
              <para>Restart the service</para>
            </entry>
          </row>
          <row>
            <entry>
              <para>shutdown</para>
            </entry>
            <entry>
              <para>Attempt to gracefully shutdown the service</para>
            </entry>
          </row>
          <row>
            <entry>
              <para>reload</para>
            </entry>
            <entry>
              <para>Attempt to gracefully restart the service</para>
            </entry>
          </row>
        </tbody>
      </tgroup>
    </informaltable>
    <para>A graceful shutdown or reload will finish any current requests before
                completely stopping the old service.  There is also a special case of
                <literal>swift-init all &lt;command&gt;</literal>, which will run the command for all swift
                services.</para>
    <para>In cases where there are multiple configs for a service, a specific config
                can be managed with <literal>swift-init &lt;service&gt;.&lt;config&gt; &lt;command&gt;</literal>.
                For example, when a separate replication network is used, there might be
                <literal>/etc/swift/object-server/public.conf</literal> for the object server and
                <literal>/etc/swift/object-server/replication.conf</literal> for the replication services.
                In this case, the replication services could be restarted with
                <literal>swift-init object-server.replication restart</literal>.</para>
  </section>
  <section xml:id="object-auditor">
    <title>Object Auditor</title>
    <para>On system failures, the XFS file system can sometimes truncate files it’s
                trying to write and produce zero-byte files. The object-auditor will catch
                these problems but in the case of a system crash it would be advisable to run
                an extra, less rate limited sweep to check for these specific files. You can
                run this command as follows:</para>
    <screen>swift-object-auditor /path/to/object-server/config/file.conf once -z 1000</screen>
    <para><literal>-z</literal> means to only check for zero-byte files at 1000 files per second.</para>
    <para>At times it is useful to be able to run the object auditor on a specific
                device or set of devices.  You can run the object-auditor as follows:</para>
    <screen>swift-object-auditor /path/to/object-server/config/file.conf once --devices=sda,sdb</screen>
    <para>This will run the object auditor on only the sda and sdb devices. This param
                accepts a comma separated list of values.</para>
  </section>
  <section xml:id="object-replicator">
    <title>Object Replicator</title>
    <para>At times it is useful to be able to run the object replicator on a specific
                device or partition.  You can run the object-replicator as follows:</para>
    <screen><?dbsuse-fo font-size="8pt"?>swift-object-replicator /path/to/object-server/config/file.conf once --devices=sda,sdb</screen>
    <para>This will run the object replicator on only the sda and sdb devices.  You can
                likewise run that command with <literal>--partitions</literal>.  Both params accept a comma
                separated list of values. If both are specified they will be ANDed together.
                These can only be run in “once” mode.</para>
  </section>
  <section>
    <title>Swift Orphans</title>
    <para>Swift Orphans are processes left over after a reload of a Swift server.</para>
    <para>For example, when upgrading a proxy server you would probably finish
                with a <literal>swift-init proxy-server reload</literal> or <literal>/etc/init.d/swift-proxy
reload</literal>. This kills the parent proxy server process and leaves the
                child processes running to finish processing whatever requests they
                might be handling at the time. It then starts up a new parent proxy
                server process and its children to handle new incoming requests. This
                allows zero-downtime upgrades with no impact to existing requests.</para>
    <para>The orphaned child processes may take a while to exit, depending on
                the length of the requests they were handling. However, sometimes an
                old process can be hung up due to some bug or hardware issue. In these
                cases, these orphaned processes will hang around
                forever. <literal>swift-orphans</literal> can be used to find and kill these orphans.</para>
    <para><literal>swift-orphans</literal> with no arguments will just list the orphans it finds
                that were started more than 24 hours ago. You shouldn’t really check
                for orphans until 24 hours after you perform a reload, as some
                requests can take a long time to process. <literal>swift-orphans -k TERM</literal> will
                send the SIG_TERM signal to the orphans processes, or you can <literal>kill
-TERM</literal> the pids yourself if you prefer.</para>
    <para>You can run <literal>swift-orphans --help</literal> for more options.</para>
  </section>
  <section>
    <title>Swift Oldies</title>
    <para>Swift Oldies are processes that have just been around for a long
                time. There’s nothing necessarily wrong with this, but it might
                indicate a hung process if you regularly upgrade and reload/restart
                services. You might have so many servers that you don’t notice when a
                reload/restart fails; <literal>swift-oldies</literal> can help with this.</para>
    <para>For example, if you upgraded and reloaded/restarted everything 2 days
                ago, and you’ve already cleaned up any orphans with <literal>swift-orphans</literal>,
                you can run <literal>swift-oldies -a 48</literal> to find any Swift processes still
                around that were started more than 2 days ago and then investigate
                them accordingly.</para>
  </section>
  <section>
    <title>Custom Log Handlers</title>
    <para>Swift supports setting up custom log handlers for services by specifying a
                comma-separated list of functions to invoke when logging is setup. It does so
                via the <literal>log_custom_handlers</literal> configuration option. Logger hooks invoked are
                passed the same arguments as Swift’s get_logger function (as well as the
                getLogger and LogAdapter object):</para>
    <informaltable>
      <tgroup cols="2">
        <colspec colname="c1" colwidth="21.9*"/>
        <colspec colname="c2" colwidth="78.1*"/>
        <tbody>
          <row>
            <entry>
              <para>Name</para>
            </entry>
            <entry>
              <para>Description</para>
            </entry>
          </row>
          <row>
            <entry>
              <para>conf</para>
            </entry>
            <entry>
              <para>Configuration dict to read settings from</para>
            </entry>
          </row>
          <row>
            <entry>
              <para>name</para>
            </entry>
            <entry>
              <para>Name of the logger received</para>
            </entry>
          </row>
          <row>
            <entry>
              <para>log_to_console</para>
            </entry>
            <entry>
              <para>(optional) Write log messages to console on stderr</para>
            </entry>
          </row>
          <row>
            <entry>
              <para>log_route</para>
            </entry>
            <entry>
              <para>Route for the logging received</para>
            </entry>
          </row>
          <row>
            <entry>
              <para>fmt</para>
            </entry>
            <entry>
              <para>Override log format received</para>
            </entry>
          </row>
          <row>
            <entry>
              <para>logger</para>
            </entry>
            <entry>
              <para>The logging.getLogger object</para>
            </entry>
          </row>
          <row>
            <entry>
              <para>adapted_logger</para>
            </entry>
            <entry>
              <para>The LogAdapter object</para>
            </entry>
          </row>
        </tbody>
      </tgroup>
    </informaltable>
    <para>A basic example that sets up a custom logger might look like the
                following:</para>
    <screen language="python">def my_logger(conf, name, log_to_console, log_route, fmt, logger,
              adapted_logger):
    my_conf_opt = conf.get('some_custom_setting')
    my_handler = third_party_logstore_handler(my_conf_opt)
    logger.addHandler(my_handler)</screen>
    <para>See <xref linkend="custom-logger-hooks-label"/> for sample use cases.</para>
  </section>
  <section>
    <title>Securing OpenStack Swift</title>
    <para>Please refer to the security guide at <link xlink:href="http://docs.openstack.org/security-guide"/>
                and in particular the <link xlink:href="http://docs.openstack.org/security-guide/object-storage.html">Object Storage</link> section.</para>
  </section>
</section>
