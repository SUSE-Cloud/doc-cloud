<?xml version="1.0" encoding="UTF-8" standalone="no"?>
<section xmlns="http://docbook.org/ns/docbook" xmlns:xlink="http://www.w3.org/1999/xlink" xmlns:xi="http://www.w3.org/2001/XInclude" version="5.1">
  <title>Expiring Object Support</title>
  <para>The <literal>swift-object-expirer</literal> offers scheduled deletion of objects. The Swift
            client would use the <literal>X-Delete-At</literal> or <literal>X-Delete-After</literal> headers during an
            object <literal>PUT</literal> or <literal>POST</literal> and the cluster would automatically quit serving
            that object at the specified time and would shortly thereafter remove the
            object from the system.</para>
  <para>The <literal>X-Delete-At</literal> header takes a Unix Epoch timestamp, in integer form; for
            example: <literal>1317070737</literal> represents <literal>Mon Sep 26 20:58:57 2011 UTC</literal>.</para>
  <para>The <literal>X-Delete-After</literal> header takes an integer number of seconds. The proxy
            server that receives the request will convert this header into an
            <literal>X-Delete-At</literal> header using its current time plus the value given.</para>
  <para>As expiring objects are added to the system, the object servers will record the
            expirations in a hidden <literal>.expiring_objects</literal> account for the
            <literal>swift-object-expirer</literal> to handle later.</para>
  <para>Usually, just one instance of the <literal>swift-object-expirer</literal> daemon needs to run
            for a cluster. This isn’t exactly automatic failover high availability, but if
            this daemon doesn’t run for a few hours it should not be any real issue. The
            expired-but-not-yet-deleted objects will still <literal>404 Not Found</literal> if someone
            tries to <literal>GET</literal> or <literal>HEAD</literal> them and they’ll just be deleted a bit later when
            the daemon is restarted.</para>
  <para>By default, the <literal>swift-object-expirer</literal> daemon will run with a concurrency of
            1.  Increase this value to get more concurrency.  A concurrency of 1 may not be
            enough to delete expiring objects in a timely fashion for a particular Swift
            cluster.</para>
  <para>It is possible to run multiple daemons to do different parts of the work if a
            single process with a concurrency of more than 1 is not enough (see the sample
            config file for details).</para>
  <para>To run the <literal>swift-object-expirer</literal> as multiple processes, set <literal>processes</literal> to
            the number of processes (either in the config file or on the command line).
            Then run one process for each part.  Use <literal>process</literal> to specify the part of the
            work to be done by a process using the command line or the config.  So, for
            example, if you’d like to run three processes, set <literal>processes</literal> to 3 and run
            three processes with <literal>process</literal> set to 0, 1, and 2 for the three processes.
            If multiple processes are used, it’s necessary to run one for each part of the
            work or that part of the work will not be done.</para>
  <para>The daemon uses the <literal>/etc/swift/object-expirer.conf</literal> by default, and here is
            a quick sample conf file:</para>
  <screen>[DEFAULT]
# swift_dir = /etc/swift
# user = swift
# You can specify default log routing here if you want:
# log_name = swift
# log_facility = LOG_LOCAL0
# log_level = INFO

[object-expirer]
interval = 300

[pipeline:main]
pipeline = catch_errors cache proxy-server

[app:proxy-server]
use = egg:swift#proxy
# See proxy-server.conf-sample for options

[filter:cache]
use = egg:swift#memcache
# See proxy-server.conf-sample for options

[filter:catch_errors]
use = egg:swift#catch_errors
# See proxy-server.conf-sample for options</screen>
  <para>The daemon needs to run on a machine with access to all the backend servers in
            the cluster, but does not need proxy server or public access. The daemon will
            use its own internal proxy code instance to access the backend servers.</para>
</section>
