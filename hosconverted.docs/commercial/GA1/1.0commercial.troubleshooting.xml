<?xml version="1.0" encoding="UTF-8"?>
<!--Edit status: not edited-->
<!DOCTYPE topic
  PUBLIC "-//OASIS//DTD DITA Topic//EN" "http://docs.oasis-open.org/dita/v1.1/OS/dtd/topic.dtd" >
<topic xml:lang="en-us" id="topic23276">
  <title>HPE Helion <tm tmtype="reg">OpenStack</tm> 1.0: Troubleshooting</title>
  <prolog>
    <metadata>
      <othermeta name="layout" content="default"/>
    </metadata>
  </prolog>
  <body>
    <p>
      <!--PUBLISHED-->
      <!--./commercial/GA1/1.0commercial.troubleshooting.md-->
      <!--permalink: /helion/openstack/services/troubleshooting/--></p>
    <p> </p>
    <p>HPE Helion OpenStack is an OpenStack technology coupled with a version of <tm tmtype="reg"
        >Linux</tm> provided by HP. This topic describes all the known issues that you might
      encounter. To help you resolve these issues, we have provided possible solutions.</p>
    <p>For easy reference, we categorized the known issues and solutions as follows:</p>
    <ul>
      <li>
        <xref type="section" href="#topic23276/baremetal-install">Baremetal installation</xref>
        <ul>
          <li>
            <xref type="section" href="#topic23276/kvm">KVM</xref>
          </li>
          <li>
            <xref type="section" href="#topic23276/esx-ovsvapp">ESX and OVSvAPP</xref>
          </li>
        </ul>
      </li>
      <li>
        <xref type="section" href="#topic23276/vsa">VSA</xref></li>
      <li>
        <xref type="section" href="#topic23276/logging">Logging</xref>
      </li>
    </ul>
    <p>If you need further assistance, contact <xref href="http://www.hpcloud.com/about/contact"
          format="html">HPE Customer Support</xref>.</p>
    <section id="baremetal-install">
      <title>Baremetal installation</title>
    </section>
    <section id="kvm">
      <title>KVM</title>
      <ol>
        <li>
          <xref type="section" href="#topic23276/fatal-pci">Fatal PCI Express Device Error</xref>
        </li>
        <li>
          <xref type="section" href="#topic23276/IPMI-fails">IPMI fails with error- unable to
            establish IPMI v2 / RMCP+ session</xref>
        </li>
        <li>
          <xref type="section" href="#topic23276/failure-update-overcloud">Failure of Update
            overcloud</xref>
        </li>
        <li>
          <xref type="section" href="#topic23276/installation-failure">Installation failure as the
            flavor to be used for overcloud nodes does not match</xref>
        </li>
        <li>
          <xref type="section" href="#topic23276/PXE-boot-on-target">PXE boot on target node keeps
            switching between interfaces</xref>
        </li>
        <li><xref href="#topic23276/bios-blocks-are-not-set-to-correct-date" format="dita"/></li>
        <li>
          <xref type="section" href="#topic23276/ilo-console">iLO console shows hLinux daemon.err
            tgtd while PXE booting</xref>
        </li>
        <li>
          <xref type="section" href="#topic23276/ilo-show-null">iLO console shows null waiting for
            notice of completion while PXE booting</xref>
        </li>
        <li>
          <xref type="section" href="#topic23276/failure-installer">Failure of
            Hp_ced_installer.sh</xref>
        </li>
        <li>
          <xref type="section" href="#topic23276/seed-install-failure">Failure of Seed
            Installation</xref>
        </li>
        <li>
          <xref type="section" href="#topic23276/ironic">Ironic intermitently set maintenance mode
            to True during update</xref>
        </li>
      </ol>
    </section>
    <section id="fatal-pci">
      <title>Fatal PCI Express Device Error</title>
      <p>
        <b>System Behavior/Message</b>
      </p>
      <p>When installing on HPE ProLiant SL390s and HPE ProLiant BL490d systems, the following error
        has occasionally occurred:</p>
      <codeblock>
  <codeph>`Fatal PCI Express Device Error PCI Slot ?
 B00/D00/F00`
</codeph>
</codeblock>
      <p>
        <b>Resolution</b>
      </p>
      <p>If you get this error, reset the system that experienced the error:</p>
      <ol>
        <li>Connect to the iLO using Internet Explorer: <codeph>https://&lt;iLO IP
            address&gt;</codeph>
        </li>
        <li>Navigate to Information / Diagnostics.</li>
        <li>Reset iLO.</li>
        <li>Log back into the iLO after 30 seconds.</li>
        <li>Navigate to Remote Console / Remote Console.</li>
        <li>Open the integrated remote console (.NET).</li>
        <li>Click Power switch / Press and Hold.</li>
        <li>
          <p>Click Power switch / Momentary Press, and wait for the system to restart.</p>
          <p>The system should now boot normally.</p>
        </li>
      </ol>
      <ul>
        <li>
          <p>If the overcloud controller is rebooted (due to a power issue, hardware upgrade, or
            similar event), OpenStack compute tools such as <codeph>nova-list</codeph> might report
            that the VMs are in an ERROR state, rendering the overcloud unusable. To restore the
            overcloud to an operational state, follow the steps below:</p>
          <ol>
            <li>
              <p>As user <codeph>root</codeph> on the overcloud controller you must:</p>
              <p>A. Run the <codeph>os-refresh-config</codeph> scripts:</p>
              <codeblock>
<codeph># os-refresh-config
</codeph>
</codeblock>
              <p>B. Restart the <codeph>mysql</codeph> service:</p>
              <codeblock>
<codeph># service mysql restart
</codeph>
</codeblock>
              <p>C. Re-run the <codeph>os-refresh-config</codeph> scripts:</p>
              <codeblock>
<codeph># os-refresh-config
</codeph>
</codeblock>
              <p>D. Restart all Networking Operations (Neutron) services:</p>
              <codeblock>
<codeph># service neutron-dhcp-agent restart
# service neutron-l3-agent restart

# service neutron-metadata-agent restart
# service neutron-openvswitch-agent restart
# service neutron-server restart
</codeph>
</codeblock>
            </li>
            <li>
              <p>On each overcloud node, restart the Neutron and Nova services:</p>
              <codeblock>
<codeph>$ sudo service neutron-openvswitch-agent restart
$ sudo service nova-compute restart
$ sudo service nova-scheduler restart
$ sudo service nova-conductor restart
</codeph>
</codeblock>
            </li>
          </ol>
        </li>
        <li>
          <p>The installer uses IPMI commands to reset nodes and change their power status. Some
            systems change to a state in which the <codeph>Server Power</codeph> status as reported
            by the iLO is stuck in <codeph>RESET</codeph>. If this occurs, you must physically
            disconnect the power from the server for 10 seconds. If the problem persists after that,
            contact HPE Support as there might be a defective component in the system.</p>
        </li>
        <li>
          <p>On the system on which the installer is run, the seed VM's networking is bridged onto
            the external LAN. If you remove HPE Helion OpenStack the network bridge persists. To
            revert the network configuration to its pre-installation state, run the following
            commands as user root:</p>
          <codeblock>
<codeph># ip addr add 192.168.185.131/16 dev eth0 scope global
# ip addr del 192.168.185.131/16 dev brbm
# ovs-vsctl del-port NIC
</codeph>
</codeblock>
          <p>where</p>
          <ul>
            <li>eth0 is the external interface </li>
            <li>192.168.185.131 is the IP address on the external interface - you should replace
              this with your own IP address.</li>
            <li>The baremetal bridge is always called 'brbm'</li>
          </ul>
        </li>
        <li>
          <p>Before you install the HPE Helion OpenStack DNSaaS or if you want to use Heat with HP
            Helion OpenStack you <b>must</b> modify the /etc/heat/heat.conf file on the overcloud
            controller as follows.</p>
          <p>
            <b>Important</b>: The installation of the HPE Helion OpenStack DNSaaS fails if you do not
            make these modifications.</p>
          <ol>
            <li>
              <p>Make sure the IP address in the following settings reflects the IP address of the
                overcloud controller, for example:</p>
              <codeblock>
<codeph>heat_metadata_server_url = http://192.0.202.2:8000
heat_waitcondition_server_url = http://192.0.202.2:8000/v1/waitcondition
heat_watch_server_url = http://192.0.202.2:8003
</codeph>
</codeblock>
              <p>
                <b>Note</b>: You must have admin ssh access to the overcloud controller.</p>
            </li>
            <li>
              <p>Save the file.</p>
            </li>
            <li>
              <p>Restart the Heat-related services - heat-api, heat-api-cfn, heat-api-cloudwatch,
                and heat-engine.</p>
            </li>
            <li>
              <p>Ensure there are no Heat resources in an error state, and then delete any stale or
                corrupted Heat-related stacks.
                <!--A BR tag was used here in the original source.--></p>
            </li>
          </ol>
        </li>
      </ul>
      <!-- ===================== horizontal rule ===================== -->
    </section>
    <section id="IPMI-fails">
      <title>IPMI fails with an error- unable to establish IPMI v2 / RMCP+ session</title>
      <p>
        <b>System Behavior/Message</b>
      </p>
      <p>When installing on HPE ProLiant BL490c systems, the following error has occasionally
        occurred:</p>
      <codeblock>
  <codeph>unable to establish IPMI v2 / RMCP+ session
</codeph>
</codeblock>
      <p>
        <b>Resolution</b>
      </p>
      <p>If you get this error, perform the following steps:</p>
      <ol>
        <li>Ensure that the iLO user has administrator privileges, which is required by the
          IPMITOOL.</li>
        <li>To check from the iLO remote console, reboot the server and press <b>F8</b> to get to
          ILO Management screen.</li>
        <li>Click <b>User</b> in the menu-bar and select <b>Edit</b>. Edit User pop-up box displays
          .</li>
        <li>If you are using a BL server in the QA C7000 enclosure, select the <b>cdl</b> user to
          edit.</li>
        <li>Use ↓(down arrow key) to select <b>Administer User Accounts</b>. </li>
        <li>Use the space bar to set the value to <b>YES</b>.</li>
        <li>Select <b>F10</b> to save.</li>
        <li>Click <b>File</b> and select <b>Exit</b> to close.
          <!--A BR tag was used here in the original source.--></li>
      </ol>
      <!-- ===================== horizontal rule ===================== -->
    </section>
    <section id="failure-update-overcloud">
      <title>Failure of Update overcloud</title>
      <p>
        <b>System Behavior/Message</b>
      </p>
      <p>Update overcloud fails with the following error:</p>
      <p>
        <codeph>Inconsistency between heat description ($OVERCLOUD_NODES) and overcloud
          configuration ($OVERCLOUD_INSTANCES)</codeph>
      </p>
      <p>
        <b>Resolution</b>
      </p>
      <p>If you get this error, perform the below steps:</p>
      <ol>
        <li>
          <p>Log in to Seed.</p>
          <codeblock>
<codeph># ssh root@&lt;Seed IP address&gt;
</codeph>
</codeblock>
          <ol>
            <li>Edit <codeph>/root/tripleo/ce_env.json</codeph>and update the right variable for
              build_number and installed_build_number. <!-- (CORE-1697) --></li>
          </ol>
        </li>
      </ol>
      <p>The ce_env_json will be displayed as the sample below.</p>
      <codeblock>
  <codeph>      "host-ip": "192.168.122.1", 
       "hp": { 
         "build_number": 11, 
         "installed_build_number": 11 
</codeph>
</codeblock>
      <p>Note that the build_number is changed from null to the right variable.</p>
      <p>3.Run the installer script to update the overcloud.</p>
      <codeblock>
  <codeph>    # bash -x tripleo/tripleo-incubator/scripts/hp_ced_installer.sh --update-overcloud |&amp; tee update_cloud.log
</codeph>
</codeblock>
      <p>During the installation, the number of build_number and installed_build_number that you
        specified are installed. </p>
      <!-- ===================== horizontal rule ===================== -->
    </section>
    <section id="installation-failure">
      <title>Installation failure as the flavor to be used for overcloud nodes does not
        match</title>
      <p>
        <b>System Behavior/Message</b>
      </p>
      <p>If you have a set of Baremetal servers which differ in specifications (e.g. memory and
        disk), the installation will fail as the flavor to be used for overcloud nodes does not
        match with the server that has the lowest specification for memory, disk, and CPU.</p>
      <p>
        <b>Probable Cause</b>
      </p>
      <p>The 2nd row in <codeph>baremetal.csv</codeph> which corresponds to the overcloud Controller
        node is used to create a flavor for the overcloud nodes.</p>
      <p>
        <b>Resolution</b>
      </p>
      <p>Edit the <b>baremetal.csv</b> file to define the lowest specification server in the second
        row. <!--A BR tag was used here in the original source.--></p>
      <!-- ===================== horizontal rule ===================== -->
    </section>
    <section id="PXE-boot-on-target">
      <title>PXE boot on target node keeps switching between interfaces</title>
      <p>
        <b>System Behavior/Message</b>
      </p>
      <p>When node boots up on iLO console it shows node waiting for PXE boot on multiple NICs.</p>
      <p>
        <b>Probable Cause</b>
      </p>
      <p>Multiple NICs are enabled for Network Boot.</p>
      <p>
        <b>Resolution</b>
      </p>
      <ul>
        <li>Reboot the node, using <b>F9</b> to get to the BIOS configuration.</li>
        <li>Assuming NIC1(eth0/em1) for the node is connected to a private network shared across
          node enable it for Network Boot.</li>
        <li>Select System Options &gt; Embedded NICs.</li>
        <li>Set NIC 1 Boot Options = Network Boot.</li>
        <li>Set NIC 2 Boot Options = Disabled.
          <!--A BR tag was used here in the original source.--></li>
      </ul>
      <!-- ===================== horizontal rule ===================== -->
    </section>
    <section id="bios-blocks-are-not-set-to-correct-date">
      <title>BIOS blocks are not set to correct date and time across all nodes</title>
      <p>
        <b>System Behavior/Message</b>
      </p>
      <p>Nodes PXE boot but ISCSI does not start.</p>
      <p>
        <b>Probable Cause</b>
      </p>
      <p>Time and date across nodes are incorrect.</p>
      <p>
        <b>Resolution</b>
      </p>
      <p>Reboot the node, using <b>F9</b> to get to the BIOS configuration. BIOS date and time are
        set correctly and the same on all the systems.</p>
      <ul>
        <li>Select Date and Time.</li>
        <li>Set the Date.</li>
        <li>Set the Time.</li>
        <li>Use the &lt;ENTER&gt; key to accept the new date and time.</li>
        <li>Save the BIOS, which reboots the node again.</li>
        <li>Once the node has rebooted, you can confirm its data and time from the iLO Overview.
          <!--A BR tag was used here in the original source.--></li>
      </ul>
      <!-- ===================== horizontal rule ===================== -->
    </section>
    <section id="ilo-console">
      <title>iLO console shows hLinux daemon.err tgtd while PXE booting</title>
      <p>
        <b>System Behavior/Message</b>
      </p>
      <p>PXE boot gets stuck after <codeph>daemon.err tgtd</codeph>
      </p>
      <p>
        <b>Probable Cause</b>
      </p>
      <p>Node does not have enough disk space</p>
      <p>
        <b>Resolution</b>
      </p>
      <ul>
        <li>Check if target node has disk space mentioned in <codeph>baremetal.csv</codeph> and is
          greater than Node_min_disk mentioned in
            <codeph>tripleo/tripleo-incubator/scripts/hp_ced_functions.sh</codeph>.</li>
        <li>If disk space is less than Node_min_disk, change Node_min_disk along with DISK_SIZE in
            <codeph>tripleo/tripleo-incubator/scripts/hp_ced_list_nodes.sh</codeph> on Seed.</li>
        <li>Re-run the installation script. </li>
      </ul>
      <!-- ===================== horizontal rule ===================== -->
    </section>
    <section id="ilo-show-null">
      <title>iLO console shows null waiting for notice of completion while PXE booting</title>
      <p>
        <b>System Behavior/Message</b>
      </p>
      <p>Node is powered on and PXE booted but it is powered off after <codeph>daemon.err</codeph>
        and stack create fails.</p>
      <p>
        <b>Probable Cause</b>
      </p>
      <p>Node does not have enough disk space. SAN boot is enabled for node or local disk is not
        attached to <codeph>/sda</codeph>
      </p>
      <p>
        <b>Resolution</b>
      </p>
      <p>Installer expects that SAN boot option is disabled for nodes. Verify whether SAN boot is
        disabled for BL 490c.</p>
      <p>Also, you can boot the targeted BL490c with Ubuntu or any Linux ISO to see what device is
        shown as the local disk. For the installer it should be <codeph>/sda</codeph>.</p>
      <p>
        <!--A BR tag was used here in the original source.-->
      </p>
      <!-- ===================== horizontal rule ===================== -->
    </section>
    <section id="failure-installer">
      <title>Failure of Hp_ced_installer.sh</title>
      <p>
        <b>System Behavior/Message</b>
      </p>
      <p>
        <codeph>Hp_ced_installer.sh</codeph> fails because of <codeph>baremetal.csv
        /sda</codeph>.</p>
      <p>
        <b>Resolution</b>
      </p>
      <p>Verify <codeph>baremetal.csv</codeph> for empty lines or special characters.
        <!--A BR tag was used here in the original source.--></p>
      <!-- ===================== horizontal rule ===================== -->
    </section>
    <section id="seed-install-failure">
      <title>Failure of Seed Installation</title>
      <p>
        <b>System Behavior/Message</b>
      </p>
      <p>Seed installation fails with no space left on device.</p>
      <p>
        <b>Resolution</b>
      </p>
      <p>Verify the tripleo directory- user owner and group. It must be <b>root:root</b>. Incase it
        is not set as <b>root:root</b> then edit it to root using- <codeph>chown root:root
          tripleo</codeph>
      </p>
      <p>
        <!--A BR tag was used here in the original source.-->
        <!--A BR tag was used here in the original source.-->
      </p>
      <!-- ===================== horizontal rule ===================== -->
      <p>
        <b>System Behavior/Message</b>
      </p>
      <p>Inconsistent Rabbitmq failure seen on controller nodes while listing queues</p>
      <codeblock>
  <codeph>rabbitmqctl list_queues
</codeph>
</codeblock>
      <p>
        <b>Resolution</b>
      </p>
      <p>Restart the Rabbitmq service.</p>
      <p>
        <!--A BR tag was used here in the original source.-->
        <!--A BR tag was used here in the original source.-->
      </p>
      <!-- ===================== horizontal rule ===================== -->
    </section>
    <section id="ironic">
      <title>Ironic intermitently set maintenance mode to True during installation</title>
      <p>This issue can happen during the update of undercloud or overcloud nodes. The update will
        fail for one or more nodes. <!-- CORE-2082 --></p>
      <p>
        <b>System Behavior/Message:</b>
      </p>
      <p>If the update fails, from undercloud node:</p>
      <ol>
        <li>
          <p>Source the stackrc file:</p>
          <codeblock>
<codeph>source stackrc 
</codeph>
</codeblock>
        </li>
        <li>
          <p>Execute the <codeph>nova list</codeph> command to determine which Compute node(s) is in
            an error state. The node will have a status of ERROR.</p>
          <codeblock>
<codeph>nova list
</codeph>
</codeblock>
        </li>
        <li>
          <p>Execute the <codeph>heat stack-list</codeph> command to determine which Heat stack is
            in an error state. The stack will have a status of <codeph>CREATE_FAILED</codeph>.</p>
          <codeblock>
<codeph>heat stack-list
</codeph>
</codeblock>
        </li>
        <li>
          <p>Execute the <codeph>ironic node-list</codeph> command to determine which node(s) is in
            maintenance mode. The stack will have a maintenance of <codeph>TRUE</codeph>.</p>
          <codeblock>
<codeph>ironic node-list
</codeph>
</codeblock>
        </li>
        <li>
          <p>Execute the <codeph>ironic node-show</codeph> command for the node that is node(s) is
            in maintenance mode. The stack will have a maintenance of <codeph>TRUE</codeph>.</p>
          <codeblock>
<codeph>ironic node-show &lt;UUID&gt;
</codeph>
</codeblock>
          <p>In the output, check the <codeph>last_error</codeph> field for an error similar to the
            following:</p>
          <codeblock>
<codeph>During sync_power_state, max retries exceeded for node 81baacd5-657e-476f-b7ef, node state None does not match expected state

'None'. Updating DB state to 'None' Switching node to maintenance mode. 
</codeph>
</codeblock>
        </li>
      </ol>
      <p>
        <b>Solution</b>
      </p>
      <ol>
        <li>
          <p>Remove the node in maintenance mode using the following command:</p>
          <codeblock>
<codeph>nova node-delete &lt;ID of error node&gt;
</codeph>
</codeblock>
        </li>
        <li>
          <p>List the stacks using the following command:</p>
          <codeblock>
<codeph>heat stack-list
</codeph>
</codeblock>
        </li>
        <li>
          <p>Delete the stack with the failed Nova node.</p>
          <codeblock>
<codeph>heat stack-delete &lt;ID of failed node&gt;
</codeph>
</codeblock>
        </li>
        <li>
          <p>Change the node(s) to false for the maintenance option, using the following
            command:</p>
          <codeblock>
<codeph>`ironic node-update &lt;id&gt; replace maintenance=False`
</codeph>
</codeblock>
        </li>
      </ol>
    </section>
    <section id="esx-ovsvapp">
      <title>ESX and OVSvAPP</title>
      <ol>
        <li>
          <xref type="section" href="#topic23276/nova-compute">nova-manage service list does not
            list the compute service as running</xref>
        </li>
        <li>
          <xref type="section" href="#topic23276/unable-login-vcenter">Unable to login to vCenter
            proxy agent</xref>
        </li>
        <li>
          <xref type="section" href="#topic23276/unable-cinder-backup">Unable to backup volumes
            using Cinder backup</xref>
        </li>
        <li>
          <xref type="section" href="#topic23276/fails-ovsvapp">Failure of OVSvAPP deployment</xref>
        </li>
      </ol>
    </section>
    <section id="nova-compute">
      <title>nova-manage service list does not list the compute service as running</title>
      <p>
        <b>System Behavior/Message</b>
      </p>
      <p>There can be multiple reason why nova-compute service is not listed or has a :) as
        status.</p>
      <p>
        <b>Resolution</b>
      </p>
      <p>To resolve the above issue verify the following:</p>
      <ol>
        <li>The ESX Management Network is able to reach the Helion Management Network.</li>
        <li>nova-compute service is running (os-svc-restart -n nova-compute).</li>
        <li>Verify <codeph>/etc/nova/nova-compute.conf</codeph> has the right entries. </li>
      </ol>
      <!-- ===================== horizontal rule ===================== -->
    </section>
    <section id="unable-login-vcenter">
      <title>Unable to login to vCenter proxy agent</title>
      <p>
        <b>System Behavior/Message</b>
      </p>
      <p>Unable to login to vCenter proxy agent through the console.</p>
      <p>
        <b>Resolution</b>
      </p>
      <p>Users can login to the system using the user <codeph>heat-admin</codeph> and the authorized
        key in the Seed VM. <!--A BR tag was used here in the original source.--></p>
      <!-- ===================== horizontal rule ===================== -->
    </section>
    <section id="unable-cinder-backup">
      <title>Unable to backup volumes using Cinder backup</title>
      <p>
        <b>System Behavior/Message</b>
      </p>
      <p>Unable to backup volumes using Cinder backup.</p>
      <p>
        <b>Resolution</b>
      </p>
      <p>Cinder-backup is not supported. <!--A BR tag was used here in the original source.--></p>
      <!-- ===================== horizontal rule ===================== -->
    </section>
    <section id="fails-ovsvapp">
      <title>Failure of OVSvAPP deployment</title>
      <p>
        <b>System Behavior/Message</b>
      </p>
      <p>Failure of OVSvAPP deployment.</p>
      <p>
        <b>Resolution</b>
      </p>
      <p>Verify <codeph>tripleo/hp-ovsvapp/log/ovs_vapp.log</codeph> in the installer directory.</p>
      <p>
        <!--A BR tag was used here in the original source.-->
        <!--A BR tag was used here in the original source.-->
      </p>
      <!-- ===================== horizontal rule ===================== -->
      <p>
        <b>System Behavior/Message</b>
      </p>
      <p>After reboot of Controller that has the VIP assigned, the hpvcn agent, nova-compute
        service, nova compute service in the proxy node and HCN agent in OVSvAPP needs to be
        restarted manually to resume normal operations.</p>
      <p>
        <b>Resolution</b>
      </p>
      <ul>
        <li>
          <p>To restart nova-compute, execute the following command in compute proxies</p>
          <codeblock>
<codeph># service nova-compute restart  
</codeph>
</codeblock>
        </li>
        <li>
          <p>To restart HPE VCN agent, execute the following command in OVSvAPP vm's</p>
          <codeblock>
<codeph>#service hpvcn-neutron-agent restart 
</codeph>
</codeblock>
          <p>
            <!--A BR tag was used here in the original source.-->
          </p>
        </li>
      </ul>
      <!-- ===================== horizontal rule ===================== -->
    </section>
    <section id="vsa">
      <title>VSA</title>
      <ol>
        <li>
          <xref type="section" href="#topic23276/fails-retrieve-netmask">Failure to retrieve netmask
            from vsa-bridge</xref>
        </li>
        <li>
          <xref type="section" href="#topic23276/install-script-detect">Installation script detects
            more than 7 available drive</xref>
        </li>
        <li>
          <xref type="section" href="#topic23276/failure-script">Failure of script due to less than
            two drives</xref>
        </li>
        <li>
          <xref type="section" href="#topic23276/cannot-enable-ao">Cannot enable AO as only one disk
            is available</xref>
        </li>
        <li>
          <xref type="section" href="#topic23276/unable-update-json">Unable to update the default
            input json file </xref>
        </li>
        <li>
          <xref type="section" href="#topic23276/storage-pool-fail">Creation of storage pool
            failed</xref>
        </li>
        <li>
          <xref type="section" href="#topic23276/post-vsa-fail">Failed during post VSA
            deployment</xref>
        </li>
        <li>
          <xref type="section" href="#topic23276/vsa-network">vsa_network cannot be destroyed</xref>
        </li>
        <li>
          <xref type="section" href="#topic23276/vsa-pool-cannot-destroy">vsa_storage_pool pool
            cannot be destroyed</xref>
        </li>
      </ol>
    </section>
    <section id="fails-retrieve-netmask">
      <title>Failure to retrieve netmask from vsa-bridge</title>
      <p>
        <b>System Behavior/Message</b>
      </p>
      <p>Cannot retrieve netmask from interface vsa-bridge</p>
      <p>
        <b>Probable Cause</b>
      </p>
      <p>VSA deployment script determines the net-mask and gateway details from the provided
        interface. When there is no IP address assigned to the VSA bridge, this error may occur.</p>
      <p>
        <b>Resolution</b>
      </p>
      <p>To resolve this issue, perform the following steps:</p>
      <ul>
        <li>
          <p>Check whether the IP address is allocated for the VSA bridge</p>
        </li>
        <li>
          <p>Verify the VSA IP address by using the following command:</p>
          <codeblock>
<codeph>ifconfig vsa-bridge
</codeph>
</codeblock>
        </li>
      </ul>
      <p>
        <!--A BR tag was used here in the original source.-->
      </p>
      <!-- ===================== horizontal rule ===================== -->
    </section>
    <section id="install-script-detect">
      <title>Installation script detects more than 7 available drive</title>
      <p>
        <b>System Behavior/Message</b>
      </p>
      <p>Maximum supported devices 7.</p>
      <p>
        <b>Probable Cause</b>
      </p>
      <p>This issue occurs when there are more than 7 available drives detected by the installation
        script to deploy StoreVirtual.</p>
      <p>
        <b>Resolution</b>
      </p>
      <p>Perform the following steps:</p>
      <ul>
        <li>
          <p>HPE StoreVirtual VSA supports up to 7 disks</p>
        </li>
        <li>
          <p>Execute <codeph>fdisk &amp;#45;l</codeph> and check for number of available drives in
            the machine other than <codeph>/dev/sda</codeph>
          </p>
        </li>
      </ul>
      <p>
        <!--A BR tag was used here in the original source.-->
      </p>
      <!-- ===================== horizontal rule ===================== -->
    </section>
    <section id="failure-script">
      <title>Failure of script due to less than two drives</title>
      <p>
        <b>System Behavior/Message</b>
      </p>
      <p>Minimum number of disks must be 2. No disks are available.</p>
      <p>
        <b>Probable Cause</b> When there are less than two drives in the machine, the script will
        fail to execute.</p>
      <p>
        <b>Resolution</b>
      </p>
      <p>To resolve, perform the following steps:</p>
      <ul>
        <li>
          <p>Execute <codeph>fdisk &amp;#45;l</codeph>
          </p>
        </li>
        <li>
          <p>Minimum two drives and maximum of 7 drives should be available for the StoreVirtual
            deployment other than boot disk(<codeph>/dev/sda</codeph>)</p>
        </li>
        <li>
          <p>At least three drives required for enabling AO </p>
        </li>
      </ul>
      <!-- ===================== horizontal rule ===================== -->
    </section>
    <section id="cannot-enable-ao">
      <title>Cannot enable AO as only one disk is available</title>
      <p>
        <b>Probable Cause</b>
      </p>
      <p>For Adaptive Optimization to be enabled, at least three drives must be available.
          <codeph>/dev/sdb</codeph> must be SSD drive(Tier 0) and the remaining will be Tier 1.</p>
      <p>
        <b>Resolution</b>
      </p>
      <p>To resolve the issue, do the following:</p>
      <ul>
        <li>
          <p>Use RAID controllers to create RAID groups.</p>
        </li>
        <li>
          <p>Ensure that you create the RAID group for SSD drives immediately after creating the
            RAID group for boot volume. For example: If three RAID groups are to be created. The
            following is recommended :</p>
          <ul>
            <li>
              <p>
                <b>Step 1</b> : Create the first RAID group for HDD drives and mark this as boot
                volume(/dev/sda)</p>
            </li>
            <li>
              <p>
                <b>Step 2</b>: Create the second RAID group for SSD drives which should be used as
                Tier 0 for AO (/dev/sdb)</p>
            </li>
            <li>
              <p>
                <b>Step 3</b>: Create the third RAID group for HDD drives which will be used as Tier
                1(/dev/sdc)</p>
            </li>
          </ul>
        </li>
      </ul>
      <p>
        <!--A BR tag was used here in the original source.-->
      </p>
      <!-- ===================== horizontal rule ===================== -->
    </section>
    <section id="unable-update-json">
      <title>Unable to update the default input json file</title>
      <p>
        <b>System Behavior/Message</b>
      </p>
      <p>Parsing the default JSON file failed. Unable to update the default input json file.</p>
      <p>
        <b>Probable Cause</b>
      </p>
      <p>The script will parse the configuration file and update the values based on the network and
        configuration files.</p>
      <p>
        <b>Resolution</b>
      </p>
      <p>Perform the following steps:</p>
      <ul>
        <li>
          <p>Verify whether the JSON content is valid in the following files:</p>
          <ul>
            <li>
              <p>
                <codeph>/home/vsa-installer/pyVins/etc/vsa/vsa_config.json</codeph>
              </p>
            </li>
            <li>
              <p>
                <codeph>/etc/vsa/vsa_network_config.json</codeph>
              </p>
            </li>
          </ul>
        </li>
      </ul>
      <p>
        <!--A BR tag was used here in the original source.-->
      </p>
      <!-- ===================== horizontal rule ===================== -->
    </section>
    <section id="storage-pool-fail">
      <title>Creation of storage pool failed</title>
      <p>
        <b>Probable Cause</b>
      </p>
      <p>Virtual storage pool will be created for placing the extracted VSA VM image. The storage
        pool will be created based on local directory on <codeph>/mnt/state/vsa-kvm-storage</codeph>
      </p>
      <p>
        <b>Resolution</b>
      </p>
      <p>Perform the following steps:</p>
      <ul>
        <li>
          <p>Check whether <codeph>/mnt/state/vsa-kvm-storage</codeph> directory is available.</p>
        </li>
        <li>
          <p>Verify for available space to create storage pool in the system.</p>
        </li>
        <li>
          <p>Check the libvirt logs for more errors</p>
        </li>
      </ul>
      <p>Refer <codeph>/var/log/libvirt/libvirt.log</codeph> on VSA system.</p>
      <p>
        <!--A BR tag was used here in the original source.-->
      </p>
      <!-- ===================== horizontal rule ===================== -->
    </section>
    <section id="post-vsa-fail">
      <title>Failed during post VSA deployment</title>
      <p>
        <b>Probable Cause</b>
      </p>
      <p>The script will persist required files in <codeph>/mnt/state/vsa</codeph> which will be
        used for recreating the VSA VM during re-imaging scenario</p>
      <p>
        <b>Resolution</b>
      </p>
      <p>This error will occur if the script fails to find <codeph>network_vsa.xml</codeph>,
          <codeph>storagepool_vsa.xml</codeph> and other configuration files which has to be
        preserved.</p>
      <ul>
        <li>
          <p>Check for the configuration files on ”/‘ path.</p>
        </li>
        <li>
          <p>On success, the script updates the <codeph>/mnt/state/vsa/vsa_config.json</codeph> file
            with the updated and created time.</p>
        </li>
      </ul>
      <p>
        <!--A BR tag was used here in the original source.-->
      </p>
      <!-- ===================== horizontal rule ===================== -->
    </section>
    <section id="vsa-install-fail">
      <title>VSA installation failed</title>
      <p>
        <b>Probable Cause</b>
      </p>
      <p>When VSA installation fails for any of the above reasons, the script will rollback the
        network and storage pool.</p>
      <p>
        <b>Resolution</b>
      </p>
      <p>Verify the <codeph>/installer.log</codeph>
      </p>
      <p>
        <!--A BR tag was used here in the original source.-->
      </p>
      <!-- ===================== horizontal rule ===================== -->
    </section>
    <section id="vsa-network">
      <title>vsa_network cannot be destroyed</title>
      <p>
        <b>Probable Cause</b>
      </p>
      <p>VSA network will be destroyed when the VSA installation fails.</p>
      <p>
        <b>Resolution</b>
      </p>
      <p>Perform the following steps:</p>
      <ul>
        <li>
          <p>Check whether the network is already undefined</p>
        </li>
        <li>
          <p>Check whether the network name in
              <codeph>&lt;PYVINS_DIRS&gt;/etc/vsa/vsa_config.json</codeph> is the same as in the
            output of <codeph>virsh net-list -all</codeph> command</p>
        </li>
      </ul>
      <p>
        <!--A BR tag was used here in the original source.-->
      </p>
      <!-- ===================== horizontal rule ===================== -->
    </section>
    <section id="vsa-pool-cannot-destroy">
      <title>vsa_storage_pool pool cannot be destroyed</title>
      <p>
        <b>Probable Cause</b>
      </p>
      <p>The storage pool will be destroyed when VSA installation fails</p>
      <p>
        <b>Resolution</b>
      </p>
      <p>Perform the following:</p>
      <ul>
        <li>
          <p>Verify whether the storage pool is already undefined</p>
        </li>
        <li>
          <p>Verify whether the pool name is same as in
              <codeph>&lt;PYVINS_DIRS&gt;/etc/vsa/vsa_config.json</codeph>
          </p>
        </li>
        <li>
          <p>Virsh command to list the pools</p>
          <codeblock>
<codeph>Virsh pool-list --all
</codeph>
</codeblock>
        </li>
      </ul>
    </section>
    <section id="recovery-when-scale-out-nodes-of-newly-added-compute-node-or-vsa">
      <title>Recovery when Scale-out nodes of newly added compute node or VSA</title>
      <p>
        <b>
          <i>System Behavior/Message</i>
        </b>
      </p>
      <p>The newly added compute node or VSA node fails during scale-out.</p>
      <p>
        <b>Resolution</b>
      </p>
      <p>You must remove a failed compute node before adding a new compute node.</p>
      <p>Perform the following steps to remove a failed compute node:</p>
      <ol>
        <li>Run <codeph>heat stack-list</codeph> on the undercloud node and search for failed
          stack.</li>
        <li>
          <p>Delete the failed stack using the following command:</p>
          <codeblock>
<codeph># heat stack-delete &lt;stackname or uuid&gt;
</codeph>
</codeblock>
        </li>
        <li>
          <p>List the newly added nova node which is created during scale-out.</p>
          <codeblock>
<codeph># nova-list
</codeph>
</codeblock>
        </li>
        <li>
          <p>Execute the following command to delete nova node. Node name and ID is obtained from
            step 3.</p>
          <codeblock>
<codeph># nova delete &lt;name or id&gt;
</codeph>
</codeblock>
        </li>
        <li>
          <p>View a newly added node using the following command:</p>
          <codeblock>
<codeph># ironic node-list
</codeph>
</codeblock>
        </li>
        <li>
          <p>If newly added node is in <b>ERROR</b> state or it has maintenance as <b>True</b> then
            remove those node(s) using following command.</p>
          <codeblock>
<codeph># ironic node-delete &lt;uuid&gt;, where uuid is the ID of the node
</codeph>
</codeblock>
        </li>
      </ol>
      <!-- ===================== horizontal rule ===================== -->
    </section>
    <section id="scale-out-nodes-os-refresh-config-on-controller-nodes-fail">
      <title>Scale-out nodes : os-refresh-config on Controller Nodes Fail</title>
      <p>
        <b>
          <i>System Behavior/Message</i>
        </b>
      </p>
      <p>The os-refresh-config on controller Nodes fail during scale-out.</p>
      <p>
        <b>
          <i>Probable Cause</i>
        </b>
      </p>
      <p>The controller nodes can fail due to following reasons:</p>
      <ul>
        <li>
          <p>wrong user input</p>
        </li>
        <li>
          <p>rabbitmq clustering</p>
        </li>
        <li>
          <p>mysql clustering</p>
        </li>
      </ul>
      <p>
        <b>Resolution</b>
      </p>
      <p>
        <b>To resolve rabbitmq cluster issue</b>
      </p>
      <ul>
        <li>
          <p>Use the following command and verify the running status of rabbitmq.</p>
          <codeblock>
<codeph>status rabbitmq-server 
</codeph>
</codeblock>
          <p>If rabbitmq is not running, start rabbitmq using <codeph>start rabbitmq-server</codeph>
            command.</p>
        </li>
        <li>
          <p>Verify that the <codeph>rabbitmqctl cluster_status</codeph> displays all 3 nodes in
              <codeph>running_nodes</codeph>, disc. If it does not display one or more nodes in
            running nodes then restart rabbitmq and run the following command on the missing
            nodes:</p>
          <codeblock>
<codeph>rabbitmqctl join_cluster &lt;clusternode&gt;
</codeph>
</codeblock>
        </li>
        <li>
          <p>If <codeph>rabbitmqctl cluster_status</codeph> shows expected output but there is an
            issue with one or more node(s) for joining rabbitmq cluster, do the following:</p>
          <ol>
            <li>
              <p>Execute the following commands on all controller nodes:</p>
              <codeblock>
<codeph>pkill -u rabbitmq  
</codeph>
</codeblock>
            </li>
            <li>
              <p>Run <codeph>os-refresh-config</codeph> command first on the
                  <codeph>cluster_name</codeph> (rabbitmqctl cluster_status output) and on the
                remaining controller nodes.</p>
            </li>
          </ol>
        </li>
      </ul>
      <p>
        <b>Resolve mysql cluster issue</b>
      </p>
      <ol>
        <li>
          <p>Use the following command and verify the running status of mysql on the node.</p>
          <codeblock>
<codeph>/etc/init.d/mysql status
</codeph>
</codeblock>
          <p>If mysql has stopped, restart it.</p>
        </li>
        <li>
          <p>If mysql fails to restart, perform the following instructions:</p>
          <ul>
            <li>Run <codeph>mysqld_safe --wsrep-recover</codeph> on all controller nodes.</li>
            <li>
              <p>Compare the output from all controller nodes for last committed transaction
                sequence number. For example:</p>
              <codeblock>
<codeph>root@overcloud-ce-controller-controller0-defen5afl75f:~# mysqld_safe --wsrep-recover
sed: -e expression #1, char 25: unknown option to `s'
sed: -e expression #1, char 24: unknown option to `s'
141113 01:00:36 mysqld_safe Logging to '/mnt/state/var/log/mysql/error.log'.
141113 01:00:36 mysqld_safe Starting mysqld daemon with databases from /mnt/state/var/lib/mysql/
141113 01:00:36 mysqld_safe Skipping wsrep-recover for 1e9d939a-6a07-11e4-9c28-aa31223485e0:220764 pair
141113 01:00:36 mysqld_safe Assigning 1e9d939a-6a07-11e4-9c28-aa31223485e0:220764 to wsrep_start_position
141113 01:00:38 mysqld_safe mysqld from pid file /var/run/mysqld/mysqld.pid ended
</codeph>
</codeblock>
            </li>
          </ul>
          <p>So the last committed transaction sequence number on this node is 220764.</p>
          <ul>
            <li>Compare the last committed transaction sequence number across all 3 nodes and
              bootstrap from the latest node using <codeph>/etc/init.d/mysql bootstrap-pxc</codeph>
              or <codeph>/etc/init.d/mysql restart</codeph> and start mysql on the remaining
              nodes.</li>
          </ul>
        </li>
      </ol>
    </section>
    <section id="logging">
      <title>Logging</title>
    </section>
    <section id="issue-in-logging">
      <title>Issue in Logging</title>
      <p>The user needs to manually follow the below steps to re-configure Kibana for logging.</p>
      <ol>
        <li>Log in to the undercloud and start screen session.</li>
        <li>In the screen, start following command <codeph>sudo -u logstash /usr/bin/java -Xmx1g
            -Djava.io.tmpdir=/var/lib/logstash/ -jar /opt/logstash/logstash.jar agent -f
            /etc/logstash/conf.d -w 10 --log /var/log/logstash/logstash.log</codeph>
        </li>
        <li>Press Control <b>&amp;</b> '<b>a</b>', then '<b>c</b>' to create another shell.</li>
        <li>In a new shell execute command <codeph>sudo -u logstash /usr/bin/java -Xmx1g
            -Djava.io.tmpdir=/var/lib/logstash/ -jar /opt/logstash/logstash.jar agent -f
            /etc/logstash/conf.d -w 10 --log /var/log/logstash/logstash.log</codeph>
        </li>
        <li>Repeat steps from <b>3-4</b> two times</li>
        <li>Press Control <b>&amp;</b> '<b>a</b>' then '<b>d</b>' to detach.</li>
      </ol>
      <p>
        <b>Note</b>: If node reboots repeat the step from <b>1-6</b>.</p>
      <p>
        <b>EDIT</b>: Added <codeph>sudo -u logstash</codeph> at beginning of commands.</p>
      <p>
        <xref href="#topic23276"> Return to Top </xref>
      </p>
      <!-- ===================== horizontal rule ===================== -->
    </section>
  </body>
</topic>
