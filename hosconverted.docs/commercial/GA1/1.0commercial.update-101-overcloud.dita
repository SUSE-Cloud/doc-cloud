<?xml version="1.0" encoding="UTF-8"?>
<!--Edit status: not edited-->
<!DOCTYPE topic
  PUBLIC "-//OASIS//DTD DITA Topic//EN" "http://docs.oasis-open.org/dita/v1.1/OS/dtd/topic.dtd" ><topic xml:lang="en-us" id="topic14215">
<title>HPE Helion <tm tmtype="reg">OpenStack</tm> 1.0: Updating the Overcloud</title>
<prolog>
<metadata>
<othermeta name="layout" content="default"/>
</metadata>
</prolog>
<body>
<p>
<!--PUBLISHED-->
 <!--./commercial/GA1/1.0commercial.update-101-overcloud.md-->
 <!--permalink: /helion/openstack/update/overcloud/101/--></p>
<!-- No README in 1.01
The *Readme.txt* that comes with a patch update lists the nodes that need to be updated as a result of this patch. This file is located in the directory described in the [Update Troubleshooting](/helion/openstack/update/troubleshooting/101/) of the Update Prerequisites.  

If the Readme.txt does not list any overcloud nodes, the update is complete. -->
<p>Use the this document when updating the overcloud nodes.</p>
<ul>
<li>
<xref type="section" href="#topic14215/pre">Prerequisites</xref>

<ul>
<li>
<xref type="section" href="#topic14215/devplatstop">Stop the HPE Helion Development Platform</xref>
</li>
</ul>
</li>
<li>
<xref type="section" href="#topic14215/update">Update the overcloud</xref>
</li>
<li>
<xref type="section" href="#topic14215/devplatstart">Restart the HPE Helion Development Platform</xref>
</li>
<li>
<xref type="section" href="#topic14215/next-steps">Next Steps</xref>
</li>
</ul>
<p>You can monitor the update process, see <xref href="../../commercial/GA1/1.0commercial.update-101-monitor.dita" >Monitoring the Update</xref>.</p>
<section id="pre"> <title>Prerequisites for the overcloud update</title>
<p>Before you begin the update:</p>
<ul>
<li>
<p>If the undercloud needed updating, perform this update before updating the overcloud, as described in <xref href="../../commercial/GA1/1.0commercial.update-101-undercloud.dita" >Updating the Undercloud Host</xref>
</p>
</li>
<li>
  <p>Make sure a current backup of the management controller, controller0, and controller1 nodes exists, as described in <xref href="../../commercial/GA1/1.0commercial.backup-restore-GA.dita#topic18277/over" type="section">Back Up and Restore</xref>.</p>
</li>
<li>
<p>Review the <xref href="../../commercial/GA1/1.0commercial.update-101-prereqs.dita" >update prerequisites</xref> and make sure all necessary tasks have been performed, including <xref href="../../commercial/GA1/1.0commercial.update-101-prereqs.dita#extract" type="section" >extracting the update scripts</xref>.</p>
</li>
<li>
<p>Point the install script to the overcloud. The patch update scripts are based on the Ansible platform. When patching the overcloud, because the script is launched from the seed cloud host, you need to point the script to the overcloud node.</p>

<p>To point the script to update the overcloud, use the following steps:</p>

<p>a. Copy the <codeph>stackrc</codeph> file to the undercloud and rename the file for the undercloud:</p>

<codeblock>
<codeph>ssh heat-admin@&lt;Undercloud IP&gt;
sudo -i
cp stackrc /home/heat-admin/uc_stackrc
</codeph>
</codeblock>

<p>b. On the seed cloud host, copy the file to the seed host.</p>

<codeblock>
<codeph>scp heat-admin@&lt;Undercloud ip&gt;:uc_stackrc ~/
</codeph>
</codeblock>

<p>c. Edit the <codeph>uc_stackrc</codeph> file to replace the localhost in the <codeph>OS_AUTH_URL</codeph> variable with the IP address of the undercloud.</p>

<codeblock>
<codeph>export OS_AUTH_URL=http://&lt;Undercloud&gt;:5000/v2.0
</codeph>
</codeblock>

<p>d. Source the file:</p>

<codeblock>
<codeph>source ~/uc_stackrc
</codeph>
</codeblock>

<p>e. Execute the following commands:</p>

<codeblock>
<codeph>source /opt/stack/venvs/ansible/bin/activate
cd /opt/stack/tripleo-ansible
bash scripts/inject_nova_meta.bash
export ANSIBLE_LOG_PATH=/var/log/ansible/ansible.log
mkdir -p /var/log/ansible
</codeph>
</codeblock>

<p>The command prompt should change to <codeph>(ansible)</codeph>. You will need to use this <codeph>(ansible)</codeph> session to perform all the update operations.</p>

<p>f. To test that the ansible environment is correctly set up, use the following command to ping all the nodes that ansible can find via its inventory:</p>

<codeblock>
<codeph>ansible all -u heat-admin -i plugins/inventory/heat.py -m ping  
</codeph>
</codeblock>

<p>If successful the ping command will show a ping of every node in a particular cloud.  It will look similar to this with one for each node.</p>

<codeblock>
<codeph>192.0.2.28 | success &gt;&gt; {
    "changed": false,
    "ping": "pong"
}   
</codeph>
</codeblock>
</li>
</ul>
</section>
<section id="devplatstop"> <title>Stop the HPE Helion Development Platform</title>
<p>Before you run the patch update on the overcloud, you must stop the HPE Helion Development Platform service using a script. See <xref href="../../commercial/GA1/1.0commercial.update-101-devplat.stopstart.dita" >Stopping and Starting the Development Platform Services</xref>. After the update is complete, you can execute another script to <xref type="section" href="#topic14215/devplatstart">restart the service</xref>.</p>
</section>
<section id="update"> <title>Update the overcloud</title>
<p>There are two methods to update the overcloud:</p>
<ul>
<li>
<xref type="section" href="#topic14215/upgradescript">Upgrade using the helper script</xref>
</li>
<li>
<xref type="section" href="#topic14215/upgrademanual">Manual upgrade</xref>
</li>
</ul>
<p>You can monitor the update process, see <xref href="../../commercial/GA1/1.0commercial.update-101-monitor.dita" >Monitoring the Update</xref>.</p>
</section>
<section id="upgradescript"> <title>Update the overcloud using the script</title>
<p>To upgrade the overcloud using the <codeph>HelionUpdate.sh</codeph> script included in the patch update download, use the following steps:</p>
<ol>
<li>
<p>Log in the seed VM host:</p>

<codeblock>
<codeph>sudo su -
</codeph>
</codeblock>
</li>
<li>
<p>SSH to the seed VM:</p>

<codeblock>
<codeph>ssh root@192.0.2.1 
</codeph>
</codeblock>
</li>
<li>
<p>Run the following command to start the update:</p>

<codeblock>
<codeph>cd /opt/stack/tripleo-ansible/
./update-helpers/HelionUpdate.sh
</codeph>
</codeblock>

<p>This script will setup the environment, allow you to do test <codeph>ping</codeph>, and perform pre-update checking, update the nodes in a specific order.</p>

<p>The script allows you to update by node type. For example all the controllers are done as a group. Inside each group the individual nodes are presented for the user to update, skip or exit. If you wish to skip a whole group skip at the group level.</p>

<p>You might want to skip a node if you want to test the update on a particular node or if you want to resume an update later on. To skip a node, press the <codeph>s</codeph> key until you get to a node you want to update.</p>

<p>
<b>Note:</b> The script will detect if a node fails to update. If such a failure is detected the script will exit.  When this happens you should refer to the <xref href="../../commercial/GA1/1.0commercial.update-101-troubleshooting.dita" >Update Troubleshooting</xref> document for known workarounds.</p>
</li>
<li>
<p>When the node update is done, validate the updated node.</p>
</li>
<li>
<p>Manually reboot the compute nodes that were updated or using <codeph>nova stop</codeph> and <codeph>nova start</codeph> in the undercloud.</p>

<p>If after the instances are started it is not possible to ping/ssh to an instance, reboot the instance.</p>
</li>
</ol>
<p>
<b>Note:</b>  If your cloud has ESX host and the update includes new images for ESX Proxy and ESX OVSvAPP, refer to <xref type="section" href="#topic14215/redeploy">Redeploy Compute Proxy and OVSvAPP on ESX Host</xref> after updating Controller Management node and before proceeding to controller nodes.</p>
</section>
<section id="upgrademanual"> <title>Update the undercloud manually</title>
<p>The manual update process allows you to upgrade each node in order.</p>
<p>Make sure that the updated node is back up and running before you update other nodes. This is especially important for the overcloud controllers.</p>
<p>The recommended update order is</p>
<ol>
<li>Controller management node </li>
<li>Controller nodes (0,1) </li>
<li>Compute service </li>
<li>Object Operations (Swift) service </li>
<li>HPE StoreVirtual VSA (for KVM only)</li>
</ol>
<p>All Ansible commands must be run in the SSH session setup that has <codeph>ansible</codeph> in the command prompt.  Validation techniques may be run from any seed SSH session.</p>
<p>Run through the appropriate commands below for this order.</p>
</section>
<section id="prerequisites-for-manual-update"> <title>Prerequisites for manual update</title>
<p>Before you start, do the following:</p>
<ul>
<li>
<p>Make sure services are in operational state by running the following command:</p>

<codeblock>
<codeph>cd /opt/stack/tripleo-ansible
ansible-playbook -vvvv -M library/cloud -i plugins/inventory/heat.py -u heat-admin playbooks/pre-flight_check.ym
</codeph>
</codeblock>
</li>
<li>
<p>Determine the overcloud nodes that will need to be updated. The <i>Readme.txt</i> that comes with a patch update will tell you what nodes need to be updated as a result of this patch. It will be located in the directory described in the <xref href="../../commercial/GA1/1.0commercial.update-101-prereqs.dita#extract" type="section">Extracting the required scripts and libraries</xref>.</p>

<p>For each node, obtain the image ID and IP address. For each of these, record the ID, to get the ID, see <xref href="../../commercial/GA1/1.0commercial.update-101-prereqs.dita#info" type="section">Gathering information needed for update</xref>.</p>
</li>
</ul>
<p>To manually update the overcloud nodes:</p>
<ol>
<li>
<p>Update the overcloud management controller using the following command:</p>

<codeblock>
<codeph>ansible-playbook -vvvv -u heat-admin -i plugins/inventory/heat.py -e force_rebuild=True -e single_controller=True  -l &lt;IP of controller mgmt&gt; -e controllermgmt_rebuild_image_id=&lt;glance Image_ID of  overcloud-control-mgmt&gt; playbooks/update_cloud.yml
</codeph>
</codeblock>
</li>
<li>
<p>Verify that the management controller is running by performing the following steps, from a separate seed SSH window:</p>

<p>a. SSH into overcloud-controller-mgmt node:</p>

<codeblock>
<codeph>ssh heat-admin@&lt;Insert IP of controller-mgmt node&gt;
sudo -i
. stackrc # - to source the credentials or setup crednetials
nova list
heat stack-list
glance image-list
mysql 
</codeph>
</codeblock>

<p>b. Run <codeph>progress.sh</codeph> script to validate that the image ID matches the one expected by the update by running the following command:</p>

<codeblock>
<codeph>cd /opt/stack/tripleo-ansible:
./update-helpers/progress.sh
</codeph>
</codeblock>

<p>The <codeph>progress.sh</codeph> should disply <codeph>needed N</codeph>, meaning the image ID matches the latest the patch knows about for the management controller node.</p>

<p>
<b>Note:</b>  If your cloud has ESX host and the update includes new images for ESX Proxy and ESX OVSvAPP, refer to <xref type="section" href="#topic14215/redeploy">Redeploy Compute Proxy and OVSvAPP on ESX Host</xref> after updating Controller Management node and before proceeding to controller nodes.</p>
</li>
<li>
<p>Update overcloud controller0 using the following command:</p>

<codeblock>
<codeph>ansible-playbook -vvvv -u heat-admin -i plugins/inventory/heat.py -e force_rebuild=True -e single_controller=True -l &lt;IP of controller&gt; -e controller_rebuild_image_id=&lt;glance Image_ID of overcloud-control&gt; playbooks/update_cloud.yml
</codeph>
</codeblock>

<p>Where:</p>

<ul>
<li>
<codeph>&lt;IP of controller&gt;</codeph> is the IP address of overcloud controller0</li>
<li>
<codeph>&lt;glance Image_ID of overcloud-control&gt;</codeph> is the image ID of controller0</li>
</ul>
</li>
<li>
<p>Verify that the controller0 is running by performing the following steps, from a separate ssh window:</p>

<p>a. SSH into overcloud-controller-mgmt node:</p>

<codeblock>
<codeph>ssh heat-admin@&lt;Insert IP of controller-mgmt node&gt;
sudo -i
. stackrc # - to source the credentials or setup crednetials
nova list
heat stack-list
glance image-list
msql 
</codeph>
</codeblock>

<p>b. Run <codeph>progress.sh</codeph> script to validate that the image ID matches the one expected by the update by running the following command:</p>

<codeblock>
<codeph>cd /opt/stack/tripleo-ansible/update-helpers/
./progress.sh
</codeph>
</codeblock>

<p>The <codeph>progress.sh</codeph> should disply <codeph>needed N</codeph>, meaning the image ID matches the latest the patch knows about for the management controller node</p>
</li>
<li>
<p>Update overcloud controller1 using the following command:</p>

<codeblock>
<codeph>ansible-playbook -vvvv -u heat-admin -i plugins/inventory/heat.py -e force_rebuild=True -e single_controller=True -l &lt;IP of controller&gt; -e controller_rebuild_image_id=&lt;glance Image_ID of overcloud-control&gt; playbooks/update_cloud.yml
</codeph>
</codeblock>

<p>Where:</p>

<ul>
<li>
<codeph>&lt;IP of controller&gt;</codeph> is the IP address of overcloud controller1</li>
<li>
<codeph>&lt;glance Image_ID of overcloud-control&gt;</codeph> is the image ID of controller1</li>
</ul>
</li>
<li>
<p>Verify that the controller0 is running by performing the following steps, from a separate ssh window:</p>

<p>a. SSH into overcloud-controller-mgmt node:</p>

<codeblock>
<codeph>ssh heat-admin@&lt;Insert IP of controller-mgmt node&gt;
sudo -i
. stackrc # - to source the credentials or setup crednetials
nova list
heat stack-list
glance image-list
msql 
</codeph>
</codeblock>

<p>b. Run <codeph>progress.sh</codeph> script to validate that the image ID matches the one expected by the update by running the following command:</p>

<codeblock>
<codeph>cd /opt/stack/tripleo-ansible/update-helpers/
./progress.sh
</codeph>
</codeblock>

<p>The <codeph>progress.sh</codeph> should disply <codeph>needed N</codeph>, meaning the image ID matches the latest the patch knows about for the management controller node.</p>
</li>
</ol>
<p>Note:  n-scale nodes are rolled out 1 per stack and can scale from 1 to n in number.</p>
</section>
<section id="compute"> <title>Compute (n-scale, not ESX)</title>
<p>You should update the Compute nodes in such a way as to allow workloads to continue to function.</p>
<p>To do this, migrate the workloads to nodes that will not be down during a particular compute node set update and then migrate them back when done.</p>
<ol>
<li>
<p>Use the <codeph>nova list</codeph> command in the undercloud to see the full set of compute nodes available to be updated.</p>
</li>
<li>
<p>If your environment uses availability zones, do not bring down all availability zones. Select one zone at a time to prevent stopping all workloads.</p>

<p>a. To get a list of availability zones:</p>

<codeblock>
<codeph>nova availability-zone-list. 
</codeph>
</codeblock>

<p>b.  To get a list of hosts inside a zone:</p>

<codeblock>
<codeph>nova host-list [--zone &lt;zone&gt;]
</codeph>
</codeblock>
</li>
<li>
<p>Migrating hosts is outside the scope of this document, however using the list and availability zone info a user can use techniques. See OpenStack documentation for <xref href="http://docs.openstack.org/admin-guide-cloud/content/section_live-migration-usage.html" scope="external" format="html" >Migrate instances</xref> and <xref href="http://docs.openstack.org/admin-guide-cloud/content/section_configuring-compute-migrations.html" scope="external" format="html" >Configure migrations</xref> to migrate workloads to a specific node.</p>
</li>
<li>
<p>If this update affects several maintenance cycles, you can run the progress script to see where they are in the update process.</p>

<p>
<b>One at a time:</b>
</p>

<p>This command will update only one compute node, you will have to repeat it for every compute IP address.</p>

<codeblock>
<codeph>ansible-playbook -vvvv -u heat-admin -i plugins/inventory/heat.py -e force_rebuild=True -l &lt;IP of overcloud-compute &gt; -e nova_compute_rebuild_image_id =&lt;glance Image_ID of overcloud-compute &gt; playbooks/update_cloud.yml
</codeph>
</codeblock>

<p>
<b>A set at a time (set can be 1 to n):</b>
</p>

<p>This command may update only update a subset of compute nodes, you are responsibe to update all the nodes with multiple commands if necessary.</p>

<codeblock>
<codeph>ansible-playbook -vvvv -u heat-admin -i plugins/inventory/heat.py  -e force_rebuild=True -l IP_Compute_1:IP_Compute_2:IP_Compute_3...:IP_Compute_n -e nova_compute_rebuild_image_id=&lt;glance Image_ID of overcloud-compute &gt; playbooks/update_cloud.yml
</codeph>
</codeblock>
</li>
<li>
<p>Manually reboot the compute nodes that were updated or using <codeph>nova stop</codeph> and <codeph>nova start</codeph> in the undercloud.</p>

<p>If after the instances are started it is not possible to ping/ssh to an instance, reboot the instance.</p>
</li>
</ol>
</section>
<section id="swift-n-scale"> <title>Swift (n-scale):</title>
<p>For Swift, it is strongly encouraged that you update one node at a time (node by node).</p>
<p>The following command updates only one Swift node; you will have to repeat it for every swift IP address.</p>
<codeblock>
  <codeph>ansible-playbook -vvvv -u heat-admin -i plugins/inventory/heat.py -e force_rebuild=True -l &lt;IP of overcloud-swift nscale&gt; -e swift_storage_rebuild_image_id =&lt;glance Image_ID of overcloud-swift nscale&gt; playbooks/update_cloud.yml
</codeph>
</codeblock>
</section>
<section id="vsa-n-scale"> <title>VSA (n-scale):</title>
<p>This command updates only one VSA node; you will have to repeat it for every VSA IP address.</p>
<codeblock>
  <codeph>ansible-playbook -vvvv -u heat-admin -i plugins/inventory/heat.py -e force_rebuild=True -l &lt;IP of vsa overcloud node&gt; -e vsa_rebuild_image_id =&lt;glance Image_ID of vsa overcloud node &gt; playbooks/update_cloud.yml
</codeph>
</codeblock>
</section>
<section id="backup"> <title>Back up the updated overcloud</title>
<p>Once the update of overcloud node is complete, you should back up the node in case of failures.  See <xref href="../../commercial/GA1/1.0commercial.backup-restore-GA.dita" >Back Up and Restore</xref>.</p>
</section>
<section id="redeploy"> <title>Redeploy Compute Proxy and OVSvAPP on ESX Host</title>
<p>If your infrastructure includes ESX proxy hosts, update the Compute Proxy and OVSvAPP after updating the overcloud controller management node. The update package contains new images for ESX. Failure to do so will prevent users from launching VMs in vCenter Hosts.
To manage the VMs launched using the older Compute proxy, use the same hostname entered for older Compute proxy.</p>
<p>
  <b>Redeploy Nova Compute Proxy</b>
</p>
<ol>
<li>
<p>Log in to the undercloud.</p>
</li>
<li>
<p>Deactivate the activated cluster.</p>
</li>
<li>
<p>Delete the <codeph>overcloud_vcenter_compute_proxy</codeph> template in the vCenter</p>
</li>
<li>
<p>Deploy the Compute proxy as described in <xref href="../../commercial/GA1/1.0commercial.install-GA-ESX-Proxy.dita" >Deploy vCenter ESX compute proxy</xref>, using the latest <codeph>overcloud_vcenter_compute_proxy.ova</codeph>.</p>
</li>
<li>
<p>Activate the cluster which was imported before.</p>
</li>
</ol>
<p>
  <b>Redeploy OVSvAPP</b>
</p>
<ol>
<li>
<p>Power off and delete the OVSvAPP VMs in the vCenter.</p>
</li>
<li>
<p>Log in to the overcloud ControllerMgmt node.</p>
</li>
<li>
<p>Delete the HPE VCN L2 agents:</p>

<codeblock>
<codeph>neutron agent-delete &lt;HPE VCN L2 agent ID&gt;
</codeph>
</codeblock>
</li>
<li>
<p>Delete the <codeph>overcloud-esx-ovsvapp</codeph> template in the vCenter.</p>
</li>
<li>
<p>Follow the steps in <xref href="../../commercial/GA1/1.0commercial.install-GA-ovsvapp.dita" >Deploying and Configuring OVSvApp on ESX hosts</xref> to install OVSvAPP, but use the latest <codeph>overcloud-esx-ovsvapp.ova</codeph> and <codeph>ovsvapp.tgz</codeph>.</p>
</li>
</ol>
</section>
<section id="devplatstart"> <title>Restart the HPE Helion Development Platform</title>
<p>After the update is complete, you can execute a script to restart the service. See <xref href="../../commercial/GA1/1.0commercial.update-101-devplat.stopstart.dita" >Stopping and Starting the Development Platform Services</xref>.</p>
</section>
<section id="next-steps"> <title>Next Steps</title>
<p>If your cloud uses the optional HPE Helion OpenStack DNSaaS (Domain Name Server as a Service), upgrade DNSaaS.</p>
<p>For installation instructions, see <xref href="../../commercial/GA1/1.0commercial.install-GA-DNSaaS.dita" >DNSaaS Installation and Configuration</xref>.</p>
<p>
  <xref href="#topic14215"> Return to Top </xref>
</p>
<!-- ===================== horizontal rule ===================== -->
</section>
</body>
</topic>
