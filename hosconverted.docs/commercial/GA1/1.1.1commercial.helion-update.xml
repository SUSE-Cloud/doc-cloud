<?xml version="1.0" encoding="UTF-8"?>
<!--Edit status: not edited-->
<!DOCTYPE topic
  PUBLIC "-//OASIS//DTD DITA Topic//EN" "http://docs.oasis-open.org/dita/v1.1/OS/dtd/topic.dtd" ><topic xml:lang="en-us" id="topic51655">
<title>HPE Helion <tm tmtype="reg">OpenStack</tm> 1.1.1: Update Procedure</title>
<titlealts>
<searchtitle>HPE Helion Openstack 1.1: Update Procedure</searchtitle>
</titlealts>
<prolog>
<metadata>
<othermeta name="layout" content="default"/>
<othermeta name="product-version" content="HPE Helion Openstack"/>
<othermeta name="product-version" content="HPE Helion Openstack 1.1"/>
<othermeta name="role" content="Systems Administrator"/>
<othermeta name="role" content="Cloud Architect"/>
<othermeta name="role" content="Storage Administrator"/>
<othermeta name="role" content="Network Administrator"/>
<othermeta name="role" content="Service Developer"/>
<othermeta name="role" content="Cloud Administrator"/>
<othermeta name="role" content="Application Developer"/>
<othermeta name="role" content="Network Engineer"/>
<othermeta name="role" content="Michael B, Paul F"/>
<othermeta name="product-version1" content="HPE Helion Openstack"/>
<othermeta name="product-version2" content="HPE Helion Openstack 1.1"/>
</metadata>
</prolog>
<body>
<p>
<!--PUBLISHED-->
 <!--./commercial/GA1/1.1.1commercial.helion-update.md-->
 <!--permalink: /helion/openstack/1.1.1/update/--></p>
<p>Welcome to the HPE Helion OpenStack v1.1.1 update instructions. These instructions apply to existing HPE Helion OpenStack v1.1 installations and describe how you can update your HPE Helion OpenStack cloud environment from v1.1 to v1.1.1. (The process of updating a HPE Helion OpenStack v1.0 or v1.01 release to HPE Helion OpenStack v1.1.1 is not supported.)</p>
<p>
<b>Note to Helion Development Platform users</b>: 
The HPE Helion Development Platform requires HPE Helion OpenStack v1.1.1. However, HPE Helion OpenStack v1.1.1 does not currently support updating the HPE Helion Development Platform from v1.1 to v1.2.</p>
    <note> You can also deploy the HPE Helion OpenStack v1.1.1 release from scratch. For those
      instructions, please see <xref
        href="../../commercial/GA1/1.1commercial.install-GA-overview.xml">Installing Helion
        OpenStack</xref>. <!--verify link--></note>
<!-- Helion OpenStack v1.1.1 update scripts update HPE Helion OpenStack v1.1 to version v1.1.1 and starts the update process of Helion Development Platform v1.1 to v1.2. To complete the HPE Helion Development Platform update to v1.2, refer to the HPE Helion Development Platform documentation listed in [Updating Helion Development Platform](/helion/devplatform/1.2/upgrade/). -->
<!--
**Note for Helion Development Platform version v1.1 users**: This Helion OpenStack version v1.1.1 update procedure supports Helion Development Platform version v1.2, (and only Helion Development Platform version v1.2). For users who have installed Helion OpenStack v1.1 and Helion Development Platform v1.1, when you update to HPE Helion OpenStack v1.1.1, you also need to update to Helion Development Platform v1.2.  

**Important for HPE Helion Development Platform users**: During the HPE Helion OpenStack v1.1.1 update, you will need to stop your Helion Development Platform services. Failure to do so will likely damage your existing Helion Development Platform v1.1 installation. 
For details on the HPE Helion Development Platform update to v1.2, see [Updating Helion Development Platform](/helion/devplatform/1.2/upgrade/). -->
<p>The process of updating HPE Helion OpenStack v1.1 to HPE Helion OpenStack v1.1.1 consists of three procedures:</p>
<ol>
<li>
        <xref type="section" href="#topic51655/seed-update-process">Update the seed</xref>
      </li>
<li>
<xref type="section" href="#topic51655/uc-update-process">Update the undercloud</xref>
</li>
<li>
<xref type="section" href="#topic51655/oc-update-process">Update the overcloud</xref>
</li>
</ol>
<p>These procedures are described in the following sections. You must perform each of these three
      procedures in the order presented: first the seed, then the undercloud, and finally, the
      overcloud.</p>
<p>Allow a full day for the  HPE Helion OpenStack v1.1.1 update to complete. During the update process, affected systems will be offline.</p>
    <p><b>Important</b>: Before starting your update: </p>
    <ol id="ol_hqm_fw1_ms">
      <li> Make a backup of the seed, undercloud, and overcloud using the standard procedures before
        commencing the update process. <!-- DOCS-1354 --></li>
      <li> Check that the nodes to be updated are in a "good" state before running the update.
        Specifically, being in a "good" state means:</li>
    </ol>
<ul>
<li>Nodes must be up</li>
<li>Nodes must be running</li>
<li>Heat state of each node to be updated must not include any FAIL status<p>If during the course of
          this update, a node fails, the update will likely fail. You will have to correct the node
          failure and start over. </p></li>
</ul>
    <note> This document describes <codeph>/root/tripleo</codeph> as the installation directory for
      the current (1.1) version. You can install this update whereever you want but if you use a
      directory other than <codeph>/root/tripleo</codeph>, you will need to modify the commands
      shown.</note>
<section id="deployment-considerations">
      <title>Deployment considerations</title>
      <p>Check that the deployed nodes (all seed VM, undercloud, and overcloud nodes) are accessible
        (SSH) and pingable. Note the network and bridge configurations on all nodes using the
          <codeph>ifconfig</codeph> and <codeph>ovs-vsctl show</codeph> commands.</p>
      <p>Check the version of the Glance images that were used to deploy each of the overcloud nodes
        before the update. To check the Glance image versions:</p>
      <ol>
        <li>SSH to the undercloud node.</li>
        <li>Enter:<codeblock><codeph>source stackrc
glance image-list
nova list
nova show &lt;instance id&gt;
</codeph></codeblock></li>
        <li>Compare the Image ID returned from the <codeph>glance image-list</codeph> command to
          that returned from the <codeph>nova show</codeph> command.</li>
      </ol>
      <p/>Make sure the following commands are working before you start the update:<ul
        id="ul_wvj_wzm_rs">
        <li>keystone user-list</li>
        <li>nova list</li>
        <li>neutron net-list</li>
        <li>glance image-list</li>
        <li>os-refresh-config</li>
        <li>os-collect-config</li>
      </ul><p>If you are unsure of the version of Helion you are running, from any Helion node (seed
        VM, undercloud, or overcloud node),
        enter:</p><codeblock><codeph># cat /etc/HP_Helion_version</codeph></codeblock><p>A sample of
        the output is:</p><codeblock><codeph>HPE Helion Openstack 1.1 Build 81</codeph></codeblock>
    </section>
<section id="seed-update-process"> <title>Helion v1.1 to v1.1.1 seed update process</title>
<p>The procedure will guide you through updating to Helion v1.1.1 from the seed host using
          <codeph>update_sd.sh</codeph>.</p>
      <note> You must have the <codeph>jq</codeph> package for your version of Ubuntu installed on
        the seed. If you need to install this package, enter:</note>
<codeblock><codeph>apt-get install jq</codeph></codeblock>
<!--
**Warning for HPE Helion Development Platform users**: Failure to source your `kvm-custom-ips.json` file could result in damage to your HPE Helion Development Platform installation. For more information see [Environment Variables being ignored](#envvarsignored).
-->
<p>To prepare and update the seed VM:</p>
<ol>
<li>Log into the seed VM by
          entering:<codeblock><codeph>ssh root@&lt;seed-ip&gt;
</codeph></codeblock></li>
<li>Remove any update folders and content from previous update attempts in the
            <codeph>/root</codeph> folder using the <codeph>rm</codeph> command.</li>
<li>From <codeph>/root</codeph>, create the <codeph>helion-update-1.1-to-1.1.1</codeph> directory
          then change to this directory by
          entering:<codeblock><codeph>cd /root
mkdir helion-update-1.1-to-1.1.1
cd helion-update-1.1-to-1.1.1
</codeph></codeblock></li>
  <li>Download the update kit <codeph>HP_Helion_OpenStack_1.1.1.tgz</codeph> to the
            <codeph>helion-update</codeph> directory on the seed VM.</li>
<li>Extract the update kit by
          entering:<codeblock><codeph>tar -xvzf HP_Helion_OpenStack_1.1.1.tgz</codeph></codeblock></li>
<li>Copy the <codeph>kvm-custom-ips.json</codeph> file to the new directory by
          entering:<codeblock><codeph>cp ~/tripleo/configs/kvm-custom-ips.json ~/helion-update-1.1-to-1.1.1/tripleo/configs/kvm-custom-ips.json</codeph></codeblock></li>
<li>Verify that there is enough free space on the system to carry out the update. The space you need
          must support a copy of the following folders:<ul>
            <li>/root/tripleo </li>
            <li>/mnt/state </li>
            <li>/root/helion-update/helion-update-1.1-to-1.1.1</li>
            <li>/tftpboot</li>
          </ul></li>
<li>To see the amount of space these folders require,
          enter:<codeblock><codeph>du -hcs /root/tripleo /mnt/state /root/helion-update* /tftpboot</codeph></codeblock></li>
<li>Check the free space on the system by
            entering:<codeblock><codeph>df -h /</codeph></codeblock><p>The space required cannot
            exceed the space available (that is, the output from the <codeph>du -hcs</codeph>
            command).</p><p>If you need to free up space, you can safely remove the
              <codeph>HP_Helion_OpenStack_1.1.1.tgz</codeph> update kit that you just
          extracted.</p></li>
<li>Verify that the following files exist in the seed VM:<ul>
            <li>/root/eca.key</li>
            <li>/root/eca.crt</li>
          </ul><p>If you specified the ephemeral CA key and certificate from a different directory
            using environment variables <codeph>EPHEMERAL_CA_KEY_FILE</codeph> and
              <codeph>EPHEMERAL_CA_CERT_FILE</codeph>, you must copy them to the seed VM
              <codeph>/root</codeph> directory. Then rename them to <codeph>eca.crt</codeph> and
              <codeph>eca.key</codeph>.</p><p>If you do not have these files, you may be able to
            regenerate them from the installer data using the following commands:</p><p>
            <codeblock>echo -e $(jq '.parameters.EphemeralCaKey' ../tripleo/overcloud-env.json | sed 's/\"//g') &gt; /root/eca.key
echo -e $(jq '.parameters.EphemeralCaCert' ../tripleo/overcloud-env.json | sed 's/\"//g') &gt; /root/eca.crt</codeblock>
          </p></li>
<li>If you generated these files from the installer data, you need to verify that the files are well
          formed by
            entering:<codeblock><codeph>openssl rsa -noout -in /root/eca.key
openssl x509 -noout -in /root/eca.crt
</codeph></codeblock><p>If
            nothing displays, it means the file is readable and at least reasonably well formatted.
            Otherwise, a message similar to this one
          displays:</p><codeblock><codeph>unable to load certificate
140170817259168:error:0906D06C:PEM routines:PEM_read_bio:no start line:pem_lib.c:703:Expecting: TRUSTED CERTIFICATE".
</codeph></codeblock></li>
<li><note>Log out of the seed VM. The remaining commands in this section are all executed from the
            seed host.</note> From <codeph>/root</codeph> on the seed host, create the
            <codeph>helion-update-1.1-to-1.1.1</codeph> directory then change to this directory by
          entering:<codeblock><codeph>cd /root
mkdir helion-update-1.1-to-1.1.1
cd helion-update-1.1-to-1.1.1 
</codeph></codeblock></li>
<li>Copy the seed update script from the downloaded update kit on the seed VM to the seed by
          entering:<codeblock><codeph>scp root@&lt;seed-vm-ip&gt;:/root/helion-update-1.1-to-1.1.1/tripleo/helion-update/seed_update/update_sd.sh .
</codeph></codeblock></li>
  <li>Source the <codeph>kvm-custom-ips.json file</codeph>: <p>
            <codeblock><codeph>source ~/tripleo/tripleo-incubator/scripts/hp_ced_load_config.sh ~/tripleo/configs/kvm-custom-ips.json</codeph></codeblock>
          </p>Any additional custom variables that were used for the initial seed installation must
          be exported again. <p>Run the update script from the helion update directory created in
            Step 12,
          above.</p><codeblock><codeph>./update_sd.sh /root/tripleo /root/helion-update-1.1-to-1.1.1 |&amp; tee seed_update.log</codeph></codeblock></li>

<li>To purge the known_hosts entries on the seed machine,
            enter:<codeblock><codeph>ssh-keygen -R &lt;seed_ip_address&gt;
</codeph></codeblock><p>You
            will be prompted to re-add the seed to your known_hosts file when you SSH into the seed
            to update the undercloud and overcloud.</p></li>
</ol>
</section>
  <section id="uc-update-process"> <title>Helion v1.1 to v1.1.1 undercloud update process</title>
<p>The procedure will guide you through updating the undercloud by running the <codeph>update_uc.sh</codeph> shell script.</p>
<!-- copy note from seed for devplat users -->
<p>
<b>Important</b>:
If you are updating to a release that includes new Orchestration (Heat) templates, you must apply these new templates to your system before you attempt to update it.</p>
<p>The update process does not automatically overwrite your Heat template, which would cause you to lose your modifications. If you have modified your Heat template, then you will need to update the new templates with these modifications.</p>
<p>To apply updated Heat templates, as root, run:</p>
<codeblock><codeph>./tripleo/tripleo-incubator/scripts/hp_ced_installer.sh --update-undercloud 
</codeph></codeblock>
<p>To update your HPE Helion OpenStack undercloud from v1.1 to v1.1.1, from the
        seed VM:</p>  
<ol>
  <li> <p>Log into the seed VM by
    entering:<codeblock><codeph>ssh root@&lt;seed-ip&gt;
</codeph></codeblock></p>
  </li>
<li>Source the <codeph>kvm-custom-ips.json</codeph> file and run the update script by
          entering:<codeblock>source tripleo/tripleo-incubator/scripts/hp_ced_load_config.sh tripleo/configs/kvm-custom-ips.json
cd helion-update-1.1-to-1.1.1/tripleo/helion-update/undercloud_update
./update_uc.sh ~/helion-update-1.1-to-1.1.1/tripleo</codeblock></li>
<li>Confirm that update was successful by examining the
            <codeph>/var/log/ansible/ansible.log</codeph>. This report should not list any
          unreachable or failed nodes. That is <codeph>unreachable</codeph> and
            <codeph>failed</codeph> must always equal 0. An example of a successful update is shown
          in the output
          below:<codeblock><codeph>PLAY RECAP ********************************************************************
10.23.67.141   : ok=122  changed=74   unreachable=0failed=0
10.23.67.143   : ok=37   changed=22   unreachable=0failed=0
10.23.67.144   : ok=122  changed=74   unreachable=0failed=0
10.23.67.145   : ok=103  changed=62   unreachable=0failed=0
10.23.67.146   : ok=37   changed=22   unreachable=0failed=0
10.23.67.147   : ok=60   changed=30   unreachable=0failed=0
10.23.67.148   : ok=60   changed=30   unreachable=0failed=0
</codeph></codeblock></li>
<li>Change file permissions by
          entering:<codeblock><codeph>chmod 775 /mnt/state/var/eon
chmod 775 /mnt/state/var/eon/data
</codeph></codeblock></li>
</ol>
</section>
<section id="oc-update-process"> <title>Helion v1.1 to v1.1.1 overcloud update process</title>
<p>This procedure explains how to update the overcloud using <codeph>update_oc.sh</codeph>.</p>
<!-- add note from seed about devplat users not destroying their install -->
<!--
**Note for Helion Development Platform users**: The Helion Development Platform must be shut down before updating the overcloud. For instructions on shutting down the Development Platform, see [Updating Helion Development Platform](/helion/devplatform/1.2/upgrade/).
-->
<p>The v1.1.1 release includes new Orchestration (Heat) templates and new passthrough files. To apply these new templates to your system before updating the overcloud:</p>
<ol id="ol_z5h_d1h_ms">
        <li>Log on to the seed
          VM.<codeblock><codeph>ssh root@&lt;seed-ip&gt;
</codeph></codeblock></li>
        <li>Copy the new installation scripts, passthrough files and heat templates to their
          respective original <codeph>/root/tripleo/</codeph> directories. If you made modifications
          to the passthrough files or heat templates, you will need to reapply these changes to the
          newly copied files. In this case it is advisable to back up the existing files for your
          own reference before
          proceeding:<codeblock>cp -r /root/helion-update-1.1-to-1.1.1/tripleo/tripleo-incubator/scripts /root/tripleo/tripleo-incubator/
cp -r /root/helion-update-1.1-to-1.1.1/tripleo/hp_passthrough /root/tripleo/
cp -r /root/helion-update-1.1-to-1.1.1/tripleo/tripleo-heat-templates /root/tripleo/</codeblock></li>
        <li>Create the new <codeph>from-heat.conf</codeph> file on the seed using the following
          command:<codeblock><codeph>root@hLinux:~# cat &gt; from-heat.conf &lt;&lt;EOF
{{#stunnel}}
{{#connect_host}}
pid = /var/run/stunnel4/from-heat.pid
cert = {{stunnel.cert_location}}
key = {{stunnel.key_location}}
options = NO_SSLv2
options = NO_SSLv3
debug = 4
output = /var/log/stunnel4/helion_stunnel.log
syslog = no

{{#verify}}
verify = {{{verify}}}
{{/verify}}
{{#ports}}

[{{{name}}}]
accept = {{#accept_host}}{{{.}}}:{{/accept_host}}{{{accept}}}
connect = {{connect_host}}:{{connect}}
{{#client}}
client = {{.}}
{{/client}}
{{#timeout}}
TIMEOUTclose = {{.}}
{{/timeout}}
{{#session_cache}}
sessionCacheSize = {{.}}
{{/session_cache}}
{{#ciphers}}
ciphers = {{{.}}}
{{/ciphers}}
{{/ports}}
{{/connect_host}}
{{/stunnel}}
EOF
</codeph></codeblock></li>
        <li>Copy the new <codeph>my.cnf</codeph> and <codeph>stunnel/from-heat.conf</codeph> files
          to each of the overcloud controller nodes: To do this,
          enter:<codeblock><codeph>TE_DATAFILE=/root/tripleo/ce_env.json . /root/tripleo/tripleo-incubator/undercloudrc
for ip in `nova list | grep controller[0-2] | awk '{print $12}' | sed -e "s/.*=\\([0-9.]*\\).*/\1/"`; do
echo $ip;
scp from-heat.conf heat-admin@$ip:~ 
scp /usr/libexec/os-apply-config/templates/mnt/state/etc/mysql/my.cnf heat-admin@$ip:~ 
ssh heat-admin@$ip "sudo cp ~heat-admin/my.cnf /usr/libexec/os-apply-config/templates/mnt/state/etc/mysql/" 
ssh heat-admin@$ip "sudo cp ~heat-admin/from-heat.conf /usr/libexec/os-apply-config/templates/etc/stunnel/" 
ssh heat-admin@$ip "sudo /usr/local/bin/os-apply-config  --validate" 
done</codeph></codeblock></li>
        <li>Load the custom-ips json file, along with any custom variables that were defined for the
          initial installation of the overcloud. Then as root user, run
            <codeph>update-overcloud</codeph> to apply the new templates and passthrough files from
          the <codeph>/root</codeph> directory on the seed. For
            example:<codeblock><codeph>cd /root
source tripleo/tripleo-incubator/scripts/hp_ced_load_config.sh tripleo/configs/kvm-custom-ips.json
OVERCLOUD_COMPUTESCALE=5
OVERCLOUD_NEUTRON_DVR=True
OVERCLOUD_NTP_SERVER=10.22.33.44 
./tripleo/tripleo-incubator/scripts/hp_ced_installer.sh --update-overcloud
</codeph></codeblock><p>Because
            the images have not yet been updated, the above command should return the
            message:</p><codeblock><codeph>++ wait_for 30 10 nova service-list --binary nova-compute '2&gt;/dev/null' '|' grep 'enabled.*\ up\ '
Timing out after 300 seconds:
COMMAND=nova service-list --binary nova-compute 2&gt;/dev/null | grep enabled.*\ up\
OUTPUT=
</codeph></codeblock><p>This
            is expected behavior for KVM-based systems.</p></li>
        <li>Restart MySQL and stunnel on the overcloud controller nodes by
              entering:<codeblock>for ip in `nova list | grep controller[0-2] | awk '{print $12}
 ' | sed -e "s/.=\\([0-9.]\\).*/\1/"`; do 
echo $ip; 
ssh heat-admin@$ip "sudo service mysql restart; sudo service mysql status”
ssh heat-admin@$ip "sudo service stunnel4 restart; sudo service stunnel4 status”
ssh heat-admin@$ip "sudo netstat -napd | grep 3307 | grep LIST"
done</codeblock><p><b>Warning</b>:
            Do not proceed with the following steps unless the <codeph>netstat</codeph> command
            shows MySQL listening on port 3307. To verify that MySQL is now listening on port 3307
            enter:</p><codeblock><codeph>netstat -napd | grep 3307 | grep LIST
tcp        0      0 127.0.0.1:3307          0.0.0.0:*               LISTEN      32049/mysqld    
</codeph></codeblock><note>Nova's
              <codeph>service-list</codeph> will report that overcloud services are 'down'. At this
            stage, this is expected behavior. (You may need to wait for approximately 10 minutes to
            allow <codeph>os-apply-config</codeph> to complete running on each controller node
            before <codeph>nova service-list</codeph> returns this information.)</note></li>
        <li>Apply the following edit to the pre-flight_check.yml
          file:<codeblock><codeph>sed -i 's/status=started/state=started/g' ~/helion-update-1.1-to-1.1.1/tripleo/helion-update/tripleo-ansible/playbooks/pre-flight_check.yml
</codeph></codeblock></li>
        <li>Add the 61-sleep playbook by creating the 61-sleep file as
          follows:<codeblock><codeph>cat &gt; ~/helion-update-1.1-to-1.1.1/tripleo/helion-update/tripleo-ansible/playbooks/files/61-sleep &lt;&lt;EOF
#!/bin/bash
set -x
echo 'Short sleep'
sleep 20
date
EOF
</codeph></codeblock></li>
        <li>Update the <codeph>step_run_occ</codeph> playbook by
          entering:<codeblock>cp -p ~/helion-update-1.1-to-1.1.1/tripleo/helion-update/tripleo-ansible/playbooks/step_run_occ.yml ~/helion-update-1.1-to-/tripleo/helion-update/tripleo-ansible/playbooks/step_run_occ.yml.orig
cat &gt; ~/helion-update-1.1-to-1.1.1/tripleo/helion-update/tripleo-ansible/playbooks/step_run_occ.yml &lt;&lt; EOF
    # Copyright (C) 2014 Hewlett Packard Enterprise Development Company, L.P.
    #
    # Licensed under the Apache License, Version 2.0 (the "License");
    # you may not use this file except in compliance with the License.
    # You may obtain a copy of the License at
    #
    #    http://www.apache.org/licenses/LICENSE-2.0
    #
    # Unless required by applicable law or agreed to in writing, software
    # distributed under the License is distributed on an "AS IS" BASIS,
    # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either 
    # express or implied.
    # See the License for the specific language governing permissions and
    # limitations under the License.
    ---
    - name: Remove os-collect-config disable sentinel file
      file: path=/mnt/state/disable-os-collect-config state=absent
      sudo: yes
    - name: "Setting default fact to run os-collect-config"
      set_fact: test_bypass_os_collect_config="False"
    - name: "Evaluate if os-collect-config needs to be run"
      command: grep -q -i "Completed phase migration" /var/log/upstart/os-collect-config.log
      register: test_did_os_collect_config_complete
      ignore_errors: yes
      when: online_upgrade is not defined
    - name: "Setting fact to bypass os-collect-config if applicable"
      set_fact: test_bypass_os_collect_config="True"
      when: online_upgrade is not defined and test_did_os_collect_config_complete.rc == 0
    - name: "SLEEP20"
      copy:
        dest: /opt/stack/os-config-refresh/configure.d/61-sleep
        src: files/61-sleep
        owner: root
        group: root
        mode: 0755
    - name: "Execute os-collect-config"
      command: os-collect-config --force --one
      when: test_bypass_os_collect_config != true
EOF</codeblock></li>
        <li>To verify that the new file is correct,
          run:<codeblock><codeph>python -c 'import yaml; f=open("/root/helion-update-1.1-to-1.1.1/tripleo/helion-update/tripleo-ansible/playbooks/step_run_occ.yml", "r"); yaml.safe_load(f);'
echo $?
0
</codeph></codeblock></li>
        <li>Run the update
            script:<codeblock><codeph>source tripleo/tripleo-incubator/scripts/hp_ced_load_config.sh tripleo/configs/kvm-custom-ips.json
cd helion-update-1.1-to-1.1.1/tripleo/helion-update/overcloud_update
./update_oc.sh ~/helion-update-1.1-to-1.1.1/tripleo |&amp; tee update_oc.log
</codeph></codeblock><p>Your
            HPE Helion OpenStack overcloud is now updated.</p></li>
        <li>Confirm that update was successful by examining the
            <codeph>/var/log/ansible/ansible.log</codeph>. This report should not list any
          unreachable or failed nodes. That is <codeph>unreachable=0</codeph> and
            <codeph>failed=0</codeph> as shown in the example output
          below:<codeblock><codeph>PLAY RECAP ********************************************************************
10.23.67.141   : ok=122  changed=74   unreachable=0failed=0
10.23.67.143   : ok=37   changed=22   unreachable=0failed=0
10.23.67.144   : ok=122  changed=74   unreachable=0failed=0
10.23.67.145   : ok=103  changed=62   unreachable=0failed=0
10.23.67.146   : ok=37   changed=22   unreachable=0failed=0
10.23.67.147   : ok=60   changed=30   unreachable=0failed=0
10.23.67.148   : ok=60   changed=30   unreachable=0failed=0
</codeph></codeblock></li>
      </ol>
</section>
<section id="post-update-cleanup"> <title>Post update cleanup</title>
<p>After you have successfully updated the overcloud, there are some files that you should delete. Perform the following steps on all nodes in the overcloud (including controller nodes, Swift nodes, Nova compute nodes, etc.) To find and delete these files:</p>
<ol>
<li>Stop the <codeph>os-collect-config</codeph> service by
          entering:<codeblock><codeph>service os-collect-config stop
</codeph></codeblock></li>
<li>Find the files named <codeph>host_metadata.js*</codeph> by
          entering:<codeblock><codeph>find / -name host_metadata.json* -ls
</codeph></codeblock></li>
<li>Delete the files by
          entering:<codeblock><codeph>find / -name host_metadata.json* -delete
</codeph></codeblock></li>
<li>Verify that the files are all gone by
            entering:<codeblock><codeph>find / -name host_metadata.json* -ls
</codeph></codeblock><p>There
            should be no files found.</p></li>
<li>Restart the service by
          entering:<codeblock><codeph>service os-collect-config start
</codeph></codeblock></li>
</ol>
<!--
**Note**: Restart the Helion Development Platform VMs after updating the overcloud. For instructions on restarting Development Platform VMS, go to [Restarting Helion Development Platform control plane VMs](#HDPstart).
-->
<!-- replace with link to /helion/devplatform/1.2/upgrade/ -->
<!-- ###Updating the overcloud manually

This procedure allows you to update your overcloud installation from HPE Helion OpenStack v1.1 to Helion v1.1.1 by executing the update commands directly and not relying on the update scripts. (HPE Helion Development Platform users must shutdown before updating the overcloud. When the update is complete, the Development Platform VMs need to be restarted.) 

To update your overcloud:

1. Log on to the seed
 
        ssh root@<seed-ip>

2. Create the new Helion update directory, change to it, and download the update kit on the seed by entering:
 
        cd /root
        mkdir helion-update-1.1-to-1.1.1
        cd helion-update-1.1-to-1.1.1
        wget http://<url>/helion_ee_1.1.X.tgz

3. Untar the file by entering:

        tar -xvzf helion_ce_1.1.X.tgz

4. Log onto the seed and change to the downloaded directory bu entering:

        ssh root@<seed-ip>
        cd helion-update-1.1-to-1.1.1

5. Source the undercloud credentials by entering:

        export TRIPLEO_ROOT=~/tripleo
        export TE_DATAFILE=~/tripleo/ce_env.json
        source ~/tripleo/tripleo-incubator/undercloudrc

6. Download the image service (Glance) images locally by entering:
 
        OLD_BUILD_NO=$(glance image-show overcloud-compute-vmlinuz \
                       | grep "Property 'build_no'" | awk '{print $5};')
        mkdir build-$OLD_BUILD_NO
        cd build-$OLD_BUILD_NO
        for image in $(glance image-list | awk '{print $4};' | grep -);do
            glance image-download -??-file ./$image $image;
        done

7. Remove old images from Glance once saved locally by entering: 

        for image in $(glance image-list | awk '{print $4}' | grep -);do
            glance image-delete  $image;done

8. Upload new images to Glance by entering:

        BUILD_NO=$(cat /root/helion-update-1.1-to-1.1.1/tripleo/ce_env.json  | grep build_number | awk '{print $2}')
        /root/helion-update-1.1-to-1.1.1/tripleo/tripleo-incubator/scripts/load-image \
          -d /root/helion-update-1.1-to-1.1.1/tripleo/images/overcloud-compute-$BUILD_NO.qcow2
        /root/helion-update-1.1-to-1.1.1/tripleo/tripleo-incubator/scripts/load-image \
          -d /root/helion-update-1.1-to-1.1.1/tripleo/images/overcloud-control-$BUILD_NO.qcow2
        /root/helion-update-1.1-to-1.1.1/tripleo/tripleo-incubator/scripts/load-image \
          -d /root/helion-update-1.1-to-1.1.1/tripleo/images/overcloud-swift-$BUILD_NO.qcow2
        glance image-create -??-name bm-deploy-kernel -??-is-public True \
          -??-disk-format aki < /root/helion-update-1.1-to-1.1.1/tripleo/images/deploy-ramdisk-ironic.kernel
        glance image-create -??-name bm-deploy-ramdisk -??-is-public True \
          -??-disk-format ari < /root/helion-update-1.1-to-1.1.1/tripleo/images/deploy-ramdisk-ironic.initramfs

9. Update image names and set build metadata by entering:
  
        cd /root/helion-update-1.1-to-1.1.1/tripleo/tripleo-incubator/scripts/
        ./set-sherpa-metadata overcloud-compute-$BUILD_NO
        ./set-sherpa-metadata overcloud-control-$BUILD_NO
        ./set-sherpa-metadata overcloud-swift-$BUILD_NO

10. Update the TripleO Ansible playbook by entering:

        mv /opt/stack/triple-ansible /opt/stack/triple-ansible-1.1
        cp -r /root/helion-update-1.1-to-1.1.1/tripleo/helion-update/tripleo-ansible/ /opt/stack/

Refer to the Ansible update README for more details on running the play.

    source /opt/stack/venvs/ansible/bin/activate
    cd /opt/stack/tripleo-ansible/
    ./scripts/inject_nova_meta.bash
    ./scripts/populate_image_vars
    ansible-playbook -vvvv -M library/cloud -i plugins/inventory/heat.py -u heat-admin playbooks/pre-flight_check.yml
    ansible-playbook -vvvv -u heat-admin -i plugins/inventory/heat.py playbooks/update_cloud.yml
 -->
<!-- see Patrick's Development Platform update doc 


### Stopping HPE Helion Development Platform services<a name="HDPstop"></a> 
 
Before stopping HPE Helion Development Platform services, check that the HPE Helion Development Platform service control plane is in healthy state (this is required for Trove and Marketplace). 

To run the HPE Helion Development Platform scripts, perform the following steps:

1. Set `dev_plat_service` to the name of the service (Trove/Marketplace).
1. Set `skip_ha_checks` if the service control plane is not running in HA mode. 
1. Provide path to the SSH private key that can be used to connect to the service control plane VMs.
1. Source credentials for the Nova account which was used to set up the HPD service by entering:

        cd /root/helion-update-1.1-to-1.1.1/tripleo/helion-update/tripleo-ansible 
        ansible-playbook -??-extra-vars dev_plat_service=<service> -??-private-key <path to SSH private key> -??-skip_ha_checks=True -u heat-admin -i plugins/inventory/dev_platform_heat.py playbooks/dev-platform/dev_platform_pre_check.yml 
   


1. If service is in healthy state, stop it by entering (if the service is not in a healthy state, contact customer support): 
  
        ansible-playbook -??-extra-vars dev_plat_service=<service> -??-private-key <path to SSH private key> -u heat-admin -i plugins/inventory/dev_platform_heat.py 
        playbooks/dev-platform/dev_platform_stop.yml
 
1. Stop the Helion Development Platform control plane VMs by entering:

        scripts/dev-platform/stop_dev_platform_instances.bash 
   
You can now run the update script.

### Restarting HPE Helion Development Platform control plane VMs<a name="HDPstart"></a> 


After running update on the overcloud, the VMs running on the Compute hosts are in a shutdown state, so you need to first start the HPE Helion Development Platform service control plane VMs. To do this: 


1. Source the credentials for the Nova account which was used to set up the HPD service by entering: 

            cd /root/helion-update-1.1-to-1.1.1/tripleo/helion-update/tripleo-ansible 
            scripts/dev-platform/restart_dev_platform_instances.bash 
   

1. Since you have already sourced correct overcloud Nova credentials, you can simply verify the status of HPD service VMs by running the nova list command and checking the status field. 



1. To start the HPE Helion Development Platform service, enter:

        ansible-playbook -??-extra-vars dev_plat_service=<service> -??-private-key <path to SSH private key> -u heat-admin -i plugins/inventory/dev_platform_heat.py playbooks/dev-platform/dev_platform_start.yml 

 -->
</section>
<section id="updating-the-hp-version-type"> <title>Updating the HP-version type</title>
<p>It is a good practice to copy the <codeph>HP_Helion_Version</codeph> type to the Tripleo directory so that it reflects the version that was updated. This lets other administrators know that the environment has been updated.</p>
<p>To replace the version file with the latest,  on the seed VM, enter:</p>
<codeblock><codeph>cp ~/helion-update-1.1-to-1.1.1/tripleo/HP_Helion_Version ~/tripleo/
</codeph></codeblock>
</section>
<section id="validating-the-update"> <title>Validating the update</title>
<p>This section explains how you should validate your update.</p>
</section>
<section id="validating-the-overcloud-controller-update"> <title>Validating the overcloud controller update</title>
<p>After the update, the overcloud controller should be back online without any errors. You should
        be able to SSH to the Nova compute node from the
        seed.<!--A BR tag was used here in the original source.--> You should be able to open the
        Horizon dashboard as well as execute lifecycle operations with any instance already deployed
        in the overcloud.</p>
<p>The node should re-join the MySQL Cluster,<!--A BR tag was used here in the original source.-->
HAProxy should show the servers as online, the<!--A BR tag was used here in the original source.-->
instances deployed before the update should still be up and accessible. It should be possible to deploy new instances and ping them and SSH to them.</p>
<p>Icinga should show that all the monitors are green for the update node. To check Icinga, go to the <b>Icinga Dashboard</b> (http://&lt;Undercloud_IP&gt;/icinga/). 
<!-- modified the following link. 
[http://<undercloudip>/icinga/](http://undercloudip/icinga)-->Log in with the credential: icingaadmin/icingaadmin</p>
<p>Do the following:</p>
<ol>
<li>Enable the HAProxy status page and check if the all the services from the node are green (meaning they are up/running). </li>
<li>SSH to the overcloud Management controller.</li>
<li>Edit the file <codeph>/etc/haproxy/haproxy.cfg</codeph> to add at the end of the file <codeph>mode http</codeph>. (Make this edit in the listen <codeph>haproxy.stats</codeph>:1993 section.)</li>
<li>Restart HAProxy (<codeph>service haproxy restart</codeph>).</li>
<li>Open the status page and check all the services. <!-- ([http://managementcontroller](http://managementcontroller.ip:1993).) --></li>
<li>Make sure that the following commands are working after the update:<ul>
            <li>keystone user-list</li>
            <li>nova list </li>
            <li>neutron net-list </li>
            <li>glance image-list </li>
            <li>os-refresh-config</li>
            <li>os-collect-config </li>
          </ul></li>
<li>Use <codeph>ifconfig</codeph> to check that all your networks still exist after the update.</li>
<li>Use <codeph>ovs-vsctl show</codeph> to check that all bridges and ports still exist after the update.   </li>
</ol>
</section>
<section id="validating-the-compute-node"> <title>Validating the compute node</title>
<p>After an update, verify that instances deployed before the update are still up and accessible.
        You should be able to SSH to the Nova Compute node from the seed. You should be able to ping
        and SSH to new instances.</p>
<p>The <codeph>os-refresh-config</codeph> command needs to be working  after  an update.</p>
<p>Use <codeph>ifconfig</codeph> to check that all your networks still exist after the update.<!--A BR tag was used here in the original source.-->
Use <codeph>ovs-vsctl show</codeph> to check that all bridges and ports still exist after the update.</p>
</section>
<section id="validating-swift"> <title>Validating Swift</title>
<p>After the update:</p>
<ul>
<li>Check that the images are still there.</li>
<li>Confirm that you can SSH to the Nova compute node from seed.</li>
<li>Verify that you can load new images.</li>
<li>Check that you can deploy new instances with a new image.</li>
</ul>
<p>The <codeph>os-refresh-config</codeph> command needs to be working  after  an update.</p>
</section>
<section id="validating-virtual-storage-appliances"> <title>Validating Virtual Storage Appliances</title>
<p>After an update:</p>
<ul>
<li>Check if the node can join the cluster (CMC).</li>
<li>Verify that you can SSH to the Nova compute node from the seed.</li>
<li>Check that the volumes are still present.</li>
<li>Verify that you can create new volumes.</li>
<li>Verify that you can delete old volumes.</li>
<li>Verify that you can  attach/detach volumes.</li>
<li>Check that the version of the Glance image used to update is the same by SSHing to the
          undercloud node and run: <ul id="ul_cg3_wc5_ts">
            <li>
              <codeph>glance image-list</codeph>
            </li>
            <li>
              <codeph>nova list</codeph>
            </li>
            <li>
              <codeph>nova show &lt;instance id&gt;</codeph>
            </li>
          </ul></li>
</ul>
<p>Compare the Image ID from <codeph>glance image-list</codeph> with 
<codeph>nova show</codeph>.</p>
</section>
<section id="troubleshooting-an-update"> <title>Troubleshooting an update</title>
<p>This section explains how to fix common problems  that might arise when performing an update to Helion.</p>
</section>
<section id="backup-fails"> <title>Backup fails</title>
<p>If you see the error message:</p>
<codeblock><codeph>+ echo 'ERROR: Backup of seed failed!!!!'
ERROR: Backup of seed failed!!!!
+ exit 1
</codeph></codeblock>
<p>Your backup attempt has failed. To recover from this state, you need to get <codeph>rabbitmq</codeph>, <codeph>mysql</codeph> and <codeph>OpenStack</codeph> services running again by running the seed recovery script. to do this, enter:</p>
<codeblock><codeph>ssh root@&lt;seed-ip&gt;
cd helion-update-1.1-to-1.1.1/tripleo/helion-update/seed_update
./seed_recover.sh
</codeph></codeblock>
</section>
<section id="backup-failed-due-to-seed-host-running-out-of-disk-space"> <title>Backup failed due to seed running out of disk space</title>
<p>If your backup fails because the disk lacks sufficient space you will note that the backup
        sequence froze or failed. You will also see that the disk space utilization check of the
        seed returns 100%. You can check disk utilization by entering:</p>
<codeblock><codeph>df -h /
</codeph></codeblock>
<p>To fix this problem:</p>
<ol>
<li>Remove the failed backup from the <codeph>/tmp</codeph> folder by
          entering:<codeblock><codeph>rm -r -f /tmp/backp_root
</codeph></codeblock></li>
<li>Restart MySQL and RabbitMQ by
          entering:<codeblock><codeph>service mysql restart
service rabbitmq-server restart
</codeph></codeblock></li>
<li>Run <codeph>os-collect-config</codeph> by
            entering:<codeblock><codeph>servie os-collect-config stop
os-collect-config --force
service os-collect-config start
</codeph></codeblock><p>If
            this command returns an error, RabbitMQ may be in a bad state as a result of running out
            of disk space. If executing <codeph>os-collect-config</codeph> errors with RabbitMQ,
            enter:</p><codeblock><codeph>rabbitmqctl stop_app
rabbitmqctl reset
service rabbitmq-server stop
</codeph></codeblock></li>
<li>Once RabbitMQ has been reset, re-attempt step 3 which will re-configure RabbitMQ and restart
          services. If the <codeph>rabbitmqctl reset</codeph> command does not work, use the
            <codeph>rabbitmqctl force_reset</codeph> command.</li>
</ol>
</section>
<section id="seed-host-update-fails-noting-unable-to-ping-192021"> <title>Seed update fails noting unable to ping 192.0.2.1</title>
<p>If your seed update fails, your deployment will NOT utilize the 192.0.2.0/24 demo IP network
        range. You will see in your log:</p>
<codeblock><codeph>"Waiting for seed host to configure"
</codeph></codeblock>
<p>Pings to host 192.0.2.1 will time out with the following message:</p>
<codeblock><codeph>Timing out after 10 seconds:
COMMAND=ping -c 1 192.0.2.1
OUTPUT=PING 192.0.2.1 (192.0.2.1) 56(84) bytes of data.
</codeph></codeblock>
<p>The likely reason for this failure is the installer update of the seed image has failed as the
        configuration that was used during the install was not available or passed on to the
        installer to perform the seed update operation. As a result, the installer has indicated
        that it has failed, although the seed has likely rebooted without issue.</p>
<p>To fix this problem:</p>
<ol>
<li>Identify the following:<ul>
            <li>Expected IP address of the seed.</li>
            <li>Location of the backup folder. It should be at
                <codeph>/root/helion-update-1.1-to-1.1.1/backup*</codeph>. For
                example:<codeph>/root/helion-update-1.1-to-1.1.74/backup-0/</codeph>
            </li>
          </ul></li>
<li>Verify that you can ping the IP address of the seed by
          entering:<codeblock><codeph>ping -c 1 &lt;seed IP address&gt;
</codeph></codeblock></li>
<li>Run the restore script directly by
            entering:<codeblock><codeph>/root/helion-update-1.1-to-1.1.1/helion-update/seed_update/seed_update.sh --restore-seed &lt;backup directory&gt; --ip-address &lt;seed IP address&gt;
</codeph></codeblock><p>For
            example:</p><codeblock><codeph>/root/helion-update-1.1-to-1.1.1/helion-update/seed_update/seed_update.sh --restore-seed /root/helion-update-1.1-to-1.1.1/backup-0/ --ip-address 192.2.0.1
</codeph></codeblock></li>
<li>The restoration command will take some time, but once completed you should be able to log in to
          the seed. To verify that the node is in working status,
          enter:<codeblock><codeph>source /root/stackrc &amp;&amp;
nova list &amp;&amp;
glance image-list &amp;&amp;
heat stack-list &amp;&amp;
neutron net-list
</codeph></codeblock></li>
</ol>
</section>
<section id="retrying-failed-actions"> <title>Retrying failed actions</title>
<p>In some cases, steps may fail because some components may still be initializing and not yet be ready for
use.
In this event, you have two options: to re-attempt or resume playbook executions.</p>
<ol>
<li>Use the Ansible <codeph>ansible-playbook</codeph> command with the  <codeph>--start-at-task="TASK NAME"</codeph> option. This command allows resumption of a playbook, when used with the <codeph>-l limit</codeph> option.</li>
<li>Use the Ansible <codeph>ansible-playbook</codeph> command with the <codeph>--step</codeph> option. This command allows you to confirm  each task before it is executed by Ansible.</li>
</ol>
</section>
<section id="a-node-goes-to-error-state-during-rebuild"> <title>A node goes to ERROR state during rebuild</title>
<p>A node can go into an error state due to network errors or a temporary overload of the undercloud. 
This can happen from time to time due to network errors or a temporary overload of the undercloud. In this case, the <codeph>nova list</codeph> command returns node in ERROR. To fix this:</p>
<ul>
<li>Make sure your hardware is in working order.</li>
<li>Verify that approximately 20% of the disk space on the Ironic server node is free.</li>
<li>
<p>Get the image ID of the machine in question using <codeph>nova show</codeph>
</p>

<codeblock><codeph>nova show $node_id
</codeph></codeblock>
</li>
<li>
<p>Manually rebuild by running:</p>

<codeblock><codeph>nova rebuild --preserve-ephemeral $node_id $image_id
</codeph></codeblock>
</li>
</ul>
</section>
<section id="a-node-times-out-after-rebuild"> <title>A node times out after rebuild</title>
<p>While rare, there is the possibility that something unexpected happened during a rebuild
and the host has failed to reboot. When this happens, you will get this error Message:</p>
<codeblock><codeph>msg: Timeout waiting for the server to come up.. Please check manually
</codeph></codeblock>
<p>To fix this problem, follow the steps detailed in: "A node goes to ERROR state during rebuild".</p>
</section>
<section id="mysql-cli-configuration-file-is-missing"> <title>MySQL CLI configuration file is missing</title>
<p>Should the post-rebuild restart fail, the cause might be that the MySQL CLI configuration file is missing. If you have this issue, attempts to access the MySQL CLI command return:</p>
<codeblock><codeph>ERROR 1045 (28000): Access denied for user 'root'@'localhost' (using password: NO)
</codeph></codeblock>
<p>To fix this problem:</p>
<ul>
<li>
          <p>Verify that the MySQL CLI config file stored on the state drive is present and has
            content. To display the contents, run:</p>
          <codeblock><codeph>sudo cat /mnt/state/root/metadata.my.cnf
</codeph></codeblock>
        </li>
<li>
<p>If the file is empty, retrieve current metadata and update  the config files on disk by running:</p>

<codeblock><codeph>sudo os-collect-config --force --one --command=os-apply-config
</codeph></codeblock>
</li>
<li>
<p>Verify that the MySQL CLI config file is present in the root user directory by running:</p>

<codeblock><codeph>sudo cat /root/.my.cnf
</codeph></codeblock>
</li>
<li>
<p>If that file does not exist, or is empty, you have two options.</p>

<ol>
<li>Add the following to your MySQL CLI command
              line:<codeblock><codeph>--defaults-extra-file=/mnt/state/root/metadata.my.cnf
</codeph></codeblock></li>
<li>Or copy the configuration from the state drive by
              entering:<codeblock><codeph>sudo cp -f /mnt/state/root/metadata.my.cnf /root/.my.cnf
</codeph></codeblock></li>
</ol>
</li>
</ul>
</section>
<section id="mysql-fails-to-start-after-retrying-the-update"> <title>MySQL fails to start after retrying the update</title>
<p>If the update was aborted or failed during the Update sequence before a single MySQL controller was operational, MySQL will fail to start upon retrying. In this case, you will see the following error messages:</p>
<codeblock><codeph>* `msg: Starting MySQL (Percona XtraDB Cluster) database server: mysqld . . . . The server quit without updating PID file (/var/run/mysqld/mysqld.pid)`

* `stderr: ERROR 2002 (HY000): Can't connect to local MySQL server through socket '/var/run/mysqld/mysqld.sock' (111)`

* `FATAL: all hosts have already failed  - aborting`

* Update automatically aborts.
</codeph></codeblock>
<p>
  <b>WARNING:</b>
</p>
<p>The command <codeph>/etc/init.d/mysql bootstrap-pxc</codeph>,  mentioned below, 
      should only ever be executed when an entire MySQL cluster is down, and
      then only on the last node to have been shut down.  Running this command
      on multiple nodes will cause the MySQL cluster to enter a split brain
      scenario effectively breaking the cluster which will result in
      unpredictable behavior.</p>
<p>To fix this problem:</p>
<ul>
<li>
<p>Verify that NTP is working.</p>
</li>
<li>
<p>Use the <codeph>nova list</codeph> command to determine the IP address of the controller0 node, then SSH into it by entering:</p>

<codeblock><codeph>ssh heat-admin@$IP
</codeph></codeblock>
</li>
<li>
<p>Verify that MySQL is down by running (as root) the <codeph>mysql</codeph> client. It should fail:</p>

<codeblock><codeph>sudo mysql -e "SELECT 1"
</codeph></codeblock>
</li>
<li>
<p>Attempt to restart MySQL in case another cluster node is online. This should fail in this error state. However, if it succeeds your cluster should again be operational and you can skip the next step:</p>

<codeblock><codeph>sudo /etc/init.d/mysql start
</codeph></codeblock>
</li>
<li>
<p>Start MySQL in single node bootstrap mode by entering:</p>

<codeblock><codeph>sudo /etc/init.d/mysql bootstrap-pxc
</codeph></codeblock>
</li>
</ul>
</section>
<section id="mysqlperconagalera-is-out-of-sync"> <title>MySQL/Percona/Galera is out of sync</title>
<p>OpenStack is configured to store all of its states in a multi-node, synchronous replication Percona XtraDB Cluster database, which uses Galera for replication. This database must be in sync and have the full
complement of servers before updates can be performed safely.</p>
<p>The problem is update fails with errors about Galera and/or MySQL being <codeph>Out of Sync</codeph>.</p>
<p>To fix this issue:</p>
<ul>
<li>
<p>Use the  <codeph>nova list</codeph> command to determine the IP address of controller0 node, then SSH to it by entering:</p>

<codeblock><codeph>ssh heat-admin@$IP
</codeph></codeblock>
</li>
<li>
<p>Verify that replication is out of sync by entering:</p>

<codeblock><codeph>sudo mysql -e "SHOW STATUS like 'wsrep_%'"
</codeph></codeblock>
</li>
<li>
<p>Stop MySQL by entering:</p>

<codeblock><codeph>sudo /etc/init.d/mysql stop
</codeph></codeblock>
</li>
<li>
<p>Verify that MySQL is down by running (as root) the <codeph>mysql</codeph> client. It should fail:</p>

<codeblock><codeph>sudo mysql -e "SELECT 1"
</codeph></codeblock>
</li>
<li>
<p>Start controller0 MySQL in single-node bootstrap mode by entering:</p>

<codeblock><codeph>sudo /etc/init.d/mysql bootstrap-pxc
</codeph></codeblock>
</li>
<li>
<p>On the remaining controller nodes observed to be having issues, get their IP addresses using the  <codeph>nova list</codeph>  command and log in to them by entering:</p>

<codeblock><codeph>ssh heat-admin@$IP
</codeph></codeblock>
</li>
<li>
<p>Verify that replication is out of sync by entering:</p>

<codeblock><codeph>sudo mysql -e "SHOW STATUS like 'wsrep_%'"
</codeph></codeblock>
</li>
<li>
<p>Stop MySQL by entering:</p>

<codeblock><codeph>sudo /etc/init.d/mysql stop
</codeph></codeblock>
</li>
<li>
<p>Verify that MySQL is down by running (as root) the <codeph>mysql</codeph> client. It should fail:</p>

<codeblock><codeph>sudo mysql -e "SELECT 1"
</codeph></codeblock>
</li>
<li>
<p>Start MySQL so it attempts to connect to controller0 by entering:</p>

<codeblock><codeph>sudo /etc/init.d/mysql start
</codeph></codeblock>
</li>
</ul>
<p>If restarting MySQL fails, then the database is most certainly out of sync and the MySQL error logs, located at <codeph>/var/log/mysql/error.log</codeph>, will need to be checked.  In this case, never attempt to restart MySQL with <codeph>sudo /etc/init.d/mysql bootstrap-pxc</codeph> as it will bootstrap the host as a single-node cluster thus worsening what already appears to be a split-brain scenario. If you need help with this matter, contact HPE support.</p>
</section>
<section id="mysql-node-appears-to-be-the-last-node-in-a-cluster-error"> <title>MysQL "Node appears to be the last node in a cluster" error</title>
<p>This error occurs when one of the controller nodes does not have MySQL running.
The playbook has detected that the current node is the last running node,
although based on its sequence, it should not be the last node.  As a result the
error is thrown and update is aborted.</p>
<p>The full message is:</p>
<codeblock><codeph>Galera Replication. Node appears to be the last node in a cluster; cannot safely proceed unless overridden via `single_controller`setting. See README.rst
</codeph></codeblock>
<p>To fix this problem:</p>
<ul>
<li>
<p>Run the <codeph>pre-flight_check.yml</codeph> playbook.  It will attempt to restart MySQL on each node in the <codeph>Ensuring MySQL is running</codeph> step.  If that step succeeds, you should be able to re-run the playbook and not encounter this <codeph>Node appears to be last node in a cluster</codeph> error.</p>
</li>
<li>
<p>If pre-flight_check fails to restart MySQL, review the MySQL logs (<codeph>/var/log/mysql/error.log</codeph>) to determine why the other nodes are not restarting.</p>
</li>
</ul>
</section>
<section id="ssh-connectivity-is-lost"> <title>SSH connectivity is lost</title>
<p>Ansible uses SSH to communicate with remote nodes. In heavily loaded, single
host virtualized environments, SSH can lose connectivity.  It should be noted
that similar issues in a physical environment may indicate issues in the
underlying network infrastructure.</p>
<p>When Ansible loses SSH connectivity causing an update attempt to fail, you will see the following output:</p>
<codeblock><codeph>fatal: [192.0.2.25] =&gt; SSH encountered an unknown error. The
output was: OpenSSH_6.6.1, OpenSSL 1.0.1i-dev xx XXX xxxx
debug1: Reading configuration data /etc/ssh/ssh_config debug1:
/etc/ssh/ssh_config line 19: Applying options for * debug1:
auto-mux: Trying existing master debug2: fd 3 setting
O_NONBLOCK mux_client_hello_exchange: write packet: Broken
pipe FATAL: all hosts have already failed  - aborting
</codeph></codeblock>
<p>To fix this problem, you can generally re-run the playbook to complete the upgrade, unless SSH connectivity is lost while all MySQL nodes are down. (See 'MySQL fails to start upon retrying update' to correct this issue.)</p>
<p>Early Ubuntu Trusty kernel versions have known issues with KVM which severely impact SSH connectivity to instances. To avoid this issue, Test hosts should have a minimum kernel version of 3.13.0-36-generic.</p>
<p>The update steps, running as root, are:</p>
<codeblock><codeph>apt-get update
apt-get dist-upgrade
reboot
</codeph></codeblock>
<p>If you continue to encounter this issue in a physical environment, check the network infrastructure for errors.</p>
<p>Similar error messages may occur with long running processes, such as database creation/upgrade steps.  These cases will generally have partial program execution log output immediately before the broken pipe message visible. Should this be the case, Ansible and OpenSSH may need to have their configuration files tuned to meet the needs of the environment.</p>
<p>Consult the Ansible configuration file to see available connection settings ssh_args, timeout, and possibly pipelining. To see this file, enter:</p>
<codeblock><codeph>https://github.com/ansible/ansible/blob/release1.7.0/examples/ansible.cfg
</codeph></codeblock>
<p>As Ansible uses OpenSSH, consult the <codeph>ssh_config</codeph> manual, in paricular the <codeph>ServerAliveInterval</codeph> and <codeph>ServerAliveCountMax</codeph> options.</p>
</section>
<section id="postfix-fails-to-reload"> <title>Postfix fails to reload</title>
<p>Occasionally the postfix mail transfer agent will fail to reload because it is not running when the system expects it to be running.</p>
<p>Confirm that this is the case by examining  the <codeph>/var/log/upstart/os-collect-config.log</codeph> for an indication that <codeph>service postfix reload</codeph> failed.</p>
<p>To fix this issue, start postfix by entering:</p>
<codeblock><codeph>sudo service postfix start
</codeph></codeblock>
</section>
<section id="ephemeral-certificates-location"> <title>Ephemeral certificates location</title>
<!-- DOCS-1101, DOCS-1257 -->
<p>The default ephemeral certificate location is <codeph>/root</codeph>. This works for normal Helion installations. If you have specified another location, when you update Helion, you must specify the certificate location as <codeph>/root</codeph>. For example, the correct environment variable location and file name is:</p>
<codeblock><codeph>EPHEMERAL_CA_KEY_FILE=/root/eca.key
EPHEMERAL_CA_CERT_FILE=/root/eca.crt
</codeph></codeblock>
</section>
<section id="apache2-fails-to-start"> <title>Apache2 fails to start</title>
<p>Apache2 requires several self-signed SSL certificates to be properly configured but because of earlier failures in the setup process these certificaties may not have been configured correctly. If this is the case, you will see the following error message:</p>
<codeblock><codeph>* failed: [192.0.2.25] =&gt; (item=apache2) =&gt; {"failed": true, "item": "apache2"}
* msg: start: Job failed to start
</codeph></codeblock>
<p>You will also note the following symptoms:</p>
<ul>
<li>the Apache2 service fails to start</li>
<li>the <codeph>/etc/ssl/certs/ssl-cert-snakeoil.pem</codeph> file is missing or empty.</li>
</ul>
<p>To fix this problem, re-run <codeph>os-collect-config</codeph> to reassert the SSL certificates by entering:</p>
<codeblock><codeph>sudo os-collect-config --force --one
</codeph></codeblock>
</section>
<section id="rabbitmq-still-running-when-restart-is-attempted"> <title>RabbitMQ still running when restart is attempted</title>
<p>There are certain system states that cause RabbitMQ to ignore normal kill signals. In these cases, RabbitMQ continues to run. You will notice that you have this issue when your attempts to start <codeph>rabbitmq</codeph> fail because it is already running.</p>
<p>To fix this problem, find any processes running as <codeph>rabbitmq</codeph> on the server, and kill them, forcibly if need be.</p>
</section>
<section id="instance-reported-with-status-as-shutoff-and-task-state-as-powering-on"> <title>Instance reported with status as "SHUTOFF" and task_state as "powering on"</title>
<p>When Nova Compute attempts to restart an instance when the Compute node is not ready, it is possible that Nova can enter a confused state where it thinks that an instance is starting when in fact the Compute node is doing nothing. You have reason to suspect that this is the case when:</p>
<ul>
<li>The command <codeph>nova list --all-tenants</codeph> reports instance(s) with STATUS of "SHUTOFF" and <codeph>task_state</codeph> is "powering on".</li>
<li>The instance does not respond to pings.</li>
<li>No instance appears to be running on the Compute node.</li>
<li>Nova hangs when retrieving logs or returns old logs from the previous boot.</li>
<li>A console session cannot be established.</li>
</ul>
<p>To fix this problem:</p>
<ul>
<li>
<p>Log in to a controller as root and enter:</p>

<p>
<codeph>source stackrc</codeph>
</p>
</li>
<li>
<p>Execute <codeph>nova list --all-tenants</codeph> to obtain instance ID(s)</p>
</li>
<li>Execute <codeph>nova show &lt;instance-id&gt;</codeph> on each suspected ID to identify suspected Compute nodes.</li>
<li>Log into the suspected Compute node(s) and execute:
 <codeph>os-collect-config --force --one</codeph>
</li>
<li>
          <p>Return to the controller node that you were logged into previously, and using the
            instancce IDs obtained previously, take the following steps:</p>
          <p>
            <ol id="ol_bzk_t25_ps">
              <li>Execute <codeph>nova reset-state --active &lt;instance-id&gt;</codeph>
              </li>
              <li>Execute <codeph>nova stop &lt;instance-id&gt;</codeph>
              </li>
              <li>Execute <codeph>nova start &lt;instance-id&gt;</codeph>
              </li>
            </ol>
          </p>
        </li>
        <li>
          <p>Once the above steps have been taken in order, you should see the instance status
            return to ACTIVE and the instance become accessible via the network.</p>
        </li>
</ul>
</section>
<section id="state-drive-mnt-is-not-mounted"> <title>State drive <codeph>/mnt</codeph> is not mounted</title>
<p>In the rare event that an error occurred between the state drive being unmounted and the rebuild command being triggered, the <codeph>/mnt</codeph> volume on the instance upon which the rebuild command was executed will be in an unmounted state.</p>
<p>In this state, you will not be able to start MySQL and RabbitMQ. You will likely see these (pre-flight check) error messages:</p>
<codeblock><codeph>failed: [192.0.2.24] =&gt; {"changed": true, "cmd":
"rabbitmqctl -n rabbit@$(hostname) status" stderr: Error:
unable to connect to node
'rabbit@overcloud-controller0-vahypr34iy2x': nodedown
</codeph></codeblock>
<p>Attempts to start MySQL or RabbitMQ  manually will fail and you will see:</p>
<codeblock><codeph>start: Job failed to start
</codeph></codeblock>
<p>Upgrade attempts return with an error indicating:</p>
<codeblock><codeph>TASK: [fail msg="Galera Replication, Node appears to be the last node in a cluster;  cannot safely proceed unless overridden via `single_controller` setting. See README.rst"]
</codeph></codeblock>
<p>If you run the <codeph>df</codeph> command, the return does not show a volume mounted as <codeph>/mnt</codeph>.</p>
<p>To fix this problem:</p>
<ul>
<li>
<p>Execute the <codeph>os-collect</codeph> config which will re-mount the state drive. This command may fail without additional intervention. However it should mount the state drive which is all that is needed to proceed to the next step. To run <codeph>os-collect-config</codeph>, enter:</p>

<codeblock><codeph>sudo os-collect-config --force --one
</codeph></codeblock>
</li>
<li>
<p>At this point, the <codeph>/mnt</codeph> volume should be visible in the output of the <codeph>df</codeph> command.</p>
</li>
<li>
<p>Start MySQL by entering:</p>

<codeblock><codeph>sudo /etc/init.d/mysqld start
</codeph></codeblock>
</li>
<li>
<p>If MySQL fails to start, and it has been verified that MySQL is not running on any controller nodes, then you will need to identify the Last node that MySQL was stopped on and consult the section "MySQL fails to start upon retrying update" for guidance on restarting the cluster.</p>
</li>
<li>
<p>Start RabbitMQ by entering:</p>

<codeblock><codeph>service rabbitmq-server start
</codeph></codeblock>
</li>
<li>
<p>If <codeph>rabbitmq-server</codeph> fails to start, then the cluster may be down. If this is the case, then the last node to be stopped will need to be identified and started before attempting to restart RabbitMQ on this node.</p>
</li>
<li>At this point, re-execute the pre-flight check, and proceed.</li>
</ul>
</section>
<section id="vms-do-not-shut-down-properly-during-upgrade"> <title>VMs do not shut down properly during upgrade</title>
<p>During the upgrade process, VMs on Compute nodes are shut down gracefully. If the VMs do not shut down, this can cause the upgrade to stop. If this is the case, you will see a playbook run which ends with a message similar to:</p>
<codeblock><codeph>failed: [10.23.210.31] =&gt; {"failed": true}} msg: The ephemeral
storage of this system failed to be cleaned up properly and
processes or files are still in use. The previous ansible play
should have information to help troubleshoot this issue.
</codeph></codeblock>
<p>The output of the playbook run prior to this message contains a process listing and a listing of open files.</p>
<p>The state drive on the Compute node, <codeph>/mnt</codeph>, is still in use and cannot be unmounted. You can confirm this by entering:</p>
<codeblock><codeph>lsof -n | grep /mnt
</codeph></codeblock>
<p>To see which VMs are running, enter:</p>
<codeblock><codeph>virsh list
</codeph></codeblock>
<p>If <codeph>virsh list</codeph> fails, you may need to restart <codeph>libvirt-bin</codeph> or <codeph>libvirtd</codeph> depending on which process you are running. To restart, enter:</p>
<codeblock><codeph>service libvirt-bin restart
</codeph></codeblock>
<p>or</p>
<codeblock><codeph>service libvirtd restart
</codeph></codeblock>
<p>To fix this problem, you will have to intervene manually. You will need to determine why the VMs did not shut down properly, and resolve the issue.</p>
<p>You can forcely shutdown non-responsive VMs by entering:</p>
<codeblock><codeph>virsh destroy &lt;id&gt;
</codeph></codeblock>
<p>Note that this can corrupt filesystems on the VM.</p>
<p>Resume the playbook run once the VMs have been shut down.</p>
</section>
<section id="instances-are-inaccessible-via-network"> <title>Instances are inaccessible via network</title>
<p>Upon restarting, it is possible that the virtual machine is unreachable due to Open vSwitch not being ready for the virtual machine networking. If this is the case, you will not be able to ping instances after a restart.</p>
<p>To fix this problem:</p>
<ul>
<li>
<p>Log into a controller node and execute:</p>

<codeblock><codeph>source /root/stackrc
</codeph></codeblock>
</li>
<li>
<p>Stop all virtual machines on a Compute node by entering:</p>

<codeblock><codeph>nova hypervisor-servers &lt;hostname&gt; 
</codeph></codeblock>

<p>and</p>

<codeblock><codeph>nova stop &lt;id&gt;
</codeph></codeblock>
</li>
<li>
<p>Log into the undercloud node and enter:</p>

<codeblock><codeph>source /root/stackrc
</codeph></codeblock>
</li>
<li>
<p>Obtain a list of nodes by entering:</p>

<codeblock><codeph>nova list
</codeph></codeblock>
</li>
<li>
<p>Execute <codeph>nova stop &lt;id&gt;</codeph> for the affected Compute node.</p>
</li>
<li>
<p>Once the compute node has stopped, execute <codeph>nova start &lt;id&gt;</codeph> to reboot the Compute node.</p>
</li>
</ul>
</section>
<section id="online-upgrade-fails-with-message-saying-glanceclient-is-not-found"> <title>Online upgrade fails with message saying glanceclient is not found</title>
<p>This problem occurs when you attempt to perform an online upgrade, However the playbook execution failed when you attempted to download the new image from Glance. You get a message that <codeph>glanceclient</codeph> was not found.</p>
<p>If you are attempting to execute the Ansible playbook on the seed or undercloud node, source the
        Ansible virtual environment by entering:</p>
<codeblock><codeph>source /opt/stack/venvs/ansible/bin/activate
</codeph></codeblock>
<p>Once the Ansible virtual environment has been sourced, on the node from which you are attempting to execute Ansible, enter:</p>
<codeblock><codeph>sudo pip install python-glanceclient 
</codeph></codeblock>
</section>
<section id="online-upgrade-of-compute-node-failed"> <title>Online upgrade of Compute node failed</title>
<p>In the event that an online upgrade of a Compute node fails, you can recover the node utilizing a traditional rebuild.</p>
<p>The problem occurs when you perform an online upgrade. The result of which is that a Compute node cannot be logged into, or is otherwise in a non-working state.</p>
<p>To fix this issue, from the undercloud enter:</p>
<codeblock><codeph>source /root/stackrc
</codeph></codeblock>
<p>Identify the instance ID of the broken Compute node using the <codeph>nova list</codeph> command.
Then stop the instance by entering:</p>
<codeblock><codeph>nova stop &lt;instance-id&gt; 
</codeph></codeblock>
<p>Return to the host from which you ran the upgrade and re-run the playbook without the <codeph>-e online_upgrade=True</codeph> option.</p>
<p>You can also utilize the <codeph>-e force_rebuild=True</codeph>  option to force the instance to rebuild.</p>
</section>
<section id="ironic-fails-because-nodes-are-in-maintenance-mode"> <title>Ironic fails because nodes are in maintenance mode</title>
<p>During an update, nodes must NOT be in maintenance mode; otherwise Ironic returns an error message such as the following:</p>
<codeblock><codeph>During sync_power_state, max retries exceeded for node 0677d7e8-2e2b-4b12-b426-4b2950d7a5f2, node state None does not match expected state 'power on'. Updating DB state to 'None' Switching node to maintenance mode.
</codeph></codeblock>
<p>If the command <codeph>ironic node-set-maintenance</codeph> fails to properly change nodes from maintenance mode, you must either update the database directly using:</p>
<codeblock><codeph>mysql&gt; update nodes set maintenance=0;
</codeph></codeblock>
<p>or enter:</p>
<codeblock><codeph>ironic node-update &lt;ironic node-id&gt; replace maintenance=False.
</codeph></codeblock>
</section>
<section id="envvarsignored"> <title>Environment variables being ignored</title>
<p>When running the <codeph>update_sd.sh</codeph> script to upgrade from HPE Helion OpenStack v1.1 to v1.1.1,  the default 192.0.6.0/24 network is used instead of the networks that are defined by the <codeph>kvm-custom-ips.json</codeph> file.
The reason this happens is the person doing the update neglected to set the ENV variables before running the update command. These ENV variables were set when the system was installed and the same ENV variable set (with the same values) must be used for the update.</p>
<p>The following is an example of ENV variables:</p>
<codeblock><codeph>SEED_NTP_SERVER=10.34.56.78 BRIDGE_INTERFACE=eth2
BM_NETWORK_SEED_IP=10.34.55.66
BM_NETWORK_CIDR=10.34.55.0/24
BM_NETWORK_GATEWAY=10.34.55.1
SEED_NAMESERVER=172.16.222.5 
./update_sd.sh /root/tripleo /root/helion-update-1.1-to-1.1.1 | tee seed_update.log
</codeph></codeblock>
</section>
<section id="overcloud-error-message-ansible-host-key-checkingfalse"> <title>Overcloud error message: ANSIBLE_HOST_KEY_CHECKING=False</title>
<p>If, during the update-overcloud operation, you see the following failure:</p>
<codeblock><codeph>   Attempting to connect to each host
    + ANSIBLE_HOST_KEY_CHECKING=False
    + ansible -o -i /opt/stack/tripleo-ansible/plugins/inventory/heat.py -u heat-admin -m ping all
    Traceback (most recent call last):
    File "/opt/stack/venvs/ansible/bin/ansible", line 194, in &lt;module&gt;
        (runner, results) = cli.run(options, args)
    File "/opt/stack/venvs/ansible/bin/ansible", line 112, in run
        inventory_manager = inventory.Inventory(options.inventory, vault_password=vault_pass)
    File "/opt/stack/venvs/ansible/local/lib/python2.7/site-packages/ansible/inventory/__init__.py", line 118, in __init__
        self.parser = InventoryScript(filename=host_list)
    File "/opt/stack/venvs/ansible/local/lib/python2.7/site-packages/ansible/inventory/script.py", line 49, in __init__
        self.groups = self._parse(stderr)
    File "/opt/stack/venvs/ansible/local/lib/python2.7/site-packages/ansible/inventory/script.py", line 57, in _parse
        self.raw  = utils.parse_json(self.data)
     File "/opt/stack/venvs/ansible/local/lib/python2.7/site-packages/ansible/utils/__init__.py", line 552, in parse_json
        results = json.loads(data)
    File "/usr/lib/python2.7/json/__init__.py", line 338, in loads
        return _default_decoder.decode(s)
    File "/usr/lib/python2.7/json/decoder.py", line 366, in decode
        obj, end = self.raw_decode(s, idx=_w(s, 0).end())
    File "/usr/lib/python2.7/json/decoder.py", line 384, in raw_decode
        raise ValueError("No JSON object could be decoded")
    ValueError: No JSON object could be decoded
</codeph></codeblock>
<p>then, you can check the source of the issue by:</p>
<codeblock><codeph>source /opt/stack/venvs/ansible/bin/activate

    (ansible)root@hLinux:~/helion-update-1.1-to-1.1.1/tripleo/helion-update/overcloud_update# ansible -vvv -o -i /opt/stack/tripleo-ansible/plugins/inventory/heat.py -u heat-admin -m ping all
    Traceback (most recent call last):
    File "/opt/stack/venvs/ansible/bin/ansible", line 194, in &lt;module&gt;
        (runner, results) = cli.run(options, args)
    File "/opt/stack/venvs/ansible/bin/ansible", line 112, in run
        inventory_manager = inventory.Inventory(options.inventory, vault_password=vault_pass)
    File "/opt/stack/venvs/ansible/local/lib/python2.7/site-packages/ansible/inventory/__init__.py", line 118, in __init__
        self.parser = InventoryScript(filename=host_list)
    File "/opt/stack/venvs/ansible/local/lib/python2.7/site-packages/ansible/inventory/script.py", line 49, in __init__
        self.groups = self._parse(stderr)
    File "/opt/stack/venvs/ansible/local/lib/python2.7/site-packages/ansible/inventory/script.py", line 57, in _parse
        self.raw  = utils.parse_json(self.data)
    File "/opt/stack/venvs/ansible/local/lib/python2.7/site-packages/ansible/utils/__init__.py", line 552, in parse_json
        results = json.loads(data)
    File "/usr/lib/python2.7/json/__init__.py", line 338, in loads
        return _default_decoder.decode(s)
    File "/usr/lib/python2.7/json/decoder.py", line 366, in decode
        obj, end = self.raw_decode(s, idx=_w(s, 0).end())
    File "/usr/lib/python2.7/json/decoder.py", line 384, in raw_decode
        raise ValueError("No JSON object could be decoded")
    ValueError: No JSON object could be decoded
    (ansible)root@hLinux:~/helion-update-1.1-to-1.1.1/tripleo/helion-update/overcloud_update# /opt/stack/tripleo-ansible/plugins/inventory/heat.py --list
    overcloud-ce-vsastorage0/6dd10abb-482e-4eaf-b061-158efe5f8a15 stack is incomplete, in state FAILED
    (ansible)root@hLinux:~/helion-update-1.1-to-1.1.1/tripleo/helion-update/overcloud_update# 
</codeph></codeblock>
<p>In this case, the overcloud-ce-vsastorage0 node was in the FAILED state for Heat.</p>
<p>
  <xref href="#topic51655"> Return to Top </xref>
</p>
<!-- ===================== horizontal rule ===================== -->
</section>
</body>
</topic>
