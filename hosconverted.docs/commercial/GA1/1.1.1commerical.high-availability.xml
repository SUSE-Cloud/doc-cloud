<?xml version="1.0" encoding="UTF-8"?>
<!--Edit status: not edited-->
<!DOCTYPE topic
  PUBLIC "-//OASIS//DTD DITA Topic//EN" "http://docs.oasis-open.org/dita/v1.1/OS/dtd/topic.dtd" >
<topic xml:lang="en-us" id="topic_qdt_cs1_4s">
  <title>HPE Helion <tm tmtype="reg">OpenStack</tm> 1.1.1: High Availability (HA)</title>
  <prolog>
    <metadata>
      <othermeta name="layout" content="default"/>
      <othermeta name="product-version" content="HPE Helion Openstack"/>
      <othermeta name="product-version" content="HPE Helion Openstack 1.0"/>
      <othermeta name="product-version" content="HPE Helion Openstack 1.0.1"/>
      <othermeta name="product-version" content="HPE Helion Openstack 1.1"/>
      <othermeta name="role" content="Storage Administrator"/>
      <othermeta name="role" content="Storage Architect"/>
      <othermeta name="role" content="Sameer V, Vandana S"/>
      <othermeta name="product-version1" content="HPE Helion Openstack"/>
      <othermeta name="product-version2" content="HPE Helion Openstack 1.0"/>
      <othermeta name="product-version3" content="HPE Helion Openstack 1.0.1"/>
      <othermeta name="product-version4" content="HPE Helion Openstack 1.1"/>
    </metadata>
  </prolog>
  <body>
    <p><!--PUBLISHED--><!--./commercial/GA1/1.1.1commerical.high-availability.md--><!--permalink: /helion/openstack/1.1.1/high-availability/--></p>
    <p>This page covers the following topics:</p>
    <ul id="ul_bjt_cs1_4s">
      <li>
        <xref type="section" href="#topic21822/concepts-overview">High Availability( HA) concepts
          overview</xref>
        <ul id="ul_cqt_cs1_4s">
          <li>
            <xref type="section" href="#topic21822/scope-ha">Scope of High Availability: Protection
              against Single Points of Failure (SPOF)</xref>
          </li>
        </ul>
      </li>
      <li>
        <xref type="section" href="#topic21822/ha-helion">Highly Available cloud services</xref>
        <ul id="ul_d5t_cs1_4s">
          <li>
            <xref type="section" href="#topic21822/ha-overcloud-controller">High Availability of
              overcloud controllers</xref>
            <ul id="ul_lyt_cs1_4s">
              <li>
                <xref type="section" href="#topic21822/api-msg-flow">API Request Message Flow</xref>
              </li>
              <li>
                <xref type="section" href="#topic21822/handling-node-failure">Handling Node
                  Failure</xref>
              </li>
              <li>
                <xref type="section" href="#topic21822/handling-network-partition">Handling Network
                  Partitions</xref>
                <ul id="ul_fzt_cs1_4s">
                  <li>
                    <xref type="section" href="#topic21822/mysql-galera">MySQL Galera Cluster</xref>
                  </li>
                </ul></li>
            </ul>
          </li>
          <li>
            <xref type="section" href="#topic21822/singleton-services">Singleton Services</xref>
            <ul id="ul_pzt_cs1_4s">
              <li>
                <xref type="section" href="#topic21822/cinder-volume">Cinder Volume</xref>
              </li>
              <li>
                <xref type="section" href="#topic21822/sherpa">Sherpa</xref>
              </li>
              <li>
                <xref type="section" href="#topic21822/nova-console-auth">nova-consoleauth</xref>
              </li>
            </ul>
          </li>
          <li>
            <xref type="section" href="#topic21822/replace-rebuild">Rebuilding or Replacing Failed
              Controller Nodes</xref>
          </li>
        </ul>
      </li>
      <li>
        <xref type="section" href="#topic21822/ha-cloud-infra">Highly Available Cloud
          Infrastructure</xref>
        <ul id="ul_i15_cs1_4s">
          <li>
            <xref type="section" href="#topic21822/availability-zones">Availability Zones</xref>
          </li>
          <li>
            <xref type="section" href="#topic21822/compute-with-kvm">Compute with KVM</xref>
            <ul id="ul_v25_cs1_4s">
              <li>
                <xref type="section" href="#topic21822/nova-availability-zones">Nova availability
                  zones</xref>
              </li>
            </ul>
          </li>
          <li>
            <xref type="section" href="#topic21822/compute-with-esx">Compute with ESX</xref>
          </li>
          <li>
            <xref type="section" href="#topic21822/block-storage">Block Storage with StoreVirtual
              VSA</xref>
          </li>
          <li>
            <xref type="section" href="#topic21822/object-storage">Object Storage with Swift</xref>
          </li>
          <li>
            <xref type="section" href="#topic21822/cinder-avail">Cinder Availability Zones</xref>
          </li>
        </ul>
      </li>
      <li>
        <xref type="section" href="#topic21822/ha-workloads">Highly Available Cloud Applications and
          Workloads</xref>
      </li>
      <li>
        <xref type="section" href="#topic21822/not-ha">What is not Highly Available?</xref>
      </li>
      <li>
        <xref type="section" href="#topic21822/more-info">More information</xref>
      </li>
    </ul>
    <section>
      <title>High Availability concepts overview</title>
      <p>Highly Available Cloud Services ensures that at least a minimum of cloud resources are
        always available on request, which results in uninterrupted operations for users.</p>
      <p>Cloud users are able to provision and manage the compute, storage, and network
        infrastructure resources at any given point in time and the Horizon Dashboard and the
        OpenStack APIs must be reachable and be able to fulfill user requests.</p>
      <p><image href="../../media/Ha-resilient-cloud-infrastructure.png" id="image_c3m_hkv_ps"/></p>
      <p>Once the Compute, Storage, and Network resources are deployed, users expect these resources
        to be reliable in the following ways:</p>
      <ul id="ul_if5_cs1_4s">
        <li>
          <p>If the Nova-Compute KVM Hypervisors/servers hosting the project compute virtual
            machine(VM) dies and the compute VM is lost along with its local ephemeral storage, the
            re-launching of the dead compute VM succeeds because it launches on another Nova-Compute
            KVM Hypervisor/server.</p>
        </li>
        <li>
          <p>If ephemeral storage loss is undesirable, the compute VM can be booted from the Cinder
            volume.</p>
        </li>
        <li>
          <p>Data stored in Block Storage service volumes is always available and volumes are rarely
            lost by the service provider.</p>
        </li>
        <li>
          <p>Data stored by the Object Operation service is always available and is rarely lost by
            the cloud service provider.</p>
        </li>
        <li>
          <p>Network resources such as routers, subnets, and floating IP addresses provisioned by
            the Networking Operation service are rarely lost by the cloud service provider and will
            continue to provide a network path to the Compute VMs.</p>
        </li>
      </ul>
      <p>The infrastructure that provides these features is called a <b>Highly Available Cloud
          Infrastructure</b>.</p>
    </section>
    <section>
      <title>Highly Available cloud-aware tenant workloads</title>
      <p>HPE Helion OpenStack Compute hypervisors do not support transparent high availability for
        user applications; as such, the project application provider is responsible for deploying
        their applications in a redundant and highly available manner, using multiple VMs spread
        appropriately across availability zones, routed through the load balancers and made highly
        available through clustering.</p>
      <p>These are known as <b>Highly Available Cloud-Aware Tenant Workloads</b>.</p>
    </section>
    <section>
      <title>Scope of High Availability: Protection against single points of failure (SPOF)</title>
      <p>In order to achieve this high availability of services, infrastructure and workloads, we
        define the scope of HA to be limited to protecting these only against single points of
        failure (SPOF).</p>
      <p>Single points of failure include:</p>
      <ol id="ol_rk5_cs1_4s">
        <li>
          <b>Hardware SPOFs</b>: Hardware failures can take the form of servers failing, memory
          going bad, power failing, hypervisors crashing, hard disks dying, NIC cards breaking,
          switch ports failing, network cables loosening, and so forth.</li>
        <li>
          <b>Software SPOFs</b>: Server processes can crash due to software defects, out-of-memory
          conditions, operating system kernel panic, and so forth.</li>
      </ol>
      <p>By design, HPE Helion OpenStack strives to create a system architecture resilient to SPOFs,
        and does not attempt to automatically protect the system against multiple cascading levels
        of failures; such cascading failures will result in an unpredictable state.</p>
      <p>Hence, the cloud operator is encouraged to recover and restore any failed component, as
        soon as the first level of failure occurs.</p>
    </section>
    <section>
      <title>Highly Available cloud services</title>
      <p>The HPE Helion OpenStack installer deploys highly available configurations of OpenStack
        cloud services, resilient against single points of failure.</p>
    </section>
    <section>
      <title>High Availability of overcloud controllers</title>
      <p>The high availability of the overcloud controller components comes in two main forms.</p>
      <ol id="ol_jp5_cs1_4s">
        <li>Many services are stateless and multiple instances are run across the control plane in
          active-active mode. The API services (nova-api, cinder-api, etc.) are accessed through the
          HA proxy load balancer whereas the internal services (nova-scheduler, cinder-scheduler,
          etc.), are accessed through the message broker. These services use the database cluster to
          persist any data.<note>The HA proxy load balancer is also run in active-active mode and
            keepalived (used for Virtual IP (VIP) Management) is run in active-active mode, with
            only one keepalived instance holding the VIP at any one point in time.</note></li>
        <li>HA of the message queue service and the database service is achieved by running these in
          a clustered mode across the three nodes of the overcloud control plane: RabbitMQ cluster
          with Mirrored Queues and Percona MySQL Galera cluster.<p/><image
            href="../../media/ha-architecture.png" id="image_y5b_4kv_ps"/><p> The above diagram
            illustrates the HA architecture with the focus on VIP management and load balancing. It
            only shows a subset of active-active API instances and does not show examples of other
            services such as nova-scheduler, cinder-scheduler, etc.</p></li>
      </ol>
      <p>In the above diagram, requests from an OpenStack client to the overcloud API services are
        sent to VIP and port combination; for example, 192.0.2.26:8774 for a Nova request. The load
        balancer listens for requests on that VIP and port. When it receives a request, it selects
        one of the controller nodes configured for handling Nova requests, in this particular case,
        and then forwards the request to the IP of the selected controller node on the same
        port.</p>
      <p>The nova-api service list, which is listening for requests on the IP of its host machine,
        then receives the request and deals with it accordingly. The database service is also
        accessed through the load balancer
        <!---(TODO: Section discussing database lockup issue with concurrent writes - this could require HA proxy always selecting a single node for access or just for writes, if possible)-->.
        RabbitMQ, on the other hand, is not currently accessed through VIP/HA proxy as the clients
        are configured with the set of nodes in the RabbitMQ cluster and failover between cluster
        nodes is automatically handled by the clients.</p>
      <p>The sections below cover the following topics in detail:</p>
      <ul id="ul_f55_cs1_4s">
        <li>
          <xref type="section" href="#topic21822/api-msg-flow">API Request Message Flow</xref>
        </li>
        <li>
          <xref type="section" href="#topic21822/handling-node-failure">Handling Node Failure</xref>
        </li>
        <li>
          <xref type="section" href="#topic21822/handling-network-partition">Handling Network
            Partitions</xref>
        </li>
        <li>
          <xref type="section" href="#topic21822/mysql-galera">MySQL Galera Cluster</xref>
        </li>
      </ul>
    </section>

    <section><title>API Request message flow</title><p>The diagram below shows the flow for an API
        request in an overcloud HA deployment. All API requests (internal and external) are sent
        through the VIP.</p><image href="../../media/Ha-api-request-flow.png" id="image_f4k_skv_ps"
        /><p>The flow of a sample API request is explained below:</p><ol id="ol_cz5_cs1_4s">
        <li>
          <p>keepalived has currently configured the VIP on the Controller0 node; client sends Nova
            request to VIP:8774</p>
        </li>
        <li>
          <p>HA proxy (listening on VIP:8774) receives the request and selects Controller0 from the
            list of available nodes (Controller0, Controller1, Controller2). The request is
            forwarded to the Controller0IP:8774</p>
        </li>
        <li>
          <p>nova-api on Controller0 receives the request and determines that a database change is
            required. It connects to the database using VIP:</p>
        </li>
        <li>
          <p>HA proxy (listening on VIP:3306) receives the database connection request and selects
            Controller1 from the list of available nodes (Controller0, Controller1, Controller2).
            The connection request is forwarded to Controller1IP:3306</p>
        </li>
      </ol></section>
    <section>
      <title>Handling node failure</title>
      <p>With the above overcloud HA set up, loss of a controller node is handled as follows:</p>
      <p>Assume that the Controller0, which is currently in control of the VIP, is lost, as shown in

        the diagram below:<image href="../../media/ha-nodefailure-1.png" id="image_sgy_zzv_ps"/></p>
      <p>When this occurs, keepalived immediately moves the VIP on to the Controller1 and can now
        receive API requests, which is load-balanced by HA proxy, as stated earlier.</p>
      <note>Although MySQL and RabbitMQ clusters have lost a node, they still continue to be
          operational.<image href="../../media/ha-nodefailure-2.png" id="image_i5y_j1w_ps"/></note>
      <p>Finally, when Controller0 comes back online, keepalived and HA proxy will resume in
        standby/slave mode and be ready to take over, should there be a failure of Controller1. The
        Controller0 rejoins the MySQL and RabbitMQ clusters as shown in the figure below:</p>
      <p><image href="../../media/ha-nodefailure-3.png" id="image_tpg_41w_ps"/></p>
    </section>
    <section>
      <title>Handling network partitions</title>
      <p>It is important for the overcloud HA setup to tolerate network failures, specifically those
        that result in a partition of the cluster, whereby one of the three nodes in the overcloud
        control plane cannot communicate with the remaining two nodes of the cluster. </p>
    </section>
    <section>
      <title>MySQL Galera cluster</title>
      <p>The handling of network partitions is illustrated in the diagram below. Galera has a quorum
        mechanism so when there is a partition in the cluster, the primary or quorate partition can
        continue to operate as normal, whereas the non-primary/minority partition cannot commit any
        requests. In the example below, Controller0 is partitioned from the rest of the control
        plane. As a result, requests can only be satisfied on Controller1 or Controller2.

        Controller0 will continue to attempt to rejoin the cluster:<image
          href="../../media/mysql-galera-cluster.png" id="image_ecc_y1w_ps"/></p>
      <p>When HA proxy detects the errors against the mysql instance on Controller0, it removes that
        node from its pool for future database requests.</p>
    </section>
    <section>
      <title>Singleton services on management controller</title>
      <p>The following three services run as singletons in the control plane for the current
        release:</p>
      <p>
        <ul id="ul_tdv_cs1_4s">
          <li>Cinder Volume</li>
          <li>Sherpa</li>
          <li>nona-consoleauth</li>
        </ul>
      </p>
      <p>HPE Helion OpenStack defines three Controllers:</p>
      <ol id="ol_ydv_cs1_4s">
        <li>OvercloudController0 </li>
        <li>OvercloudController1</li>
        <li>OvercloudController2</li>
      </ol>
      <p> The above services are deployed and activated on one of these Controllers. AutAutomated
        failover for these singleton services is not provided and in the event of irrecoverable
        failure of the management controller server, you must resort to de-commissioning the failed
        server from the control plane and deploying the management controller on a new server using
        instructions provided in the <xref href="1.1commercial.backup-restore-GA.xml#topic19267"
          >Backup and Restore</xref> document. </p>
    </section>
    <section>
      <title>Cinder-volume</title>
      <p>Due to the single threading required in both cinder-volume and the drivers, the Cinder

        volume service is run as a singleton in the control plane. <image
          href="../../media/ha-cinder-volume.png" id="image_tvz_bbw_ps"/></p>
      <p>Cinder-volume is deployed on all three controller nodes, but kept active on only one node
        at a time. By default, Cinder-volume is kept active on the overcloud controller0 node. If
        the controller fails, you must follow the procedure below to ensure that the cinder-volume
        service is deployed on the designated host, and only that host. If a failed controller0 is
        brought back after moving the cinder-volume service to another controller (i.e.
        controller1), the service will start on both nodes and cause serious problems.</p>
      <p>Since cinder.conf is kept synchronized across all three nodes, Cinder volume can be run on
        any of the nodes at any given time. Ensure that it is run on only one node at a time.</p>
      <p>To run Cinder volume on a node other than the default controller0:</p>
      <ol id="ol_ajv_cs1_4s">
        <li>Get the hostname of the new node. For controller1 this would be a unique name similar
          to:<codeblock><codeph>$ hostname
overcloud-controller1-px7gyh74dccx
</codeph></codeblock></li>
        <li>Export<codeblock><codeph>OVERCLOUD_CINDER_VOLUME_SINGLETON=&lt;hostname&gt;
</codeph></codeblock></li>
        <li>and update the overcloud from the seed node. In our example the command would
          be:<codeblock><codeph>OVERCLOUD_CINDER_VOLUME_SINGLETON=overcloud-controller1-px7gyh74dccx bash -x tripleo/tripleo-incubator/scripts/hp_ced_installer.sh --update-overcloud
</codeph></codeblock></li>
        <li>
          <p>After hp_ced_installer.sh runs to update the overcloud, the cinder-volume service
            should be running on the new node (in this example, controller1). If controller0 is
            restarted or running, cinder-volume should NOT be running on controller0.</p>
        </li>
      </ol>
      <p>
        <!--**Steps to start Cinder-Volume**

    os-svc-enable-upstart cinder-volume enable
    
    service cinder-volume start

**Steps to stop Cinder-Volume**

    service cinder-volume stop
    
    os-svc-enable-upstart cinder-volume disable
-->
      </p>
      <note> Following a system update, the volume manager may attempt to start before the storage
        nodes have been properly initialized, and so the volume manager must be restarted after the
        automatic start procedure has completed.</note>
      <p>To restart the Cinder-volume service, on the overcloud controller node where the
        Cinder-volume singleton service is being run, the Cinder-volume service can be restarted
        using</p>
      <codeblock><codeph>service cinder-volume restart
</codeph>
</codeblock>
    </section>
    <section>
      <title>Sherpa</title>
      <p>You must take periodic backups of the Sherpa service since it does maintain some state
        information on local disk storage on the controller. If the controller fails, Sherpa becomes
        unavailable until you rebuild or restore the controller. After restoring the controller, you
        should restore the Sherpa state from its latest backup.</p>
    </section>
    <section>
      <title>Nova consoleauth</title>
      <p>If the controller fails, the Nova consoleauth service will become unavailable and users
        will not be able to connect to their VM consoles via VNC. The service will be restored once
        you restore the controller.</p>
    </section>
    <section>
      <title>Rebuilding or replacing failed controller nodes</title>
      <p>As described above, the three node controller cluster provides a robust, highly available
        control plane of OpenStack services. Either OvercloudController1 or OvercloudController2
        servers can be shut down for a short duration for maintenance activities without impacting
        cloud service availability. (OvercloudController0 cannot be shut down without affecting
        cloud service availability.)</p>
      <note>The HA design is only robust against single points of failure and may not protect you
        against multiple levels of failure. As soon as first-level failure occurs, you must try to
        fix the symptom/root cause and recover from the failure, as soon as possible.</note>
      <p>In the unlikely event that one of the controller servers suffers an irreparable hardware
        failure, you can decommission and delete it from the cluster. You can then deploy the failed
        controller on a new server and connect it back into the original three node controller
        cluster. Learn more about <xref
          href="../../commercial/GA1/1.1commerical.services-remove-replace-failed-overcloud-nodes.xml"
          >Replacing/Rebuilding Controller Nodes</xref>.</p>
    </section>
    <section>
      <title>Highly Available cloud infrastructure</title>
      <p>The highly available cloud infrastructure consists of the following:</p>
      <ul id="ul_unv_cs1_4s">
        <li>
          <xref type="section" href="#topic21822/availability-zones">Availability Zones</xref>
        </li>
        <li>
          <xref type="section" href="#topic21822/compute-with-kvm">Compute with KVM</xref>
        </li>
        <li>
          <xref type="section" href="#topic21822/nova-availability-zones">Nova availability
            zones</xref>
        </li>
        <li>
          <xref type="section" href="#topic21822/compute-with-esx">Compute with ESX</xref>
        </li>
        <li>
          <xref type="section" href="#topic21822/block-storage">Block Storage with StoreVirtual
            VSA</xref>
        </li>
        <li>
          <xref type="section" href="#topic21822/object-storage">Object Storage with Swift</xref>
        </li>
        <li>
          <xref type="section" href="#topic21822/networking">Networking</xref>
        </li>
      </ul>
    </section>
    <section>
      <title>Availability zones</title>

      <p><image href="../../media/ha-availability-zone.png" id="image_oks_bcw_ps"/></p>
      <p>While planning your OpenStack deployment, you should decide on how to zone various types of
        nodes - such as compute, block storage, and object storage. For example, you may decide to
        place all servers in the same rack in the same zone. For larger deployments, you may plan
        more elaborate redundancy schemes for redundant power, network ISP connection, and even
        physical firewalling between zones (<i>this aspect is outside the scope of this
        document</i>).</p>
      <p>HPE Helion OpenStack offers APIs, CLIs and Horizon UIs for the administrator to define and
        user to consume, availability zones for each of Nova, Cinder and Swift services. This
        section outlines the process to deploy specific types of nodes to specific physical servers,
        and makes a statement of available support for these types of availability zones in the
        current release.</p>
      <note>By default, HPE Helion OpenStack is deployed in a single availability zone upon
        installation. Multiple availability zones can be configured by an administrator
        post-install, if required. Refer to the <xref
          href="http://docs.openstack.org/openstack-ops/content/scaling.html" format="html"
          scope="external">Chapter 5:
          Scaling</xref> in the <i>OpenStack
          Operations Guide</i> for more information.</note>
    </section>
    <section>
      <title>Compute with KVM</title>
      <p>You can deploy your KVM Nova-Compute nodes either during initial installation, or by adding
        compute nodes post initial installation.</p>
      <p>While adding compute nodes post initial installation, you can specify the target physical
        servers for deploying the compute nodes.</p>
      <p>Learn more about <xref href="../../commercial/GA1/1.1commercial.install-add-nodes.xml"
          >Adding Compute Nodes after Initial Installation</xref>.</p>
    </section>
    <section>
      <title>Nova availability zones</title>
      <p>Nova host aggregates and availability zones are not supported for general consumption in
        the current release.</p>
      <p>However, Nova availability zones will be supported for the narrow use of the HPE Helion
        OpenStack Platform Services deployed on HPE Helion OpenStack Please refer to the Platform
        Service documentation for further information.</p>
    </section>
    <section>
      <title>Compute with ESX hypervisor</title>
      <p>Compute nodes deployed on ESX Hypervisor can be made highly available using the HA feature
        of VMware ESX Clusters. For more information on VMware HA, please refer to your VMware ESX
        documentation.</p>
    </section>
    <section><title>Block Storage with StoreVirtual VSA</title><p>Highly available Cinder block
        storage volumes are provided by the network RAID 10 implementation in the HPE StoreVirtual
        VSA software. You can deploy the VSA nodes in three node cluster and specify Network RAID 10
        protection for Cinder volumes.</p><p>The underlying SAN/iQ operating system of the
        StoreVirtual VSA ensures that the two-way replication maintains two mirrored copies of data
        for each volume.</p><p>This Network RAID 10 capability ensures that failure of any single
        server does not cause data loss, and maintains data access to the
        clients.</p><p>Furthermore, each of the VSA nodes of the cluster can be strategically
        deployed in different zones of your data center for maximum redundancy and resiliency. For
        more information on how to deploy VSA nodes on desired target servers, refer to the<xref
          href="../../commercial/GA1/1.1commercial.install-GA-vsa.xml"> Deploy VSA</xref>

        document.</p><image href="../../media/ha-block-storage.png" id="image_vy2_3cw_ps"
      /></section>
    <section/>
    <section>
      <title>Cinder availability zones</title>
      <p>Cinder availability zones are not supported for general consumption in the current
        release.</p>
    </section>
    <section><title>Object storage with Swift</title><p>You can configure your Swift Rings by
        specifying a target zone for each drive that is added to the ring. Learn more about
        configuring zones for various drives using the Ringos tool here. Please refer to the <xref
          href="../../commercial/GA1/1.1commerical.services-object-pyringos.xml">Ringos</xref>

        document.</p><image href="../../media/ha-swift.png" id="image_dkk_mcw_ps"
      /><!--

Neutron Networking {#networking}
 
Neutron Netwoking in HA covers the following:

1. Neutron Ser
2. ver (API) and its underlying MySQL and RabbitMQ are HA using HAproxy, Galera and Mirrored queue technologies for clustering.

2. With the advent of DVR, the Neutron L3 agent no longer lives on central controller nodes, but is distributed and runs on each Compute node, such that, if it fails, its impact is only limited to VMs within that compute node. (do you have some pointer we could refer the user for further info on DVR concepts)

3. DHCP agent is still running on central Cloud controllers - should a cloud controller running DHCP services fail, you must manually migrate over all the DHCP services to any of the remaining cloud controller nodes using the procedures outlined and validated (still work in progress) in https://jira.hpcloud.net/browse/CORE-1777. Until the DHCP services are migrated, no new VMs will be able to launch and existing VMs will not be able to renew their DHCP leases upon expiration.

4. SNAT services still run on one of the central Cloud controller - should a cloud controller running SNAT services fail, your VMs will lose SNAT ability and will not be able to reach services on external networks. You can work around this by attaching a floating IP to your VM, which will allow it to reach to external networks without going through the central SNAT. 
 --></section>
    <section>
      <title>Highly available cloud applications and workloads</title>
      <p>Projects writing applications to be deployed in the cloud must be aware of the cloud
        architecture and potential points of failure and architect their applications accordingly
        for high availability.</p>
      <p>Some guidelines for consideration:</p>
      <ol id="ol_osv_cs1_4s">
        <li>Assume intermittent failures and plan for retries<ul id="ul_lxv_cs1_4s">
            <li>
              <p>
                <b>OpenStack Service APIs</b>: invocations can fail - you should carefully evaluate
                the response of each invocation, and retry in case of failures.</p>
            </li>
            <li>
              <p>
                <b>Compute</b>: VMs can die - monitor and restart them</p>
            </li>
            <li>
              <p>
                <b>Network</b>: Network calls can fail - retry should be successful</p>
            </li>
            <li>
              <p>
                <b>Storage</b>: Storage connection can hiccup - retry should be successful</p>
            </li>
          </ul></li>
        <li>Build redundancy into your application tiers<ul id="ul_ayv_cs1_4s">
            <li>
              <p>Servers running your VM instances can die.</p>
              <ul id="ul_qyv_cs1_4s">
                <li>
                  <p>Replicate VMs containing stateless services such as Web application tier or Web
                    service API tier and put them behind load balancers (you must implement your own
                    HA Proxy type load balancer in your application VMs until HPE Helion OpenStack
                    delivers the LBaaS service).</p>
                </li>
                <li>
                  <p>Boot the replicated VMs into different Nova availability zones.</p>
                </li>
                <li>
                  <p>If your VM stores state information on its local disk (Ephemeral Storage), and
                    you cannot afford to lose it, then boot the VM off a Cinder volume.</p>
                </li>
                <li>
                  <p>Take periodic snapshots of the VM which will back it up to Swift through
                    Glance.</p>
                </li>
                <li>
                  <p>Your data on ephemeral may get corrupted (but not your backup data in Swift and
                    not your data on Cinder volumes).</p>
                </li>
                <li>
                  <p>Take regular snapshots of Cinder volumes and also back up Cinder volumes or
                    your data exports into Swift.</p>
                </li>
              </ul>
            </li>
          </ul></li>
        <li>Instead of rolling your own highly available stateful services, use readily available HP
          Helion OpenStack platform services such as:<ul id="ul_xyv_cs1_4s">
            <li>Database as a Service (DBaaS)</li>
            <li>DNS as a Service(DNSaaS)</li>
            <li>Messaging as a Service (MSGaaS)</li>
          </ul></li>
      </ol>
    </section>
    <section>
      <title>What is not Highly Available?</title>
      <p>The following section defines the different services in Seed and Undercloud that are not HA
        supported:</p>
    </section>
    <section>
      <title>Seed</title>
      <p>All services running on the Seed VM are not highly available and if the Seed VM fails, it
        has an impact on the Undercloud functionality.</p>
      <p>Ensure the following points:</p>
      <ul id="ul_jzv_cs1_4s">
        <li>
          <p>Ensure a periodic backup of Seed VM. Please refer to the <xref
              href="../../commercial/GA1/1.1commercial.backup-restore-GA.xml">Backup and
              Restore</xref> document for more details.</p>
        </li>
        <li>
          <p>In case of Seed VM failure, the Undercloud continues to function. However, if any of
            the Undercloud nodes crash or is shutdown, it does not boot up as it is dependent on
            Seed to provide the PXE image.</p>
        </li>
        <li>
          <p>In the above case, it is recommended to recover Seed VM by following the documented
            restore process and bring it up in order for it to serve Undercloud node PXE requests
            during the restart process.</p>
        </li>
      </ul>
    </section>
    <section>
      <title>Undercloud</title>
      <p>All the services running on Undercloud node are not highly available and if the Undercloud
        node fails, it impacts the Overcloud functionality.</p>
      <p>Ensure the following points:</p>
      <ul id="ul_d2w_cs1_4s">
        <li>
          <p>Ensure the periodic backup of Undercloud node. Please refer to the <xref
              href="../../commercial/GA1/1.1commercial.backup-restore-GA.xml">Backup and
              Restore</xref> document for more details.</p>
        </li>
        <li>
          <p>In case of Undercloud node failure, the Overcloud continues to function. However, if
            any of the Overcloud nodes crash or is shutdown, it does not boot up as it is dependent
            on the Undercloud to provide the PXE image.</p>
        </li>
        <li>
          <p>In the above case, it is recommended to recover the Undercloud node by following the
            documented restore process and bring it up for it to serve Overcloud node PXE requests
            during the restart process.</p>
        </li>
      </ul>
    </section>
    <section>
      <title>Other</title>
      <ul id="ul_fjw_cs1_4s">
        <li>High availability is not supported for Centralized Virtual Router.</li>
      </ul>
    </section>
    <section>
      <title>More information</title>
      <ul id="ul_vnw_cs1_4s">
        <li>
          <xref href="http://docs.openstack.org/high-availability-guide/content/ch-intro.html"
            scope="external" format="html">OpenStack High-availability Guide</xref>
        </li>
      </ul>
      <!-- http://www.ibm.com/developerworks/cloud/library/cl-cloudappdevelop/ **pretty sure we can't link HP's website to IBM's** -->
      <ul id="ul_nsw_cs1_4s">
        <li>
          <xref href="http://12factor.net/" scope="external" format="html">12-Factor Apps</xref>
        </li>
      </ul>
      <p>
        <xref type="section" href="#topic21822"> Return to Top </xref>
      </p>
      <!-- ===================== horizontal rule ===================== -->
    </section>
  </body>
</topic>
