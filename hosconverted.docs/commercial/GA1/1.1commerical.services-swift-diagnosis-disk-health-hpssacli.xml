<?xml version="1.0" encoding="UTF-8"?>
<!--Edit status: not edited-->
<!DOCTYPE topic
  PUBLIC "-//OASIS//DTD DITA Topic//EN" "http://docs.oasis-open.org/dita/v1.1/OS/dtd/topic.dtd" ><topic xml:lang="en-us" id="topic6748">
<title>HPE Helion <tm tmtype="reg">OpenStack</tm> 1.1: Monitoring Disk Drives in Object Storage Using hpssacli Utility</title>
<prolog>
<metadata>
<othermeta name="layout" content="default"/>
<othermeta name="product-version" content="HPE Helion Openstack"/>
<othermeta name="product-version" content="HPE Helion Openstack 1.0"/>
<othermeta name="product-version" content="HPE Helion Openstack 1.0.1"/>
<othermeta name="product-version" content="HPE Helion Openstack 1.1"/>
<othermeta name="role" content="Storage Administrator"/>
<othermeta name="role" content="Storage Architect"/>
<othermeta name="role" content="Karthik P, Binamra S"/>
<othermeta name="product-version1" content="HPE Helion Openstack"/>
<othermeta name="product-version2" content="HPE Helion Openstack 1.0"/>
<othermeta name="product-version3" content="HPE Helion Openstack 1.0.1"/>
<othermeta name="product-version4" content="HPE Helion Openstack 1.1"/>
</metadata>
</prolog>
<body>
<p>
<!--PUBLISHED-->
 <!--./commercial/GA1/1.1commerical.services-swift-diagnosis-disk-health-hpssacli.md-->
 <!--permalink: /helion/openstack/1.1/services/swift/diagnosis-disk-health/hpssacli/--></p>
<p>

</p>
<p>HPE Helion Object Storage (Swift) provides native monitoring of Swift services and hardware resources like disk drives.To monitor the health status of the disk drives of the Swift cluster, monitoring scripts are introduced for HPE hardware. The monitoring solution depends on hpssacli application to fetch various details of hardware to determine its health and publish to Icinga. hpssacli utility does not get installed implicitly as part of Swift node deployment.</p>
<p>You are advised to download and install utility as per procedure mentioned below. If utility is not installed then Icinga dashboard will reflect the various disk parameters as critical.</p>
<section id="hp-smart-storage-administrator-cli-20220-hpssacli"> <title>HPE Smart Storage Administrator CLI 2.0.22.0 (HPSSACLI)</title>
<p>The HPE Smart Storage Administrator CLI (HPSSACLI) is a command line disk configuration program for Smart Array Controllers.</p>
<p>The <b>
<i>hpssacli</i>
</b> utility is deployed onto the servers wherever the disks are to be monitored, usually all Object Storage(Swift), VSA, and Compute nodes.</p>
<p>This page explains the downloading and installation of <b>hpssacli</b> utility.</p>
<ul>
<li>
<xref type="section" href="#topic6748/download">Download the hpssacli debian package in the KVM host</xref>
</li>
</ul>
</section>
<section id="download"> <title>Download the hpssacli debian package in the KVM host</title>
<p>
  <xref href="http://downloads.linux.hp.com/SDR/repo/mcp/pool/non-free/hpssacli-2.0-16.0_amd64.deb" scope="external" format="html" >http://downloads.linux.hp.com/SDR/repo/mcp/pool/non-free/hpssacli-2.0-16.0_amd64.deb</xref>
</p>
</section>
<section id="copy-debian-package-to-seed-and-to-the-swift-nodes-where-the-disks-has-to-be-monitored"> <title>Copy debian package to seed and to the Swift nodes where the disks has to be monitored</title>
<p>Use <codeph>scp</codeph> to copy the utility package on to the Swift node(s) and install it.</p>
<ol>
<li>Copy the package from KVM host to
          seed.<codeblock><codeph>scp "hpssacli-2.0-16.0_amd64.deb" root@&lt;IP address of seed&gt;
</codeph></codeblock></li>
<li>Copy the package from seed to Swift node(s) where the disks are to be
          monitored.<codeblock><codeph>scp "hpssacli-2.0-16.0_amd64.deb" heat-admin@&lt;IP address of Swift node(s)&gt;
</codeph></codeblock></li>
</ol>
<p>
        <note>Repeat this process for all the Swift nodes.</note>
      </p>
</section>
<section id="install-debian-package-to-the-swift-nodes"> <title>Install debian package to the Swift nodes</title>
<ol>
<li>Log in to the Swift node(s) where the debian is
          copied.<codeblock><codeph>ssh heat-admin@&lt;IP address of Swift node(s)&gt;
</codeph></codeblock></li>
<li>Change the directory.<codeblock><codeph>cd /home/heat-admin/
</codeph></codeblock></li>
<li>Install the
          package.<codeblock><codeph>dpkg -i hpssacli-2.0-16.0_amd64.deb
</codeph></codeblock></li>
</ol>
<p>
        <note>Repeat this process for all the Swift nodes.</note>
      </p>
<p>The hpssacli program is now installed. Refer the following section for the collection of the diagnostic report.</p>
<p>This utility is used to monitor the usage of <xref href="../../commercial/GA1/1.1commerical.services-swift-deployment-monitor-disk-drives.xml" >Object Storage(Swift) disk drive(s)</xref>.</p>
<p>Once the hpssacli package is installed, Swift monitoring (ICINGA) scripts monitor the drives.</p>
<p>(<b>Optional</b>) You can also  manually collect the diagnostic reports from disks in the servers as explained in the following sections.</p>
</section>
<section id="manual-procedure-to-collect-the-diagnostic-report"> <title>Manual procedure to collect the diagnostic report</title>
<p>Perform the following steps to manually collect the diagnostic report.</p>
<p>
  <b>Using the server controller:</b>
</p>
<ol>
<li>Enter the following command to determine the controller
            slot.<codeblock><codeph>sudo hpssacli ctrl all show status
</codeph></codeblock><p>The
            following sample displays slot
          details:</p><codeblock><codeph>Smart Array P420i in Slot 0 (Embedded)

   Controller Status: OK

   Cache Status: OK

   Battery/Capacitor Status: OK
</codeph></codeblock></li>
<li>Generate the diagnostic report for a particular
            slot.<codeblock><codeph>sudo hpssacli ctrl slot=(slot number) diag file=&lt;filename.zip&gt;
</codeph></codeblock><p>Or
            </p><p> Generate the report for all the
            slots</p><codeblock><codeph>sudo hpssacli ctrl all diag file=&lt;filename.zip&gt;
</codeph></codeblock><p>The
            file will be stored in the selected location.</p></li>
<li>Copy the generated file to the desired location.</li>
<li>Extract the file.</li>
</ol>
<p>
  <b>Using ssh from seed:</b>
</p>
<ol>
<li>Generate the diagnostic report of the particular
          slot.<codeblock><codeph> ssh heat-admin@&lt;Machine IP address&gt; "sudo hpssacli ctrl slot=&lt;slot number&gt; diag file=details_slot_&lt;slot number&gt;.zip"
</codeph></codeblock></li>
<li>Copy the report from the server to seed
          VM.<codeblock><codeph>scp heat-admin@&lt;Machine IP address&gt;:/home/heat-admin/&lt;filename.zip&gt; 
</codeph></codeblock></li>
<li>Copy the report &lt;filename.zip&gt; to the
          KVM_host<codeblock><codeph>scp details_slot_&lt;slot number&gt;.zip ubuntu@&lt;KVM_Host IP address&gt;:
</codeph></codeblock><!--Enter login credentails ???--></li>
<li>Extract the file.</li>
<li>Open the <codeph>ADUReport.htm</codeph> file in the browser. The html page displays complete
          details of the controller and the health status of the physical disks available in the
          machine. </li>
<li>Generate SmartSSD Wear Gauge Report from either the local server or from the seed:<ul>
            <li>
              <p>From the local server</p>
              <p>
                <codeph>hpssacli ctrl slot=&lt;slot number&gt; diag file=&lt;filename.zip&gt;
                  ssdrpt=on</codeph>
              </p>
            </li>
            <li>
              <p>From the seed</p>
              <p>
                <codeph>ssh heat-admin@&lt;machine IP address&gt; "sudo hpssacli ctrl slot=&lt;slot
                  number&gt; diag file=&lt;filename.zip&gt;</codeph>
              </p>
            </li>
          </ul></li>
</ol>
<p>The following SmartSSDWearGaugeReport.txt sample file displays the details of the  SSD drives.</p>
<codeblock><codeph>ADU Version                             2.0.22.0

Diagnostic Module Version               8.0.22.0

Time Generated                          Tuesday September 16, 2014 12:26:57PM

Device Summary:

   Smart Array P420i in Embedded Slot

Report for Smart Array P420i in Embedded Slot

---------------------------------------------

Smart Array P420i in Embedded Slot : Internal Drive Cage at Port 2I : Box 2 : Physical Drive (400 GB SAS SSD) 2I:2:6 : SmartSSD Wear Gauge

   Status                               OK

   Supported                            TRUE

   Log Full                             FALSE

   Utilization                          0.000000

   Power On Hours                       1798

   Has Smart Trip SSD Wearout           FALSE

Smart Array P420i in Embedded Slot : Internal Drive Cage at Port 2I : Box 2 : Physical Drive (400 GB SAS SSD) 2I:2:7 : SmartSSD Wear Gauge

   Status                               OK

   Supported                            TRUE

   Log Full                             FALSE

   Utilization                          0.000000

   Power On Hours                       1797

   Has Smart Trip SSD Wearout           FALSE

Smart Array P420i in Embedded Slot : Internal Drive Cage at Port 2I : Box 2 : Physical Drive (400 GB SAS SSD) 2I:2:8 : SmartSSD Wear Gauge

   Status                               OK

   Supported                            TRUE

   Log Full                             FALSE

   Utilization                          0.000000

   Power On Hours                       1798

   Has Smart Trip SSD Wearout           FALSE
</codeph></codeblock>
</section>
<section id="other-useful-commands"> <title>Other useful commands</title>
<ul>
<li>
<p>To get the details of the physical disks and also the logical group status on the server</p>

<codeblock><codeph>ssh heat-admin@&lt;machine IP address&gt; "sudo hpssacli ctrl slot=&lt;slot number&gt; show config detail"
</codeph></codeblock>
</li>
<li>
<p>To execute other operations on physical drives</p>

<codeblock><codeph>ssh heat-admin@&lt;machine IP address&gt; "sudo hpssacli help physicaldrive"
</codeph></codeblock>
</li>
<li>
<p>Open the help to explore other options with the utility</p>

<codeblock><codeph>ssh heat-admin@&lt;machine IP address&gt; "sudo hpssacli help"
</codeph></codeblock>
</li>
</ul>
<p>
  <xref href="#topic6748"> Return to Top </xref>
</p>
<!-- ===================== horizontal rule ===================== -->
</section>
</body>
</topic>
