<?xml version="1.0" encoding="UTF-8"?>
<!--Edit status: not edited-->
<!DOCTYPE topic
  PUBLIC "-//OASIS//DTD DITA Topic//EN" "http://docs.oasis-open.org/dita/v1.1/OS/dtd/topic.dtd" ><topic xml:lang="en-us" id="topic5937">
<title>HPE Helion <tm tmtype="reg">OpenStack</tm> 1.1: Centralized Logging Overview</title>
<prolog>
<metadata>
<othermeta name="layout" content="default"/>
<othermeta name="product-version" content="HPE Helion Openstack"/>
<othermeta name="product-version" content="HPE Helion Openstack 1.1"/>
<othermeta name="role" content="Systems Administrator"/>
<othermeta name="role" content="Cloud Architect"/>
<othermeta name="role" content="Storage Administrator"/>
<othermeta name="role" content="Network Administrator"/>
<othermeta name="role" content="Service Developer"/>
<othermeta name="role" content="Cloud Administrator"/>
<othermeta name="role" content="Application Developer"/>
<othermeta name="role" content="Network Engineer"/>
<othermeta name="role" content="Paul F"/>
<othermeta name="product-version1" content="HPE Helion Openstack"/>
<othermeta name="product-version2" content="HPE Helion Openstack 1.1"/>
</metadata>
</prolog>
<body>
<p>
<!--PUBLISHED-->
 <!--./commercial/GA1/1.1commerical.services-logging-overview.md-->
 <!--permalink: /helion/openstack/1.1/services/logging/overview/--></p>
<p>Because a typical HPE Helion OpenStack cloud consists of multiple servers, locating a specific log from a specific server can be difficult.</p>
<p>The HPE Helion OpenStack Centralized Logging feature collects logs on a central system, rather than leaving the logs scattered across the network. The administrator can use a single Kibana interface to view log information in charts, graphs, tables, histograms, and other forms.</p>
<p>Centralized logging helps the administrator triage and troubleshoot the distributed HPE Helion OpenStack cloud deployment from a single location. The admin is not required to access the several remote servers (SSH) to view the individual log files.</p>
<p>In addition to each of the <xref href="../../commercial/GA1/1.1commerical.services-overview.dita" >HPE Helion services</xref>, Centralized Logging also processes logs for the following HPE Helion OpenStack features:</p>
<ul>
<li>HAProxy</li>
<li>syslog</li>
<li>keepalived </li>
</ul>
<p>This document describes the Centralized Logging feature and contains the following sections:</p>
<ul>
<li>
<xref type="section" href="#topic5937/install">Installation</xref>
</li>
<li>
<xref type="section" href="#topic5937/components">Centralized Logging components</xref>
</li>
<li>
<xref type="section" href="#topic5937/types">Centralized Logging log types</xref>
</li>
<li>
<xref type="section" href="#topic5937/kibana">Kibana configuration</xref>

<ul>
<li>
<xref type="section" href="#topic5937/interface">Logging into Kibana</xref>
</li>
</ul>
</li>
<li>
<xref type="section" href="#topic5937/info">For more information</xref>
</li>
</ul>
<section id="install"> <title>Installation</title>
<p>The Centralized Logging feature is automatically installed in the undercloud as part of the HPE Helion OpenStack installation.</p>
<p>No specific configuration is required to use Centralized Logging. However, you can tune or configure the individual components as needed for your environment. Tuning and configuring these components is outside the scope of this document.</p>
</section>
<section id="components"> <title>Centralized Logging components</title>
<p>Centralized logging consists of several components, including Logstash, Elasticsearch, RabbitMQ, and Kibana.</p>
<ul>
<li>
<p>
<b>Beaver</b> is a python daemon that takes information in log files and sends the content to RabbitMQ.</p>
</li>
<li>
<p>
<b>RabbitMQ</b> is a message broker for collection of logging data across nodes.</p>
</li>
<li>
<p>
<b>logstash</b> is a log processing system for receiving, processing and outputting logs. logstash retrieves logs from RabbitMQ, processes and enriches the data, then stores the data in Elasticsearch.</p>
</li>
<li>
<p>
<b>Elasticsearch</b> is data store offering fast indexing and querying.</p>
</li>
<li>
<p>
<b>Kibana</b> is a client-side JavaScript application to visualize the data in Elasticsearch through a web browser. Kibana enables you to create charts and graphs using the log data.</p>
</li>
</ul>
<p>These components are configured to work out-of-the-box and the admin should be able to view log data using the default configurations.</p>
<p>At a high level, the Helion services forward logs to Beaver. Then, Beaver forwards JSON messages to RabbitMQ on the controller0 (management controller) node. Logstash connects to RabbitMQ to read queued messages and process the messages according to the Logstash configuration file. Logstash then forwards the processed log files in Elasticsearch. Users can use the Kibana interface to view and analyze the information, as shown in the following figure:</p>
<p>
  <image href="../../media/centrallogging75.png" placement="break"/>
</p>
      <note>Logging requires 4GB of disk space to make sure that all logging messages are
        retained.</note>
</section>
<section id="types"> <title>Centralized Logging log types</title>
<p>The following table lists the types of logs collected by Centralized Logging and provides information on how the logs are maintained.</p>
<table>
<tgroup cols="6">
<colspec colname="col1"/>
<colspec colname="col2"/>
<colspec colname="col3"/>
<colspec colname="col4"/>
<colspec colname="col5"/>
<colspec colname="col6"/>
<thead>
<row>
    <entry>Data name</entry>
    <entry>Confidentiality</entry>
    <entry>Integrity</entry>
    <entry>
Availability</entry>
    <entry>Backup?</entry>
    <entry>Description</entry>
  </row>
</thead>
<tbody>
<row>
    <entry>Log records</entry>
    <entry>Restricted</entry>
    <entry>High</entry>
    <entry>Medium</entry>
    <entry>No</entry>
    <entry>Log records have a limited life, and are not archived. The log file on the local filesystem provides a fallback source of logging data (up to 20GB or 45 days) if the logging system fails.</entry>
  </row>
<row>
    <entry>Log metadata</entry>
    <entry>Restricted</entry>
    <entry>High</entry>
    <entry>Medium</entry>
    <entry>No</entry>
    <entry>Elasticsearch indexes logged data to allow flexible searching.</entry>
  </row>
<row>
    <entry>Credentials</entry>
    <entry>Confidential</entry>
    <entry>High</entry>
    <entry>Medium</entry>
    <entry>No</entry>
    <entry>Credentials for access to Elasticsearch and RabbitMQ are stored in configuration files owned by root with mode 0600.</entry>
  </row>
</tbody>
</tgroup>
</table>
<p>Log rotation will happen daily or when the current logfile reaches 2GB, whichever happens sooner. The number of rotations held will be balanced to attempt to cap logs from all services at 200GB.</p>
</section>
<section id="kibana"> <title>Kibana configuration</title>
<p>You can use the Kibana dashboards to view log data. Kibana is a tool developed to create charts, graphs, tables, and histograms based on logs send to Elasticsearch by logstash.</p>
<p>While creating Kibana dashboards is beyond the scope of this document, it is important to know that you can use the default Kibana dashboards or create custom dashboards. The dashboards are JSON files that you can modify or create new dashboards based on existing dashboards.</p>
      <note>Kibana is client-side software. To operate properly, the browser must be able to access
        port 81 on undercloud.</note>
</section>
<section id="interface"> <title>Logging into Kibana</title>
<p>Launch a web browser on the seed cloud host to the following IP address, using the undercloud IP address from the end of the install:</p>
<codeblock>
  <codeph>http://&lt;undercloud IP&gt;:81 
</codeph>
</codeblock>
<p>
  <b>Example:</b>
</p>
<codeblock>
  <codeph>http://192.0.2.2:81
</codeph>
</codeblock>
<p>Log in with the user name <codeph>kibana</codeph> and the password.</p>
<p>If you did not retrieve the undercloud IP from the end of the install,</p>
<ol>
<li>
<p>From the seed cloud host log in to the undercloud as super user:</p>

<codeblock>
<codeph>ssh heat-admin@&lt;undercloud IP&gt; 
sudo su - 
</codeph>
</codeblock>
</li>
<li>
<p>Enter the following command to display the password:</p>

<codeblock>
<codeph>cat  /root/work/tripleo/tripleo-undercloud-passwords
</codeph>
</codeblock>

<p>Make note of the password.</p>
</li>
</ol>
<p>Because centralized logging is separate service from the Horizon dashboards, there is separate username/password authentication. By default, the HPE Horizon dashboard username and password will not work with Kibana.</p>
</section>
<section id="info"> <title>For more information</title>
<p>For information the centralized logging components, use the following links:</p>
<ul>
<li>
<xref href="http://logstash.net/" scope="external" format="html" >Logstash</xref> </li>
<li>
<xref href="http://www.elasticsearch.org/" scope="external" format="html" >Elasticsearch</xref>
</li>
<li>
<xref href="http://www.rabbitmq.com/" scope="external" format="html" >RabbitMQ</xref>
</li>
<li>
<p>
<xref href="http://www.elasticsearch.org/guide/en/kibana/current/_dashboard_schema.html" scope="external" format="html" >Kibana Dashboard</xref>
</p>

<p>
<xref href="#topic5937"> Return to Top </xref>
</p>
</li>
</ul>
<!-- ===================== horizontal rule ===================== -->
</section>
</body>
</topic>
