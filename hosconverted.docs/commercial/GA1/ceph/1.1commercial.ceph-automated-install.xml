<?xml version="1.0" encoding="UTF-8"?>
<!--Edit status: not edited-->
<!DOCTYPE topic
  PUBLIC "-//OASIS//DTD DITA Topic//EN" "http://docs.oasis-open.org/dita/v1.1/OS/dtd/topic.dtd" ><topic xml:lang="en-us" id="topic10341">
<title>HPE Helion <tm tmtype="reg">OpenStack</tm> 1.1 and 1.1.1: Ceph Automated Installation</title>
<titlealts>
<searchtitle>HPE Helion Openstack 1.1: Ceph Automated Installation</searchtitle>
</titlealts>
<prolog>
<metadata>
<othermeta name="layout" content="default"/>
<othermeta name="product-version" content="HPE Helion Openstack"/>
<othermeta name="product-version" content="HPE Helion Openstack 1.1"/>
<othermeta name="role" content="Storage Engineer"/>
<othermeta name="role" content="Storage Architect"/>
<othermeta name="role" content="Storage Administrator"/>
<othermeta name="role" content="Storage Engineer"/>
<othermeta name="role" content="Service Developer"/>
<othermeta name="role" content="Cloud Administrator"/>
<othermeta name="role" content="Application Developer"/>
<othermeta name="role" content="Binamra S, Paul F"/>
<othermeta name="product-version1" content="HPE Helion Openstack"/>
<othermeta name="product-version2" content="HPE Helion Openstack 1.1"/>
</metadata>
</prolog>
<body>
<p>
<!--PUBLISHED-->
 <!--./commercial/GA1/ceph/1.1commercial.ceph-automated-install.md-->
 <!--permalink: /helion/openstack/1.1/commercial.ceph-automated-install/--></p>
<p>The installation workflow of HPE Helion OpenStack 1.1 or 1.1.1 is shown in the following diagram.</p>
<p>
  <image href="../../../media/ceph-installation-flow.png" placement="break"/>
</p>
<p>The next diagram shows the logical architecture of Ceph.</p>
<p>
  <image href="../../../media/ceph-logical-architecture.png" placement="break"/>
</p>
<p>This section covers the following topics:</p>
<ol>
<li>
<xref type="section" href="#topic10341/tarball-files">Tarball Files</xref>
</li>
<li>
<xref type="section" href="#topic10341/running-provision-tool">Running the Provisioning Tool</xref>
</li>
<li>
<xref type="section" href="#topic10341/verify-instance">Verifying your Instance</xref>
</li>
<li>
<xref type="section" href="#topic10341/provisioning-additional-ceph-nodes">Provisioning Additional Ceph Nodes</xref>
</li>
</ol>
<section id="tarball-files"> <title>Tarball Files</title>
<p>Log into <xref href="https://helion.hpwsportal.com"   format="html" >https://helion.hpwsportal.com</xref>, navigate to Workloads &gt; Storage &gt; Ceph Update. Download  the <codeph>helion-ceph.tar</codeph> (ceph package).  The ceph package contains the following tar files:</p>
<ul>
<li>
<codeph>cephsetup.tar</codeph>
</li>
<li>
<codeph>cephconfiguration.tar</codeph>
</li>
<li>
<codeph>heartbeat-tool.tar</codeph>
</li>
</ul>
</section>
<section id="the-cephsetuptar-file"> <title>The cephsetup.tar file</title>
<p>The <codeph>cephsetup.tar</codeph> file contains the following packages:</p>
<ul>
<li>
<codeph>images</codeph>
</li>
<li>
<codeph>helion-patch</codeph>
</li>
<li>
<codeph>node-provisioner</codeph>
</li>
</ul>
</section>
<section id="images-package"> <title>Images package</title>
<p>Ceph-automation provides two types of images:</p>
<ul>
<li>
          <codeph>ceph-admin-image</codeph> includes all the packages required to run configuration
          scripts. This image is used on the Ceph admin node.</li>
<li>
<p>
            <codeph>ceph-cluster-image</codeph> includes all the packages for setting up the
              <codeph>ceph-monitor</codeph>,  the <codeph>ceph-osd</codeph> and  the
              <codeph>overcloud-ceph-rados</codeph> Gateway.</p>

<p>The <codeph>ceph-cluster-image</codeph> is shared across all the nodes participating in cluster
            with the exception of the Ceph admin node.</p>
</li>
</ul>
</section>
<section id="helion-patch-package"> <title>helion-patch package</title>
<p>The <codeph>helion-patch</codeph> package includes:</p>
<ul>
<li>
<codeph>hp_ced_check_deployed_images</codeph>
</li>
<li>
<codeph>hp_ced_pre_update_checks.sh</codeph>
</li>
<li>
<codeph>register-cephnodes.sh</codeph> </li>
</ul>
</section>
<section id="node-provisioner-package"> <title>node-provisioner package</title>
  <p>The <codeph>node-provisioner</codeph> package includes:</p>
  <ul>
    <li>
      <codeph>client</codeph>
    </li>
    <li>
      <codeph>server</codeph> </li>
  </ul>
<p>To deploy these images using the provisioning tool that runs from the seed VM:</p>
<ol>
        <li>Log into the Helion seed
          VM.<codeblock><codeph>ssh root @ &lt;seed IP address&gt;
</codeph></codeblock></li>
        <li>Create a directory on any
          location.<codeblock><codeph>mkdir /helion-ceph
</codeph></codeblock></li>
        <li>Untar and copy the downloaded <codeph>cephsetup.tar</codeph> file to the
            <codeph>/helion-ceph</codeph> folder on the seed VM.
          <codeblock>tar -xvf helion-ceph.tar</codeblock>Untarring this file results in the follow
          directory structure:<ul>
            <li>
              <codeph>images</codeph>
            </li>
            <li>
              <codeph>helion-patch</codeph>
            </li>
            <li><codeph>node-provisioner</codeph>
            </li>
          </ul></li>
        <li>To view the contents of the <codeph>/helion-ceph/images</codeph> directory from the seed
          VM, enter (for
            example):<codeblock><codeph>root@hLinux:~/helion-ceph/cephsetup/images# ls
bm-deploy-kernel
bm-deploy-ramdisk
overcloud-ceph-admin-initrd
overcloud-ceph-admin-vmlinuz
overcloud-ceph-admin.qcow2
overcloud-ceph-cluster-initrd
overcloud-ceph-cluster-vmlinuz
overcloud-ceph-cluster.qcow2
</codeph></codeblock><note>To
            simplify your Ceph installation, move all the <codeph>/cephsetup</codeph> directories up
            a level to /<codeph>helion-ceph</codeph></note></li>
        <li> To view the contents in the <codeph>/helion-ceph/node-provisioner</codeph>directory
          from the seed VM, enter (for
          example):<codeblock><codeph>/helion-ceph/node-provisioner# ls
server
client  

/helion-ceph/node-provisioner/server# ls
app.wsgi
bottle.py
httpd.conf
server.json
 server.py

/helion-ceph/node-provisioner/client# ls
baremetal.csv
common.py
orchestration.json
orchestration.py
</codeph></codeblock></li>
        <li>Modify <codeph>/server/server.json</codeph> to include OpenStack credentials from the
          Undercloud (<codeph>stackrc</codeph>), network, and the keypair that you want to use. The
          following is a sample of
          <codeph>server.json</codeph>:<codeblock><codeph>root@hLinux:~/helion-ceph/node-provisioner/server# cat server.json
{
"authentication": {
   "HOST": "127.0.0.1",
   "PORT": "8085",
   "OS_VERSION": "2",
   "OS_USER": "admin",
   "OS_PASSWORD": "de44ff17c309c7b2a74465104cd59728b121dd60",
   "OS_TENANT_NAME": "admin",
   "OS_AUTH_URL": "http://&lt;undercloud IP&gt;:5000/v2.0",
   "keypair": "cephadmin",
   "netid": "823ad730-6c69-42e6-b455-8a8173ceae81"
    }
}
</codeph></codeblock></li>
      </ol>
<p>where:</p>
<p>
<b>HOST</b> is the local machine IP or loopback IP which will be your seed IP.</p>
<p>
<b>PORT</b> is 8085 which can be used as a service port.</p>
<p>
<b>OS_VERSION</b> is the OpenStack API version. Should be set to <b>2</b>.</p>
<p>
<b>OS_USER</b> is supplied from the Undercloud <codeph>stackrc</codeph> file. For example: <b>admin</b>.</p>
<p>
<b>OS_PASSWORD</b> -is supplied from the Undercloud <codeph>stackrc</codeph> file. For example: <b>de44ff17c309c7b2a74465104cd59728b121dd60</b>.</p>
<p>
<b>OS    _TENANT_NAME</b> is supplied from the Undercloud <codeph>stackrc</codeph> file.</p>
<p>
<b>OS    _AUTH_URL</b> is the Undercloud IP. For example: <b>http://192.168.51.23:5000/v2.0</b>.</p>
<p>
        <b>keypair</b> for "cephadmin". You generate this from the Undercloud node using <b>nova
          keypair-add cephadmin &gt; cephadmin.pem</b>
        <note>To enable login to other nodes, provide 600 permission to the
            <codeph>cephadmin.pem</codeph> key. Execute <codeph>chmod 600 cephadmin.pem</codeph>
          command to update the permission.</note></p>
<p>
<b>netid</b>: Execute <codeph>nova network-list</codeph> from the Undercloud to get the <codeph>netid</codeph>. For example: <b>823ad730-6c69-42e6-b455-8a8173ceae81</b>.</p>
<p>You can verify that the <codeph>pem</codeph> file is present in Undercloud node by:</p>
<ol>
<li>Logging to the
          Undercloud.<codeblock><codeph># ssh heat-admin@&lt;undercloud IP address&gt;
</codeph></codeblock></li>
<li>Then entering.<codeblock><codeph>ls -ls
</codeph></codeblock><p>For
          example:</p><codeblock><codeph>root@undercloud-undercloud-zyjo2vylo3tb:~# ls -ls
total 12
4 -rw------- 1 root       root       1680 Feb 10 22:17 cephadmin.pem
4 -rw------- 1 heat-admin heat-admin  347 Feb 11 21:12 overcloud.stackrc
4 -rw-r--r-- 1 root       root        311 Feb 10 20:34 stackrc
</codeph></codeblock></li>
</ol>
</section>
<section id="running-provision-tool">
      <title>Running the provisioning tool</title>
      <p>To start the provisioning tool:</p>
      <ol>
        <li>Open a new terminal.</li>
        <li>Log into the seed VM and launch a server from the
            <codeph>/helion-ceph/node-provisioner/server/</codeph> directory using the
            <codeph>./server.py</codeph> script.<p>When you start the server, the following
            information
          displays:</p><codeblock><codeph>root@hLinux:~/helion-ceph/node-provisioner/server# ./server.py
Bottle v0.13-dev server starting up (using WSGIRefServer())...
Listening on http://192.168.51.22:8085/
Hit Ctrl-C to quit.
</codeph></codeblock></li>
        <li>Open another seed terminal window and modify the <codeph>orchestration.json</codeph>
          file in <codeph>/helion-ceph/node-provisioner/client/orchestration.json</codeph><p>Your
              <codeph>baremetal.csv</codeph> inputs should match the flavor list. For example you
            need to specify the right RAM, vCPUs, disk size in "GB" and version number.</p><p>
            <ul id="ul_nm1_vjg_ps">
              <li>
                <p>Update the <codeph>"ws_url"</codeph> file with the URL of the seed node VM where
                  the server is running by entering:</p>
                <codeblock><codeph>"ws_url": "http://192.168.51.22:8085/"
</codeph></codeblock>
              </li>
              <li>
                <p>Modify <codeph>imagepath</codeph> to include the path to images (deploy images +
                  qcow + ramdisk + kernel) by entering:</p>
                <codeblock><codeph>"imagepath": "/helion-ceph/images/"
</codeph></codeblock>
                <p>For example, make changes to the following based on your
                  setup.<codeblock><codeph>{
"authentication": {
       "ws_url": "http://192.168.51.22:8085/"
    },
    "api":{
       "imagepath": "/helion-ceph/images/",
       "deploy-image-prefix": "bm-deploy"
    },
    "orchestration":{
       "hypervisorsleepduration": "300",
       "hypervisorsmoniteringfrequency": "10",
       "bootsleepduration": "1200",
       "bootinitialwaitduration": "30",
       "hypervisortype": "baremetal",
      "hypervisordriver": "ironic",
       "bootmoniteringfrequency": "5",
       "destinationpath": "/helion-ceph/"
    },
    "flavor": {
           "001": {
               "ram": "16384",
               "vcpus": "2",
               "disk": "275",
               "architecture": "x86_64",
               "version" : "001"
          },
          "002": {
               "ram": "16384",
               "vcpus": "2",
               "disk": "200",
               "architecture": "x86_64",
               "version" : "001"
           }
    }
</codeph></codeblock></p>
              </li>
            </ul>
          </p></li>
        <li>Modify the <codeph>baremetal.csv</codeph> file in
            <codeph>/helion-ceph/node-provisioner/client/</codeph> to include the details of the
          node. For example:
            <codeblock><codeph>f0:92:1c:05:57:30,helioncsel,m0ng00s3,10.1.67.123,2,16384,200,overcloud-ceph-cluster,c1mon1,002
f0:92:1c:05:bb:b0,helioncsel,m0ng00s3,10.1.67.124,2,16384,200,overcloud-ceph-cluster,c1mon2,002
f0:92:1c:05:4c:10,helioncsel,m0ng00s3,10.1.67.125,2,16384,200,overcloud-ceph-cluster,c1mon3,002
f0:92:1c:05:6c:78,helioncsel,m0ng00s3,10.1.67.126,2,16384,200,overcloud-ceph-cluster,c1gw1,002
f0:92:1c:05:ac:30,helioncsel,m0ng00s3,10.1.67.127,2,16384,200,overcloud-ceph-cluster,c1gw2,002
f0:92:1c:05:aa:d8,helioncsel,m0ng00s3,10.1.67.128,2,16384,200,overcloud-ceph-cluster,c1osd1,002
9c:b6:54:97:44:10,helioncsel,m0ng00s3,10.1.67.129,2,16384,200,overcloud-ceph-cluster,c1osd2,002
f0:92:1c:05:7c:58,helioncsel,m0ng00s3,10.1.67.130,2,16384,200,overcloud-ceph-cluster,c1osd3,002
9c:b6:54:97:a3:e0,helioncsel,m0ng00s3,10.1.67.131,2,16384,200,overcloud-ceph-admin,c1admin,002
</codeph></codeblock><p>Note
            that the flavor definition (<codeph>flavor ID</codeph>) in the
              <codeph>baremetal.csv</codeph> file should match the flavor defined in
              <codeph>orchestration.json</codeph>. (The version number is set to 001 for all
            nodes.)</p></li>
        <li>Open another seed terminal window and from the seed node VM directory
            <codeph>/helion-ceph/node-provisioner/client/</codeph>, launch the client by
          entering:<codeblock><codeph>python orchestration.py
</codeph></codeblock></li>
        <li> To monitor this process, open a new terminal seed window and
          run<codeblock><codeph>tail -f -50 /helion-ceph/node-provisioner/client/orchestration.log
</codeph></codeblock></li>
      </ol>
    </section>
<section id="verify-instance"> <title>Verifying your Instance</title>
<p>The provisioning tool takes about 10-15 minutes to provision a single baremetal node.</p>
<p>Check the <codeph>nova list</codeph> to see if the instance goes from spawning  to active state. You should be able to SSH to the instance after 12 minutes once you run the script.</p>
<p>If the instance does not spawn, check:</p>
<ul>
<li>The flavor definition matches the actual machine hardware configuration</li>
<li>Check for any errors in <codeph>orchestration.log</codeph>
</li>
<li>Verify entries in <codeph>server.json</codeph> and <codeph>orchestration.json</codeph>
</li>
<li>Check for any errors in the server terminal window</li>
<li>Check that all the images are loaded to Glance</li>
<li>Make sure that the instance you are tying to provision does not exist in <codeph>ironic node-list</codeph> or <codeph>nova list</codeph>
</li>
<li>Check for any other Nova-related errors in <codeph>/var/log/nova/nova-compute.log or /var.log/ironic/ dir</codeph>
</li>
</ul>
<p>Once all your instances are active and up, move on to the next step.</p>
<p>SSH to your newly launched instance using the <codeph>cephadmin</codeph> key that was generated from the Undercloud in the previous steps.</p>
<p>For example:</p>
<codeblock><codeph>root@undercloud-undercloud-zyjo2vylo3tb:~# ssh -i cephadmin.pem hlinux@192.168.51.133
</codeph></codeblock>
<p>The programs included with the hLinux system are free software. For information on specific
licensing terms for each program, see the individual files in
<codeph>/usr/share/doc/*/copyright</codeph>.</p>
<p>When you SSH into your instance, you should see (for example):</p>
<codeblock><codeph>Last login: Thu Feb 12 04:33:42 2015 from 192.168.51.23
hlinux@admin-overcloud-ceph-admin:~$ 
</codeph></codeblock>
</section>
<section id="provisioning-additional-ceph-nodes"> <title>Provisioning Additional Ceph Nodes</title>
<p>To add new nodes, create new <codeph>baremetal.csv</codeph> entries and follow the above provisioning steps.</p>
      <note>Do not append new entries to existing entries in the <codeph>baremetal.csv</codeph> file
        or else the existing nodes will be reprovisioned.</note>
</section>
<section id="next-steps"> <title>Next Steps</title>
<p>
  <xref href="../../../commercial/GA1/ceph/1.1commercial.ceph-cluster-client-node-configuration-ansible.xml" >Configuring Ceph Clusters and Client Nodes using Ansible Playbooks</xref>
</p>
<!-- ===================== horizontal rule ===================== -->
<p>
  <xref href="#topic10341"> Return to Top </xref>
</p>
</section>
</body>
</topic>
