<?xml version="1.0" encoding="UTF-8"?>
<!--Edit status: not edited-->
<!DOCTYPE topic
  PUBLIC "-//OASIS//DTD DITA Topic//EN" "http://docs.oasis-open.org/dita/v1.1/OS/dtd/topic.dtd" ><topic xml:lang="en-us" id="topic42821">
<title>HPE Helion <tm tmtype="reg">OpenStack</tm> 1.1 and 1.1.1: Manually Bringing up your Ceph Cluster</title>
<titlealts>
<searchtitle>HPE Helion Openstack 1.1: Manually bringing up your Ceph cluster</searchtitle>
</titlealts>
<prolog>
<metadata>
<othermeta name="layout" content="default"/>
<othermeta name="product-version" content="HPE Helion Openstack"/>
<othermeta name="product-version" content="HPE Helion Openstack 1.1"/>
<othermeta name="role" content="Storage Engineer"/>
<othermeta name="role" content="Storage Architect"/>
<othermeta name="role" content="Storage Administrator"/>
<othermeta name="role" content="Storage Engineer"/>
<othermeta name="role" content="Service Developer"/>
<othermeta name="role" content="Cloud Administrator"/>
<othermeta name="role" content="Application Developer"/>
<othermeta name="role" content="Paul F, Binamra S"/>
<othermeta name="product-version1" content="HPE Helion Openstack"/>
<othermeta name="product-version2" content="HPE Helion Openstack 1.1"/>
</metadata>
</prolog>
<body>
<p>
<!--PUBLISHED-->
 <!--./commercial/GA1/ceph/1.1commercial.ceph-manual-install.md-->
 <!--permalink: /helion/openstack/1.1/ceph-manual-install/--></p>
<p>The Ceph cluster requires at least one monitor, and at least three, or as many OSDs as there are copies of an object stored on the cluster - whichever is greater. For more details, refer to <xref href="http://docs.ceph.com/docs/master/install/manual-deployment/." scope="external" format="html" >http://docs.ceph.com/docs/master/install/manual-deployment/.</xref>
</p>
<p>This section includes the following topics:</p>
<ul>
<li>
<xref type="section" href="#topic42821/setting-up-monitor-nodes">Setting up the Monitor Node</xref>
</li>
<li>
<xref type="section" href="#topic42821/adding-osd">Adding OSDs</xref>
</li>
<li>
<xref type="section" href="#topic42821/file-system-tunning">File System Tuning</xref>
</li>
<li>
<xref type="section" href="#topic42821/ceph-tunning">Ceph Tuning</xref>
</li>
<li>
<xref type="section" href="#topic42821/setting-up-additional-nodes">Setting up additional monitor nodes</xref>
</li>
</ul>
<section id="assumptions-and-dependencies"> <title>Assumptions and dependencies</title>
<ul>
<li>Ceph nodes are deployed using the provision tool that also has Ceph Firefly 0.80.7, other tools and packages.</li>
</ul>
</section>
<section id="setting-up-monitor-nodes"> <title>Setting up the monitor node</title>
<p>Bootstrap the initial monitor(s) to deploying a Ceph Storage cluster by performing the following steps:</p>
<ol>
<li>Log into monitor node as a root.</li>
<li>Execute <codeph>apt-get install ceph</codeph>.</li>
<li>To generate a unique <b>fsid</b>, enter:<p>
            <codeph>uuidgen</codeph>
          </p></li>
<li>Create the <codeph>ceph.conf</codeph> file in the <codeph>/etc/ceph</codeph> directory as shown
          in the following example:
          [global]<codeblock><codeph>fsid = 498afec5-57f0-4e48-9cf7-c8a5b89c1c80

mon_initial_members = ceph-mon1 - monitor host name

mon_host = 192.x.x.x - monitor IP

auth_cluster_required = cephx

auth_service_required = cephx

auth_client_required = cephx

filestore_xattr_use_omap = true

osd journal size = 1024
</codeph></codeblock></li>
<li>Create a keyring for the cluster and generate a secret key for the monitor by
          entering.<codeblock><codeph>ceph-authtool --create-keyring /tmp/ceph.mon.keyring --gen-key -n mon. --cap mon 'allow *'

ceph-authtool --create-keyring /tmp/ceph.mon.keyring --gen-key -n mon. --cap mon 'allow *'
</codeph></codeblock></li>
<li>Create an admin keyring, an admin user, and add the user to the keyring by
          entering.<codeblock><codeph>ceph-authtool --create-keyring /etc/ceph/ceph.client.admin.keyring --gen-key -n client.admin --set-uid=0 --cap mon 'allow *' --cap osd 'allow *' --cap mds 'allow'

ceph-authtool --create-keyring /etc/ceph/ceph.client.admin.keyring --gen-key -n client.admin --set-uid=0 --cap mon 'allow *' --cap osd 'allow *' --cap mds 'allow'
</codeph></codeblock></li>
<li>Add the <codeph>cleint.admin</codeph> key to the <codeph>ceph.mon.keyring</codeph> by
          entering:<codeblock><codeph>ceph-authtool /tmp/ceph.mon.keyring --import-keyring /etc/ceph/ceph.client.admin.keyring

ceph-authtool /tmp/ceph.mon.keyring --import-keyring /etc/ceph/ceph.client.admin.keyring
</codeph></codeblock></li>
<li>Generate a monitor map using the monitor hostname, IP and fsid and save it as
            <codeph>/tmp/monmap</codeph> by
          entering:<codeblock><codeph>monmaptool --create --add {hostname} {ip-address} --fsid {uuid} /tmp/monmap
monmaptool --create --add ceph-mon1 192.x.x.x --fsid 498afec5-57f0-4e48-9cf7-c8a5b89c1c80 /tmp/monmap
</codeph></codeblock></li>
<li>Create a default data directory by
          entering:<codeblock><codeph>sudo mkdir /var/lib/ceph/mon/{cluster-name}-{hostname}
mkdir /var/lib/ceph/mon/ceph-ceph-mon1
</codeph></codeblock></li>
<li>Populate the monitor daemon with the monitor map and keyring by
          entering.<codeblock><codeph>ceph-mon --mkfs -i {hostname} --monmap /tmp/monmap --keyring /tmp/ceph.mon.keyring
ceph-mon --mkfs -i mon1 --monmap /tmp/monmap --keyring /tmp/ceph.mon.keyring
</codeph></codeblock></li>
<li>Touch the completed file by
          entering.<codeblock><codeph>sudo touch /var/lib/ceph/mon/{cluster-name}-{hostname}/upstart

touch /var/lib/ceph/mon/ceph-ceph-mon1/done
</codeph></codeblock></li>
<li>Verify that the monitor daemon is running and verifying output by
          entering:<codeblock><codeph>ps aux | grep ceph
</codeph></codeblock></li>
<li>Update the <codeph>ceph.conf</codeph> file as shown
          below:<codeblock><codeph>[mon.ceph-mon1]

host = ceph-mon1

mon addr = 192.x.x.x
</codeph></codeblock></li>
<li>Start monitor daemon by
          entering:<codeblock><codeph>sudo /etc/init.d/ceph start mon.node1

/etc/init.d/ceph start mon.ceph-mon1
</codeph></codeblock></li>
<li>Restart the monitor daemon by
          entering:<codeblock><codeph>/etc/init.d/ceph restart mon.ceph-mon1
</codeph></codeblock></li>
<li>Verify that the Ceph default pools are created as shown
          below:<codeblock><codeph>ceph osd lspools
0 data, 1 metadata, 2rbd,
</codeph></codeblock></li>
<li>Verify that the monitor is running by
          entering:<codeblock><codeph>ceph -s

root@ceph-mon1gw1:/var/lib/ceph/mon# ceph -s
cluster e0f2ad6b-588f-432c-99c1-d81f0f71cb77
health HEALTH_ERR 192 pgs stuck inactive; 192 pgs stuck unclean; no osds
monmap e1: 1 mons at {ceph-mon1gw1=192.168.116.54:6789/0}, election epoch 2, quorum 0
ceph-mon1gw1
osdmap e1: 0 osds: 0 up, 0 in
pgmap v2: 192 pgs, 3 pools, 0 bytes data, 0 objects
    0 kB used, 0 kB / 0 kB avail
    192 creating
</codeph></codeblock></li>
</ol>
</section>
<section id="adding-osd"> <title>Adding Object Storage Daemons (OSDs)</title>
<p>Once you have your initial monitor(s) running, you must add OSDs. To make a cluster reach an active and clean state, you must have enough OSDs to handle the number of copies of an object (for example, osd pool default size = 4 requires at least four OSDs).  It is recommend to have  a minimum of three OSD nodes in a production environment. After bootstrapping your monitor, your cluster has a default CRUSH map. However, the CRUSH map does not have any Ceph OSD Daemons mapped to a Ceph Node. For more details, refer to <xref href="http://ceph.com/docs/master/install/manual-deployment/" scope="external" format="html" >http://ceph.com/docs/maste/install/manual-deployment/</xref>
</p>
</section>
<section id="pre-requisites"> <title>Pre-requisites</title>
<ul>
<li>
<p>Ceph cluster nodes are deployed using provision tool.</p>
</li>
<li>
<p>3 OSD nodes with individual drives other than OS drive on each of the nodes.</p>
</li>
<li>
<p>For performance reasons it is recommended to have <b>SSD</b> drive for Journal partition on the OSD nodes.</p>
</li>
</ul>
</section>
<section id="osd-node-with-journal-partitions"> <title>OSD node with journal partitions</title>
<p>Perform the following steps.</p>
<ul>
<li>
<p>SSH to OSD node</p>

<codeblock><codeph>ssh [node-name]
</codeph></codeblock>
</li>
</ul>
<p>
  <b>Running fdisk on journal disk</b>
</p>
<p>If you have SSD drives on your OSD node, you can use them for journal partitioning. You can have Raid1 with two SSD drives. Follow the server specification to see details on how to configure RAIDs on the drives.</p>
<p>You must select the SSD drive that you will be using on each of the OSD node.</p>
<p>You can also follow the similar steps on other high performance disks, if you plan to use them for journaling.</p>
<p>The following example explains the creation of journal partitions on a 200G SSD drive to match the OSD daemon. counts.</p>
<p>First partition is extended partition and next is the logical partitions. These logical partitions are equal to the number of OSD drives that are used on each OSD node. The logical numbering starts with "5".</p>
<p>For example: here <codeph>/dev/sda</codeph> is the journal disk which is 200G size. You must calculate the size with number of OSD drives that are used in that system. If this system has 13 OSD drives, then divide it with the size of the journal and specify that size.</p>
<p>200 [size of the disk]/13 [no of osd drives]= 15753 MB (approx) per disk. It is close to 15750M for each disk.</p>
<ol>
<li>Log in to OSD as root user.</li>
<li>Change the directory to <codeph>/etc/ceph</codeph> and
          enter:<codeblock><codeph>#fdisk /dev/sda

Command (m for help): p
Disk /dev/sda: 200.0 GB, 199992852480 bytes
255 heads, 63 sectors/track, 24314 cylinders, total 390611040 sectors
Units = sectors of 1 * 512 = 512 bytes
Sector size (logical/physical): 512 bytes / 512 bytes
I/O size (minimum/optimal): 262144 bytes / 524288 bytes
Disk identifier: 0x000007d0

Device Boot     Start   End Blocks  Id  System
Command (m for help): n

Partition type:
p primary (0 primary, 0 extended, 4 free)
e extended
Select (default p): e
Partition number (1-4, default 1): 1
First sector (2048-390611039, default 2048):
Using default value 2048
Last sector, +sectors or +size{K,M,G} (2048-390611039, default 390611039):
Using default value 390611039

Command (m for help): p
Disk /dev/sda: 200.0 GB, 199992852480 bytes
255 heads, 63 sectors/track, 24314 cylinders, total 390611040 sectors
Units = sectors of 1 * 512 = 512 bytes
Sector size (logical/physical): 512 bytes / 512 bytes
I/O size (minimum/optimal): 262144 bytes / 524288 bytes
Disk identifier: 0x000007d0

Device Boot   Start   End    Blocks     Id  System

/dev/sda1     2048 390611039 195304496 5 Extended
</codeph></codeblock></li>
<li>To create the second partition on the same disk,
          enter:<codeblock><codeph>Command (m for help): n
Partition type:
p primary (0 primary, 1 extended, 3 free)
l logical (numbered from 5)
Select (default p): l
Adding logical partition 5
First sector (4096-390611039, default 4096):
Using default value 4096
Last sector, +sectors or +size{K,M,G} (4096-390611039, default 390611039): +15750M

Command (m for help): p

Disk /dev/sda: 200.0 GB, 199992852480 bytes
255 heads, 63 sectors/track, 24314 cylinders, total 390611040 sectors
Units = sectors of 1 * 512 = 512 bytes
Sector size (logical/physical): 512 bytes / 512 bytes
I/O size (minimum/optimal): 262144 bytes / 524288 bytes
Disk identifier: 0x000007d0

Device Boot    Start    End   Blocks Id   System

/dev/sda1      2048 390611039 195304496 5 Extended

/dev/sda5      4096 32260095 16128000 83 Linux
</codeph></codeblock></li>
<li>To create the third partition on the same disk,
          enter:<codeblock><codeph>Command (m for help): n
Partition type:
p primary (0 primary, 1 extended, 3 free)
l logical (numbered from 5)
Select (default p): l
Adding logical partition 6
First sector (32262144-390611039, default 32262144):
Using default value 32262144
Last sector, +sectors or +size{K,M,G} (32262144-390611039, default 390611039): +15750M

Command (m for help): p
Disk /dev/sda: 200.0 GB, 199992852480 bytes
255 heads, 63 sectors/track, 24314 cylinders, total 390611040 sectors
Units = sectors of 1 * 512 = 512 bytes
Sector size (logical/physical): 512 bytes / 512 bytes
I/O size (minimum/optimal): 262144 bytes / 524288 bytes
Disk identifier: 0x000007d0

Device Boot    Start   End   Blocks  Id  System

/dev/sda1      2048390611039 195304496 5 Extended

/dev/sda5      409632260095 16128000 83 Linux

/dev/sda6      32262144 64518143 16128000 83 Linux
</codeph></codeblock></li>
<li>Repeat this command for rest of the partition.</li>
</ol>
<p>
  <b>Running fdisk on OSD data disk</b>
</p>
<p>Run <codeph>fdisk</codeph> on each of the OSD disks to create a partition. It should run on all the OSD node for each disk that is present.</p>
<p>
<b>Example</b>: <codeph>fdisk</codeph> runs on <codeph>/dev/sdc</codeph> that is 300G in size.</p>
<ol>
<li>Log in to OSD as root user.</li>
<li>Change the directory to <codeph>/etc/ceph</codeph> and
          enter:<codeblock><codeph>#fdisk /dev/sdc

Welcome to fdisk (util-linux 2.25.2).
Changes will remain in memory only, until you decide to write them.
Be careful before using the write command.

Device does not contain a recognized partition table.
Created a new DOS disklabel with disk identifier 0x6a938816.
93

Command (m for help): n
Partition type
p primary (0 primary, 0 extended, 4 free)
e extended (container for logical partitions)
Select (default p): p
Partition number (1-4, default 1):
First sector (2048-585871963, default 2048):
Last sector, +sectors or +size{K,M,G,T,P} (2048-585871963, default 585871963):

Created a new partition 1 of type 'Linux' and of size 279.4 GiB.

Command (m for help): p
Disk /dev/sdc: 279.4 GiB, 299966445568 bytes, 585871964 sectors
Units: sectors of 1 * 512 = 512 bytes
Sector size (logical/physical): 512 bytes / 512 bytes
I/O size (minimum/optimal): 262144 bytes / 262144 bytes
Disklabel type: dos
Disk identifier: 0x6a938816

Device Boot Start End Sectors Size Id Type
/dev/sdc1 2048 585871963 585869916 279.4G 83 Linux

Command (m for help): w
The partition table has been altered.
Calling ioctl() to re-read partition table.
Syncing disks.
</codeph></codeblock></li>
</ol>
<p>
  <b>Setting up the OSD Node</b>
</p>
<p>To create an OSD and add it to cluster and CRUSH map, perform the following steps:</p>
<ol>
<li>Log in to OSD host as root user. Connect to the OSD
          node.<codeblock><codeph>ssh [node-name]
</codeph></codeblock></li>
<li>Create the OSD and note the OSD
            number.<codeblock><codeph>ceph osd create
</codeph></codeblock><p>For multiple physical
            drives, execute the <codeph>ceph OSD create</codeph> command as many times as the number
            of OSDs.</p></li>
<li>Consider the output of the OSD number from the above step to create a default directory by
            entering:<codeblock><codeph>mkdir /var/lib/ceph/osd/ceph-{osd-number}
</codeph></codeblock><p>For
            example:</p><codeblock><codeph>`mkdir /var/lib/ceph/osd/ceph-0`
</codeph></codeblock><p>The
            following example displays the commands ran in the script to create 13 directory
            matching the OSD
          daemon.</p><codeblock><codeph>#!/bin/bash

mkdir /var/lib/ceph/osd/ceph-0
mkdir /var/lib/ceph/osd/ceph-1
mkdir /var/lib/ceph/osd/ceph-2
mkdir /var/lib/ceph/osd/ceph-3
mkdir /var/lib/ceph/osd/ceph-4
mkdir /var/lib/ceph/osd/ceph-5
mkdir /var/lib/ceph/osd/ceph-6
mkdir /var/lib/ceph/osd/ceph-7
mkdir /var/lib/ceph/osd/ceph-8
mkdir /var/lib/ceph/osd/ceph-9
mkdir /var/lib/ceph/osd/ceph-10
mkdir /var/lib/ceph/osd/ceph-11
mkdir /var/lib/ceph/osd/ceph-12
</codeph></codeblock></li>
<li>If the OSD is for a drive other than the OS drive, prepare it for use with Ceph, and mount it to
          the directory you just
          created:<codeblock><codeph>mkfs -t {fstype} /dev/{hdd}
[fs-type can be {ext4|xfs|btrfs}]
</codeph></codeblock></li>
</ol>
<p>Use <b>xfs</b> for better performance.</p>
</section>
<section id="script-to-create-xfs-file-system-on-partitioned-disks"> <title>Script to create xfs file system on partitioned disks</title>
<p>The following example uses 13 drives in the system to create xfs file system.</p>
<codeblock><codeph>#!/bin/bash
mkfs.xfs -l logdev=/dev/sda5,size=2136997888 -f -i size=2048 /dev/sdb1
mkfs.xfs -l logdev=/dev/sda6,size=2136997888 -f -i size=2048 /dev/sdc1
mkfs.xfs -l logdev=/dev/sda7,size=2136997888 -f -i size=2048 /dev/sdd1
mkfs.xfs -l logdev=/dev/sda8,size=2136997888 -f -i size=2048 /dev/sde1
mkfs.xfs -l logdev=/dev/sda9,size=2136997888 -f -i size=2048 /dev/sdf1
mkfs.xfs -l logdev=/dev/sda10,size=2136997888 -f -i size=2048 /dev/sdg1
mkfs.xfs -l logdev=/dev/sda11,size=2136997888 -f -i size=2048 /dev/sdh1
mkfs.xfs -l logdev=/dev/sda12,size=2136997888 -f -i size=2048 /dev/sdi1
mkfs.xfs -l logdev=/dev/sda13,size=2136997888 -f -i size=2048 /dev/sdj1
mkfs.xfs -l logdev=/dev/sda14,size=2136997888 -f -i size=2048 /dev/sdk1
mkfs.xfs -l logdev=/dev/sda15,size=2136997888 -f -i size=2048 /dev/sdl1
mkfs.xfs -l logdev=/dev/sda16,size=2136997888 -f -i size=2048 /dev/sdm1
mkfs.xfs -l logdev=/dev/sda17,size=2136997888 -f -i size=2048 /dev/sdn1
mount -o user_xattr /dev/{hdd} /var/lib/ceph/osd/ceph-{osd-number}
</codeph></codeblock>
<p>Use the following command to mount</p>
<codeblock><codeph>mount -o user_xattr /dev/{hdd} /var/lib/ceph/osd/ceph-{osd-number}
</codeph></codeblock>
      <note>Save the following commands to a file and save as <codeph>mount.sh</codeph>. Make
        changes to the partitions as per your configuration and run <codeph>chmod +x
          mount.sh</codeph> and <codeph>./mount.sh</codeph>.</note>
<codeblock><codeph>The following example mounts 13 drives to journal partitions.

#!/bin/bash
mount -t xfs -o logdev=/dev/sda5 /dev/sdb1 /var/lib/ceph/osd/ceph-0/
mount -t xfs -o logdev=/dev/sda6 /dev/sdc1 /var/lib/ceph/osd/ceph-1/
mount -t xfs -o logdev=/dev/sda7 /dev/sdd1 /var/lib/ceph/osd/ceph-2/
mount -t xfs -o logdev=/dev/sda8 /dev/sde1 /var/lib/ceph/osd/ceph-3/
mount -t xfs -o logdev=/dev/sda9 /dev/sdf1 /var/lib/ceph/osd/ceph-4/
mount -t xfs -o logdev=/dev/sda10 /dev/sdg1 /var/lib/ceph/osd/ceph-5/
mount -t xfs -o logdev=/dev/sda11 /dev/sdh1 /var/lib/ceph/osd/ceph-6/
mount -t xfs -o logdev=/dev/sda12 /dev/sdi1 /var/lib/ceph/osd/ceph-7/
mount -t xfs -o logdev=/dev/sda13 /dev/sdj1 /var/lib/ceph/osd/ceph-8/
mount -t xfs -o logdev=/dev/sda14 /dev/sdk1 /var/lib/ceph/osd/ceph-9/
mount -t xfs -o logdev=/dev/sda15 /dev/sdl1 /var/lib/ceph/osd/ceph-10/
mount -t xfs -o logdev=/dev/sda16 /dev/sdm1 /var/lib/ceph/osd/ceph-11/
mount -t xfs -o logdev=/dev/sda17 /dev/sdn1 /var/lib/ceph/osd/ceph-12/
</codeph></codeblock>
<p>
  <b>Initialize the OSD data directory</b>
</p>
      <note>Save the following commands to a file and save as <codeph>initialize.sh</codeph>. Make
        changes to the OSD number as per your configuration and run <codeph>chmod +x
          initialize.sh</codeph> and <codeph>./initialize.sh</codeph>.</note>
<p>To initialize the OSD data directory, enter:</p>
<codeblock><codeph>ceph-osd -i {osd-num} --mkfs -mkkey

The following example initializes 13 data directories.

#!/bin/bash
ceph-osd -i 0 --mkfs --mkkey
ceph-osd -i 1 --mkfs --mkkey
ceph-osd -i 2 --mkfs --mkkey
ceph-osd -i 3 --mkfs --mkkey
ceph-osd -i 4 --mkfs --mkkey
ceph-osd -i 5 --mkfs --mkkey
ceph-osd -i 6 --mkfs --mkkey
ceph-osd -i 7 --mkfs --mkkey
ceph-osd -i 8 --mkfs --mkkey
ceph-osd -i 9 --mkfs --mkkey
ceph-osd -i 10 --mkfs --mkkey
ceph-osd -i 11 --mkfs --mkkey
ceph-osd -i 12 --mkfs --mkkey
</codeph></codeblock>
<p>
  <b>Register OSD authentication key</b>
</p>
<p>To register the OSD authentication key, enter:</p>
<codeblock><codeph>ceph auth add osd.{osd-num} osd 'allow *' mon 'allow profile osd' -i /var/lib/ceph/osd/ceph-{osd-num}/keyring
</codeph></codeblock>
<p>The following example registers 13 OSD authentication keys.</p>
      <note>Save the following commands to a file and save as <codeph>auth.sh</codeph>.Update the
        entries based on your OSDs and run <codeph>chmod +x auth.sh</codeph> and
          <codeph>./auth.sh</codeph>.</note>
<codeblock><codeph>#!/bin/bash

ceph auth add osd.0 osd 'allow *' mon 'allow profile osd' -i /var/lib/ceph/osd/ceph-0/keyring
ceph auth add osd.1 osd 'allow *' mon 'allow profile osd' -i /var/lib/ceph/osd/ceph-1/keyring
ceph auth add osd.2 osd 'allow *' mon 'allow profile osd' -i /var/lib/ceph/osd/ceph-2/keyring
ceph auth add osd.3 osd 'allow *' mon 'allow profile osd' -i /var/lib/ceph/osd/ceph-3/keyring
ceph auth add osd.4 osd 'allow *' mon 'allow profile osd' -i /var/lib/ceph/osd/ceph-4/keyring
ceph auth add osd.5 osd 'allow *' mon 'allow profile osd' -i /var/lib/ceph/osd/ceph-5/keyring
ceph auth add osd.6 osd 'allow *' mon 'allow profile osd' -i /var/lib/ceph/osd/ceph-6/keyring
ceph auth add osd.7 osd 'allow *' mon 'allow profile osd' -i /var/lib/ceph/osd/ceph-7/keyring
ceph auth add osd.8 osd 'allow *' mon 'allow profile osd' -i /var/lib/ceph/osd/ceph-8/keyring
ceph auth add osd.9 osd 'allow *' mon 'allow profile osd' -i /var/lib/ceph/osd/ceph-9/keyring
ceph auth add osd.10 osd 'allow *' mon 'allow profile osd' -i /var/lib/ceph/osd/ceph-10/keyring
ceph auth add osd.11 osd 'allow *' mon 'allow profile osd' -i /var/lib/ceph/osd/ceph-11/keyring
ceph auth add osd.12 osd 'allow *' mon 'allow profile osd' -i /var/lib/ceph/osd/ceph-12/keyring
</codeph></codeblock>
<p>
  <b>Add OSD node to CRUSH map</b>
</p>
<p>Add OSD node to CRUSH map by entering:</p>
<codeblock><codeph>ceph osd crush add-bucket {hostname} host
</codeph></codeblock>
<p>The following example is the sample output of adding OSD node to CRUSH map:</p>
<codeblock><codeph>root@ftcceph1-osd1:/home/ceph# ceph osd crush add-bucket ftcceph1-osd1 host
 added bucket ftcceph1-osd1 type host to crush map
</codeph></codeblock>
<p>
  <b>Place OSD node under root default</b>
</p>
<p>Place the OSD node under root by default by entering:</p>
<codeblock><codeph>ceph osd crush move {hostname} root=default
</codeph></codeblock>
<p>The following example is the sample output of placing OSD node under root default:</p>
<codeblock><codeph>root@ftcceph1-osd1:/home/ceph# ceph osd crush move ftcceph1-osd1 root=default
moved item id -2 name 'ftcceph1-osd1' to location {root=default} in crush map
</codeph></codeblock>
<p>
  <b>Add OSD node to CRUSH map for receiving data</b>
</p>
<p>Add the OSD node to the CRUSH map for receiving data by entering:</p>
<codeblock><codeph>ceph osd crush add {id-or-name} {weight} [{bucket-type}={bucket-name} ...]
ceph osd crush add osd.0 1.0 host=ceph-osd1
</codeph></codeblock>
      <note>Save the following commands to a file and save as <codeph>crush.sh</codeph>. Update the
        hostname after <codeph>host=</codeph> and run <codeph>chmod +x crush.sh</codeph> and
          <codeph>./crush.sh</codeph>.</note>
<p>The following example adds 13 OSDs to the CRUSH map.</p>
<codeblock><codeph>#!/bin/bash

ceph osd crush add osd.0 1.0 host=ceph-osd1
ceph osd crush add osd.1 1.0 host=ceph-osd1
ceph osd crush add osd.2 1.0 host=ceph-osd1
ceph osd crush add osd.3 1.0 host=ceph-osd1
ceph osd crush add osd.4 1.0 host=ceph-osd1
ceph osd crush add osd.5 1.0 host=ceph-osd1
ceph osd crush add osd.6 1.0 host=ceph-osd1
ceph osd crush add osd.7 1.0 host=ceph-osd1
ceph osd crush add osd.8 1.0 host=ceph-osd1
ceph osd crush add osd.9 1.0 host=ceph-osd1
ceph osd crush add osd.10 1.0 host=ceph-osd1
ceph osd crush add osd.11 1.0 host=ceph-osd1
ceph osd crush add osd.12 1.0 host=ceph-osd1
</codeph></codeblock>
<p>The following example is a sample output after executing the script:</p>
<codeblock><codeph>root@ftcceph1-osd1:/home/ceph# ./crush.sh
add item id 0 name 'osd.0' weight 1 at location {host=ftcceph1-osd1} to crush map
add item id 1 name 'osd.1' weight 1 at location {host=ftcceph1-osd1} to crush map
add item id 2 name 'osd.2' weight 1 at location {host=ftcceph1-osd1} to crush map
add item id 3 name 'osd.3' weight 1 at location {host=ftcceph1-osd1} to crush map
</codeph></codeblock>
<p>
  <b>Updating ceph.conf file for each of the OSD nodes</b>
</p>
<p>Update the <codeph>ceph.conf</codeph> file as shown below for each of the OSD nodes. Also add the [global] section in <codeph>ceph.conf</codeph> files with file parameter <codeph>tunable</codeph> as listed in the <xref type="section" href="#topic42821/ceph-tunning">tunable</xref>.</p>
<codeblock><codeph>[osd.0]
host = ceph-osd1

[osd.1]
host = ceph-osd1

[osd.2]
host = ceph-osd1
</codeph></codeblock>
<p>Copy the new <codeph>ceph.conf</codeph> file to all the other nodes in the Ceph cluster.</p>
<p>
  <b>Start OSD daemon</b>
</p>
<p>Start the OSD daemon by entering:</p>
<codeblock><codeph>/etc/init.d/ceph start osd.0
</codeph></codeblock>
<p>The above command must be started for each OSD daemon.</p>
<p>Example:</p>
<codeblock><codeph>/etc/init.d/ceph start osd.1
/etc/init.d/ceph start osd.2
</codeph></codeblock>
<p>Ensure OSD created is up and in by verifying output of following:</p>
<codeblock><codeph>ceph -w
</codeph></codeblock>
</section>
<section id="setting-up-additional-osd-nodes"> <title>Setting up additional OSD nodes</title>
<p>A healthy Ceph cluster needs at least 3 OSD nodes. (Follow steps similar to previous section to add additional nodes to form the quorum.) Update the <codeph>ceph.conf</codeph> file like that shown below before proceeding.</p>
<codeblock><codeph>[osd.0]
host = ceph-osd1
[osd.1]
host = ceph-osd1

..

[osd.3]
host = ceph-osd3
[osd.4]
host = ceph-osd4

..
</codeph></codeblock>
<p>Ensure Ceph health and status is <b>OK</b> by entering:</p>
<codeblock><codeph>ceph health
HEALTH_OK


ceph status

root@ceph-mon1:/home/ceph# ceph -s
 cluster 6a710689-5b19-4ba3-b2c5-c23ddd26dce9
 health HEALTH_OK
 monmap e1: 1 mons at {ceph-mon1=192.168.116.54:6789/0}, election epoch 1, quorum 0 ceph-mon1
 osdmap e336: 39 osds: 39 up, 39 in
 pgmap v106607: 11456 pgs, 17 pools, 7878 MB data, 1319 kobjects
   83315 MB used, 99199 GB / 99280 GB avail
    11456 active+clean
</codeph></codeblock>
<p>Repeat the above steps for each OSD node.</p>
</section>
<section id="osd-nodes-without-journal-partitions"> <title>OSD Nodes without journal partitions</title>
<p>Perform the following steps, if you do not plan or have disk for journal partitions.</p>
<p>
  <b>Running fdisk on OSD data  disk</b>
</p>
<p>Run <codeph>fdisk</codeph> on each of the OSD disk to create a partition. It should run on all the OSD node for each disk that is present.</p>
<p>For example: Execute <codeph>fdisk</codeph> on <codeph>/dev/sdc</codeph>, that is, 300 G in size.</p>
<ol>
<li>Log in to OSD as root user.</li>
<li>Change the directory to <codeph>/etc/ceph</codeph> and
          enter:<codeblock><codeph>#fdisk /dev/sdc

Welcome to fdisk (util-linux 2.25.2).
Changes will remain in memory only, until you decide to write them.
Be careful before using the write command.

Device does not contain a recognized partition table.
Created a new DOS disklabel with disk identifier 0x6a938816.

Command (m for help): n
Partition type
    p primary (0 primary, 0 extended, 4 free)
    e extended (container for logical partitions)
Select (default p): p
Partition number (1-4, default 1):
First sector (2048-585871963, default 2048):
Last sector, +sectors or +size{K,M,G,T,P} (2048-585871963, default 585871963):

Created a new partition 1 of type 'Linux' and of size 279.4 GiB.

Command (m for help): p
Disk /dev/sdc: 279.4 GiB, 299966445568 bytes, 585871964 sectors
Units: sectors of 1 * 512 = 512 bytes
Sector size (logical/physical): 512 bytes / 512 bytes
I/O size (minimum/optimal): 262144 bytes / 262144 bytes
Disklabel type: dos
Disk identifier: 0x6a938816

Device Boot Start End Sectors Size Id Type
/dev/sdc1 2048 585871963 585869916 279.4G 83 Linux

Command (m for help): w
The partition table has been altered

Calling ioctl() to re-read partition table.
Syncing disks.
</codeph></codeblock></li>
</ol>
<p>
  <b>Setting up the OSD Node</b>
</p>
<p>To create an OSD and add it to cluster and CRUSH map, perform the following steps:</p>
<ol>
<li>Log in to OSD host as root user. Connect to the OSD
          node.<codeblock><codeph>ssh [node-name]
</codeph></codeblock></li>
<li>Create the OSD and note the OSD
            number.<codeblock><codeph>ceph osd create
</codeph></codeblock><p>For multiple physical
            drives, execute the <codeph>ceph OSD create</codeph> command as many times as the number
            of OSDs.</p></li>
<li>Consider the output of the OSD number from the above step to create a default directory by
            entering:<codeblock><codeph>mkdir /var/lib/ceph/osd/ceph-{osd-number}
</codeph></codeblock><p>For
            example:</p><codeblock><codeph>mkdir /var/lib/ceph/osd/ceph-0
</codeph></codeblock><p>The
            following example displays the commands ran in the script to create 13 directory
            matching the OSD
          daemon.</p><codeblock><codeph>#!/bin/bash

mkdir /var/lib/ceph/osd/ceph-0
mkdir /var/lib/ceph/osd/ceph-1
mkdir /var/lib/ceph/osd/ceph-2
mkdir /var/lib/ceph/osd/ceph-3
mkdir /var/lib/ceph/osd/ceph-4
mkdir /var/lib/ceph/osd/ceph-5
mkdir /var/lib/ceph/osd/ceph-6
mkdir /var/lib/ceph/osd/ceph-7
mkdir /var/lib/ceph/osd/ceph-8
mkdir /var/lib/ceph/osd/ceph-9
mkdir /var/lib/ceph/osd/ceph-10
mkdir /var/lib/ceph/osd/ceph-11
mkdir /var/lib/ceph/osd/ceph-12
</codeph></codeblock></li>
<li>If the OSD is for a drive other than the OS drive, prepare it for use with Ceph, and mount it to
          the directory you just
          created:<codeblock><codeph>mkfs -t {fstype} /dev/{hdd}
[fs-type can be {ext4|xfs|btrfs}]
</codeph></codeblock></li>
</ol>
<p>Use <b>xfs</b> for better performance.</p>
</section>
<section id="script-to-create-xfs-file-system-on-partitioned-disks2"> <title>Script to create xfs file system on partitioned disks</title>
<p>The following example uses 13 drives in the system to create xfs file system.</p>
<codeblock><codeph>#!/bin/bash
mkfs.xfs -l logdev=/dev/sda5,size=2136997888 -f -i size=2048 /dev/sdb1
mkfs.xfs -l logdev=/dev/sda6,size=2136997888 -f -i size=2048 /dev/sdc1
mkfs.xfs -l logdev=/dev/sda7,size=2136997888 -f -i size=2048 /dev/sdd1
mkfs.xfs -l logdev=/dev/sda8,size=2136997888 -f -i size=2048 /dev/sde1
mkfs.xfs -l logdev=/dev/sda9,size=2136997888 -f -i size=2048 /dev/sdf1
mkfs.xfs -l logdev=/dev/sda10,size=2136997888 -f -i size=2048 /dev/sdg1
mkfs.xfs -l logdev=/dev/sda11,size=2136997888 -f -i size=2048 /dev/sdh1
mkfs.xfs -l logdev=/dev/sda12,size=2136997888 -f -i size=2048 /dev/sdi1
mkfs.xfs -l logdev=/dev/sda13,size=2136997888 -f -i size=2048 /dev/sdj1
mkfs.xfs -l logdev=/dev/sda14,size=2136997888 -f -i size=2048 /dev/sdk1
mkfs.xfs -l logdev=/dev/sda15,size=2136997888 -f -i size=2048 /dev/sdl1
mkfs.xfs -l logdev=/dev/sda16,size=2136997888 -f -i size=2048 /dev/sdm1
mkfs.xfs -l logdev=/dev/sda17,size=2136997888 -f -i size=2048 /dev/sdn1
mount -o user_xattr /dev/{hdd} /var/lib/ceph/osd/ceph-{osd-number}
</codeph></codeblock>
<p>Use the following command to mount</p>
<codeblock><codeph>mount -o user_xattr /dev/{hdd} /var/lib/ceph/osd/ceph-{osd-number}
</codeph></codeblock>
<p>The following example mounts 13 drives to journal partitions.</p>
<codeblock><codeph>#!/bin/bash
mount -t xfs -o logdev=/dev/sda5 /dev/sdb1 /var/lib/ceph/osd/ceph-0/
mount -t xfs -o logdev=/dev/sda6 /dev/sdc1 /var/lib/ceph/osd/ceph-1/
mount -t xfs -o logdev=/dev/sda7 /dev/sdd1 /var/lib/ceph/osd/ceph-2/
mount -t xfs -o logdev=/dev/sda8 /dev/sde1 /var/lib/ceph/osd/ceph-3/
mount -t xfs -o logdev=/dev/sda9 /dev/sdf1 /var/lib/ceph/osd/ceph-4/
mount -t xfs -o logdev=/dev/sda10 /dev/sdg1 /var/lib/ceph/osd/ceph-5/
mount -t xfs -o logdev=/dev/sda11 /dev/sdh1 /var/lib/ceph/osd/ceph-6/
mount -t xfs -o logdev=/dev/sda12 /dev/sdi1 /var/lib/ceph/osd/ceph-7/
mount -t xfs -o logdev=/dev/sda13 /dev/sdj1 /var/lib/ceph/osd/ceph-8/
mount -t xfs -o logdev=/dev/sda14 /dev/sdk1 /var/lib/ceph/osd/ceph-9/
mount -t xfs -o logdev=/dev/sda15 /dev/sdl1 /var/lib/ceph/osd/ceph-10/
mount -t xfs -o logdev=/dev/sda16 /dev/sdm1 /var/lib/ceph/osd/ceph-11/
mount -t xfs -o logdev=/dev/sda17 /dev/sdn1 /var/lib/ceph/osd/ceph-12/
</codeph></codeblock>
<p>
  <b>Initialize the OSD data directory</b>
</p>
      <note>Save the following commands to a file and save as <codeph>initialize.sh</codeph>. Make
        changes to the OSD number as per your configuration and run <codeph>chmod +x
          initialize.sh</codeph> and <codeph>./initialize.sh</codeph>.</note>
<p>To initialize the OSD data directory, enter:</p>
<codeblock><codeph>ceph-osd -i {osd-num} --mkfs -mkkey


The following example initializes 13 data directories.

#!/bin/bash
ceph-osd -i 0 --mkfs --mkkey
ceph-osd -i 1 --mkfs --mkkey
ceph-osd -i 2 --mkfs --mkkey
ceph-osd -i 3 --mkfs --mkkey
ceph-osd -i 4 --mkfs --mkkey
ceph-osd -i 5 --mkfs --mkkey
ceph-osd -i 6 --mkfs --mkkey
ceph-osd -i 7 --mkfs --mkkey
ceph-osd -i 8 --mkfs --mkkey
ceph-osd -i 9 --mkfs --mkkey
ceph-osd -i 10 --mkfs --mkkey
ceph-osd -i 11 --mkfs --mkkey
ceph-osd -i 12 --mkfs --mkkey
</codeph></codeblock>
<p>
  <b>Register OSD authentication key</b>
</p>
<p>To register the OSD authentication key, enter:</p>
<codeblock><codeph>ceph auth add osd.{osd-num} osd 'allow *' mon 'allow profile osd' -i /var/lib/ceph/osd/ceph-{osd-num}/keyring
</codeph></codeblock>
<p>The following example registers 13 OSD authentication keys.</p>
      <note>Save the following commands to a file and save as <codeph>auth.sh</codeph>.Update the
        entries based on your OSDs and run <codeph>chmod +x auth.sh</codeph> and
          <codeph>./auth.sh</codeph>.</note>
<codeblock><codeph>#!/bin/bash

ceph auth add osd.0 osd 'allow *' mon 'allow profile osd' -i /var/lib/ceph/osd/ceph-0/keyring
ceph auth add osd.1 osd 'allow *' mon 'allow profile osd' -i /var/lib/ceph/osd/ceph-1/keyring
ceph auth add osd.2 osd 'allow *' mon 'allow profile osd' -i /var/lib/ceph/osd/ceph-2/keyring
ceph auth add osd.3 osd 'allow *' mon 'allow profile osd' -i /var/lib/ceph/osd/ceph-3/keyring
ceph auth add osd.4 osd 'allow *' mon 'allow profile osd' -i /var/lib/ceph/osd/ceph-4/keyring
ceph auth add osd.5 osd 'allow *' mon 'allow profile osd' -i /var/lib/ceph/osd/ceph-5/keyring
ceph auth add osd.6 osd 'allow *' mon 'allow profile osd' -i /var/lib/ceph/osd/ceph-6/keyring
ceph auth add osd.7 osd 'allow *' mon 'allow profile osd' -i /var/lib/ceph/osd/ceph-7/keyring
ceph auth add osd.8 osd 'allow *' mon 'allow profile osd' -i /var/lib/ceph/osd/ceph-8/keyring
ceph auth add osd.9 osd 'allow *' mon 'allow profile osd' -i /var/lib/ceph/osd/ceph-9/keyring
ceph auth add osd.10 osd 'allow *' mon 'allow profile osd' -i /var/lib/ceph/osd/ceph-10/keyring
ceph auth add osd.11 osd 'allow *' mon 'allow profile osd' -i /var/lib/ceph/osd/ceph-11/keyring
ceph auth add osd.12 osd 'allow *' mon 'allow profile osd' -i /var/lib/ceph/osd/ceph-12/keyring
</codeph></codeblock>
<p>
  <b>Add OSD node to CRUSH map</b>
</p>
<p>Add OSD node to CRUSH map by entering:</p>
<codeblock><codeph>ceph osd crush add-bucket {hostname} host
</codeph></codeblock>
<p>The following example is the sample output of adding OSD node to CRUSH map:</p>
<codeblock><codeph>root@ftcceph1-osd1:/home/ceph# ceph osd crush add-bucket ftcceph1-osd1 host
    added bucket ftcceph1-osd1 type host to crush map
</codeph></codeblock>
<p>
  <b>Place OSD node under root default</b>
</p>
<p>Place the OSD node under root by default by entering:</p>
<codeblock><codeph>ceph osd crush move {hostname} root=default
</codeph></codeblock>
<p>The following example is the sample output of placing OSD node under root default:</p>
<codeblock><codeph>root@ftcceph1-osd1:/home/ceph# ceph osd crush move ftcceph1-osd1 root=default
moved item id -2 name 'ftcceph1-osd1' to location {root=default} in crush map
</codeph></codeblock>
<p>
  <b>Add OSD node to CRUSH map for receiving data</b>
</p>
<p>Add the OSD node to the CRUSH map for receiving data by entering:</p>
<codeblock><codeph>ceph osd crush add {id-or-name} {weight} [{bucket-type}={bucket-name} ...]
ceph osd crush add osd.0 1.0 host=ceph-osd1
</codeph></codeblock>
      <note>Save the following commands to a file and save as <codeph>crush.sh</codeph>. Update the
        hostname after <codeph>host=</codeph> and run <codeph>chmod +x crush.sh</codeph> and
          <codeph>./crush.sh</codeph>.</note>
<p>The following example adds 13 OSDs to the CRUSH map.</p>
<codeblock><codeph>#!/bin/bash

ceph osd crush add osd.0 1.0 host=ceph-osd1
ceph osd crush add osd.1 1.0 host=ceph-osd1
ceph osd crush add osd.2 1.0 host=ceph-osd1
ceph osd crush add osd.3 1.0 host=ceph-osd1
ceph osd crush add osd.4 1.0 host=ceph-osd1
ceph osd crush add osd.5 1.0 host=ceph-osd1
ceph osd crush add osd.6 1.0 host=ceph-osd1
ceph osd crush add osd.7 1.0 host=ceph-osd1
ceph osd crush add osd.8 1.0 host=ceph-osd1
ceph osd crush add osd.9 1.0 host=ceph-osd1
ceph osd crush add osd.10 1.0 host=ceph-osd1
ceph osd crush add osd.11 1.0 host=ceph-osd1
ceph osd crush add osd.12 1.0 host=ceph-osd1
</codeph></codeblock>
<p>The following example is a sample output after executing the script:</p>
<codeblock><codeph>root@ftcceph1-osd1:/home/ceph# ./crush.sh
add item id 0 name 'osd.0' weight 1 at location {host=ftcceph1-osd1} to crush map
add item id 1 name 'osd.1' weight 1 at location {host=ftcceph1-osd1} to crush map
add item id 2 name 'osd.2' weight 1 at location {host=ftcceph1-osd1} to crush map
add item id 3 name 'osd.3' weight 1 at location {host=ftcceph1-osd1} to crush map
</codeph></codeblock>
<p>
  <b>Updating ceph.conf file for each of the OSD nodes</b>
</p>
<p>Update the <codeph>ceph.conf</codeph> file as shown below for each of the OSD nodes. Also add the [global] section in <codeph>ceph.conf</codeph> files with file parameter <codeph>tunable</codeph> as listed in the <xref type="section" href="#topic42821/ceph-tunning">tunable</xref>.</p>
<codeblock><codeph>[osd.0]
host = ceph-osd1

[osd.1]
host = ceph-osd1

[osd.2]
host = ceph-osd1
</codeph></codeblock>
<p>Copy the new <codeph>ceph.conf</codeph> file to all the other nodes in the Ceph cluster.</p>
<p>
  <b>Start OSD daemon</b>
</p>
<p>Start the OSD daemon by entering:</p>
<codeblock><codeph>/etc/init.d/ceph start osd.0
</codeph></codeblock>
<p>The above command must be started for each OSD daemon.</p>
<p>Example:</p>
<codeblock><codeph>/etc/init.d/ceph start osd.1
/etc/init.d/ceph start osd.2
</codeph></codeblock>
<p>Ensure OSD created is up and in by verifying output of following:</p>
<codeblock><codeph>ceph -w
</codeph></codeblock>
</section>
<section id="setting-up-additional-osd-nodes2"> <title>Setting up additional OSD nodes</title>
<p>A healthy Ceph cluster needs at least 3 OSD nodes. (Follow steps similar to previous section to add additional nodes to form the quorum.) Update the <codeph>ceph.conf</codeph> file like that shown below before proceeding.</p>
<codeblock><codeph>[osd.0]
host = ceph-osd1
[osd.1]
host = ceph-osd1

..

[osd.3]
host = ceph-osd3
[osd.4]
host = ceph-osd4

..
</codeph></codeblock>
<p>Ensure Ceph health and status is OK by entering:</p>
<codeblock><codeph>ceph health
HEALTH_OK


ceph status

root@ceph-mon1:/home/ceph# ceph -s
 cluster 6a710689-5b19-4ba3-b2c5-c23ddd26dce9
 health HEALTH_OK
 monmap e1: 1 mons at {ceph-mon1=192.168.116.54:6789/0}, election epoch 1, quorum 0 ceph-mon1
 osdmap e336: 39 osds: 39 up, 39 in
 pgmap v106607: 11456 pgs, 17 pools, 7878 MB data, 1319 kobjects
   83315 MB used, 99199 GB / 99280 GB avail
    11456 active+clean
</codeph></codeblock>
</section>
<section id="file-system-tunning"> <title>File system tuning</title>
<p>XFS Journal Partitions best reside on SSDs, with a discrete path and a Controller Cache. Additionally, specify One partition per data, and JBOD for the SSDs, with a ratio of one SSD to every four Data Partitions. Each XFS Data partition should be configured with rw, noatime, attr2, inode64, noquota. Additionally, you should also consider: nobarrier, logbsize=256k, logbufs=8, allocsize=4m.</p>
<p>For long running Ceph Clusters, XFS fragmentation is useful to monitor and correct.</p>
<codeblock><codeph>Fragmentation on /dev/sdb1 - osd3
actual 22722, ideal 22557, fragmentation factor 0.73%
Example - checks the fragmentation on 13 osd daemons.

#!/bin/sh
echo "Fragmentation on /dev/sdb1 - osd1"
xfs_db -c frag -r /dev/sdb1
echo "Fragmentation on /dev/sdc1 - osd1"
xfs_db -c frag -r /dev/sdc1
echo "Fragmentation on /dev/sdd1 - osd1"
xfs_db -c frag -r /dev/sdd1
echo "Fragmentation on /dev/sde1 - osd1"
xfs_db -c frag -r /dev/sde1
echo "Fragmentation on /dev/sdf1 - osd1"
xfs_db -c frag -r /dev/sdf1
echo "Fragmentation on /dev/sdg1 - osd1"
xfs_db -c frag -r /dev/sdg1
echo "Fragmentation on /dev/sdh1 - osd1"
xfs_db -c frag -r /dev/sdh1
echo "Fragmentation on /dev/sdi1 - osd1"
xfs_db -c frag -r /dev/sdi1
echo "Fragmentation on /dev/sdj1 - osd1"
xfs_db -c frag -r /dev/sdj1
echo "Fragmentation on /dev/sdk1 - osd1"
xfs_db -c frag -r /dev/sdk1
echo "Fragmentation on /dev/sdl1 - osd1"
xfs_db -c frag -r /dev/sdl1
echo "Fragmentation on /dev/sdm1 - osd1"
xfs_db -c frag -r /dev/sdm1
echo "Fragmentation on /dev/sdn1 - osd1"
xfs_db -c frag -r /dev/sdn1
</codeph></codeblock>
</section>
<section id="ceph-tunning"> <title>Ceph tuning</title>
<p>Ceph configuration file is shown below:</p>
<p>Add the below file tunable parameters to support OSD in the <b>[global]</b> section on <codeph>ceph.conf</codeph> file.</p>
<codeblock><codeph>[global]

fsid = xxxxxxxxxx

mon_initial_members = ceph-mon1
mon_host = xxxx
auth_cluster_required = cephx
auth_service_required = cephx
auth_client_required = cephx
filestore_xattr_use_omap = true
#osd journal size = 2048
osd pool default size = 3
osd pool default min size = 2
#osd pool default pg num = 333
#osd pool default pgp num = 333
#osd crush chooseleaf type = 1


#Added for perf tuning
osd_op_threads = 8
filestore_queue_max_bytes = 536870912
filestore_queue_max_ops = 2000
filestore_queue_committing_max_ops = 2000
filestore_queue_committing_max_bytes = 536870912

[mon.ceph-mon1]
host = ceph-mon1
mon addr = xxxx

[client.admin]
keyring = /etc/ceph/ceph.client.admin.keyring

[client.glance]
keyring = /etc/ceph/ceph.client.glance.keyring

[client.cinder]
keyring = /etc/ceph/ceph.client.cinder.keyring

[client.nova]
keyring = /etc/ceph/ceph.client.nova.keyring

[client.radosgw.ceph-admin]
host = ceph-admin
keyring = /etc/ceph/ceph.client.radosgw.keyring
rgw socket path = /var/run/ceph/ceph.radosgw.gateway.fastcgi.sock
log file = /var/log/ceph/client.radosgw.gateway.log
rgw dns name = ceph-admin
rgw print continue = false

[client.radosgw.ceph-gateway2]
host = ceph-gateway2
keyring = /etc/ceph/ceph.client.radosgw.keyring
rgw socket path = /var/run/ceph/ceph.radosgw.gateway.fastcgi.sock
log file = /var/log/ceph/client.radosgw.gateway.log
rgw dns name = ceph-gateway2
rgw print continue = false

[osd.0]
host = ceph-osd1

[osd.1]
host = ceph-osd1

[osd.2]
host = ceph-osd1

[osd.3]
host = ceph-osd1

...osd.4-37...
[osd.38]
host = ceph-osd3
</codeph></codeblock>
<p>
  <b>Relevant tuning parameters</b>
</p>
<p>Following are the relevant tuning parameters:</p>
<codeblock><codeph>osd op threads = 8
osd max backfills = 1
osd recovery max active = 1
filestore max sync interval = 100
filestore min sync interval = 50
filestore queue max ops = 2000
filestore queue max bytes = 536870912
filestore queue committing max ops = 2000
filestore queue committing max bytes = 536870912
</codeph></codeblock>
</section>
<section id="setting-up-additional-nodes"> <title>Setting up additional monitor nodes</title>
<p>Ceph monitors are light-weight processes that maintains a master copy of the cluster map. For production clusters, we recommend at least 3 monitors and always advisable to run with odd number of monitors. And odd-number of monitors has a higher resiliency to failures than the even number monitors.</p>
<p>On a two monitor deployment, no failures can be tolerated to maintain a quirum but with 3 monitors, one failure  can be tolerated and with 5 monitors, two failures can be tolerated. Since, monitors are light-weight, it is possible to run on same host but it is recommended to run them on separate hosts because fsync issues with kernel may impair performance. Deploy your hardware and install all the required software. Follow the similar steps as how other Ceph cluster nodes are deployed.</p>
<p>For more details refer to <xref href="http://docs.ceph.com/docs/master/rados/operations/add-or-rm-mons/" scope="external" format="html" >http://docs.ceph.com/docs/master/rados/operations/add-or-rm-mons/</xref>
</p>
</section>
<section id="adding-a-monitor-manual"> <title>Adding a monitor (manual)</title>
<p>The following steps create a <codeph>ceph-mon</codeph> data directory, retrieves the monitor keyring and monmap, and adds a <codeph>ceph-mon</codeph> daemon to your cluster. You can add more monitors by repeating these steps to achieve a quorum.</p>
<ol>
<li>Create a default directory on the second monitor
            machine<codeblock><codeph>ssh {new-mon-host}
sudo mkdir /var/lib/ceph/mon/ceph-{mon-id}
</codeph></codeblock><p>For
            example:</p><codeblock><codeph># mkdir /var/lib/ceph/mon/ceph-ftcceph1-mon2
</codeph></codeblock></li>
<li>Create a temporary directory {temp} to keep files need during the process. This directory is
          different from the monitor's default
            directory.<codeblock><codeph>mkdir {tmp}
</codeph></codeblock><p>For
          example:</p><codeblock><codeph>mkdir tmp
</codeph></codeblock></li>
<li>Retrieve the keyring for your
            monitor.<codeblock><codeph>ceph auth get mon. -o {tmp}/{key-filename}
{tmp} -&gt;is the path to the keyring
{key-filename} is the name of the file that has monitor key.
</codeph></codeblock><p>For
            example:</p><codeblock><codeph>ceph auth get mon. -o tmp/mon2keyring
</codeph></codeblock><p>Expected
            output</p><codeblock><codeph>exported keyring for mon.
</codeph></codeblock></li>
<li>Retrieve the monitor map. ceph mon getmap -o {tmp}/{map-filename} {tmp} is the path to the
          monitor map {map-filename} is the name of the file containing the monitor map For example:
          ceph mon getmap -o tmp/mon2monmap<p>Expected
          output</p><codeblock><codeph>got monmap epoch 1
</codeph></codeblock></li>
<li>Prepare the monitor's data directory created
            earlier.<codeblock><codeph>sudo ceph-mon -i {mon-id} --mkfs --monmap {tmp}/{map-filename} --keyring {tmp}/{key-filename}
</codeph></codeblock><p>For
            example:</p><codeblock><codeph>ceph-mon -i ftcceph1-mon2 --mkfs --monmap tmp/mon2monmap --keyring tmp/mon2keyring
</codeph></codeblock><p>Expected
            output</p><codeblock><codeph>ceph-mon: set fsid to 328d1702-67e7-4dfa-a0a9-77f0b96be57f
ceph-mon: created monfs at /var/lib/ceph/mon/ceph-ftcceph1-mon2 for mon.ftcceph1-mon2
</codeph></codeblock></li>
<li>Update <codeph>/etc/ceph/ceph.conf</codeph> file with the new monitor node details and push the
          updated <codeph>ceph.conf</codeph> file to all the nodes in the Ceph cluster. [mon.mon-id]
          host = new-mon-host addr =
          ip-addr:6789<p>Sample</p><codeblock><codeph>[mon.ftcceph1-mon2]
host = ftcceph1-mon2
mon addr = 192.168.51.52:6789
</codeph></codeblock></li>
<li>Add the new monitor to the list of monitors and this will enable other nodes to use this monitor
          during start up. This command is run from the new added
            monitor.<codeblock><codeph>ceph mon add &lt;mon-id&gt; &lt;ip&gt;[:&lt;port&gt;]
</codeph></codeblock><p>sample</p><codeblock><codeph>ceph mon add ftcceph1-mon2 192.168.51.52:6789
added mon.ftcceph1-mon2 at 192.168.51.52:6789/0
</codeph></codeblock></li>
<li>Start the new monitor so it will automatically join the
            cluster.<codeblock><codeph>ceph-mon -i {mon-id} --public-addr {ip:port}
</codeph></codeblock><p>sample</p><codeblock><codeph> ceph-mon -i ftcceph1-mon2 --public-addr 192.168.51.52:6789
</codeph></codeblock></li>
</ol>
<p>Follow the above steps for the 3rd monitor. Once the 3rd monitor is added to the cluster, run <codeph>ceph-w</codeph> and check the status of health state as "OK". A <codeph>HEALTH_WARN</codeph> clock skew message appears if the time is not in sync across all the Ceph cluster. It is always good to set NTP setup on all the ceph nodes so that the time is in sync.</p>
<p>The following is the sample output of <codeph>HEALTH_WARN</codeph> clock skew warns</p>
<codeblock><codeph>root@ftcceph1-mon3:/home/ceph# ceph -w
 cluster 328d1702-67e7-4dfa-a0a9-77f0b96be57f
 health HEALTH_WARN clock skew detected on mon.ftcceph1-mon2, mon.ftcceph1-mon3
 monmap e3: 3 mons at {ftcceph1-mon1=192.168.51.51:6789/0,ftcceph1-mon2=192.168.51.52:6789/0,ftcceph1-mon3=192.168.51.53:6789/0}, election epoch 6, quorum 0,1,2 ftcceph1-mon1,ftcceph1-mon2,ftcceph1-mon3
osdmap e122: 21 osds: 21 up, 21 in
pgmap v2845: 3256 pgs, 17 pools, 3783 bytes data, 69 objects
  22389 MB used, 5844 GB / 5866 GB avail
   3256 active+clean
</codeph></codeblock>
</section>
<section id="steps-to-setup-ntp-server"> <title>Steps to setup NTP server</title>
<ol>
<li>Install NTP package. This package is a part of the image deployment.</li>
<li>Update <codeph>/etc/ntp.conf</codeph> file with your local NTP server across all the nodes in the Ceph cluster.</li>
<li>Once <codeph>ntp.conf</codeph> file is updated, restart ntp service by running <codeph>/etc/init.d/ntp restart6</codeph>
</li>
<li>Run <codeph>ntpq -p</codeph> to see the ntp server is
          listed.<codeblock><codeph>ntpq -p
remote refid st t when poll reach delay offset jitter
=================================================
192.168.51.21 .INIT. 16 u - 64 0 0.000 0.000 0.000
</codeph></codeblock></li>
</ol>
<p>Sample <codeph>ntp.conf</codeph> file</p>
      <note>Commented out server and added local ntp server and restrict under cryptographically
        authenticated section.</note>
<codeblock><codeph>==========================================
# /etc/ntp.conf, configuration for ntpd; see ntp.conf(5) for help
driftfile /var/lib/ntp/ntp.drift

# Enable this if you want statistics to be logged.
#statsdir /var/log/ntpstats/

statistics loopstats peerstats clockstats
filegen loopstats file loopstats type day enable
filegen peerstats file peerstats type day enable
filegen clockstats file clockstats type day enable

# You do need to talk to an NTP server or two (or three).
#server ntp.your-provider.example
# pool.ntp.org maps to about 1000 low-stratum NTP servers. Your server will
# pick a different set every time it starts up. Please consider joining the
# pool: &lt;http://www.pool.ntp.org/join.html&gt;
#server 0.debian.pool.ntp.org iburst
#server 1.debian.pool.ntp.org iburst
#server 2.debian.pool.ntp.org iburst
#server 3.debian.pool.ntp.org iburst
server 192.168.51.21

# Access control configuration; see /usr/share/doc/ntp-doc/html/accopt.html for
# details. The web page &lt;http://support.ntp.org/bin/view/Support/AccessRestrictions&gt;
67
# might also be helpful.
#
# Note that "restrict" applies to both servers and clients, so a configuration
# that might be intended to block requests from certain clients could also end
# up blocking replies from your own upstream servers.

# By default, exchange time with everybody, but do not allow configuration.
restrict -4 default kod notrap nomodify nopeer noquery
restrict -6 default kod notrap nomodify nopeer noquery

# Local users may interrogate the ntp server more closely.
restrict 127.0.0.1
restrict ::1

# Clients from this (example!) subnet have unlimited access, but only if
# cryptographically authenticated.
#restrict 192.168.123.0 mask 255.255.255.0 notrust
restrict 192.168.51.0 mask 255.255.255.0 notrust

# If you want to provide time to your local subnet, change the next line.
# (Again, the address is an example only.)
#broadcast 192.168.123.255

# If you want to listen to time broadcasts on your local subnet, de-comment the
# next lines. Please do this only if you trust everybody on the network!
==========================================================
</codeph></codeblock>
<p>Once the ntp server is configured and after the time is in sync across all the servers, the Ceph health will be in "OK" state.</p>
<codeblock><codeph>ceph -w
 cluster 328d1702-67e7-4dfa-a0a9-77f0b96be57f
 health HEALTH_OK
 monmap e3: 3 mons at {ftcceph1-mon1=192.168.51.51:6789/0,ftcceph1-mon2=192.168.51.52:6789/0,ftcceph1-mon3=192.168.51.53:6789/0}, election epoch 12, quorum 0,1,2 ftcceph1-mon1,ftcceph1-mon2,ftcceph1-mon3
 osdmap e122: 21 osds: 21 up, 21 in
 pgmap v3238: 3256 pgs, 17 pools, 3783 bytes data, 69 objects
   22384 MB used, 5844 GB / 5866 GB avail
    3256 active+clean
</codeph></codeblock>
</section>
<section id="next-steps"> <title>Next Steps</title>
<p>
<xref href="../../../commercial/GA1/ceph/1.1commercia.ceph-high-availability-RADOSGW-authentication.dita" >Ceph Authentication</xref>.</p>
<p>
  <xref href="#topic42821"> Return to Top </xref>
</p>
</section>
</body>
</topic>
