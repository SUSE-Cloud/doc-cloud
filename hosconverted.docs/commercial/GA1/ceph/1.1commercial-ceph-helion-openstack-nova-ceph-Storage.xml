<?xml version="1.0" encoding="UTF-8"?>
<!--Edit status: not edited-->
<!DOCTYPE topic
  PUBLIC "-//OASIS//DTD DITA Topic//EN" "http://docs.oasis-open.org/dita/v1.1/OS/dtd/topic.dtd" ><topic xml:lang="en-us" id="topic2174">
<title>HPE Helion <tm tmtype="reg">OpenStack</tm> 1.1 and 1.1.1: Ceph Nova Storage</title>
<titlealts>
<searchtitle>HPE Helion Openstack 1.1: Ceph Nova Storage</searchtitle>
</titlealts>
<prolog>
<metadata>
<othermeta name="layout" content="default"/>
<othermeta name="product-version" content="HPE Helion Openstack"/>
<othermeta name="product-version" content="HPE Helion Openstack 1.1"/>
<othermeta name="role" content="Storage Engineer"/>
<othermeta name="role" content="Storage Architect"/>
<othermeta name="role" content="Storage Administrator"/>
<othermeta name="role" content="Storage Engineer"/>
<othermeta name="role" content="Service Developer"/>
<othermeta name="role" content="Cloud Administrator"/>
<othermeta name="role" content="Application Developer"/>
<othermeta name="role" content="Paul F"/>
<othermeta name="product-version1" content="HPE Helion Openstack"/>
<othermeta name="product-version2" content="HPE Helion Openstack 1.1"/>
</metadata>
</prolog>
<body>
<p>
<!--PUBLISHED-->
 <!--./commercial/GA1/ceph/1.1commercial-ceph-helion-openstack-nova-ceph-Storage.md-->
 <!--permalink: /helion/openstack/1.1/ceph-helion-openstack-nova-ceph-storage/--></p>
<p>The following steps are automatically performed when you download and execute <codeph>Ceph_client</codeph> script from the controller and compute nodes as per the readme file in the tar file.</p>
<ol>
<li>Verify Ceph
                                health.<codeblock><codeph>ceph health
</codeph></codeblock><p>Output:</p><codeblock><codeph>HEALTH_OK
</codeph></codeblock></li>
<li>Create a new pool for the Cinder
                                        volume.<codeblock><codeph>ceph osd pool create &lt;cinder pool name&gt; &lt;pg-num&gt;
</codeph></codeblock><p>For
                                        example:</p><codeblock><codeph>ceph osd pool create helion-ceph-nova 100
</codeph></codeblock></li>
<li>Verify that a new pool was
                                        created.<codeblock><codeph>rados lspools
</codeph></codeblock><p>Output:</p><codeblock><codeph>rbd ls helion-ceph-nova
</codeph></codeblock></li>
<li>
<p>Create symbolic or softlinks to the relevant files on the overcloud controller nodes.</p>

<codeblock><codeph>ln -s /usr/lib/python2.7/dist-packages/rados* /opt/stack/venvs/openstack/lib/python2.7/site-packages/.
ln -s /usr/lib/python2.7/dist-packages/rbd* /opt/stack/venvs/openstack/lib/python2.7/site-packages/.
ln -s /usr/lib/python2.7/dist-packages/rados* /opt/stack/venvs/nova/lib/python2.7/site-packages
ln -s /usr/lib/python2.7/dist-packages/rbd* /opt/stack/venvs/nova/lib/python2.7/site-packages
</codeph></codeblock>
</li>
</ol>
<section id="configure-nova"> <title>Configure Nova</title>
<p>Configure Nova to boot all Virtual Machines directly to Ceph. Edit <codeph>nova.conf</codeph> on each overcloud controller and compute node.</p>
<p>Perform the following steps:</p>
<ol>
<li>Edit <codeph>/etc/nova/nova.conf</codeph> and add the following options in the respective
                                        sections:<codeblock><codeph># For Ceph integration
[DEFAULT]
libvirt_inject_password=false
libvirt_inject_key=false
libvirt_inject_partition=-2

The last libvirt section in the conf file
[libvirt]

#Ceph Related Changes
images_type=rbd
images_rbd_pool=helion-ceph-nova
images_rbd_ceph_conf=/etc/ceph/ceph.conf
</codeph></codeblock></li>
<li>Restart the Nova services:<ul>
                                                <li>
                                                  <p>From the compute node</p>
                                                  <codeblock><codeph>service nova-compute restart
</codeph></codeblock>
                                                </li>
                                                <li>
                                                  <p>From all controller nodes:</p>
                                                  <codeblock><codeph>service nova-api restart
service nova-scheduler restart
service nova-conductor restart
</codeph></codeblock>
                                                </li>
                                        </ul></li>
<li>From the overcloud controller node0, execute the following
                                        command:<codeblock><codeph>source /root/stackrc
</codeph></codeblock></li>
</ol>
<!--
[[end of nova flow]]

2. List the existing VM

        nova list 
3. List the existing Glance image

        glance index 
4. Display the Nova list

        rados lspools

5. List Ceph resource

        ceph df

6. Execute the following command

        ceph -w

7. Create an instance with RBD backend.
   
        nova boot -?-?flavor <flavor name or id>  -?-?image <image name or id>  -?-?key-name <key name>  <instance name>  -?-?nic net-id=<id>

8. List all the Nova instances

        nova list

9. Execute the following command

        ceph -w

You can also view the instance from Horizon overcloud dashboard.
-->
<!--###Attaching the Cinder volume to the Nova instance

There are two ways to attach a Cinder volume to a Nova instance.

* Use the Horizon overcloud dashboard
* Use the Command Line Interface (CLI)


**Attaching a Cinder volume from Horizon overcloud dashboard**

To attach a Cinder volume to Nova instance [OpenStack User Guide](http://docs.openstack.org/user-guide/content/dashboard_manage_volumes.html)
-->
<!--
1. In the Horizon Dashboard, click the **Project** Tab.  
2. Click **Compute** and then **Volume** to open the Volume page.
3. Click the **More Action** tab, and select **Edit Attachments**.  
4. Click the **Attach to Instance** drop-down list and select the instance. 
5. In the **Device Name** box, enter the name of the selected instance.
6. Click **Attach Volume** to attach the Cinder volume to the Nova instance. To undo these changes, click **Cancel**.


**Listing the instance using Horizon**:

1. From the Horizon Dashboard, click the **Project** Tab. 
2. Click **Compute** and then select **Instance** to open Volume page. You can view the all the instances and the console log from the Horizon UI.

-->
<!-- **Attaching a Cinder volume using the Command Line Interface (CLI)**

To attach a Cinder volume to a Nova instance:

1. SSH into the seed VM.

        ssh <seed IP address>


2. Log in to the  overcloud.

        ssh heat-admin@<overcloud IP address> 

3. From overcloud controller node0, enter:

        source /root/stackrc 

4. List the existing VMs:

        nova list

5. List the Glance image:

        glance index


6. Execute the following command to attach volume to a nova instance

        nova volume-attach <server> <volume> [<device>]

8. List Nova:

        rados lapools

7. List the Ceph resource:

        ceoh df
        ceph -w

9. To create an instance with the RADOS block device (RDB) back end, enter:

**[[command required?**


10. To list all Nova instances, enter:
        
        nova list


11. To verify status, enter:

        ceph -w


12. From the KVM host, log into the instance:

**commands required?**


    The output displays the instances where the Cinder volume is attached.

-->
<!-- #####Verifying the Cinder volume attachment to a Nova instance

The Cinder volume is now attached to a Nova instance. Verify the attachment by performing the following steps: 

1. To view all the volumes, enter:

        # cinder list


2. To view the Nova instance, enter:

        nova list 

3. To view the details of the attached volume, enter:

        # cinder show  <volume ID>

    For example:

        cinder show 580d3e95-970f-4a9c-92ea-284799dcbc82

    Output:

        
        | Property                             | Value                                                                                                                           |
        

        | attachments                          | [{u'device': u'/dev/vde', u'server_id': u'd6c98de0-b65e-4e43-bd5e-04c81ad26cd1', u'id': u'580d3e95-970f-4a9c-92ea-284799dcbc82',                                            u'host_name': None, u'volume_id': u'580d3e95-970f-4a9c-92ea-284799dcbc82'}]                                                     |
        | availability_zone                    | nova                                                                                                                            |
        | bootable                             | false                                                                                                                           |
        | created_at                           | 2014-08-13T03:38:27.000000                                                                                                      |
        | display_description                  | None                                                                                                                            |
        | display_name                         | volume2_RBD                                                                                                                     |
        | encrypetd                            | False                                                                                                                           |
        |  id                                  | 580d3e95-970f-4a9c-92ea-284799dcbc82                                                                                            |
        | metadata                             | {u'readonly': u'False', u'attached_mode': u'rw'}                                                                                |
        | os-vol-host-attr:host                | overcloud-controller1-thg43e77ptei                                                                                              |
        | os-vol-mig-status-attr:migstat       | None                                                                                                                            |
        | os-vol-mig-status-attr:name_id       | None                                                                                                                            | 
        | os-vol-tenant-attr:tenant_id         | 98ae295c1958428a890cf6441d70db08                                                                                                | 
        | size                                 | 2                                                                                                                               |  
        | snapshot_id                          | None                                                                                                                            |
        | source_volid                         | None                                                                                                                            |
        | status                               | in-use                                                                                                                          | 
        | volume_type                          | None                                                                                                                            |  
        



5. To view the details of the Nova instance, enter:

        nova show < nova instance ID> 

    For example:
 
        # nova show d6c98de0-b65e-4e43-bd5e-04c81ad26cd1

    Output:


        
        | Property                             | Value                                                                                                                           
        
        | OS-EXT-AZ:availability_zone          | nova                                                                           |                                                
        | OS-EXT-SRV-ATTR:host                 | overcloud-novacompute0-k3kakatgtgb2                                            |                                                
        | OS-EXT-SRV-ATTR:hypervisor_hostname  | overcloud-novacompute0-k3kakatgtgb2.novalocal                                  |                                                
        | OS-EXT-SRV-ATTR:instance_name        | instance-00000087                                                              |                                                
        | OS-EXT-STS:power_state               | 1                                                                              |                                                   
        | OS-EXT-STS:task_state                | -                                                                              |                                                
        | OS-EXT-STS:vm_state                  | active                                                                         |                                                
        | accessIPv4                           |                                                                                |                                           
        | accessIPv6                           |                                                                                |
        | config_drive                         |                                                                                |
        | created                              | 2014-08-12T23:43:50Z                                                           |
        | default-net network                  | 10.0.0.43, 192.168.100.108                                                     |
        | flavor                               | m1.tiny (1)                                                                    |
        | hostId                               | cf6bb4eb58517b0e06246628e3d0559267a2594c06ea44100e2fae1e                       |
        | id                                   | d6c98de0-b65e-4e43-bd5e-04c81ad26cd1                                           |
        | image                                | debian-wheezy-server-amd64-disk (39565ba5-bfe7-4ee7-be2b-abab70eeb989)         |
        | key_name                             | default                                                                        |
        | metadata                             | {}                                                                             |
        | name                                 | vm1                                                                            |
        | progress                             | 0                                                                              |
        | security_groups                      | default                                                                        |
        | status                               | ACTIVE                                                                         |
        | tenant_id                            | 98ae295c1958428a890cf6441d70db08                                               |
        | updated                              | 2014-08-12T23:44:23Z                                                           |
        | user_id                              | 835261faa1454b56bfab6cd07edfd433                                               |   
        

-->
<!-- #####Verify Cinder to check the usage of RBD backend 


The following procedure verifies the usage of RBD backend from cinder. It is verified from VM.

1. Debian version of instance

        uname -a


To list a device file after attaching a volume to an instance - vdc is the block device from RBD
**[[command**

Listing pools
**<screen shot>**

Debian version on the instance
<screen shot>

Creating ext4 file system on vdc 1
**<screen shot>**

Mounting vdc
**<screen shot>**

vdc volume size after copying cirros img 1

**<screen shot>**

-->
</section>
<section id="next-steps"> <title>Next Steps</title>
<p>
  <xref href="../../../commercial/GA1/ceph/1.1commercial.ceph-rados-gateway.xml" >Ceph RADOS Gateway</xref>
</p>
<p>
  <xref href="#topic2174"> Return to Top </xref>
</p>
</section>
</body>
</topic>
