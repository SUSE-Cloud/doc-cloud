<?xml version="1.0" encoding="UTF-8"?>
<!--Edit status: not edited-->
<!DOCTYPE topic
  PUBLIC "-//OASIS//DTD DITA Topic//EN" "http://docs.oasis-open.org/dita/v1.1/OS/dtd/topic.dtd" ><topic xml:lang="en-us" id="topic11650">
<title>HPE Helion <tm tmtype="reg">OpenStack</tm> 1.1 and 1.1.1: Ceph Administration Services</title>
<titlealts>
<searchtitle>HPE Helion Openstack HPE Helion Ceph Administration Services</searchtitle>
</titlealts>
<prolog>
<metadata>
<othermeta name="layout" content="default"/>
<othermeta name="product-version" content="HPE Helion Openstack"/>
<othermeta name="product-version" content="HPE Helion Openstack 1.1"/>
<othermeta name="role" content="Storage Engineer"/>
<othermeta name="role" content="Storage Architect"/>
<othermeta name="role" content="Storage Administrator"/>
<othermeta name="role" content="Storage Engineer"/>
<othermeta name="role" content="Service Developer"/>
<othermeta name="role" content="Cloud Administrator"/>
<othermeta name="role" content="Application Developer"/>
<othermeta name="role" content="Paul F, Binamra S"/>
<othermeta name="product-version1" content="HPE Helion Openstack"/>
<othermeta name="product-version2" content="HPE Helion Openstack 1.1"/>
</metadata>
</prolog>
<body>
<p>
<!--PUBLISHED-->
 <!--./commercial/GA1/ceph/1.1commercial.ceph-helion-openstack-ceph-storage-administration.md-->
 <!--permalink: /helion/openstack/1.1/ceph-helion-openstack-ceph-administration-services/--></p>
<p>The following sections include useful commands and approaches to mange and administrator a growing Ceph cluster.</p>
<section id="block-device-commands"> <title>Block Device Commands</title>
<p>The <codeph>rbd</codeph> command enables you to create, list, inspect and remove block device images. You can also use it to clone images, create snapshots, rollback an image to a snapshot, view a snapshot, etc.</p>
<p>To use Ceph Block Device commands, you must have access to a running Ceph cluster.</p>
</section>
<section id="creating-a-block-device-name"> <title>Creating a block device name</title>
<p>Before adding a block device to a node, you must first create an image for it in the Ceph Storage Cluster. To create a block device image, enter:</p>
<codeblock><codeph>rbd create {image-name} --size {megabytes} --pool {pool-name}
</codeph></codeblock>
<p>For example, to create a 1GB image named image1 that stores information in a pool named imagelist, enter:</p>
<codeblock><codeph>rbd create image1 --size 1024
rbd create bar --size 1024 --pool imagelist
</codeph></codeblock>
      <note>You must first create a pool before you can specify it as a source.</note>
</section>
<section id="listing-block-device-images"> <title>Listing block device images</title>
<ul>
<li>
<p>To list block devices in the <codeph>rbd</codeph> pool, (where rbd is the default pool name), enter:</p>

<p>rbd ls</p>
</li>
<li>
<p>To list block devices in a particular pool, execute the following command:</p>

<p>rbd ls {poolname}</p>
</li>
</ul>
<p>For example:</p>
<codeblock><codeph>rbd ls imagelist
</codeph></codeblock>
</section>
<section id="retrieving-image-information"> <title>Retrieving image information</title>
<ul>
<li>
<p>To retrieve information from a particular image, enter:</p>

<p>rbd --image {image-name} info</p>
</li>
</ul>
<p>For example:</p>
<codeblock><codeph>rbd --image image1 info
</codeph></codeblock>
<ul>
<li>
<p>To retrieve information from an image within a pool, enter:</p>

<p>rbd --image {image-name} -p {pool-name} info</p>
</li>
</ul>
<p>For example:</p>
<codeblock><codeph>rbd --image lin -p imagelist info
</codeph></codeblock>
</section>
<section id="resizing-a-block-device-image"> <title>Resizing a block device image</title>
<p>Ceph block device images are thin provisioned. They do not actually use any physical storage until you begin saving data to them. However, they do have a maximum capacity that you set with the <codeph>--size</codeph> option. If you want to increase (or decrease) the maximum size of a Ceph block device image, enter:</p>
<codeblock><codeph>rbd resize --image image1 --size 2048
</codeph></codeblock>
</section>
<section id="removing-a-block-device-image"> <title>Removing a block device image</title>
<p>To remove a block device, enter:</p>
<codeblock><codeph>rbd rm {image-name}
</codeph></codeblock>
<p>For example:</p>
<codeblock><codeph>rbd rm image1
</codeph></codeblock>
<p>To remove a block device from a pool, enter:</p>
<codeblock><codeph>rbd rm {image-name} -p {pool-name}
</codeph></codeblock>
<p>For example:</p>
<codeblock><codeph>rbd rm lin -p imagelist
</codeph></codeblock>
</section>
<section id="kernel-module-operations"> <title>Kernel module operations</title>
<p>
<b>Important</b> - To use kernel module operations, you must have a running Ceph cluster.</p>
</section>
<section id="get-a-list-of-images"> <title>Get a list of images</title>
<p>To mount a block device image, return a list of the images.</p>
<codeblock><codeph>rbd list
</codeph></codeblock>
</section>
<section id="map-a-block-device"> <title>Map a block device</title>
<p>Use <codeph>rbd</codeph> to map an image name to a kernel module. You must specify the image name, the pool name, and the user name. If it is not already loaded, <codeph>rbd</codeph> loads the RBD kernel module.</p>
<codeblock><codeph>sudo rbd map {image-name} --pool {pool-name} --id {user-name}
</codeph></codeblock>
<p>For example:</p>
<codeblock><codeph>sudo rbd map --pool rbd myimage --id admin
</codeph></codeblock>
<p>If you use cephx authentication, you must also specify a secret. It may come from a keyring or a file containing the secret.</p>
<codeblock><codeph>sudo rbd map --pool rbd myimage --id admin --keyring /path/to/keyring
sudo rbd map --pool rbd myimage --id admin --keyfile /path/to/file
</codeph></codeblock>
</section>
<section id="show-mapped-block-devices"> <title>Show Mapped block devices</title>
<p>To show block device images mapped to kernel modules with the <codeph>rbd</codeph> command, use the showmapped option.</p>
<codeblock><codeph>rbd showmapped
UNMAPPING A BLOCK DEVICE
</codeph></codeblock>
<p>To unmap a block device image with the <codeph>rbd</codeph> command, use the unmap option and the device name. (By convention, the device name is the same as the block device image name).</p>
<codeblock><codeph>sudo rbd unmap /dev/rbd/{poolname}/{imagename}
</codeph></codeblock>
<p>For example:</p>
<codeblock><codeph>sudo rbd unmap /dev/rbd/rbd/foo
</codeph></codeblock>
</section>
<section id="control-commands"> <title>Control Commands</title>
</section>
<section id="monitor-commands"> <title>Monitor commands</title>
<p>Use the ceph utility to issue the monitor commands:</p>
<codeblock><codeph>ceph [-m monhost] {command}
</codeph></codeblock>
<p>The command is usually (though not always) of the form:</p>
<codeblock><codeph>ceph {subsystem} {command}
</codeph></codeblock>
</section>
<section id="system-commands"> <title>System Commands</title>
<ul>
<li>
<p>To display the current status of the cluster, enter:</p>

<codeblock><codeph>ceph -s
ceph status
</codeph></codeblock>
</li>
<li>
<p>To display a running summary of the status of the cluster, and major events, enter:</p>

<codeblock><codeph>ceph -w
</codeph></codeblock>
</li>
<li>
<p>To show the monitor quorum, including which monitors are participating and which one is the leader, enter:</p>

<codeblock><codeph>ceph quorum_status
</codeph></codeblock>
</li>
<li>
<p>To query the status of a single monitor, including whether or not it is in the quorum, enter:</p>

<codeblock><codeph>ceph [-m monhost] mon_status
</codeph></codeblock>
</li>
</ul>
</section>
<section id="authentication-subsystem"> <title>Authentication subsystem</title>
<ul>
<li>
<p>To add a keyring for an OSD, enter:</p>

<codeblock><codeph>ceph auth add {osd} {--in-file|-i} {path-to-osd-keyring}
</codeph></codeblock>
</li>
<li>
<p>To list the cluster's keys and their capabilities, enter:</p>

<codeblock><codeph>ceph auth list
</codeph></codeblock>
</li>
</ul>
</section>
<section id="placement-group-subsystem"> <title>Placement group subsystem</title>
<ul>
<li>
<p>To display the statistics for all placement groups, enter:</p>

<codeblock><codeph>ceph pg dump [--format {format}]
</codeph></codeblock>
</li>
</ul>
<p>The valid formats are plain (default) and JSON.</p>
<ul>
<li>
<p>To display the statistics for all placement groups stuck in a specified state, enter:</p>

<codeblock><codeph>ceph pg dump_stuck inactive|unclean|stale [--format {format}] [-t|--threshold {seconds}]
</codeph></codeblock>

<p>
<codeph>--format</codeph> may be plain (default) or JSON</p>

<p>
<codeph>--threshold</codeph> defines how many seconds ?stuck?. The default value is 300.</p>
</li>
<li>
<p>Inactive placement groups cannot process reads or writes because they are waiting for an OSD with the most up-to-date data to come back.</p>
</li>
<li>
<p>Unclean placement groups contain objects that are not replicated the desired number of times. They should be  in the state of recover.</p>
</li>
<li>
<p>Stale Placement groups are in an unknown state - the OSDs that host them have not reported to the monitor cluster in a while (configured by <codeph>mon_osd_report_timeout</codeph>).</p>
</li>
<li>
<p>To revert "lost" objects to their prior state, either a previous version or delete them if they were just created, enter:</p>

<codeblock><codeph>ceph pg {pgid} mark_unfound_lost revert
</codeph></codeblock>
</li>
</ul>
</section>
<section id="osd-subsystem">
      <title>OSD subsystem</title>
      <ul>
        <li>
          <p>To query the OSD subsystem status, enter:</p>
          <codeblock><codeph>ceph osd stat
</codeph></codeblock>
        </li>
        <li>
          <p>To write a copy of the most recent OSD map to a file, enter:</p>
          <codeblock><codeph>ceph osd getmap -o file
</codeph></codeblock>
        </li>
        <li>
          <p>To write a copy of the crush map from the most recent OSD map to file, enter:</p>
          <codeblock><codeph>ceph osd getcrushmap -o file
</codeph></codeblock>
        </li>
      </ul>
      <p>Which is functionally equivalent to:</p>
      <codeblock><codeph>ceph osd getmap -o /tmp/osdmap
 osdmaptool /tmp/osdmap --export-crush file 
</codeph></codeblock>
      <ul>
        <li>
          <p>To dump the OSD map, enter:</p>
          <codeblock><codeph>ceph osd dump [--format {format}]
</codeph></codeblock>
        </li>
      </ul>
      <p>Valid formats for <codeph>--format</codeph> are plain and JSON. If you do not specify a
        format, the OSD map is dumped as plain text.</p>
      <ul>
        <li>
          <p>To dump the OSD map as a tree with one line per OSD containing weight and state,
            enter:</p>
          <codeblock><codeph>ceph osd tree [--format {format}]
</codeph></codeblock>
        </li>
        <li>
          <p>To find out where a specific object is or would be stored in the system, enter:</p>
          <codeblock><codeph>ceph osd map &lt;pool-name&gt; &lt;object-name&gt;
</codeph></codeblock>
        </li>
        <li>
          <p>To add or move a new item (OSD) with the given <codeph>id/name/weight</codeph> at the
            specified location, enter:</p>
          <codeblock><codeph>ceph osd crush set {id} {weight} [{loc1} [{loc2} ...]]
</codeph></codeblock>
        </li>
        <li>
          <p>To remove an existing item from the CRUSH map, enter:</p>
          <codeblock><codeph>ceph osd crush remove {id} 
</codeph></codeblock>
        </li>
        <li>
          <p>To move an existing bucket from one position in the hierarchy to another, enter:</p>
          <codeblock><codeph>ceph osd crush move {id} {loc1} [{loc2} ...] 
</codeph></codeblock>
        </li>
        <li>
          <p>To set the weight of the item given by {name} to {weight}, enter:</p>
          <codeblock><codeph>ceph osd crush reweight {name} {weight}
</codeph></codeblock>
        </li>
        <li>
          <p>To create a cluster snapshot, enter:</p>
          <codeblock><codeph>ceph osd cluster_snap {name}
</codeph></codeblock>
        </li>
        <li>
          <p>To mark an OSD as lost, enter:</p>
          <codeblock><codeph>ceph osd lost {id} [--yes-i-really-mean-it]
</codeph></codeblock>
        </li>
      </ul>
      <p>
        <b>Caution</b>: This may result in a permanent data loss.</p>
      <ul>
        <li>
          <p>To create a new OSD, enter:</p>
          <codeblock><codeph>ceph osd create [{uuid}]
</codeph></codeblock>
        </li>
        <li>
          <p>If you do not provide a UUID, it will be set automatically when the OSD starts up.</p>
        </li>
        <li>
          <p>To remove the given OSD(s), enter:</p>
          <codeblock><codeph>ceph osd rm [{id}...]
</codeph></codeblock>
        </li>
        <li>
          <p>To query the current <codeph>max_osd</codeph> parameter in the OSD map, enter:</p>
          <codeblock><codeph>ceph osd getmaxosd
</codeph></codeblock>
        </li>
        <li>
          <p>To import the given OSD map, enter:</p>
          <codeblock><codeph>ceph osd setmap -i file
</codeph></codeblock>
          <p>
            <b>Caution</b>: Since the OSD map includes the dynamic state about which OSDs are
            current on or offline, only use this command if you have just modified a (very) recent
            copy of the map.</p>
        </li>
        <li>
          <p>To import the given crush map, enter:</p>
          <codeblock><codeph>ceph osd setcrushmap -i file
</codeph></codeblock>
        </li>
        <li>
          <p>To set the <codeph>max_osd</codeph> parameter in the OSD map, enter:</p>
          <codeblock><codeph>ceph osd setmaxosd
</codeph></codeblock>
        </li>
        <li>
          <p>To mark OSD <codeph>{osd-num}</codeph> down</p>
          <codeblock><codeph> osd down {osd-num} 
</codeph></codeblock>
        </li>
      </ul>
      <note>This is necessary when expanding the storage cluster.</note>
      <ul>
        <li>
          <p>To mark the OSD <codeph>{osd-num}</codeph> out of the distribution (that is, allocated
            no data), enter:</p>
          <codeblock><codeph>ceph osd out {osd-num}
</codeph></codeblock>
        </li>
        <li>
          <p>To mark <codeph>{osd-num}</codeph> in the distribution (that is, allocated data),
            enter:</p>
          <codeblock><codeph>ceph osd in {osd-num}
</codeph></codeblock>
        </li>
        <li>
          <p>To list classes that are loaded in the Ceph cluster, enter:</p>
          <codeblock><codeph>ceph class list
</codeph></codeblock>
        </li>
        <li>
          <p>To set or clear the pause flags in the OSD map, enter:</p>
          <codeblock><codeph>ceph osd pause
ceph osd unpause
</codeph></codeblock>
        </li>
      </ul>
      <note> If set, no IO requests are sent to OSDs. Clearing the flags through
          <codeph>unpause</codeph> results in resending pending requests.</note>
      <ul>
        <li>
          <p>To set the weight of <codeph>{osd-num}</codeph> to <codeph>{weight}</codeph>,
            enter:</p>
          <codeblock><codeph>ceph osd reweight {osd-num} {weight}
</codeph></codeblock>
        </li>
      </ul>
      <note>Two OSDs with the same weight receive a similar number of I/O requests and store a
        similar amount of data.</note>
      <ul>
        <li>
          <p>To reduce the weight of OSDs which are heavily overused, enter:</p>
          <codeblock><codeph>ceph osd reweight-by-utilization [threshold]
</codeph></codeblock>
        </li>
      </ul>
      <note>By default, this command will adjust the weights downward on OSDs which have 120% of the
        average utilization, but if you include threshold it will use that percentage
        instead.</note>
      <ul>
        <li>
          <p>To add/remove an address to/from the blacklist, enter:</p>
          <p>ceph osd blacklist add ADDRESS[:source_port] [TIME] ceph osd blacklist rm
            ADDRESS[:source_port]</p>
        </li>
      </ul>
      <note>When adding an address, you can specify the time (in seconds) to be blacklisted;
        otherwise, it will configure a default time of 1 hour. A blacklisted address is prevented
        from connecting to any OSD. Blacklisting is most often used to prevent a lagging metadata
        server from making unwanted changes to data on the OSDs.</note>
      <p>These commands are useful for failure testing, as blacklists are normally maintained
        automatically, and do not need manual intervention.</p>
      <ul>
        <li>
          <p>To create/delete a snapshot of a pool, execute the following command:</p>
          <codeblock><codeph>ceph osd pool mksnap {pool-name} {snap-name}
</codeph></codeblock>
        </li>
        <li>
          <p>To delete a snapshot of a pool, enter:</p>
          <codeblock><codeph>ceph osd pool rmsnap {pool-name} {snap-name}
</codeph></codeblock>
        </li>
        <li>
          <p>To create a storage pool, enter:</p>
          <codeblock><codeph>ceph osd pool create {pool-name} pg_num [pgp_num]
</codeph></codeblock>
        </li>
        <li>
          <p>To delete a storage pool, enter:</p>
          <codeblock><codeph>ceph osd pool delete {pool-name} [{pool-name} --yes-i-really-really-mean-it]
</codeph></codeblock>
        </li>
        <li>
          <p>To rename a storage pool, enter:</p>
          <codeblock><codeph>ceph osd pool rename {old-name} {new-name}
</codeph></codeblock>
        </li>
        <li>
          <p>To change a pool setting, enter:</p>
          <codeblock><codeph>ceph osd pool set {pool-name} {field} {value}
</codeph></codeblock>
        </li>
      </ul>
      <p>Where:</p>
      <ul>
        <li>
          <p>
            <b>size</b>: Sets the number of copies of data in the pool.</p>
        </li>
        <li>
          <p>
            <b>crash_replay_interval</b>: The number of seconds to allow clients to replay
            acknowledged but uncommited requests.</p>
        </li>
        <li>
          <p>
            <b>pg_num</b>: The placement group number.</p>
        </li>
        <li>
          <p>
            <b>pgp_num</b>: Effective number when calculating pg placement.</p>
        </li>
        <li>
          <p>
            <b>crush_ruleset</b>: rule number for mapping placement.</p>
        </li>
      </ul>
      <p>To get the value of a pool setting, enter:</p>
      <codeblock><codeph>ceph osd pool get {pool-name} {field}
</codeph></codeblock>
      <p>Where:</p>
      <ul>
        <li>
          <p>
            <b>pg_num</b>: The placement group number.</p>
        </li>
        <li>
          <p>
            <b>pgp_num</b>: Effective number of placement groups when calculating placement.</p>
        </li>
        <li>
          <p>
            <b>lpg_num</b>: The number of local placement groups.</p>
        </li>
        <li>
          <p>
            <b>lpgp_num</b>: The number used for placing the local placement groups.</p>
        </li>
      </ul>
      <p>To send a scrub command to OSD <codeph>{osd-num}</codeph>, execute the following
        command:</p>
      <codeblock><codeph>ceph osd scrub {osd-num}
</codeph></codeblock>
      <note>To send the command to all OSDs, use <b>*</b>.</note>
      <p>To send a repair command to <codeph>OSD.N</codeph>, enter:</p>
      <codeblock><codeph>ceph osd repair N
</codeph></codeblock>
      <note>To send the command to all OSDs, use *.</note>
      <p>To run a simple throughput benchmark against <codeph>OSD.N</codeph>, enter:</p>
      <codeblock><codeph>ceph osd tell N bench [BYTES_PER_WRITE] [TOTAL_BYTES]
</codeph></codeblock>
      <note>This writes <codeph>TOTAL_BYTES</codeph> in write requests of
          <codeph>BYTES_PER_WRITE</codeph> each. By default, the test writes 1 GB in total in 4-MB
        increments.</note>
    </section>
<section id="mon-subsystem"> <title>MON Subsystem</title>
<p>The following commands show the status of the monitor.</p>
<ul>
<li>
<p>To show monitor statistics, execute the following command:</p>

<codeblock><codeph>ceph mon stat
</codeph></codeblock>
</li>
<li>
<p>To list the monitor nodes that are part of the current quorum, execute the following command:</p>

<codeblock><codeph>$ ./ceph quorum_status
</codeph></codeblock>
</li>
<li>
<p>To get a status of just the monitor you connect to (use -m HOST:PORT to select), execute the following command:</p>

<codeblock><codeph>ceph mon_status
</codeph></codeblock>
</li>
<li>
<p>To get a dump of the monitor state, execute the following command:</p>

<codeblock><codeph>ceph mon dump
</codeph></codeblock>
</li>
</ul>
</section>
<section id="next-steps"> <title>Next Steps</title>
<p>
  <xref href="1.1commercial.ceph-rados-gateway-dmz-ha-proxy.dita" >Ceph RADOSGW DMZ HAProxy</xref>
</p>
<p>
  <xref href="#topic11650"> Return to Top </xref>
</p>
</section>
</body>
</topic>
