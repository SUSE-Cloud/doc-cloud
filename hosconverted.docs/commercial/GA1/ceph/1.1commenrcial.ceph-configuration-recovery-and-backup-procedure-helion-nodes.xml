<?xml version="1.0" encoding="UTF-8"?>
<!--Edit status: not edited-->
<!DOCTYPE topic
  PUBLIC "-//OASIS//DTD DITA Topic//EN" "http://docs.oasis-open.org/dita/v1.1/OS/dtd/topic.dtd" ><topic xml:lang="en-us" id="topic3172">
<title>Useful tips</title>
<titlealts>
<searchtitle>HPE Helion Openstack HPE Helion Ceph Backup and Recovery</searchtitle>
</titlealts>
<prolog>
<metadata>
<othermeta name="layout" content="default"/>
<othermeta name="product-version" content="HPE Helion Openstack"/>
<othermeta name="product-version" content="HPE Helion Openstack 1.1"/>
<othermeta name="role" content="Storage Engineer"/>
<othermeta name="role" content="Storage Architect"/>
<othermeta name="role" content="Storage Administrator"/>
<othermeta name="role" content="Storage Engineer"/>
<othermeta name="role" content="Service Developer"/>
<othermeta name="role" content="Cloud Administrator"/>
<othermeta name="role" content="Application Developer"/>
<othermeta name="role" content="Paul F"/>
<othermeta name="product-version1" content="HPE Helion Openstack"/>
<othermeta name="product-version2" content="HPE Helion Openstack 1.1"/>
</metadata>
</prolog>
<body>

<!--PUBLISHED-->
 <!--./commercial/GA1/ceph/1.1commenrcial.ceph-configuration-recovery-and-backup-procedure-helion-nodes.md  -->
 <!--permalink: /helion/openstack/1.1/ceph-configuration-recovery-and-backup-procedure-HP-Helion-nodes/  -->

<!-- # HPE Helion <tm tmtype="reg">OpenStack</tm> 1.1: Ceph Configuring Backup and Recovery for HPE Helion Nodes-->
<p> The following steps ensure the recovery of the lost configuration. </p>
    <p>
      <ol id="ol_csb_fgk_4s">
        <li>Run the <codeph>ceph_client_setup</codeph> script available at <xref
            href="https://helion.hpwsportal.com" format="html"> HDN</xref> on each of the
          controller/compute nodes. (For information on what files need to be copied from the Ceph
          cluster node to their respective directories, refer to the script's README file.) </li>
        <li>To confirm ceph health, run: ceph health. </li>
        <li>If you reboot your controller and compute node after running the
            <codeph>ceph_client_setup</codeph> script, the patch update for glance-api.conf,
          nova.conf, cinder.conf might default to the original state. <p>In this case, from each
            controller and compute node, go to:
              <codeph>/home/ceph_client_setup/setup_scripts</codeph>
            <ol id="ol_hnw_jgk_4s">
              <li>Execute <codeph>sh patch_config_files.sh </codeph></li>
              <li>Execute <codeph>sh helion_service_restarts.sh </codeph></li>
            </ol></p></li>
      </ol>
    </p>
    <p>Sample files</p>

<table>
  <tgroup cols="3">

    <thead> 
      <row>
        <entry colsep="1" rowsep="1">Helion Node</entry>
        <entry colsep="1" rowsep="1"> Service/File name</entry>    
        <entry colsep="1" rowsep="1">File Attachment</entry>
       </row>
    </thead>
<tbody>
<row>
  <entry colsep="1" rowsep="1"> Controller node</entry>
  <entry colsep="1" rowsep="1">
              <codeph>/etc/glance/glance-api.conf </codeph><codeph>/etc/cinder/cinder.conf</codeph>
              <codeph>/etc/nova/nova.conf</codeph></entry>

  <entry colsep="1" rowsep="1">
              <ol id="ol_fgl_3hk_4s">
                <li>Log in to the <xref href="https://helion.hpwsportal.com" format="html">
                    HDN</xref></li>
                <li>Click the workloads category on the left side.</li>
                <li>Click the storage subcategory to find all the Ceph related files.
                    <b>Filename</b>
                </li>
              </ol>
            </entry>
</row>
  <row>
    <entry colsep="1" rowsep="1">Compute Node</entry>
    <entry colsep="1" rowsep="1"><codeph>/etc/nova/nova.conf</codeph></entry>
    <entry colsep="1" rowsep="1">
              <ol id="ol_qql_khk_4s">
                <li>Log in to the <xref href="https://helion.hpwsportal.com" format="html">
                    HDN</xref>
                </li>
                <li>Click the workloads category on the left side.</li>
                <li>Click the storage subcategory to find all the Ceph related files.
                  <!-- <br /> Filename need ?-->
                </li>
              </ol>
            </entry>
  </row>
</tbody>
  </tgroup>
</table>




<p>The following tips will help you troubleshoot errors:</p>
<ul>
<li>
<p>Verify that the http proxy settings are correct in both <codeph>/etc/environment</codeph> and <codeph>/etc/apt/apt.conf</codeph> for connecting to external resources.</p>
</li>
<li>
<p>The Firefly version of Ceph packages has a default replication set to 3x (three times). Therefore you need a minimum of three OSDs.</p>
</li>
<li>
<p>If a <codeph>Decrypt error</codeph> occurs from any of the Helion nodes with glance, cinder, or nova, make sure that all the Ceph cluster nodes are in same time zone. HPE recommends that you configure a NTP server in the <codeph>/etc/ntp.conf</codeph> file on all Ceph cluster nodes.  After making changes, restart NTP [<codeph>/etc/init.d/ntp restart</codeph>]. Install the <codeph>ntp</codeph> package, if it is not there.</p>
</li>
<li>
<p>Make sure the Ceph cluster uses the same IP range as the HPE Helion OpenStack overcloud nodes. Also, make sure the IP range for the Ceph cluster does not conflict with those used by Helion set up.</p>
</li>
<li>
<p>Be sure to check the log files in <codeph>/var/log/ceph/</codeph> for each node. Any errors or exceptions are useful for troubleshooting.</p>
</li>
<li>
<p>Any changes you make to the Ceph configuration file requires a restart for the changes to take affect.</p>
</li>
<li>
<p>You cannot change the default Cephx Authentication to <codeph>None</codeph> once the cluster is up and running. You will have to purge and reinstall the Ceph packages and build the cluster.</p>
</li>
<li>
<p>Enable logging if you encounter a problem at any of these steps:</p>

<p>To do this, edit <codeph>glance-api.conf</codeph>, <codeph>cinder.conf</codeph> and <codeph>nova.conf</codeph> files with the following in the default section.</p>

<codeblock><codeph>debug = True
verbose = True
</codeph></codeblock>

<p>Restart Glance, Cinder, and Nova services respectively.</p>

<p>On HPE Helion Commercial, the logs for Glance and Cinder are stored by default in the following directories:</p>

<codeblock><codeph>Glance - /var/log/glance
Cinder - /var/log/upstart
</codeph></codeblock>
</li>
</ul>
<p>Gather these logs and contact HPE support team.</p>
<ul>
<li>
<p>If any Placement Group (PG)-related issues occur in the Ceph cluster, refer to the following link:</p>

<p>
<xref href="http://docs.ceph.com/docs/master/rados/operations/placement-groups/" scope="external" format="html" >http://docs.ceph.com/docs/master/rados/operations/placement-groups/</xref>
</p>
</li>
</ul>
<section id="choosing-the-number-of-the-placement-group"> <title>Choosing the number of the placement group</title>
<p>If there are more than 50 OSDs, we recommend approximately 50-100 PGs per OSD to balance resource usage, data durability and distribution. To get a baseline for a single pool of objects, use the following formula:</p>
<p>
  <image href="../../../media/commercial-ceph-formula-placement-group.png" placement="break"/>
</p>
<p>Where pool size is either the number of replicas for replicated pools or the K+M sum for erasure coded pools (as returned by <codeph>ceph osd erasure-code-profile get</codeph>).
Round up the results to the nearest power of two. Rounding up is optional, but recommended for CRUSH to evenly balance the number of objects among PGs.</p>
<p>For example, for a cluster with 200 OSDs and a pool size of 3 replicas, estimate the number of PGs as follows:</p>
<codeblock><codeph>(200 * 100)
----------- = 6667. Nearest power of 2: 8192
       3
</codeph></codeblock>

</section>
 
</body>
</topic>
