<?xml version="1.0" encoding="UTF-8"?>
<!--Edit status: not edited-->
<!DOCTYPE topic
  PUBLIC "-//OASIS//DTD DITA Topic//EN" "http://docs.oasis-open.org/dita/v1.1/OS/dtd/topic.dtd" ><topic xml:lang="en-us" id="topic4920">
<title>HPE Helion <tm tmtype="reg">OpenStack</tm> 1.1 and 1.1.1: Cinder Ceph Storage</title>
<titlealts>
<searchtitle>HPE Helion Openstack 1.1: Cinder Ceph Storage</searchtitle>
</titlealts>
<prolog>
<metadata>
<othermeta name="layout" content="default"/>
<othermeta name="product-version" content="HPE Helion Openstack"/>
<othermeta name="product-version" content="HPE Helion Openstack 1.1"/>
<othermeta name="role" content="Storage Engineer"/>
<othermeta name="role" content="Storage Architect"/>
<othermeta name="role" content="Storage Administrator"/>
<othermeta name="role" content="Storage Engineer"/>
<othermeta name="role" content="Service Developer"/>
<othermeta name="role" content="Cloud Administrator"/>
<othermeta name="role" content="Application Developer"/>
<othermeta name="role" content="Paul F, Binamra S"/>
<othermeta name="product-version1" content="HPE Helion Openstack"/>
<othermeta name="product-version2" content="HPE Helion Openstack 1.1"/>
</metadata>
</prolog>
<body>
<p>
<!--PUBLISHED-->
 <!--./commercial/GA1/ceph/1.1commenrcial.ceph-helion-openstack-ceph-hp-helion-openstack-cinder-storage.md-->
 <!--permalink: /helion/openstack/1.1/ceph-hp-helion-openstack-cinder-storage/--></p>
<p>The following steps are automatically performed when you download and execute <codeph>Ceph_client</codeph> script from the controller and compute nodes as per the readme file in the tar file.</p>
<ol>
<li> Verify the Ceph health by
          entering:<codeblock><codeph>ceph health
</codeph></codeblock><p>Output:</p><codeblock><codeph>HEALTH_OK
</codeph></codeblock></li>
<li>Create a new pool for the Cinder volume by
          entering:<codeblock><codeph>ceph osd pool create &lt;cinder pool name&gt; &lt;pg-num&gt;
</codeph></codeblock><p>For
          example:</p><codeblock><codeph>ceph osd pool create helion-ceph-cinder 100
</codeph></codeblock></li>
<li>To verify a new pool creation,
          enter:<codeblock><codeph>rados lspools
</codeph></codeblock><p>Output:</p><codeblock><codeph>rbd ls helion-ceph-cinder
</codeph></codeblock></li>
<li>Create symbolic or softlinks to the relevant files on overcloud controller nodes by
        entering:<codeblock><codeph>ln -s /usr/lib/python2.7/dist-packages/rados* /opt/stack/venvs/openstack/lib/python2.7/site-packages/.
ln -s /usr/lib/python2.7/dist-packages/rbd* /opt/stack/venvs/openstack/lib/python2.7/site-packages/.
</codeph></codeblock></li>
</ol>
<section id="configuring-cinder"> <title>Configuring Cinder</title>
<p>HPE Helion OpenStack requires the <codeph>rbd</codeph> driver to interact with Ceph block devices. The pool name must be specified for the block device. To specify the pool name on the overcloud controller nodes edit <codeph>cinder.conf</codeph>.</p>
<p>Perform the following steps to configure Cinder.</p>
<ol>
<li>Edit <codeph>/etc/cinder/cinder.conf</codeph> and add the
            following:<codeblock><codeph>[DEFAULT]

# For ceph integration
volume_driver=cinder.volume.drivers.rbd.RBDDriver
rbd_pool=helion-ceph-cinder
rbd_ceph_conf=/etc/ceph/ceph.conf
rbd_flatten_volume_from_snapshot=false
rbd_max_clone_depth=5
</codeph></codeblock><p>You
            can also insert a comment on the <codeph>lvm</codeph> backend in the
              <codeph>cinder.conf</codeph> file for your
          reference.</p><codeblock><codeph># LVM thin provision. This way we do not dd the disk

#enabled_backends = lvm-1
</codeph>
</codeblock></li>
<li>Restart the Cinder service by
          entering:<codeblock><codeph>service cinder-volume restart
service cinder-scheduler restart
service cinder-api restart
</codeph></codeblock></li>
</ol>
<p>Once the cinder service is restarted, you can create a volume.</p>
</section>
<section id="helion-openstack-cinder-backup-ceph-storage"> <title>Helion OpenStack Cinder Backup Ceph Storage</title>
<p>The Ceph backup driver performs the data backup operation on the volume to a Ceph RADOS Block Device (RBD). Backups can be performed between different Ceph pools and Ceph clusters. This section explains the backup and restore procedure for Cinder volumes.</p>
<p>On the Helion controller and compute node, if the Ceph client softwares are successfully installed and the Ceph configuration and keyring files are copied in the Helion controller and compute nodes, then the HPE Helion OpenStack Ceph Configuration service performs the following steps:</p>
<ol>
<li>Verify Ceph health status by
            entering:<codeblock><codeph>ceph health
</codeph></codeblock><p>Output:</p><codeblock><codeph>HEALTH_OK
</codeph></codeblock></li>
<li>Create a new pool for Cinder backup by
            entering:<codeblock><codeph>ceph osd pool create &lt;cinder backup pool name&gt; &lt;pg-num&gt;
</codeph></codeblock><p>For
            example:</p><codeblock><codeph>ceph osd pool create helion-ceph-backups 100
</codeph></codeblock></li>
<li>To verify that a new backup pool is created,
            enter:<codeblock><codeph>rados lspools
</codeph></codeblock><p>Output:</p><codeblock><codeph>rbd ls helion-ceph-backups
</codeph></codeblock></li>
<li>Create symbolic or softlinks to the relevant files on overcloud controller nodes (if the
          following links are already created for other services, then ignore them) by
          entering:<codeblock><codeph>ln -s /usr/lib/python2.7/dist-packages/rados* /opt/stack/venvs/openstack/lib/python2.7/site-packages/.

ln -s /usr/lib/python2.7/dist-packages/rbd* /opt/stack/venvs/openstack/lib/python2.7/site-packages/.
</codeph></codeblock></li>
</ol>
</section>
<section id="configuring-cinder-backup"> <title>Configuring Cinder backup</title>
<p>Perform the following steps to enable the Ceph backup driver:</p>
<ol>
<li>Edit <codeph>/etc/cinder/cinder.conf</codeph> and add the following options in all three
          controller nodes [OCC mgmt, OCC0,
            OCC1].<codeblock><codeph>[DEFAULT]

# For cinder backup
    backup_driver=cinder.backup.drivers.ceph
backup_ceph_conf=/etc/ceph/ceph.conf
backup_ceph_user=cinder-backup
backup_ceph_chunk_size=134217728
backup_ceph_pool=helion-ceph-backups
backup_ceph_stripe_unit=0
backup_ceph_stripe_count=0
restore_discard_excess_bytes=true
</codeph></codeblock><p>The
            following table provides the description of the parameters.</p><table>
            <tgroup cols="2">
              <colspec colname="col1" colsep="1" rowsep="1"/>
              <colspec colname="col2" colsep="1" rowsep="1"/>
              <thead>
                <row>
                  <entry colsep="1" rowsep="1"> Parameters</entry>
                  <entry colsep="1" rowsep="1">Descriptions </entry>
                </row>
              </thead>
              <tbody>
                <row>
                  <entry colsep="1" rowsep="1">backup_ceph_chunk_size = 134217728</entry>
                  <entry colsep="1" rowsep="1">The chunk size, in bytes, that a backup is broken
                    into before transfer to the Ceph object store.</entry>
                </row>
                <row>
                  <entry colsep="1" rowsep="1">backup_ceph_conf = /etc/ceph/ceph.conf</entry>
                  <entry colsep="1" rowsep="1">Ceph configuration file to use.</entry>
                </row>
                <row>
                  <entry colsep="1" rowsep="1">backup_ceph_pool = helion-ceph-backups</entry>
                  <entry colsep="1" rowsep="1">The Ceph pool where volume backups are
                    stored.</entry>
                </row>
                <row>
                  <entry colsep="1" rowsep="1">backup_ceph_stripe_count = 0</entry>
                  <entry colsep="1" rowsep="1">RBD stripe count to use when creating a backup
                    image.</entry>
                </row>
                <row>
                  <entry>backup_ceph_stripe_unit = 0</entry>
                  <entry>RBD stripe unit to use when creating a backup image.</entry>
                </row>
                <row>
                  <entry>backup_ceph_user = cinder</entry>
                  <entry>The Ceph user to connect with. Default here is to use the same user as for
                    Cinder volumes. If not using cephx this should be set to None.</entry>
                </row>
                <row>
                  <entry>restore_discard_excess_bytes = True</entry>
                  <entry>If True, always discard excess bytes when restoring volumes (that is, pad
                    with zeroes).</entry>
                </row>
              </tbody>
            </tgroup>
          </table></li>
<li>Restart the Cinder service by
          entering:<codeblock><codeph>service cinder-backup restart
</codeph></codeblock></li>
</ol>
</section>
<section id="next-steps"> <title>Next Steps</title>
<p>
  <xref href="../../../commercial/GA1/ceph/1.1commercial-ceph-helion-openstack-nova-ceph-Storage.dita" >Ceph Nova Storage</xref>
</p>
<p>
  <xref href="#topic4920"> Return to Top </xref>
</p>
</section>
</body>
</topic>
