<?xml version="1.0" encoding="UTF-8"?>
<!--Edit status: not edited-->
<!DOCTYPE topic
  PUBLIC "-//OASIS//DTD DITA Topic//EN" "http://docs.oasis-open.org/dita/v1.1/OS/dtd/topic.dtd" ><topic xml:lang="en-us" id="topic3651">
<title>HPE Helion <tm tmtype="reg">OpenStack</tm> 1.1 and 1.1.1: Troubleshooting Ceph</title>
<titlealts>
<searchtitle>HPE Helion Openstack 1.1: troubleshooting tips</searchtitle>
</titlealts>
<prolog>
<metadata>
<othermeta name="layout" content="default"/>
<othermeta name="product-version" content="HPE Helion Openstack"/>
<othermeta name="product-version" content="HPE Helion Openstack 1.1"/>
<othermeta name="role" content="Storage Engineer"/>
<othermeta name="role" content="Storage Architect"/>
<othermeta name="role" content="Storage Administrator"/>
<othermeta name="role" content="Storage Engineer"/>
<othermeta name="role" content="Service Developer"/>
<othermeta name="role" content="Cloud Administrator"/>
<othermeta name="role" content="Application Developer"/>
<othermeta name="role" content="Paul F"/>
<othermeta name="product-version1" content="HPE Helion Openstack"/>
<othermeta name="product-version2" content="HPE Helion Openstack 1.1"/>
</metadata>
</prolog>
<body>
<p>
<!--PUBLISHED-->
 <!--./commercial/GA1/ceph/1.1commenrcial.ceph-troubleshooting.md-->
 <!--permalink: /helion/openstack/1.1/ceph-troubleshooting/--></p>
<p>The following tips will help you troubleshoot errors:</p>
<ul>
<li>
<p>Verify that the http proxy settings are correct in both <codeph>/etc/environment</codeph> and <codeph>/etc/apt/apt.conf</codeph> for connecting to external resources.</p>
</li>
<li>
<p>The Firefly version of Ceph packages has a default replication set to 3x (three times). Therefore you need a minimum of three OSDs.</p>
</li>
<li>
<p>If a <codeph>Decrypt error</codeph> occurs from any of the Helion nodes with glance, cinder, or nova, make sure that all the Ceph cluster nodes are in same time zone. HPE recommends that you configure a NTP server in the <codeph>/etc/ntp.conf</codeph> file on all Ceph cluster nodes.  After making changes, restart NTP [<codeph>/etc/init.d/ntp restart</codeph>]. Install the <codeph>ntp</codeph> package, if it is not there.</p>
</li>
<li>
<p>Make sure the Ceph cluster uses the same IP range as the HPE Helion OpenStack overcloud nodes. Also, make sure the IP range for the Ceph cluster does not conflict with those used by Helion setup.</p>
</li>
<li>
<p>Be sure to check the log files in <codeph>/var/log/ceph/</codeph> for each node. Any errors or exceptions are useful for troubleshooting.</p>
</li>
<li>
<p>Any changes you make to the Ceph configuration file requires a restart for the changes to take affect.</p>
</li>
<li>
<p>You cannot change the default Cephx Authentication to <codeph>None</codeph> once the cluster is up and running. You will have to purge and reinstall the Ceph packages and build the cluster.</p>
</li>
<li>
<p>Enable logging if you encounter a problem at any of these steps:</p>

<p>To do this, edit <codeph>glance-api.conf</codeph>, <codeph>cinder.conf</codeph> and <codeph>nova.conf</codeph> files with the following in the default section.</p>

<codeblock><codeph>debug = True
verbose = True
</codeph></codeblock>

<p>Restart Glance, Cinder, and Nova services respectively.</p>

<p>On HPE Helion Commercial, the logs for Glance and Cinder are stored by default in the following directories:</p>

<codeblock><codeph>Glance - /var/log/glance
Cinder - /var/log/upstart
</codeph></codeblock>

<p>Gather these logs and contact HPE support team.</p>
</li>
<li>
<p>If any Placement Group (PG)-related issues occur in the Ceph cluster, refer to the following link:</p>

<p>
<xref href="http://docs.ceph.com/docs/master/rados/operations/placement-groups/" scope="external" format="html" >http://docs.ceph.com/docs/master/rados/operations/placement-groups/</xref>
</p>
</li>
</ul>
<section id="choosing-the-number-of-the-placement-group"> <title>Choosing the number of the placement group</title>
<p>If there are more than 50 OSDs, we recommend approximately 50-100 PGs per OSD to balance resource usage, data durability and distribution. To get a baseline for a single pool of objects, use the following formula:</p>
<p>
  <image href="../../../media/commercial-ceph-formula-placement-group.png" placement="break"/>
</p>
<p>Where pool size is either the number of replicas for replicated pools or the K+M sum for erasure coded pools (as returned by <codeph>ceph osd erasure-code-profile get</codeph>).
Round up the results to the nearest power of two. Rounding up is optional, but recommended for CRUSH to evenly balance the number of objects among PGs.</p>
<p>For example, for a cluster with 200 OSDs and a pool size of 3 replicas, estimate the number of PGs as follows:</p>
<codeblock><codeph>(200 * 100)
----------- = 6667. Nearest power of 2: 8192
 3
</codeph></codeblock>
</section>
<section id="ceph-osd-cleanup"> <title>Ceph OSD cleanup</title>
<p>To do a Ceph OSD cleanup:</p>
<ol>
<li>Enter:
<codeph>ceph osd crush remove osd.x</codeph>
</li>
<li>Enter: <codeph>ceph auth del osd.x</codeph>
</li>
<li>Run this command multiple times until it says osd.x is already down. Then run the next step immediately:<codeph>ceph osd down osd.x</codeph> </li>
<li>If this command throws a warning saying osd is busy, repeat the previous step:  <codeph>ceph osd rm osd.x</codeph> </li>
<li>Enter: <codeph>umount &lt;osd data drive (as specified in playbook)&gt;</codeph>
</li>
<li>Enter: <codeph>rm ?r /var/lib/ceph/osd/ceph-x</codeph>
</li>
<li>Remove the <codeph>osd</codeph> entry from <codeph>ceph.conf</codeph> and <codeph>scp ceph.conf</codeph> to other cluster nodes.</li>
<li>Verify that <codeph>osd</codeph> was removed by executing: <codeph>- sudo ceph ?w</codeph> </li>
</ol>
</section>
<section id="next-steps">
<p>
  <xref href="#topic3651"> Return to Top </xref>
</p>
</section>
</body>
</topic>
