<?xml version="1.0" encoding="UTF-8"?>
<!--Edit status: not edited-->
<!DOCTYPE topic
  PUBLIC "-//OASIS//DTD DITA Topic//EN" "http://docs.oasis-open.org/dita/v1.1/OS/dtd/topic.dtd" ><topic xml:lang="en-us" id="topic6505">
<title>HPE Helion <tm tmtype="reg">OpenStack</tm> 1.1 and 1.1.1: Ceph Configuration</title>
<titlealts>
<searchtitle>HPE Helion Openstack 1.1: Ceph Configuration</searchtitle>
</titlealts>
<prolog>
<metadata>
<othermeta name="layout" content="default"/>
<othermeta name="product-version" content="HPE Helion Openstack"/>
<othermeta name="product-version" content="HPE Helion Openstack 1.1"/>
<othermeta name="role" content="Storage Engineer"/>
<othermeta name="role" content="Storage Architect"/>
<othermeta name="role" content="Storage Administrator"/>
<othermeta name="role" content="Storage Engineer"/>
<othermeta name="role" content="Service Developer"/>
<othermeta name="role" content="Cloud Administrator"/>
<othermeta name="role" content="Application Developer"/>
<othermeta name="role" content="Paul F"/>
<othermeta name="product-version1" content="HPE Helion Openstack"/>
<othermeta name="product-version2" content="HPE Helion Openstack 1.1"/>
</metadata>
</prolog>
<body>
<p>
<!--PUBLISHED-->
 <!--./commercial/GA1/ceph/1.1commercial-ceph-hp-helion-openstack-ceph-configuration.md-->
 <!--permalink: /helion/openstack/1.1/ceph-hp-helion-openstack-ceph-configuration/--></p>
<p>This section describes the integration of HPE Helion OpenStack 1.1 or 1.1.1, and Ceph Firefly 0.80.7 on a hLinux3.14.29.4 kernel.</p>
<p>Before configuring Helion OpenStack Ceph make sure that HPE Helion OpenStack 1.1 or 1.1.1 is installed successfully.</p>
    <note>The Ceph client script is only run on the Helion controller and compute nodes during a
      manual deployment of the Ceph cluster. If you are using Ansible to configure your Helion nodes
      and Ceph cluster, you can ignore the manual section.</note>
<section id="ceph-client-install"> <title>Ceph Client Install</title>
<p>The Helion Overcloud Controller and compute nodes are consumers of the storage services provided by the Ceph cluster. To enable them to act as Ceph clients, perform the following steps (either the automated or manual process).</p>
      <note>See note above. If you are using Ansible to configure your Helion nodes and Ceph
        cluster, you can ignore the manual section.</note>
</section>
<section id="automated-install"> <title>Automated Install</title>
<p>The <codeph>ceph_client_setup.tar</codeph> contains Debian packages, Helion OpenStack Ceph configuration scripts, and the patch file to make the required alterations to the Nova, Glance, and Cinder configuration files. These Debian packages must be installed while installing the Ceph firefly 0.80.7 client packages on the HPE Helion overcloud controller and compute nodes. The script present in the tar file helps to automate the installation process.</p>
<p>Once the Ceph cluster is fully operational, run the tar file on the Helion controller nodes (occ0, occ1, and oc mgmt nodes) and the compute node. At the final stage of the installation process, the Ceph client confirms connectivity to the Ceph cluster.</p>
</section>
<section id="running-the-automated-scripts"> <title>Running the automated scripts</title>
<p>Perform the following steps to run the automated installation scripts on the Helion overcloud controller nodes and the compute nodes.</p>
<p>For more details about the Ceph storage cluster with Glance integration, refer to <xref href="http://ceph.com/docs/master/rbd/rbd-openstack/?highlight=openstack%20glance"   format="html" >http://ceph.com/docs/master/rbd/rbd-openstack/?highlight=openstack%20glance </xref>
</p>
<ol>
<li>Untar <codeph>ceph_client_setup.tar</codeph> in root home on each of the Helion overcloud
          controller and compute nodes where the Ceph client needs to be
            installed.<codeblock><codeph>/home
</codeph></codeblock><p>A directory named
              <codeph>ceph_client_setup</codeph> is created. There are several sub directories
            created under the <codeph>ceph_client_setup</codeph> directory.</p></li>
<li>Copy <codeph>ceph.conf</codeph> file and the keyrings from the admin node to the install
          directories.</li>
<li>Copy the Ceph cluster config files from the ceph admin node into the
            <codeph>ceph_cluster_config_files</codeph> directory.</li>
<li>The script <codeph>fixuuid.sh</codeph> in the <codeph>setup_scripts</codeph> directory modifies
          the Helion configuration file patches with the new UUID from the cephx authentication
          process. To
            execute<!--A BR tag was used here in the original source.--><codeph>fixuuid.sh</codeph>,
            enter:<codeblock><codeph>fixuuid.sh *&lt;the new-UUID*&gt;
</codeph></codeblock><p>The
            new Cephx UUID is passed as an argument to the <codeph>fixuuid.sh</codeph> script. This
            will load the new UUID into the patch files that will be used to modify the Helion
            config files.</p></li>
<li>Run <codeph>ceph_client_install.sh</codeph> on one of the controller nodes or compute nodes.
            <note> The installer script is located at
              <codeph>/home/ceph_client_setup/setup_scripts/ceph_client_install.sh</codeph>
          </note></li>
</ol>
<p>The installation script does the following:</p>
<ol>
<li>Checks if the file is untarred in the correct location (<codeph>/home/ceph_client_setup</codeph>) and exits if the script is not available. </li>
<li>Verifies that the ceph cluster <codeph>ceph.conf</codeph> and the keyring files are copied in the correct location (<codeph>/home/ceph_client_setup/ceph_cluster_config_files</codeph>/) and exits if they are not present. If they are present in the correct location, the <codeph>/etc/ceph</codeph> directory is created and the <codeph>ceph.conf</codeph> and the keyring files are copied into the <codeph>/etc/ceph</codeph> directory.</li>
<li>The script will search for the following files in
            <codeph>/home/ceph_client_setup/ceph_cluster_config_files/</codeph>
          <p>a. ceph.conf - main Ceph configuration file</p><p>b. ceph.client.nova.keyring - Nova
            user authorization key file</p><p>c. ceph.client.cinder.keyring - Cinder user
            authorization key file</p><p>d. ceph.client.cinder-backup.keyring - Cinder backup user
            authorization key file</p><p>e. ceph.client.glance.keyring - Glance user authorization
            key file</p><p>f. ceph.client.admin.keyring - admin user authorization key file</p><p>g.
            ceph.client.radosgw.keyring - RADOSGW authorization key file</p><p>
            <note>if a RADOS gateway is not going to be used for this installation, create an empty
              file with this name in the
                <codeph>/home/ceph_client_setup/ceph_cluster_config_files/</codeph> directory so the
              script will still find a file with the expected name.</note>
          </p></li>
<li>Checks the availability of the Ceph client Debian packages in
            <codeph>/ceph_client_setup/ceph_client_debs/</codeph> and exits the script if it is
          unavailable.</li>
<li>Checks whether the system is a hLinux system and verifies the existence of the
            <codeph>/etc/apt/sources.list</codeph> file. If it meets these requirements, then a
          backup copy of the original <codeph>sources.list</codeph> file is created in
            <codeph>/home/ceph_client_setup/orig_backup/</codeph> directory and then the local Ceph
          packages location is added to the file as a valid local repository.</li>
<li>Does an <codeph>apt-get update</codeph> and <codeph>apt-get install</codeph> of the Ceph client
          packages once the repository is added to the <codeph>apt sources.list</codeph> file. If
            <codeph>apt-get update</codeph> and <codeph>apt-get install</codeph> of the Ceph client
          packages executes without errors, <codeph>dpkg</codeph> is executed to verify the
          installation of the correct packages.</li>
<li>Checks the <codeph>ceph</codeph> command to verify if it is available and executable. If the
            <codeph>ceph</codeph> command is available and executable then it is executed to verify
          whether the health of the Ceph cluster can be determined. If the Ceph cluster health shows
          as <codeph>HEALTH_OK</codeph>, the script displays the status and continues.</li>
</ol>
<p>The following sub-scripts run upon successful completion of the above steps:</p>
<p>
<b>
<codeph>copy_icinga_monscripts.sh</codeph>
</b>- copies the Ceph cluster monitoring scripts in the iCinga script directory on the Helion overcloud controller nodes only.</p>
<p>
<b>
<codeph>patch_config_files.sh</codeph>
</b> - patches the Helion configuration files with the changes required for  Ceph communication.</p>
<p>
<b>
<codeph>ceph_pythonlinks_create.sh</codeph>
</b> - creates softlinks to the necessary Ceph Python library files in the Helion Python directories.</p>
<p>
<b>
<codeph>create_helion_pools.sh</codeph>
</b> - creates the required Helion pools in the Ceph storage (only runs on  the overcloud controller node and management node)</p>
<p>
<b>
<codeph>helion_service_restarts.sh</codeph>
</b> - restarts the Helion services.</p>
<p>
<!--


####Availability of Ceph Client Script

<table>
<table style="text-align: left; vertical-align: top; width:650px;">
<tr style="background-color: #C8C8C8;">
    <th > Description</th>
    <th>File Location </th>
</tr>
    <tr>
<td>Ceph client package file to be installed on the running Controller and Compute nodes in the
HPE Helion OpenStack commercial build</td>
<td>`https://helion.hpwsportal.com` Click on the workloads category on the left side and then you click on the storage subcategory to find all the Ceph related files.<br /></td>
</tr>
  <table>

As a sanity-check, cross check with the attached "dpkg -l" output after Ceph package installation:

<table>
<table style="text-align: left; vertical-align: top; width:650px;">
<tr style="background-color: #C8C8C8;">
    <th > Description</th>
    <th > Description</th>  
    <th>File Location </th>
</tr>
    <tr>
<td>Controller node</td>
<td>DPKG -l output after Ceph package install</td>
<td>`https://helion.hpwsportal.com` Click on the workloads category on the left side and then you click on the storage subcategory to find all the Ceph related files.<tdt>
</tr>
<tr>
<td>Compute node</td>
<td>DPKG -l output after Ceph package install</td>
<td>https://helion.hpwsportal.com Click on the workloads category on the left side and then you click on the storage subcategory to find all the Ceph related files. <tdt>
</tr>
  </table>
-->
<!--

####Verify the Ceph Version

* Execute the following command to verify the Ceph version on the overcloud nova compute node:

        # dpkg -l|grep ceph

The following example displays the ceph version:

    root@overcloud-novacompute0-c6y5lbj2hvlu:/home# dpkg -l|grep ceph
    ii ceph 0.80.7-1+hLinux amd64
    [hLinux]distributed storage and file system
    ii ceph-common 0.80.7-1+hLinux amd64
    [hLinux]common utilities to mount and interact with a ceph storage cluster
    ii libcephfs1 0.80.7-1 amd64
    Ceph distributed file system client library
    ii python-ceph 0.80.7-1+hLinux amd64
    [hLinux]Python libraries for the Ceph distributed filesystem



--></p>
</section>
<section id="next-steps"> <title>Next Steps</title>
<p>
  <xref href="../../../commercial/GA1/ceph/1.1commenrcial.ceph-helion-openstack-glance-ceph-interoperability.xml" >Ceph Glance-Ceph Interoperability</xref>
</p>
<!-- ===================== horizontal rule ===================== -->
<p>
  <xref href="#topic6505"> Return to Top </xref>
</p>
</section>
</body>
</topic>
