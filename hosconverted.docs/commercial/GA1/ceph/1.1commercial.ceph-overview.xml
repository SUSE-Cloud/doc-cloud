<?xml version="1.0" encoding="UTF-8"?>
<!--Edit status: not edited-->
<!DOCTYPE topic
  PUBLIC "-//OASIS//DTD DITA Topic//EN" "http://docs.oasis-open.org/dita/v1.1/OS/dtd/topic.dtd"><topic xml:lang="en-us" id="topic7563">
<title>HPE Helion <tm tmtype="reg">OpenStack</tm> 1.1 and 1.1.1: Ceph Firefly 0.80.7 Storage Solution</title>
<titlealts>
<searchtitle>HPE Helion Openstack 1.1: Ceph Firefly 0.80.7 Storage Solution</searchtitle>
</titlealts>
<prolog>
<metadata>
<othermeta name="layout" content="default"/>
<othermeta name="product-version" content="HPE Helion Openstack"/>
<othermeta name="product-version" content="HPE Helion Openstack 1.1"/>
<othermeta name="role" content="Storage Engineer"/>
<othermeta name="role" content="Storage Architect"/>
<othermeta name="role" content="Storage Administrator"/>
<othermeta name="role" content="Storage Engineer"/>
<othermeta name="role" content="Service Developer"/>
<othermeta name="role" content="Cloud Administrator"/>
<othermeta name="role" content="Application Developer"/>
<othermeta name="role" content="Paul F"/>
<othermeta name="product-version1" content="HPE Helion Openstack"/>
<othermeta name="product-version2" content="HPE Helion Openstack 1.1"/>
</metadata>
</prolog>
<body>
<p>
<!--PUBLISHED-->
 <!--./commercial/GA1/ceph/1.1commercial.ceph-overview.md  -->
 <!--permalink: /helion/openstack/1.1/services/ceph/--></p>
<p>The HPE Helion OpenStack 1.1 and 1.1.1 Ceph Storage Solution provides an unified scaleable and stable storage solution for the management of HPE Helion OpenStack Image service (Glance), Compute (Nova Boot Volumes) service, and Volume Storage (Cinder persistent Volumes) service. The solution also supports user backup and archive workloads to the Object Storage (Swift) API service writing to the same unified Ceph storage platform.</p>
<p>This guide focuses on post installation, configuration and integration between HPE Helion OpenStack: 1.1 and 1.1.1 and Ceph Firefly 0.80.7 running on the hlinux kernel 3.14-29.4.</p>
<p>This guide assumes that you are familiar with the concepts of OpenStack and Ceph. The main purpose of this guide is describe the integration of Ceph Block Storage with HPE Helion OpenStack 1.1 and 1.1.1, detail steps to install dependencies, configure HPE Helion OpenStack and Ceph Firefly, and provide troubleshooting guidance.</p>
<!--Although installation steps are outlined, these are mostly as validity checks for dependencies. Most Enterprise Customers should have HPE size and assist with the installation of HPE Helion OpenStack 1.1 and 1.1.1, and Inktank size and assist with the installation of Ceph Firefly 0.80.7. -->
<section id="ceph-overview"> <title>Ceph Overview</title>
<p>Ceph is an Open Source, scalable, software-defined storage system running on HPE servers comprised of a  <xref type="section" href="#topic7563/block-storage">block storage</xref>, <xref type="section" href="#topic7563/object-storage">object storage</xref> and <xref type="section" href="#topic7563/file-system">file system</xref> with a unified management. <!--HPE is committed to contribute to OpenStack integration with  management and extensions to Ceph Open Source Storage as a Solution.--></p>
<p>Ceph is designed to deliver different types of storage interfaces to the end user in the same storage platform. The integration  of HPE Helion OpenStack and Ceph is the usage of <xref type="section" href="#topic7563/block-storage">block storage</xref> of Ceph's RADOS Block Device (RDB). Also, the integration between User Application Archive and Backup Workloads running externally or in Virtual Machines in HPE Helion OpenStack is the usage of <xref type="section" href="#topic7563/object-storage">object storage</xref> of the Ceph RADOS Gateway (RADOSGW).</p>
<p>The Ceph RADOSGW provides a REST interface with extensions which offers compatibility with the Swift API. Therefore the existing applications with the integration to the HPE Helion OpenStack Swift API can port seamlessly from a OpenStack Swift backend storage platform to the Ceph Solution.</p>
</section>
<section id="block-storage"> <title>Understading Block Storage</title>
<p>The Ceph Block Device is a network-attached device, which is introduced to the HPE Helion OpenStack Controller and Compute Nodes for Glance, Nova and Cinder storage utilization, or, in order to meet unique SLAs, to the Virtual Machines in the KVM Helion OpenStack managed hypervisor. Glance can be configured with either Ceph object or block storage. Hence, HPE has followed Ceph's best practices and is limiting support for Glance storage of images to block storage. This allows for the support of Ceph's Clone and Copy on Write Feature (COW).</p>
</section>
<section id="object-storage"> <title>Understanding Object Storage</title>
<p>The Ceph RADOSGW is a REST API which supports the Swift API. HPE Helion OpenStack supports object storage primarily for user workloads, ranging from COTS applications running in Virtual Machines (such as MySQL which frequently needs to archive tar files to a reliable and resilient archive) to custom LOB Solutions (which require frequent and aggressive snapshots orchestrated in a consistency group across many VMs and external baremetal systems).</p>
</section>
<section id="file-system"> <title>File System</title>
<p>Ceph provides a POSIX-compliant network file system that aims for high performance, large data storage and so on. In this release we are not supporting the File Interface. There is an alternative way suggested, that is, either leveraging Ceph's RBD Block storage, Ceph's RADOSGW Object Storage through Swift API, or, for user archive and backup type workloads, Open Source Swift API-compatible File System interfaces such as Duplicity.</p>
</section>
<section id="ceph-cluster"> <title>Ceph Cluster</title>
<p>The Ceph Object Storage Daemon (OSD) stores data, handles data replication, recovery, rebalancing, and provides information to Ceph monitors by verifying other Ceph OSD Daemons for a heartbeat. It is best to maintain three OSD Servers to take into account the default three replicas. The number of OSD Daemons per Server is a part of the sizing exercise unique to each customer. For an example, a stable configuration is nine OSD servers with 12 2TB disks per server. Other examples are nine OSD servers with 25 4TB disks per server such as the HPE SL4540.</p>
<p>The Ceph Monitor maintains a master copy of the Ceph Cluster Map including the OSD map, monitor map, placement group (PG) map, and the CRUSH map. Ceph maintains a history called an "epoch" of each state change in the Ceph monitors, Ceph OSD daemons, and placement groups. It is best to maintain in Production three Ceph Monitors (if one Monitor fails, the System will still be operational). If you need additional monitors, upgrade the number of Monitors in increments of odd numbers, such as five Ceph monitors. Server types range from HPE SL230s to HPE DL360s.</p>
</section>
<section id="ceph-object-gateway"> <title>Ceph Object Gateway</title>
<p>The Ceph object gateway is built on <codeph>librgw</codeph> to provide applications with a RESTful gateway to Ceph storage clusters. The Ceph object storage supports the following two interfaces:</p>
<ol>
<li>
          <b>Swift-compatible</b>: Provides object storage functionality with a large subset of the
          OpenStack Swift API. HPE supports this interface and 3rd-party integrations with the Swift
          API.</li>
<li>
          <b>S3-compatible</b>: Provides object storage functionality with a large subset of the
          Amazon S3 RESTful API. This functionality is not support in this release
          <!--This is not supported by HPE as part of the Solution, but it has passed minimal API testing.--></li>
</ol>
<p>The Ceph RADOSGW daemon (radosgw) is a fastCGI module for interacting with a Ceph storage cluster. The Ceph object gateway can store data in the same Ceph storage cluster used 
to store data from Ceph block device clients or from Ceph file system clients. Ceph maintains its own user authentication and allows for extension to HPE Helion OpenStack Keystone Authentication. For User Archive and Backup workloads originating from HPE Helion OpenStack Tenant Project Virtual Machines, HPE recommends integrating through Keystone as a consistent authentication model.</p>
<p>The following diagram shows Helion OpenStack Virtual Machine Access to Ceph Storage</p>
<p>
  <image href="../../../media/helion_virtual_machine_access_ceph_storage.png" placement="break"/>
</p>
</section>

<section id="integrating-ceph-block-storage-with-hp-helion-openstack"> <title>Integrating Ceph Block Storage with HPE Helion OpenStack</title>
  <p>Please see the Ceph topics on the left for integration with HPE Helion OpenStack.</p>
  <!--
<p>The following sections provides details on the integration of Ceph Block Storage with HPE Helion OpenStack</p>
<ol>
<li>
<xref href="1.1commerical.services-ceph-archref.xml">HPE Helion OpenStack 1.x Ceph Architectural Reference</xref>
</li>
<li>
<xref href="1.1commenrcial.ceph-pre-requisite.xml">HPE Helion OpenStack 1.x Ceph Firefly 0.80.7 Storage Solution: Prerequisites</xref>
</li>
<li>
<xref href="1.1commercial.ceph-automated-install.xml">Automated Installation</xref>
</li>
<li>
<xref href="1.1commercial.ceph-cluster-client-node-configuration-ansible.xml">Configure Ceph Cluster and Client Nodes using Ansible Playbooks</xref>
</li>
<li>
<xref href="../1.1commenrcial.ceph-monitoring.xml">Ceph Monitoring</xref>
</li>
<li>
          <xref
            href="1.1commercial.ceph-helion-openstack-ceph-storage-administration.xml">Ceph
            Administration</xref>
             </li>
<li>
<xref href="1.1commercial.ceph-rados-gateway-dmz-ha-proxy.xml">Ceph RADOS Gateway - DMZ HAProxy</xref>
</li>
<li>
<xref href="1.1commercial.ceph-rados-gateway-client.xml">RADOS Gateway Client</xref>
</li>
<li>
<xref href="1.1commercial.ceph-manual-install.xml">Manual Validation</xref>
</li>
<li>
<xref href="1.1commercial.ceph-authentication.xml">Ceph Authentication</xref>
</li>
<li>
<xref href="1.1commercial-ceph-hp-helion-openstack-ceph-configuration.xml">Manual Ceph Configuration</xref>
</li>
<li>
<xref href="1.1commenrcial.ceph-helion-openstack-glance-ceph-interoperability.xml">Helion OpenStack Glance-Ceph Interoperability</xref>
</li>
<li>
  <xref href="1.1commenrcial.ceph-helion-openstack-ceph-hp-helion-openstack-cinder-storage.xml">Helion OpenStack Cinder Ceph Storage</xref>
</li>
<li>
<xref href="1.1commercial-ceph-helion-openstack-nova-ceph-Storage.xml">Helion OpenStack Nova Ceph Storage</xref>
</li>
<li>
<xref href="1.1commercial.ceph-rados-gateway.xml">Ceph RADOS Gateway</xref>
</li>
<li>
<xref href="1.1commercial.ceph-rados-gateway-pools.xml">Ceph RADOS Gateway Pools</xref>
</li>
<li>
<xref href="1.1commercia.ceph-high-availability-RADOSGW-authentication.xml">Ceph RADOS Gateway- Keystone Authentication</xref>
</li>
<li>
<xref href="1.1commenrcial.ceph-troubleshooting.xml">Troubleshooting</xref>
</li>
<li>
<xref href="1.1commercial.ceph-glossary.xml">Glossary</xref>
</li>
<li>
<xref href="1.1commenrcial.ceph-related-topics.xml">Related Topics</xref>
</li>
</ol>
-->
</section>
<!--<section id="next-steps"> <title>Next Steps</title>
<p>
  <xref href="1.1commerical.services-ceph-archref.xml">HPE  Helion OpenStack Ceph Architectural Reference</xref>
</p>

</section>-->
</body>
</topic>
