<?xml version="1.0" encoding="UTF-8"?>
<!--Edit status: not edited-->
<!DOCTYPE topic
  PUBLIC "-//OASIS//DTD DITA Topic//EN" "http://docs.oasis-open.org/dita/v1.1/OS/dtd/topic.dtd" >
<topic xml:lang="en-us" id="topic14660">
  <title>HPE Helion <tm tmtype="reg">OpenStack</tm> 1.1: Deploying Additional Scale-out Object
    Storage (Swift) Nodes</title>
  <prolog>
    <metadata>
      <othermeta name="layout" content="default"/>
      <othermeta name="product-version" content="HPE Helion Openstack"/>
      <othermeta name="product-version" content="HPE Helion Openstack 1.0"/>
      <othermeta name="product-version" content="HPE Helion Openstack 1.0.1"/>
      <othermeta name="product-version" content="HPE Helion Openstack 1.1"/>
      <othermeta name="role" content="Storage Administrator"/>
      <othermeta name="role" content="Storage Architect"/>
      <othermeta name="role" content="Ranjan J, Keshava HPE Binamra S"/>
      <othermeta name="product-version1" content="HPE Helion Openstack"/>
      <othermeta name="product-version2" content="HPE Helion Openstack 1.0"/>
      <othermeta name="product-version3" content="HPE Helion Openstack 1.0.1"/>
      <othermeta name="product-version4" content="HPE Helion Openstack 1.1"/>
    </metadata>
  </prolog>
  <body>
    <p>
      <!--PUBLISHED-->
      <!--./commercial/GA1/1.1commerical.services-swift-deployment.md-->
      <!--permalink: /helion/openstack/1.1/services/swift/deployment-scale-out/--></p>
    <p> </p>
    <p>The scale-out object storage is realized by defining a new storage policy - storage-policy:1.
      Object-1 ring is associated with storage-policy:1. This ring is used to store end user data.
      Once the storage-policy:1 is created, it is the default policy and all of the containers would
      be on this policy unless otherwise specified. We recommend you to use at least <b>two</b>
      nodes to implement storage-policy:1. Also, you can extend the object storage by adding one or
      more nodes to object-ring:1 as per your requirement.</p>
    <p>Perform the following steps to deploy scale-out object-ring:1</p>
    <ol>
      <li>
        <xref type="section" href="#topic14660/preq">Prerequisite</xref>
      </li>
      <li>
        <xref type="section" href="#topic14660/define-object-ring:1">Define the attributes of Object
          Ring-1</xref>
      </li>
      <li>
        <xref type="section" href="#topic14660/prepare-undercloudswift">Prepare the undercloud to
          manage Swift Clusters</xref>
      </li>
      <li>
        <xref type="section" href="#topic14660/deploying-scale-out-Swift-object-nodes">Deploy the
          Scale-out Object Nodes</xref>
      </li>
      <li>
        <xref href="#topic14660/verifying-deployed-Swift-nodes" format="dita"/></li>
      <li><xref href="#topic14660/preparing-disks-on-swift-nodes" format="dita"/>
      </li>
      <li>
        <xref type="section" href="#topic14660/creating-scale-out-object-ring">Create a New
          Scale-out Object Ring</xref>
      </li>
      <li>
        <xref type="section" href="#topic14660/copying-the-rings-to-all-Swift-nodes">Copy the New
          Ring(s) to All Nodes</xref>
      </li>
      <li>
        <xref type="section" href="#topic14660/update-storage-scaleout-swift">Update the Storage
          Policy</xref>
      </li>
    </ol>
    <section id="preq">
      <title>Prerequisites</title>
      <p>A HPE Helion OpenStack cloud must be deployed. Functional Swift starter nodes are created
        as an integral part of cloud deployment.</p>
    </section>
    <section id="define-object-ring:1">
      <title>Define the Ring Attributes of Object Ring-1</title>
      <p>
        <b>Caution</b>:Plan the following ring attributes before deployment of object-ring:1 because
        many attributes, such as <b>part power</b>, cannot be changed after the ring has been
        deployed.</p>
    </section>
    <section id="table-a">
      <title>Table A</title>
      <table>
        <tgroup cols="3">
          <colspec colname="col1" colsep="1" rowsep="1"/>
          <colspec colname="col2" colsep="1" rowsep="1"/>
          <colspec colname="col3" colsep="1" rowsep="1"/>
          <thead>
            <row>
              <entry>Attributes</entry>
              <entry><ph>Definition</ph>
              </entry>
              <entry><ph>Recommendation</ph>
              </entry>
            </row>
          </thead>
          <tbody>
            <row>
              <entry>
                <b>Zone</b>
              </entry>
              <entry>Defines single point of failure within your cluster. </entry>
              <entry>It is recommended to use a single zone with multiple servers. Having multiple
                servers (at least three) ensures that the replicas are distributed across servers,
                not all on one.</entry>
            </row>
            <row>
              <entry>
                <b>Replica Count</b>
              </entry>
              <entry>Defines the number of copies of objects created.</entry>
              <entry>The recommended value is 3.</entry>
            </row>
            <row>
              <entry>
                <b>Part Power</b>
              </entry>
              <entry>Defines the storage cluster capacity. Set the <varname>partition
                  power</varname> value based on the total amount of storage you expect your entire
                ring to use.</entry>
              <entry> The parameter is an exponent: 2^X. Assuming that the average available drive
                capacity is 1-3 TB. For the partition power values, refer to <xref
                  href="1.1commerical.services-swift-deployment-part-power.dita">Ring Power and
                  Number of Partitions</xref>. </entry>
            </row>
            <row>
              <entry>
                <b>Min part hour</b>
              </entry>
              <entry>This should be set to however long a full replication/update cycle takes. No
                partition is moved twice during the specified amount of time.</entry>
              <entry> The recommended value is 24 hours. Note that this restriction is ignored in
                the case of device failure when there is no option other than reassignment.</entry>
            </row>
          </tbody>
        </tgroup>
      </table>
    </section>
    <section id="table-b-other-attributes">
      <title>Table B - Other Attributes</title>
      <table>
        <tgroup cols="2">
          <colspec colname="col1" colsep="1" rowsep="1"/>
          <colspec colname="col2" colsep="1" rowsep="1"/>
          <thead>
            <row>
              <entry>Attributes</entry>
              <entry><ph>Definition</ph>
              </entry>
            </row>
          </thead>
          <tbody>
            <row>
              <entry>
                <b>Disk Label</b>
              </entry>
              <entry>Defines disk label of the disk. For more information refer to <xref
                  href="#topic14660/preparing-disks-on-swift-nodes" format="dita">Preparing disks on
                  Swift nodes</xref>
              </entry>
            </row>
            <row>
              <entry>
                <b>Port</b>
              </entry>
              <entry>Port specifies the port number of the node. It is recommended to use the
                following port number: <!--A BR tag was used here in the original source.--><ul>
                  <li> Object ring - 6000</li>
                  <li> Container ring - 6001 </li>
                  <li> Account ring - 6002</li>
                </ul>
              </entry>
            </row>
            <row>
              <entry>
                <b>Weight</b>
              </entry>
              <entry>Weight helps the cluster to calculate the partition that must be assigned to
                the drive. The higher the weight, the greater number of partitions Swift must assign
                to the drive. It is recommended to add or remove drives gradually using a weighted
                approach to avoid degraded performance of the Swift cluster. The weight will
                gradually increase or decrease by 25% until it becomes 100% or 0.</entry>
            </row>
          </tbody>
        </tgroup>
      </table>
    </section>
    <section id="prepare-undercloudswift">
      <title>Prepare the undercloud to manage Swift clusters</title>
      <p>Using the <b>
          <i>ringos</i>
        </b> utility you can add the <xref
          href="../../commercial/GA1/1.1commerical.services-swift-deployment-provision-swift-node.dita"
          >provisioned nodes</xref> to the Swift cluster.</p>
      <ol>
        <li>Enable SSH from the undercloud to the overcloud node by copying the SSH keys from the
          seed to the undercloud.</li>
        <li>Log in to the
          seed.<codeblock><codeph># ssh root@&lt;seed IP address&gt;
</codeph></codeblock></li>
        <li>Copy the SSH key from the seed cloud to
          undercloud.<codeblock><codeph># scp ~/.ssh/id_rsa heat-admin@&lt;\undercloud IP address&gt;:/home/heat-admin
</codeph></codeblock></li>
        <li>Log in to the
          undercloud.<codeblock><codeph># ssh heat-admin@&lt;undercloud IP address&gt;
# sudo -i

# mv ~heat-admin/id_rsa ~/.ssh/ 
</codeph></codeblock></li>
      </ol>
    </section>
    <section id="deploying-scale-out-Swift-object-nodes">
      <title>Deploy the Scale-out Object Nodes</title>
      <!--Source the same file (`kvm-default.json`or `kvm-custom-ips.json`)that is used during the initial installation for specifying the environment IPs. In the following steps we are assuming the usage of kvm-custom-ips.json file at the initial installation.-->
      <p>
        <!--
Before starting the deployment of scale-out object nodes you must configure the `kvm-default.json` file. If `kvm-default.json` has already been created during installation, edit the file instead.-->
        Perform the following steps to deploy scale-out Object nodes:</p>
      <ol>
        <li>Log in to the
          seed.<codeblock><codeph>ssh root@&lt;seed IP address&gt;
</codeph></codeblock></li>
        <li>Update the <codeph>so_swift_storage_scale</codeph> parameter in the environment
          variables file, used during the initial installation, according to your storage
            needs.<!--    
2. Update the `so_swift_storage_scale` parameter in  `/root/configs/kvm-custom-ips.json ` file according to your storage needs.--><p>Refer
              <xref
              href="../../commercial/GA1/1.1commerical.services-swift-deployment-provision-swift-node.dita"
              >Provisioning Swift node(s)</xref> for details of adding physical server for scale-out
            Swift in <codeph>baremetal.csv</codeph> file.</p></li>
        <li>Enter the following command to source the environment variables file for the new
            values.<codeblock><codeph># source /root/tripleo/tripleo-incubator/scripts/hp_ced_load_config.sh /root/tripleo/configs/&lt;environment variables file name&gt;
</codeph></codeblock><p>For
            example:</p><codeblock><codeph># source /root/tripleo/tripleo-incubator/scripts/hp_ced_load_config.sh /root/tripleo/configs/kvm-custom-ips.json
</codeph></codeblock><!--
3. Enter the following command to source the `kvm-default.json`  for the new values.

        # source /root/tripleo/tripleo-incubator/scripts/hp_ced_load_config.sh /root/tripleo/configs/kvm-default.json

3. Source the environment variables file created during initial installation. 

        # source /root/kvm-custom-ips.json 


        source tripleo/tripleo-incubator/scripts/hp_ced_load_config.sh tripleo/configs/kvm-custom-ips.json

--></li>
        <li>Run the installer script to update the
          cloud.<codeblock><codeph># bash -x tripleo/tripleo-incubator/scripts/hp_ced_installer.sh --update-overcloud |&amp; tee update_cloud.log
</codeph></codeblock></li>
      </ol>
    </section>
    <section id="verifying-deployed-Swift-nodes">
      <title>Verify the Deployed Nodes</title>
      <p>Perform the following steps to verify the deployment of Swift nodes:</p>
      <ol>
        <li>Log in to the undercloud from the
          seed.<codeblock><codeph># ssh heat-admin@&lt;undercloud IP address&gt; 
# sudo -i
</codeph></codeblock></li>
        <li>Source <codeph>stackrc</codeph> using the following
          command:<codeblock><codeph># source stackrc 
</codeph></codeblock></li>
        <li>List the available scale-out Swift
            nodes.<codeblock><codeph># ringos list-swift-nodes -t object
</codeph></codeblock><p>The
            following sample displays scale-out Swift
          nodes:</p><codeblock><codeph>+--------------+
| object-nodes |
+--------------+
| 192.0.2.29   |
| 192.0.2.30   |
+--------------+
</codeph></codeblock></li>
        <li>List the disks available on each
            node.<codeblock><codeph># ringos list-disks -n &lt;object nodes IP address&gt; 
</codeph></codeblock><p>The
            following sample displays the disk available on the node
          <b>192.0.2.29</b>.</p><codeblock><codeph>+----------+------------+
| disk     | size       |
+----------+------------+
| /dev/sda | 1073741824 |
|          |            |
| /dev/sdb | 1073741824 |
+----------+------------+
</codeph></codeblock></li>
        <li>Repeat the verification process for all of the object node(s).</li>
      </ol>
    </section>
    <section id="preparing-disks-on-swift-nodes">
      <title>Prepare the Disks on the Deployed Nodes</title>
      <p>Once Swift nodes are deployed, the next step is to format and mount the required disks so
        they can be added to the Swift cluster.</p>
      <p>To format a given disk:</p>
      <codeblock><codeph># ringos format-disks -n &lt;object node IP address&gt; -d all
</codeph></codeblock>
      <p>The following sample displays the output of formatted disk of <b>192.0.2.29</b>.</p>
      <codeblock><codeph>+----------+-----------+---------+---------------------------------+-------------+------------+
| disk     | formatted | mounted | mount_point                     | label       | size       |
+----------+-----------+---------+---------------------------------+-------------+------------+
| /dev/sda | y         | y       | /mnt/state/srv/node/a1410063335 | a1410063335 | 1073741824 |
| /dev/sdb | y         | y       | /mnt/state/srv/node/b1410063336 | b1410063336 | 1073741824 |               
+----------+-----------+---------+---------------------------------+-------------+------------+
</codeph></codeblock>
      <p>If the disks are already mounted and you want to format the disk then execute the following
        command:</p>
      <codeblock><codeph># ringos format-disks -n &lt;object node IP address&gt; -d /dev/sdb -f
</codeph></codeblock>
      <p>The following sample displays the output of forceful formatted disk.</p>
      <codeblock><codeph>+----------+-----------+---------+-------------+-------+------+
| disk     | formatted | mounted | mount_point | label | size |
+----------+-----------+---------+-------------+-------+------+
| /dev/sdb | -         | -       | -           | -     | -    |
|          |           |         |             |       |      |
+----------+-----------+---------+-------------+-------+------+
</codeph></codeblock>
      <p>
        <note>You can also format disks individually by using <codeph>-d
            &lt;device-name&gt;</codeph>. For more details, see the <xref
            href="../../commercial/GA1/1.1commerical.services-object-pyringos.dita">ringos</xref>
          manual.</note>
      </p>
      <p>Repeat the above steps for all of the object nodes.</p>
    </section>
    <section id="creating-scale-out-object-ring">
      <title>Create a New Scale-out Object Ring</title>
      <p>Once the disk is formatted you can create a scale-out object ring. This ring is created for
        the scale-out Swift which is an extension of the Starter Swift nodes which are installed by
        default during HPE Helion OpenStack cloud deployment.</p>
      <ol>
        <li>Create a directory named
          <codeph>ring-building</codeph>.<codeblock><codeph># mkdir -p /root/ring-building
# cd /root/ring-building
</codeph></codeblock></li>
        <li>Create a ring with the attribute specified in <xref type="section"
            href="#topic14660/define-object-ring:1">Defining ring attributes of
            object-ring:1</xref>.<codeblock><codeph># ringos create-ring -f /root/ring-building/object-1.builder -p &lt;part_power&gt; -r &lt;replicas&gt; -m &lt;min_part_hours&gt;
</codeph></codeblock><p>The
            following sample displays the creation of ring by adding scale-out Swift node to a zone
            with partition power =10, replicas =3, min_part_hours
          =24</p><codeblock><codeph># ringos create-ring -f /root/ring-building/object-1.builder -p 10 -r 3 -m 24
created ring /root/ring-building/object-1.builder
</codeph></codeblock></li>
        <li>Add the disk to the ring.<codeblock><codeph># ringos add-disk-to-ring -f /root/ring-building/object-1.builder -i  &lt;object node IP address&gt; -p  &lt;port&gt; -d &lt;disk label&gt; -w &lt;weight&gt; -r &lt;region&gt; -z &lt;zone&gt;
</codeph></codeblock><p>
            <note> You can use the labels and disks which are obtained in the output of the section
                <xref type="section" href="#topic14660/preparing-disks-on-swift-nodes">Preparing
                disks on Swift nodes</xref>. For more information on the ring attributes refer to
                <xref type="section" href="#topic14660/define-object-ring:1">Defining ring
                attributes of object-ring:1</xref>.</note>
          </p><p>The following sample displays the addition of disk to <b>192.0.2.29</b> and its
            output.</p><codeblock><codeph># ringos add-disk-to-ring -f /root/ring-building/object-1.builder -i 192.0.2.29 -p 6000 -d a1410063335 -w 100 -r 1 -z 1
Added disk 192.0.2.29:a1410063335 to ring
</codeph></codeblock></li>
        <li>Verify the content of <codeph>object-1.builder</codeph> file to ensure that it meets
          your required
            configuration.<codeblock><codeph>`# ringos view-ring -f /root/ring-building/object-1.builder`
</codeph></codeblock><p>The
            following sample displays the content of <codeph>object-1.builder</codeph>
          file:</p><codeblock><codeph>object-1.builder, build version 9
1024 partitions, 3.000000 replicas, 1 regions, 3 zones, 9 devices, 100.00 balance
The minimum number of hours before a partition can be reassigned is 1
Devices:id  region  zone  ip address  port  replication ip  replication port  name weight partitions balance meta
 0   1 1  192.0.2.29  6000  192.0.2.29  6000 a1410063335 100.00  0 -100.00
 1   1 1  192.0.2.29  6000  192.0.2.29  6000 b1410063336 100.00  0 -100.00
</codeph></codeblock></li>
        <li>Re-balance the
            ring.<codeblock><codeph># ringos rebalance-ring -f /root/ring-building/object-1.builder
</codeph></codeblock><p>This
            will generate an <b>object-1.ring.gz</b> file.</p><p>The following sample displays the
            output of re-balancing the
          ring:</p><codeblock><codeph>Rebalanced ring /root/ring-building/object-1.builder
</codeph></codeblock></li>
        <li>Verify the content <codeph>object-1.builder</codeph> file after re-balancing the
          ring.<codeblock><codeph># ringos view-ring -f /root/ring-building/object-1.builder
</codeph></codeblock></li>
      </ol>
    </section>
    <section id="copying-the-rings-to-all-Swift-nodes">
      <title>Copy the New Ring(s) to All Nodes</title>
      <ol>
        <li>List all the rings from the starter Swift
            nodes.<codeblock><codeph># ringos list-swift-nodes -t starter
</codeph></codeblock><p>The
            following sample displays the list of Starter Swift
          nodes.</p><codeblock><codeph>+---------------+
| starter-nodes |
+---------------+
| 192.0.2.22    |
| 192.0.2.24    |
+---------------+
</codeph></codeblock></li>
        <li>Get all the rings (first three files in the sample below) and builder files (last three
          files) from either of the starter
            nodes.<codeblock><codeph>rsync -qzp --rsync-path="sudo rsync" heat-admin@&lt;starter Swift nodes IP address&gt;:/etc/swift/object.ring.gz /root/ring-building/
rsync -qzp --rsync-path="sudo rsync" heat-admin@&lt;starter Swift nodes IP address&gt;:/etc/swift/account.ring.gz /root/ring-building/
rsync -qzp --rsync-path="sudo rsync" heat-admin@&lt;starter Swift nodes IP address&gt;:/etc/swift/container.ring.gz /root/ring-building/
rsync -qzp --rsync-path="sudo rsync" heat-admin@&lt;starter Swift nodes IP address&gt;:/etc/swift/object.builder /root/ring-building/
rsync -qzp --rsync-path="sudo rsync" heat-admin@&lt;starter Swift nodes IP address&gt;:/etc/swift/account.builder /root/ring-building/
rsync -qzp --rsync-path="sudo rsync" heat-admin@&lt;starter Swift nodes IP address&gt;:/etc/swift/container.builder /root/ring-building/
</codeph></codeblock><p>The
            following sample displays all three rings and three builder files from <b>19.0.2.22</b>
          </p><codeblock><codeph>rsync -qzp --rsync-path="sudo rsync" heat-admin@19.0.2.22:/etc/swift/object.ring.gz /root/ring-building/
rsync -qzp --rsync-path="sudo rsync" heat-admin@19.0.2.22:/etc/swift/account.ring.gz /root/ring-building/
rsync -qzp --rsync-path="sudo rsync" heat-admin@19.0.2.22:/etc/swift/container.ring.gz /root/ring-building/
rsync -qzp --rsync-path="sudo rsync" heat-admin@19.0.2.22:/etc/swift/object.builder /root/ring-building/
rsync -qzp --rsync-path="sudo rsync" heat-admin@19.0.2.22:/etc/swift/account.builder /root/ring-building/
rsync -qzp --rsync-path="sudo rsync" heat-admin@19.0.2.22:/etc/swift/container.builder /root/ring-building/
</codeph></codeblock><p>
            <note>The ring files are identical on both the starter Swift nodes. You can copy
                <b>.gz</b> files in either of the starter nodes as they are identical.</note>
          </p></li>
        <li>List all the Swift
          nodes.<codeblock><codeph># ringos list-swift-nodes -t  all
</codeph></codeblock></li>
        <li>Copy account, container, object, and generated <codeph>object-1.ring.gz</codeph> files
          to all Swift nodes and press <b>yes</b> when asked to authenticate
            node.<codeblock><codeph># ringos copy-ring -s /root/ring-building/\*.ring.gz -n &lt;Swift node IP address&gt;
</codeph></codeblock><p>In
            the following example account, container, object, and generated
              <codeph>object-1.ring.gz</codeph> are copied to all the
          nodes:</p><codeblock><codeph># ringos copy-ring -s /root/ring-building/\*.ring.gz -n 192.0.2.22
# ringos copy-ring -s /root/ring-building/\*.ring.gz -n 192.0.2.29

The authenticity of host '192.0.2.29 (192.0.2.29)' can't be established.
ECDSA key fingerprint is 8a:eb:b7:66:3b:5f:fa:d6:d1:49:80:1a:a7:90:79:20.
Are you sure you want to continue connecting (yes/no)? yes
Copied ring /root/ring-building/object-1.ring.gz onto 192.0.2.29
</codeph></codeblock></li>
        <li>Copy <codeph>object-1.builder</codeph> files to all Swift nodes and press <b>yes</b>
          when asked to authenticate node.<codeblock><codeph># ringos copy-ring -s /root/ring-building/\*.builder -n &lt;Swift node IP address&gt;
</codeph></codeblock><p>
            <note> The <codeph>.buldier</codeph> and <codeph>.ring.gz</codeph> files <b>must</b> be
              present in the Swift nodes.</note>
          </p></li>
      </ol>
      <p>You can also copy the ring or builder files to all the Swift nodes:</p>
      <codeblock><codeph># ringos copy-ring -s /root/ring-building/\*.ring.gz -n all

# ringos copy-ring -s /root/ring-building/\*.builder -n all 
</codeph></codeblock>
      <!--**Note**: The system may escape the authentication of node sometimes. -->
    </section>
    <section id="update-storage-scaleout-swift">
      <title>Update the Storage Policy</title>
      <ol>
        <li>Log in to
          seed.<codeblock><codeph># ssh root@&lt;seed IP address&gt;
</codeph></codeblock></li>
        <li>Edit <codeph>/root/tripleo/hp_passthrough/overcloud_swift_conf.json</codeph> to replace
          the default values with the ones
          listed:<codeblock><codeph>{"swift":
  {"config":
    [ 
      {"section": "storage-policy:0",
       "values":
        [ 
          {"option": "name",
           "value": "Policy-0"
          },
          {"option": "default",
           "value": "no"
          }
        ]
      },
      {"section": "storage-policy:1",
       "values":
        [
          {"option": "name",
           "value": "ScaleOut"
          },
          {"option": "default",
           "value": "yes"
          }
        ]
      }
    ]
  }
}
</codeph></codeblock><!--

3. Source the `kvm-default.json` file.

    # source /root/tripleo/tripleo-incubator/scripts/hp_ced_load_config.sh /root/tripleo/configs/kvm-default.json
--></li>
        <li>Source the environment variables
            file.<codeblock><codeph># source tripleo/tripleo-incubator/scripts/hp_ced_load_config.sh tripleo/configs/&lt;environment variables file name&gt;
</codeph></codeblock><p>For
            example:</p><codeblock><codeph># source tripleo/tripleo-incubator/scripts/hp_ced_load_config.sh tripleo/configs/kvm-custom-ips.json
</codeph></codeblock></li>
        <li>Run the installer script to update the storage policies across the
          cloud.<codeblock><codeph># bash -x tripleo/tripleo-incubator/scripts/hp_ced_installer.sh --update-overcloud |&amp; tee update_cloud.log
</codeph></codeblock></li>
      </ol>
      <p>
        <xref href="#topic14660"> Return to Top </xref>
      </p>
      <!-- ===================== horizontal rule ===================== -->
    </section>
  </body>
</topic>
