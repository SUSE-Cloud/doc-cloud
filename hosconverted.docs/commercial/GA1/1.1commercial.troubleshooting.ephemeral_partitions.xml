<?xml version="1.0" encoding="UTF-8"?>
<!--Edit status: not edited-->
<!DOCTYPE topic
  PUBLIC "-//OASIS//DTD DITA Topic//EN" "http://docs.oasis-open.org/dita/v1.1/OS/dtd/topic.dtd" ><topic xml:lang="en-us" id="topic2138">
<title>HPE Helion <tm tmtype="reg">OpenStack</tm> 1.1: Troubleshooting Ephemeral Partitions</title>
<prolog>
<metadata>
<othermeta name="layout" content="default"/>
<othermeta name="product-version" content="HPE Helion Openstack"/>
<othermeta name="product-version" content="HPE Helion Openstack 1.1"/>
<othermeta name="role" content="Systems Administrator"/>
<othermeta name="role" content="Cloud Architect"/>
<othermeta name="role" content="Storage Administrator"/>
<othermeta name="role" content="Network Administrator"/>
<othermeta name="role" content="Service Developer"/>
<othermeta name="role" content="Cloud Administrator"/>
<othermeta name="role" content="Application Developer"/>
<othermeta name="role" content="Network Engineer"/>
<othermeta name="role" content="Paul F"/>
<othermeta name="product-version1" content="HPE Helion Openstack"/>
<othermeta name="product-version2" content="HPE Helion Openstack 1.1"/>
</metadata>
</prolog>
<body>
<p>
<!--PUBLISHED-->
 <!--./commercial/GA1/1.1commercial.troubleshooting.ephemeral_partitions.md-->
 <!--permalink: /helion/openstack/1.1/services/troubleshooting/ephemeral_partitions/--></p>
<p>HPE Helion OpenStack is an OpenStack technology coupled with a version of <tm tmtype="reg">Linux</tm> provided by
      HP. This topic describes all the known issues that you might encounter. To help you resolve
      these issues, we have provided possible solutions.</p>
<section id="an-ephemeral-partition-fails-to-unmount"> <title>An ephemeral partition fails to unmount</title>
<p>If during an update, you receive the following error message:</p>
<codeblock>
  <codeph>    The ephemeral storage of this system failed to be cleaned up properly and processes or files are still in use. The previous ansible play should have information to help troubleshoot this issue.
</codeph>
</codeblock>
<p>The most likely reason the partition failed to unmount is due to a process still holding a file handle on the partition. This normally happens when a process has failed to stop (where stop is issued by the Ansible playbook) or has been restarted (by <codeph>cron</codeph> for example) after the stop was issued.</p>
<p>To find out which process is still running:</p>
<ul>
<li>
<p>Log onto the failed nodes and enter:</p>
</li>
</ul>
<codeblock>
<codeph>ssh heat-admin@10.23.67.139 
$ sudo su -
root@overcloud-ce-controller-swiftstorage0-h7t3467lf7fx:~# lsof /mnt
COMMAND  PID USER   FD   TYPE DEVICE SIZE/OFF NODE NAME 
stunnel4 33047 root   24w   REG8,1  4812923 62128141 /mnt/state/var/log/stunnel4/helion_stunnel.log
</codeph>
</codeblock>

<ul>
  <li>
<p>In the above example <codeph>stunnel4</codeph> has failed to stop.</p>
</li>
<li>
<p>To stop the process and verify nothing is holding a file reference on <codeph>/mnt</codeph> enter:</p>
</li>
</ul>
<codeblock>
<codeph>root@overcloud-ce-controller-swiftstorage0-h7t3467lf7fx:~# pkill stunnel4
root@overcloud-ce-controller-swiftstorage0-h7t3467lf7fx:~# lsof /mnt
root@overcloud-ce-controller-swiftstorage0-h7t3467lf7fx:~# 
</codeph>
</codeblock>
<ul>
<li>
<p>Follow the procedure to restart <codeph>mysql</codeph> and <codeph>rabbitmq</codeph> from the <i>Tripleo-ansible Troubleshooting Guide</i>.
Once restarted, re-run the overcloud ansible playbook and not the <codeph>update_oc.sh</codeph> script</p>
</li>
</ul>
</section>
</body>
</topic>
