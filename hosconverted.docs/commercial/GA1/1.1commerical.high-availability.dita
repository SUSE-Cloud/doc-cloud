<?xml version="1.0" encoding="UTF-8"?>
<!--Edit status: not edited-->
<!DOCTYPE topic
  PUBLIC "-//OASIS//DTD DITA Topic//EN" "http://docs.oasis-open.org/dita/v1.1/OS/dtd/topic.dtd" ><topic xml:lang="en-us" id="topic21283">
<title>HPE Helion <tm tmtype="reg">OpenStack</tm> 1.1: High Availability (HA)</title>
<prolog>
<metadata>
<othermeta name="layout" content="default"/>
<othermeta name="product-version" content="HPE Helion Openstack"/>
<othermeta name="product-version" content="HPE Helion Openstack 1.0"/>
<othermeta name="product-version" content="HPE Helion Openstack 1.0.1"/>
<othermeta name="product-version" content="HPE Helion Openstack 1.1"/>
<othermeta name="role" content="Storage Administrator"/>
<othermeta name="role" content="Storage Architect"/>
<othermeta name="role" content="Sameer V, Vandana S"/>
<othermeta name="product-version1" content="HPE Helion Openstack"/>
<othermeta name="product-version2" content="HPE Helion Openstack 1.0"/>
<othermeta name="product-version3" content="HPE Helion Openstack 1.0.1"/>
<othermeta name="product-version4" content="HPE Helion Openstack 1.1"/>
</metadata>
</prolog>
<body>
<p>
<!--PUBLISHED-->
 <!--./commercial/GA1/1.1commerical.high-availability.md-->
 <!--permalink: /helion/openstack/1.1/high-availability/--></p>
<p>This page covers the following topics:</p>
<ul>
<li>
<xref type="section" href="#topic21283/concepts-overview">High Availability( HA) concepts overview</xref> 

<ul>
<li>
<xref type="section" href="#topic21283/scope-ha">Scope of High Availability: Protection against Single Points of Failure (SPOF)</xref>
</li>
</ul>
</li>
<li>
<xref type="section" href="#topic21283/ha-helion">Highly Available cloud services</xref>

<ul>
<li>
<xref type="section" href="#topic21283/ha-overcloud-controller">High Availability of Overcloud Controllers</xref>

<ul>
<li>
<xref type="section" href="#topic21283/api-msg-flow">API Request Message Flow</xref>
</li>
<li>
<xref type="section" href="#topic21283/handling-node-failure">Handling Node Failure</xref>
</li>
<li>
<xref type="section" href="#topic21283/handling-network-partition">Handling Network Partitions</xref>

<ul>
<li>
<xref type="section" href="#topic21283/mysql-galera">MySQL Galera Cluster</xref>
</li>
</ul>
</li>
</ul>
</li>
<li>
<xref type="section" href="#topic21283/singleton-services">Singleton Services</xref>

<ul>
<li>
<xref type="section" href="#topic21283/cinder-volume">Cinder Volume</xref>
</li>
<li>
<xref type="section" href="#topic21283/sherpa">Sherpa</xref> </li>
  <li><xref href="#topic21283/nova-consoleauth" format="dita">Nova consoleauth</xref></li>
</ul>
</li>
<li>
<xref type="section" href="#topic21283/replace-rebuild">Rebuilding or Replacing Failed Controller Nodes</xref>
</li>
</ul>
</li>
<li>
<xref type="section" href="#topic21283/ha-cloud-infra">Highly Available Cloud Infrastructure</xref>

<ul>
<li>
<xref type="section" href="#topic21283/availability-zones">Availability Zones</xref>
</li>
<li>
            <xref type="section" href="#topic21283/compute-with-kvm">Compute with KVM</xref>
          </li>
          <li><xref href="#topic21283/nova-availability-zone" format="dita"/>
          </li>
<li>
<xref type="section" href="#topic21283/compute-with-esx">Compute with ESX</xref>
</li>
<li>
<xref type="section" href="#topic21283/block-storage">Block Storage with StoreVirtual VSA</xref>
</li>
<li>
<xref type="section" href="#topic21283/object-storage">Object Storage with Swift</xref>
</li>
<li>
<xref type="section" href="#topic21283/cinder-avail">Cinder Availability Zones</xref>
</li>
</ul>
</li>
<li>
<xref type="section" href="#topic21283/ha-workloads">Highly Available Cloud Applications and Workloads</xref>
</li>
<li>
<xref type="section" href="#topic21283/not-ha">What is not Highly Available?</xref>
</li>
<li>
<xref type="section" href="#topic21283/more-info">More information</xref>
</li>
</ul>
<section id="concepts-overview"> <title>High Availability Concepts Overview</title>
<p>Highly Available Cloud Services ensures that at least a minimum of cloud resources are always available on request, which results in uninterrupted operations for users.</p>
<p>Cloud users are able to provision and manage the compute, storage, and network infrastructure
        resources at any given point in time and the Horizon Dashboard and the OpenStack APIs must
        be reachable and be able to fulfill user requests.</p>
      <p><image href="../../media/Ha-resilient-cloud-infrastructure.png" id="image_nqj_jkv_ps"/></p>
<p> Once the Compute, Storage, and Network resources are deployed, users expect these resources to
        be reliable in the following ways:</p>
<ul>
<li>
<p>If the Nova-Compute KVM Hypervisors/servers hosting the project compute virtual machine(VM) dies and the compute VM is lost along with its local ephemeral storage, the re-launching of the dead compute VM succeeds because it launches on another Nova-Compute KVM Hypervisor/server.</p>
</li>
<li>
<p>If ephemeral storage loss is undesirable, the compute VM can be booted from the Cinder volume.</p>
</li>
<li>
<p>Data stored in Block Storage service volumes is always available and volumes are rarely lost by the service provider.</p>
</li>
<li>
<p>Data stored by the Object service is always available and is rarely lost by the cloud service provider.</p>
</li>
<li>
<p>Network resources such as routers, subnets, and floating IP addresses provisioned by the Networking Operation service are rarely lost by the cloud service provider and will continue to provide a network path to the Compute VMs.</p>
</li>
</ul>
<p>The infrastructure that provides these features is called a <b>Highly Available Cloud Infrastructure</b>.</p>
</section>
<section id="highly-available-cloud-aware-tenant-workloads"> <title>Highly Available Cloud-Aware Tenant Workloads</title>
<p>HPE Helion OpenStack Compute hypervisors do not support transparent high availability for user applications; as such, the project application provider is responsible for deploying their applications in a redundant and highly available manner, using multiple VMs spread appropriately across availability zones, routed through the load balancers and made highly available through clustering.</p>
<p>These are known as <b>Highly Available Cloud-Aware Tenant Workloads</b>.</p>
</section>
<section id="scope-ha"> <title>Scope of High Availability: Protection against Single Points of Failure(SPOF)</title>
<p>In order to achieve this high availability of services, infrastructure and workloads, we define the scope of HA to be limited to protecting these only against single points of failure (SPOF).</p>
<p>Single points of failure include:</p>
<ol>
<li>
          <b>Hardware SPOFs</b>: Hardware failures can take the form of servers failing, memory
          going bad, power failing, hypervisors crashing, hard disks dying, NIC cards breaking,
          switch ports failing, network cables loosening, and so forth.</li>
<li>
          <b>Software SPOFs</b>: Server processes can crash due to software defects, out-of-memory
          conditions, operating system kernel panic, and so forth.</li>
</ol>
<p>By design, HPE Helion OpenStack strives to create a system architecture resilient to SPOFs, and does not attempt to automatically protect the system against multiple cascading levels of failures; such cascading failures will result in an unpredictable state.</p>
<p>Hence, the cloud operator is encouraged to recover and restore any failed component, as soon as the first level of failure occurs.</p>
</section>
<section id="ha-helion"> <title>Highly Available cloud services</title>
<p>The HPE Helion OpenStack installer deploys highly available configurations of OpenStack cloud services, resilient against single points of failure.</p>
</section>
<section id="ha-overcloud-controller"> <title>High Availability of Overcloud Controllers</title>
<p>The high availability of the overcloud controller components comes in two main forms.</p>
<ol>

<li>Many services are stateless and multiple instances are run across the control plane in
          active-active mode. The API services (nova-api, cinder-api, etc.) are accessed through the
          HA proxy load balancer whereas the internal services (nova-scheduler, cinder-scheduler,
          etc.), are accessed through the message broker. These services use the database cluster to
          persist any data.<note>The HA proxy load balancer is also run in active-active mode and
            keepalived (used for Virtual IP (VIP) Management) is run in active-active mode, with
            only one keepalived instance holding the VIP at any one point in time.</note></li>
<li>HA of the message queue service and the database service is achieved by running these in a
          clustered mode across the three nodes of the overcloud control plane: RabbitMQ cluster
          with Mirrored Queues and Percona MySQL Galera cluster. <p/><image
            href="../../media/ha-architecture.png" id="image_y5b_4kv_ps"/></li>
</ol>
<p>The above diagram  illustrates the HA architecture with the focus on VIP management and load balancing. It only shows a subset of active-active API instances and does not show examples of other services such as nova-scheduler, cinder-scheduler, etc.</p>
<p>In the above diagram, requests from an OpenStack client to the overcloud API services are sent to VIP and port combination; for example, 192.0.2.26:8774 for a Nova request. The load balancer listens for requests on that VIP and port. When it receives a request, it selects one of the controller nodes configured for handling Nova requests, in this particular case, and then forwards the request to the IP of the selected controller node on the same port.</p>
<p>The nova-api service list, which is listening for requests on the IP of its host machine, then receives the request and deals with it accordingly. The database service is also accessed through the load balancer <!---(TODO: Section discussing database lockup issue with concurrent writes - this could require HA proxy always selecting a single node for access or just for writes, if possible)-->. RabbitMQ, on the other hand, is not currently accessed through VIP/HA proxy as the clients are configured with the set of nodes in the RabbitMQ cluster and failover between cluster nodes is automatically handled by the clients.</p>
<p>The sections below cover the following topics in detail:</p>
<ul>
<li>
<xref type="section" href="#topic21283/api-msg-flow">API Request Message Flow</xref>
</li>
<li>
<xref type="section" href="#topic21283/handling-node-failure">Handling Node Failure</xref>
</li>
<li>
<xref type="section" href="#topic21283/handling-network-partition">Handling Network Partitions</xref>
</li>
<li>
<xref type="section" href="#topic21283/mysql-galera">MySQL Galera Cluster</xref>
</li>
</ul>
</section>
<section id="api-msg-flow"><title>API Request Message Flow</title><p>The diagram below shows the
        flow for an API request in an overcloud HA deployment. All API requests (internal and
        external) are sent through the VIP.</p><image href="../../media/Ha-api-request-flow.png"
        id="image_f4k_skv_ps"/><p>The flow of a sample API request is explained below:</p><ol>
        <li>keepalived has currently configured the VIP on the Controller0 node; client sends Nova
          request to VIP:8774</li>
        <li>HA proxy (listening on VIP:8774) receives the request and selects Controller0 from the
          list of available nodes (Controller0, Controller1, Controller2). The request is forwarded
          to the Controller0IP:8774</li>
        <li>nova-api on Controller0 receives the request and determines that a database change is
          required. It connects to the database using VIP:</li>
        <li> HA proxy (listening on VIP:3306) receives the database connection request and selects
          Controller1 from the list of available nodes (Controller0, Controller1, Controller2). The
          connection request is forwarded to Controller1IP:3306</li>
      </ol></section>
<section id="handling-node-failure"> <title>Handling Node Failure</title>
<p>With the above overcloud HA set up, loss of a controller node is handled as follows:</p>
<p>Assume that the Controller0, which is currently in control of the VIP, is lost, as shown in the
        diagram below:<image href="../../media/ha-nodefailure-1.png" id="image_sgy_zzv_ps"/></p>
<p>When this occurs, keepalived immediately moves the VIP on to the Controller1 and can now receive API requests, which is load-balanced by HA proxy, as stated earlier.</p>
      <note>Although MySQL and RabbitMQ clusters have lost a node, they still continue to be
        operational: <image href="../../media/ha-nodefailure-2.png" id="image_i5y_j1w_ps"/></note>
<p>Finally, when Controller0 comes back online, keepalived and HA proxy will resume in standby/slave
        mode and be ready to take over, should there be a failure of Controller1. The Controller0
        rejoins the MySQL and RabbitMQ clusters as shown in the figure below: </p>
      <p/>
      <p><image href="../../media/ha-nodefailure-3.png" id="image_yhl_s1w_ps"/></p>
</section>
<section id="handling-network-partition"> <title>Handling Network Partitions</title>
<p>It is important for the overcloud HA setup to tolerate network failures, specifically those that result in a partition of the cluster, whereby one of the three nodes in the overcloud control plane cannot communicate with the remaining two nodes of the cluster. The description of network partition handling is separated into the main HA components of the overcloud.<!---**sentence seems to be incomplete. Please validate** ??--></p>
</section>
<section id="mysql-galera"><title>MySQL Galera Cluster</title><p>The handling of network partitions
        is illustrated in the diagram below. Galera has a quorum mechanism so when there is a
        partition in the cluster, the primary or quorate partition can continue to operate as
        normal, whereas the non-primary/minority partition cannot commit any requests. In the
        example below, Controller0 is partitioned from the rest of the control plane. As a result,
        requests can only be satisfied on Controller1 or Controller2. Controller0 will continue to
        attempt to rejoin the cluster:</p><image href="../../media/mysql-galera-cluster.png"
        id="image_ecc_y1w_ps"/><p>When HA proxy detects the errors against the mysql instance on
        Controller0, it removes that node from its pool for future database requests.</p></section>
<section id="singleton-services"> <title>Singleton Services</title>
<p>HPE Helion OpenStack defines three Controllers:</p>
<ol>
<li>OvercloudController0 </li>
<li>OvercloudController1</li>
<li>OvercloudController2</li>
</ol>
</section>
<section id="cinder-volume"> <title>Cinder-Volume</title>
<p>Due to the single threading required in both cinder-volume and the drivers, the Cinder volume
        service is run as a singleton in the control plane. <image
          href="../../media/ha-cinder-volume.png" id="image_tvz_bbw_ps"/></p>
<p>Cinder-volume is deployed on all three controller nodes, but kept active on only one node at a time. By default, cinder-volume is kept active on the overcloud controller. If the controller fails, you must enable and start the cinder-volume service on one of the other overcloud controller nodes, until it is restored. Once the  controller is restored, you must shut down the Cinder volume service from all other nodes and start it on the controller to ensure it runs as a singleton.</p>
<p>Since cinder.conf is kept synchronized across all the 3 nodes, Cinder volume can be run on any of the nodes at any given time. Ensure that it is run on only one node at a time.</p>
<p>
  <b>Steps to start Cinder-Volume</b>
</p>
<codeblock><codeph>os-svc-enable-upstart cinder-volume enable

service cinder-volume start
</codeph></codeblock>
<p>
  <b>Steps to stop Cinder-Volume</b>
</p>
<codeblock><codeph>service cinder-volume stop

os-svc-enable-upstart cinder-volume disable
</codeph></codeblock>
      <note>Following a system Update, the volume manager may attempt to start before the storage
        nodes have been properly initialized, and so the volume manager must be restarted after the
        automatic start procedure has completed.</note>
<p>To restart the Cinder-Volume service,
On the OverCloud Controller node where the Cinder-Volume singleton service is being run, the Cinder-Volume service can be restarted using</p>
<codeblock><codeph>service cinder-volume restart
</codeph></codeblock>
<p>For information on starting Cinder in version 1.1.1, please see the <xref href="../../commercial/GA1/1.1.1commerical.high-availability.dita" >High Availibility page for 1.1.1</xref>.</p>
</section>
<section id="sherpa"> <title>Sherpa</title>
<p>You must take periodic backups of the Sherpa service since it does maintain some state information on local disk storage on the controller. 
If the controller fails, Sherpa becomes unavailable until you rebuild or restore the controller. After restoring the controller, you should restore the Sherpa state from its latest backup.</p>
</section>
<section id="nova-consoleauth"> <title>Nova consoleauth</title>
<p>If the controller fails, the Nova consoleauth service will become unavailable and users will not be able to connect to their VM consoles via VNC. The service will be restored once you restore the  controller.</p>
</section>
<section id="replace-rebuild"> <title>Rebuilding or Replacing failed Controller Nodes</title>
<p>As described above, the three node controller cluster provides a robust, highly available control plane of OpenStack services. 
Either OvercloudController1 or OvercloudController2 servers can be shut down for a short duration for maintenance activities without impacting cloud service availability. (OvercloudController0 cannot be shut down without affecting cloud service availability.)</p>
      <note>The HA design is only robust against single points of failure and may not protect you
        against multiple levels of failure. As soon as first-level failure occurs, you must try to
        fix the symptom/root cause and recover from the failure, as soon as possible.</note>
<p>In the unlikely event that one of the controller servers suffers an irreparable hardware failure, you can decommission and delete it from the cluster. You can then deploy the failed controller on a new server and connect it back into the original three node controller cluster. Learn more about <xref href="../../commercial/GA1/1.1commerical.services-remove-replace-failed-overcloud-nodes.dita" >Replacing/Rebuilding Controller Nodes</xref>.</p>
</section>
<section id="ha-cloud-infra"> <title>Highly Available Cloud Infrastructure</title>
<p>The highly available cloud infrastructure consists of the following:</p>
<ul>
<li>
<xref type="section" href="#topic21283/availability-zones">Availability Zones</xref>
</li>
<li>
<xref type="section" href="#topic21283/compute-with-kvm">Compute with KVM</xref>
</li>
        <li><xref href="#topic21283/nova-availability-zone" format="dita"/></li>
<li>
<xref type="section" href="#topic21283/compute-with-esx">Compute with ESX</xref>
</li>
<li>
<xref type="section" href="#topic21283/block-storage">Block Storage with StoreVirtual VSA</xref>
</li>
<li>
<xref type="section" href="#topic21283/object-storage">Object Storage with Swift</xref>
</li>
</ul>
</section>
<section id="availability-zones"><title>Availability Zones</title><image
        href="../../media/ha-availability-zone.png" id="image_oks_bcw_ps"/><p>While planning your
        OpenStack deployment, you should decide on how to zone various types of nodes - such as
        compute, block storage, and object storage. For example, you may decide to place all servers
        in the same rack in the same zone. For larger deployments, you may plan more elaborate
        redundancy schemes for redundant power, network ISP connection, and even physical
        firewalling between zones (<i>this aspect is outside the scope of this
        document</i>).</p><p>HPE Helion OpenStack offers APIs, CLIs and Horizon UIs for the
        administrator to define and user to consume, availability zones for each of Nova, Cinder and
        Swift services. This section outlines the process to deploy specific types of nodes to
        specific physical servers, and makes a statement of available support for these types of
        availability zones in the current release.</p><note>By default, HPE Helion OpenStack is
        deployed in a single availability zone upon installation. Multiple availability zones can be
        configured by an administrator post-install, if required. Refer to the <xref
          href="http://docs.openstack.org/openstack-ops/content/scaling.html" format="html"
          scope="external">Chapter 5: Scaling</xref>( in the <i>OpenStack Operations Guide</i> for
        more information.</note></section>
<section id="compute-with-kvm"> <title>Compute with KVM</title>
<p>You can deploy your KVM Nova-Compute nodes either during initial installation, or by adding compute nodes post initial installation.</p>
<p>While adding compute nodes post initial installation, you can specify the target physical servers for deploying the compute nodes.</p>
<p>Learn more about <xref href="../../commercial/GA1/1.1commercial.install-add-nodes.dita" >Adding Compute Nodes after Initial Installation</xref>.</p>
</section>
<section id="nova-availability-zone"> <title>Nova Availability Zones</title>
<p>Nova host aggregates and availability zones are not supported for general consumption in the current release.</p>
<p>However, Nova availability zones will be supported for the narrow use of the HPE Helion OpenStack Platform Services deployed on HPE Helion OpenStack Please refer to the Platform Service documentation for further information.</p>
</section>
<section id="compute-with-esx"> <title>Compute with ESX Hypervisor</title>
<p>Compute nodes deployed on ESX Hypervisor can be made highly available using the HA feature of VMware ESX Clusters. For more information on VMware HA, please refer to your VMware ESX documentation.</p>
</section>
<section id="block-storage"> <title>Block Storage with StoreVirtual VSA</title>
<p>Highly available Cinder block storage volumes are provided by the network RAID 10 implementation in the HPE StoreVirtual VSA software. You can deploy the VSA nodes in three node cluster and specify Network RAID 10 protection for Cinder volumes.</p>
<p>The underlying SAN/iQ operating system of the StoreVirtual VSA ensures that the two-way replication maintains two mirrored copies of data for each volume.</p>
<p>This Network RAID 10 capability ensures that failure of any single server does not cause data loss, and maintains data access to the clients.</p>
<p>Furthermore, each of the VSA nodes of the cluster can be strategically deployed in different
        zones of your data center for maximum redundancy and resiliency. For more information on how
        to deploy VSA nodes on desired target servers, refer to the<xref
          href="../../commercial/GA1/1.1commercial.install-GA-vsa.dita"> Deploy VSA</xref>
        document.</p>
      <p>
        <image href="../../media/ha-block-storage.png" id="image_vy2_3cw_ps"/></p>
</section>
<section id="block-storeserv"/>
<section id="cinder-avail"> <title>Cinder Availability Zones</title>
<p>Cinder availability zones are not supported for general consumption in the current release.</p>
</section>
<section id="object-storage"> <title>Object Storage with Swift</title>

<p>You can configure your Swift Rings by specifying a target zone for each drive that is added to
        the ring. Learn more about configuring zones for various drives using the Ringos tool here.
        Please refer to the <xref
          href="../../commercial/GA1/1.1commerical.services-object-pyringos.dita">Ringos</xref>
        document. <image href="../../media/ha-swift.png" id="image_dkk_mcw_ps"/></p>
<!--
### Neutron Networking {#networking}
 
Neutron Netwoking in HA covers the following:

1. Neutron Ser
2. ver (API) and its underlying MySQL and RabbitMQ are HA using HAproxy, Galera and Mirrored queue technologies for clustering.

2. With the advent of DVR, the Neutron L3 agent no longer lives on central controller nodes, but is distributed and runs on each Compute node, such that, if it fails, its impact is only limited to VMs within that compute node. (do you have some pointer we could refer the user for further info on DVR concepts)

3. DHCP agent is still running on central Cloud controllers - should a cloud controller running DHCP services fail, you must manually migrate over all the DHCP services to any of the remaining cloud controller nodes using the procedures outlined and validated (still work in progress) in https://jira.hpcloud.net/browse/CORE-1777. Until the DHCP services are migrated, no new VMs will be able to launch and existing VMs will not be able to renew their DHCP leases upon expiration.

4. SNAT services still run on one of the central Cloud controller - should a cloud controller running SNAT services fail, your VMs will lose SNAT ability and will not be able to reach services on external networks. You can work around this by attaching a floating IP to your VM, which will allow it to reach to external networks without going through the central SNAT. 
 -->
</section>
<section id="ha-workloads"> <title>Highly Available Cloud Applications and Workloads</title>
<p>Projects writing applications to be deployed in the cloud must be aware of the cloud architecture and potential points of failure and architect their applications accordingly for high availability.</p>
<p>Some guidelines for consideration:</p>
<ol>
<li>
<p>Assume intermittent failures and plan for retries</p>

<ul>
<li>
<p>
<b>OpenStack Service APIs</b>: invocations can fail - you should carefully evaluate the response of each invocation, and retry in case of failures.</p>
</li>
<li>
<p>
<b>Compute</b>: VMs can die - monitor and restart them</p>
</li>
<li>
<p>
<b>Network</b>: Network calls can fail - retry should be successful</p>
</li>
<li>
<p>
<b>Storage</b>: Storage connection can hiccup - retry should be successful</p>
</li>
</ul>
</li>
<li>
<p>Build redundancy into your application tiers</p>

<ul>
<li>
<p>Servers running your VM instances can die.</p>

<ul>
<li>
<p>Replicate VMs containing stateless services such as Web application tier or Web service API tier and put them behind load balancers (you must implement your own HA Proxy type load balancer in your application VMs until HPE Helion OpenStack delivers the LBaaS service).</p>
</li>
<li>
<p>Boot the replicated VMs into different Nova availability zones.</p>
</li>
<li>
<p>If your VM stores state information on its local disk (Ephemeral Storage), and you cannot afford to lose it, then boot the VM off a Cinder volume.</p>
</li>
<li>
<p>Take periodic snapshots of the VM which will back it up to Swift through Glance.</p>
</li>
<li>
<p>Your data on ephemeral may get corrupted (but not your backup data in Swift and not your data on Cinder volumes).</p>
</li>
<li>
<p>Take regular snapshots of Cinder volumes and also back up Cinder volumes or your data exports into Swift.</p>
</li>
</ul>
</li>
</ul>
</li>
<li>
<p>Instead of rolling your own highly available stateful services, use readily available HPE Helion OpenStack platform services such as:</p>

<ul>
<li>Database as a Service (DBaaS)</li>
<li>DNS as a Service(DNSaaS)</li>
<li>Messaging as a Service (MSGaaS)</li>
</ul>
</li>
</ol>
</section>
<section id="not-ha"> <title>What is not Highly Available?</title>
<p>The following section defines the different services in Seed and Undercloud that are not HA supported:</p>
</section>
<section id="seed"> <title>Seed</title>
<p>All services running on the Seed VM are not highly available and if the Seed VM fails, it has an impact on the Undercloud functionality.</p>
<p>Ensure the following points:</p>
<ul>
<li>
<p>Ensure a periodic backup of Seed VM. Please refer to the <xref href="../../commercial/GA1/1.1commercial.backup-restore-GA.dita" >Backup and Restore</xref> document for more details.</p>
</li>
<li>
<p>In case of Seed VM failure, the Undercloud continues to function. However, if any of the Undercloud nodes crash or is shutdown, it does not boot up as it is dependent on Seed to provide the PXE image.</p>
</li>
<li>
<p>In the above case, it is recommended to recover Seed VM by following the documented restore process and bring it up in order for it to serve Undercloud node PXE requests during the restart process.</p>
</li>
</ul>
</section>
<section id="undercloud"> <title>Undercloud</title>
<p>All the services running on Undercloud node are not highly available and if the Undercloud node fails, it impacts the Overcloud functionality.</p>
<p>Ensure the following points:</p>
<ul>
<li>
<p>Ensure the periodic backup of Undercloud node. Please refer to the <xref href="../../commercial/GA1/1.1commercial.backup-restore-GA.dita" >Backup and Restore</xref> document for more details.</p>
</li>
<li>
<p>In case of Undercloud node failure, the Overcloud continues to function. However, if any of the Overcloud nodes crash or is shutdown, it does not boot up as it is dependent on the Undercloud to provide the PXE image.</p>
</li>
<li>
<p>In the above case, it is recommended to recover the Undercloud node by following the documented restore process and bring it up for it to serve Overcloud node PXE requests during the restart process.</p>
</li>
</ul>
</section>
<section id="other"> <title>Other</title>
<ul>
<li>High availability is not supported for Centralized Virtual Router.</li>
</ul>
</section>
<section id="more-info"> <title>More information</title>
<ul>
<li>
<xref href="http://docs.openstack.org/high-availability-guide/content/ch-intro.html" scope="external" format="html" >OpenStack High-availability Guide</xref>
</li>
</ul>
<!-- http://www.ibm.com/developerworks/cloud/library/cl-cloudappdevelop/ **pretty sure we can't link HP's website to IBM's** -->
<ul>
<li>
<xref href="http://12factor.net/" scope="external" format="html" >12-Factor Apps</xref>
</li>
</ul>
<p>
  <xref href="#topic21283"> Return to Top </xref>
</p>
<!-- ===================== horizontal rule ===================== -->
</section>
</body>
</topic>
