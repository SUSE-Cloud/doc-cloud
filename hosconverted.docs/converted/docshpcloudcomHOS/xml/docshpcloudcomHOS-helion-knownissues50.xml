<?xml version="1.0"?>
<!DOCTYPE section PUBLIC "-//OASIS//DTD DocBook XML V4.5//EN" "http://www.oasis-open.org/docbook/xml/4.5/docbookx.dtd" [ <!ENTITY % entities SYSTEM "entities.xml"> %entities; ]><section id="knownissues50">
   <title>Known Issues in this Release</title>
    
    
      <bridgehead renderas="sect4">Upgrading from <phrase/>
      </bridgehead>
   
    
    
      <bridgehead renderas="sect4">OpenStack Security Advisory OSSA-2017-001</bridgehead>
      <para>The OpenStack Security Advisory OSSA-2017-001 which describes CVE-2017-2592 was released on
        January 26, 2017. It discloses a security defect which causes Keystone tokens to be logged
        when a Neutron API call encounters unexpected error conditions. When this happens, the log
        entry that contains the token is propagated to the centralized logging facility. The result
        is that anyone with access to the centralized logging facility or the local log file
        containing the token could use the token to impersonate the owner of the token until the
        token expires. Tokens expire in 4 hours by default. Neutron is the only <phrase/> service affected by this issue. The error conditions which cause this
        token to leak into the logs are not expected to happen frequently. However, customers should
        be aware of this risk. Customers may wish to further limit the access to the centralized
        logging facility and watch for suspicious activity that could indicate usage of a stolen
        Keystone token to impersonate another user. The Keystone token timeout value could also be
        changed but it is not recommended; testing would be required to ensure things still work in
        the customer environment if the token timeout value is reduced.
      </para>

   
    
    
      <bridgehead renderas="sect4">
         <phrase/> Directory Umask</bridgehead>
      <para><phrase/> directory umask is 0022. Changing this value may cause unstable
        internal communication in the <phrase/> product and will negatively affect upgrade.</para>

   
    
    
    
      <bridgehead renderas="sect4">Adding Top-Level Domains with Designate</bridgehead>
      <para>In <phrase/> and earlier ( with designate V2.0.1 ) when using
        Designate to add Top-Level Domains (that controls who can create zones), there is a caching
        defect. When an entry is made to the tld list (<literal>openstack tld create --name abc.net</literal>), in
        order to take effect the designate-central on all the controllers needs to be restarted. 
        </para>
<screen>$ sudo systemctl restart designate-central</screen>

   
    
    
      <bridgehead renderas="sect4">Adding New Flavors After Upgrade</bridgehead>
      <para>
        After upgrading to <phrase/> from <phrase/>
        you may experience an error when attempting to add new flavors. </para>

      <para> Example:
        </para>
<screen>$ openstack flavor create --ram 512 --vcpus 1 --disk 0 --ephemeral 0 --swap 0 --public m1.tinystu
        
Not all flavors have been migrated to the API database (HTTP 409) (Request-ID: req-bdae0a10-e6c3-42d4-9a58-b4438fcc4309)</screen>

      <para>This is caused by the time fields associated with the flavor not being updated correctly.
        You can view the time fields by selecting all from the instance_types table in the nova
        mysql database.
        </para>
<screen>$ sudo mysql
$ use nova
$ select * from instance_types;</screen>

      <para>The time fields <emphasis role="bold">created_at</emphasis>, <emphasis role="bold">updated_at</emphasis> or <emphasis role="bold">deleted_at</emphasis> may have
         invalid dates or not all of the fields are NULL.
       </para>

      <para>To resolve the new flavors issues the time fields must all be set to NULL.
        </para>
<screen>$ sudo mysql
$ use nova
$ update instance_types set created_at=NULL, updated_at=NULL, deleted_at=NULL;</screen>

      <para>Once the tables are updated, exit mysql and run the following command.
        </para>
<screen>$ sudo /opt/stack/service/nova-api/venv/bin/nova-manage --config-dir /opt/stack/service/nova-api/etc/nova db online_data_migrations</screen>

      <para>You can now create a new flavors in <phrase/>. </para>

   
    
    
      <bridgehead renderas="sect4">Using VSA as a Cinder Backend</bridgehead>
      <para>When using VSA as a Cinder backend it is necessary to make some changes to the kvm-hypervisor.conf.j2 file with the following steps:
      </para>
<orderedlist>
            <listitem>
               <para>Log into the lifecycle manager</para>
            </listitem>
            <listitem>
               <para>Edit the <literal>~/helion/my_cloud/config/nova/kvm-hypervisor.conf.j2</literal> file.</para>
            </listitem>
            <listitem>
               <para>Remove the following lines under the libvert section: 
          </para>
               <screen>volume_use_multipath = True</screen>
            </listitem>
            <listitem>
               <para>Commit the configuration to the local git repo
          </para>
               <screen>cd ~/helion/hos/ansible
git add -A
git commit -m "My config or other commit message"</screen>
            </listitem>
            <listitem>
               <para>Run the configuration processor
          </para>
               <screen>cd ~/helion/hos/ansible
ansible-playbook -i hosts/localhost config-processor-run.yml</screen>
            </listitem>
            <listitem>
               <para>Use the playbook below to create a deployment directory
          </para>
               <screen>cd ~/helion/hos/ansible
ansible-playbook i hosts/localhost ready-deployment.yml</screen>
            </listitem>
            <listitem>
               <para>Run the Nova reconfigure playbook
            </para>
               <screen>cd ~/scratch/ansible/next/hos/ansible
ansible-playbook -i hosts/verb_hosts nova-reconfigure.yml</screen>
            </listitem>
        </orderedlist>
<para>
      This step is not needed if Cinder backend is HP 3PAR.</para>

      <para>When using storage solutions other than 3PAR or VSA, please check with the storage provider
        whether the <emphasis role="bold">volume_use_multipath = True</emphasis> line should be removed.</para>

   
    
    
      <bridgehead renderas="sect4">Vertica Log Rotation</bridgehead>
      <para>Vertica database logs are not rotated automatically for non-primary cluster nodes. As a
        result, Vertica logs on those nodes may grow continuously, eventually filling the filesystem
        to its capacity. This issue can be patched by logging into each node with Vertica installed
        and run the following command. </para>
<note>
            <para>Vertica nodes are all listed as members of the FND-VDB
          group in the deployer's
            <literal>~/scratch/ansible/next/hos/ansible/hosts/verb_hosts</literal> file. </para>
         </note>
<screen>echo -e '#!/bin/sh\nlogrotate /opt/vertica/config/logrotate_base.conf' | sudo tee /etc/cron.daily/vertica-logrotate; sudo chmod 755 /etc/cron.daily/vertica-logrotate</screen>
<para>
        While this step is not necessary on the Vertica cluster's primary node, it is also not
        harmful. This command is idempotent and future-proof with regard to upgrading to later
          <phrase/> versions. </para>

   
    
    
      <bridgehead renderas="sect4">VPNaaS does not support SHA-384 and SHA-512</bridgehead>
      <para>The VPNaaS plugin limits the ipsec and ike auth algorithm to <emphasis role="bold">SHA-1</emphasis> and
          <emphasis role="bold">SHA-256</emphasis>. If you attempt to  add a new driver (for example,  a hardware VPN
        gateway), and the new driver supports additional auth algorithm, such as <emphasis role="bold">SHA2-384</emphasis>,
          <emphasis role="bold">SHA2-512</emphasis>, it cannot be integrated with the current VPNaaS plugin.</para>

   
    
    
      <bridgehead renderas="sect4">SLES - No kdump initial ramdisk found</bridgehead>
      <para>If you are deploying SLES 12 nodes and you see the following exception during the initial
        boot of those nodes: </para>
<screen>No kdump initial ramdisk found....</screen>
<para> No
        further action is required. Please see: <ulink url="https://www.novell.com/support/kb/doc.php?id=7018428"/> for additional information:
        </para>
<note>
            <para>This exception may also occur by running <literal>journalctl -u kdump.service</literal></para>
         </note>

   
    
    
      <bridgehead renderas="sect4">Manual Cleaning</bridgehead>
      <para>Manual Cleaning (Raid Configuration) is not supported if the target disk type is SATA SSD.</para>

   
    
    
      <bridgehead renderas="sect4">Transparent VLAN</bridgehead>
      <para>In <phrase/> Transparent VLAN is not supported.</para>

   
    
    
      <bridgehead renderas="sect4">Disaster Recovery Panel</bridgehead>
      <para>Although the Disaster Recovery panel can be viewed when logged in as an <emphasis role="bold">admin</emphasis> user,
        only the <emphasis role="bold">hlm_backup</emphasis> user is the supported user of this feature. The <emphasis role="bold">admin</emphasis> user
        is unable to view the default backup jobs or create new jobs. Therefore, all backup and recovery actions should
        be performed only when logged in as the <emphasis role="bold">hlm_backup</emphasis> user.</para>

   
    
    
      <bridgehead renderas="sect4">Update ironic-conductor.conf.j2 file to be able to connect to Swift</bridgehead>
      <para>Ironic conductor uses credentials from the <literal>[swift]</literal> section of the config
        file, and falls back to <literal>[keystone_authtoken]</literal> section if no credentials are
        specified in the <literal>[swift]</literal> section.</para>

      <para>As a work-around, add the following to the <literal>/ironic-conductor.conf.j2</literal> file.
        </para>
<screen>auth_type = password
auth_url = {{ ironic_cnd_admin.identity_uri }}/v3
username = {{ ironic_swift.glance_user }}
password = {{ ironic_swift.glance_password}}
project_name = {{ ironic_swift.glance_tenant }}
default_domain_name = default</screen>

   
    
    
      <bridgehead renderas="sect4">Attaching a volume with Device Name in Horizon</bridgehead>
      <para>In Horizon the screen for attaching a volume to an instance requires a <emphasis role="bold">Device Name</emphasis>.
        This field is editable but cannot be empty or it will revert to the default value.</para>

      <para>
        To attach a volume to an instance without specifying a Device Name, use the command line (CLI). </para>

   
    
    
      <bridgehead renderas="sect4">Ironic: ILO Firmware</bridgehead>
      <para>In <phrase/> the ILO Firmware version needs to be minimum 2.30 for
        agent_ilo driver.</para>

   
    
    
      <bridgehead renderas="sect4">Ironic 'shreading' (cleaning) may take a long time depending on the disk size/performance.</bridgehead>
      <para>During deployment, with <literal>automated_cleaning</literal> set to True and when the Nova instance is
        deleted, it may take an unusually long time to finish or may even seem as though it's hung
        depending on the disk performance/size.</para>

      <para>Example Log msg in the ironic-conductor.log:</para>

      <screen>2017-05-25 18:08:37.648 40436 DEBUG ironic.drivers.modules.agent_base_vendor
[req-89ff4f08-c13f-49f9-b761-f0e0a5611cd1 - - - - -] Cleaning command status for node
84f57c65-caf5-4a4a-941f-82d27afc93e6 on step {u'priority': 10, u'interface': u'deploy',
u'reboot_requested': False, u'abortable': True, u'step': u'erase_devices'}: None
continue_cleaning /opt/stack/venv/ironic-20170522T163729Z/lib/python2.7/site-packages/ironic/drivers/modules/agent_base_vendor.py:347</screen>
      <para>For more information on OpenStack Ironic Node cleaning, see: <ulink url="https://docs.openstack.org/developer/ironic/deploy/cleaning.html">https://docs.openstack.org/developer/ironic/deploy/cleaning.html</ulink></para>

   
    
    
      <bridgehead renderas="sect4">Ironic Multi-Tenancy Conversion</bridgehead>
      <para>Ironic Multi-tenancy is only supported as a fresh install of <phrase/>. An existing Ironic flat environment to multi-tenancy environment
        conversion is not supported. </para>

   
    
    
      <bridgehead renderas="sect4">Do not enable HEAT Auditing</bridgehead>
      <para>Audit log is disabled by default and cannot be enabled due to a Keystone middleware defect in Newton.</para>

   
    
    
      <bridgehead renderas="sect4">Ops Console Notification Panel</bridgehead>
      <para>If there are no Alarm notification methods configured, the Ops Console notifications tab
        will not allow creation of new notification methods. It is recommended to not delete the
        default notification method if no other methods have been created.</para>

      <para>You can work around this issue by creating a new notification method using the <ulink url="https://docs.openstack.org/cli-reference/monasca.html">Monasca command-line</ulink>, after which the UI will function again.
      </para>

   
    
    
      <bridgehead renderas="sect4">LBaaS</bridgehead>
      <para>LBaaS Hot-Plugin is not supported with DPDK.</para>

      <para>LBaaS <literal>load_balancer_expiry_age</literal> value in the housekeeping section of the
        configuration for octavia-housekeeping service needs to be configured appropriately to the
        default <literal>value=604800</literal>.</para>

   
    
    
      <bridgehead renderas="sect4">F5 Loadbalancer Support</bridgehead>
      <para>If you are planning to upgrade from <phrase/> or <phrase/>,
        please contact F5 and HPE PointNext to determine which F5 drivers have been certified for
        use with Helion OpenStack. Loading drivers not certified by HPE may result in catastrophic
        failure of your cloud deployment. The last tested versions are 8.0.8 for <phrase/> 3.x and 9.0.3
        for <phrase/> 4.x . More information is expected in 4th quarter 2017, including the correct
        drivers for <phrase/> 5.x.</para>

   
    
    
      <bridgehead renderas="sect4">VMs fail to get an IP after upgrade</bridgehead>
      <para>When a VMs failed to get an IP after an upgrade, apply the following work-around:</para>

      <para>After running the rabbitmq-disaster-recovery on a controller node, check the
        ownership of <literal>/var/run/neutron/lock/neutron-iptables</literal>. If it is not owned by
          <emphasis role="bold">neutron:neutron</emphasis>, use the chown command to change the ownership to
          <emphasis role="bold">neutron:neutron</emphasis>: 
          </para>
<screen>sudo chown neutron:neutron /var/run/neutron/lock/neutron-iptables </screen>

   
    
    
      <bridgehead renderas="sect4">Persistent failures during the early phase of boot</bridgehead>
      <para>If you are experiencing intermittent or persistent failures during the early phase of boot
        process for certain classes of machines (e.g. DL360 Gen8 and DL160 Gen9), the likely issue
        is DHCP timeouts due to slow convergence of STP. 
      </para>

      <para>You can work around this issue by enabling rapid spanning tree
        protocol on the switch. If this is not an option due to conservative loop detection
        strategies, rebuild ipxe images with higher DHCP timeouts, or alternatively maintain link
        status to the server NICs during power cycles. </para>

   
    
    
      <bridgehead renderas="sect4">Swarm cluster behind proxy fails</bridgehead>
      <para>Swarm cluster is not support behind a proxy even with -http(s)-proxy parameters
        specified.</para>

   
    
    
      <bridgehead renderas="sect4">Neutron Quota Failures</bridgehead>
      <para>The options <literal>--security_group</literal> and <literal>--security_group_rule</literal>
        are not valid command options for the <literal>neutron quota-update</literal> command. Please
        use following options: </para>
<itemizedlist>
            <listitem>
               <para>-security-group</para>
            </listitem>
            <listitem>
               <para>security-group-rule</para>
            </listitem>
        </itemizedlist>

   
    
    
      <bridgehead renderas="sect4">Starting/Stopping Swift</bridgehead>
      <para>The command line (CLI) cmd, <literal>swift-init</literal> is not supported. To start/stop
        Swift use the Ansible playbooks, <literal>swift-start.yml</literal> and
          <literal>swift-stop.yml</literal>.</para>

   
    
    
      <bridgehead renderas="sect4">Live Migrations</bridgehead>
      <para>Live Migration from  non-HLinux nodes including SLES nodes to HLinux nodes are not supported.</para>

   
    
  </section>
