<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE topic PUBLIC "-//OASIS//DTD DITA Topic//EN" "topic.dtd">
<topic id="storage_alarmdefinitions">
  <title><ph conkeyref="HOS-conrefs/product-title"/>Storage Alarms</title>
  <body>
    <section>
      <p>These alarms show under the Storage section of the HPE Helion Ops Console.</p>
      <p>
        <table frame="all" rowsep="1" colsep="1" id="storage_alarms">
          <tgroup cols="5">
            <colspec colname="c1" colnum="1"/>
            <colspec colname="c2" colnum="2"/>
            <colspec colname="c3" colnum="3"/>
            <colspec colname="c4" colnum="4"/>
            <colspec colname="c5" colnum="5"/>
            <thead>
              <row>
                <entry>Service</entry>
                <entry>Alarm Name</entry>
                <entry>Description</entry>
                <entry>Likely Cause</entry>
                <entry>Mitigation Tasks to Perform</entry>
              </row>
            </thead>
            <tbody>
              <row>
                <entry morerows="25">object-storage</entry>
                <entry>swiftlm-scan monitor</entry>
                <entry>Alarms if <codeph>swiftlm-scan</codeph> cannot execute a monitoring
                  task.</entry>
                <entry>The <codeph>swiftlm-scan</codeph> program is used to monitor and measure a
                  number of metrics. If it is unable to monitor or measure something, it raises this
                  alarm.</entry>
                <entry>Click on the alarm to examine the <codeph>Details</codeph> field and look for
                  a <codeph>msg</codeph> field. The text may explain the error problem. To
                  view/confirm this, you can also log into the host specified by the
                    <codeph>hostname</codeph> dimension, and then run this command: <codeblock>sudo swiftlm-scan | python -mjson.tool</codeblock>
                  <p>The <codeph>msg</codeph> field is contained in the <codeph>value_meta</codeph>
                    item.</p></entry>
              </row>
              <row>
                <entry>Swift account replicator last completed in 12 hours</entry>
                <entry>Alarms if an <codeph>account-replicator</codeph> process did not complete a
                  replication cycle within the last 12 hours.</entry>
                <entry>This can indicate that the <codeph>account-replication</codeph> process is
                  stuck.</entry>
                <entry>
                  <p>SSH to the affected host and restart the process with this
                    command:<codeblock>sudo systemctl restart swift-account-replicator</codeblock></p>
                  <p>Another cause of this problem may be that a file system may be corrupt. Look
                    for sign of this in these logs on the affected node:</p>
                  <codeblock>/var/log/swift/swift.log
/var/log/kern.log</codeblock>
                  <p>The file system may need to be wiped, contact HPE Support for advice on the
                    best way to do that if needed. You can then reformat the file system with these
                    steps:</p>
                  <ol id="ol_jbr_2zp_mx">
                    <li>Log in to the lifecycle manager.</li>
                    <li>Run the Swift deploy playbook against the affected node, which will format
                      the wiped file system:
                      <codeblock>cd ~/scratch/ansible/next/hos/ansible/
ansible-playbook -i hosts/verb_hosts swift-deploy.yml --limit &#60;hostname></codeblock></li>
                  </ol>
                </entry>
              </row>
              <row>
                <entry>Swift container replicator last completed in 12 hours</entry>
                <entry>Alarms if a container-replicator process did not complete a replication cycle
                  within the last 12 hours</entry>
                <entry>This can indicate that the container-replication process is stuck.</entry>
                <entry>
                  <p>SSH to the affected host and restart the process with this
                    command:<codeblock>sudo systemctl restart swift-container-replicator</codeblock></p>
                  <p>Another cause of this problem may be that a file system may be corrupt. Look
                    for sign of this in these logs on the affected node:</p>
                  <codeblock>/var/log/swift/swift.log
/var/log/kern.log</codeblock>
                  <p>The file system may need to be wiped, contact HPE Support for advice on the
                    best way to do that if needed. You can then reformat the file system with these
                    steps:</p>
                  <ol id="ol_kbr_2zp_mx">
                    <li>Log in to the lifecycle manager.</li>
                    <li>Run the Swift deploy playbook against the affected node, which will format
                      the wiped file system:
                      <codeblock>cd ~/scratch/ansible/next/hos/ansible/
ansible-playbook -i hosts/verb_hosts swift-deploy.yml --limit &#60;hostname></codeblock></li>
                  </ol>
                </entry>
              </row>
              <row>
                <entry>Swift object replicator last completed in 24 hours</entry>
                <entry>Alarms if an object-replicator process did not complete a replication cycle
                  within the last 24 hours</entry>
                <entry>This can indicate that the object-replication process is stuck.</entry>
                <entry>
                  <p>SSH to the affected host and restart the process with this
                    command:<codeblock>sudo systemctl restart swift-account-replicator</codeblock></p>
                  <p>Another cause of this problem may be that a file system may be corrupt. Look
                    for sign of this in these logs on the affected node:</p>
                  <codeblock>/var/log/swift/swift.log
/var/log/kern.log</codeblock>
                  <p>The file system may need to be wiped, contact HPE Support for advice on the
                    best way to do that if needed. You can then reformat the file system with these
                    steps:</p>
                  <ol id="ol_lbr_2zp_mx">
                    <li>Log in to the lifecycle manager.</li>
                    <li>Run the Swift deploy playbook against the affected node, which will format
                      the wiped file system:
                      <codeblock>cd ~/scratch/ansible/next/hos/ansible/
ansible-playbook -i hosts/verb_hosts swift-deploy.yml --limit &#60;hostname></codeblock></li>
                  </ol>
                </entry>
              </row>
              <row>
                <entry>Swift configuration file ownership</entry>
                <entry>Alarms if files/directories in <codeph>/etc/swift</codeph> are not owned by
                  Swift.</entry>
                <entry>For files in <codeph>/etc/swift</codeph>, somebody may have manually edited
                  or created a file.</entry>
                <entry>For files in <codeph>/etc/swift</codeph>, use this command to change the file
                  ownership:<codeblock>sudo chown swift.swift /etc/swift/, /etc/swift/*</codeblock></entry>
              </row>
              <row>
                <entry>Swift data filesystem ownership</entry>
                <entry>Alarms if files/directories in <codeph>/srv/node</codeph> are not owned by
                  Swift.</entry>
                <entry>For directories in <codeph>/srv/node/*</codeph>, it may happen that the root
                  partition was reimaged or reinstalled and the UID assigned to the Swift user
                  changes. The directories and files are then not owned by the UID assigned to the
                  Swift user.</entry>
                <entry>For directories and files in <codeph>/srv/node/*</codeph>, compare the swift
                  UID of this system and other systems and the UID of the owner of
                    <codeph>/srv/node/*</codeph>. If possible, make the UID of the Swift user match
                  the directories/files. Otherwise, change the ownership of all files and
                  directories under the <codeph>/srv/node</codeph> path using a similar command as
                  above.</entry>
              </row>
              <row>
                <entry>Drive URE errors detected</entry>
                <entry>Alarms if <codeph>swift-drive-audit</codeph> reports an unrecoverable read
                  error on a drive used by the Swift service.</entry>
                <entry>An unrecoverable read error has occurred when Swift attempted to access a
                  directory.</entry>
                <entry>
                  <p>The UREs reported only apply to file system metadata (i.e., directory
                    structures). For UREs in object files, the Swift system automatically deletes
                    the file and replicates a fresh copy from one of the other replicas.</p>
                  <p>UREs are a normal feature of large disk drives. It does not mean that the drive
                    has failed. However, if you get regular UREs on a specific drive, then this may
                    indicate that the drive has indeed failed and should be replaced.</p>
                  <p>You can use standard XFS repair actions to correct the UREs in the file
                    system.</p>
                  <p>If the XFS repair fails, you should wipe the GPT table as follows (where
                    &lt;drive_name> is replaced by the actual drive name):</p>
                  <codeblock>sudo dd if=/dev/zero of=/dev/sd&lt;drive_name&gt; bs=$((1024*1024)) count=1</codeblock>
                  <p>Then, follow the steps below which will reformat the drive, remount it, and
                    restart Swift services on the affected node.</p>
                  <ol id="ol_mbr_2zp_mx">
                    <li>Log in to the lifecycle manager.</li>
                    <li>Run the Swift reconfigure playbook, specifying the affected node:
                      <codeblock>cd ~/scratch/ansible/next/hos/ansible/
ansible-playbook -i hosts/verb_hosts _swift-configure.yml --limit &#60;hostname></codeblock></li>
                  </ol>
                  <p>It is safe to reformat drives containing Swift data because Swift maintains
                    other copies of the data (usually, Swift is configured to have three replicas of
                    all data).</p>
                </entry>
              </row>
              <row>
                <entry>Swift service</entry>
                <entry>Alarms if a Swift process, specified by the <codeph>component</codeph> field,
                  is not running. </entry>
                <entry>A daemon specified by the <codeph>component</codeph> dimension on the host
                  specified by the <codeph>hostname</codeph> dimension has stopped running.</entry>
                <entry>
                  <p>Examine the <codeph>/var/log/swift/swift.log</codeph> file for possible error
                    messages related the Swift process. The process in question is listed in the
                    alarm dimensions in the <codeph>component</codeph> dimension.</p>
                  <p>Restart Swift processes by running the <codeph>swift-start.yml</codeph>
                    playbook, with these steps:</p>
                  <ol id="ol_nbr_2zp_mx">
                    <li>Log in to the lifecycle manager.</li>
                    <li>Run the Swift start playbook against the affected host:
                      <codeblock>cd ~/scratch/ansible/next/hos/ansible/
ansible-playbook -i hosts/verb_hosts swift-start.yml --limit &#60;hostname></codeblock></li>
                  </ol>
                </entry>
              </row>
              <row>
                <entry>Swift filesystem mount point status</entry>
                <entry>Alarms if a file system/drive used by Swift is not correctly mounted.</entry>
                <entry>
                  <p>The device specified by the <codeph>device</codeph> dimension is not correctly
                    mounted at the mountpoint specified by the <codeph>mount</codeph> dimension.</p>
                  <p>The most probable cause is that the drive has failed or that it had a temporary
                    failure during the boot process and remained unmounted.</p>
                  <p>Other possible causes are a file system corruption that prevents the device
                    from being mounted.</p>
                </entry>
                <entry>
                  <p>Reboot the node and see if the file system remains unmounted.</p>
                  <p>If the file system is corrupt, see the process used for the "Drive URE errors"
                    alarm to wipe and reformat the drive.</p>
                </entry>
              </row>
              <row>
                <entry>Swift uptime-monitor status</entry>
                <entry>Alarms if the swiftlm-uptime-monitor has errors using Keystone
                  (keystone-get-token), Swift (rest-api) or Swift's healthcheck.</entry>
                <entry>The swiftlm-uptime-monitor cannot get a token from Keystone or cannot get a
                  successful response from the Swift Object-Storage API.</entry>
                <entry>
                  <p>Check that the Keystone service is running:</p>
                  <ol id="ol_obr_2zp_mx">
                    <li>Log in to the lifecycle manager.</li>
                    <li>Check the status of the Keystone service:
                      <codeblock>cd ~/scratch/ansible/next/hos/ansible/
ansible-playbook -i hosts/verb_hosts keystone-status.yml</codeblock></li>
                    <li>If it is not running, start the service:
                      <codeblock>cd ~/scratch/ansible/next/hos/ansible/
ansible-playbook -i hosts/verb_hosts keystone-start.yml</codeblock></li>
                    <li>Contact the support team if further assistance troubleshooting the Keystone
                      service is needed.</li>
                  </ol>
                  <p>Check that Swift is running:</p>
                  <ol id="ol_pbr_2zp_mx">
                    <li>Log in to the lifecycle manager.</li>
                    <li>Check the status of the Keystone service:
                      <codeblock>cd ~/scratch/ansible/next/hos/ansible/
ansible-playbook -i hosts/verb_hosts swift-status.yml</codeblock></li>
                    <li>If it is not running, start the service:
                      <codeblock>cd ~/scratch/ansible/next/hos/ansible/
ansible-playbook -i hosts/verb_hosts swift-start.yml</codeblock></li>
                  </ol>
                  <p>Restart the swiftlm-uptime-montitor as follows:</p>
                  <ol id="ol_qbr_2zp_mx">
                    <li>Log into the first server running the swift-proxy-server service. Use this
                      playbook below to determine whcih host this is:
                      <codeblock>cd ~/scratch/ansible/next/hos/ansible/
ansible-playbook -i hosts/verb_hosts swift-status.yml --limit SWF-PRX[0]</codeblock></li>
                    <li>Restart the swiftlm-uptime-montitor with this command:
                      <codeblock>sudo systemctl restart swiftlm-uptime-monitor</codeblock></li>
                  </ol>
                </entry>
              </row>
              <row>
                <entry>Swift Keystone server connect</entry>
                <entry>Alarms if a socket cannot be opened to the Keystone service (used for token
                  validation)</entry>
                <entry>The Identity service (Keystone) server may be down. Another possible cause is
                  that the network between the host reporting the problem and the Keystone server or
                  the <codeph>haproxy</codeph> process is not forwarding requests to
                  Keystone.</entry>
                <entry>The <codeph>URL</codeph> dimension contains the name of the virtual IP
                  address. Use cURL or a similar program to confirm that a connection can or cannot
                  be made to the virtual IP address. Check that <codeph>haproxy</codeph> is running.
                  Check that the Keystone service is working.</entry>
              </row>
              <row>
                <entry>Swift service listening on ip and port</entry>
                <entry>Alarms when a swift service is not listening on the correct port or
                  ip.</entry>
                <entry>The Swift service may be down.</entry>
                <entry>Verify the status of the Swift service on the affected host, as specified by
                  the <codeph>hostname</codeph> dimension.<ol id="ol_l4w_tys_lx">
                    <li>Log in to the lifecycle manager.</li>
                    <li>Run the Swift status playbook to confirm
                      status:<codeblock>cd ~/scratch/ansible/next/hos/ansible/
ansible-playbook -i hosts/verb_hosts swift-status.yml --limit &lt;hostname></codeblock></li>
                  </ol><p>If an issue is determined, you can stop and restart the Swift service with
                    these steps:</p><ol id="ol_rbr_2zp_mx">
                    <li>Log in to the lifecycle manager.</li>
                    <li>Stop the Swift service on the affected host:
                      <codeblock>cd ~/scratch/ansible/next/hos/ansible/
ansible-playbook -i hosts/verb_hosts swift-stop.yml --limit &lt;hostname></codeblock></li>
                    <li>Restart the Swift service on the affected host:
                      <codeblock>cd ~/scratch/ansible/next/hos/ansible/
ansible-playbook -i hosts/verb_hosts swift-start.yml --limit &lt;hostname></codeblock></li>
                  </ol>
                </entry>
              </row>
              <row>
                <entry>Swift rings checksum</entry>
                <entry>Alarms if the swift rings checksums do not match on all hosts.</entry>
                <entry>
                  <p>The Swift ring files must be the same on every node. The files are located in
                      <codeph>/etc/swift/*.ring.gz</codeph></p>
                  <p>If you have just changed any of the rings and you are still deploying the
                    change, it is normal for this alarm to trigger.</p>
                </entry>
                <entry>
                  <p>If you have just changed any of your Swift rings, if you wait until the changes
                    complete then this alarm will likely clear on its own. If it does not, then
                    continue with these steps.</p>
                  <p>Use <codeph>sudo swift-recon --md5</codeph> to find which node has outdated
                    rings.</p>
                  <p>Run the <codeph>swift-reconfigure.yml</codeph> playbook, using the steps below.
                    This should deploy the same set of rings to every node.</p>
                  <ol id="ol_sbr_2zp_mx">
                    <li>Log in to the lifecycle manager.</li>
                    <li>Run the Swift start playbook against the affected host:
                      <codeblock>cd ~/scratch/ansible/next/hos/ansible/
ansible-playbook -i hosts/verb_hosts swift-reconfigure.yml</codeblock></li>
                  </ol>
                </entry>
              </row>
              <row>
                <entry>Swift memcached server connect</entry>
                <entry>Alarms if a socket cannot be opened to the specified memcached
                  server.</entry>
                <entry>The server may be down. The memcached deamon running the server may have
                  stopped.</entry>
                <entry>
                  <p>If the server is down, restart it.</p>
                  <p>If memcached has stopped, you can restart it by using the
                      <codeph>memcached-start.yml</codeph> playbook, using the steps below. If this
                    fails, rebooting the node will restart the process.</p>
                  <ol id="ol_tbr_2zp_mx">
                    <li>Log in to the lifecycle manager.</li>
                    <li>Run the memcached start playbook against the affected host:
                      <codeblock>cd ~/scratch/ansible/next/hos/ansible/
ansible-playbook -i hosts/verb_hosts memcached-start.yml --limit &lt;hostname></codeblock></li>
                  </ol>
                  <p>If the server is running and memcached is running, there may be a network
                    problem blocking port 11211.</p>
                  <p>If you see sporadic alarms on different servers, the system may be running out
                    of resources. Contact HPE Support for advice.</p>
                </entry>
              </row>
              <row>
                <entry>Swift individual disk usage exceeds 80%</entry>
                <entry>Alarms when a disk drive used by Swift exceeds 80% utilization.</entry>
                <entry>Generally all disk drives will fill roughly at the same rate. If an
                  individual disk drive becomes filled faster than other drives it can indicate a
                  problem with the replication process.</entry>
                <entry>
                  <p>If many/most of your disk drives are 80% full you need to add more nodes to
                    your system or delete existing objects.</p>
                  <p>If one disk drive is noticeably (more than 30%) more utilized than the average
                    of other disk drives, you should check that Swift processes are working on the
                    server (use the steps below) and also look for alarms related to the host.
                    Otherwise continue to monitor the situation.</p>
                  <ol id="ol_ubr_2zp_mx">
                    <li>Log in to the lifecycle manager.</li>
                    <li>Run the Swift status:
                      <codeblock>cd ~/scratch/ansible/next/hos/ansible/
ansible-playbook -i hosts/verb_hosts swift-status.yml</codeblock></li>
                  </ol>
                </entry>
              </row>
              <row>
                <entry>Swift individual disk usage exceeds 90%</entry>
                <entry>Alarms when a disk drive used by Swift exceeds 90% utilization.</entry>
                <entry>Generally all disk drives will fill roughly at the same rate. If an
                  individual disk drive becomes filled faster than other drives it can indicate a
                  problem with the replication process.</entry>
                <entry>If one disk drive is noticeably (more than 30%) more utilized than the
                  average of other disk drives, you should check that Swift processes are working on
                  the server, using these steps: <ol id="ol_vbr_2zp_mx">
                    <li>Log in to the lifecycle manager.</li>
                    <li>Run the Swift status:
                      <codeblock>cd ~/scratch/ansible/next/hos/ansible/
ansible-playbook -i hosts/verb_hosts swift-status.yml</codeblock></li>
                  </ol>
                  <p>Also look for alarms related to the host. An individual disk drive filling can
                    indicate a problem with the replication process.</p>
                  <p>Restart Swift on that host using the <codeph>--limit</codeph> argument to
                    target the host:</p>
                  <ol id="ol_wbr_2zp_mx">
                    <li>Log in to the lifecycle manager.</li>
                    <li>Stop the Swift service:
                      <codeblock>cd ~/scratch/ansible/next/hos/ansible/
ansible-playbook -i hosts/verb_hosts swift-stop.yml --limit &lt;hostname></codeblock></li>
                    <li>Start the Swift service back up:
                      <codeblock>cd ~/scratch/ansible/next/hos/ansible/
ansible-playbook -i hosts/verb_hosts swift-start.yml --limit &lt;hostname></codeblock></li>
                  </ol>
                  <p>If the utilization does not return to similar values as other disk drives, you
                    can reformat the disk drive. You should only do this if the average utilization
                    of all disk drives is less than 80%. To format a disk drive contact HPE Support
                    for instructions.</p></entry>
              </row>
              <row>
                <entry>Swift total disk usage exceeds 80%</entry>
                <entry>Alarms when the average disk utilization of Swift disk drives exceeds 80%
                  utilization.</entry>
                <entry>The number and size of objects in your system is beginning to fill the
                  available disk space. Account and container storage is included in disk
                  utilization. However, this generally consumes 1-2% of space compared to objects,
                  so object storage is the dominate consumer of disk space.</entry>
                <entry>
                  <p>You need to add more nodes to your system or delete existing objects to remain
                    under 80% utilization.</p>
                  <p>
                    <note>If you delete a project/account, the objects in that account are not
                      removed until a week later by the <codeph>account-reaper</codeph> process, so
                      this is not a good way of quickly freeing up space.</note>
                  </p>
                </entry>
              </row>
              <row>
                <entry>Swift total disk usage exceeds 90%</entry>
                <entry>Alarms when the average disk utilization of Swift disk drives exceeds 90%
                  utilization.</entry>
                <entry>The number and size of objects in your system is beginning to fill the
                  available disk space. Account and container storage is included in disk
                  utilization. However, this generally consumes 1-2% of space compared to objects,
                  so object storage is the dominate consumer of disk space.</entry>
                <entry>
                  <p>If your disk drives are 90% full you must immediately stop all applications
                    that put new objects into the system. At that point you can either delete
                    objects or add more servers.</p>
                  <p>Using the steps below, you should also set the
                      <codeph>fallocate_reserve</codeph> value to a value higher than the currently
                    available space on disk drives. This will prevent more objects being
                    created.</p>
                  <ol id="ol_xbr_2zp_mx">
                    <li>Log in to the lifecycle manager.</li>
                    <li>Edit the configuration files below and change the value for
                        <codeph>fallocate_reserve</codeph> to a value higher than the currently
                      available space on the disk drives:
                      <codeblock>~/helion/my_cloud/config/swift/account-server.conf.j2
~/helion/my_cloud/config/swift/container-server.conf.j2
~/helion/my_cloud/config/swift/object-server.conf.j2</codeblock></li>
                    <li>Commit the changes to git:
                      <codeblock>git add -A
git commit -a -m "changing Swift fallocate_reserve value"</codeblock></li>
                    <li>Run the configuration processor:
                      <codeblock>cd ~/helion/hos/ansible
ansible-playbook -i hosts/localhost config-processor-run.yml</codeblock></li>
                    <li>Update your deployment directory:
                      <codeblock>cd ~/helion/hos/ansible
ansible-playbook -i hosts/localhost ready-deployment.yml</codeblock></li>
                    <li>Run the Swift reconfigure playbook to deploy the change:
                      <codeblock>cd ~/scratch/ansible/next/hos/ansible/
ansible-playbook -i hosts/verb_hosts swift-reconfigure.yml</codeblock></li>
                  </ol>
                  <p>If you allow your file systems to become full, you will be unable to delete
                    objects or add more nodes to the system. This is because the system needs some
                    free space to handle the replication process when adding nodes. With no free
                    space, the replication process cannot work.</p>
                </entry>
              </row>
              <row>
                <entry>Swift service per-minute availability</entry>
                <entry>Alarms if the swift service reports unavailable for the previous
                  minute.</entry>
                <entry>The <codeph>swiftlm-uptime-monitor</codeph> service runs on the first proxy
                  server. It monitors the Swift endpoint and reports latency data. If the endpoint
                  stops reporting, it generates this alarm.</entry>
                <entry>
                  <p>There are many reasons why the endpoint may stop running. Check:</p>
                  <ul id="ul_ghg_4hc_4v">
                    <li>Is <codeph>haproxy</codeph> running on the control nodes?</li>
                    <li>Is <codeph>swift-proxy-server</codeph> running on the Swift proxy
                      servers?</li>
                  </ul>
                </entry>
              </row>
              <row>
                <entry>Swift rsync connect</entry>
                <entry>Alarms if a socket cannot be opened to the specified rsync server</entry>
                <entry>The rsync daemon on the specified node cannot be contacted. The most probable
                  cause is that the node is down. The rsync service might also have been stopped on
                  the node.</entry>
                <entry>
                  <p>Reboot the server if it is down.</p>
                  <p>Attempt to restart rsync with this
                    command:<codeblock>systemctl restart rsync.service</codeblock></p>
                </entry>
              </row>
              <row>
                <entry>Swift smart array controller status</entry>
                <entry>Alarms if there is a failure in the Smart Array.</entry>
                <entry>The Smart Array or Smart HBA controller has a fault or a component of the
                  controller (such as a battery) is failed or caching is disabled.</entry>
                <entry>Log in to the reported host and run these commands to find out the status of
                  the
                    controllers:<codeblock>sudo hpssacli
=> controller show all detail</codeblock><p>For
                    hardware failures (such as failed battery), replace the failed component. If the
                    cache is disabled, reenable the cache.</p></entry>
              </row>
              <row>
                <entry>Swift physical drive status</entry>
                <entry>Alarms if there is a failure in the Physical Drive.</entry>
                <entry>A disk drive on the server has failed or has warnings.</entry>
                <entry>Log in to the reported and run these commands to find out the status of the
                    drive:<codeblock>sudo hpssacli
=> ctrl slot=1 pd all show</codeblock><p>Replace
                    any broken drives.</p></entry>
              </row>
              <row>
                <entry>Swift logical drive status</entry>
                <entry>Alarms if there is a failure in the Logical Drive.</entry>
                <entry>A LUN on the server is degraded or has failed.</entry>
                <entry>Log in to the reported host and run these commands to find out the status of
                  the
                    LUN:<codeblock>sudo hpssacli
=> ctrl slot=1 ld all show
=> ctrl slot=1 pd all show</codeblock><p>Replace
                    any broken drives.</p></entry>
              </row>
              <row>
                <entry>Process Check</entry>
                <entry>Alarms when the specified process is not running.</entry>
                <entry>If the <codeph>service</codeph> dimension is <codeph>object-store</codeph>,
                  see the description of the "Swift Service" alarm for possible causes.</entry>
                <entry>If the <codeph>service</codeph> dimension is <codeph>object-storage</codeph>,
                  see the description of the "Swift Service" alarm for possible mitigation
                  tasks.</entry>
              </row>
              <row>
                <entry>HTTP Status</entry>
                <entry>Alarms when the specified HTTP endpoint is down or not reachable.</entry>
                <entry>If the <codeph>service</codeph> dimension is <codeph>object-store</codeph>,
                  see the description of the "Swift host socket connect" alarm for possible
                  causes.</entry>
                <entry>If the <codeph>service</codeph> dimension is <codeph>object-storage</codeph>,
                  see the description of the "Swift host socket connect" alarm for possible
                  mitigation tasks.</entry>
              </row>
              <row>
                <entry>Service Log Directory Size</entry>
                <entry>Service log directory consuming more disk than its quota.</entry>
                <entry>This could be due to a service set to <codeph>DEBUG</codeph> instead of
                    <codeph>INFO</codeph> level. Another reason could be due to a repeating error
                  message filling up the log files. Finally, it could be due to log rotate not
                  configured properly so old log files are not being deleted properly.</entry>
                <entry>Find the service that is consuming too much disk space. Look at the logs. If
                    <codeph>DEBUG</codeph> log entries exist, set the logging level to
                    <codeph>INFO</codeph>. If the logs are repeatedly logging an error message, do
                  what is needed to resolve the error. If old log files exist, configure log rotate
                  to remove them. You could also choose to remove old log files by hand after
                  backing them up if needed.</entry>
              </row>
              <row>
                <entry morerows="8">block-storage</entry>
                <entry>Process Check</entry>
                <entry>Separate alarms for each of these Cinder services, specified by the
                    <codeph>component</codeph> dimension: <ul id="ul_ybr_2zp_mx">
                    <li>cinder-api</li>
                    <li>cinder-backup</li>
                    <li>cinder-scheduler</li>
                    <li>cinder-volume</li>
                  </ul></entry>
                <entry>Process crashed.</entry>
                <entry>Restart the process on the affected node. Review the associated logs.<ol
                    id="ol_yhz_rpk_pw">
                    <li>Log in to the lifecycle manager.</li>
                    <li>Run the cinder-start.yml playbook to start the process back
                        up:<codeblock>cd ~/scratch/ansible/next/hos/ansible
ansible-playbook -i hosts/verb_hosts cinder-start.yml --limit &lt;hostname></codeblock><note>The
                          <codeph>--limit &lt;hostname></codeph> switch is optional. If it is
                        included, then the <codeph>&lt;hostname></codeph> you should use is the host
                        where the alarm was raised.</note></li>
                  </ol></entry>
              </row>
              <row>
                <entry>Process Check</entry>
                <entry>Alarms when the specified process is not running.
                  <codeblock>process_name=cinder-backup</codeblock>
                </entry>
                <entry>Process crashed.</entry>
                <entry>Alert may be incorrect if the service has migrated. Validate that the service
                  is intended to be running on this node before restarting the service. Review the
                  associated logs.</entry>
              </row>
              <row>
                <entry>Process Check</entry>
                <entry>Alarms when the specified process is not running.
                  <codeblock>process_name=cinder-scheduler</codeblock>
                </entry>
                <entry>Process crashed.</entry>
                <entry>Restart the process on the affected node. Review the associated logs. <ol
                    id="ol_zbr_2zp_mx">
                    <li>Log in to the lifecycle manager.</li>
                    <li>Run the cinder-start.yml playbook to start the process back
                        up:<codeblock>cd ~/scratch/ansible/next/hos/ansible
ansible-playbook -i hosts/verb_hosts cinder-start.yml --limit &lt;hostname></codeblock><note>The
                          <codeph>--limit &lt;hostname></codeph> switch is optional. If it is
                        included, then the <codeph>&lt;hostname></codeph> you should use is the host
                        where the alarm was raised.</note></li>
                  </ol></entry>
              </row>
              <row>
                <entry>Process Check</entry>
                <entry>Alarms when the specified process is not running.
                  <codeblock>process_name=cinder-volume</codeblock>
                </entry>
                <entry>Process crashed.</entry>
                <entry>Alert may be incorrect if the service has migrated. Validate that the service
                  is intended to be running on this node before restarting the service. Review the
                  associated logs.</entry>
              </row>
              <row>
                <entry>Cinder backup running &lt;hostname&gt; check</entry>
                <entry>Cinder backup singleton check.</entry>
                <entry>
                  <p>Backup process is either:</p>
                  <p>
                    <ul id="ul_acr_2zp_mx">
                      <li>running on a node it should not be on, or</li>
                      <li>not running on a node it should be on</li>
                    </ul>
                  </p>
                </entry>
                <entry>Run the <codeph>cinder-migrate-volume.yml</codeph> playbook to migrate the
                  volume and backup to the correct node:<ol id="ol_ztz_mpk_pw">
                    <li>Log in to the lifecycle manager.</li>
                    <li>Run this playbook to migrate the
                      service:<codeblock>cd ~/scratch/ansible/next/hos/ansible/
ansible-playbook -i hosts/verb_hosts cinder-migrate-volume.yml</codeblock></li>
                  </ol></entry>
              </row>
              <row>
                <entry>Cinder volume running &lt;hostname&gt; check</entry>
                <entry>Cinder volume singleton check.</entry>
                <entry>
                  <p>The <codeph>cinder-volume</codeph> process is either:</p>
                  <p>
                    <ul id="ul_bcr_2zp_mx">
                      <li>running on a node it should not be on, or</li>
                      <li>not running on a node it should be on</li>
                    </ul>
                  </p>
                </entry>
                <entry>Run the <codeph>cinder-migrate-volume.yml</codeph> playbook to migrate the
                  volume and backup to correct node:<ol id="ol_kgy_4pk_pw">
                    <li>Log in to the lifecycle manager.</li>
                    <li>Run this playbook to migrate the service:
                      <codeblock>cd ~/scratch/ansible/next/hos/ansible/
ansible-playbook -i hosts/verb_hosts cinder-migrate-volume.yml</codeblock></li>
                  </ol></entry>
              </row>
              <row>
                <entry>Storage faulty lun check</entry>
                <entry>Alarms if local LUNs on your HPE servers using smartarray are not OK.
                    <note>This alarm only pertains to those using HPE servers with
                    smartarray.</note></entry>
                <entry>A LUN on the server is degraded or has failed.</entry>
                <entry>Log in to the reported host and run these commands to find out the status of
                  the
                    LUN:<codeblock>sudo hpssacli
=> ctrl slot=1 ld all show
=> ctrl slot=1 pd all show</codeblock><p>Replace
                    any broken drives.</p></entry>
              </row>
              <row>
                <entry>Storage faulty drive check</entry>
                <entry>Alarms if the local disk drives on your HPE servers using smartarray are not
                    OK.<note>This alarm only pertains to those using HPE servers with
                    smartarray.</note></entry>
                <entry>A disk drive on the server has failed or has warnings.</entry>
                <entry>Log in to the reported and run these commands to find out the status of the
                    drive:<codeblock>sudo hpssacli
=> ctrl slot=1 pd all show</codeblock><p>Replace
                    any broken drives.</p></entry>
              </row>
              <row>
                <entry>Service Log Directory Size</entry>
                <entry>Service log directory consuming more disk than its quota.</entry>
                <entry>This could be due to a service set to <codeph>DEBUG</codeph> instead of
                    <codeph>INFO</codeph> level. Another reason could be due to a repeating error
                  message filling up the log files. Finally, it could be due to log rotate not
                  configured properly so old log files are not being deleted properly.</entry>
                <entry>Find the service that is consuming too much disk space. Look at the logs. If
                    <codeph>DEBUG</codeph> log entries exist, set the logging level to
                    <codeph>INFO</codeph>. If the logs are repeatedly logging an error message, do
                  what is needed to resolve the error. If old log files exist, configure log rotate
                  to remove them. You could also choose to remove old log files by hand after
                  backing them up if needed.</entry>
              </row>
              <!-- DOCS-2905 -->
              <row>
                <entry morerows="1">vsa</entry>
                <entry>VSA VM Status</entry>
                <entry>Alarms if the VSA applicance VM is down, indicated by
                    <codeph>vsa_vm_status</codeph> not running.</entry>
                <entry>A VSA node may have rebooted or the VSA appliance VM may be in a paused state
                  due to disk errors. Insufficient hard drive space is another possible
                  cause.</entry>
                <entry>Investigate the VSA appliance VM status in the node. You can check the status
                  by using these steps: <ol id="ol_ccr_2zp_mx">
                    <li>SSH to the affected VSA host.</li>
                    <li>Use this command to check the status of the VSA appliance VM:
                      <codeblock>sudo virsh list --all</codeblock></li>
                    <li>If it is in a paused state, you can restart it by using this command. Note
                      that the <codeph>&lt;vsa_vm_name></codeph> value will be the name in the
                      output of the <codeph>virsh list</codeph> command
                      above.<codeblock>sudo virsh start &lt;vsa_vm_name></codeblock></li>
                  </ol> If you wish to resolve this issue from the lifecycle manager, you can use
                  these steps: <ol id="ol_dcr_2zp_mx">
                    <li>Log in to the lifecycle manager.</li>
                    <li>Use the VSA start playbook against the affected node:
                      <codeblock>cd ~/scratch/ansible/next/hos/ansible
ansible-playbook -i hosts/verb_hosts vsa-start.yml --limit &lt;vsa_hostname></codeblock></li>
                  </ol></entry>
              </row>
              <row>
                <entry>VSA VM Network Status</entry>
                <entry>Alarms if the VSA appliance VM has lost connectivity to the bridge network,
                  indicated by <codeph>vsa_vm_net_status</codeph> not running.</entry>
                <entry>VSA appliance VM network/bridge issues.</entry>
                <entry>Investigate the VSA VM network status in the node. You can check the status
                  by using these steps:<ol id="ol_ecr_2zp_mx">
                    <li>SSH to the affected VSA host.</li>
                    <li>Use this command to check the status of the VSA VM
                      network:<codeblock>sudo virsh net-list</codeblock></li>
                    <li>If it is in an inactive state, you can restart it by using this command.
                      Note that the <codeph>&lt;vsa_vm_network_name></codeph> value will be the name
                      in the output of the <codeph>virsh net-list</codeph> command
                      above.<codeblock>sudo virsh net-start &lt;vsa_vm_network_name></codeblock></li>
                  </ol> If you wish to resolve this issue from the lifecycle manager, you can use
                  these steps: <ol id="ol_fcr_2zp_mx">
                    <li>Log in to the lifecycle manager.</li>
                    <li>Use the VSA start playbook against the affected node:
                      <codeblock>cd ~/scratch/ansible/next/hos/ansible
ansible-playbook -i hosts/verb_hosts vsa-start.yml --limit &lt;vsa_hostname></codeblock></li>
                  </ol></entry>
              </row>
              <row>
                <entry morerows="20">ceph-storage</entry>
                <entry>Process Check</entry>
                <entry>Separate alarms for each of these Ceph services, specified by the
                    <codeph>component</codeph> dimension: <ul id="ul_vyv_1z5_qv">
                    <li>&lt;cluster_name>-osd.&lt;id></li>
                    <li>&lt;cluster_name>-mon.&lt;id></li>
                    <li>&lt;cluster_name>-radosgw.&lt;id></li>
                  </ul></entry>
                <entry>Process crashed.</entry>
                <entry>Restart the process on the affected node using these steps: <ol
                    id="ol_gcr_2zp_mx">
                    <li>Log in to the lifecycle manager.</li>
                    <li>Use the Ceph start playbook against the affected node:
                      <codeblock>cd ~/scratch/ansible/next/hos/ansible
ansible-playbook -i hosts/verb_hosts ceph-start.yml --limit &lt;hostname></codeblock></li>
                  </ol></entry>
              </row>
              <row>
                <entry>Degraded Ceph cluster alert</entry>
                <entry>Alarms if ceph cluster status is not ok.</entry>
                <entry>
                  <p>The likely cause is one of the following possibilities:</p>
                  <ul id="ul_bxh_xkt_2y">
                    <li>OSDs being down</li>
                    <li>Clock skew</li>
                    <li>PG errors</li>
                  </ul>
                </entry>
                <entry>
                  <p>You should check the following:</p>
                  <ul id="ul_l35_xkt_2y">
                    <li>cluster overall status, by running the following
                      command:<codeblock>ceph -s</codeblock></li>
                    <li>the reason, which can be found in the command output</li>
                  </ul>
                  <p>Based on reason, you might have to perform specific actions to correct the
                    failure. For e.g. if cluster is in HEALTH_WARN state because one of OSD being
                    down then you need to recover failed OSD. In general, you might like to perform
                    following steps to triage the reason of failure:</p>
                  <ul id="ul_m35_xkt_2y">
                    <li>Run <b>'ceph -s'</b> command to get cluster status</li>
                    <li>Observing following logs: <b>/var/log/ceph/&lt;cluster name>-cluster.log</b>
                      on monitor nodes, <b>/var/log/ceph/ceph-mon.&lt;hostname>.log</b></li>
                    <li>Run <b>'ceph osd tree'</b> command to check OSD status</li>
                  </ul>
                </entry>
              </row>
              <row>
                <entry>Broken Ceph cluster alert</entry>
                <entry>Alarms if the Ceph cluster is unusable.<p>Generates alarm only when cluster is
                  in fatal error state, where no I/O operations can be performed.</p></entry>
                <entry>
                  <p>The cause may be one or more of the following possibilities:</p>
                  <ul id="ul_vdh_zkt_2y">
                    <li>All OSDs being down</li>
                    <li>Ceph Monitor quorum is broken</li>
                    <li>Cluster is full</li>
                  </ul>
                </entry>
                <entry>
                  <p>Please observe following:</p>
                  <ul id="ul_ym4_zkt_2y">
                    <li>cluster overall status (by running the command 'ceph -s')</li>
                    <li>reason (can be found in command output)</li>
                  </ul>
                  <p>Based on reason, you might have to perform specific actions to correct the
                    failure. For e.g. if cluster is in HEALTH_ERR state because of a broken Ceph
                    Monitor quorum, check whether the ceph-monitor services are up and running and
                    are reachable from each other. In general, you might like to perform following
                    steps to triage the reason of failure:</p>
                  <ul id="ul_zm4_zkt_2y">
                    <li>Run <b>'ceph -s'</b> command to get cluster status</li>
                    <li>Observing following logs: <b>/var/log/ceph/&lt;cluster name>-cluster.log</b>
                      on monitor nodes, <b>/var/log/ceph/ceph-mon.&lt;hostname>.log</b></li>
                    <li>Run <b>'ceph osd tree'</b> command to check OSD status</li>
                  </ul>
                </entry>
              </row>
              <row>
                <entry>Degraded RGW availability alert</entry>
                <entry>Alarms if any of the Ceph RADOS gateways is not reachable.</entry>
                <entry>
                  <p>The likely cause can be one or more of the following conditions on <b>one or
                      more</b> of your Ceph RADOS gateway nodes:</p>
                  <ul id="ul_uzs_blt_2y">
                    <li>Failure in <codeph>radosgw</codeph> service</li>
                    <li>Failure in <codeph>apache2</codeph> service</li>
                    <li>Network connectivity issues</li>
                  </ul></entry>
                <entry>
                  <p>The mitigation plan depends on the type of error reported by the plugin.When a
                    failure is reported, the following parameters should be checked:</p>
                  <ol id="ol_fn3_clt_2y">
                    <li>Ensure the following services on the radosgw node are running:</li>
                  </ol>
                  <ul id="ul_gn3_clt_2y">
                    <li>
                      <ul id="ul_hn3_clt_2y">
                        <li>radosgw</li>
                        <li>apache2</li>
                      </ul>
                    </li>
                  </ul>
                  <p> 2. Ensure that the radosgw site in apache is enabled.</p>
                  <p> 3. Verify the radosgw apache site configuration
                      (<b>/etc/apache2/sites-available/rgw.conf</b>) is available/accessible and has
                      '<b>Listen</b>' directive.</p>
                  <p> 4. If the issue still persists, check the radosgw service logs (i.e.
                      <b>/var/log/ceph/radosgw.log</b>) for more details.</p>
                </entry>
              </row>
              <row>
                <entry>Broken RGW availability alert</entry>
                <entry>Alarms if all of the Ceph RADOS gateways are not reachable.</entry>
                <entry>
                  <p>The likely cause can be one or more of the following conditions on <b>all</b>
                    of your Ceph RADOS gateway nodes:</p>
                  <ul id="ul_lq2_2lt_2y">
                    <li>Failure in <codeph>radosgw</codeph> service</li>
                    <li>Failure in <codeph>apache2</codeph> service</li>
                    <li>Network connectivity issues</li>
                  </ul>
                  </entry>
                <entry>
                  <p>The mitigation plan depends on the type of error reported by the plugin.When a
                    failure is reported, the following parameters should be checked:</p>
                  <ol id="ol_fsl_2lt_2y">
                    <li>Ensure the following services on the radosgw node are running:</li>
                  </ol>
                  <ul id="ul_gsl_2lt_2y">
                    <li>
                      <ul id="ul_hsl_2lt_2y">
                        <li>radosgw</li>
                        <li>apache2</li>
                      </ul>
                    </li>
                  </ul>
                  <p> 2. Ensure that the radosgw site in apache is enabled.</p>
                  <p> 3. Verify the radosgw apache site configuration
                      (<b>/etc/apache2/sites-available/rgw.conf</b>) is available/accessible and has
                      '<b>Listen</b>' directive.</p>
                  <p> 4. If the issue still persists, check the radosgw service logs (i.e.
                      <b>/var/log/ceph/radosgw.log</b>) for more details.</p>
                </entry>
              </row>
              <row>
                <entry>OSD failure alert</entry>
                <entry>Alarms if any of the OSDs are down.</entry>
                <entry>The likely cause can be one or more of the following conditions on <b>one or
                    more</b> of your Ceph OSD nodes:<ul id="ul_n4l_flt_2y">
                    <li>disk failure</li>
                    <li>service failure</li>
                    <li>network connectivity failure</li>
                    <li>host failure</li>
                  </ul></entry>
                <entry>
                  <p>Depending on the cause of the failure it will be useful to:</p>
                  <ul id="ul_mtt_flt_2y">
                    <li>Check node hosting OSD disk is accessible or not.</li>
                    <li>Check node connectivity to ensure that monitors can be accessed from the
                      node hosting failed OSD disk</li>
                    <li>Check disk failure. The system log <b>/var/log/syslog</b> might provide
                      useful information.</li>
                    <li>Check service status. The <b>/var/log/ceph/ceph-osd.&lt;id>.log</b> provides
                      useful information for the respective osd services</li>
                  </ul>
                  <p>Most of cases, OSD going in down state is because of either disk failure or
                    service not running as expected. In case of disk failure, you might like to
                    replace disk.</p>
                </entry>
              </row>
              <row>
                <entry>All OSD failure alert</entry>
                <entry>Alarms if all the OSDs are down.</entry>
                <entry>The likely cause can be one or more of the following conditions on <b>all</b>
                  of your Ceph OSD nodes:<ul id="ul_y2t_glt_2y">
                    <li>disk failure</li>
                    <li>service failure</li>
                    <li>network connectivity failure</li>
                    <li>host failure</li>
                  </ul></entry>
                <entry>
                  <p>Depending on the cause of the failure it will be useful to:</p>
                  <ul id="ul_oyz_glt_2y">
                    <li>Check node hosting OSD disk is accessible or not.</li>
                    <li>Check node connectivity to ensure that monitors can be accessed from the
                      node hosting failed OSD disk</li>
                    <li>Check disk failure. The system log <b>/var/log/syslog</b> might provide
                      useful information.</li>
                    <li>Check service status. The <b>/var/log/ceph/ceph-osd.&lt;id>.log</b> provides
                      useful information for the respective osd services</li>
                  </ul>
                  <p>Most of cases, OSD going in down state is because of either disk failure or
                    service not running as expected. In case of disk failure, you might like to
                    replace disk.</p>
                </entry>
              </row>
              <row>
                <entry>Unutilized active OSD alert</entry>
                <entry>Alarms if any Ceph OSDs are up but not part of the cluster.</entry>
                <entry>The likely cause is the cluster has one or more OSDs that are running but not
                  part of the cluster, which results in unutilized disk space.</entry>
                <entry>
                  <p>Run the following command on any ceph monitor node to identify the OSDs that
                    are up and out:</p>
                  <p>
                    <codeblock>ceph osd tree | grep -e 'up.* 0'</codeblock>
                  </p>
                  <p>The above command lists out nodes that are up but out of the cluster.You can
                    make them a part of the cluster by running the below command:</p>
                  <p>
                    <codeblock>ceph osd in &lt;osd.id></codeblock>
                  </p>
                </entry>
              </row>
              <row>
                <entry>Lower threshold (75% filled) Ceph Cluster capacity alert</entry>
                <entry>Alarms if the used capacity of the Ceph cluster exceeds 75% of the total
                  capacity.</entry>
                <entry>Generates alarm if the used capacity of the Ceph cluster exceeds 75% of the
                  total capacity.</entry>
                <entry>Increase the cluster capacity by adding more OSDs to the cluster. To add more
                  OSDs update the disk input model and run ceph playbooks</entry>
              </row>
              <row>
                <entry>High threshold (85% filled) Ceph Cluster capacity alert</entry>
                <entry>Alarms if the used capacity of the ceph cluster exceeds 85% of the total
                  capacity,</entry>
                <entry>Generates alarm if the used capacity of the ceph cluster exceeds 85% of the
                  total capacity.</entry>
                <entry>Increase the cluster capacity by adding more OSDs to the cluster. To add more
                  OSDs update the disk input model and run ceph playbooks</entry>
              </row>
              <row>
                <entry>Degraded Ceph monitor quorum alert</entry>
                <entry>Alarms if any of the Monitors are not in quorum.</entry>
                <entry>
                  <p>The likely cause can be one or more of the following conditions on <b>one or
                      more</b> of your Ceph monitor nodes:</p>
                  <ul id="ul_ecv_mlt_2y">
                    <li>Monitor services are down</li>
                    <li>Network connectivity issues</li>
                    <li>Clock skew</li>
                  </ul></entry>
                <entry>
                  <p>Mitigation plan depends on which state the quorum is in. The message shows the
                    monitors which are out of quorum.For the monitors which are not in quorum:</p>
                  <ul id="ul_r3b_nlt_2y">
                    <li>
                      <p>Check the status of monitor services.</p>
                    </li>
                    <li>
                      <p>Check the network connectivity between the monitor nodes having monitors
                        out of quorum and other monitor nodes.</p>
                    </li>
                    <li>
                      <p>Check the logs of the monitors in their respective nodes.</p>
                    </li>
                    <li>In case of clock skew, make sure all the nodes running the monitor services
                      are in sync with ntp server.</li>
                  </ul>
                </entry>
              </row>
              <row>
                <entry>Broken Ceph monitor quorum alert</entry>
                <entry>Alarms if all of the Monitors are not in quorum.</entry>
                <entry>
                  <p>The likely cause can be one or more of the following conditions on <b>all</b>
                    of your Ceph monitor nodes:</p>
                  <ul id="ul_ksz_nlt_2y">
                    <li>Monitor services are down</li>
                    <li>Network connectivity issues</li>
                    <li>Clock skew</li>
                  </ul>
                </entry>
                <entry>
                  <p>Mitigation plan depends on which state the quorum is in. The message shows the
                    monitors which are out of quorum.For the monitors which are not in quorum:</p>
                  <ul id="ul_dxf_4lt_2y">
                    <li>
                      <p>Check the status of monitor services.</p>
                    </li>
                    <li>
                      <p>Check the network connectivity between the monitor nodes having monitors
                        out of quorum and other monitor nodes.</p>
                    </li>
                    <li>
                      <p>Check the logs of the monitors in their respective nodes.</p>
                    </li>
                    <li>In case of clock skew, make sure all the nodes running the monitor services
                      are in sync with ntp server.</li>
                  </ul>
                </entry>
              </row>
              <row>
                <entry>Degraded Ceph monitor connectivity alert</entry>
                <entry>Alarms if <b>one or more</b> of the Ceph Monitors are not reachable from the
                  other Ceph nodes.</entry>
                <entry>
                  <p>The likely cause can be one or more of the following conditions on <b>one or
                      more</b> of your Ceph monitor nodes:</p>
                  <ul id="ul_h4s_qnt_2y">
                    <li>Network connectivity issue between the given node and the monitor</li>
                    <li>Monitor service is down</li>
                    <li>Firewall issues</li>
                  </ul>
                  <p>in <b>one or more</b> monitor nodes</p>
                </entry>
                <entry>
                  <p>The mitigation plan depends on multiple factors, like:</p>
                  <ol id="ol_l31_rnt_2y">
                    <li>Only one node reports a monitor unreachable</li>
                    <li>All nodes in the Ceph cluster report a specific monitor as unreachable</li>
                  </ol>
                  <p>Please observe the overall cluster health, as this might point to the cause of
                    all Ceph nodes reporting a specific monitor node as unreachable. In this
                    scenario, ensure that:</p>
                  <ol id="ol_m31_rnt_2y">
                    <li>Monitor service on the (unreachable) node is up and running.</li>
                    <li>Verify the firewall rules on the node to ensure that it is not blocking
                      traffic to the Monitor service from all the Ceph nodes.</li>
                  </ol>
                  <p>We can be in this state because of one or combinations of following
                    reasons:</p>
                  <ol id="ol_n31_rnt_2y">
                    <li>Monitor node is down</li>
                    <li>A specific host is not able to access specific set of monitor</li>
                  </ol>
                  <p>In case of first one, you might be seeing same reported status from other nodes
                    as well. In this case, you are strongly advice to diagnose specific monitor to
                    check various aspects like network connectivity, service status etc. For e.g. if
                      <b>levelDB</b> is full then monitor service shuts itself down which you can
                    identify by looking at log files. In case of second one, you might like to check
                    whether network connectivity or firewall is fine between the respective node and
                    monitor node. Prime reason of failure is expected to be<b> network
                      connectivity</b> only in this case.</p>
                </entry>
              </row>
              <row>
                <entry>Broken Ceph monitor connectivity alert</entry>
                <entry>Alarms if <b>all</b> of the Ceph Monitors are not reachable from the other
                  Ceph nodes.</entry>
                <entry>The likely cause can be one or more of the following conditions on <b>all</b>
                  of your Ceph monitor nodes:<ul id="ul_owg_snt_2y">
                    <li>Network connectivity issue between the specified node and the monitor</li>
                    <li>Monitor service is down</li>
                    <li>Firewall issues</li>
                  </ul></entry>
                <entry>
                  <p>The mitigation plan depends on multiple factors, like:</p>
                  <ol id="ol_ogp_snt_2y">
                    <li>Only one node reports a monitor unreachable</li>
                    <li>All nodes in the Ceph cluster report a specific monitor as unreachable</li>
                  </ol>
                  <p>Please observe the overall cluster health, as this might point to the cause of
                    all Ceph nodes reporting a specific monitor node as unreachable. In this
                    scenario, ensure that:</p>
                  <ol id="ol_pgp_snt_2y">
                    <li>Monitor service on the (unreachable) node is up and running.</li>
                    <li>Verify the firewall rules on the node to ensure that it is not blocking
                      traffic to the Monitor service from all the Ceph nodes.</li>
                  </ol>
                  <p>We can be in this state because of one or combinations of following
                    reasons:</p>
                  <ol id="ol_qgp_snt_2y">
                    <li>Monitor node is down</li>
                    <li>A specific host is not able to access specific set of monitor</li>
                  </ol>
                  <p>In case of first one, you might be seeing same reported status from other nodes
                    as well. In this case, you are strongly advice to diagnose specific monitor to
                    check various aspects like network connectivity, service status etc. For e.g. if
                      <b>levelDB</b> is full then monitor service shuts itself down which you can
                    identify by looking at log files. In case of second one, you might like to check
                    whether network connectivity or firewall is fine between the respective node and
                    monitor node. Prime reason of failure is expected to be<b> network
                      connectivity</b> only in this case.</p>
                </entry>
              </row>
              <row>
                <entry>Recommended OSD memory alert</entry>
                <entry>
                  <p>Alarms if the recommended memory configuration for OSD nodes is not met.</p>
                  <p>The recommendation is <b>1GB RAM</b> per <b>1TB of data disk</b>.</p>
                </entry>
                <entry>The specified OSD node does not adhere to the minimum recommended memory-disk
                  ratio of 1GB RAM per 1TB of data disk.</entry>
                <entry>
                  <p>Having low RAM is not going to cause system failure and hence it does not
                    warrant attention to make the cluster functional. However, note that falling
                    short in RAM per host can cause signficant performance bottleneck and you might
                    observe following:</p>
                  <ol id="ol_wtd_5nt_2y">
                    <li>Throughput of cluster is low</li>
                    <li>Timeout in high I/O traffic scenarios</li>
                    <li>Recovering of failed OSD node or rebuiliding of data is taking longer
                      duration</li>
                  </ol>
                  <p>If you see the alarm please allocate more RAM for the given host node.</p>
                </entry>
              </row>
              <row>
                <entry>Recommended OSD-Journal disk ratio alert</entry>
                <entry>
                  <p>Alarms if the recommended ratio of OSD to journal disks is not adhered to.</p>
                  <p>The recommendation is a maximum of<b> 4 OSDs</b> per journal disk.</p>
                </entry>
                <entry>The specified Ceph OSD node does not adhere to the minimum recommended ratio
                  of 4 OSDs per journal disk.</entry>
                <entry>If you see the alarms then please check input model to figure out
                    following:<ol id="ol_urk_vnt_2y">
                    <li>No disk is left with explicit declaration to use journal disk</li>
                    <li>Number of disks referring same journal disk is not exceeding <b>4:1
                        ratio</b></li>
                  </ol><p>It might be required to re-define disk model for OSD nodes and reconfigure
                    services using ceph playbooks.</p></entry>
              </row>
              <row>
                <entry>Recommended Ceph Public network NIC speed</entry>
                <entry>
                  <p>Alarms if recommended Ceph public network NIC speed is not met.</p>
                  <p>Additionally if there are non Ceph networks detected on the same NIC the alarm
                    is raised.</p>
                  <p>The recommended NIC speeds are:</p>
                  <p><b>10Gb/s</b> for dedicated public network NICs,</p>
                  <p><b>40Gb/s</b> for shared public/private network NIC</p>
                </entry>
                <entry>
                  <p>The node specified by the <codeph>hostname</codeph> dimension's Ceph public
                    network NIC speed is less than the minimum recommended speed</p>
                  <p>(or)</p>
                  <p>Non Ceph networks are detected on the same NIC as the public network NIC.</p>
                </entry>
                <entry>
                  <p>Having low NIC speeds is not going to cause system failure and hence it does
                    not warrant attention to make cluster functional. However, note that falling
                    short in NIC speeds can cause <b>signficant performance</b>
                    <b>bottleneck</b> and you might observe following:</p>
                  <ol id="ol_oy5_wnt_2y">
                    <li>Throughput of cluster is low</li>
                    <li>Timeout in high I/O traffic scenarios</li>
                  </ol>
                  <p>If you see the alarm please switch to a high speed network interface as per
                    recommendations</p>
                </entry>
              </row>
              <row>
                <entry>Recommended Ceph Private network NIC speed</entry>
                <entry>
                  <p>Alarms if recommended Ceph private network NIC speed is not met.</p>
                  <p>Additionally if there are non ceph networks detected on the same NIC the alarm
                    is raised.</p>
                  <p>The recommended NIC speeds are:</p>
                  <p><b>10Gb/s</b> for dedicated private network NICs,</p>
                  <p><b>40Gb/s</b> for shared public/private network NIC</p>
                </entry>
                <entry>
                  <p>The node specified by the <codeph>hostname</codeph> dimension's Ceph private
                    network NIC speed is less than the minimum recommended speed.</p>
                  <p>(or)</p>
                  <p>Non Ceph networks are detected on the same NIC as the private network NIC.</p>
                </entry>
                <entry>
                  <p>Having low NIC speeds is not going to cause system failure and hence it does
                    not warrant attention to make cluster functional. However, note that falling
                    short in NIC speeds can cause <b>signficant</b>
                    <b>performance</b>
                    <b>bottleneck</b> and you might observe following:</p>
                  <ol id="ol_tcd_ynt_2y">
                    <li>Throughput of cluster is low</li>
                    <li>Timeout in high I/O traffic scenarios</li>
                  </ol>
                  <p>If you see the alarm please switch to a high speed network interface as per
                    recommendations</p>
                </entry>
              </row>
              <row>
                <entry>Cephlm probe check</entry>
                <entry>Alarms if the <b>cephlm-probe</b> tool cannot execute a monitoring
                  task</entry>
                <entry>
                  <p>The likely cause can be one or more of the following conditions on <b>one or
                      more</b> of your Ceph nodes, specified by the <codeph>hostname</codeph>
                    dimension:</p>
                  <ul id="ul_afg_b4t_2y">
                    <li>Ceph configuration errors</li>
                    <li>Command timeouts</li>
                  </ul></entry>
                <entry>
                  <p>Depending on the alarm message it is recommended to:</p>
                  <ul id="ul_dxl_b4t_2y">
                    <li>Check for any errors in ceph configuration files</li>
                    <li>Modify the command timeout (default 30 seconds) by editing the configurable
                      parameters in ceph deployment playbooks and re-configuring ceph</li>
                    <li>In case of ceph commands getting timed out repeatedly, please check if
                        "<b>ceph -s</b>" command is working fine on the node. It is observed that
                      when monitor nodes are down the command fails due to timeouts. In such cases,
                      you will need to fix the monitor issues</li>
                  </ul>
                </entry>
              </row>
              <row>
                <entry>Cephlm monitor check</entry>
                <entry>Alarms if monasca cephlm check plugin fails to run or it cannot
                  collect/process the generated metrics.</entry>
                <entry>
                  <p>The likely cause can be one or more of the following conditions on <b>one or
                      more</b> of your Ceph monitor nodes, specified by the
                      <codeph>hostname</codeph> dimension:</p>
                  <ul id="ul_pcg_c4t_2y">
                    <li>Monasca collector failures</li>
                    <li>Stale metrics</li>
                    <li>Stuck cron jobs</li>
                    <li>System time out of sync</li>
                  </ul>
                </entry>
                <entry>
                  <p>Depending on the alarm message it is recommended to:</p>
                  <ul id="ul_wdm_c4t_2y">
                    <li>Check the <b>/var/log/monasca/collector.log </b>and look for any issues
                      pertaining to ceph metrics</li>
                    <li>Check the contents of <b>/var/cache/cephlm</b> on the node to see if the
                      files are getting updated at regular intervals (should not have old files > 6
                      minutes)</li>
                    <li>Check <b>/var/log/syslog</b> for cephlm cron job errors</li>
                  </ul>
                </entry>
              </row>
              <row>
                <entry>Service Log Directory Size</entry>
                <entry>Service log directory consuming more disk than its quota.</entry>
                <entry>This could be due to a service set to <codeph>DEBUG</codeph> instead of
                    <codeph>INFO</codeph> level. Another reason could be due to a repeating error
                  message filling up the log files. Finally, it could be due to log rotate not
                  configured properly so old log files are not being deleted properly.</entry>
                <entry>Find the service that is consuming too much disk space. Look at the logs. If
                    <codeph>DEBUG</codeph> log entries exist, set the logging level to
                    <codeph>INFO</codeph>. If the logs are repeatedly logging an error message, do
                  what is needed to resolve the error. If old log files exist, configure log rotate
                  to remove them. You could also choose to remove old log files by hand after
                  backing them up if needed.</entry>
              </row>
            </tbody>
          </tgroup>
        </table>
      </p>
    </section>
  </body>
</topic>
