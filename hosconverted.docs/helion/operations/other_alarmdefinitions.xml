<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE topic PUBLIC "-//OASIS//DTD DITA Topic//EN" "topic.dtd">
<topic id="other_alarmdefinitions">
  <title><ph conkeyref="HOS-conrefs/product-title"/>Other Services Alarms</title>
  <body>
    <section>
      <p>These alarms show under the Other Services section of the HPE Helion Ops Console.</p>
      <p>
        <table frame="all" rowsep="1" colsep="1" id="otherservices_alarms">
          <tgroup cols="5">
            <colspec colname="c1" colnum="1"/>
            <colspec colname="c2" colnum="2"/>
            <colspec colname="c3" colnum="3"/>
            <colspec colname="c4" colnum="4"/>
            <colspec colname="c5" colnum="5"/>
            <thead>
              <row>
                <entry>Service</entry>
                <entry>Alarm Name</entry>
                <entry>Description</entry>
                <entry>Likely Cause</entry>
                <entry>Mitigation Tasks to Perform</entry>
              </row>
            </thead>
            <tbody>
              <row>
                <entry morerows="2">apache</entry>
                <entry>Apache Status</entry>
                <entry>Alarms on failure to reach the Apache status endpoint.</entry>
                <entry/>
                <entry/>
              </row>
              <row>
                <entry>Process Check</entry>
                <entry>Alarms when the specified process is not running.<p>
                    <codeblock>process_name = apache2</codeblock>
                  </p></entry>
                <entry/>
                <entry>If the Apache process goes down, connect to the affected node via SSH and
                  restart it with this command:
                  <codeblock>sudo systemctl restart apache2</codeblock>
                </entry>
              </row>
              <row>
                <entry>Apache Idle Worker Count</entry>
                <entry>Alarms when there are no idle workers in the Apache server.</entry>
                <entry/>
                <entry/>
              </row>
              <row>
                <entry morerows="2">backup</entry>
                <entry>Process Check</entry>
                <entry>Alarms when the specified process is not running. <p>
                    <codeblock>process_name = freezer-scheduler</codeblock>
                  </p></entry>
                <entry>Process crashed.</entry>
                <entry>Restart the process on the affected node. Review the associated logs.</entry>
              </row>
              <row>
                <entry>HTTP Status</entry>
                <entry>Alarms when the specified HTTP endpoint is down or not reachable. <p>
                    <codeblock>process_name = freezer-api</codeblock>
                  </p></entry>
                <entry/>
                <entry/>
              </row>
              <row>
                <entry>Service Log Directory Size</entry>
                <entry>Service log directory consuming more disk than its quota.</entry>
                <entry>The service log directory, as indicated by the <codeph>path</codeph>
                  dimension, is over the 2.5 GB quota.</entry>
                <entry>Find the service that is consuming too much disk space. Look at the logs. If
                    <codeph>DEBUG</codeph> log entries exist, set the logging level to
                    <codeph>INFO</codeph>. If the logs are repeatedly logging an error message, do
                  what is needed to resolve the error. If old log files exist, configure log rotate
                  to remove them. You could also choose to remove old log files by hand after
                  backing them up if needed.</entry>
              </row>
              <!-- EON -->
              <row>
                <entry>eon</entry>
                <entry>Service Log Directory Size</entry>
                <entry>Service log directory consuming more disk than its quota.</entry>
                <entry>The service log directory, as indicated by the <codeph>path</codeph>
                  dimension, is over the 2.5 GB quota.</entry>
                <entry>Find the service that is consuming too much disk space. Look at the logs. If
                    <codeph>DEBUG</codeph> log entries exist, set the logging level to
                    <codeph>INFO</codeph>. If the logs are repeatedly logging an error message, do
                  what is needed to resolve the error. If old log files exist, configure log rotate
                  to remove them. You could also choose to remove old log files by hand after
                  backing them up if needed.</entry>
              </row>
              <!-- HAPROXY -->
              <row>
                <entry>haproxy</entry>
                <entry>Process Check</entry>
                <entry>Alarms when the specified process is not running.
                  <codeblock>process_name = haproxy</codeblock></entry>
                <entry>HA Proxy is not running on this machine.</entry>
                <entry>Restart the process on the affected node using these steps: <ol
                    id="ol_krq_lzp_mx">
                    <li>Log in to the lifecycle manager.</li>
                    <li>Use this playbook against the affected node:
                      <codeblock>cd ~/scratch/ansible/next/hos/ansible
ansible-playbook -i hosts/verb_hosts FND-CLU-start.yml --limit &lt;hostname></codeblock></li>
                  </ol>
                  <p>Review the associated logs.</p></entry>
              </row>
              <!-- hlm-ux-services -->
              <row>
                <entry>hlm-ux-services</entry>
                <entry>HTTP Status</entry>
                <entry>Alarms when the specified HTTP endpoint is down or not reachable.</entry>
                <entry/>
                <entry/>
              </row>

              <!-- key-manager / barbican -->
              <row>
                <entry morerows="3">key-manager</entry>
                <entry>Process Check</entry>
                <entry>Alarms when the specified process is not running.
                  <codeblock>process_name = barbican-api</codeblock></entry>
                <entry>Process has crashed.</entry>
                <entry>Restart the process on the affected node using these steps: <ol
                    id="ol_mrq_lzp_mx">
                    <li>Log in to the lifecycle manager.</li>
                    <li>Use the Barbican start playbook against the affected node:
                      <codeblock>cd ~/scratch/ansible/next/hos/ansible
ansible-playbook -i hosts/verb_hosts barbican-start.yml --limit &lt;hostname></codeblock></li>
                  </ol></entry>
              </row>
              <row>
                <entry>HTTP Status</entry>
                <entry>Alarms when the specified HTTP endpoint is down or not
                  reachable.<codeblock>component = barbican-api
api_endpoint = public or internal</codeblock></entry>
                <entry>The endpoint is not responsive, it may be down.</entry>
                <entry>For the HTTP Status alarms for the public and internal endpoints, restart the
                  process on the affected node using these steps: <ol id="ol_nrq_lzp_mx">
                    <li>Log in to the lifecycle manager.</li>
                    <li>Stop the barbican service:
                      <codeblock>cd ~/scratch/ansible/next/hos/ansible
ansible-playbook -i hosts/verb_hosts barbican-stop.yml --limit &lt;hostname></codeblock></li>
                    <li>Restart the barbican service back up:
                      <codeblock>cd ~/scratch/ansible/next/hos/ansible
ansible-playbook -i hosts/verb_hosts barbican-start.yml --limit &lt;hostname></codeblock></li>
                  </ol>
                  <p>Examine the logs <codeph>/var/log/barbican/</codeph> for possible error
                    messages.</p></entry>
              </row>
              <row>
                <entry>HTTP Status</entry>
                <entry>Alarms when the specified HTTP endpoint is down or not reachable.
                  <codeblock>component = barbican-api
monitored_host_type = vip</codeblock></entry>
                <entry>The Barbican API on the admin virtual IP is down.</entry>
                <entry>This alarm is verifying access to the Barbican API via the virtual IP address
                  (HAProxy). If this check is failing but the other two HTTP Status alarms for the
                  key-manager service are not then the issue is likely with HAProxy so you should
                  view the alarms for that service. If the other two HTTP Status alarms are alerting
                  as well then restart Barbican using the steps listed.</entry>
              </row>
              <row>
                <entry>Service Log Directory Size</entry>
                <entry>Service log directory consuming more disk than its quota.</entry>
                <entry>The service log directory, as indicated by the <codeph>path</codeph>
                  dimension, is over the 2.5 GB quota.</entry>
                <entry>Find the service that is consuming too much disk space. Look at the logs. If
                    <codeph>DEBUG</codeph> log entries exist, set the logging level to
                    <codeph>INFO</codeph>. If the logs are repeatedly logging an error message, do
                  what is needed to resolve the error. If old log files exist, configure log rotate
                  to remove them. You could also choose to remove old log files by hand after
                  backing them up if needed.</entry>
              </row>
              <!-- MYSQL -->
              <row>
                <entry morerows="1">mysql</entry>
                <entry>MySQL Slow Query Rate</entry>
                <entry>Alarms when the slow query rate is high.</entry>
                <entry>The system load is too high.</entry>
                <entry>This could be an indication of near capacity limits or an exposed bad query.
                  First, check overall system load and then investigate MySQL details.</entry>
              </row>
              <row>
                <entry>Process Check</entry>
                <entry>Alarms when the specified process is not running.
                  <codeblock>process_name = mysqld</codeblock></entry>
                <entry>MySQL crashed.</entry>
                <entry>Restart MySQL on the affected node.</entry>
              </row>
              <!-- Octavia -->
              <row>
                <entry morerows="2">octavia</entry>
                <entry>Process Check</entry>
                <entry>Alarms when the specified process is not running. There are individual alarms
                  for each of these processes: <ul id="ul_orq_lzp_mx">
                    <li>octavia-worker</li>
                    <li>octavia-housekeeping</li>
                    <li>octavia-api</li>
                    <li>octavia-health-manager</li>
                  </ul></entry>
                <entry>The process has crashed.</entry>
                <entry>Restart the process on the affected node using these steps: <ol
                    id="ol_prq_lzp_mx">
                    <li>Log in to the lifecycle manager.</li>
                    <li>Use the Octavia start playbook against the affected node:
                      <codeblock>cd ~/scratch/ansible/next/hos/ansible
ansible-playbook -i hosts/verb_hosts octavia-start.yml --limit &lt;hostname></codeblock></li>
                  </ol></entry>
              </row>
              <row>
                <entry>HTTP Status</entry>
                <entry>Alarms when the specified HTTP endpoint is down or not reachable.</entry>
                <entry>The <codeph>octavia-api </codeph>process could be down or you could be
                  experiencing an issue with either haproxy or another network related
                  issue.</entry>
                <entry>If the <codeph>octavia-api</codeph> process is is down, restart it on the
                  affected node using these steps: <ol id="ol_qrq_lzp_mx">
                    <li>Log in to the lifecycle manager.</li>
                    <li>Use the Octavia start playbook against the affected node:
                      <codeblock>cd ~/scratch/ansible/next/hos/ansible
ansible-playbook -i hosts/verb_hosts octavia-start.yml --limit &lt;hostname></codeblock></li>
                  </ol><p>If it's not the <codeph>octavia-process</codeph> that is the issue, then
                    check if there is an issue with <codeph>haproxy</codeph> or possibly a network
                    issue and troubleshoot accordingly.</p></entry>
              </row>
              <row>
                <entry>Service Log Directory Size</entry>
                <entry>Service log directory consuming more disk than its quota.</entry>
                <entry>The service log directory, as indicated by the <codeph>path</codeph>
                  dimension, is over the 2.5 GB quota.</entry>
                <entry>Find the service that is consuming too much disk space. Look at the logs. If
                    <codeph>DEBUG</codeph> log entries exist, set the logging level to
                    <codeph>INFO</codeph>. If the logs are repeatedly logging an error message, do
                  what is needed to resolve the error. If old log files exist, configure log rotate
                  to remove them. You could also choose to remove old log files by hand after
                  backing them up if needed.</entry>
              </row>
              <!-- orchestration / heat -->
              <row>
                <entry morerows="2">orchestration</entry>
                <entry>Process Check</entry>
                <entry>Alarms when the specified process is not running. There are individual alarms
                  for each of these processes:<ul id="ul_kvs_yjq_rv">
                    <li>heat-api</li>
                    <li>heat-api-cfn</li>
                    <li>heat-api-cloudwatch</li>
                    <li>heat-engine</li>
                  </ul>heat-api process check on each node</entry>
                <entry>Process crashed.</entry>
                <entry>Restart the process with these steps: <ol id="ol_rrq_lzp_mx">
                    <li>Log in to the lifecycle manager.</li>
                    <li>Stop all the Heat processes:
                      <codeblock>cd ~/scratch/ansible/next/hos/ansible
ansible-playbook -i hosts/verb_hosts heat-start.yml</codeblock></li>
                    <li>Start the Heat processes back up:
                      <codeblock>cd ~/scratch/ansible/next/hos/ansible
ansible-playbook -i hosts/verb_hosts heat-start.yml</codeblock></li>
                  </ol>
                  <p>Review the relevant log at the following locations on the affected node:</p>
                  <codeblock>/var/log/heat/heat-api.log
/var/log/heat/heat-cfn.log
/var/log/heat/heat-cloudwatch.log
/var/log/heat/heat-engine.log</codeblock></entry>
              </row>
              <row>
                <entry>HTTP Status</entry>
                <entry>Alarms when the specified HTTP endpoint is down or not reachable.<p>
                    <ul id="ul_ztx_1kq_rv">
                      <li>heat-api</li>
                      <li>heat-api-cfn</li>
                      <li>heat-api-cloudwatch</li>
                    </ul>
                  </p></entry>
                <entry/>
                <entry>Restart the Heat service with these steps: <ol id="ol_srq_lzp_mx">
                    <li>Log in to the lifecycle manager.</li>
                    <li>Stop all the Heat processes:
                      <codeblock>cd ~/scratch/ansible/next/hos/ansible
ansible-playbook -i hosts/verb_hosts heat-start.yml</codeblock></li>
                    <li>Start the Heat processes back up:
                      <codeblock>cd ~/scratch/ansible/next/hos/ansible
ansible-playbook -i hosts/verb_hosts heat-start.yml</codeblock></li>
                  </ol>
                  <p>Review the relevant log at the following locations on the affected node:</p>
                  <codeblock>/var/log/heat/heat-api.log
/var/log/heat/heat-cfn.log
/var/log/heat/heat-cloudwatch.log</codeblock></entry>
              </row>
              <row>
                <entry>Service Log Directory Size</entry>
                <entry>Service log directory consuming more disk than its quota.</entry>
                <entry>The service log directory, as indicated by the <codeph>path</codeph>
                  dimension, is over the 2.5 GB quota.</entry>
                <entry>Find the service that is consuming too much disk space. Look at the logs. If
                    <codeph>DEBUG</codeph> log entries exist, set the logging level to
                    <codeph>INFO</codeph>. If the logs are repeatedly logging an error message, do
                  what is needed to resolve the error. If old log files exist, configure log rotate
                  to remove them. You could also choose to remove old log files by hand after
                  backing them up if needed.</entry>
              </row>
              <!-- OVSvApp -->
              <row>
                <entry>OVSvApp-ServiceVM</entry>
                <entry>Process Check</entry>
                <entry>Alarms when the specified process is not running.<p>
                  <codeblock>process_name = ovs-vswitchd
process_name = neutron-ovsvapp-agent
process_name = ovsdb-server</codeblock>
                </p></entry>
                <entry>Process has crashed.</entry>
                <entry>Restart process on affected node. Review logs.</entry>
              </row>
              <!-- RabbitMQ -->
              <row>
                <entry>rabbitmq</entry>
                <entry>Process Check</entry>
                <entry>Alarms when the specified process is not running.<p>
                    <codeblock>process_name = rabbitmq
process_name = epmd</codeblock>
                  </p></entry>
                <entry>Process has crashed.</entry>
                <entry>Restart process on affected node. Review logs.</entry>
              </row>
              <row>
                <entry>spark</entry>
                <entry>Process Check</entry>
                <entry>Alarms when the specified process is not running.<p>
                    <codeblock>process_name = org.apache.spark.deploy.master.Master
process_name = org.apache.spark.deploy.worker.Worker</codeblock>
                  </p></entry>
                <entry>Process has crashed.</entry>
                <entry>Restart process on affected node. Review logs.</entry>
              </row>
              <!-- web-ui / HORIZON -->
              <row>
                <entry morerows="1">web-ui</entry>
                <entry>HTTP Status</entry>
                <entry>Alarms when the specified HTTP endpoint is down or not reachable.</entry>
                <entry>Apache is not running or there is a misconfiguration.</entry>
                <entry>Check that Apache is running; investigate Horizon logs.</entry>
              </row>
              <row>
                <entry>Service Log Directory Size</entry>
                <entry>Service log directory consuming more disk than its quota.</entry>
                <entry>The service log directory, as indicated by the <codeph>path</codeph>
                  dimension, is over the 2.5 GB quota.</entry>
                <entry>Find the service that is consuming too much disk space. Look at the logs. If
                    <codeph>DEBUG</codeph> log entries exist, set the logging level to
                    <codeph>INFO</codeph>. If the logs are repeatedly logging an error message, do
                  what is needed to resolve the error. If old log files exist, configure log rotate
                  to remove them. You could also choose to remove old log files by hand after
                  backing them up if needed.</entry>
              </row>
              <!-- zookeeper -->
              <row>
                <entry morerows="1">zookeeper</entry>
                <entry>Process Check</entry>
                <entry>Alarms when the specified process is not running.<p>
                    <codeblock>process_name = org.apache.zookeeper.server</codeblock>
                  </p></entry>
                <entry>Process crashed.</entry>
                <entry>Restart the process on the affected node. Review the associated logs.</entry>
              </row>
              <row>
                <entry>ZooKeeper Latency</entry>
                <entry>Alarms when the ZooKeeper latency is high.</entry>
                <entry>Heavy system load.</entry>
                <entry>Check the individual system as well as activity across the entire
                  service.</entry>
              </row>
            </tbody>
          </tgroup>
        </table>
      </p>
    </section>
  </body>
</topic>
