<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE topic PUBLIC "-//OASIS//DTD DITA Topic//EN" "topic.dtd">
<topic id="system_alarmdefinitions">
  <title><ph conkeyref="HOS-conrefs/product-title"/>System Alarms</title>
  <body>
    <section>
      <p>These alarms show under the System section and are setup per <codeph>hostname</codeph>
        and/or <codeph>mount_point</codeph>.</p>
      <table frame="all" rowsep="1" colsep="1" id="system_alarms">
        <tgroup cols="5">
          <colspec colname="c1" colnum="1"/>
          <colspec colname="c2" colnum="2"/>
          <colspec colname="c3" colnum="3"/>
          <colspec colname="c4" colnum="4"/>
          <colspec colname="c5" colnum="5"/>
          <thead>
            <row>
              <entry>Service</entry>
              <entry>Alarm Name</entry>
              <entry>Description</entry>
              <entry>Likely Cause</entry>
              <entry>Mitigation Tasks to Perform</entry>
            </row>
          </thead>
          <tbody>
            <row>
              <entry morerows="11">system</entry>
              <entry>CPU Usage</entry>
              <entry>Alarms on high CPU usage.</entry>
              <entry>Heavy load or runaway processes.</entry>
              <entry>Log onto the reporting host and diagnose the heavy CPU usage.</entry>
            </row>
            <row>
              <entry>Elasticsearch Low Watermark</entry>
              <entry><p>
                  <codeblock>component = elasticsearch</codeblock>
                </p>Elasticsearch Disk LOW Watermark. Backup indices. If high watermark is reached,
                indices will be deleted. Adjust curator_low_watermark_percent,
                curator_high_watermark_percent, and elasticsearch_max_total_indices_size_in_bytes if
                needed.</entry>
              <entry>Running out of disk space for <codeph>/var/lib/elasticsearch</codeph>.</entry>
              <entry>Free up space by removing indices (backing them up first if desired).
                Alternatively, adjust <codeph>curator_low_watermark_percent</codeph>,
                  <codeph>curator_high_watermark_percent</codeph>, and/or
                  <codeph>elasticsearch_max_total_indices_size_in_bytes</codeph> if needed, <p>For
                  more information about how to back up your centralized logs, see <xref
                    href="../operations/central_log_configure_CL.dita#central_log_configure_settings/CL_BU_Elasticsearch"
                  />.</p></entry>
            </row>
            <row>
              <entry>Elasticsearch High Watermark</entry>
              <entry><p>
                  <codeblock>component = elasticsearch</codeblock>
                </p>Elasticsearch Disk HIGH Watermark. Attempting to delete indices to free disk
                space. Adjust <codeph>curator_low_watermark_percent</codeph>,
                  <codeph>curator_high_watermark_percent</codeph>, and
                  <codeph>elasticsearch_max_total_indices_size_in_bytes</codeph> if needed.</entry>
              <entry>Running out of disk space for <codeph>/var/lib/elasticsearch</codeph>.</entry>
              <entry>Verify that disk space was freed up by the curator. If needed, free up
                additional space by removing indices (backing them up first if desired).
                Alternatively, adjust curator_low_watermark_percent, curator_high_watermark_percent,
                and/or elasticsearch_max_total_indices_size_in_bytes if needed. <p>For more
                  information about how to back up your centralized logs, see <xref
                    href="../operations/central_log_configure_CL.dita#central_log_configure_settings/CL_BU_Elasticsearch"
                  />.</p></entry>
            </row>
            <row>
              <entry>Log Partition Low Watermark</entry>
              <entry>The <codeph>/var/log</codeph> disk space usage has crossed the low watermark.
                If the high watermark is reached, <codeph>logrotate</codeph> will be run to free up
                disk space. Adjust <codeph>var_log_low_watermark_percent</codeph> if needed.</entry>
              <entry>This could be due to a service set to <codeph>DEBUG</codeph> instead of
                  <codeph>INFO</codeph> level. Another reason could be due to a repeating error
                message filling up the log files. Finally, it could be due to log rotate not
                configured properly so old log files are not being deleted properly.</entry>
              <entry>Find the service that is consuming too much disk space. Look at the logs. If
                  <codeph>DEBUG</codeph> log entries exist, set the logging level to
                  <codeph>INFO</codeph>. If the logs are repeatedly logging an error message, do
                what is needed to resolve the error. If old log files exist, configure log rotate to
                remove them. You could also choose to remove old log files by hand after backing
                them up if needed.</entry>
            </row>
            <row>
              <entry>Log Partition High Watermark</entry>
              <entry>The <codeph>/var/log</codeph> volume is running low on disk space. Logrotate
                will be run now to free up space. Adjust
                  <codeph>var_log_high_watermark_percent</codeph> if needed.</entry>
              <entry>This could be due to a service set to <codeph>DEBUG</codeph> instead of
                  <codeph>INFO</codeph> level. Another reason could be due to a repeating error
                message filling up the log files. Finally, it could be due to log rotate not
                configured properly so old log files are not being deleted properly.</entry>
              <entry>Find the service that is consuming too much disk space. Look at the logs. If
                  <codeph>DEBUG</codeph> log entries exist, set the logging level to
                  <codeph>INFO</codeph>. If the logs are repeatedly logging an error message, do
                what is needed to resolve the error. If old log files exist, configure log rotate to
                remove them. You could also choose to remove old log files by hand after backing
                them up if needed.</entry>
            </row>
            <row>
              <entry>Crash Dump Count</entry>
              <entry>Alarms if it receives any metrics with <codeph>crash.dump_count</codeph> >
                0</entry>
              <entry>When a crash dump is generated by kdump, the crash dump file is put into the
                  <codeph>/var/crash</codeph> directory by default. Any crash dump files in this
                directory will cause the <codeph>crash.dump_count</codeph> metric to show a value
                greater than 0.</entry>
              <entry>Analyze the crash dump file(s) located in <codeph>/var/crash</codeph> on the
                host that generated the alarm to try to determine if a service or hardware caused
                the crash.<p>Move the file to a new location so that a developer can take a look at
                  it. Make sure all of the processes are back up after the crash (run the
                    <codeph>&lt;service>-status.yml</codeph> playbooks). When the
                    <codeph>/var/crash</codeph> directory is empty the Crash Dump Count alarm should
                  transition back to OK.</p></entry>
            </row>
            <row>
              <entry>Disk Inode Usage</entry>
              <entry>Nearly out of inodes for a partition, as indicated by the
                  <codeph>mount_point</codeph> reported.</entry>
              <entry>Many files on the disk.</entry>
              <entry>Investigate cleanup of data or migration to other
                partitions.<!-- DOCS-2942 --></entry>
            </row>
            <row>
              <entry>Disk Usage</entry>
              <entry>High disk usage, as indicated by the <codeph>mount_point</codeph>
                reported.</entry>
              <entry>Large files on the disk.</entry>
              <entry>Investigate cleanup of data or migration to other partitions. <p>If the
                    <codeph>mount_point</codeph> is <codeph>/var/vertica</codeph> then see <xref
                    href="troubleshooting/recover_vertica.dita"/> for details on how to
                resolve.</p></entry>
            </row>
            <row>
              <entry>Host Status</entry>
              <entry>
                <p>Alerts when a host is unreachable.</p>
                <p>
                  <codeblock>test_type = ping</codeblock>
                </p>
              </entry>
              <entry>Host or network is down.</entry>
              <entry>If a single host, attempt to restart the system. If multiple hosts, investigate
                network issues.</entry>
            </row>
            <row>
              <entry>Memory Usage</entry>
              <entry>High memory usage.</entry>
              <entry>Overloaded system or services with memory leaks.</entry>
              <entry>Log onto the reporting host to investigate high memory users.</entry>
            </row>
            <row>
              <entry>Network Errors</entry>
              <entry>Alarms on a high network error rate.</entry>
              <entry>Bad network or cabling.</entry>
              <entry>Take this host out of service until the network can be fixed.</entry>
            </row>
            <row>
              <entry>NTP Time Sync</entry>
              <entry>Alarms when the NTP time offset is high.</entry>
              <entry/>
              <entry>Log in to the reported host and check if the ntp service is running. <p>If it
                  is running, then use these steps:</p>
                <ol id="ol_hdn_kzp_mx">
                  <li>Stop the service. <p>On a Linux for HPE Helion host:</p>
                    <codeblock>service ntp stop</codeblock>
                    <p>On a RHEL host:</p>
                    <codeblock>service ntpd stop</codeblock></li>
                  <li>Resync the nodes time:
                    <codeblock>/usr/sbin/ntpdate -b  &lt;ntp-server></codeblock></li>
                  <li>Restart the ntp service back up. <p>On a Linux for HPE Helion host:</p>
                    <codeblock>service ntp start</codeblock>
                    <p>On a RHEL host:</p>
                    <codeblock>service ntpd start</codeblock></li>
                  <li>Restart rsyslog: <codeblock>service rsyslog restart</codeblock></li>
                </ol>
              </entry>
            </row>
          </tbody>
        </tgroup>
      </table>
    </section>
  </body>
</topic>
