<?xml version="1.0"?>
<!DOCTYPE section PUBLIC "-//OASIS//DTD DocBook XML V4.5//EN" "http://www.oasis-open.org/docbook/xml/4.5/docbookx.dtd" [ <!ENTITY % entities SYSTEM "entities.xml"> %entities; ]><section id="hugepages">
   <title>
      <phrase/>Configuring Hugepages for DPDK in Neutron
    Networks</title>
    
      <bridgehead renderas="sect4">Configuring hugepages to enable DPDK</bridgehead>
      <para>In order to take advantge of <xref linkend="dpdk"/> and its network
        performance enhancements, new in <phrase/>, you must first enable
        hugepages.</para>
      <para>With hugepages, physical RAM is reserved at boot time and dedicated to a virtual machine.
        Only that virtual machine and Open vSwitch can use this specifically allocated RAM. The host
        OS cannot access it. This memory is contiguous, and because of its larger size, reduces the
        number of entries in the memory map and number of times it must be read.</para>
      <para>The hugepage reservation is made in <literal>/etc/default/grub</literal>, but this is handled
        by the lifecycle manager.</para>
      <para>In addition to hugepages, to use DPDK, CPU isolation is required. This is achieved with the
        'isolcups' command in <literal>/etc/default/grub</literal>, but this is also managed by the
        lifecycle manager using a new input model file.</para>
      <para>The two new input model files introduced with this release to help you configure the
        necessary settings and persist them are:</para>
      <itemizedlist id="ul_er1_pkg_qw">
        <listitem>
            <para>memory_models.yml (for hugepages)</para>
         </listitem>
        <listitem>
            <para>cpu_models.yml (for CPU isolation)</para>
         </listitem>
      </itemizedlist>
   
    
      <bridgehead renderas="sect4">memory_models.yml</bridgehead>
      <para>In this file you set your huge page size along with the number of such huge-page
        allocations.</para>
      <screen> ---
  product:
    version: 2

  memory-models:
    - name: COMPUTE-MEMORY-NUMA
      default-huge-page-size: 1G
      huge-pages:
        - size: 1G
          count: 24
          numa-node: 0
        - size: 1G
          count: 24
          numa-node: 1
        - size: 1G
          count: 48</screen>
   
    
      <bridgehead renderas="sect4">cpu_models.yml</bridgehead>
      <screen>---
  product:
    version: 2

  cpu-models:

    - name: COMPUTE-CPU
      assignments:
       - components:
           - nova-compute-kvm
         cpu:
           - processor-ids: 3-5,12-17
             role: vm

       - components:
           - openvswitch
         cpu:
           - processor-ids: 0
             role: eal
           - processor-ids: 1-2
             role: pmd</screen>
   

    
      <bridgehead renderas="sect4">NUMA memory allocation</bridgehead>
      <para>As mentioned above, the memory used for hugepages is locked down at boot time by an entry
        in <literal>/etc/default/grub</literal>. As an admin, you can specify in the input model how
        to arrange this memory on NUMA nodes. It can be spread across NUMA nodes or you can specify
        where you want it. For example, if you have only one NIC, you would probably want all the
        hugepages memory to be on the NUMA node closest to that NIC.
        <!-- Input model looks something
      like:             product:       version: 2              memory-models:              - name:
      COMPUTE-MEMORY-NUMA       default-huge-page-size: 1G       huge-pages:       - size: 1G
      count: 24       numa-node: 0       - size: 1G       count: 24       numa-node: 1       - size:
      1G       count: 48--></para>
      <para>If you donâ€™t specify the <literal>numa-node</literal> settings in the
          <literal>memory_models.yml</literal> input model file and use only the last entry indicating
        "size: 1G" and "count: 48" then this memory is spread evenly across all NUMA nodes.</para>
      <para>Also note that the hugepage service runs once at boot time and then goes to an inactive
        state so you should not expect to see it running. If you decide to make changes to the NUMA
        memory allocation, you will need to reboot the compute node for the changes to take
        effect.</para>
   
  </section>
