<?xml version="1.0"?>
<!DOCTYPE section PUBLIC "-//OASIS//DTD DocBook XML V4.5//EN" "http://www.oasis-open.org/docbook/xml/4.5/docbookx.dtd" [ <!ENTITY % entities SYSTEM "entities.xml"> %entities; ]><!--Edit status: not edited-->
<section id="replacing_osd_journal_disks">
   <title>
      <phrase/>Replacing the Journal Disk in a Ceph OSD
    Node</title>
    <para>
      <phrase/> supports a Ceph environment with the following OSD node
        configurations:</para>
   <itemizedlist>
        <listitem>
         <para>OSD nodes with no journal disk</para>
      </listitem>
        <listitem>
         <para>OSD nodes with a dedicated journal disk</para>
      </listitem>
        <listitem>
         <para>OSD nodes with a shared journal disk (partition)</para>
      </listitem>
      </itemizedlist>
   <para>For the first scenario where the journal information is stored on a partition on the OSD
        data disk, the steps to replace the data disk should be followed. For more information, see
          <xref linkend="replacing_os_disks"/>.</para>
   <para>For the other two scenarios where there is a separate drive used for storing journal
        information you can use the steps below to replace the journal disk on a Ceph OSD node.</para>
    
      <bridgehead renderas="sect4">Steps for replacing a journal disk on a Ceph OSD node</bridgehead>
      <orderedlist>
        <listitem>
            <para>SSH to the Ceph OSD node with the journal disk you want to replace.</para>
         </listitem>
        <listitem>
            <para>Use the command below to determine the details for the affected OSD nodes: </para>
            <screen>sudo ceph-disk list | grep &lt;disk&gt; | grep osd</screen>
            <para>Here is an example output:</para>
            <screen>$ sudo ceph-disk list | grep /dev/sdg | grep osd
 /dev/sdc1 ceph data, active, cluster ceph, osd.8, journal /dev/sdg2
 /dev/sde1 ceph data, active, cluster ceph, osd.5, journal /dev/sdg3
 /dev/sdf1 ceph data, active, cluster ceph, osd.0, journal /dev/sdg1</screen>
            <para>In this example, if the shared journal disk <literal>/dev/sdg</literal> is to be replaced
            then 0, 5, and 8 are the affected OSDs.</para>
         </listitem>
        <listitem>
            <para>Use the following command to avoid the cluster rebalancing itself as the journal disk is
          being replaced: </para>
            <screen>ceph osd set noout --cluster &lt;ceph-cluster&gt;</screen>
         </listitem>
        <listitem>
            <para>Stop the affected OSDs and flush their journal with these commands: </para>
            <screen>sudo systemctl stop ceph-osd@&lt;osd-number&gt;
sudo ceph-osd -i &lt;osd-number&gt; --flush-journal --cluster &lt;ceph-cluster&gt;</screen>
            <para>Here is an example output, using the same information from the earlier example:</para>
            <screen>$ sudo systemctl stop ceph-osd@0
$ sudo ceph-osd -i 0 --flush-journal --cluster ceph
$ sudo systemctl stop ceph-osd@5
$ sudo ceph-osd -i 5 --flush-journal --cluster ceph
$ sudo systemctl stop ceph-osd@8
$ sudo ceph-osd -i 8 --flush-journal --cluster ceph</screen>
         </listitem>
        <listitem>
            <para>Replace the faulty journal disk with a new physical disk. Verify that new disk gets the
          same device name (for example, <literal>/dev/sdg</literal> where the failed drive was also
            <literal>/dev/sdg</literal>) and then proceed with the next steps.</para>
         </listitem>
        <listitem>
            <para>Use the following command which generates a script file, which will be used later when
          creating the required journal partitions:
          </para>
            <screen>printf '#!/bin/bash
# script to create a journal partition for Ceph OSD.

# DO NOT CHANGE it is the journal disk identifier expected by ceph-disk utility
ptype=45b0969e-9b03-4f30-b4c6-b4b80ceff106
# name of the Ceph cluster
ceph_cluster=$1
# disk to be used
new_journal=$2
# journal partition size in MB
partition_size_mb=$3
# the Ceph OSD ID
osd_id=$4
# partition number to be created on the journal disk
part_num=$5

if [ $# -ne 5 ]; then
    echo "Error: Expected input parameters not received!"
    echo "Usage: /bin/bash create_journal_partition.sh ceph_cluster journal_disk partition_size_mb osd_id partition_number"
    exit 1
fi

journal_uuid=$(sudo cat /var/lib/ceph/osd/ceph-$osd_id/journal_uuid)
part_details=$part_num:0:+$partition_size_mb
part_details+=M
sudo sgdisk --new=$part_details --change-name=$part_num:"ceph journal" --partition-guid=$part_num:$journal_uuid --typecode=$part_num:$ptype --mbrtogpt -- $new_journal
sudo partprobe
sudo ceph-osd --cluster $ceph_cluster --mkjournal -i $osd_id' &gt;&gt; create_journal_partition.sh</screen>
         </listitem>
        <listitem>
            <para>For each OSD node that requires a journal partition on the new disk, execute the
            <literal>create_journal_partition.sh</literal> script with the appropriate inputs as
          below: </para>
            <screen>/bin/bash create_journal_partition.sh &lt;ceph_cluster&gt; &lt;journal_disk&gt; &lt;partition_size_mb&gt; &lt;osd_id&gt; &lt;partition_number&gt;</screen>
            <para>Here are some example outputs, using the same partitions as our earlier examples:</para>
            <screen># for OSD 0
$ /bin/bash create_journal_partition.sh ceph /dev/sdk 5120 0 1

# for OSD 5
$ /bin/bash create_journal_partition.sh ceph /dev/sdk 5120 5 2

# for OSD 8
$ /bin/bash create_journal_partition.sh ceph /dev/sdk 5120 8 3</screen>
         </listitem>
        <listitem>
            <para>Bring the OSDs back online with this command: </para>
            <screen>sudo systemctl start ceph-osd@&lt;osd_id&gt;</screen>
            <para>Here is an example output:</para>
            <screen>$ sudo systemctl start ceph-osd@0
$ sudo systemctl start ceph-osd@5
$ sudo systemctl start ceph-osd@8</screen>
         </listitem>
        <listitem>
            <para>Unset the cluster from noout:
          </para>
            <screen>ceph osd unset noout --cluster &lt;ceph-cluster&gt;</screen>
         </listitem>
        <listitem>
            <para>Use the following command and verify that the status of the affected OSDs are now
          appearing as <literal>up 1</literal>:
          </para>
            <screen>ceph osd tree --cluster &lt;cluster-name&gt;</screen>
         </listitem>
      </orderedlist>
   
  </section>
