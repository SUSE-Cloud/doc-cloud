<?xml version="1.0" encoding="UTF-8"?>
<!--Edit status: not edited-->
<!DOCTYPE topic PUBLIC "-//OASIS//DTD DITA Topic//EN" "topic.dtd">
<topic id="topic_ah5_4yx_qt">
    <title><ph conkeyref="HOS-conrefs/product-title"/>Deploying Ceph Monitor Services on Dedicated
        Resource Nodes</title>
    <body>
        <!--not tested-->
        <p conkeyref="HOS-conrefs/applies-to"/>
        <p>In the <keyword keyref="kw-hos-phrase"/> example configurations, the Ceph monitor service
            is installed on the controller nodes by default. If you wish to break these out into
            their own cluster then you can do so by modifying the input model to form a separate
                cluster.<note type="attention">If you want to deploy the monitor service as a
                dedicated resource node, then you must decide prior to the deployment of Ceph.
                    <keyword keyref="kw-hos-phrase"/> does not support deployment transition. Once
                Ceph is deployed, you cannot migrate the monitor service from controller to
                dedicated resource nodes. </note></p>
        <section>
            <title>Prerequisite</title>
            <p>The lifecycle manager must be set up before starting Ceph deployment. For more
                details on the installation of the lifecycle manager, see <xref
                    href="../../../installation/installing_kvm.dita"/>.</p>
        </section>
        <section id="deploy"><title>To Install the Ceph Monitor Service on Dedicated Resource
                Nodes</title>
            <p>Perform the following procedure to install the Ceph monitor on dedicated nodes. Note
                that Ceph requires at least 3 monitoring servers to form a cluster in case of node
                failure.</p>
            <ol>
                <li>Log in to the lifecycle manager.</li>
                <li>You will use the <codeph>entry-scale-kvm-ceph</codeph> example configuration as
                    the base for these steps. Copy the example configuration files into the required
                    setup directory before beginning the edit process:
                    <codeblock>cp -r ~/helion/examples/entry-scale-kvm-ceph/* ~/helion/my_cloud/definition/</codeblock></li>
                <li>Make the following edits to the
                        <codeph>~/helion/my_cloud/definition/data/control_plane.yml</codeph> file: <ol>
                        <li>Remove the reference to <codeph>- ceph-monitor</codeph> under the
                                <codeph>service-components</codeph> section for your control plane
                            cluster.</li>
                        <li>Add the details for your Ceph monitoring cluster. It is shown as the
              bolded portion in the example below, we added the rest to show the proper positioning: <codeblock>clusters:
  - name: cluster1
    cluster-prefix: c1
    server-role: CONTROLLER-ROLE
    member-count: 3
    allocation-policy: strict
    service-components:
      - lifecycle-manager
      - ntp-server
      ...
                                    
  <b>- name: ceph-mon
    cluster-prefix: ceph-mon
    server-role: CEP-MON-ROLE
    min-count: 3
    allocation-policy: strict
    service-components:
      - ntp-client
      - ceph-monitor</b>
    
  - name: rgw
    cluster-prefix: rgw
    server-role: RGW-ROLE
    ...</codeblock>
              <note>The indentation in the file is important to review the file to ensure it matches
                before continuing on.</note></li>
                    </ol>
                </li>
                <li>Edit the <codeph>~/helion/my_cloud/definition/data/servers.yml</codeph> file to
          define all of the Ceph monitor nodes in the cluster. Here is an example, you will want to
          edit the values to match your
          environment:<codeblock># Ceph Monitor Nodes
- id: ceph-mon1
  ip-addr: 10.13.111.141
  server-group: RACK1
  role: CEP-MON-ROLE
  nic-mapping: MY-4PORT-SERVER
  mac-addr: "f0:92:1c:05:69:10"
  ilo-ip: 10.12.8.217
  ilo-password: password
  ilo-user: admin

- id: ceph-mon2
  ip-addr: 10.13.111.142
  server-group: RACK2
  role: CEP-MON-ROLE
  nic-mapping: MY-4PORT-SERVER
  mac-addr: "83:92:1c:55:69:b0"
  ilo-ip: 10.12.8.218
  ilo-password: password
  ilo-user: admin

- id: ceph-mon3
  ip-addr: 10.13.111.143
  server-group: RACK3
  role: CEP-MON-ROLE
  nic-mapping: MY-4PORT-SERVER
  mac-addr: "d9:92:1c:25:69:e0"
  ilo-ip: 10.12.8.219
  ilo-password: password
  ilo-user: admin

# Ceph RGW Nodes
- id: rgw1
  ...</codeblock></li>
                <li>Edit the <codeph>~/helion/my_cloud/definition/data/net_interfaces.yml</codeph>
          file to define a new network interface set for your Ceph monitors. You can copy the
            <codeph>RGW-INTERFACES</codeph> model as a base and then edit it to match your
          environment: <p><b>Three-network Ceph example:</b></p>
          <codeblock>interface-models:
      # Edit the device names and bond options
      # to match your environment
      #
    - name: CONTROLLER-INTERFACES
      network-interfaces:
    ## This bonded interface is used by the controller 
    ## nodes for cloud management traffic.
        - name: BOND0
          device:
             name: bond0
          bond-data:
             options:
                mode: active-backup
                miimon: 200
                primary: hed1
             provider: linux
             devices:
                - name: hed1
                - name: hed2
            network-groups:
            - EXTERNAL-API
            - EXTERNAL-VM
            - GUEST
            - MANAGEMENT
    ## This interface is used to connect the controller
    ## node to the Ceph nodes so that any Ceph client
    ## like cinder-volume can route data directly to
    ## Ceph over thisinterface.
        - name: HETH3
          device:
            name: hed3
          forced-network-groups:
            - OSD-CLIENT

    - name: COMPUTE-INTERFACES
      network-interfaces:
        - name: HETH3
          device:
              name: hed3
          network-groups:
            - EXTERNAL-VM
            - GUEST
            - MANAGEMENT
      ## This interface is used to connect the compute node
      ## to the Ceph cluster so that a workload VM can route
      ## data traffic to the Ceph cluster over thisinterface.
        - name: HETH4
          device:
              name: hed4
          forced-network-groups:
            - OSD-CLIENT

    - name: CEP-MON-INTERFACES
      network-interfaces:
      ## This defines the interface used for management
      ## traffic like logging, monitoring, etc.
        - name: BOND0
          device:
              name: bond0
          bond-data:
              options:
                  mode: active-backup
                  miimon: 200
                  primary: hed1
              provider: linux
              devices:
                - name: hed1
                - name: hed2
          network-groups:
            - MANAGEMENT
      ## This interface is used to connect the client
      ## node to the Ceph nodes so that any Ceph client
      ## like cinder-volume can route data directly to
      ## Ceph over thisinterface.
        - name: HETH3
          device:
              name: hed3
          forced-network-groups:
            - OSD-CLIENT

    - name: OSD-INTERFACES
      network-interfaces:
      ## This defines the interface used for management
      ## traffic like logging, monitoring, etc.
        - name: BOND0
          device:
              name: bond0
          bond-data:
              options:
                  mode: active-backup
                  miimon: 200
                  primary: hed1
              provider: linux
              devices:
                - name: hed1
                - name: hed2
          network-groups:
            - MANAGEMENT
      ## This defines the interface used for client
      ## or data traffic.
        - name: HETH3
          device:
              name: hed3
          network-groups:
            - OSD-CLIENT
      ## This defines the interface used for internal
      ## cluster communication among OSD nodes.
        - name: HETH4
          device:
              name: hed4
          network-groups:
            - OSD-INTERNAL</codeblock>
          <p><b>Two-network Ceph example:</b></p>
          <codeblock>interface-models:
      # Edit the device names and bond options
      # to match your environment
      #
    - name: CONTROLLER-INTERFACES
      network-interfaces:
    ## This bonded interface is used by the controller 
    ## nodes for cloud management traffic.
    ## The same interface is also used to connect the client
    ## node to the Ceph nodes so that any Ceph client
    ## like cinder-volume can route data directly to
    ## Ceph over thisinterface.
        - name: BOND0
          device:
             name: bond0
          bond-data:
             options:
                mode: active-backup
                miimon: 200
                primary: hed1
             provider: linux
             devices:
                - name: hed1
                - name: hed2
            network-groups:
            - EXTERNAL-API
            - EXTERNAL-VM
            - GUEST
            - MANAGEMENT

    - name: COMPUTE-INTERFACES
      network-interfaces:
      ## The same interface is also used to connect the compute node
      ## to the Ceph cluster so that a workload VM can route
      ## data traffic to the Ceph cluster over thisinterface.
        - name: HETH3
          device:
              name: hed3
          network-groups:
            - EXTERNAL-VM
            - GUEST
            - MANAGEMENT

    - name: CEP-MON-INTERFACES
      network-interfaces:
      ## This defines the interface used for management
      ## traffic like logging, monitoring, etc.
      ## The same interface is also used to connect the client
      ## node to the Ceph nodes so that any Ceph client
      ## like cinder-volume can route data directly to
      ## Ceph over thisinterface.
        - name: BOND0
          device:
              name: bond0
          bond-data:
              options:
                  mode: active-backup
                  miimon: 200
                  primary: hed1
              provider: linux
              devices:
                - name: hed1
                - name: hed2
          network-groups:
            - MANAGEMENT

    - name: OSD-INTERFACES
      network-interfaces:
      ## This defines the interface used for management
      ## traffic like logging, monitoring, etc.
      ## The same interface is also used for client
      ## or data traffic.
        - name: BOND0
          device:
              name: bond0
          bond-data:
              options:
                  mode: active-backup
                  miimon: 200
                  primary: hed1
              provider: linux
              devices:
                - name: hed1
                - name: hed2
          network-groups:
            - MANAGEMENT
      ## This defines the interface used for internal
      ## cluster communication among OSD nodes.
        - name: HETH4
          device:
              name: hed4
          network-groups:
            - OSD-INTERNAL</codeblock>
          <p><b>Single-network Ceph example:</b></p>
          <codeblock>interface-models:
      # Edit the device names and bond options
      # to match your environment
      #
    - name: CONTROLLER-INTERFACES
      network-interfaces:
    ## This bonded interface is used by the controller 
    ## nodes for cloud management traffic.
    ## The same interface is also used to connect the client
    ## node to the Ceph nodes so that any Ceph client
    ## like cinder-volume can route data directly to
    ## Ceph over thisinterface.
        - name: BOND0
          device:
             name: bond0
          bond-data:
             options:
                mode: active-backup
                miimon: 200
                primary: hed1
             provider: linux
             devices:
                - name: hed1
                - name: hed2
            network-groups:
            - EXTERNAL-API
            - EXTERNAL-VM
            - GUEST
            - MANAGEMENT

    - name: COMPUTE-INTERFACES
    ## This interface is also used to connect the compute node
    ## to the Ceph cluster so that a workload VM can route
    ## data traffic to the Ceph cluster over thisinterface.
      network-interfaces:
        - name: HETH3
          device:
              name: hed3
          network-groups:
            - EXTERNAL-VM
            - GUEST
            - MANAGEMENT

    - name: CEP-MON-INTERFACES
      network-interfaces:
      ## This defines the interface used for management
      ## traffic like logging, monitoring, etc.
      ## The same interface is also used to connect the client
      ## node to the Ceph nodes so that any Ceph client
      ## like cinder-volume can route data directly to
      ## Ceph over thisinterface.
        - name: BOND0
          device:
              name: bond0
          bond-data:
              options:
                  mode: active-backup
                  miimon: 200
                  primary: hed1
              provider: linux
              devices:
                - name: hed1
                - name: hed2
          network-groups:
            - MANAGEMENT

    - name: OSD-INTERFACES
      network-interfaces:
      ## This defines the interface used for management
      ## traffic like logging, monitoring, etc.
      ## The same interface is also used for internal cluster
      ## communication among the OSD nodes.
      ## The same interface is also used for internal
      ## cluster communication among OSD nodes.
        - name: BOND0
          device:
              name: bond0
          bond-data:
              options:
                  mode: active-backup
                  miimon: 200
                  primary: hed1
              provider: linux
              devices:
                - name: hed1
                - name: hed2
          network-groups:
            - MANAGEMENT</codeblock>
        </li>
                <li>Create a new file named <codeph>disks_ceph_monitor.yml</codeph> in the
            <codeph>~/helion/my_cloud/definition/data/</codeph> directory which will define the disk
          model for your Ceph monitors. You can use the <codeph>disks_rgw.yml</codeph> file as a
          base and then edit to match your environment:
          <codeblock>disk-models:
- name: CEP-MON-DISKS
  # Disk model to be used for Ceph monitor nodes
  # /dev/sda_root is used as a volume group for /, /var/log and /var/crash
  # sda_root is a templated value to align with whatever partition is really used
  # This value is checked in os config and replaced by the partition actually used
  # on sda e.g. sda1 or sda5
                            
  volume-groups:
    - name: hlm-vg
      physical-volumes:
        - /dev/sda_root
                            
      logical-volumes:
      # The policy is not to consume 100% of the space of each volume group.
      # 5% should be left free for snapshots and to allow for some flexibility.
        - name: root
          size: 30%
          fstype: ext4
          mount: /
        - name: log
          size: 45%
          mount: /var/log
          fstype: ext4
          mkfs-opts: -O large_file
        - name: crash
          size: 20%
          mount: /var/crash
          fstype: ext4
          mkfs-opts: -O large_file
      consumer:
         name: os</codeblock></li>
                <li>Edit the <codeph>~/helion/my_cloud/definition/data/server_roles.yml</codeph>
                    file to define a new server role for your Ceph monitors:
                    <codeblock>- name: CEP-MON-ROLE
  interface-model: CEP-MON-INTERFACES
  disk-model: CEP-MON-DISKS</codeblock></li>
                <li>Commit your
                    configuration:<codeblock>cd ~/helion/hos/ansible
git add -A
git commit -m "adding dedicated Ceph monitor cluster"</codeblock></li>
                <li>Run the following playbook to add your nodes into Cobbler:
                    <codeblock>cd ~/helion/hos/ansible/
ansible-playbook -i hosts/localhost cobbler-deploy.yml</codeblock></li>
                <li>To reimage all the nodes using PXE, run the following playbook:
                    <codeblock>cd ~/helion/hos/ansible/
ansible-playbook -i hosts/localhost bm-reimage.yml</codeblock></li>
                <li>Run the configuration processor:
                    <codeblock>cd ~/helion/hos/ansible/
ansible-playbook -i hosts/localhost config-processor-run.yml</codeblock></li>
                <li>Update your deployment directory with this playbook:
                    <codeblock>cd ~/helion/hos/ansible/
ansible-playbook -i hosts/localhost ready-deployment.yml</codeblock></li>
                <li>Deploy these changes:
                    <codeblock>cd ~/scratch/ansible/next/hos/ansible
ansible-playbook -i hosts/verb_hosts site.yml</codeblock></li>
            </ol>
        </section>
    </body>
</topic>
