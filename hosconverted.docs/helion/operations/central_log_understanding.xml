<?xml version="1.0" encoding="UTF-8"?>
<!--Edit status: not edited-->
<!DOCTYPE topic
  PUBLIC "-//OASIS//DTD DITA Topic//EN" "topic.dtd" >
<topic xml:lang="en-us" id="central_log_concepts">
  <title>Understanding the Centralized Logging Service</title>
  <body>

    <p>The Centralized Logging feature collects logs on a central system, rather than leaving the
      logs scattered across the network. The administrator can use a single Kibana interface to view
      log information in charts, graphs, tables, histograms, and other forms.</p>
    <ul>
      <li><xref href="#central_log_concepts/components">What Components are Part of
        Logging?</xref></li>
      <li><xref href="#central_log_concepts/retaining_logs">How Long are Log Files
        Retained?</xref></li>
      <li><xref href="#central_log_concepts/rotating_logs">How are Logs Rotated?</xref></li>
      <li><xref href="#central_log_concepts/BUElasticsearch">Are Log Files Backed-Up To
          Elasticsearch?</xref>
      </li>
    </ul>

    <section id="components">
      <title>What Components are Part of Centralized Logging?</title>
      <p>Centralized logging consists of several components, detailed below:</p>
      <ul>
        <li><b>Administrator's Browser:</b> Operations Console can be used to access logging alarms
          or to access Kibana's dashboards to review logging data.</li>
        <li><b>Apache Website for Kibana:</b> a standard Apache website that proxies web/REST
          requests to the Kibana NodeJS server.</li>
        <li><b>Beaver:</b> a python daemon that collects information in log files and sends it to
          the Logging API (monasca-log API) over a secure connection.</li>
        <li><b>Cloud Auditing Data Federation (CADF):</b> defines a standard, full-event model
          anyone can use to fill in the essential data needed to certify, self-manage and self-audit
          application security in cloud environments.</li>
        <li><b>Centralized Logging and Monitoring (CLM):</b> used to evaluate and troubleshoot your
            <keyword keyref="kw-hos"/> distributed cloud environment from a single location.</li>
        <li><b>Curator:</b> a tool provided by Elasticsearch to manage indices.</li>
        <li><b>Elasticsearch:</b> A data store offering fast indexing and querying.</li>
        <li><b><keyword keyref="kw-hos"/>:</b> provides public, private, and managed cloud solutions
          to get you moving on your cloud journey.</li>
        <li><b>JavaScript Object Notation (JSON) log file:</b> a file stored in the JSON format and
          used to exchange data. JSON uses JavaScript syntax, but the JSON format is text only. Text
          can be read and used as a data format by any programming language. This format is used by
          the Beaver and Logstash components.</li>
        <li><b>Kafka:</b> a messaging broker used for collection of <keyword keyref="kw-hos"/>
          centralized logging data across nodes. It is highly available, scalable and performant.
          Kafka stores logs in disk instead of memory and is therefore more tolerant to consumer
          down times. <note type="important">Make sure not to undersized your Kafka partition or the
            data retention period may be lower than expected. If Helion OpenStack receives more data
            than expected, then Kafka will fall back to the following settings: <table frame="all"
              rowsep="1" colsep="1" id="kafka_retention_table">
              <tgroup cols="2">
                <colspec colname="c1" colnum="1" colwidth="1.0*"/>
                <colspec colname="c2" colnum="2" colwidth="1.0*"/>
                <thead>
                  <row>
                    <entry>Kafka Partition Capacity</entry>
                    <entry>Retention Period</entry>
                  </row>
                </thead>
                <tbody>
                  <row>
                    <entry>85%</entry>
                    <entry>30 minutes</entry>
                  </row>
                  <row>
                    <entry>25%</entry>
                    <entry>Default</entry>
                  </row>
                </tbody>
              </tgroup>
            </table>
          <p>Over time Kafka will also eject old data.</p>
          </note>
        </li>
        <li><b>Kibana:</b> a client/server application with rich dashboards to visualize the data in
          Elasticsearch through a web browser. Kibana enables you to create charts and graphs using
          the log data.</li>
        <li><b>Logging API (monasca-log-api):</b>
          <keyword keyref="kw-hos"/> API provides a standard REST interface to store logs. It uses
          Keystone authentication and role-based access control support.</li>
        <li><b>Logstash:</b> a log processing system for receiving, processing and outputting logs.
          Logstash retrieves logs from Kafka, processes and enriches the data, then stores the data
          in Elasticsearch.</li>
        <li><b>MML Service Node:</b> Metering, Monitoring, and Logging (MML) service node. All
          services associated with metering, monitoring, and logging run on a dedicated three-node
          cluster. Three nodes are required for high availability with quorum.</li>
        <li><b> Monasca:</b> OpenStack monitoring at scale infrastructure for the cloud that
          supports alarms and reporting.</li>
        <li><b>OpenStack Service:</b> An OpenStack service process that requires logging
          services.</li>
        <li><b> Oslo.log:</b> An OpenStack library for log handling. The library functions automate
          configuration, deployment and scaling of complete, ready-for-work application platforms.
          Some PaaS solutions, such as Cloud Foundry, combine operating systems, containers, and
          orchestrators with developer tools, operations utilities, metrics, and security to create
          a developer-rich solution.</li>
        <li><b> Text log:</b> A type of file used in the logging process that contains
          human-readable records.</li>
      </ul>
      <p>These components are configured to work out-of-the-box and the admin should be able to view
        log data using the default configurations.</p>
      <p>In addition to each of the services, Centralized Logging also processes logs for the
        following features:</p>
      <ul>
        <li>HAProxy</li>
        <li>Syslog</li>
        <li>keepalived</li>
      </ul>
      <p>The purpose of the logging service is to provide a common logging infrastructure with
        centralized user access. Since there are numerous services and applications running in each
        node of a <keyword keyref="kw-hos"/> cloud, and there could be hundreds of nodes, all of
        these services and applications can generate enough log files to make it very difficult to
        search for specific events in log files across all of the nodes. Centralized Logging
        addresses this issue by sending log messages in real time to a central Elasticsearch,
        Logstash, and Kibana cluster. In this cluster they are indexed and organized for easier and
        visual searches. The following illustration describes the architecture used to collect
        operational logs.</p>
      <p>
        <image href="../../media/logservice_arch.png" placement="break"/>
      </p>
      <note>The arrows come from the active (requesting) side to the passive (listening) side. The
        active side is always the one providing credentials, so the arrows may also be seen as
        coming from the credential holder to the application requiring authentication.</note>
    </section>


    <section id="log_arch_st1to2">
      <title>Steps 1- 2</title>
      <p>Services configured to generate log files record the data. Beaver listens for changes to
        the files and sends the log files to the Logging Service. The first step the Logging service
        takes is to re-format the original log file to a new log file with text only and to remove
        all network operations. In Step 1a, the Logging service uses the Oslo.log library to
        re-format the file to text-only. In Step 1b, the Logging service uses the Python-Logstash
        library to format the original audit log file to a JSON file.</p>
      <dl>
        <dlentry>
          <dt>Step 1a</dt>
          <dd>Beaver watches configured service operational log files for changes and reads
            incremental log changes from the files.</dd>
        </dlentry>
        <dlentry>
          <dt>Step 1b</dt>
          <dd>Beaver watches configured service operational log files for changes and reads
            incremental log changes from the files.</dd>
        </dlentry>
        <dlentry>
          <dt>Step 2a</dt>
          <dd>The monascalog transport of Beaver makes a token request call to Keystone passing in
            credentials. The token returned is cached to avoid multiple network round-trips.</dd>
        </dlentry>
        <dlentry>
          <dt>Step 2b</dt>
          <dd>The monascalog transport of Beaver batches multiple logs (operational or audit) and
            posts them to the monasca-log-api VIP over a secure connection. Failure logs are written
            to the local Beaver log.</dd>
        </dlentry>
        <dlentry>
          <dt>Step 2c</dt>
          <dd>The REST API client for monasca-log-api makes a token-request call to Keystone passing
            in credentials. The token returned is cached to avoid multiple network round-trips.</dd>
        </dlentry>
        <dlentry>
          <dt>Step 2d</dt>
          <dd>The REST API client for monasca-log-api batches multiple logs (operational or audit)
            and posts them to the monasca-log-api VIP over a secure connection.</dd>
        </dlentry>
      </dl>
    </section>

    <section id="log_arch_st3ab">
      <title>Steps 3a- 3b</title>
      <p>The Logging API (monasca-log API) communicates with Keystone to validate the incoming
        request, and then sends the logs to Kafka.</p>
      <dl>
        <dlentry>
          <dt>Step 3a</dt>
          <dd>The monasca-log-api WSGI pipeline is configured to validate incoming request tokens
            with Keystone. The keystone middleware used for this purpose is configured to use the
            monasca-log-api admin user, password and project that have the required keystone role to
            validate a token. <note type="note">If successful, the response also includes details
              about the project to which the token was granted and the roles that the token has on
              that project.</note></dd>
        </dlentry>
        <dlentry>
          <dt>Step 3b</dt>
          <dd>Monasca-log-api sends log messages to Kafka using a language-agnostic TCP
            protocol.</dd>
        </dlentry>
      </dl>
    </section>

    <section id="log_arch_st4to8">
      <title>Steps 4- 8</title>
      <p>Logstash pulls messages from Kafka, identifies the log type, and transforms the messages
        into either the audit log format or operational format. Then Logstash sends the messages to
        Elasticsearch, using either an audit or operational indices.</p>
      <dl>
        <dlentry>
          <dt>Step 4</dt>
          <dd>Logstash input workers pull log messages from the Kafka-Logstash topic using TCP.</dd>
        </dlentry>
        <dlentry>
          <dt>Step 5</dt>
          <dd>This Logstash filter processes the log message in-memory in the request pipeline.
            Logstash identifies the log type from this field.</dd>
        </dlentry>
        <dlentry>
          <dt>Step 6</dt>
          <dd> This Logstash filter processes the log message in-memory in the request pipeline. If
            the message is of audit-log type, Logstash transforms it from the monasca-log-api
            envelope format to the original CADF format.</dd>
        </dlentry>
        <dlentry>
          <dt>Step 7</dt>
          <dd>This Logstash filter determines which index should receive the log message. There are
            separate indices in Elasticsearch for operational versus audit logs.</dd>
        </dlentry>
        <dlentry>
          <dt>Step 8</dt>
          <dd>Logstash output workers write the messages read from Kafka to the daily index in the
            local Elasticsearch instance. <note type="note">The Elasticsearch write operations
              outside of localhost are secured by a basic HTTP authentication to Logstash web
              server. Therefore, this Logstash write happens only inside the localhost.</note></dd>
        </dlentry>
      </dl>
    </section>

    <section id="log_arch_st9to12">
      <title>Steps 9- 12</title>
      <p>When an administrator who has access to the guest network accesses the Kibana client and
        makes a request, Apache forwards the request to the Kibana NodeJS server. Then the server
        uses the Elasticsearch REST API to service the client requests.</p>
      <dl>
        <dlentry>
          <dt>Step 9</dt>
          <dd>An administrator who has access to the guest network accesses the Kibana client to
            view and search log data. The request can originate from the external network in the
            cloud through a tenant that has a pre-defined access route to the guest network.</dd>
        </dlentry>
        <dlentry>
          <dt>Step 10</dt>
          <dd>An administrator who has access to the guest network uses a web browser and points to
            the Kibana URL. This allows the user to search logs and view Dashboard reports.</dd>
        </dlentry>
        <dlentry>
          <dt>Step 11</dt>
          <dd>The authenticated request is forwarded to the Kibana NodeJS server to render the
            required dashboard, visualization, or search page.</dd>
        </dlentry>
        <dlentry>
          <dt>Step 12</dt>
          <dd>The Kibana NodeJS web server uses the Elasticsearch REST API in localhost to service
            the UI requests.</dd>
        </dlentry>
      </dl>
    </section>

    <section id="log_arch_st13to15">
      <title>Steps 13- 15</title>
      <p>Log data is backed-up and deleted in the final steps.</p>
      <dl>
        <dlentry>
          <dt>Step 13</dt>
          <dd>A daily cron job running in the ELK node runs curator to prune old Elasticsearch log
            indices.</dd>
        </dlentry>
        <dlentry>
          <dt>Step 14</dt>
          <dd>The curator configuration is done at the deployer node through the Ansible role
            logging-common. Curator is scripted to then prune or clone old indices based on this
            configuration.</dd>
        </dlentry>
        <dlentry>
          <dt>Step 15</dt>
          <dd>The audit logs are configured to be backed up by the <keyword keyref="kw-hos"/>
            Freezer product. For more information about Freezer, read the <keyword keyref="kw-hos"/>
            documentation on <xref
              href="http://docs.hpcloud.com/#3.x/helion/bura/bura_overview.html" scope="external"
              format="html">Backup and Restore</xref>.</dd>
        </dlentry>
      </dl>
    </section>

    <section id="retaining_logs"><title>How Long are Log Files Retained?</title>
      <p>The logs that are centrally stored are saved to persistent storage as Elasticsearch
        indices. These indices are stored in the partition <codeph>/var/lib/elasticsearch</codeph>
        on each of the Elasticsearch cluster nodes. Out of the box, logs are stored in one
        Elasticsearch index per service. As more days go by, the number of indices stored in this
        disk partition grows. Eventually the partition fills up. If they are <b>open</b>, each of
        these indices takes up CPU and memory. If these indices are left unattended they will
        continue to consume system resources and eventually deplete them.</p>
      <p>Elasticsearch, by itself, doesn't prevent this from happening.</p>
      <p><keyword keyref="kw-hos"/> uses a tool called curator that is developed by the
        Elasticsearch community to handle these situations. <keyword keyref="kw-hos"/> installs and
        uses a curator in conjunction with several configurable settings. This curator is called by
        cron and performs the following checks:</p>
      <ul>
        <li><b>First Check.</b> The hourly cron job checks to see if the currently used
          Elasticsearch partition size is over the value set in:
          <codeblock>curator_low_watermark_percent</codeblock> If it is higher than this value, the
          curator deletes old indices according to the value set in:
          <codeblock>curator_num_of_indices_to_keep</codeblock></li>
        <li><b>Second Check.</b> Another check is made to verify if the partition size is below the
          high watermark percent. If it is still too high, curator will delete all indices except
          the current one that is over the size as set in:
          <codeblock>curator_max_index_size_in_gb</codeblock></li>
        <li><b>Third Check.</b> A third check verifies if the partition size is still too high. If
          it is, curator will delete all indices except the current one.</li>
        <li><b>Final Check.</b> A final check verifies if the partition size is still high. If it
          is, an error message is written to the log file but the current index is NOT deleted.</li>
      </ul>
      <p>In the case of an extreme network issue, log files can run out of disk space in under an
        hour. To avoid this <keyword keyref="kw-hos"/> uses a shell script called
          <codeph>logrotate_if_needed.sh</codeph>. The cron process runs this script every 5 minutes
        to see if the size of <codeph>/var/log</codeph> has exceeded the high_watermark_percent (95%
        of the disk, by default). If it is at or above this level,
          <codeph>logrotate_if_needed.sh</codeph> runs the <codeph>logrotate</codeph> script to
        rotate logs and to free up extra space. This script helps to minimize the chance of running
        out of disk space on <codeph>/var/log</codeph>.</p>
    </section>

    <section id="rotating_logs"><title>How Are Logs Rotated?</title>
      <p><keyword keyref="kw-hos"/> uses the cron process which in turn calls Logrotate to provide
        rotation, compression, and removal of log files. Each log file can be rotated hourly, daily,
        weekly, or monthly. If no rotation period is set then the log file will only be rotated when
        it grows too large.</p>
      <p>Rotating a file means that the Logrotate process creates a copy of the log file with a new
        extension, for example, the .1 extension, then empties the contents of the original file. If
        a .1 file already exists, then that file is first renamed with a .2 extension. If a .2 file
        already exists, it is renamed to .3, etc., up to the maximum number of rotated files
        specified in the settings file. When Logrotate reaches the last possible file extension, it
        will delete the last file first on the next rotation. By the time the Logrotate process
        needs to delete a file, the results will have been copied to Elasticsearch, the central
        logging database.</p>
      <p>The log rotation setting files can be found in the following directory
        <codeblock>~/scratch/ansible/next/hos/ansible/roles/logging-common/vars</codeblock></p>
      <p>These files allow you to set the following options: </p>
      <dl>
        <dlentry>
          <dt>Service</dt>
          <dd>The name of the service that creates the log entries.</dd>
        </dlentry>
        <dlentry>
          <dt>Rotated Log Files</dt>
          <dd>List of log files to be rotated. These files are kept locally on the server and will
            continue to be rotated. If the file is also listed as Centrally Logged, it will also be
            copied to Elasticsearch.</dd>
        </dlentry>
        <dlentry>
          <dt>Frequency</dt>
          <dd>The timing of when the logs are rotated. Options include:hourly, daily, weekly, or
            monthly.</dd>
        </dlentry>
        <dlentry>
          <dt>Max Size</dt>
          <dd>The maximum file size the log can be before it is rotated out.</dd>
        </dlentry>
        <dlentry>
          <dt>Rotation</dt>
          <dd>The number of log files that are rotated.</dd>
        </dlentry>
        <dlentry>
          <dt>Centrally Logged Files</dt>
          <dd>These files will be indexed by Elasticsearch and will be available for searching in
            the Kibana user interface.</dd>
        </dlentry>
      </dl>
      <p>As an example, Freezer, the Backup and Restore (BURA) service, may be configured to create
        log files by setting the Rotated Log Files section to contain:
        <codeblock>/var/log/freezer-agent/freezer-scheduler.log</codeblock></p>
      <p>This configuration means that in the /var/log/freezer-agent directory, in a live
        environment, there should be a file called freezer-scheduler.log. As the log file grows, the
        cron process runs every hour to check the log file size against the settings in the
        configuration files. The example freezer-agent settings are described below. <table
          frame="all" rowsep="1" colsep="1" id="table_itd_tjh_ht">
          <tgroup cols="7">
            <colspec colname="c1" colnum="1" colwidth="1.32*"/>
            <colspec colname="c2" colnum="2" colwidth="1*"/>
            <colspec colname="c3" colnum="3" colwidth="3.26*"/>
            <colspec colname="c4" colnum="4" colwidth="1.46*"/>
            <colspec colname="c5" colnum="5" colwidth="1.05*"/>
            <colspec colname="c6" colnum="6" colwidth="1.4*"/>
            <colspec colname="c7" colnum="7" colwidth="3.38*"/>
            <thead>
              <row>
                <entry>Service</entry>
                <entry>Node Type</entry>
                <entry>Rotated Log Files</entry>
                <entry>Frequency</entry>
                <entry>Max Size</entry>
                <entry>Rotation</entry>
                <entry>Centrally Logged Files</entry>
              </row>
            </thead>
            <tbody>
              <row>
                <entry>
                  <p>Freezer</p>
                </entry>
                <entry>
                  <p>Control</p>
                </entry>
                <entry>
                  <p>/var/log/freezer-agent/freezer-scheduler.log</p>
                  <p>/var/log/freezer-agent/freezer-agent-json.log</p>
                </entry>
                <entry>
                  <p>Daily</p>
                </entry>
                <entry>
                  <p>45 MB</p>
                </entry>
                <entry>
                  <p>7</p>
                </entry>
                <entry>
                  <p>/var/log/freezer-agent/freezer-agent-json.log</p>
                </entry>
              </row>
            </tbody>
          </tgroup>
        </table>
      </p>
      <p>For the <codeph>freezer-scheduler.log</codeph> file specifically, the information in the
        table tells the Logrotate process that the log file is to be rotated daily, and it can have
        a maximum size of 45 MB. After a week of log rotation, you might see something similar to
        this list:
          <codeblock>freezer-scheduler.log at 10K
freezer-scheduler.log.1 at 123K
freezer-scheduler.log.2.gz at 13K
freezer-scheduler.log.3.gz at 17K
freezer-scheduler.log.4.gz at 128K
freezer-scheduler.log.5.gz at 22K
freezer-scheduler.log.6.gz at 323K
freezer-scheduler.log.7.gz at 123K        </codeblock><note
          type="note">The log files beyond .1 are compressed so they have a .gz for a file
          extension.</note> Since the Rotation value is set to 7 for this log file, there will never
        be a <codeph>freezer-scheduler.log.8.gz</codeph>. When the cron process runs its checks, if
        the <codeph>freezer-scheduler.log</codeph> size is more than 45 MB, then Logrotate rotates
        the file.</p>
      <p>In this example, the following log files are rotated:
        <codeblock>/var/log/freezer-agent/freezer-scheduler.log
/var/log/freezer-agent/freezer-agent-json.log      </codeblock></p>
      <p>However, in this example, only the following file is centrally logged with Elasticsearch:
        <codeblock>/var/log/freezer-agent/freezer-agent-json.log        </codeblock> Only files that
        are listed in the <b>Centrally Logged Files</b> section are copied to Elasticsearch. </p>
      <p>All of the variables for the Logrotate process are found in the following file:
        <codeblock>~/scratch/ansible/next/hos/ansible/roles/logging-ansible/logging-common/defaults/main.yml          </codeblock>
      </p>
      <p>Cron runs Logrotate hourly. Every 5 minutes another process is run called
          <b>"logrotate_if_needed"</b> which uses a watermark value to determine if the Logrotate
        process needs to be run. If the <b>"high watermark"</b> has been reached, and the /var/log
        partition is more than 95% full (by default - this can be adjusted), then Logrotate will be
        run within 5 minutes.</p>
    </section>

    <section id="BUElasticsearch">
      <title>Are Log Files Backed-Up To Elasticsearch?</title>
      <p>While centralized logging is enabled out of the box, the backup of these logs is not. The
        reason is because Centralized Logging relies on the Elasticsearch FileSystem Repository
        plugin, which in turn requires shared disk partitions to be configured and accessible from
        each of the Elasticsearch nodes. Since there are multiple ways to setup a shared disk
        partition, <keyword keyref="kw-hos"/> allows you to choose an approach that works best for
        your deployment before enabling the back-up of log files to Elasticsearch.</p>
      <p>If you enable automatic back-up of centralized log files, then all the logs collected from
        the cloud nodes will be backed-up to Elasticsearch. Every hour, in the management controller
        nodes where Elasticsearch is setup, a cron job runs to check if Elasticsearch is running low
        on disk space. If the check succeeds, it further checks if the backup feature is enabled. If
        enabled, the cron job saves a snapshot of the Elasticsearch indices to the configured shared
        disk partition using curator. Next, the script starts deleting the oldest index and moves
        down from there checking each time if there is enough space for Elasticsearch. A check is
        also made to ensure that the backup runs only once a day.</p>
      <p>For steps on how to enable automatic back-up, refer to the <xref
          href="central_log_configure_CL.dita">Settings for Centralized Logging</xref> topic.</p>
    </section>

  </body>
</topic>
