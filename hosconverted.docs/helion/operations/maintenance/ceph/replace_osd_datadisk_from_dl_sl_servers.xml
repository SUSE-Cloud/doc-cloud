<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE topic PUBLIC "-//OASIS//DTD DITA Topic//EN" "topic.dtd">
<topic id="remove_datadisk_ceph_osd">
    <title><ph conkeyref="HOS-conrefs/product-title"/>Replacing a Ceph OSD Data Disk from DL and SL
        Servers</title>
    <abstract><shortdesc outputclass="hdphidden">A sample procedure to replace data disks from DL
            and SL servers.</shortdesc>The following example provides the procedures to replace the
        OSD data disks from HP servers (DL and SL).</abstract>
    <body>
        <p conkeyref="HOS-conrefs/applies-to"/>
        <section id="steps">
            <title>Replacing a Ceph OSD Data Disk from DL and SL Servers</title>
            <ol>
                <li>Login to the OSD node, on which the disk is to be removed, and acquire sudo
                    privileges.<codeblock>sudo -i</codeblock></li>
                <li>Execute the following commands to collect the Ceph cluster information.<ol
                        id="ol_cwq_d5b_lx">
                        <li>Show the status of Ceph
                            cluster.<codeblock>ceph --cluster &lt;ceph_cluster&gt; -s</codeblock></li>
                        <li>Search for the word osd.<codeblock>ps -aef | grep osd</codeblock></li>
                        <li>View the CRUSH map by executing the following
                            command.<codeblock>ceph --cluster &lt;ceph_cluster&gt; osd tree</codeblock></li>
                        <li>List the OSD disk.<codeblock>ceph-disk list</codeblock></li>
                        <li>Check the Ceph cluster usage
                            status.<codeblock>ceph --cluster &lt;ceph_cluster&gt; df</codeblock></li>
                        <li>Find the health status of the Ceph cluster.
                            <codeblock>ceph --cluster &lt;ceph_cluster> health | grep 'nearfull osds'</codeblock></li>
                    </ol><note> If step (f) reports the health status of some of the OSD(s) with
                        nearfull capacity, do not proceed further. </note></li>
                <li>Identify the OSD ID corresponding to the disk that is being removed.<ol
                        id="ol_h3r_fsc_lx">
                        <li>List the disks and find the name of the disk that must be
                                removed.<codeblock>Run 'ceph-disk list | grep &lt;name of disk to be removed>'</codeblock><p>Note
                                down the OSD ID. The ID must be in <codeph>osd.[n]</codeph> (n being
                                an integer) format.</p></li>
                    </ol></li>
                <li>Identify the process ID corresponding to the disk that is being removed.<ol
                        id="ol_yv3_2tc_lx">
                        <li>List the disks and find the name of the disk that must be
                                removed.<codeblock>Run 'ps -aef | grep osd | grep "id [n]"'</codeblock><p>Note
                                down the OSD process ID. For example:
                                    <codeph>osd.process_id</codeph>.</p></li>
                    </ol></li>
                <li>Kill OSD process mapped to the disk that is being removed and execute the
                    following commands to ensure that it is not running.
                    <codeblock>kill -9 &lt;osd.process_id&gt;
ps -aef | grep osd | grep "id [n]"</codeblock></li>
                <li>Remove the OSD from the CRUSH map, delete its authentication key, and remove the
                    OSD ID from Ceph
                        inventory:<codeblock>ceph --cluster &lt;ceph_cluster> osd crush remove osd.&lt;osd-id>
ceph --cluster &lt;ceph_cluster> auth del osd.&lt;osd-id>
ceph --cluster &lt;ceph_cluster> osd rm &lt;osd-id></codeblock><p>The
                        osd-id is the OSD identifier as noted in 3(a) earlier.</p></li>
                <li>Remove the mount instructions for the same disk(s) from the
                        <codeph>/etc/fstab</codeph> file. For example, to remove the <codeph>data
                        disk/dev/sde</codeph> you must know the UUID. Execute the following command
                    to find the UUID.<codeblock>sudo blkid | grep "ceph data"</codeblock>Example
                        output:<codeblock>$ sudo blkid |grep "ceph data"
/dev/sdc1: UUID="45b7c7e8-4f2d-4226-989b-c83e7e9e2596" TYPE="xfs" PARTLABEL="ceph data" PARTUUID="6c37a311-eabf-4872-bd8e-7d426585432e"
/dev/sdf1: UUID="1cfb4de2-fe21-4e20-9a6e-ad45e90949ae" TYPE="xfs" PARTLABEL="ceph data" PARTUUID="209b9af4-8d25-4d60-a430-2934ceca6042"
/dev/sde1: UUID="51c32ca1-0ee1-4c75-bfbc-5d113a4b4402" TYPE="xfs" PARTLABEL="ceph data" PARTUUID="dfcd410d-9c56-4bb1-93c6-496bae29de5f"</codeblock><p>In
                        the preceeding example, the UUID 51c32ca1-0ee1-4c75-bfbc-5d113a4b4402
                        corresponds to the data disk that is removed (i.e.<codeph>
                        /dev/sde</codeph>). Remove the line in the <codeph>/etc/fstab</codeph> file
                        whose entry starts with the text
                        UUID="51c32ca1-0ee1-4c75-bfbc-5d113a4b4402".</p></li>
                <li>Verify that OSD is not a part of Ceph
                    cluster.<codeblock>ceph osd tree</codeblock></li>
                <li>Run the hpssacli utility.<codeblock>hpssacli</codeblock></li>
                <li>Identify (array_name, logicaldrive_number) for the failed OSD disk using
                    hpssacli. </li>
                <li>Collect the following data from the hpssacli command output.<ol
                        id="ol_tjs_rvc_lx">
                        <li>List the logical drive of a particular
                            slot.<codeblock>Run 'controller slot=&lt;slot_id> logicaldrive all show'</codeblock></li>
                        <li>List the physical drive of a particular
                            slot.<codeblock>Run 'controller slot=&lt;slot_id&gt; physicaldrive all show'</codeblock></li>
                        <li>List the logical drive number
                            details.<codeblock>Run 'controller slot=&lt;slot_id&gt; logicaldrive &lt;logicaldrive_number&gt; show'</codeblock></li>
                        <li>List the physical drive number
                            details.<codeblock>Run 'controller slot=&lt;slot_id&gt; physicaldrive &lt;x:y:z> show</codeblock></li>
                    </ol></li>
                <li>You can check the mount status of the logical drive in the output of
                        <codeph>controller slot=&lt;slot>logicaldrive &lt;logicaldrive_number>
                        show</codeph>. You must remove the mount points. Run <codeph>umount -f
                        &lt;mount point>.</codeph> If umount does not succeed, perform the following
                        steps:<ol id="ol_enb_ddd_lx">
                        <li>Run <codeph>ceph osd set noout</codeph>.</li>
                        <li>Reboot the node.</li>
                        <li>Wait for the node to come up.</li>
                        <li>Run <codeph>ceph osd unset noout</codeph>.</li>
                    </ol></li>
                <li> Remove the failed disk physically and check that the logical drive is in the
                    failed state using hpssacli
                    command.<codeblock>controller slot=&lt;slot> logicaldrive all show</codeblock></li>
                <li>Insert a new disk. Please ensure the following points:<ol id="ol_g1g_12d_lx">
                        <li>Note down the serial number of physical drive for validation (if you
                            can).</li>
                        <li>New disk is of same size as the old one.</li>
                        <li>New disk is clean and does not have any RAID on it.</li>
                    </ol><note>The <codeph>wipe_one_disk.yml</codeph> playbook can be used to ensure
                        that the replacement disk is clean as follows (assuming that the new disk
                        shows up as
                        <codeph>sde</codeph>):<codeblock>cd ~/scratch/ansible/next/hos/ansible
ansible-playbook -i hosts/verb_hosts wipe_one_disk.yml --limit &lt;OSD-hostname&gt; -e wipe_one_disk=sde </codeblock></note></li>
                <li>Run the hpssacli command <codeph>controller slot=&lt;slot> logicaldrive
                        &lt;logicaldrive_number> modify reenable</codeph> to re-enable logical drive
                    with the new disk.</li>
                <li> Run the following hpssacli command:<ol id="ol_ljs_p2d_lx">
                        <li>List the logical
                                drive.<codeblock>Run controller slot=&lt;slot&gt; logicaldrive &lt;logicaldrive_number&gt; show</codeblock><p>The
                                    &lt;<codeph>logicaldrive_number</codeph>> is the same number
                                that was used at the time of removing the failed drive.</p></li>
                        <li>List the physical
                                drive.<codeblock>Run controller slot=&lt;slot&gt; physicaldrive &lt;x:y:z&gt; show</codeblock><note>Observe
                                the serial number as you have already noted in step
                            (14.a)</note></li>
                    </ol></li>
                <li> Run <codeph>rescan</codeph> using hpssacli utility.</li>
                <li>Run <codeph>partprobe</codeph>.</li>
                <li>Perform the following operations:<ol id="ol_qnf_lwd_lx">
                        <li>Run <codeph>ceph osd set noout</codeph>.</li>
                        <li>Reboot the node as clean up operation might not succeed for the affected
                            OSD. Primarily, it is observed when I/O (external or internal) is going
                            on failed OSD disk.</li>
                        <li>Wait for node to power up.</li>
                        <li>Run <codeph>ceph osd unset noout</codeph>.</li>
                    </ol></li>
                <li> Check the drive name of the replaced disks using hpssacli.
                        <codeblock>hpssacli controller slot=&lt;slot&gt; ld &lt;logicaldrive_number&gt; show</codeblock><note
                        type="important"
                        >D<?oxy_custom_start type="oxy_content_highlight" color="255,60,255"?>isk
                        name should be same as what was there for failed OSD before
                        removal.*<?oxy_custom_end?></note></li>
                <li>Unmount the OSD data
                    disk:<codeblock>sudo umount &lt;mount_path_of_datadisk></codeblock></li>
                <li>Remove the OSD's directories.
                    <codeblock>sudo rm -rf /var/lib/ceph/osd/&lt;ceph_cluster>-&lt;osd-id></codeblock></li>
                <li>Delete the journal partition for each OSD number removed and instruct the kernel
                    to reload the table as
                        follows:<codeblock>sudo sgdisk -d=&lt;partition&gt; &lt;device&gt;; sudo partprobe</codeblock><p>For
                        example, if <codeph>/dev/sdj1</codeph> is the journal partition, it can be
                        removed by executing the following command.
                        <codeblock>$ sudo sgdisk -d=1 /dev/sdj; sudo partprobe</codeblock></p></li>
                <li>Execute the following commands to collect the Ceph cluster information.<ol
                        id="ol_bpn_5h2_lx">
                        <li>Show the status of Ceph cluster.<codeblock>ceph -s</codeblock></li>
                        <li>Search for the word osd.<codeblock>ps -aef | grep osd</codeblock></li>
                        <li>View the CRUSH map by executing the following
                            command.<codeblock>ceph osd tree</codeblock></li>
                        <li>List the OSD disk.<codeblock>ceph-disk list</codeblock></li>
                        <li>Show the clusters free space stats.<codeblock>ceph df</codeblock></li>
                    </ol></li>
                <li>Login to the life cycle manage and execute the following
                    command.<codeblock>ansible-playbook -i hosts/verb_hosts ceph-deploy.yml</codeblock></li>
                <li>Execute the following commands to collect the Ceph cluster
                    information.<codeblock>ceph -s
ps -aef | grep osd
ceph osd tree
ceph-disk list
ceph df</codeblock></li>
            </ol>
        </section>
    </body>
</topic>

