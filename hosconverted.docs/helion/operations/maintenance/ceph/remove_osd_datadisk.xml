<?xml version="1.0" encoding="UTF-8"?>
<!--Edit status: not edited-->
<!DOCTYPE topic PUBLIC "-//OASIS//DTD DITA Topic//EN" "topic.dtd">
<topic id="remove_datadisk_osd">
  <title><ph conkeyref="HOS-conrefs/product-title"/>Removing a Data Disk from a Ceph OSD
    Node</title>
  <abstract><shortdesc outputclass="hdphidden">Steps for removing data disks from your OSD
      nodes.</shortdesc>If you want to remove a data disks from your existing OSD nodes, these steps
    will show you how to.</abstract>
  <body>
    <!--not tested-->
    <p conkeyref="HOS-conrefs/applies-to"/>
    <section id="steps"><title>Removing a Data Disk from an Object Storage Daemon (OSD) Node</title>
      <ol>
        <li>Log in to the OSD node you want to remove the data disk from.</li>
        <li>Figure out the OSD number for the corresponding disk using this command:
          <codeblock>sudo ceph-disk list | grep &lt;disk-to-be-removed&gt; | grep osd</codeblock></li>
        <li>Using the output from the command in the previous step, locate the OSD number. <p>In the
            example below, the OSD number is <codeph>4</codeph>:</p>
          <codeblock>/dev/sdn1 ceph data, active, cluster ceph, osd.4, journal /dev/sdj1</codeblock>
          <note>The value for &lt;osd_number> in the next steps should be replaced with the OSD
            number you locate above.</note></li>
        <li>Use the following commands to take the OSD node out of the Ceph cluster and observe the
          cluster rebalance itself: <codeblock>ceph osd out &lt;osd_number&gt;
ceph -w</codeblock>
          <p>The placement group states will change from <codeph>active+clean</codeph> to
              <codeph>active</codeph>, some <codeph>degraded</codeph> objects, and then finally back
            to <codeph>active+clean</codeph> once the migration completes. You can press CTRL+C to
            exit.</p></li>
        <li>Use the following commands to stop the OSD service, remove it from the CRUSH map, delete
          its authentication key, and remove the OSD node:
          <codeblock>sudo systemctl stop ceph-osd@&lt;osd_number&gt;
ceph osd crush remove osd.&lt;osd_number&gt;
ceph auth del osd.&lt;osd_number&gt;
ceph osd rm &lt;osd_number&gt;</codeblock></li>
        <li>Unmount the OSD data
          disk:<codeblock>sudo umount &lt;mount_path_of_datadisk></codeblock></li>
        <li>This step will remove the OSD's directories and should be performed with caution: <codeblock>sudo rm -rf /var/lib/ceph/osd/&lt;ceph_cluster&gt;-&lt;osd_number&gt;</codeblock>
          <p>where:</p>
          <table frame="all" rowsep="1" colsep="1" id="table_sgn_m5r_35">
            <tgroup cols="2">
              <colspec colname="c1" colnum="1" colwidth="1.0*"/>
              <colspec colname="c2" colnum="2" colwidth="1.0*"/>
              <thead>
                <row>
                  <entry>Variable</entry>
                  <entry>Description</entry>
                </row>
              </thead>
              <tbody>
                <row>
                  <entry>&lt;ceph_cluster&gt;</entry>
                  <entry>This should be replaced with the name of your Ceph cluster.</entry>
                </row>
                <row>
                  <entry>&lt;osd_number&gt;</entry>
                  <entry>This should be replaced with the number identified in step #3
                    earlier.</entry>
                </row>
              </tbody>
            </tgroup>
          </table></li>

        <li>Remove the mount instructions for the same disk(s) from the <codeph>/etc/fstab</codeph>
          file. For example, to remove the data disk <codeph>/dev/sde</codeph>, find out its UUID by
          executing this command:<codeblock>sudo blkid | grep "ceph data"</codeblock>
          <p>Example output:</p>
          <codeblock>$ sudo blkid |grep "ceph data"
/dev/sdc1: UUID="45b7c7e8-4f2d-4226-989b-c83e7e9e2596" TYPE="xfs" PARTLABEL="ceph data" PARTUUID="6c37a311-eabf-4872-bd8e-7d426585432e"
/dev/sdf1: UUID="1cfb4de2-fe21-4e20-9a6e-ad45e90949ae" TYPE="xfs" PARTLABEL="ceph data" PARTUUID="209b9af4-8d25-4d60-a430-2934ceca6042"
/dev/sde1: UUID="51c32ca1-0ee1-4c75-bfbc-5d113a4b4402" TYPE="xfs" PARTLABEL="ceph data" PARTUUID="dfcd410d-9c56-4bb1-93c6-496bae29de5f"</codeblock>
          <p>In the example above, the UUID <codeph>51c32ca1-0ee1-4c75-bfbc-5d113a4b4402</codeph>
            corresponds to the data disk being removed (i.e. <codeph>/dev/sde</codeph>. Remove the
            line in the <codeph>/etc/fstab</codeph> file having the entry starting with the text
              <codeph>UUID="51c32ca1-0ee1-4c75-bfbc-5d113a4b4402"</codeph>.</p></li>
        <li>Delete the journal partition for each OSD number removed and instruct the kernel to
          reload the table as
          follows:<codeblock>(echo d; echo &lt;partition-number>; echo w) | sudo fdisk &lt;disk>
sudo partprobe

Example:
In step #3 above, /dev/sdj1 journal partition can be removed by executing below commands: 
$ (echo d; echo 1; echo w) | sudo fdisk /dev/sdj
$ sudo partprobe</codeblock></li>
        <li>Repeat steps 1-9 for each OSD node in the Ceph cluster one at a time.</li>
        <li>Log in to the lifecycle manager.</li>
        <li>Edit the <codeph>~/helion/my_cloud/definition/data/disks_osd.yml</codeph> file and
          remove the details for the corresponding data disk that you removed earlier.
            <p>Example:</p>
          <codeblock>- name: ceph-osd-data-and-journal
  devices:
     - name: /dev/sdn
  consumer:
     name: ceph
     attrs:
     usage: data
     journal_disk: /dev/sdj</codeblock>
          <note>The <codeph>journal_disk</codeph> in the above example is optional and may be
            missing from your configuration files.</note></li>
        <li>Commit your configuration to the <xref href="../../../installation/using_git.dita">local
            git repo</xref>, as follows:
          <codeblock>cd ~/helion/hos/ansible
git add -A
git commit -m "Removing OSD data disk"</codeblock></li>
        <li>Run the configuration processor:
          <codeblock>cd ~/helion/hos/ansible
ansible-playbook -i hosts/localhost config-processor-run.yml</codeblock></li>
        <li>Update your deployment directory:
          <codeblock>cd ~/helion/hos/ansible
ansible-playbook -i hosts/localhost ready-deployment.yml</codeblock></li>
        <li>Check the status of Ceph services with this playbook:
          <codeblock>cd ~/scratch/ansible/next/hos/ansible
ansible-playbook -i hosts/verb_hosts ceph-status.yml</codeblock></li>
      </ol>
    </section>
  </body>
</topic>
