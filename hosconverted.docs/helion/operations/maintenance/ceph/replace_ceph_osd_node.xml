<?xml version="1.0" encoding="UTF-8"?>
<!--Edit status: not edited-->
<!DOCTYPE topic PUBLIC "-//OASIS//DTD DITA Topic//EN" "topic.dtd">
<topic id="replace_osd_node">
  <title><ph conkeyref="HOS-conrefs/product-title"/>Replacing a Ceph OSD Node</title>
  <abstract>
    <shortdesc outputclass="hdphidden">Steps for replacing a Ceph OSD node.</shortdesc>
  </abstract>
  <body>
    <!--not tested-->
    <p conkeyref="HOS-conrefs/applies-to"/>
    <section id="notes">
      <title>Prerequisites</title>
      <p>
        <ul id="ul_vls_b1l_yv">
          <li>Make sure that the replacement OSD node has all the disks as specified in the input
            model for OSD nodes. The input model is the
              <codeph>~/helion/my_cloud/definition/data/disks_osd.yml</codeph> file.</li>
          <li>Make sure that OSD data disks are free and there are no pre-existing partitions.</li>
        </ul>
      </p>
    </section>
    <section>
      <title>Replace the Ceph OSD node</title>
      <p>Follow the below steps to replace Ceph OSD node: <ol id="ol_x41_41l_yv">
          <li>Add the replacement OSD node using the following the steps in <xref
              href="add_osd_node.dita#add_osdnode">Adding an Object Storage Daemon (OSD)
            Node</xref>. <note>Make sure that the <codeph>server-group</codeph> value for the
              replacement node is same as the node being replaced. </note></li>
          <li>Execute the following command to determine the OSDs on the OSD node to be replaced:
            <codeblock>ceph osd tree 
# id    weight  type name       up/down reweight
-1      6       root default
-2      3               host padawan-ceph-ccp-ceph0003-mgmt
0       1                       osd.0   up      1
3       1                       osd.3   up      1
6       1                       osd.6   up      1
-4      3               host padawan-ceph-ccp-ceph0001-mgmt
1       1                       osd.1   up      1
4       1                       osd.4   up      1
7       1                       osd.7   up      1
-3      3               host padawan-ceph-ccp-ceph0002-mgmt
2       1                       osd.2   up      1
5       1                       osd.5   up      1
8       1                       osd.8   up      1</codeblock>For
            example, to replace the OSD node <codeph>padawan-ceph-ccp-ceph0001-mgmt</codeph>, the
            OSD numbers 1, 4, and 7 are relevant. </li>
          <li>Log in to the OSD node that is to be replaced and take the OSDs out of the cluster so
            that Ceph can begin re-balancing and copying its data to other OSDs.
            <codeblock>ceph osd out &lt;osd-number> --cluster &lt;cluster-name></codeblock>For
            example: <codeblock>ceph osd out 1 4 7 --cluster ceph</codeblock><note>To reduce the
              impact of cluster re-balancing on the overall cluster performance for ongoing
              requests, you can also mark one OSD out at a time and after the migration completes,
              proceed to marking the next OSD out. However, it will lead to multiple cycles of
              cluster re-balancing and take a while for replacing the OSD node.</note></li>
          <li>Ceph will begin re-balancing the cluster by migrating placement groups out of the OSDs
            marked out (in the earlier step). Use the following command to view the re-balancing:
            <codeblock>ceph -w --cluster &lt;cluster-name></codeblock>You should see the placement
            group states change from <codeph>active+clean</codeph> to <codeph>active</codeph>,
              <codeph>some degraded objects</codeph>, then <codeph>active+clean</codeph> when
            migration completes. Execute <codeph>Control-c</codeph> to exit. </li>
          <li>Remove the OSD node that is being replaced by following the steps in <xref
              href="remove_osd_node.dita#remove_osd_node">Removing Object Storage Daemon (OSD)
              Node</xref>. </li>
        </ol></p>
    </section>
  </body>
</topic>
