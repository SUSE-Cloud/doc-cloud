<?xml version="1.0" encoding="UTF-8"?>
<!--Edit status: not edited-->
<!DOCTYPE topic PUBLIC "-//OASIS//DTD DITA Topic//EN" "topic.dtd">
<topic id="restart_ceph">
  <title><ph conkeyref="HOS-conrefs/product-title"/>Recovering Ceph Hosts After a Reboot</title>
  <abstract><shortdesc outputclass="hdphidden">Steps for recovering Ceph hosts after a
      reboot.</shortdesc>If you have one or more Ceph hosts that have been rebooted, these steps
    will assist you in the recovery.</abstract>
  <body><!--Jeremy tested 7/11/2016-->
    <p conkeyref="HOS-conrefs/applies-to"/>
    <section id="steps"><title>Recovering Ceph Hosts After a Reboot</title>
      <ol>
        <li>Reboot the Ceph monitor node.</li>
        <li>From the lifecycle manager, once the rebooted Ceph monitor node has succesfully booted,
          wait approximately one minute and then execute the following playbook:
          <codeblock>cd ~/scratch/ansible/next/hos/ansible
ansible-playbook -i hosts/verb_hosts ceph-status.yml --limit &lt;monitor-hostname&gt;</codeblock></li>
        <li>If the playbook executes successfully, repeat the steps above for each Ceph monitor node
          that was rebooted.</li>
        <li>Once all the Ceph monitor nodes have been rebooted and are confirmed to be up then
          execute the command below on each monitor node: <codeblock>ceph quorum_status</codeblock>
          <p>Ensure that all of the monitor nodes have joined the quorum by checking that the
              <codeph>quorum_names</codeph> section in the output from the above command lists all
            of the monitors in the Ceph cluster.</p></li>
        <li>Next, reboot the OSD node in the cluster.</li>
        <li>Once the OSD node comes up, wait approximately five minutes and then execute the
          following playbook from the lifecycle manager:
          <codeblock>cd ~/scratch/ansible/next/hos/ansible
ansible-playbook -i hosts/verb_hosts ceph-status.yml --limit &lt;OSD-hostname&gt;</codeblock></li>
        <li>If the playbook executes successfully, repeat steps 5-6 for each OSD node in the
          cluster.</li>
        <li>Once the entire cluster has been rebooted, check the health of the Ceph cluster by
          executing the below command from any OSD node, and ensuring that output is
            <codeph>HEALTH_OK</codeph>: <codeblock>ceph health</codeblock></li><!-- The previously listed command 'ceph health details' produces a usage error regarding 'Invalid Command: unused arguments: details'. The command 'ceph health' provides the 'HEALTH_OK' response -->
      </ol>
    </section>
  </body>
</topic>
