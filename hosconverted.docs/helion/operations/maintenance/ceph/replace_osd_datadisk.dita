<?xml version="1.0" encoding="UTF-8"?>
<!--Edit status: not edited-->
<!DOCTYPE topic PUBLIC "-//OASIS//DTD DITA Topic//EN" "topic.dtd">
<topic id="topic_ryt_gt4_zv">
  <title><ph conkeyref="HOS-conrefs/product-title"/>Replacing the Data Disk in a Ceph OSD
    Node</title>
  <abstract><shortdesc outputclass="hdphidden">Maintenance steps for replacing a data disk in a Ceph
      OSD node.</shortdesc>
  </abstract>
  <body>
    <p>The following steps can be used to replace a failed data disk in a Ceph OSD node.</p>
    <ol>
      <li>Once the failed data disk has been identified, determine what the corresponding journal
        partition and the affected OSD ID is by SSH'ing to the Ceph OSD node and using this command: <codeblock>sudo ceph-disk list |grep &lt;failed_disk></codeblock>
        <p>Example, where the failed data disk is <codeph>/dev/sde</codeph>:</p>
        <codeblock>sudo ceph-disk list |grep /dev/sde
/dev/sde :
 /dev/sde1 ceph data, active, cluster ceph, osd.5, journal /dev/sdg3
 /dev/sdg3 ceph journal, for /dev/sde1</codeblock>
        <p>In this example, the affected OSD ID is <codeph>5</codeph> and the journal partition is
            <codeph>/dev/sdg3</codeph>.</p></li>
      <li>Determine that the OSD that is consuming the failed drive is still running by using this
        command:
        <!--Let's add some output for this command. Show what a running vs not running OSD looks like--><codeblock>ceph osd tree --cluster &lt;ceph-cluster> | grep -i osd.&lt;osd-number></codeblock></li>
      <li>If the OSD is still running, check the status and stop the OSD service by using this
        command:
        <codeblock>sudo systemctl status ceph-osd@&lt;osd-number>
sudo systemctl stop ceph-osd@&lt;osd-number></codeblock></li>
      <li>Check if the OSD disk is mounted and if it is, unmount
        it:<codeblock>sudo df -h
sudo umount &lt;source> | &lt;directory></codeblock></li>
      <li>You will also need to remove the mount instruction for the failed disk from the fstab
        file. You can determine what the failed drive's UUID is by using this command:<!--This resulted in no output for me. Running it without the pipe to grep returned UUIDs though...--><codeblock>sudo blkid | grep "ceph data"</codeblock>
        <p>Example output, where the UUID <codeph>51c32ca1-0ee1-4c75-bfbc-5d113a4b4402</codeph>
          corresponds to the data disk being replaced in our series of examples
            (<codeph>/dev/sde</codeph>).</p>
        <codeblock>$ sudo blkid |grep "ceph data"
/dev/sdc1: UUID="45b7c7e8-4f2d-4226-989b-c83e7e9e2596" TYPE="xfs" PARTLABEL="ceph data" PARTUUID="6c37a311-eabf-4872-bd8e-7d426585432e"
/dev/sdf1: UUID="1cfb4de2-fe21-4e20-9a6e-ad45e90949ae" TYPE="xfs" PARTLABEL="ceph data" PARTUUID="209b9af4-8d25-4d60-a430-2934ceca6042"
/dev/sde1: UUID="51c32ca1-0ee1-4c75-bfbc-5d113a4b4402" TYPE="xfs" PARTLABEL="ceph data" PARTUUID="dfcd410d-9c56-4bb1-93c6-496bae29de5f"</codeblock></li>
      <li>Remove the line in the <codeph>/etc/fstab</codeph> file pertaining to the failed
        disk.</li>
      <li>Delete the journal partition for the OSD and instruct the kernel to reload the
        table:<!--I'm unclear on what <partition number> is in the command. When I ran (echo d; echo /dev/sdd1; echo w) | sudo fdisk /dev/sdc I got the error: "fdisk: cannot open /dev/sdc/ not a directory". Also, this command is strange; we're deleting the partition on the failed disk, but the instructions say we're deleting the journal partition, which exists on another disk...--><codeblock>(echo d; echo &lt;partition-number>; echo w) | sudo fdisk &lt;failed_disk>
sudo partprobe</codeblock></li>
      <li>Remove the failed drive and insert a new, replacement, physical drive. After replacing the
        drive, wait for some time so that the new drive becomes stable. Ensure that there are no
        pre-existing partitions on the new disk.</li>
      <li>Verify that replacement data disk gets the same device name and then proceed with the
        steps below to create a new OSD with the replacement data disk.</li>
      <li>Make the OSD out, ensure that the OSD has stopped and if not, stop it with this
        command:<codeblock>ceph osd out osd.&lt;osd-number> --cluster &lt;ceph-cluster>
sudo systemctl status ceph-osd@&lt;osd-number>
sudo systemctl stop ceph-osd@&lt;osd-number></codeblock></li>
      <li>Remove this OSD from the CRUSH
        map:<codeblock>ceph osd crush remove osd.&lt;osd-number> --cluster &lt;ceph-cluster></codeblock></li>
      <li>As soon as it is removed from the CRUSH map, Ceph starts making PG copies that were
        located on the failed disk and it places these PG on other disks. So a recovery process will
        start, which can be observed by periodically executing this command:<codeblock>ceph -s --cluster &lt;ceph-cluster></codeblock>
        <p>Once the <codeph>active+remapped</codeph> and <codeph>active+recovering</codeph> PGs have
          become <codeph>active+clean</codeph> then the data recovery can be considered
          complete.</p></li>
      <li>Now delete the keyrings for that OSD and finally remove the OSD with these
        commands:<codeblock>ceph auth del osd.&lt;osd-number> --cluster &lt;ceph-cluster>
ceph osd rm &lt;osd-number> --cluster &lt;ceph-cluster></codeblock></li>
      <li>Remove the OSD's directories with this command. Ensure you run this against the affected
        OSD node
        only.<codeblock>sudo rmdir /var/lib/ceph/osd/&lt;ceph_cluster>-&lt;osd_number>

where:
&lt;ceph_cluster>	This should be replaced with the name of your Ceph cluster.
&lt;osd_number>	  This should be replaced with the number identified in step #3 earlier.

Example:
In the earlier example, where OSD ID 5 is the affected one (and Ceph cluster name is 'ceph'), following command should be executed:
sudo rmdir  /var/lib/ceph/osd/ceph-5
</codeblock></li>
      <li>Log in to the lifecycle manager.</li>
      <li>Execute the ceph-deploy.yml playbook against the affected OSD node so that the OSD service
        on that node will be
        started:<codeblock>cd ~/scratch/ansible/next/hos/ansible
ansible-playbook -i hosts/verb_hosts ceph-deploy.yml --limit &lt;OSD_node_with_replaced_disk></codeblock></li>
    </ol>
  </body>
</topic>
