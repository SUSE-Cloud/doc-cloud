<?xml version="1.0" encoding="UTF-8"?>
<!--Edit status: Edited-->
<!DOCTYPE topic PUBLIC "-//OASIS//DTD DITA Topic//EN" "topic.dtd">
<topic id="recover_computenode">
  <title><ph conkeyref="HOS-conrefs/product-title"/>Recovering a Compute Node</title>
  <abstract>
    <shortdesc outputclass="hdphidden">If one or more of your compute nodes has experienced an issue
      such as power loss or hardware failure, then you need to perform disaster recovery. Here we
      provide different scenarios and how to resolve them to get your cloud repaired.</shortdesc>
  </abstract>
  <body>
    <!-- Tested by Joel on 7/21-22 -->
    <p conkeyref="HOS-conrefs/applies-to"/>
    <section id="about">
      <p>Typical scenarios in which you will need to recover a compute node include the
        following:</p>
      <ul>
        <li>The node has failed, either because it has shut down has a hardware failure, or for
          another reason.</li>
        <li>The node is working but the <codeph>nova-compute</codeph> process is not responding,
          thus instances are working but you cannot manage them (for example to delete, reboot, and
          attach/detach volumes).</li>
        <li>The node is fully operational but monitoring indicates a potential issue (such as disk
          errors) that require down time to fix.</li>
      </ul>
      <p>We cover these scenarios in the following sections:</p>
      <ul>
        <li><xref href="#recover_computenode/down">What to do if your compute node is
          down</xref></li>
        <li><xref href="#recover_computenode/unplanned">Scenarios involving disk failures on your
            compute nodes</xref></li>
      </ul>
    </section>
    <section id="down">
      <title>What to do if your compute node is down</title>
      <p><b>Compute node has power but is not powered on</b></p>
      <p>If your compute node has power but is not powered on, use these steps to restore the
        node:</p>
      <ol>
        <li>Log in to the lifecycle manager.</li>
        <li>Obtain the name for your compute node in Cobbler:
          <codeblock>sudo cobbler system list</codeblock></li>
        <li>Power the node back up with this playbook, specifying the node name from Cobbler:
          <codeblock>cd ~/helion/hos/ansible
ansible-playbook -i hosts/localhost bm-power-up.yml -e nodelist=&lt;node name></codeblock></li>
      </ol>
      <p><b>Compute node is powered on but services are not running on it</b></p>
      <p>If your compute node is powered on but you are unsure if services are running, you can use
        these steps to ensure that they are running:<ol id="ol_mss_rsp_xv">
          <li>Log in to the lifecycle manager.</li>
          <li>Confirm the status of the compute service on the node with this
            playbook:<codeblock>cd ~/scratch/ansible/next/hos/ansible
ansible-playbook -i hosts/verb_hosts nova-status.yml --limit &lt;hostname></codeblock></li>
          <li>You can start the compute service on the node with this
            playbook:<codeblock>cd ~/scratch/ansible/next/hos/ansible
ansible-playbook -i hosts/verb_hosts nova-start.yml --limit &lt;hostname></codeblock></li>
        </ol></p>
    </section>
    <section id="unplanned">
      <title>Scenarios involving disk failures on your compute nodes</title>
      <p>Your compute nodes should have a minimum of two disks, one that is used for the operating
        system and one that is used as the data disk. These are defined during the installation of
        your cloud, in the <codeph>~/helion/my_cloud/definition/data/disks_compute.yml</codeph> file
        on the lifecycle manager. The data disk(s) are where the <codeph>nova-compute</codeph>
        service lives. Recovery scenarios will depend on whether one or the other, or both, of these
        disks experienced failures.</p>
      <p><b>If your operating system disk failed but the data disk(s) are okay</b></p>
      <p>If you have had issues with the physical volume that nodes your operating system you need
        to ensure that your physical volume is restored and then you can use the following steps to
        restore the operating system:</p>
      <ol>
        <li>Log in to the lifecycle manager.</li>
        <li>Source the administrator credentials: <codeblock>source ~/service.osrc</codeblock></li>
        <li>Obtain the hostname for your compute node, which you will use in subsequent commands
          when <codeph>&lt;hostname></codeph> is
          requested:<codeblock>nova host-list | grep compute</codeblock></li>
        <li>Obtain the status of the <codeph>nova-compute</codeph> service on that node:
          <codeblock>nova service-list --host &lt;hostname></codeblock></li>
        <li>You will likely want to disable provisioning on that node to ensure that
            <codeph>nova-scheduler</codeph> does not attempt to place any additional instances on
          the node while you are repairing it:
          <codeblock>nova service-disable --reason "node is being rebuilt" &lt;hostname> nova-compute</codeblock></li>
        <li>Obtain the status of the instances on the compute
          node:<codeblock>nova list --host &lt;hostname> --all-tenants</codeblock></li>
        <li>Before continuing, you should either evacuate all of the instances off your compute node
          or shut them down. If the instances are booted from volumes, then you can use the
            <codeph>nova evacuate</codeph> or <codeph>nova host-evacuate</codeph> commands to do
          this. See <xref href="../live_migration.dita"/> for more details on how to do this.<p>If
            your instances are not booted from volumes, you will need to stop the instances using
            the <codeph>nova stop</codeph> command. Because the <codeph>nova-compute</codeph>
            service is not running on the node you will not see the instance status change, but the
              <codeph>Task State</codeph> for the instance should change to
              <codeph>powering-off</codeph>.<codeblock>nova stop &lt;instance_uuid></codeblock></p><p>Verify
            the status of each of the instances using these commands, verifying the <codeph>Task
              State</codeph> states
            <codeph>powering-off</codeph>:<codeblock>nova list --host &lt;hostname> --all-tenants
nova show &lt;instance_uuid></codeblock></p></li>
        <li>At this point you should be ready with a functioning hard disk in the node that you can
          use for the operating system. Follow these steps:<ol id="ol_tzh_4rp_xv">
            <li>Obtain the name for your compute node in Cobbler, which you will use in subsequent
              commands when <codeph>&lt;node_name></codeph> is requested:
              <codeblock>sudo cobbler system list</codeblock></li>
            <li>Reimage the compute node with this playbook:
              <codeblock>cd ~/helion/hos/ansible
ansible-playbook -i hosts/localhost bm-reimage.yml -e nodelist=&lt;node name></codeblock></li>
          </ol></li>
        <li>Once reimaging is complete, use the following playbook to configure the operating system
          and start up
          services:<codeblock>cd ~/scratch/ansible/next/hos/ansible
ansible-playbook -i hosts/verb_hosts site.yml --limit &lt;hostname></codeblock></li>
        <li>You should then ensure any instances on the recovered node are in an
            <codeph>ACTIVE</codeph> state. If they are not then use the <codeph>nova start</codeph>
          command to bring them to the <codeph>ACTIVE</codeph>
          state:<codeblock>nova list --host &lt;hostname> --all-tenants
nova start &lt;instance_uuid></codeblock></li>
        <li>Reenable
          provisioning:<codeblock>nova service-enable &lt;hostname> nova-compute</codeblock></li>
        <li>Start any instances that you had stopped
          previously:<codeblock>nova list --host &lt;hostname> --all-tenants
nova start &lt;instance_uuid></codeblock></li>
      </ol>
      <p><b>If your data disk(s) failed but the operating system disk is okay OR if all drives
          failed</b></p>
      <p>In this scenario your instances on the node are lost. First, follow steps 1 to 5 and 8 to 9
        in the previous scenario.</p>
      <p>After that is complete, use the <codeph>nova rebuild</codeph> command to respawn your
        instances, which will also ensure that they receive the same IP
        address:<codeblock>nova list --host &lt;hostname> --all-tenants
nova rebuild &lt;instance_uuid></codeblock></p>
    </section>
  </body>
</topic>
