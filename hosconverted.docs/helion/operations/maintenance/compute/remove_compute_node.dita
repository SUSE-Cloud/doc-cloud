<?xml version="1.0" encoding="UTF-8"?>
<!--Edit status: not edited-->
<!DOCTYPE topic PUBLIC "-//OASIS//DTD DITA Topic//EN" "topic.dtd">
<topic id="remove_compute_node">
  <title><ph conkeyref="HOS-conrefs/product-title"/>Removing a Compute Node</title>
  <abstract><shortdesc outputclass="hdphidden">Removing a Compute node allows you to remove
      capacity.</shortdesc><p>You may have a need to remove a Compute node and these steps will help
      you achieve this.</p>
    <p>The steps involved are:</p></abstract>
  <body>
    <!--not tested-->
    <p conkeyref="HOS-conrefs/applies-to"/>

    <section id="disable_provisioning"><title>Disable Provisioning on the Compute Host</title>
      <ol>
        <li>Get a list of the Nova services running which will provide us with the details we need
          to disable the provisioning on the Compute host you are wanting to remove:
            <codeblock>nova service-list</codeblock><p>Here is an example below. I've highlighted
            the Compute node we are going to remove in the
          examples:</p><codeblock>$ nova service-list
+----+------------------+--------------------------+----------+---------+-------+----------------------------+-----------------+
| Id | Binary           | Host                     | Zone     | Status  | State | Updated_at                 | Disabled Reason |
+----+------------------+--------------------------+----------+---------+-------+----------------------------+-----------------+
| 1  | nova-conductor   | helion-cp1-c1-m1-mgmt    | internal | enabled | up    | 2015-11-22T22:50:43.000000 | -               |
| 10 | nova-scheduler   | helion-cp1-c1-m1-mgmt    | internal | enabled | up    | 2015-11-22T22:50:34.000000 | -               |
| 13 | nova-conductor   | helion-cp1-c1-m3-mgmt    | internal | enabled | up    | 2015-11-22T22:50:43.000000 | -               |
| 16 | nova-conductor   | helion-cp1-c1-m2-mgmt    | internal | enabled | up    | 2015-11-22T22:50:43.000000 | -               |
| 25 | nova-consoleauth | helion-cp1-c1-m1-mgmt    | internal | enabled | up    | 2015-11-22T22:50:38.000000 | -               |
| 28 | nova-scheduler   | helion-cp1-c1-m2-mgmt    | internal | enabled | up    | 2015-11-22T22:50:38.000000 | -               |
| 31 | nova-scheduler   | helion-cp1-c1-m3-mgmt    | internal | enabled | up    | 2015-11-22T22:50:42.000000 | -               |
| 34 | nova-compute     | helion-cp1-comp0001-mgmt | AZ1      | enabled | up    | 2015-11-22T22:50:35.000000 | -               |
<b>| 37 | nova-compute     | helion-cp1-comp0002-mgmt | AZ2      | enabled | up    | 2015-11-22T22:50:44.000000 | -               |</b>
+----+------------------+--------------------------+----------+---------+-------+----------------------------+-----------------+</codeblock></li>
        <li>Disable the Nova service on the Compute node you are wanting to remove which will ensure
          it is taken out of the scheduling rotation:
            <codeblock>nova service-disable --reason "&lt;enter reason here>" &lt;node hostname> nova-compute</codeblock><p>Here
            is an example if I wanted to remove the <codeph>helion-cp1-comp0002-mgmt</codeph> in the
            output
          above:</p><codeblock>$ nova service-disable --reason "hardware reallocation" helion-cp1-comp0002-mgmt nova-compute
+--------------------------+--------------+----------+-----------------------+
| Host                     | Binary       | Status   | Disabled Reason       |
+--------------------------+--------------+----------+-----------------------+
| helion-cp1-comp0002-mgmt | nova-compute | disabled | hardware reallocation |
+--------------------------+--------------+----------+-----------------------+</codeblock></li>
      </ol>
    </section>
    <section id="remove_az"><title>Remove the Compute Host from its Availability Zone</title>
      <p>If you configured the Compute host to be part of an availability zone, these steps will
        show you how to remove it.</p>
      <ol>
        <li>Get a list of the Nova services running which will provide us with the details we need
          to remove a Compute node: <codeblock>nova service-list</codeblock><p>Here is an example
            below. I've highlighted the Compute node we are going to remove in the
          examples:</p><codeblock>$ nova service-list
+----+------------------+--------------------------+----------+---------+-------+----------------------------+-----------------------+
| Id | Binary           | Host                     | Zone     | Status  | State | Updated_at                 | Disabled Reason       |
+----+------------------+--------------------------+----------+---------+-------+----------------------------+-----------------------+
| 1  | nova-conductor   | helion-cp1-c1-m1-mgmt    | internal | enabled | up    | 2015-11-22T22:50:43.000000 | -                     |
| 10 | nova-scheduler   | helion-cp1-c1-m1-mgmt    | internal | enabled | up    | 2015-11-22T22:50:34.000000 | -                     |
| 13 | nova-conductor   | helion-cp1-c1-m3-mgmt    | internal | enabled | up    | 2015-11-22T22:50:43.000000 | -                     |
| 16 | nova-conductor   | helion-cp1-c1-m2-mgmt    | internal | enabled | up    | 2015-11-22T22:50:43.000000 | -                     |
| 25 | nova-consoleauth | helion-cp1-c1-m1-mgmt    | internal | enabled | up    | 2015-11-22T22:50:38.000000 | -                     |
| 28 | nova-scheduler   | helion-cp1-c1-m2-mgmt    | internal | enabled | up    | 2015-11-22T22:50:38.000000 | -                     |
| 31 | nova-scheduler   | helion-cp1-c1-m3-mgmt    | internal | enabled | up    | 2015-11-22T22:50:42.000000 | -                     |
| 34 | nova-compute     | helion-cp1-comp0001-mgmt | AZ1      | enabled | up    | 2015-11-22T22:50:35.000000 | -                     |
<b>| 37 | nova-compute     | helion-cp1-comp0002-mgmt | AZ2      | enabled | up    | 2015-11-22T22:50:44.000000 | hardware reallocation |</b>
+----+------------------+--------------------------+----------+---------+-------+----------------------------+-----------------------+</codeblock></li>
        <li>You can remove the Compute host from the availability zone it was a part of with this
          command:
            <codeblock>nova aggregate-remove-host &lt;availability zone> &lt;nova hostname></codeblock><p>So
            for the same example as the previous step, the <codeph>helion-cp1-comp0002-mgmt</codeph>
            host was in the <codeph>AZ2</codeph> availability zone so I would use this command to
            remove
          it:</p><codeblock>$ nova aggregate-remove-host AZ2 helion-cp1-comp0002-mgmt
Host helion-cp1-comp0002-mgmt has been successfully removed from aggregate 4
+----+------+-------------------+-------+-------------------------+
| Id | Name | Availability Zone | Hosts | Metadata                |
+----+------+-------------------+-------+-------------------------+
| 4  | AZ2  | AZ2               |       | 'availability_zone=AZ2' |
+----+------+-------------------+-------+-------------------------+</codeblock></li>
        <li>You can confirm the last two steps completed successfully by running another
            <codeph>nova service-list</codeph>. <p>Here is an example which confirms that the node
            has been disabled and that it has been removed from the availability zone. I have
            highlighted
          these:</p><codeblock>$ nova service-list
+----+------------------+--------------------------+----------+----------+-------+----------------------------+-----------------------+
| Id | Binary           | Host                     | Zone     | Status   | State | Updated_at                 | Disabled Reason       |
+----+------------------+--------------------------+----------+----------+-------+----------------------------+-----------------------+
| 1  | nova-conductor   | helion-cp1-c1-m1-mgmt    | internal | enabled  | up    | 2015-11-22T23:04:33.000000 | -                     |
| 10 | nova-scheduler   | helion-cp1-c1-m1-mgmt    | internal | enabled  | up    | 2015-11-22T23:04:34.000000 | -                     |
| 13 | nova-conductor   | helion-cp1-c1-m3-mgmt    | internal | enabled  | up    | 2015-11-22T23:04:33.000000 | -                     |
| 16 | nova-conductor   | helion-cp1-c1-m2-mgmt    | internal | enabled  | up    | 2015-11-22T23:04:33.000000 | -                     |
| 25 | nova-consoleauth | helion-cp1-c1-m1-mgmt    | internal | enabled  | up    | 2015-11-22T23:04:28.000000 | -                     |
| 28 | nova-scheduler   | helion-cp1-c1-m2-mgmt    | internal | enabled  | up    | 2015-11-22T23:04:28.000000 | -                     |
| 31 | nova-scheduler   | helion-cp1-c1-m3-mgmt    | internal | enabled  | up    | 2015-11-22T23:04:32.000000 | -                     |
| 34 | nova-compute     | helion-cp1-comp0001-mgmt | AZ1      | enabled  | up    | 2015-11-22T23:04:25.000000 | -                     |
<b>| 37 | nova-compute     | helion-cp1-comp0002-mgmt | nova     | disabled | up    | 2015-11-22T23:04:34.000000 | hardware reallocation |</b>
+----+------------------+--------------------------+----------+----------+-------+----------------------------+-----------------------+</codeblock></li>
      </ol>
    </section>
    <section id="live_migration"><title>Use Live Migration to Move Any Instances on this Host to
        Other Hosts</title>
      <p>
        <ol>
          <li>You will need to verify if the Compute node is currently hosting any instances on it.
            You can do this with the command below:
              <codeblock>nova list --host=&lt;nova hostname> --all_tenants=1</codeblock><p>Here is
              an example below which shows that we have a single running instance on this node
              currently:</p><codeblock>$ nova list --host=helion-cp1-comp0002-mgmt --all_tenants=1
+--------------------------------------+--------+----------------------------------+--------+------------+-------------+-----------------+
| ID                                   | Name   | Tenant ID                        | Status | Task State | Power State | Networks        |
+--------------------------------------+--------+----------------------------------+--------+------------+-------------+-----------------+
| 78fdb938-a89c-4a0c-a0d4-b88f1555c3b9 | paul4d | 5e9998f1b1824ea9a3b06ad142f09ca5 | ACTIVE | -          | Running     | paul=10.10.10.7 |
+--------------------------------------+--------+----------------------------------+--------+------------+-------------+-----------------+</codeblock></li>
          <li>You will likely want to migrate this instance off of this node before removing it. You
            can do this with the live migration functionality within Nova. The command will look
            like this:
              <codeblock>nova live-migration --block-migrate &lt;nova instance ID></codeblock><p>Here
              is an example using the instance in the previous
              step:</p><codeblock>$ nova live-migration --block-migrate 78fdb938-a89c-4a0c-a0d4-b88f1555c3b9</codeblock><p>You
              can check the status of the migration using the same command from the previous
              step:</p><codeblock>$ nova list --host=helion-cp1-comp0002-mgmt --all_tenants=1
+--------------------------------------+--------+----------------------------------+-----------+------------+-------------+-----------------+
| ID                                   | Name   | Tenant ID                        | Status    | Task State | Power State | Networks        |
+--------------------------------------+--------+----------------------------------+-----------+------------+-------------+-----------------+
| 78fdb938-a89c-4a0c-a0d4-b88f1555c3b9 | paul4d | 5e9998f1b1824ea9a3b06ad142f09ca5 | MIGRATING | migrating  | Running     | paul=10.10.10.7 |
+--------------------------------------+--------+----------------------------------+-----------+------------+-------------+-----------------+</codeblock></li>
          <li> Run nova list again
            <codeblock>$ nova list --host=helion-cp1-comp0002-mgmt --all_tenants=1</codeblock> to
            see that the running instance has been
            migrated:<codeblock>+----+------+-----------+--------+------------+-------------+----------+
| ID | Name | Tenant ID | Status | Task State | Power State | Networks |
+----+------+-----------+--------+------------+-------------+----------+
+----+------+-----------+--------+------------+-------------+----------+</codeblock></li>
        </ol>
      </p>
    </section>
    <section><title>Disable Neutron Agents on Node to be Removed</title>
      <p>You should also locate and disable or remove neutron agents. To see the neutron agents
        running:</p>
      <codeblock>$ neutron agent-list | grep NODE_NAME
+--------------------------------------+----------------------+--------------------------+-------+----------------+---------------------------+
| id                                   | agent_type           | host                     | alive | admin_state_up | binary                    |
+--------------------------------------+----------------------+--------------------------+-------+----------------+---------------------------+
| 08f16dbc-4ba2-4c1d-a4a3-a2ff2526ebe4 | L3 agent             | helion-cp1-comp0002-mgmt | :-)   | True           | neutron-l3-agent          |
| dbe4fe11-8f08-4306-8244-cc68e98bb770 | Metadata agent       | helion-cp1-comp0002-mgmt | :-)   | True           | neutron-metadata-agent    |
| f0d262d1-7139-40c7-bdc2-f227c6dee5c8 | Open vSwitch agent   | helion-cp1-comp0002-mgmt | :-)   | True           | neutron-openvswitch-agent |
+--------------------------------------+----------------------+--------------------------+-------+----------------+---------------------------+

$ neutron agent-update --admin-state-down 08f16dbc-4ba2-4c1d-a4a3-a2ff2526ebe4
$ neutron agent-update --admin-state-down dbe4fe11-8f08-4306-8244-cc68e98bb770
$ neutron agent-update --admin-state-down f0d262d1-7139-40c7-bdc2-f227c6dee5c8</codeblock>
    </section>
    <section id="shutdown_node"><title>Shut down or Stop the Nova and Neutron Services on the
        Compute Host</title><p>To perform this step you have a few options. You can SSH into the
        Compute host and run the following commands:</p><codeblock>sudo systemctl stop nova-compute</codeblock><codeblock>sudo systemctl stop neutron-*</codeblock>
      <!-- docs 3068 --> Because the Neutron agent self-registers against Neutron server, you may
      want to prevent the following services from coming back online. Here is how you can get the
      list:<codeblock>sudo systemctl list-units neutron-* --all</codeblock> Here are the
      results:<codeblock>UNIT                                  LOAD        ACTIVE     SUB      DESCRIPTION
neutron-common-rundir.service         loaded      inactive   dead     Create /var/run/neutron
&#8226;neutron-dhcp-agent.service         not-found     inactive   dead     neutron-dhcp-agent.service
neutron-l3-agent.service              loaded      inactive   dead     neutron-l3-agent Service
neutron-lbaasv2-agent.service         loaded      inactive   dead     neutron-lbaasv2-agent Service
neutron-metadata-agent.service        loaded      inactive   dead     neutron-metadata-agent Service
&#8226;neutron-openvswitch-agent.service    loaded      failed     failed   neutron-openvswitch-agent Service
neutron-ovs-cleanup.service           loaded      inactive   dead     Neutron OVS Cleanup Service
        
        LOAD   = Reflects whether the unit definition was properly loaded.
        ACTIVE = The high-level unit activation state, i.e. generalization of SUB.
        SUB    = The low-level unit activation state, values depend on unit type.
        
        7 loaded units listed.
        To show all installed unit files use 'systemctl list-unit-files'.</codeblock>
      For each loaded service issue the command
      <codeblock>sudo systemctl disable &lt;service-name></codeblock> In the above example that
      would be each service, <i>except </i><codeblock>neutron-dhcp-agent.service</codeblock> For
      example: <codeblock>sudo systemctl disable neutron-common-rundir neutron-l3-agent neutron-lbaasv2-agent neutron-metadata-agent neutron-openvswitch-agent</codeblock>
      <!-- end docs 3068 --> Now you can shut down the node:
        <codeblock>sudo shutdown now</codeblock><p>OR</p><p>From the lifecycle manager you can use
        the <codeph>bm-power-down.yml</codeph> playbook to shut the node
        down:</p><codeblock>cd ~/helion/hos/ansible
ansible-playbook -i hosts/localhost bm-power-down.yml -e nodelist=&#60;node name></codeblock><note>The
          <codeph>&#60;node name></codeph> value will be the value corresponding to this node in
        Cobbler. You can run <codeph>sudo cobbler system list</codeph> to retrieve these
        names.</note></section>
    <section id="delete_node"><title>Delete the Compute Host from Nova</title>
      <p>Retrieve the list of Nova services:</p>
      <codeblock>nova service-list</codeblock>
      <p>Here is an example highlighting the Compute host we're going to remove:</p>
      <codeblock>$ nova service-list
+----+------------------+--------------------------+----------+----------+-------+----------------------------+-----------------------+
| Id | Binary           | Host                     | Zone     | Status   | State | Updated_at                 | Disabled Reason       |
+----+------------------+--------------------------+----------+----------+-------+----------------------------+-----------------------+
| 1  | nova-conductor   | helion-cp1-c1-m1-mgmt    | internal | enabled  | up    | 2015-11-22T23:04:33.000000 | -                     |
| 10 | nova-scheduler   | helion-cp1-c1-m1-mgmt    | internal | enabled  | up    | 2015-11-22T23:04:34.000000 | -                     |
| 13 | nova-conductor   | helion-cp1-c1-m3-mgmt    | internal | enabled  | up    | 2015-11-22T23:04:33.000000 | -                     |
| 16 | nova-conductor   | helion-cp1-c1-m2-mgmt    | internal | enabled  | up    | 2015-11-22T23:04:33.000000 | -                     |
| 25 | nova-consoleauth | helion-cp1-c1-m1-mgmt    | internal | enabled  | up    | 2015-11-22T23:04:28.000000 | -                     |
| 28 | nova-scheduler   | helion-cp1-c1-m2-mgmt    | internal | enabled  | up    | 2015-11-22T23:04:28.000000 | -                     |
| 31 | nova-scheduler   | helion-cp1-c1-m3-mgmt    | internal | enabled  | up    | 2015-11-22T23:04:32.000000 | -                     |
| 34 | nova-compute     | helion-cp1-comp0001-mgmt | AZ1      | enabled  | up    | 2015-11-22T23:04:25.000000 | -                     |
<b>| 37 | nova-compute     | helion-cp1-comp0002-mgmt | nova     | disabled | up    | 2015-11-22T23:04:34.000000 | hardware reallocation |</b>
+----+------------------+--------------------------+----------+----------+-------+----------------------------+-----------------------+</codeblock>
      <p>Delete the host from Nova using the command below:</p>
      <codeblock>nova service-delete &lt;service ID></codeblock>
      <p>Following our example above, you would use:</p>
      <codeblock>nova service-delete 37</codeblock>
      <p>Use the command below to confirm that the Compute host has been completely removed from
        Nova:</p>
      <codeblock>nova hypervisor-list</codeblock>
    </section>
    <section id="deletefromneutron"><title>Delete the Compute Host from Neutron</title>
      <p>Multiple Neutron agents are running on the compute node. You have to remove all of the
        agents running on the node using the "neutron agent-delete" command. In the example below,
        the l3-agent, openvswitch-agent and metadata-agent are running:</p>
      <codeblock>$ neutron agent-list | grep NODE_NAME
+--------------------------------------+----------------------+--------------------------+-------+----------------+---------------------------+
| id                                   | agent_type           | host                     | alive | admin_state_up | binary                    |
+--------------------------------------+----------------------+--------------------------+-------+----------------+---------------------------+
| 08f16dbc-4ba2-4c1d-a4a3-a2ff2526ebe4 | L3 agent             | helion-cp1-comp0002-mgmt | :-)   | False          | neutron-l3-agent          |
| dbe4fe11-8f08-4306-8244-cc68e98bb770 | Metadata agent       | helion-cp1-comp0002-mgmt | :-)   | False          | neutron-metadata-agent    |
| f0d262d1-7139-40c7-bdc2-f227c6dee5c8 | Open vSwitch agent   | helion-cp1-comp0002-mgmt | :-)   | False          | neutron-openvswitch-agent |
+--------------------------------------+----------------------+--------------------------+-------+----------------+---------------------------+

$ neutron agent-delete AGENT_ID 

$ neutron agent-delete 08f16dbc-4ba2-4c1d-a4a3-a2ff2526ebe4
$ neutron agent-delete dbe4fe11-8f08-4306-8244-cc68e98bb770
$ neutron agent-delete f0d262d1-7139-40c7-bdc2-f227c6dee5c8</codeblock>
    </section>
    <section id="remove_node"><title>Remove the Compute Host from the servers.yml File and Run the
        Configuration Processor</title>
      <p>Complete these steps from the lifecycle manager to remove the Compute node:</p>
      <ol>
        <li>Log in to the lifecycle manager</li>
        <li>Edit your <codeph>servers.yml</codeph> file in the location below to remove references
          to the Compute node(s) you want to remove:
          <codeblock>~/helion/my_cloud/definition/data/servers.yml</codeblock></li>
        <li>You may also need to edit your <codeph>control_plane.yml</codeph> file to update the
          values for <codeph>member-count</codeph>, <codeph>min-count</codeph>, and
            <codeph>max-count</codeph> if you used those to ensure they reflect the proper number of
          nodes you are using. <p>See <xref keyref="configobj_controlplane">Input Model - Control
              Plane</xref> for more details.</p></li>
        <li>Commit the changes to git:
          <codeblock>git commit -a -m "Remove node &lt;name>"</codeblock></li>
        <li>Run the configuration processor:
            <codeblock>cd ~/helion/hos/ansible
ansible-playbook -i hosts/localhost config-processor-run.yml</codeblock><p>You
            may want to use the <codeph>remove_deleted_servers</codeph> and
              <codeph>free_unused_addresses</codeph> switches to free up the resources when running
            the configuration processor. See <xref keyref="persisteddata">Persisted Data</xref> for
            more
          details.</p><codeblock>cd ~/helion/hos/ansible
ansible-playbook -i hosts/localhost config-processor-run.yml -e remove_deleted_servers="y" -e free_unused_addresses="y"</codeblock></li>
        <li>Update your deployment directory:
          <codeblock>cd ~/helion/hos/ansible
ansible-playbook -i hosts/localhost ready-deployment.yml</codeblock></li>
      </ol>
    </section>
    <section id="remove_cobbler"><title>Remove the Compute Host from Cobbler</title>
      <p>Complete these steps to remove the node from Cobbler:</p>
      <ol>
        <li>Confirm the system name in Cobbler with this command:
          <codeblock>sudo cobbler system list</codeblock></li>
        <li>Remove the system from Cobbler using this command:
          <codeblock>sudo cobbler system remove --name=&lt;node></codeblock></li>
        <li>Run the <codeph>cobbler-deploy.yml</codeph> playbook to complete the process:
          <codeblock>cd ~/helion/hos/ansible
ansible-playbook -i hosts/localhost cobbler-deploy.yml</codeblock></li>
      </ol>
    </section>
    <section id="remove_monitoring"><title>Remove the Compute Host from Monitoring</title>
      <p>Once you have removed the Compute nodes, the alarms against them will trigger so there are
        additional steps to take to resolve this issue.</p>
      <p>You will want to SSH to each of the Monasca API servers and edit the
          <codeph>/etc/monasca/agent/conf.d/host_alive.yaml</codeph> file to remove references to
        the Compute node you removed. This will require <codeph>sudo</codeph> access. The entries
        will look similar to the one below:</p>
      <codeblock>- alive_test: ping
  built_by: HostAlive
  host_name: helion-cp1-comp0001-mgmt
  name: helion-cp1-comp0001-mgmt ping</codeblock>
      <p>Once you have removed the references on each of your Monasca API servers you then need to
        restart the monasca-agent on each of those servers with this command:</p>
      <codeblock>sudo service monasca-agent restart</codeblock>
      <p>With the Compute node references removed and the monasca-agent restarted, you can then
        delete the corresponding alarm to finish this process. To do so we recommend using the
        Monasca CLI which should be installed on each of your Monasca API servers by default:</p>
      <codeblock>monasca alarm-list --metric-dimensions hostname=&#60;compute node deleted></codeblock>
      <p>For example, if your Compute node looked like the example above then you would use this
        command to get the alarm ID:</p>
      <codeblock>monasca alarm-list --metric-dimensions hostname=helion-cp1-comp0001-mgmt</codeblock>
      <p>You can then delete the alarm with this command:</p>
      <codeblock>monasca alarm-delete &#60;alarm ID></codeblock>
    </section>
  </body>
</topic>
