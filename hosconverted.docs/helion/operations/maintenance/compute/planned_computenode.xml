<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE topic PUBLIC "-//OASIS//DTD DITA Topic//EN" "topic.dtd">
<topic id="planned_computenode">
  <title><ph conkeyref="HOS-conrefs/product-title"/>Planned Maintenance for a Compute Node</title>
  <abstract>
    <shortdesc outputclass="hdphidden">If one or more of your compute nodes needs hardware
      maintenance and you can schedule a planned maintenance then this procedure should be
      followed.</shortdesc>
  </abstract>
  <body>
    <section>
      <p>If one or more of your compute nodes needs hardware maintenance and you can schedule a
        planned maintenance then this procedure should be followed.</p>
    </section>
    <section>
      <title>Performing planned maintenance on a compute node</title>
      <p>If you have planned maintenance to perform on a compute node, you have to take it offline,
        repair it, and restart it. To do so, follow these steps:</p>
      <ol id="ol_ufd_y3y_kx">
        <li>Log in to the lifecycle manager.</li>
        <li>Source the administrator credentials: <codeblock>source ~/service.osrc</codeblock></li>
        <li>Obtain the hostname for your compute node, which you will use in subsequent commands
          when <codeph>&lt;hostname></codeph> is
            requested:<codeblock>nova host-list | grep compute</codeblock><p>The following example
            shows two compute
          nodes:</p><codeblock>$ nova host-list | grep compute
| helion-cp1-comp0001-mgmt | compute     | AZ1      |
| helion-cp1-comp0002-mgmt | compute     | AZ2      |</codeblock></li>
        <li>Disable provisioning on the compute node, which will prevent additional instances from
          being spawned on it:
            <codeblock>nova service-disable --reason "Maintenance mode" &lt;hostname> nova-compute</codeblock><note>Make
            sure you reenable provisioning after the maintenance is complete if you want to continue
            to be able to spawn instances on the node. You can do this with the <codeph>nova
              service-enable &lt;hostname> nova-compute</codeph> command.</note></li>
        <li>At this point you have two choices: <ol id="ol_vfd_y3y_kx">
            <li><b>Live migration</b>: This option enables you to migrate the instances off the
              compute node with minimal downtime so you can perform the maintenance without risk of
              losing data.</li>
            <li><b>Stop/start the instances</b>: Issuing <codeph>nova stop</codeph> commands to each
              of the instances will halt them. This option lets you do maintenance and then start
              the instances back up, as long as no disk failures occur on the compute node data
              disks. This method involves downtime for the length of the maintenance.</li>
          </ol><p>If you choose the live migration route, See <xref href="../live_migration.dita"/>
            for more details. Skip to step #6 after you finish live migration.</p><p>If you choose
            the stop start method, continue on.</p><ol id="ol_wfd_y3y_kx">
            <li>List all of the instances on the node so you can issue stop commands to them:
              <codeblock>nova list --host &lt;hostname> --all-tenants</codeblock></li>
            <li>Issue the <codeph>nova stop</codeph> command against each of the instances:
              <codeblock>nova stop &lt;instance uuid></codeblock></li>
            <li>Confirm that the instances are stopped. If stoppage was successful you should see
              the instances in a <codeph>SHUTOFF</codeph> state, as shown here:
              <codeblock>$ nova list --host helion-cp1-comp0002-mgmt --all-tenants
+--------------------------------------+-----------+----------------------------------+---------+------------+-------------+-----------------------+
| ID                                   | Name      | Tenant ID                        | Status  | Task State | Power State | Networks              |
+--------------------------------------+-----------+----------------------------------+---------+------------+-------------+-----------------------+
| ef31c453-f046-4355-9bd3-11e774b1772f | instance1 | 4365472e025c407c8d751fc578b7e368 | <b>SHUTOFF</b> | -          | Shutdown    | demo_network=10.0.0.5 |
+--------------------------------------+-----------+----------------------------------+---------+------------+-------------+-----------------------+</codeblock></li>
            <li>Do your required maintenance. If this maintenance does not take down the disks
              completely then you should be able to list the instances again after the repair and
              confirm that they are still in their <codeph>SHUTOFF</codeph> state:
              <codeblock>nova list --host &lt;hostname> --all-tenants</codeblock></li>
            <li>Start the instances back up using this command:
                <codeblock>nova start &lt;instance uuid></codeblock><p>Example:</p><codeblock>$ nova start ef31c453-f046-4355-9bd3-11e774b1772f
Request to start server ef31c453-f046-4355-9bd3-11e774b1772f has been accepted.</codeblock></li>
            <li>Confirm that the instances started back up. If restarting is successful you should
              see the instances in an <codeph>ACTIVE</codeph> state, as shown here:
              <codeblock>$ nova list --host helion-cp1-comp0002-mgmt --all-tenants
+--------------------------------------+-----------+----------------------------------+--------+------------+-------------+-----------------------+
| ID                                   | Name      | Tenant ID                        | Status | Task State | Power State | Networks              |
+--------------------------------------+-----------+----------------------------------+--------+------------+-------------+-----------------------+
| ef31c453-f046-4355-9bd3-11e774b1772f | instance1 | 4365472e025c407c8d751fc578b7e368 | <b>ACTIVE</b> | -          | Running     | demo_network=10.0.0.5 |
+--------------------------------------+-----------+----------------------------------+--------+------------+-------------+-----------------------+</codeblock></li>
            <li>If the <codeph>nova start</codeph> fails, you can try doing a hard reboot:
                <codeblock>nova reboot --hard &lt;instance uuid></codeblock><p>If this does not
                resolve the issue you may want to contact support.</p></li>
          </ol></li>
        <li>Reenable provisioning when the node is fixed:
          <codeblock>nova service-enable &lt;hostname> nova-compute</codeblock></li>
      </ol>
    </section>
  </body>
</topic>
