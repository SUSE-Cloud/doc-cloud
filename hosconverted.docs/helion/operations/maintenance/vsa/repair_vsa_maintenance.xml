<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE topic PUBLIC "-//OASIS//DTD DITA Topic//EN" "topic.dtd">
<topic id="topic_e32_tm2_rt">
  <title><ph conkeyref="HOS-conrefs/product-title"/>Repairing a VSA Node</title>
  <abstract>
    <shortdesc outputclass="hdphidden">Repairing a VSA node temporarily for maintenance.</shortdesc>
    <p>This process is used when you want to remove a VSA node from a cluster or management group
      for maintenance, using the HPE StoreVirtual Management Console (CMC) utility.</p>
    <p>After maintenance, the VSA node is added back to the cluster.</p>
  </abstract>
  <body>
    <!--not tested-->
    <p conkeyref="HOS-conrefs/applies-to"/>
    <section id="notes">
      <title>Notes</title>
      <p>In order for the Block Storage volumes to remain available even after the VSA storage node
        goes offline, you must ensure that the VSA network RAID configuration/data protection
        aspects are taken care of prior to removing a VSA node. See the following sections of the
        HPE StoreVirtual Storage Online Help (available in the StoreVirtual Management Console)
        prior to configuring RAID:</p>
      <ul>
        <li>Using disk RAID with Network RAID in a cluster</li>
        <li>Planning the RAID configuration</li>
        <li>Data protection</li>
      </ul>
      <p>You must configure RAID before adding the storage system to the management group. You must
        set the data protection level while creating the volume type.</p>
      <p>See the following pages for more information:</p>
      <ul>
        <li><xref
            href="http://docs.openstack.org/liberty/config-reference/content/HP-LeftHand-StoreVirtual-driver.html"
            format="html" scope="external">HPE LeftHand/StoreVirtual driver</xref></li>
        <li><xref href="../../blockstorage/creating_voltype.dita"/></li>
      </ul>
    </section>
    <section>
      <title>Repair a Disk used by VSA</title>
      <p>Repairing a storage system allows you to perform maintenance on or repair a storage system
        that contains volumes configured for data protection levels other than Network RAID-0.
        Repairing a storage system has the advantage of triggering only one resynchronization of the
        data on the storage system, rather than a complete restripe of the data on the cluster.
        Resynchronizing the data is a shorter operation than a restripe.</p>
      <p>
        <ol id="ol_qxx_3gy_qw">
          <li>[OPTIONAL] If manager is running on the VSA node you are going to repair then you have
            to first stop manager. To stop manager do the following:<ol id="ol_qkq_qgy_qw">
              <li>Right click on VSA node and select Stop Manager. It will pop-up warning "Are you
                sure you want to stop the manager running on ..". </li>
              <li>Click OK to stop manager. <p>Once the manger is stopped, start a manager on a
                  different VSA node where manager is not already running to maintain quorum and the
                  best fault tolerance, if necessary. To start manager, right click on VSA node and
                  select Start Manager. Please ensure there is an odd number of managers running to
                  avoid split brain syndrome.</p></li>
            </ol></li>
          <li>Right-click the storage system, and select Repair Storage System.</li>
          <li>From the Repair Storage System window, select the item that describes the problem to
            solve. See the "Repairing a storage system" section of the <b>HPE StoreVirtual Storage
              Online Help</b> (available in the StoreVirtual Management Console) for more
            information.</li>
        </ol>
      </p>
    </section>
    <section>
      <title>Repair NIC of Host Machine Running VSA</title>
      <p>Due to network issue, you will lose connectivity to VSA. Thus showing Storage system (VSA)
        offline in CMC. This is more of physical NIC failure of Host machine. Repair the NIC of Host
        machine and reboot.</p>
    </section>
    <section>
      <title>Repair VSA Appliance Networking Issues</title>
      <p>Due to network issue, you will lose connectivity to VSA. Thus showing Storage system (VSA)
        offline in CMC. Login to VSA node and try to fix networking issue using virsh
        commands.<codeblock>sudo virsh --help</codeblock></p>
    </section>
    <section>
      <title>Excluding the VSA Node while Maintenance</title>
      <p>Even after the VSA node is under maintenance, it should continue to remain in the<codeph>
          data/servers.yml</codeph> file. You should also create a file (for example offline-vsa) on
        the lifecycle manager with a list of VSA nodes that are currently offline, so that these
        nodes get excluded from any further reconfiguration or upgrade. Perform the following steps
        to remove the VSA node for maintenance:<ol id="ol_tsn_4tf_rw">
          <li>Login to the lifecycle manager.</li>
          <li> Verify the node that needs to be kept under maintenance from <codeph>
              ~/scratch/ansible/next/hos/ansible/host_vars</codeph>
            directory.<codeblock>ls ~/scratch/ansible/next/hos/ansible/host_vars</codeblock></li>
          <li> Create a file (for example: offline-vsa) and enter the node information that needs to
            be placed under maintenance. For example, you want to place
              <codeph>helion-cp1-vsa0004-mgmt</codeph> under maintenance, you must use the host name
            along with ! (for example:
            <codeph>!helion-cp1-vsa0004-mgmt</codeph>).<codeblock>cat offline-vsa
!helion-cp1-vsa0004-mgmt</codeblock></li>
          <li>When performing Helion lifecycle management deployments/reconfigurations/upgrades or
            any other operation, specify the name of this file (prepend @ to the file ) in a --limit
            option to prevent the access of the VSA nodes that are offline till maintenance activity
            is
              done.<codeblock>ansible-playbook -i hosts/verb_hosts vsa-status.yml --limit &lt;@file location></codeblock><p>In
              the following example offline-vsa file has VSA node name and the file is created under
                <codeph>/home/stack/</codeph>.</p><codeblock>ansible-playbook -i hosts/verb_hosts vsa-status.yml --limit @/home/stack/offline-vsa</codeblock></li>
        </ol></p>
    </section>
    <section>
      <title>Host Operating System Corrupted</title>
      <p>If operating system of Host machine where VSA appliance is running gets corrupted then you
        have to reimage VSA node and  re-deploy VSA appliance on this node. VSA node IP and VSA
        appliance IP remain same. Please consult HPSD team for this. OS recovery is performed with
        HPSD support intervention.</p>
    </section>
  </body>
</topic>
