<?xml version="1.0" encoding="UTF-8"?>
<!--Edit status: not edited-->
<!DOCTYPE topic PUBLIC "-//OASIS//DTD DITA Topic//EN" "topic.dtd">
<topic id="remove_vsa">
  <title><ph conkeyref="HOS-conrefs/product-title"/>Removing a VSA Node</title>
  <abstract><shortdesc outputclass="hdphidden">Removing a VSA node allows you to remove
      capacity.</shortdesc></abstract>
  <body>
    <!-- Joel tested 5/1/2016 -->
    <p conkeyref="HOS-conrefs/applies-to"/>

    <p>The process for removing VSA nodes from your cloud environment involves two major steps:</p>
    <p>
      <ul>
        <li>Removing the VSA node(s) from your input model</li>
        <li>Updating your VSA management group and cluster to remove the node(s)</li>
      </ul>
    </p>
    <p>Once you have completed the removal of your VSA node(s) you will also want to remove those
      hosts from your monitoring checks and we show how to do that as well.</p>

    <section id="notes"><title>Notes</title>
      <p>While removing VSA storage system permanently, the operator needs to ensure there is an odd
        number of managers running to avoid split brain syndrome. For example, if there are five
        nodes and one node is getting removed the operator needs to stop the manager running on any
        of the remaining four nodes.</p>
    </section>

    <section id="howto"><title>Removing a VSA Node</title>
      <p>Perform the following steps to remove the VSA node from your cloud.</p>
      <ol>
        <li>Log in to the lifecycle manager.</li>
        <li>Edit your <codeph>~/helion/my_cloud/definition/data/servers.yml</codeph> file and
          comment out or remove the reference to the VSA node you are removing from your
          environment.</li>
        <li>Commit your configuration to the <xref href="../../../installation/using_git.dita">local
            git repo</xref>, as follows:
          <codeblock>git commit -a -m "Removing VSA node"</codeblock></li>
        <li>Run the configuration processor:
          <codeblock>cd ~/helion/hos/ansible
ansible-playbook -i hosts/localhost config-processor-run.yml</codeblock></li>
        <li>Update your deployment directory:
          <codeblock>cd ~/helion/hos/ansible
ansible-playbook -i hosts/localhost ready-deployment.yml</codeblock></li>
        <li>Update Cobbler to remove the node using these steps. <ol>
            <li>List out your current systems in Cobbler:
              <codeblock>sudo cobbler system list</codeblock></li>
            <li>Remove them with this command:
              <codeblock>sudo cobbler system remove --name=&lt;node></codeblock></li>
            <li>Run the following command to update
              Cobbler:<codeblock>sudo cobbler sync</codeblock></li>
          </ol>
        </li>
        <li>Ensure that the node you removed is also removed in the
            <codeph>~/scratch/ansible/next/hos/ansible/hosts/verb_hosts</codeph> file.</li>
      </ol>
    </section>

    <section id="update_cluster"><title>How to remove a VSA node from your management group and
        cluster</title>

      <p>You can remove VSA node(s) in an automated process using the provided Ansible playbooks
        (using the <xref href="../../../installation/configure_vsa.dita#config_vsa/create_cluster"/>
        instructions) only if you had used the Ansible playbook method initially to create the
        cluster. If you used the HPE StoreVirtual Centralized Management Console (CMC) GUI to create
        the cluster, you must continue to use the CMC utility to remove node(s) and manage your
        cluster. </p>
      <p>
        <note>Using the Ansible playbook method to perform cluster operations is highly
          recommended.</note>
      </p>
      <p><b>Using Ansible</b></p>
      <p>If you created the VSA cluster using Ansible playbook, you can remove VSA from that cluster
        using the same playbook.</p>
      <p>Perform the following steps:</p>
      <ol>
        <li>Log in to the lifecycle manager.</li>
        <li>Run the following playbook, which will update both your management group and your
          cluster:
          <codeblock>cd ~/scratch/ansible/next/hos/ansible
ansible-playbook -i hosts/verb_hosts vsalm-configure-cluster.yml</codeblock></li>
        <li>When prompted, enter an administrative user name and password you will use to administer
          your cluster via the CMC utility, should it be needed in the future.</li>
        <li>Confirm in the output of the playbook that your node was removed.</li>
        <li>You can view the added node to the cluster in CMC GUI as follows: <ol id="ol_r5f_ll3_wv">
            <li>Launch the CMC utility. See <xref
                href="../../../installation/configure_vsa.dita#config_vsa/cmc">Launching the
                CMC utility GUI</xref> for more details.</li>
            <li>The removed node is not present in the cluster.</li>
          </ol></li>
      </ol>
      <p outputclass="expandcode"><b>Show | Hide</b> Using the CMC Utility</p>
      <p outputclass="hiddencode">
        <ol>
          <li>Launch the CMC utility. You will likely want to use the same method you used when you
            did your initial VSA cluster configuration. See <xref
              href="../../../installation/configure_vsa.dita#config_vsa/cmc">Launching the
              CMC utility GUI</xref> for more details.</li>
          <li>[OPTIONAL] If manager is running on the VSA node you want to remove from the cluster
            then you have to first stop manager. <p>To stop manager, right-click on the VSA node and
              select <codeph>Stop Manager</codeph>. It will give a pop-up warning stating "Are you
              sure you want to stop the manager running on ..". Go ahead and click OK to stop
              manager. Start a manager on a different VSA node where manager is not already running
              to maintain quorum and the best fault tolerance, if necessary. To start manager, Right
              click on VSA node and select Start Manager. Please ensure there is an odd number of
              managers running to avoid split brain syndrome.</p></li>
          <li>Remove the VSA node from the cluster, using these steps: <ol>
              <li>Right click on the VSA node and select <codeph>Remove from Cluster</codeph>
                <p><image href="../../../../media/hos.docs/operations/remove_vsa_node_30_1.jpg"
                  /></p></li>
              <li>In the Remove Storage Systems from Cluster window, select the specific storage
                system from the list and click OK: <p><image
                    href="../../../../media/hos.docs/operations/remove_vsa_node_30_2.jpg"/></p></li>
              <li>Click Continue to continue with the storage system removal operation: <p><image
                    href="../../../../media/hos.docs/operations/remove_vsa_node_30_3.jpg"/></p></li>
            </ol></li>
          <li>Remove the VSA node from the management group, using these steps: <ol>
              <li>Right click on the VSA node and select Remove from Management Group: <p><image
                    href="../../../../media/hos.docs/operations/remove_vsa_node_30_4.jpg"/></p></li>
              <li>Select a system to remove from the management group and click Remove from
                Management Group: <p><image
                    href="../../../../media/hos.docs/operations/remove_vsa_node_30_5.jpg"/></p></li>
              <li>It will ask for confirmation. Click OK: <p><image
                    href="../../../../media/hos.docs/operations/remove_vsa_node_30_6.jpg"/></p></li>
            </ol></li>
          <li>After successful removal of the VSA node, it will be available under the Available
            Systems list: <p><image
                href="../../../../media/hos.docs/operations/remove_vsa_node_30_7.jpg"/></p></li>
        </ol>
      </p>
    </section>

    <section id="remove_monitoring"><title>How to remove a VSA node from your monitoring
        checks</title>
      <p>Once you have removed the VSA node from your cloud infrastructure, the Host Status alarm
        against it will trigger so there are additional steps to take to resolve this issue.</p>
      <ol>
        <li>SSH to each of the Monasca API servers in your environment individually and do each of
          the following steps on them.</li>
        <li>Edit the <codeph>/etc/monasca/agent/conf.d/host_alive.yaml</codeph> file to remove
          references to the VSA node you removed. This will require <codeph>sudo</codeph> access.
          The entries will look similar to the one below:
          <codeblock>- alive_test: ping
 built_by: HostAlive  
 host_name: helion-cp1-vsa0001-mgmt
 name: helion-cp1-vsa0001-mgmt ping</codeblock></li>
        <li>Once you have removed the references on each of your Monasca API servers you then need
          to restart the monasca-agent on each of those servers with this command:
          <codeblock>sudo service monasca-agent restart</codeblock></li>
        <li>Repeat steps 2-3 for each of your Monasca API servers.</li>
        <li>With the VSA node references removed and the monasca-agent restarted on each of your
          Monasca API hosts, you can then delete the corresponding alarm to finish this process. To
          do so we recommend using the Monasca CLI which should be installed on each of your Monasca
          API servers by default. <p>This command will list out all the associated alarms for the
            VSA node you removed:</p>
          <codeblock>monasca alarm-list --metric-dimensions hostname=&#60;vsa node deleted></codeblock>
          <p>For example, if your VSA node looked like the example above then you would use this
            command to get the alarm ID:</p>
          <codeblock>monasca alarm-list --metric-dimensions hostname=helion-cp1-vsa0001-mgmt</codeblock></li>
        <li>You can then delete the alarm with this command:
          <codeblock>monasca alarm-delete &#60;alarm ID></codeblock></li>

      </ol>
    </section>


  </body>
</topic>
