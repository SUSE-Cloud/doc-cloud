<?xml version="1.0" encoding="UTF-8"?>
<!--Edit status: not edited-->
<!DOCTYPE topic PUBLIC "-//OASIS//DTD DITA Topic//EN" "topic.dtd">
<topic id="adding_proxy">
  <title><ph conkeyref="HOS-conrefs/product-title"/>Adding a Swift Proxy, Account, Container (PAC)
    Node</title>
  <abstract><shortdesc outputclass="hdphidden">Steps for adding additional PAC nodes to your Swift
      system.</shortdesc>This topic describes how to add additional Swift proxy, account, and
    container (PAC) servers to an existing system.</abstract>
  <body>
    <!--not tested-->
    <p conkeyref="HOS-conrefs/applies-to"/>

    <section>
      <title>To add a new node</title>
      <p>To add a new node to your cloud, you will need to add it to <codeph>servers.yml</codeph>,
        and then run the scripts that update your cloud configuration. To begin, access the
          <codeph>servers.yml file</codeph> by checking out the Git branch where you are required to
        make the changes:
        <!--You must add a new node details in the <codeph>server.yml</codeph> file. --></p>
      <p>Then, perform the following steps to add a new node:<ol id="ol_p4w_gbr_4t">
          <li>Log in to the lifecycle manager.</li>
          <li>Get the <codeph>servers.yml</codeph> file stored in Git:
            <codeblock>cd ~/helion/my_cloud/definition/data
git checkout site</codeblock></li>
          <li>If not already done, set the weight-step attribute. For instructions, see <xref
              href="../../objectstorage/swift_weight_attribute.xml"/>.</li>
          <li>Add details of new nodes to the <codeph>servers.yml</codeph>
              file:<codeblock>servers:
...
- id: swpac4 
  role: SWPAC-ROLE
  server-group: &lt;server-group-name>
  mac-addr: &lt;mac-address>
  nic-mapping: &lt;nic-mapping-name>
  ip-addr: &lt;ip-address>
  ilo-ip: &lt;ilo-ip-address>
  ilo-user: &lt;ilo-username> 
  ilo-password: &lt;ilo-password></codeblock><p>
              In the above example, only one new server <b>swpac6</b> is added. However, you can add
              multiple servers by providing the server details in the <codeph>servers.yml</codeph>
              file.</p><p>In the entry-scale configurations there is no dedicated Swift PAC cluster.
              Instead, there is a cluster using servers that have a role of
                <codeph>CONTROLLER-ROLE</codeph>. You cannot add <codeph>swpac4</codeph> to this
              cluster because that would change the <codeph>member-count</codeph>. If your system
              does not already have a dedicated Swift PAC cluster you will need to add it to the
              configuration files. For details on how to do this, see <xref
                href="../../../planning/objectstorage/creating_pac_cluster.xml">Creating a Swift
                Proxy, Account, and Container (PAC) Cluster</xref>.</p><p>If using a new PAC nodes
              you must add the PAC node's configuration details in the following yaml
              files:</p><codeblock>control_plane.yml
disks_pac.yml
net_interfaces.yml
servers.yml
server_roles.yml</codeblock><p>You
              can see a good example of this in the example configurations for the mid-scale model
              in the <codeph>~/helion/tech-preview/mid-scale</codeph> directory.</p><p>The following
              steps assume that you have already created a dedicated Swift PAC cluster and that it
              has two members (<b>swpac4</b> and <b>swpac5</b>).</p></li>
          <li>Increase the member count of the Swift PAC cluster, as appropriate. For example, if
            you are adding <b>swpac6</b> and you previously had two Swift PAC nodes, the increased
            member count should be 3 as shown in the following example:<p>
              <codeblock>control-planes:
    - name: control-plane-1
      control-plane-prefix: cp1

  . . .
  clusters:
  . . .
     - name: ....
       cluster-prefix: c2
       server-role: SWPAC-ROLE
       member-count: 3
   . . .
       <!--allocation-policy: strict
       service-components:
         - ntp-client
         - swift-ring-builder
         - swift-proxy
         - swift-account
         - swift-container
         - swift-client--></codeblock>
            </p></li>
          <li>Commit your
              changes:<codeblock>git add -A
git commit -m "Add Node &lt;name>"</codeblock><note>Before
              you run any playbooks, remember that you need to export the encryption key in the
              following environment variable: <codeph>export
                HOS_USER_PASSWORD_ENCRYPT_KEY=&lt;encryption key></codeph>. For instructions, see
                <xref href="../../../installation/installing_swift_object_storage.xml#install_swift"
              />.</note></li>
          <li>Run the configuration
            processor:<codeblock>cd ~/helion/hos/ansible
ansible-playbook -i hosts/localhost config-processor-run.yml</codeblock></li>
          <li>Create a deployment
            directory:<codeblock>cd ~/helion/hos/ansible
ansible-playbook -i hosts/localhost ready-deployment.yml</codeblock></li>
          <li>Configure Cobbler to include the new node and reimage the node (if you are adding
            several nodes, use a comma-separated list for the <codeph>nodelist</codeph>
              argument):<codeblock>ansible-playbook -i hosts/localhost cobbler-deploy.yml
ansible-playbook -i hosts/localhost bm-reimage.yml -e nodelist=&lt;server-id></codeblock><p>In
              the following example, the server id is <b>swpac6</b> (mentioned in step
              3):<codeblock>ansible-playbook -i hosts/localhost cobbler-deploy.yml
ansible-playbook -i hosts/localhost bm-reimage.yml -e nodelist=swpac6</codeblock></p><note>You
              must use the server id as it appears in the <codeph>servers.yml</codeph> file in the
              field &lt;server-id>.</note></li>
          <li>Review the <codeph>cloudConfig.yml</codeph> and
              <codeph>data/control_plane.yml</codeph> files to get the host prefix (for example,
            helion) and the control plane name (for example, cp1). This gives you the hostname of
            the node. Configure the operating
              system:<codeblock>cd ~/scratch/ansible/next/hos/ansible
ansible-playbook -i hosts/verb_hosts osconfig-run.yml --limit &lt;hostname&gt;</codeblock><p>For
              example, for <b>swpac6</b>, the hostname is
              <b>helion-cp1-c2-m3-mgmt</b>:<codeblock>cd ~/scratch/ansible/next/hos/ansible
ansible-playbook -i hosts/verb_hosts osconfig-run.yml --limit helion-cp1-c2-m3-mgmt</codeblock></p></li>
          <li>Validate that the disk drives of the new node are compatible with the disk model used
            by the
            node:<codeblock>cd ~/scratch/ansible/next/hos/ansible
ansible-playbook -i hosts/verb_hosts swift-compare-model-rings.yml</codeblock>If
            any errors occur, correct them. For instructions, see <xref
              href="../../troubleshooting/objectstorage/interpreting_swift_validate_input_model.xml"
            />.</li>
          <li>Run the following playbook to ensure that all other server's host file are updated
            with the new server:
            <codeblock>cd ~/scratch/ansible/next/hos/ansible
ansible-playbook -i hosts/verb_hosts site.yml --tags "generate_hosts_file"</codeblock></li>
          <li>Run the <codeph>hlm-deploy.yml</codeph> playbook to rebalance the rings to include the
            node, deploy the rings, and configure the new node. Do not limit this to just the node
              (<b>swpac6</b>) that you are
            adding:<codeblock>cd ~/scratch/ansible/next/hos/ansible
ansible-playbook -i hosts/verb_hosts hlm-deploy.yml </codeblock></li>
          <li>You may need to perform further rebalances of the rings. For instructions, see the
            "Weight Change Phase of Ring Rebalance" and the "Final Rebalance Phase" sections of
              <xref href="../../objectstorage/input_model_change_existing_rings.xml"
              />.<p><!--Once you have added a new node/server to the system, you must enure that the Swift rings are updated with the new node/server(s) details and the rings are distributed to all Swift servers.--></p></li>
        </ol></p>
    </section>
  </body>
</topic>
