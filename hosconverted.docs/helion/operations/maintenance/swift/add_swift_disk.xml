<?xml version="1.0" encoding="UTF-8"?>
<!--Edit status: not edited-->
<!DOCTYPE topic PUBLIC "-//OASIS//DTD DITA Topic//EN" "topic.dtd">
<topic id="add_swift_disk">
  <title><ph conkeyref="HOS-conrefs/product-title"/>Adding Additional Disks to a Swift Node</title>
  <abstract><shortdesc outputclass="hdphidden">Steps for adding additional disks to any nodes
      hosting Swift services.</shortdesc>You may have a need to add additional disks to a node for
    Swift usage and we can show you how. These steps work for adding additional disks to Swift
    object or proxy, account, container (PAC) nodes. It can also apply to adding additional disks to
    a controller node that is hosting the Swift service, like you would see if you are using one of
    the entry-scale example models.<p>Read through the notes below before beginning the
    process.</p></abstract>
  <body>


    <section id="notes"><title>Notes</title>
      <p>The following are things to keep in mind when increasing your disk capacity in your Swift
        nodes:</p>
      <ul>
        <li>You can add multiple disks at the same time, there is no need to do it one at a
          time.</li>
        <li><b>IMPORTANT!</b> You must add the <b>same</b> number of disks to each server that the
          disk model applies to. So, for example, if you have a single cluster of three Swift
          servers and you want to increase capacity and decide to add two additional disks, you must
          add two to each of your three Swift servers.</li>
      </ul>
    </section>

    <section><title>Adding additional disks to your Swift servers</title>
      <ol>
        <li>Verify the general health of the Swift system and that it is safe to rebalance your
          rings. See <xref href="../../objectstorage/safe_rebalance_deploy_ring.dita"/> for details
          on how to do this.</li>
        <li>Perform the disk maintenance. <ol>
            <li>Shut down the first Swift server you wish to add disks to.</li>
            <li>Add the additional disks to the physical server. The disk drives that are added
              should be clean. They should either contain no partitions or a single partition the
              size of the entire disk. It should not contain a file system or any volume groups.
              Failure to comply will cause errors and the disk will not be added. <p>For more
                details, see <xref href="../../../planning/objectstorage/swift_device_groups.dita"
                />.</p></li>
            <li>Power the server on.</li>
            <li>While the server was shutdown, data that normally would have been placed on the
              server is placed elsewhere. When the server is rebooted, the Swift replication process
              will move that data back onto the server. Monitor the replication process to determine
              when it is complete. See <xref
                href="../../objectstorage/safe_rebalance_deploy_ring.dita"/> for details on how to
              do this.</li>
            <li>Repeat steps a-d for each of the Swift servers you are adding the disks to, one at a
              time.</li>
          </ol> <note>If the additional disks can be added to the Swift servers online (for example, via
          hot plugging) then there is no need to do the last two steps.</note> </li>
        <li>On the lifecycle manager, update your cloud configuration with the details of your
          additional disks. <ol>
            <li>Edit the disk configuration file that correlates to the type of server you are
              adding your new disks to. <p>Path to the typical disk configuration
                files:</p><codeblock>~/helion/my_cloud/definition/data/disks_swobj.yml
~/helion/my_cloud/definition/data/disks_swpac.yml
~/helion/my_cloud/definition/data/disks_controller_*.yml</codeblock><p>Example
                showing the addition of a single new disk, indicated by the
                  <codeph>/dev/sdd</codeph>, in bold:</p><codeblock>device-groups:
  - name: SwiftObject
    devices:
      - name: "/dev/sdb"
      - name: "/dev/sdc"
      <b>- name: "/dev/sdd"</b>
    consumer:
      name: swift
      ...</codeblock>
              <note>For more details, see <xref
                  href="../../../architecture/input_model/configobj/configurationobjects.dita"/> for
                more details on how the disk model works.</note></li>
            <li>Configure the Swift weight-step value in the
                <codeph>~/helion/my_cloud/definition/data/swift/rings.yml</codeph> file. See <xref
                href="../../objectstorage/swift_weight_attribute.dita"/> for details on how to do
              this.</li>
            <li>Commit the changes to
              Git:<codeblock>cd ~helion
git commit -a -m "adding additional Swift disks"</codeblock></li>
            <li>Run the configuration
              processor:<codeblock>cd ~/helion/hos/ansible
ansible-playbook -i hosts/localhost config-processor-run.yml</codeblock></li>
            <li>Update your deployment
              directory:<codeblock>cd ~/helion/hos/ansible
ansible-playbook -i hosts/localhost ready-deployment.yml</codeblock></li>
          </ol>
        </li>
        <li>Run the <codeph>osconfig-run.yml</codeph> playbook against the Swift nodes you have
          added disks to. Use the <codeph>--limit</codeph> switch to target the specific nodes:
            <codeblock>cd ~/scratch/ansible/next/hos/ansible
ansible-playbook -i hosts/verb_hosts osconfig-run.yml --limit &lt;hostnames></codeblock><p>You
            can use a wildcard when specifying the hostnames with the <codeph>--limit</codeph>
            switch. If you added disks to all of the Swift servers in your environment and they all
            have the same prefix (for example, <codeph>helion-cp1-swobj...</codeph>) then you can
            use a wildcard like <codeph>helion-cp1-swobj*</codeph>. If you only added disks to a set
            of nodes but not all of them, you can use a comma deliminated list and enter the
            hostnames of each of the nodes you added disks to.</p></li>
        <li>Validate your Swift configuration with this playbook which will also provide details of
          each drive being added:
          <codeblock>cd ~/scratch/ansible/next/hos/ansible
ansible-playbook -i hosts/verb_hosts swift-compare-model-rings.yml --extra-vars "drive_detail=yes"</codeblock></li>
        <li>Verify that Swift services are running on all of your servers:
          <codeblock>cd ~/scratch/ansible/next/hos/ansible
ansible-playbook -i hosts/verb_hosts swift-status.yml</codeblock></li>
        <li>If everything looks okay with the Swift status, then apply the changes to your Swift
          rings with this playbook:
          <codeblock>cd ~/scratch/ansible/next/hos/ansible
ansible-playbook -i hosts/verb_hosts swift-deploy.yml</codeblock></li>
        <li>At this point your Swift rings will begin rebalancing. You should wait until replication
          has completed or min-part-hours has elapsed (whichever is longer), as described in <xref
            href="../../objectstorage/safe_rebalance_deploy_ring.dita"/> and then follow the "Weight
          Change Phase of Ring Rebalance" process as described in <xref
            href="../../objectstorage/input_model_change_existing_rings.dita"/>.</li>
      </ol>
    </section>
  </body>
</topic>
