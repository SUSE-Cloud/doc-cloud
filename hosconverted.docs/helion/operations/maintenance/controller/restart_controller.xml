<?xml version="1.0" encoding="UTF-8"?>
<!--Edit status: not edited-->
<!DOCTYPE topic PUBLIC "-//OASIS//DTD DITA Topic//EN" "topic.dtd">
<topic id="recover_downed_cluster">
  <title><ph conkeyref="HOS-conrefs/product-title"/>Restarting Controller Nodes After a
    Reboot</title>
  <abstract><shortdesc outputclass="hdphidden">Steps to follow if one or more of your controller
      nodes lose network connectivity or power, which includes if the node is either rebooted or
      needs hardware maintenance.</shortdesc></abstract>
  <body>
    <!--not tested-->
    <p conkeyref="HOS-conrefs/applies-to"/>

    <!-- WHEN, WHY -->
    <section><p>When a controller node is rebooted, needs hardware maintenance, loses network
        connectivity or loses power, these steps will help you recover the node.</p><p>These steps
        may also be used if the Host Status (ping) alarm is triggered for one or more of your
        controller nodes. Here is what the alarm looks like in the Operations Console:</p>
      <image href="../../../../media/hos.docs/opsconsole_hostalarm.png"/>
    </section>
    <section id="expandCollapse">
      <sectiondiv outputclass="expandall">Expand All Sections</sectiondiv>
      <sectiondiv outputclass="collapseall">Collapse All Sections</sectiondiv>
    </section>

    <!-- HOW, WHO, WHERE -->
    <section id="prereqs"><title outputclass="headerH">Prerequisites</title>
      <sectiondiv outputclass="insideSection">
        <p>The following conditions must be true in order to perform these steps successfully:</p>
        <ul>
          <li>Each of your controller nodes should be powered on.</li>
          <li>Each of your controller nodes should have network connectivity, verified by SSH
            connectivity from the lifecycle manager to them.</li>
          <li>The operator who performs these steps will need access to the lifecycle manager.</li>
        </ul>
      </sectiondiv>
    </section>
    <section id="mysql"><title outputclass="headerH">Recovering the MySQL Database</title>
      <sectiondiv outputclass="insideSection">
        <p>The recovery process for your MySQL database cluster will depend on how many of your
          controller nodes need to be recovered. We will cover two scenarios, detailed below:</p>
        <p><b>If you need to recover one or two of your controller nodes but not the entire
            cluster</b></p>
        <p>If you need to recover one or two of your controller nodes but not the entire cluster,
          then use these steps:</p>
        <ol>
          <li>Ensure the controller nodes have power and are booted to the command prompt.</li>
          <li>If the MySQL service is not started, start it with this command:
            <codeblock>sudo service mysql start</codeblock></li>
          <li>If MySQL fails to start using the above command then look in the section below for the
            bootstrap process which should resolve the issue.</li>
        </ol>
        <p><b>If you need to recover the whole controller cluster or the steps above failed in a one
            or two controller node scenario</b></p>
        <p>If you need to recover your entire control plane cluster or if the scenario above failed
          for your one or two controller node issue then use the bootstrap command described below
          to recover the MySQL database.</p>
        <p>The node that you perform these steps on should be the last node to go down. So if only
          one of your controller nodes went down, then you would skip to step #2 and run the
          bootstrap commands on that node. If multiple controller nodes went down then you should
          use the instructions in step #1 to determine which was the last node to go down and then
          proceed to run the rest of the steps on that node.</p>
        <ol>
          <li>Make sure there is no <codeph>mysqld</codeph> daemon running on any node in the
            cluster before you continue with the steps in this procedure. If there is a
              <codeph>mysqld</codeph> daemon running then use the command below to shutdown the
              daemon.<codeblock>sudo service mysql stop</codeblock><p>If the daemon does not go down
              following the service stop, then kill the daemon using <codeph>kill -9</codeph> before
              continuing.</p></li>
          <li>SSH into each controller node that went down and use the commands below to get the log
            sequence number:
              <codeblock>sudo /usr/bin/mysqld_safe --wsrep-recover
sudo grep --text 'Recovered position' /var/log/mysql/error.log| tail -1</codeblock><p>Here
              is an example where I have bolded the log sequence numbers from a three controller
              node cluster:</p><note type="important">In this example, two nodes have the same
              sequence number which means you can run the bootstrap on either node but you can only
              run the bootstrap command once and only on one
            node.</note><codeblock>stack@helion-cp-c1-m1-mgmt:~$ sudo /usr/bin/mysqld_safe --wsrep-recover
stack@helion-cp-c1-m1-mgmt:~$ sudo grep --text 'Recovered position' /var/log/mysql/error.log| tail -1
151119 14:37:12 32123 [Note] WSREP: Recovered position: a0cd6b29-f027-11e5-b9e3-fb0ed503c0ac:<b>165002105</b>

stack@helion-cp-c1-m2-mgmt:~$ sudo /usr/bin/mysqld_safe --wsrep-recover
stack@helion-cp-c1-m2-mgmt:~$ sudo grep --text 'Recovered position' /var/log/mysql/error.log| tail -1
151119 14:37:1214:37:52 32336 [Note] WSREP: Recovered position: a0cd6b29-f027-11e5-b9e3-fb0ed503c0ac:<b>165252407</b>

stack@helion-cp-c1-m3-mgmt:~$  sudo /usr/bin/mysqld_safe --wsrep-recover
stack@helion-cp-c1-m3-mgmt:~$ sudo grep --text 'Recovered position' /var/log/mysql/error.log| tail -1
151119 14:37:1214:37:52 32332 [Note] WSREP: Recovered position: a0cd6b29-f027-11e5-b9e3-fb0ed503c0ac:<b>165252407</b></codeblock></li>
          <li>If the node with the highest log sequence number is a shared lifecycle
            manager/controller node then run this playbook from the lifecycle
            manager:<codeblock>cd ~/scratch/ansible/next/hos/ansible
ansible-playbook -i hosts/verb_hosts percona-bootstrap.yml</codeblock>If
            the node with the highest log sequence number is a standalone controller node then SSH
            into the node and bootstrap the cluster with this command:
            <codeblock>sudo /etc/init.d/mysql bootstrap-pxc</codeblock></li>
          <li>Log back into the lifecycle manager and run this playbook to start MySQL back up:
            <codeblock>cd ~/scratch/ansible/next/hos/ansible
ansible-playbook -i hosts/verb_hosts percona-start.yml</codeblock></li>
          <li>Wait for a few minutes for the services to be fully up and synced.</li>
        </ol>
      </sectiondiv>
    </section>

    <section id="vertica"><title outputclass="headerH">Recovering the Vertica Database</title>
      <sectiondiv outputclass="insideSection">
        <p>In order to recover your Vertica databases, run the following playbook:</p>
        <codeblock>cd ~/scratch/ansible/next/hos/ansible
ansible-playbook -i hosts/verb_hosts monasca-vertica-recovery.yml</codeblock>
        <p>or if you have encryption enabled, use:</p>
        <codeblock>ansible-playbook -i hosts/verb_hosts monasca-vertica-recovery.yml --ask-vault-pass</codeblock>
      </sectiondiv>
    </section>

    <section id="hlm"><title outputclass="headerH">Restarting Services on the Controller
        Nodes</title>
      <sectiondiv outputclass="insideSection">
        <p>From the lifecycle manager you should execute the <codeph>hlm-start.yml</codeph> playbook
          for each node that was brought down so the services can be started back up.</p>
        <p>If you have a dedicated (separate) lifecycle manager node you can use this syntax:</p>
        <codeblock>cd ~/scratch/ansible/next/hos/ansible
ansible-playbook -i hosts/verb_hosts hlm-start.yml --limit=&lt;hostname_of_node&gt;</codeblock>
        <p>If you have a shared lifecycle manager/controller setup and need to restart services on
          this shared node, you can use <codeph>localhost</codeph> to indicate the shared node, like
          this:</p>
        <codeblock>cd ~/scratch/ansible/next/hos/ansible
ansible-playbook -i hosts/verb_hosts hlm-start.yml --limit=&lt;hostname_of_node&gt;,localhost</codeblock>
        <note>If you leave off the <codeph>--limit</codeph> switch, the playbook will be run against
          all nodes.</note>
        <p>Here is an example using the above command using the example in the previous step.</p>
        <codeblock>ansible-playbook -i hosts/verb_hosts hlm-start.yml --limit=helion-cp-c1-m1-mgmt,helion-cp-c1-m2-mgmt,helion-cp-c1-m3-mgmt</codeblock>
      </sectiondiv>
    </section>

    <section id="monasca"><title outputclass="headerH">Restart the Monitoring Agents</title>
      <sectiondiv outputclass="insideSection">
        <p>As part of the recovery process, you should also restart the
            <codeph>monasca-agent</codeph> and these steps will show you how:</p>
        <ol>
          <li>Log in to the lifecycle manager.</li>
          <li>Stop the <codeph>monasca-agent</codeph>:
            <codeblock>cd ~/scratch/ansible/next/hos/ansible
ansible-playbook -i hosts/verb_hosts monasca-agent-stop.yml</codeblock></li>
          <li>Restart the <codeph>monasca-agent</codeph>:
            <codeblock>cd ~/scratch/ansible/next/hos/ansible
ansible-playbook -i hosts/verb_hosts monasca-agent-start.yml</codeblock></li>
          <li>You can then confirm the status of the <codeph>monasca-agent</codeph> with this
            playbook:
            <codeblock>cd ~/scratch/ansible/next/hos/ansible
ansible-playbook -i hosts/verb_hosts monasca-agent-status.yml</codeblock></li>
        </ol>
      </sectiondiv>
    </section>
  </body>
</topic>
