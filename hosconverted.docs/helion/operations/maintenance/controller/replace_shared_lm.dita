<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE topic PUBLIC "-//OASIS//DTD DITA Topic//EN" "topic.dtd">
<topic id="replace_shared_lm">
  <title><ph conkeyref="HOS-conrefs/product-title"/>Replacing a Shared Lifecycle Manager/Controller
    Node</title>
  <abstract><shortdesc outputclass="hdphidden">If the controller node you need to replace was also
      being used as your lifecycle manager then use these steps below. If this is not a shared
      controller then skip to the next section.</shortdesc></abstract>
  <body>
    <p>If the controller node you need to replace was also being used as your lifecycle manager then
      use these steps below. If this is not a shared controller then skip to the next section.</p>
    <ol>
      <li>Ensuring that you use the same version of <keyword keyref="kw-hos"/> that you previously
        had loaded on your lifecycle manager, you will need to download and install the lifecycle
        management software using the instructions from the installation guide: <ol>
          <li><xref href="../../../installation/installing_kvm.dita#install_kvm/setup_deployer">Set
              up the Lifecycle Manager</xref></li>
        </ol></li>
      <li>Then you will want to restore your data using the <xref
          href="ded_lifecyclemanager_recovery.dita">Lifecycle Manager Disaster Recovery</xref>
        instructions.</li>
      <li> Update your cloud model (<codeph>servers.yml</codeph>) with the new
          <codeph>mac-addr</codeph>, <codeph>ilo-ip</codeph>, <codeph>ilo-password</codeph>, or
          <codeph>ilo-user</codeph> fields where these have changed. Do not change the
          <codeph>id</codeph>, <codeph>ip-addr</codeph>, <codeph>role</codeph>, or
          <codeph>server-group</codeph> settings. (Please follow the procedure for updating your
        cloud model in the git repo)</li>
      <li> Get the <b>servers.yml</b> file stored in git:
        <codeblock>cd ~/helion/my_cloud/definition/data
git checkout site</codeblock> then change,
        as necessary, the <codeph>mac-addr</codeph>, <codeph>ilo-ip</codeph>,
          <codeph>ilo-password</codeph>, and <codeph>ilo-user</codeph> fields of this existing
        controller node. Save and commit the change
        <codeblock>git commit -a -m "repaired node X"</codeblock></li>
      <li>Run the configuration processor as follows:
        <codeblock>cd ~/helion/hos/ansible
ansible-playbook -i hosts/localhost config-processor-run.yml</codeblock>
        Then run ready-deployment:
        <codeblock>cd ~/helion/hos/ansible
ansible-playbook -i hosts/localhost ready-deployment.yml</codeblock></li>
      <li>Once that is complete, copy the Cobbler images to the correct location:
        <codeblock>mkdir /srv/www/cobbler/ks_mirror/hlinux-cattleprod 
cp /opt/hlm_packager/hos-3.0.0/hlinux_venv/hlm/hlinux/dists/cattleprod/main/installer-amd64/current/images/netboot/debian-installer/amd64/linux /srv/www/cobbler/ks_mirror/hlinux-cattleprod
cp /opt/hlm_packager/hos-3.0.0/hlinux_venv/hlm/hlinux/dists/cattleprod/main/installer-amd64/current/images/netboot/debian-installer/amd64/initrd.gz /srv/www/cobbler/ks_mirror/hlinux-cattleprod</codeblock></li>
      <li>Deploy Cobbler:
          <codeblock>cd ~/helion/hos/ansible
ansible-playbook -i hosts/localhost cobbler-deploy.yml</codeblock><note>After
          this step you may see failures because MySQL has not finished syncing. If so, please rerun
          this step (7).</note></li>
      <li>Delete the haproxy user: <codeblock>deluser haproxy</codeblock></li>
      <li>Install the software on your new lifecycle manager/controller node with these three
        playbooks:
        <codeblock>cd ~/scratch/ansible/next/hos/ansible
ansible-playbook -i hosts/verb_hosts monasca-rebuild-pretasks.yml
ansible-playbook -i hosts/verb_hosts osconfig-run.yml -e rebuild=True --limit=&lt;controller-hostname&gt;
ansible-playbook -i hosts/verb_hosts hlm-deploy.yml -e rebuild=True --limit=&lt;controller-hostname&gt;,&lt;first-proxy-hostname&gt;</codeblock></li>
      <li>During the replacement of the node there will be alarms that show up during the process.
        If those do not clear after the node is back up and healthy, restart the threshold engine by
        running the following
        playbooks:<codeblock>cd ~/scratch/ansible/next/hos/ansible
ansible-playbook -i hosts/verb_hosts monasca-stop.yaml --tags thresh
ansible-playbook -i hosts/verb_hosts monasca-start.yaml --tags thresh</codeblock></li>
    </ol>
  </body>
</topic>
