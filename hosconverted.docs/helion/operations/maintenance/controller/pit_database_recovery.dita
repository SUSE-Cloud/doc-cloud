<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE topic PUBLIC "-//OASIS//DTD DITA Topic//EN" "topic.dtd">
<topic id="pit_database_recovery">
  <title><ph conkeyref="HOS-conrefs/product-title"/>Point-in-Time MySQL Database Recovery</title>
  <abstract><shortdesc outputclass="hdphidden">In this scenario, everything is still running
      (lifecycle manager, cloud controller nodes, and compute nodes) but you want to restore the
      MySQL database to a previous state.</shortdesc></abstract>
  <body>
    <p>In this scenario, everything is still running (lifecycle manager, cloud controller nodes, and
      compute nodes) but you want to restore the MySQL database to a previous state.</p>
    <section>
      <title>Restore from a Swift backup</title>
      <ol>
        <li>Log in to the lifecycle manager.</li>
        <li>Determine which node is the first host member in the <codeph>FND-MDB</codeph> group,
          which will be the first node hosting the MySQL service in your cloud. You can do this by
          using these
            commands:<codeblock>cd ~/scratch/ansible/next/hos/ansible
grep -A1 FND-MDB--first-member hosts/verb_hosts</codeblock><p>Example
            output:<codeblock>$ grep -A1 FND-MDB--first-member hosts/verb_hosts
[FND-MDB--first-member:children]
helion-cp1-c1-m1</codeblock></p></li>
        <li>Log in to the first host member from the <codeph>FND-MDB</codeph> group.</li>
        <li>Become root: <codeblock>sudo su</codeblock></li>
        <li>Source the backup environment file
          <codeblock>source /home/stack/backup.osrc</codeblock></li>
        <li>List the jobs<codeblock>freezer-scheduler -c &lt;hostname&gt; job-list</codeblock></li>
        <li>Get the id corresponding to the job <codeph>HLM Default: Mysql restore from
            Swift</codeph>.</li>
        <li>Launch the restore.
          <codeblock>freezer-scheduler -c &lt;hostname&gt; job-start -j &lt;job-id&gt;</codeblock></li>
        <li>This will take some time. While you wait you can follow the progress in the
            <codeph>/var/log/freezer-agent/freezer-scheduler.log</codeph> log file.</li>
        <li>Log in to the lifecycle manager.</li>
        <li>Stop the Percona DB service:
          <codeblock>cd ~/scratch/ansible/next/hos/ansible
ansible-playbook -i hosts/verb_hosts percona-stop.yml</codeblock></li>
        <li>Log back in to the first node running the MySQL service. (Same node as step #3)</li>
        <li>Clean the MySQL directory using this command:
          <codeblock>sudo rm -r /var/lib/mysql/*</codeblock></li>
        <li>Copy the restored files back to the MySQL
          directory:<codeblock>sudo cp -pr /tmp/mysql_restore/* /var/lib/mysql</codeblock></li>
        <li>Log in to each of the other nodes in your MySQL cluster (the other nodes determined from
          step #2) and remove the <codeph>grastate.dat</codeph> file from each of
            them:<codeblock>sudo rm /var/lib/mysql/grastate.dat</codeblock><note>Do not remove this
            file from the first node in your MySQL cluster. Ensure you only do this from the other
            cluster nodes.</note></li>
        <li>Log back in to the lifecycle manager.</li>
        <li>Start the Percona DB service back up:
          <codeblock>cd ~/scratch/ansible/next/hos/ansible
ansible-playbook -i hosts/verb_hosts percona-bootstrap.yml</codeblock></li>
      </ol>
    </section>
    <section>
      <title>Restore from an SSH backup</title>
      <p>Follow the same procedure as the one for Swift but select the job <codeph>HLM Default:
          Mysql restore from SSH</codeph>.</p>
    </section>
    <section>
      <title>Restore MySQL manually</title>
      <p>If restoring MySQL fails during the procedure outlined above, you can follow this procedure
        to manually restore MySQL:</p>
      <ol>
        <li>Log in to the lifecycle manager.</li>
        <li>Stop the MySQL cluster:
          <codeblock>cd ~/scratch/ansible/next/hos/ansible
ansible-playbook -i hosts/verb_hosts percona-stop.yml</codeblock></li>
        <li>On all of the nodes running the MySQL service, which should be all of your controller
          nodes, run the following command to purge the old
          database:<codeblock>sudo rm -r /var/lib/mysql/*</codeblock></li>
        <li>On the first node running the MySQL service restore the backup with the command below.
          If you have already restored to a temporary directory, copy the files
          again.<codeblock>sudo cp -pr /tmp/mysql_restore/* /var/lib/mysql</codeblock></li>
        <li>If you need to restore the files manually from SSH, follow these steps: <ol>
            <li>Become root: <codeblock>sudo su</codeblock></li>
            <li>Create the <codeph>/root/mysql_restore.ini</codeph> file with the contents below. Be
              careful to substitute the <codeph>{{ values }}</codeph>. Note that the SSH information
              refers to the SSH server you configured for backup before installing.
              <codeblock>[default]
action = restore
storage = ssh
ssh_host = {{ freezer_ssh_host }}
ssh_username = {{ freezer_ssh_username }}
container = {{ freezer_ssh_base_dir }}/freezer_mysql_backup
ssh_key = /etc/freezer/ssh_key
backup_name = freezer_mysql_backup
restore_abs_path = /var/lib/mysql/
log_file = /var/log/freezer-agent/freezer-agent.log
hostname = {{ hostname of the first MySQL node }}</codeblock></li>
            <li>Execute the restore
              job:<codeblock>freezer-agent --config /root/mysql_restore.ini</codeblock></li>
          </ol></li>
        <li>Also on the first node running the MySQL service, follow the next steps to start the
          cluster. <ol>
            <li>Become root: <codeblock>sudo su</codeblock></li>
            <li>When the last step executed successfully, start the MySQL cluster:
              <codeblock>/etc/init.d/mysql bootstrap-pxc</codeblock></li>
            <li>Start the process with systemctl to make sure the process is monitored by upstard:
              <codeblock>systemctl start mysql</codeblock></li>
            <li>Make sure the mysql process started successfully by getting the status:
              <codeblock>systemctl status mysql</codeblock></li>
          </ol></li>
        <li>Log back in to the lifecycle manager.</li>
        <li>From the lifecycle manager, execute the following playbook to start all MySQL
          instances:<codeblock>cd ~/scratch/ansible/next/hos/ansible
ansible-playbook -i hosts/verb_hosts percona-start.yml</codeblock></li>
        <li>MySQL cluster status can be checked using the <codeph>percona-status.yml
          </codeph>playbook:<codeblock>cd ~/scratch/ansible/next/hos/ansible
ansible-playbook -i hosts/verb_hosts percona-status.yml</codeblock></li>
        <li>On all of the nodes running the MySQL service, run the following commands as
          root:<codeblock>sudo su
touch /var/lib/mysql/galera.initialised
chown mysql:mysql /var/lib/mysql/galera.initialised</codeblock></li>
        <li>After approximately 10-15 minutes, the output of the <codeph>percona-status.yml</codeph>
          playbook should show all the MySQL nodes in sync. MySQL cluster status can be checked
          using this
            playbook:<codeblock>cd ~/scratch/ansible/next/hos/ansible
ansible-playbook -i hosts/verb_hosts percona-status.yml</codeblock><p>An
            example output is as
          follows:</p><codeblock>TASK: [FND-MDB | status | Report status of "{{ mysql_service }}"] ************* 
  ok: [helion-cp1-c1-m1-mgmt] => {
  "msg": "mysql is synced."
  }
  ok: [helion-cp1-c1-m2-mgmt] => {
  "msg": "mysql is synced."
  }
  ok: [helion-cp1-c1-m3-mgmt] => {
  "msg": "mysql is synced."
  }</codeblock></li>
      </ol>
    </section>
  </body>
</topic>
