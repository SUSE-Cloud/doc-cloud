<?xml version="1.0" encoding="UTF-8"?>
<!--Edit status: not edited-->
<!DOCTYPE topic PUBLIC "-//OASIS//DTD DITA Topic//EN" "topic.dtd">
<topic id="stopStartSvcs">
  <title><ph conkeyref="HOS-conrefs/product-title"/>Starting and Stopping Individual
    Services</title>
  <body>
    <!--not tested-->
    <p conkeyref="HOS-conrefs/applies-to"/>
    <section id="NovaCompute">
      <title>Nova Compute Services</title>
      <p>When troubleshooting issues with the Compute service you may have a need to stop or start
        one or all of the Nova services.</p>
      <p><keyword keyref="kw-hos"/> provides an ansible playbook that can stop or start all Nova
        services on one or more nodes that can be run from the lifecycle manager. This may be useful
        in some scenarios. However, if you do not have access to the lifecycle manager or if it is
        necessary to stop or start only one specific Nova service on one or more nodes we will show
        you that process as well.</p>
      <ul>
        <li><xref href="start_stop_services.dita#stopStartSvcs/stop_compute_all">Stopping All Nova
            Services</xref></li>
        <li><xref href="start_stop_services.dita#stopStartSvcs/start_compute_all">Starting All Nova
            Services</xref></li>
        <li><xref href="start_stop_services.dita#stopStartSvcs/start_stop_compute_one">Starting or
            Stopping a Specific Nova Service</xref></li>
      </ul>
    </section>
    <section id="stop_compute_all">
      <title>Stopping All Nova Services</title>
      <p>If you have access to the lifecycle manager, you can use the ansible playbook instructions
        below. Using this method has the added benefit of modifying the monitoring (Monasca) service
        to remove the alarms associated with the Nova services.</p>
      <ol>
        <li>Log in to the lifecycle manager.</li>
        <li>Stop the Nova services: <codeblock>cd ~/scratch/ansible/next/hos/ansible
ansible-playbook -i hosts/verb_hosts nova-stop.yml</codeblock>
          <p>If you specify the <codeph> --limit &lt;hostname></codeph> option at the end of the
            ansible-playbook command then you will only target the specific hosts in your
            environment that you specify and thus only the Nova services that live on that host will
            be stopped.</p></li>
        <li>You can then confirm the services have stopped by using the <codeph>nova
            service-list</codeph> command: <codeblock>source ~/service.osrc
nova service-list
+----+------------------+--------------------------+----------+---------+-------+----------------------------+-----------------+
| Id | Binary           | Host                     | Zone     | Status  | State | Updated_at                 | Disabled Reason |
+----+------------------+--------------------------+----------+---------+-------+----------------------------+-----------------+
| 1  | nova-conductor   | helion-cp1-c1-m1-mgmt    | internal | enabled | down  | 2016-01-15T16:30:50.000000 | -               |
| 7  | nova-scheduler   | helion-cp1-c1-m1-mgmt    | internal | enabled | down  | 2016-01-15T16:30:50.000000 | -               |
| 10 | nova-conductor   | helion-cp1-c1-m3-mgmt    | internal | enabled | down  | 2016-01-15T16:30:51.000000 | -               |
| 19 | nova-conductor   | helion-cp1-c1-m2-mgmt    | internal | enabled | down  | 2016-01-15T16:30:53.000000 | -               |
| 22 | nova-consoleauth | helion-cp1-c1-m1-mgmt    | internal | enabled | down  | 2016-01-15T16:30:53.000000 | -               |
| 25 | nova-scheduler   | helion-cp1-c1-m2-mgmt    | internal | enabled | down  | 2016-01-15T16:30:50.000000 | -               |
| 28 | nova-scheduler   | helion-cp1-c1-m3-mgmt    | internal | enabled | down  | 2016-01-15T16:30:52.000000 | -               |
| 31 | nova-compute     | helion-cp1-comp0001-mgmt | nova     | enabled | down  | 2016-01-15T16:30:47.000000 | -               |
+----+------------------+--------------------------+----------+---------+-------+----------------------------+-----------------+</codeblock>
          <note>It will usually take ~60 seconds for the service states to update.</note></li>
      </ol>
    </section>

    <section id="start_compute_all">
      <title>Starting All Nova Services</title>
      <p>If you have access to the lifecycle manager, you can use the ansible playbook instructions
        below. Using this method has the added benefit of modifying the monitoring (Monasca) service
        to add the alarms for the affected nodes.</p>
      <ol>
        <li>Log in to the lifecycle manager.</li>
        <li>Start the nova-compute service: <codeblock>cd ~/scratch/ansible/next/hos/ansible
ansible-playbook -i hosts/verb_hosts nova-start.yml</codeblock>
          <p>If you specify the <codeph> --limit &lt;hostname></codeph> option at the end of the
            ansible-playbook command then you will only target the specific hosts in your
            environment that you specify and thus only the Nova services that live on that host will
            be started.</p></li>
        <li>You can then confirm the services are back up by using the <codeph>nova
            service-list</codeph> command: <codeblock>source ~/service.osrc
nova service-list
+----+------------------+--------------------------+----------+---------+-------+----------------------------+-----------------+
| Id | Binary           | Host                     | Zone     | Status  | State | Updated_at                 | Disabled Reason |
+----+------------------+--------------------------+----------+---------+-------+----------------------------+-----------------+
| 1  | nova-conductor   | helion-cp1-c1-m1-mgmt    | internal | enabled | up    | 2016-01-15T16:30:50.000000 | -               |
| 7  | nova-scheduler   | helion-cp1-c1-m1-mgmt    | internal | enabled | up    | 2016-01-15T16:30:50.000000 | -               |
| 10 | nova-conductor   | helion-cp1-c1-m3-mgmt    | internal | enabled | up    | 2016-01-15T16:30:51.000000 | -               |
| 19 | nova-conductor   | helion-cp1-c1-m2-mgmt    | internal | enabled | up    | 2016-01-15T16:30:53.000000 | -               |
| 22 | nova-consoleauth | helion-cp1-c1-m1-mgmt    | internal | enabled | up    | 2016-01-15T16:30:53.000000 | -               |
| 25 | nova-scheduler   | helion-cp1-c1-m2-mgmt    | internal | enabled | up    | 2016-01-15T16:30:50.000000 | -               |
| 28 | nova-scheduler   | helion-cp1-c1-m3-mgmt    | internal | enabled | up    | 2016-01-15T16:30:52.000000 | -               |
| 31 | nova-compute     | helion-cp1-comp0001-mgmt | nova     | enabled | up    | 2016-01-15T16:30:47.000000 | -               |
+----+------------------+--------------------------+----------+---------+-------+----------------------------+-----------------+</codeblock>
          <note>It will usually take ~60 seconds for the service states to update.</note></li>
      </ol>
    </section>

    <section id="start_stop_compute_one"><title>Starting or Stopping a Specific Nova Service</title>
      <p>If you do not have access to the lifecycle manager in your cloud environment or if you only
        need to stop a single Nova service then this is the process you should follow.</p>
      <ol>
        <li>You can list out your Nova services showing the host they exist on and what their
          current state is by using the following command with admin credentials: <codeblock>nova service-list</codeblock>
          <p>Here is an example showing an environment with three controller nodes and a single
            Compute node:
            <codeblock>nova service-list
+----+------------------+--------------------------+----------+---------+-------+----------------------------+-----------------+
| Id | Binary           | Host                     | Zone     | Status  | State | Updated_at                 | Disabled Reason |
+----+------------------+--------------------------+----------+---------+-------+----------------------------+-----------------+
| 1  | nova-conductor   | helion-cp1-c1-m1-mgmt    | internal | enabled | up    | 2016-01-15T16:30:50.000000 | -               |
| 7  | nova-scheduler   | helion-cp1-c1-m1-mgmt    | internal | enabled | up    | 2016-01-15T16:30:50.000000 | -               |
| 10 | nova-conductor   | helion-cp1-c1-m3-mgmt    | internal | enabled | up    | 2016-01-15T16:30:51.000000 | -               |
| 19 | nova-conductor   | helion-cp1-c1-m2-mgmt    | internal | enabled | up    | 2016-01-15T16:30:53.000000 | -               |
| 22 | nova-consoleauth | helion-cp1-c1-m1-mgmt    | internal | enabled | up    | 2016-01-15T16:30:53.000000 | -               |
| 25 | nova-scheduler   | helion-cp1-c1-m2-mgmt    | internal | enabled | up    | 2016-01-15T16:30:50.000000 | -               |
| 28 | nova-scheduler   | helion-cp1-c1-m3-mgmt    | internal | enabled | up    | 2016-01-15T16:30:52.000000 | -               |
| 31 | nova-compute     | helion-cp1-comp0001-mgmt | nova     | enabled | up    | 2016-01-15T16:30:47.000000 | -               |
+----+------------------+--------------------------+----------+---------+-------+----------------------------+-----------------+</codeblock></p></li>
        <li>SSH to the host that the service you want to start or stop exists on.</li>
        <li>Use this command to stop that service: <codeblock>systemctl stop &lt;service-name></codeblock>
          <p>For example, for the <codeph>nova-compute</codeph> service you would SSH to the
              <codeph>helion-cp1-comp0001-mgmt</codeph> host and use this command:
            <codeblock>systemctl stop nova-compute</codeblock></p></li>
        <li>Use this command to start that service: <codeblock>systemctl start &lt;service-name></codeblock>
          <p>For example, for the <codeph>nova-compute</codeph> service you would SSH to the
              <codeph>helion-cp1-comp0001-mgmt</codeph> host and use this command:
            <codeblock>systemctl start nova-compute</codeblock></p></li>
        <li>You can then confirm the status of the service by again using this command using admin
          credentials: <codeblock>nova service-list</codeblock></li>
      </ol>
    </section>


    <section id="ceph"><title>Ceph</title>
      <p>After the successful deployment of Ceph, you can perform the following Ceph operations:<ul
          id="ul_j45_td3_pt">
          <li>Check the status of the Ceph object storage daemon (OSD) and monitor services</li>
          <li>Start OSD nodes and monitor services</li>
          <li>Stop OSD nodes and monitor services<p><b>Check the Status of Ceph OSD and Monitor
                Services</b>
            </p><p>Perform the following steps to check the stays of Ceph OSD nodes and monitor
                services:<ol id="ol_np5_td3_pt">
                <li>Login to the lifecycle manager.</li>
                <li>Change to the following
                  directory:<codeblock>~/scratch/ansible/next/hos/ansible</codeblock></li>
                <li>Run the Ansible
                  playbook:<codeblock>ansible-playbook -i hosts/verb_hosts ceph-status.yml</codeblock></li>
              </ol></p><p><b>Start the OSD and Monitor Services</b></p><p>Perform the following
              steps to start the OSD nodes and monitor services:<ol id="ol_hq5_td3_pt">
                <li>Log in to the lifecycle manager.</li>
                <li>Change to the following
                  directory:<codeblock>cd ~/scratch/ansible/next/hos/ansible</codeblock></li>
                <li>Run the Ansible
                  playbook:<codeblock>ansible-playbook -i hosts/verb_hosts ceph-start.yml</codeblock></li>
              </ol></p><p><b>Stop the OSD Nodes and Monitor Services</b></p><p>Perform the following
              steps to stop OSD nodes and monitor services:</p><p>
              <ol id="ol_br5_td3_pt">
                <li>Login to the lifecycle manager.</li>
                <li>Change to the following
                  directory:<codeblock>cd ~/scratch/ansible/next/hos/ansible</codeblock></li>
                <li>Run the Ansible
                  playbook:<codeblock>ansible-playbook -i hosts/verb_hosts ceph-stop.yml</codeblock></li>
              </ol>
            </p>
          </li>
        </ul></p></section>





    <p><b>Starting and stopping Freezer</b></p>
    <section conref="../bura/start_stop_freezer_services.dita#topic_dsm_fbs_st/main"/>
    <section conref="../bura/start_stop_freezer_services.dita#topic_dsm_fbs_st/manual"/>

    <section id="Logging">
      <title>Logging</title>
      <p>These playbooks must be run from the lifecycle manager.</p>
      <p><b>To stop Logging:</b></p>
      <codeblock>cd ~/scratch/ansible/next/hos/ansible
ansible-playbook -i hosts/verb_hosts logging-stop.yml</codeblock>
      <p><b>To start Logging:</b></p>
      <codeblock>cd ~/scratch/ansible/next/hos/ansible
ansible-playbook -i hosts/verb_hosts logging-start.yml</codeblock>
      <note>The above steps only impact centralized logging. Logrotate is an essential feature (to
        keep the service log files from filling the disk) and will not be affected.</note>
    </section>
  </body>
</topic>
