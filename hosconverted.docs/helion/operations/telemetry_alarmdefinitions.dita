<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE topic PUBLIC "-//OASIS//DTD DITA Topic//EN" "topic.dtd">
<topic id="telemetry_alarmdefinitions">
  <title><ph conkeyref="HOS-conrefs/product-title"/>Telemetry Alarms</title>
  <body>
    <section>
      <p>These alarms show under the Telemetry section of the HPE Helion Ops Console.</p>
      <p>
        <table frame="all" rowsep="1" colsep="1" id="telemetry_alarms">
          <tgroup cols="5">
            <colspec colname="c1" colnum="1"/>
            <colspec colname="c2" colnum="2"/>
            <colspec colname="c3" colnum="3"/>
            <colspec colname="c4" colnum="4"/>
            <colspec colname="c5" colnum="5"/>
            <thead>
              <row>
                <entry>Service</entry>
                <entry>Alarm Name</entry>
                <entry>Description</entry>
                <entry>Likely Cause</entry>
                <entry>Mitigation Tasks to Perform</entry>
              </row>
            </thead>
            <tbody>
              <!-- service=telemetry -->
              <row>
                <entry morerows="4">telemetry</entry>
                <entry>Process Check</entry>
                <entry>Alarms when the <codeph>ceilometer-agent-notification</codeph> process is not
                  running.</entry>
                <entry>Process has crashed.</entry>
                <entry>Review the logs on the alarming host in the following location for the cause:
                    <codeblock>/var/log/ceilometer/ceilometer-agent-notification-json.log</codeblock><p>Restart
                    the process on the affected node using these steps:</p><ol id="ol_ybg_3zp_mx">
                    <li>Log in to the lifecycle manager.</li>
                    <li>Use the Ceilometer start playbook against the affected node:
                      <codeblock>cd ~/scratch/ansible/next/hos/ansible
ansible-playbook -i hosts/verb_hosts ceilometer-start.yml --limit &lt;hostname></codeblock></li>
                  </ol>
                </entry>
              </row>
              <row>
                <entry>Process Check</entry>
                <entry>Alarms when the <codeph>ceilometer-polling</codeph> process is not
                  running.</entry>
                <entry>Process has crashed.</entry>
                <entry>Review the logs on the alarming host in the following location for the cause:
                    <codeblock>/var/log/ceilometer/ceilometer-polling-json.log</codeblock><p>Restart
                    the process on the affected node using these steps:</p><ol id="ol_zbg_3zp_mx">
                    <li>Log in to the lifecycle manager.</li>
                    <li>Use the Ceilometer start playbook against the affected node:
                      <codeblock>cd ~/scratch/ansible/next/hos/ansible
ansible-playbook -i hosts/verb_hosts ceilometer-start.yml --limit &lt;hostname></codeblock></li>
                  </ol>
                </entry>
              </row>
              <row>
                <entry>Process Check</entry>
                <entry>Alarms when the <codeph>ceilometer-api</codeph> process is not
                  running.</entry>
                <entry>Process has crashed.</entry>
                <entry>Review the logs on the alarming host in the following location for the cause:
                    <codeblock>/var/log/ceilometer/ceilometer-api-json.log</codeblock><p>Restart the
                    process on the affected node using these steps:</p><ol id="ol_dfv_vbt_lx">
                    <li>Log in to the lifecycle manager.</li>
                    <li>Use the Ceilometer start playbook against the affected node:
                      <codeblock>cd ~/scratch/ansible/next/hos/ansible
ansible-playbook -i hosts/verb_hosts ceilometer-start.yml --limit &lt;hostname></codeblock></li>
                  </ol></entry>
              </row>
              <row>
                <entry>HTTP Status</entry>
                <entry>Alarms when the specified HTTP endpoint is down or not reachable.<p>
                    <codeblock>endpoint_type=host_endpoint</codeblock>
                  </p></entry>
                <entry>The Ceilometer API on the host defined in the <codeph>hostname</codeph> is
                  down or not reachable.</entry>
                <entry>Restart Apache on the affected node using these steps: <ol id="ol_a53_gct_lx">
                    <li>Log in to the lifecycle manager.</li>
                    <li>Confirm the status of Apache with this
                      playbook:<codeblock>cd ~/scratch/ansible/next/hos/ansible
ansible-playbook -i hosts/verb_hosts FND-AP2-status.yml</codeblock></li>
                    <li>Stop the Apache service, if
                      necessary:<codeblock>cd ~/scratch/ansible/next/hos/ansible
ansible-playbook -i hosts/verb_hosts FND-AP2-stop.yml --limit &lt;hostname></codeblock></li>
                    <li>Use this playbook against the affected node to restart Apache:
                      <codeblock>cd ~/scratch/ansible/next/hos/ansible
ansible-playbook -i hosts/verb_hosts FND-AP2-start.yml --limit &lt;hostname></codeblock></li>
                  </ol></entry>
              </row>
              <row>
                <entry>HTTP Status</entry>
                <entry>Alarms when the specified HTTP endpoint is down or not reachable.<p>
                    <codeblock>endpoint_type=internal</codeblock>
                  </p></entry>
                <entry>The Ceilometer API on the internal virtual IP address is down or not
                  reachable.</entry>
                <entry>
                  <p>If this occurs with an <codeph>http_status</codeph> in error on all nodes, then
                    restart Apache on all controllers with these steps:<ol id="ol_dk3_kct_lx">
                      <li>Log in to the lifecycle manager.</li>
                      <li>Confirm the status of Apache with this
                        playbook:<codeblock>cd ~/scratch/ansible/next/hos/ansible
ansible-playbook -i hosts/verb_hosts FND-AP2-status.yml</codeblock></li>
                      <li>Stop the Apache service, if
                        necessary:<codeblock>cd ~/scratch/ansible/next/hos/ansible
ansible-playbook -i hosts/verb_hosts FND-AP2-stop.yml --limit &lt;hostname></codeblock></li>
                      <li>Use this playbook against your controller nodes to restart Apache:
                        <codeblock>cd ~/scratch/ansible/next/hos/ansible
ansible-playbook -i hosts/verb_hosts FND-AP2-start.yml --limit &lt;hostname></codeblock></li>
                    </ol></p>
                  <p>If this occurs with a specific host with <codeph>http_status</codeph> in
                    non-error for telemetry, then it should be a haproxy issue and it needs to be
                    restarted.</p>
                  <p>
                    <ol id="ol_acg_3zp_mx">
                      <li>Log in to the lifecycle manager.</li>
                      <li>Confirm the status of haproxy with this
                        playbook:<codeblock>cd ~/scratch/ansible/next/hos/ansible
ansible-playbook -i hosts/verb_hosts FND-CLU-status.yml --limit &lt;hostname></codeblock></li>
                      <li>Stop the haproxy service, if
                        necessary:<codeblock>cd ~/scratch/ansible/next/hos/ansible
ansible-playbook -i hosts/verb_hosts FND-CLU-stop.yml --limit &lt;hostname></codeblock></li>
                      <li>Restart the haproxy service with this
                        playbook:<codeblock>cd ~/scratch/ansible/next/hos/ansible
ansible-playbook -i hosts/verb_hosts FND-CLU-start.yml --limit &lt;hostname></codeblock></li>
                    </ol>
                  </p>
                  <p>For further troubleshooting, SSH to the affected host and look at the
                    folllowing Ceilometer access
                    logs:<codeblock>/var/log/ceilometer/ceilometer_modwsgi.log
/var/log/ceilometer/ceilometer-api.log</codeblock></p>
                </entry>
              </row>
              <row>
                <entry>metering</entry>
                <entry>Service Log Directory Size</entry>
                <entry>Service log directory consuming more disk than its quota.</entry>
                <entry>This could be due to a service set to <codeph>DEBUG</codeph> instead of
                    <codeph>INFO</codeph> level. Another reason could be due to a repeating error
                  message filling up the log files. Finally, it could be due to log rotate not
                  configured properly so old log files are not being deleted properly.</entry>
                <entry>Find the service that is consuming too much disk space. Look at the logs. If
                    <codeph>DEBUG</codeph> log entries exist, set the logging level to
                    <codeph>INFO</codeph>. If the logs are repeatedly logging an error message, do
                  what is needed to resolve the error. If old log files exist, configure log rotate
                  to remove them. You could also choose to remove old log files by hand after
                  backing them up if needed.</entry>
              </row>

              <!-- KAFKA -->
              <row>
                <entry morerows="3">kafka</entry>
                <entry>Kafka Persister Metric Consumer Lag</entry>
                <entry>Alarms when the Persister consumer group is not keeping up with the incoming
                  messages on the metric topic.</entry>
                <entry>There is a slow down in the system or heavy load.</entry>
                <entry>Verify that all of the monasca-persister services are up with these steps:<ol
                    id="ol_z3p_2ff_wv">
                    <li>Log in to the lifecycle manager</li>
                    <li>Verify that all of the <codeph>monasca-persister</codeph> services are up
                      with this
                      playbook:<codeblock>cd ~/scratch/ansible/next/hos/ansible
ansible-playbook -i hosts/verb_hosts monasca-status.yml --tags monasca-persister</codeblock></li>
                  </ol><p>Look for high load in the various systems. This alert can fire for
                    multiple topics or on multiple hosts. Which alarms are firing can help diagnose
                    likely causes. For example, if the alarm is alerting all on one machine it could
                    be the machine. If one topic across multiple machines it is likely the consumers
                    of that topic, etc.</p></entry>
              </row>
              <row>
                <entry>Kafka Alarm Transition Consumer Lag</entry>
                <entry>Alarms when the specified consumer group is not keeping up with the incoming
                  messages on the alarm state transistion topic.</entry>
                <entry>There is a slow down in the system or heavy load.</entry>
                <entry>
                  <p>Check that monasca-thresh and monasca-notification are up.</p>
                  <p>Look for high load in the various systems. This alert can fire for multiple
                    topics or on multiple hosts. Which alarms are firing can help diagnose likely
                    causes, ie if all on one machine it could be the machine. If one topic across
                    multiple machines it is likely the consumers of that topic, etc.</p>
                </entry>
              </row>
              <row>
                <entry>Kafka Kronos Consumer Lag</entry>
                <entry>Alarms when the Kronos consumer group is not keeping up with the incoming
                  messages on the metric topic.</entry>
                <entry>There is a slow down in the system or heavy load.</entry>
                <entry>Look for high load in the various systems. This alert can fire for multiple
                  topics or on multiple hosts. Which alarms are firing can help diagnose likely
                  causes, ie if all on one machine it could be the machine. If one topic across
                  multiple machines it is likely the consumers of that topic, etc.</entry>
              </row>
              <row>
                <entry>Process Check</entry>
                <entry>Alarms when the specified process is not running. <p>
                    <codeblock>process_name = kafka.Kafka</codeblock>
                  </p></entry>
                <entry/>
                <entry>Restart the process on the affected node using these steps: <ol
                    id="ol_lrq_lzp_mx">
                    <li>Log in to the lifecycle manager.</li>
                    <li>Stop the kafka service with this playbook:
                      <codeblock>cd ~/scratch/ansible/next/hos/ansible
ansible-playbook -i hosts/verb_hosts monasca-start.yml --tags kafka</codeblock></li>
                    <li>Start the kafka service back up with this playbook:
                      <codeblock>cd ~/scratch/ansible/next/hos/ansible
ansible-playbook -i hosts/verb_hosts monasca-status.yml --tags kafka</codeblock></li>
                  </ol><p>Review the logs in this
                  location:</p><codeblock>/var/log/kafka/server.log</codeblock></entry>
              </row>

              <!-- service=logging -->
              <row>
                <entry morerows="7">logging</entry>
                <entry>Beaver Memory Usage</entry>
                <entry>Beaver is using more memory than expected. This may indicate that it cannot
                  forward messages and it's queue is filling up. If you continue to see this, see
                  the troubleshooting guide.</entry>
                <entry>Overloaded system or services with memory leaks.</entry>
                <entry>Log onto the reporting host to investigate high memory users.</entry>
              </row>
              <row>
                <entry>Audit Log Partition Low Watermark</entry>
                <entry>The <codeph>/var/audit</codeph> disk space usage has crossed low watermark.
                  If the high watermark is reached, logrotate will be run to free up disk space.
                  Adjust <codeph>var_audit_low_watermark_percent</codeph> if needed.</entry>
                <entry>This could be due to a service set to DEBUG instead of INFO level. Another
                  reason could be due to a repeating error message filling up the log files.
                  Finally, it could be due to log rotate not configured properly so old log files
                  are not being deleted properly.</entry>
                <entry>Find the service that is consuming too much disk space. Look at the logs. If
                  DEBUG log entries exist, set the logging level to INFO. If the logs are repeatedly
                  logging an error message, do what is needed to resolve the error. If old log files
                  exist, configure log rotate to remove them. You could also choose to remove old
                  log files by hand after backing them up if needed.</entry>
              </row>
              <row>
                <entry>Audit Log Partition High Watermark</entry>
                <entry>The <codeph>/var/audit</codeph> volume is running low on disk space.
                  Logrotate will be run now to free up space. Adjust
                    <codeph>var_audit_high_watermark_percent</codeph> if needed.</entry>
                <entry>This could be due to a service set to DEBUG instead of INFO level. Another
                  reason could be due to a repeating error message filling up the log files.
                  Finally, it could be due to log rotate not configured properly so old log files
                  are not being deleted properly.</entry>
                <entry>Find the service that is consuming too much disk space. Look at the logs. If
                  DEBUG log entries exist, set the logging level to INFO. If the logs are repeatedly
                  logging an error message, do what is needed to resolve the error. If old log files
                  exist, configure log rotate to remove them. You could also choose to remove old
                  log files by hand after backing them up if needed.</entry>
              </row>
              <row>
                <entry>Elasticsearch Unassigned Shards</entry>
                <entry><p>
                    <codeblock>component = elasticsearch</codeblock>
                  </p>Elasticsearch unassigned shards count is greater than 0.</entry>
                <entry>Environment could be misconfigured.</entry>
                <entry>
                  <p>To find the unassigned shards, run the following command on the lifecycle
                    manager from the <codeph>~/scratch/ansible/next/hos/ansible</codeph>
                    directory:</p>
                  <codeblock>cd ~/scratch/ansible/next/hos/ansible
ansible -i hosts/verb_hosts LOG-SVR[0] -m shell -a "curl localhost:9200/_cat/shards?pretty -s" | grep UNASSIGNED</codeblock>
                  <p>This should show which shards are unassigned, like this:</p>
                  <codeblock>logstash-2015.10.21 4 p UNASSIGNED 11412371  3.2gb 10.241.67.11 Keith Kilham</codeblock>
                  <p>The last column shows the name that Elasticsearch uses for the node that the
                    unassigned shards are on. To find the actual hostname, run:</p>
                  <codeblock>cd ~/scratch/ansible/next/hos/ansible
ansible -i hosts/verb_hosts LOG-SVR[0] -m shell -a "curl localhost:9200/_nodes/_all/name?pretty -s"</codeblock>
                  <p>Once you find the hostname, you can try the following:</p>
                  <ol id="ol_bcg_3zp_mx">
                    <li>Make sure the node is not out of disk space, and free up space if
                      needed.</li>
                    <li>Restart the node (use caution, as this may affect other services as
                      well).</li>
                    <li>Check to make sure all versions of Elasticsearch are the same with this:
                      <codeblock>cd ~/scratch/ansible/next/hos/ansible
ansible -i hosts/verb_hosts LOG-SVR -m shell -a "curl localhost:9200/_nodes/_local/name?pretty -s" | grep version</codeblock></li>
                    <li>Contact customer support.</li>
                  </ol>
                </entry>
              </row>
              <row>
                <entry>Elasticsearch Number of Log Entries</entry>
                <entry><p>
                    <codeblock>component = elasticsearch</codeblock>
                  </p>Elasticsearch Number of Log Entries.</entry>
                <entry>The number of log entries may get too large.</entry>
                <entry>Older versions of Kibana (version 3 and earlier) may hang if the number of
                  log entries is too large (e.g. above 40,000), and the page size would need to be
                  small enough (about 20,000 results), because if it is larger (e.g. 200,000), it
                  may hang the browser, but Kibana 4 should not have this issue.</entry>
              </row>
              <row>
                <entry>Elasticsearch Field Data Evictions</entry>
                <entry><p>
                    <codeblock>component = elasticsearch</codeblock>
                  </p>Elasticsearch Field Data Evictions count is greater than 0.</entry>
                <entry>Field Data Evictions may be found even though it is nowhere near the limit
                  set.</entry>
                <entry>The <codeph>elasticsearch_indices_fielddata_cache_size</codeph> is set to
                    <codeph>unbounded</codeph> by default. If this is set by the user to a value
                  that is insufficient, you may need to increase this configuration parameter or set
                  it to unbounded and run a reconfigure using the steps below:<ol id="ol_ccg_3zp_mx">
                    <li>Log in to the lifecycle manager.</li>
                    <li>Edit the configuration file below and change the value for
                        <codeph>elasticsearch_indices_fielddata_cache_size</codeph> to your desired
                      value: <codeblock>~/helion/my_cloud/config/logging/main.yml</codeblock></li>
                    <li>Commit the changes to git:
                      <codeblock>git add -A
git commit -a -m "changing Elasticsearch fielddata cache size"</codeblock></li>
                    <li>Run the configuration processor:
                      <codeblock>cd ~/helion/hos/ansible
ansible-playbook -i hosts/localhost config-processor-run.yml</codeblock></li>
                    <li>Update your deployment directory:
                      <codeblock>cd ~/helion/hos/ansible
ansible-playbook -i hosts/localhost ready-deployment.yml</codeblock></li>
                    <li>Run the Logging reconfigure playbook to deploy the change:
                      <codeblock>cd ~/scratch/ansible/next/hos/ansible/
ansible-playbook -i hosts/verb_hosts kronos-reconfigure.yml</codeblock></li>
                  </ol></entry>
              </row>
              <row>
                <entry>Service Log Directory Size</entry>
                <entry>
                  <p>Service log directory consuming more disk than its quota.</p>
                </entry>
                <entry>This could be due to a service set to <codeph>DEBUG</codeph> instead of
                    <codeph>INFO</codeph> level. Another reason could be due to a repeating error
                  message filling up the log files. Finally, it could be due to log rotate not
                  configured properly so old log files are not being deleted properly.</entry>
                <entry>Find the service that is consuming too much disk space. Look at the logs. If
                    <codeph>DEBUG</codeph> log entries exist, set the logging level to
                    <codeph>INFO</codeph>. If the logs are repeatedly logging an error message, do
                  what is needed to resolve the error. If old log files exist, configure log rotate
                  to remove them. You could also choose to remove old log files by hand after
                  backing them up if needed.</entry>
              </row>
              <row>
                <entry>Process Check</entry>
                <entry>Separate alarms for each of these logging services, specified by the
                    <codeph>process_name</codeph> dimension:<ul id="ul_dcg_3zp_mx">
                    <li>elasticsearch</li>
                    <li>logstash</li>
                    <li>beaver</li>
                    <li>apache2</li>
                    <li>kibana</li>
                  </ul></entry>
                <entry>Process has crashed.</entry>
                <entry>On the affected node, attempt to restart the process.<p>If the elasticsearch
                    process has crashed,
                    use:</p><codeblock>sudo systemctl restart elasticsearch</codeblock><p>If the
                    logstash process has crashed,
                    use:</p><codeblock>sudo systemctl restart logstash</codeblock><p>The rest of the
                    processes can be restarted using similar commands, listed
                  here:</p><codeblock>sudo systemctl restart beaver
sudo systemctl restart apache2
sudo systemctl restart kibana</codeblock>
                </entry>
              </row>
              <!-- MONASCA-TRANSFORM -->
              <row>
                <entry morerows="2">monasca-transform</entry>
                <entry>Process Check</entry>
                <entry>
                  <p>
                    <codeblock>process_name = pyspark</codeblock>
                  </p>
                </entry>
                <entry>Service process has crashed.</entry>
                <entry>
                  <p>Restart process on affected node. Review logs.</p>
                  <p>Child process of <codeph>spark-worker</codeph> but created once the
                      <codeph>monasca-transform</codeph> process begins processing streams. If the
                    process fails on one node only, along with the pyspark process, it's very likely
                    that the <codeph>spark-worker</codeph> has failed to connect to the elected
                    leader of the <codeph>spark-master</codeph> service. In this case the
                      <codeph>spark-worker</codeph> service should be started on the affected node.
                    If on multiple nodes check the <codeph>spark-worker</codeph>,
                      <codeph>spark-master</codeph> and <codeph>monasca-transform</codeph> services
                    and logs. If the <codeph>monasca-transform</codeph> or <codeph>spark</codeph>
                    services have been interrupted this process may not re-appear for up to ten
                    minutes (the stream processing interval).</p>
                </entry>
              </row>
              <row>
                <entry>Process Check</entry>
                <entry>
                  <p>
                    <codeblock>process_name = org.apache.spark.executor.CoarseGrainedExecutorBackend</codeblock>
                  </p>
                </entry>
                <entry>Service process has crashed.</entry>
                <entry>
                  <p>Restart process on affected node. Review logs.</p>
                  <p>Child process of <codeph>spark-worker</codeph> but created once the
                      <codeph>monasca-transform</codeph> process begins processing streams. If the
                    process fails on one node only, along with the pyspark process, it's very likely
                    that the <codeph>spark-worker</codeph> has failed to connect to the elected
                    leader of the <codeph>spark-master</codeph> service. In this case the
                      <codeph>spark-worker</codeph> service should be started on the affected node.
                    If on multiple nodes check the <codeph>spark-worker</codeph>,
                      <codeph>spark-master</codeph> and <codeph>monasca-transform</codeph> services
                    and logs. If the <codeph>monasca-transform</codeph> or <codeph>spark</codeph>
                    services have been interrupted this process may not re-appear for up to ten
                    minutes (the stream processing interval).</p>
                </entry>
              </row>
              <row>
                <entry>Process Check</entry>
                <entry>
                  <codeblock>process_name = monasca-transform</codeblock>
                </entry>
                <entry>Service process has crashed.</entry>
                <entry>
                  <p>Restart the service on affected node. Review logs.</p>
                </entry>
              </row>
              <!-- MONITORING -->
              <row>
                <entry morerows="12">monitoring</entry>
                <entry>HTTP Status</entry>
                <entry><p>
                    <codeblock>component = monasca-persister</codeblock>
                  </p>Persister Health Check</entry>
                <entry>The process has crashed or a dependency is out.</entry>
                <entry>If the process has crashed, restart it using the steps below. If a dependent
                  service is down, address that issue. <p>Restart the process on the affected node
                    using these steps:</p>
                  <ol id="ol_ecg_3zp_mx">
                    <li>Log in to the lifecycle manager.</li>
                    <li>Check if <codeph>monasca-api</codeph> is running on all nodes with this
                      playbook:
                      <codeblock>cd ~/scratch/ansible/next/hos/ansible
ansible-playbook -i hosts/verb_hosts monasca-status.yml --tags monasca-persister</codeblock></li>
                    <li>Use the Monasca start playbook against the affected node to restart it:
                      <codeblock>cd ~/scratch/ansible/next/hos/ansible
ansible-playbook -i hosts/verb_hosts monasca-start.yml --tags persister</codeblock></li>
                    <li>Verify that it is running on all nodes with this playbook:
                      <codeblock>cd ~/scratch/ansible/next/hos/ansible
ansible-playbook -i hosts/verb_hosts monasca-status.yml --tags monasca-persister</codeblock></li>
                  </ol>
                  <p>Review the associated logs.</p></entry>
              </row>
              <row>
                <entry>HTTP Status</entry>
                <entry><p>
                    <codeblock>component = monasca-api</codeblock>
                  </p>API Health Check</entry>
                <entry>The process has crashed or a dependency is out.</entry>
                <entry>If the process has crashed, restart it using the steps below. If a dependent
                  service is down, address that issue. <p>Restart the process on the affected node
                    using these steps:</p>
                  <ol>
                    <li>Log in to the lifecycle manager.</li>
                    <li>Check if <codeph>monasca-api</codeph> is running on all nodes with this
                      playbook:
                      <codeblock>cd ~/scratch/ansible/next/hos/ansible
ansible-playbook -i hosts/verb_hosts monasca-status.yml --tags monasca-api</codeblock></li>
                    <li>Use the Monasca start playbook against the affected node to restart it:
                      <codeblock>cd ~/scratch/ansible/next/hos/ansible
ansible-playbook -i hosts/verb_hosts monasca-start.yml --tags monasca-api</codeblock></li>
                    <li>Verify that it is running on all nodes with this playbook:
                      <codeblock>cd ~/scratch/ansible/next/hos/ansible
ansible-playbook -i hosts/verb_hosts monasca-status.yml --tags monasca-api</codeblock></li>
                  </ol>
                  <p>Review the associated logs.</p></entry>
              </row>
              <row>
                <entry>Monasca Agent Collection Time</entry>
                <entry>Alarms when the elapsed time the <codeph>monasca-agent</codeph> takes to
                  collect metrics is high.</entry>
                <entry>Heavy load on the box or a stuck agent plug-in.</entry>
                <entry>Address the load issue on the machine. If needed, restart the agent using the
                  steps below: <p>Restart the agent on the affected node using these steps:</p>
                  <ol id="ol_gcg_3zp_mx">
                    <li>Log in to the lifecycle manager.</li>
                    <li>Check if <codeph>monasca-agent</codeph> is running on all nodes with this
                      playbook:
                      <codeblock>cd ~/scratch/ansible/next/hos/ansible
ansible-playbook -i hosts/verb_hosts monasca-agent-status.yml</codeblock></li>
                    <li>Use the Monasca start playbook against the affected node to restart it:
                      <codeblock>cd ~/scratch/ansible/next/hos/ansible
ansible-playbook -i hosts/verb_hosts monasca-start.yml --limit &lt;hostname></codeblock></li>
                    <li>Verify that it is running on all nodes with this playbook:
                      <codeblock>cd ~/scratch/ansible/next/hos/ansible
ansible-playbook -i hosts/verb_hosts monasca-agent-status.yml</codeblock></li>
                  </ol>
                  <p>Review the associated logs.</p></entry>
              </row>
              <row>
                <entry>Process Check</entry>
                <entry>Alarms when the specified process is not running.
                  <codeblock>component = kafka</codeblock>
                </entry>
                <entry>Process crashed.</entry>
                <entry>Restart the process on the affected node using these steps: <ol>
                    <li>Log in to the lifecycle manager.</li>
                    <li>Check if Kafka is running on all nodes with this playbook:
                      <codeblock>cd ~/scratch/ansible/next/hos/ansible
ansible-playbook -i hosts/verb_hosts monasca-status.yml --tags kafka</codeblock></li>
                    <li>Use the Monasca start playbook against the affected node to restart it:
                      <codeblock>cd ~/scratch/ansible/next/hos/ansible
ansible-playbook -i hosts/verb_hosts monasca-start.yml --tags kafka</codeblock></li>
                    <li>Verify that Kafka is running on all nodes with this playbook:
                      <codeblock>cd ~/scratch/ansible/next/hos/ansible
ansible-playbook -i hosts/verb_hosts monasca-status.yml --tags kafka</codeblock></li>
                  </ol>
                  <p>Review the associated logs.</p></entry>
              </row>
              <row>
                <entry>Process Check</entry>
                <entry>Alarms when the specified process is not running.
                  <codeblock>process_name = monasca-notification</codeblock></entry>
                <entry>Process crashed.</entry>
                <entry>Restart the process on the affected node using these steps: <ol>
                    <li>Log in to the lifecycle manager.</li>
                    <li>Check if <codeph>monasca-api</codeph> is running on all nodes with this
                      playbook:
                      <codeblock>cd ~/scratch/ansible/next/hos/ansible
ansible-playbook -i hosts/verb_hosts monasca-status.yml --tags notification</codeblock></li>
                    <li>Use the Monasca start playbook against the affected node to restart it:
                      <codeblock>cd ~/scratch/ansible/next/hos/ansible
ansible-playbook -i hosts/verb_hosts monasca-start.yml --tags notification</codeblock></li>
                    <li>Verify that it is running on all nodes with this playbook:
                      <codeblock>cd ~/scratch/ansible/next/hos/ansible
ansible-playbook -i hosts/verb_hosts monasca-status.yml --tags notification</codeblock></li>
                  </ol>
                  <p>Review the associated logs.</p></entry>
              </row>
              <row>
                <entry>Process Check</entry>
                <entry>Alarms when the specified process is not running.
                  <codeblock>process_name = monasca-agent</codeblock>
                </entry>
                <entry>Process crashed.</entry>
                <entry>Restart the agent on the affected node using these steps: <ol>
                    <li>Log in to the lifecycle manager.</li>
                    <li>Check if <codeph>monasca-agent</codeph> is running on all nodes with this
                      playbook:
                      <codeblock>cd ~/scratch/ansible/next/hos/ansible
ansible-playbook -i hosts/verb_hosts monasca-agent-status.yml</codeblock></li>
                    <li>Use the Monasca start playbook against the affected node to restart it:
                      <codeblock>cd ~/scratch/ansible/next/hos/ansible
ansible-playbook -i hosts/verb_hosts monasca-start.yml --limit &lt;hostname></codeblock></li>
                    <li>Verify that it is running on all nodes with this playbook:
                      <codeblock>cd ~/scratch/ansible/next/hos/ansible
ansible-playbook -i hosts/verb_hosts monasca-agent-status.yml</codeblock></li>
                  </ol>
                  <p>Review the associated logs.</p></entry>
              </row>
              <row>
                <entry>Process Check</entry>
                <entry>Alarms when the specified process is not running.
                  <codeblock>process_name = monasca-api</codeblock>
                </entry>
                <entry>Process crashed.</entry>
                <entry>Restart the process on the affected node using these steps: <ol>
                    <li>Log in to the lifecycle manager.</li>
                    <li>Check if <codeph>monasca-api</codeph> is running on all nodes with this
                      playbook:
                      <codeblock>cd ~/scratch/ansible/next/hos/ansible
ansible-playbook -i hosts/verb_hosts monasca-status.yml --tags monasca-api</codeblock></li>
                    <li>Use the Monasca start playbook against the affected node to restart it:
                      <codeblock>cd ~/scratch/ansible/next/hos/ansible
ansible-playbook -i hosts/verb_hosts monasca-start.yml --tags monasca-api</codeblock></li>
                    <li>Verify that it is running on all nodes with this playbook:
                      <codeblock>cd ~/scratch/ansible/next/hos/ansible
ansible-playbook -i hosts/verb_hosts monasca-status.yml --tags monasca-api</codeblock></li>
                  </ol>
                  <p>Review the associated logs.</p></entry>
              </row>
              <row>
                <entry>Process Check</entry>
                <entry>Alarms when the specified process is not running.
                  <codeblock>process_name = monasca-persister</codeblock>
                </entry>
                <entry>Process crashed.</entry>
                <entry>Restart the process on the affected node using these steps: <ol>
                    <li>Log in to the lifecycle manager.</li>
                    <li>Check if <codeph>monasca-api</codeph> is running on all nodes with this
                      playbook:
                      <codeblock>cd ~/scratch/ansible/next/hos/ansible
ansible-playbook -i hosts/verb_hosts monasca-status.yml --tags monasca-persister</codeblock></li>
                    <li>Use the Monasca start playbook against the affected node to restart it:
                      <codeblock>cd ~/scratch/ansible/next/hos/ansible
ansible-playbook -i hosts/verb_hosts monasca-start.yml --tags persister</codeblock></li>
                    <li>Verify that it is running on all nodes with this playbook:
                      <codeblock>cd ~/scratch/ansible/next/hos/ansible
ansible-playbook -i hosts/verb_hosts monasca-status.yml --tags monasca-persister</codeblock></li>
                  </ol>
                  <p>Review the associated logs.</p></entry>
              </row>
              <row>
                <entry>Process Check</entry>
                <entry>Alarms when the specified process is not
                  running.<codeblock>process_name = backtype.storm.daemon.nimbus
component = apache-storm</codeblock></entry>
                <entry>Process crashed.</entry>
                <entry>Review the logs in the <codeph>/var/log/storm</codeph> directory on all storm
                  hosts to find the root cause.<p>
                    <note>The logs containing threshold engine logging are on the 2nd and 3rd
                      controller nodes.</note>
                  </p><p>Restart monasca-thresh, if necessary, with these steps:</p><ol>
                    <li>Log in to the lifecycle manager.</li>
                    <li>Check if <codeph>monasca-thresh</codeph> is running on all nodes with this
                      playbook:
                      <codeblock>cd ~/scratch/ansible/next/hos/ansible
ansible-playbook -i hosts/verb_hosts monasca-status.yml --tags thresh</codeblock></li>
                    <li>Use the Monasca start playbook against the affected node to restart it:
                      <codeblock>cd ~/scratch/ansible/next/hos/ansible
ansible-playbook -i hosts/verb_hosts monasca-start.yml --tags thresh</codeblock></li>
                    <li>Verify that it is running on all nodes with this playbook:
                      <codeblock>cd ~/scratch/ansible/next/hos/ansible
ansible-playbook -i hosts/verb_hosts monasca-status.yml --tags thresh</codeblock></li>
                  </ol>
                </entry>
              </row>
              <row>
                <entry>Process Check</entry>
                <entry>Alarms when the specified process is not
                  running.<codeblock>process_name = backtype.storm.daemon.supervisor
component = apache-storm</codeblock></entry>
                <entry>Process crashed.</entry>
                <entry>Review the logs in the <codeph>/var/log/storm</codeph> directory on all storm
                  hosts to find the root cause.<p>
                    <note>The logs containing threshold engine logging are on the 2nd and 3rd
                      controller nodes.</note>
                  </p><p>Restart monasca-thresh with these steps:</p><ol>
                    <li>Log in to the lifecycle manager.</li>
                    <li>Stop the monasca-thresh service:
                      <codeblock>cd ~/scratch/ansible/next/hos/ansible
ansible-playbook -i hosts/verb_hosts monasca-stop.yml --tags thresh</codeblock></li>
                    <li>Start the monasca-thresh service back up:
                      <codeblock>cd ~/scratch/ansible/next/hos/ansible
ansible-playbook -i hosts/verb_hosts monasca-start.yml --tags thresh</codeblock></li>
                    <li>Verify that it is running on all nodes with this playbook:
                      <codeblock>cd ~/scratch/ansible/next/hos/ansible
ansible-playbook -i hosts/verb_hosts monasca-status.yml --tags thresh</codeblock></li>
                  </ol></entry>
              </row>
              <row>
                <entry>Process Check</entry>
                <entry>Alarms when the specified process is not
                  running.<codeblock>process_name = backtype.storm.daemon.worker
component = apache-storm</codeblock></entry>
                <entry>Process crashed.</entry>
                <entry>Review the logs in the <codeph>/var/log/storm</codeph> directory on all storm
                  hosts to find the root cause.<p>
                    <note>The logs containing threshold engine logging are on the 2nd and 3rd
                      controller nodes.</note>
                  </p><p>Restart monasca-thresh with these steps:</p><ol>
                    <li>Log in to the lifecycle manager.</li>
                    <li>Stop the monasca-thresh service:
                      <codeblock>cd ~/scratch/ansible/next/hos/ansible
ansible-playbook -i hosts/verb_hosts monasca-stop.yml --tags thresh</codeblock></li>
                    <li>Start the monasca-thresh service back up:
                      <codeblock>cd ~/scratch/ansible/next/hos/ansible
ansible-playbook -i hosts/verb_hosts monasca-start.yml --tags thresh</codeblock></li>
                    <li>Verify that it is running on all nodes with this playbook:
                      <codeblock>cd ~/scratch/ansible/next/hos/ansible
ansible-playbook -i hosts/verb_hosts monasca-status.yml --tags thresh</codeblock></li>
                  </ol></entry>
              </row>
              <row>
                <entry>Process Check</entry>
                <entry>Alarms when the specified process is not running.
                  <codeblock>process_name = monasca-thresh
component = apache-storm</codeblock></entry>
                <entry>Process crashed.</entry>
                <entry>Restart the process on the affected node using these steps: <ol>
                    <li>Log in to the lifecycle manager.</li>
                    <li>Check if <codeph>monasca-thresh</codeph> is running on all nodes with this
                      playbook:
                      <codeblock>cd ~/scratch/ansible/next/hos/ansible
ansible-playbook -i hosts/verb_hosts monasca-status.yml --tags thresh</codeblock></li>
                    <li>Use the Monasca start playbook against the affected node to restart it:
                      <codeblock>cd ~/scratch/ansible/next/hos/ansible
ansible-playbook -i hosts/verb_hosts monasca-start.yml --tags thresh</codeblock></li>
                    <li>Verify that it is running on all nodes with this playbook:
                      <codeblock>cd ~/scratch/ansible/next/hos/ansible
ansible-playbook -i hosts/verb_hosts monasca-status.yml --tags thresh</codeblock></li>
                  </ol><p>Review the associated logs.</p></entry>
              </row>
              <row>
                <entry>Service Log Directory Size</entry>
                <entry>Service log directory consuming more disk than its quota.</entry>
                <entry>The service log directory, as indicated by the <codeph>path</codeph>
                  dimension, is over the 2.5 GB quota.</entry>
                <entry>Find the service that is consuming too much disk space. Look at the logs. If
                    <codeph>DEBUG</codeph> log entries exist, set the logging level to
                    <codeph>INFO</codeph>. If the logs are repeatedly logging an error message, do
                  what is needed to resolve the error. If old log files exist, configure log rotate
                  to remove them. You could also choose to remove old log files by hand after
                  backing them up if needed.</entry>
              </row>
              <!-- VERTICA -->
              <row>
                <entry morerows="8">vertica</entry>
                <entry>Vertica Resource Rejection</entry>
                <entry>Alarms when a resource rejection occurs.</entry>
                <entry>Vertica is getting a resource rejection on one of its resource pools.
                  Resource pools are a pre-allocated subset of system resources and are associated
                  with a queue of queries that will be executed against it. <p>When this alarm
                    occurs, that resource pool is hitting issues due to lack of memory, threads,
                    file handles or execution slots when executing its queries.</p></entry>
                <entry><p>To troubleshoot this issue you will need to access the Vertica database on
                    the reporting monitoring node, as indicated by the <codeph>hostname</codeph>
                    dimension.</p>To obtain the Vertica database admin user credentials, use these
                    steps:<ol id="ol_qvm_px4_mx">
                    <li>If you do not use data encryption on your lifecycle manager, ues these
                      steps: <ol id="ol_ocg_3zp_mx">
                        <li>Log in to the lifecycle manager.</li>
                        <li>Use this command to obtain the
                          password:<codeblock>grep -r dbadmin_user_password ~/scratch/ansible/next/hos/ansible/group_vars/</codeblock></li>
                      </ol></li>
                    <li>If you do use data encryption on your lifecycle manager, contact Support for
                      assistance in obtaining your Vertica database admin credentials.</li>
                  </ol><p>Once you have your credentials, follow these steps:</p><ol
                    id="ol_pcg_3zp_mx">
                    <li>Log in to the monitoring node as indicated by the <codeph>hostname</codeph>
                      dimension of the alarm.</li>
                    <li>Become root:<codeblock>sudo su</codeblock></li>
                    <li>Run this command, using your Vertica admin
                      password<codeblock>/opt/vertica/bin/vsql -U dbadmin -w <i>&lt;vertica_admin_user_password></i></codeblock></li>
                    <li>Query the database and look at its resource rejections to see what is
                      causing the failure. This can be obtained by querying the resource rejection
                      table. For details on how to do this, see the <xref
                        href="https://my.vertica.com/docs/7.2.x/HTML/Content/Authoring/SQLReferenceManual/SystemTables/MONITOR/RESOURCE_REJECTIONS.htm"
                        format="html" scope="external">Vertica documentation</xref> on this
                      topic.</li>
                    <li>Once the cause of the failure is determined, you can address the problems
                      with the resource pool using the <xref
                        href="https://my.vertica.com/docs/7.2.x/HTML/index.htm#Authoring/AdministratorsGuide/ResourceManager/ManagingWorkloads.htm%3FTocPath%3DAdministrator"
                        scope="external" format="html">Vertica documentation</xref> which may
                      include altering specific resource pools to allocate more resources.</li>
                  </ol></entry>
              </row>
              <row>
                <entry>Vertica Status</entry>
                <entry>Alarms if a vertica node cannot connect to its database.<p>If both this alarm
                    and the Process Check alarm for the <codeph>vertica</codeph> process go off at
                    the same time, do the mitigation steps listed for the Process Check alarm
                    first.</p></entry>
                <entry>Process crashed.</entry>
                <entry>Verify that the database node is down and, if it is, then recover it by using
                  these steps: <ol id="ol_urq_lzp_mx">
                    <li>Log in to the lifecycle manager.</li>
                    <li>Run the Monasca status playbook:
                      <codeblock>cd ~/scratch/ansible/next/hos/ansible
ansible-playbook -i hosts/verb_hosts monasca-status.yml --tags vertica</codeblock></li>
                    <li>Run this playbook to do a recovery on Vertica:
                      <codeblock>cd ~/scratch/ansible/next/hos/ansible
ansible-playbook -i hosts/verb_hosts monasca-vertica-recovery.yml</codeblock></li>
                  </ol>
                </entry>
              </row>
              <row>
                <entry>Process Check</entry>
                <entry>Alarms when the specified process is not running.<p>
                    <codeblock>process_name=vertica</codeblock>
                  </p></entry>
                <entry>Process crashed.</entry>
                <entry>Review the logs on the alarming host to find the root cause.<ol
                    id="ol_onk_gfv_qx">
                    <li>Log in to the alarming host.</li>
                    <li>Run the following
                      commands:<codeblock>sudo su dbadmin
cd /var/vertica
find . -name vertica.log</codeblock></li>
                  </ol><p>If the process needs to be restarted, use the following steps:<ol
                      id="ol_zdd_kfv_qx">
                      <li>Log in to the lifecycle manager.</li>
                      <li>Run this
                        playbook:<codeblock>cd ~/scratch/ansible/next/hos/ansible
ansible-playbook -i hosts/verb_hosts monasca-start.yml --tags vertica</codeblock></li>
                      <li>Verify that the process has
                        restarted:<codeblock>cd ~/scratch/ansible/next/hos/ansible
ansible-playbook -i hosts/verb_hosts monasca-status.yml --tags vertica</codeblock></li>
                    </ol><note>Vertica is not started under <codeph>systemd</codeph>. To check the
                      process status from a host command line use <codeph>sudo /etc/init.d/verticad
                        status</codeph>. The <codeph>monasca-status.yml</codeph> playbook reports
                      whether Vertica thinks a node is down. The <codeph>verticad</codeph> process
                      could be running when the node is down, so the
                        <codeph>monasca-status.yml</codeph> playbook is a more accurate way to tell
                      if a Vertica host is up.</note></p></entry>
              </row>
              <row>
                <entry>Vertica Disk Rejection</entry>
                <entry>Alarms when vertica can not write to disk.</entry>
                <entry>Vertica got a rejection error when writing to disk. This can mean one of two
                  things: <ul id="ul_qcg_3zp_mx">
                    <li>That the file system that vertica is writing to is full, or</li>
                    <li>There is a disk problem.</li>
                  </ul><p>If the disk is full then the load that is on the system is higher then
                    expected or the retention period for Vertica is not aggressive enough and needs
                    to be addressed.</p></entry>
                <entry>To resolve this issue, use these steps:<ol id="ol_bhr_drp_mx">
                    <li>Log in to the node having the issue, as indicated by the
                        <codeph>hostname</codeph> dimension.</li>
                    <li>Check the Vertica filesystem using this
                        command:<codeblock>df -h</codeblock><p>If <codeph>/var/vertica</codeph>
                        shows greater than 90% usage then you have hit the limit of the Vertica
                        filesystem.</p></li>
                    <li>If that is the case, then you should recreate your Vertica database using
                      the steps documented here: <xref
                        href="troubleshooting/recover_vertica.dita#recover_vertica/recreate_vertica"
                        >Recreating the Vertica database</xref>.</li>
                  </ol><p>To ensure you do not run into this issue again, you should do the
                    following:</p><ul id="ul_rcg_3zp_mx">
                    <li>Turn down your metric load.</li>
                    <li>Lower the retention period on Vertica, as detailed in <xref
                        href="troubleshooting/recover_vertica.dita#recover_vertica/changing_retention"
                        >Changing the Vertica retention period</xref>.</li>
                    <li>Allocate more disk space to the Vertica file system.</li>
                  </ul><p>If the file system is not showing greater than 90% usage, then the Vertica
                    file system could be in a bad state. Verify that the Vertica file system can
                    still be written to and that it is functional.</p></entry>
              </row>
              <row>
                <entry>Vertica Low Watermark License Usage</entry>
                <entry>Alarms when vertica database is taking up 70 percent of the license
                  size.</entry>
                <entry>Alarms when the amount of raw data in your vertica database takes up 70
                  percent of the current vertica license limit.</entry>
                <entry>
                  <p>Since the system is approaching the Vertica license limit you may want to start
                    looking into these options:</p>
                  <ul id="ul_bm4_dsp_mx">
                    <li>Contacting Vertica to obtain a new license with more size than the current
                      12 TB</li>
                    <li>Look at the metric knobs and see if there are any you can turn down.</li>
                    <li>Lowering the Vertica retention period, as detailed in <xref
                        href="troubleshooting/recover_vertica.dita#recover_vertica/changing_retention"
                        >Changing the Vertica retention period</xref>.</li>
                  </ul>
                  <note>This is more of a warning alarm. If the Vertica High Watermark License Usage
                    alarm triggers, one of these options above will need to be taken.</note>
                </entry>
              </row>
              <row>
                <entry>Vertica High Watermark License Usage</entry>
                <entry>Alarms when vertica database is taking up 90 percent of the license
                  size.</entry>
                <entry>Alarms when the amount of raw data in your vertica database takes up 90
                  percent of the current vertica license limit.</entry>
                <entry>
                  <p>If this alarm triggers you will need to use one of the following options to
                    resolve this issue:</p>
                  <ul>
                    <li>Turn down the Vertica retention period, as detailed in <xref
                        href="troubleshooting/recover_vertica.dita#recover_vertica/changing_retention"
                        >Changing the Vertica retention period</xref>.</li>
                    <li>Turn down the metric load.</li>
                    <li>Purchase a Vertica license and then upgrade the Vertica license key, as
                      detailed in <xref href="monitoring/update_vertica_license.dita">Updating the
                        Vertica License Key</xref>.</li>
                  </ul>
                </entry>
              </row>
              <row>
                <entry>Vertica Node Status</entry>
                <entry>Alarms when the Vertica node is not up.</entry>
                <entry>The vertica node is not up.</entry>
                <entry>
                  <p>Verify that the database node is down and, if it is, then recover it by using
                    these steps:</p>
                  <ol id="ol_my3_fsp_mx">
                    <li>Log in to the lifecycle manager.</li>
                    <li>Run the Monasca status
                      playbook:<pre>cd ~/scratch/ansible/next/hos/ansible
ansible-playbook -i hosts/verb_hosts monasca-status.yml --tags vertica</pre></li>
                    <li>Run this playbook to do a recovery on
                      Vertica:<pre>cd ~/scratch/ansible/next/hos/ansible
ansible-playbook -i hosts/verb_hosts monasca-vertica-recovery</pre></li>
                  </ol>
                </entry>
              </row>
              <row>
                <entry>Vertica Connection Status</entry>
                <entry>Alarms when there is no connection to Vertica.</entry>
                <entry>Usually means that the node is down.</entry>
                <entry>
                  <p> Verify that the database node is down and, if it is, then recover it by using
                    these steps:</p>
                  <ol id="ol_syc_gsp_mx">
                    <li>Log in to the lifecycle manager.</li>
                    <li>Run the Monasca status
                      playbook:<pre>cd ~/scratch/ansible/next/hos/ansible
ansible-playbook -i hosts/verb_hosts monasca-status.yml --tags vertica</pre></li>
                    <li>Run this playbook to do a recovery on
                      Vertica:<pre>cd ~/scratch/ansible/next/hos/ansible
ansible-playbook -i hosts/verb_hosts monasca-vertica-recovery</pre></li>
                  </ol>
                </entry>
              </row>
              <row>
                <entry>Vertica Projection Ros Count</entry>
                <entry>Alarms when ros count for a projection goes over the limit.</entry>
                <entry>This usually means that the tuple mover is getting backed up and can not
                  merge the files on disks. If not addressed it has the potential of putting your
                  vertica database into a failed state.</entry>
                <entry><p>To troubleshoot this issue you will need to access the Vertica database on
                    the reporting monitoring node, as indicated by the <codeph>hostname</codeph>
                    dimension.</p>To obtain the Vertica database admin user credentials, use these
                    steps:<ol id="ol_vz1_fvp_mx">
                    <li>If you do not use data encryption on your lifecycle manager, ues these
                      steps: <ol id="ol_wz1_fvp_mx">
                        <li>Log in to the lifecycle manager.</li>
                        <li>Use this command to obtain the
                          password:<codeblock>grep -r dbadmin_user_password ~/scratch/ansible/next/hos/ansible/group_vars/</codeblock></li>
                      </ol></li>
                    <li>If you do use data encryption on your lifecycle manager, contact Support for
                      assistance in obtaining your Vertica database admin credentials.</li>
                  </ol><p>Once you have your credentials, follow these steps:</p><ol
                    id="ol_scg_3zp_mx">
                    <li>Log in to the lifecycle manager.</li>
                    <li>Run this playbook to stop the Monasca persister
                      process:<codeblock>cd ~/scratch/ansible/next/hos/ansible
ansible-playbook -i hosts/verb_hosts monasca-stop.yml --tags persister</codeblock></li>
                    <li>Manually merge the ros files for Vertica</li>
                    <li>Log onto the monitoring node.</li>
                    <li>Log into Vertica using the password obtained from the steps above (as root)
                      <codeblock>sudo su
/opt/vertica/bin/vsql -U dbadmin -w {{ vertica_admin_user_password }}</codeblock></li>
                    <li>Manually merge ros files running the following vsql commands:
                      <codeblock>SELECT DO_TM_TASK('mergeout', 'MonAlarms.StateHistory');
SELECT DO_TM_TASK('mergeout', 'MonMetrics.DefinitionDimensions');
SELECT DO_TM_TASK('mergeout', 'MonMetrics.Definitions');
SELECT DO_TM_TASK('mergeout', 'MonMetrics.Dimensions');
SELECT DO_TM_TASK('mergeout', 'MonMetrics.Measurements');</codeblock></li>
                  </ol><p>Restart the monasca-persister at this time, using these steps:</p><ol
                    id="ol_ozw_gsp_mx">
                    <li>Log in to the lifecycle manager.</li>
                    <li>Run monasca-stop focused on the
                      persister<codeblock>cd ~/scratch/ansible/next/hos/ansible
ansible-playbook -i hosts/verb_hosts monasca-start.yml --tags persister</codeblock></li>
                  </ol><p>To make sure this does not occur again, increase the Monasca wos resource
                    pool max memory.</p></entry>
              </row>
            </tbody>
          </tgroup>
        </table>
      </p>
    </section>
  </body>
</topic>
