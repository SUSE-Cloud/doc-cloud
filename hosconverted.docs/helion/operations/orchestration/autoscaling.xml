<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE topic PUBLIC "-//OASIS//DTD DITA Topic//EN" "topic.dtd">
<topic id="topic_sqg_cvb_dx">
  <title><ph conkeyref="HOS-conrefs/product-title"/>Autoscaling using the Orchestration
    Service</title>
  <abstract><shortdesc outputclass="hdphidden">Autoscaling is a process that can be used to scale up
      and down your compute resources based on the load they are currently experiencing to ensure a
      balanced load.</shortdesc></abstract>
  <body>
    <!-- https://wiki.hpcloud.net/display/auto/Heat+Monasca+AutoScaling+Docs -->
    <!-- DOCS-3308 -->
    <section id="about">
      <title>What is autoscaling?</title>
      <p>Autoscaling is a process that can be used to scale up and down your compute resources based
        on the load they are currently experiencing to ensure a balanced load across your compute
        environment.
        <note type="important">Autoscaling is only supported for KVM and RHEL KVM.</note>
      </p>
    </section>
    <section id="use">
      <title>How does autoscaling work?</title>
      <p>The monitoring service, Monasca, monitors your infrastructure resources and generates
        alarms based on their state. The orchestration service, Heat, talks to the Monasca API and
        offers the capability to templatize the existing Monasca resources, which are the Monasca
        Notification and Monasca Alarm definition. Heat can configure certain alarms for the
        infrastructure resources (compute instances and block storage volumes) it creates and can
        expect Monasca to notify continuously if a certain evaluation pattern in an alarm definition
        is met.</p>
      <p>For example, Heat can tell Monasca that it needs an alarm generated if the average CPU
        utilization of the compute instance in a scaling group goes beyond 90%.</p>
      <p>As Monasca continuously monitors all the resources in the cloud, if it happens to see a
        compute instance spiking above 90% load as configured by Heat, it generates an alarm and in
        turn sends a notification to Heat. Once Heat is notified, it will execute an action that was
        preconfigured in the template. Commonly, this action will be a scale up to increase the
        number of compute instances to balance the load that is being taken by the compute instance
        scaling group.</p>
      <p>Monasca sends a notification every 60 seconds while the alarm is in the ALARM state.</p>
    </section>
    <section>
      <title>Autoscaling template example</title>
      <p>The following Monasca alarm definition template snippet is an example of instructing
        Monasca to generate an alarm if the average CPU utilization in a group of compute instances
        exceeds beyond 50%. If the alarm is triggered, it will invoke the
          <codeph>up_notification</codeph> webhook once the alarm evaluation expression is
        satisfied.</p>
      <codeblock>cpu_alarm_high:
  type: OS::Monasca::AlarmDefinition
  properties:
    name: CPU utilization beyond 50 percent
    description: CPU utilization reached beyond 50 percent
    expression:
    str_replace:
    template: avg(cpu.utilization_perc{scale_group=scale_group_id}) > 50 times 3
    params:
    scale_group_id: {get_param: "OS::stack_id"}
    severity: high
    alarm_actions:
      - {get_resource: up_notification }</codeblock>
      <p>The following Monasca notification template snippet is an example of creating a Monasca
        notification resource that will be used by the alarm definition snippet to notify
      Heat.</p><codeblock>up_notification:
  type: OS::Monasca::Notification
  properties:
    type: webhook
    address: {get_attr: [scale_up_policy, alarm_url]}</codeblock>
    </section>
    <section>
      <title>Monasca Agent configuration options</title>
      <p>There is a Monasca Agent configuration option which controls the behavior around compute
        instance creation and the measurements being received from the compute instance.</p>
      <p>The variable is <codeph>monasca_libvirt_vm_probation</codeph> which is set in the
          <codeph>~/helion/my_cloud/config/nova/libvirt-monitoring.yml</codeph> file. Here is a
        snippet of the file showing the description and variable:</p>
      <codeblock># The period of time (in seconds) in which to suspend metrics from a
# newly-created VM. This is used to prevent creating and storing
# quickly-obsolete metrics in an environment with a high amount of instance
# churn (VMs created and destroyed in rapid succession).  Setting to 0
# disables VM probation and metrics will be recorded as soon as possible
# after a VM is created.  Decreasing this value in an environment with a high
# amount of instance churn can have a large effect on the total number of
# metrics collected and increase the amount of CPU, disk space and network
# bandwidth required for Monasca. This value may need to be decreased if
# Heat Autoscaling is in use so that Heat knows that a new VM has been
# created and is handling some of the load.
monasca_libvirt_vm_probation: 300</codeblock>
      <p>The default value is <codeph>300</codeph>. This is the time in seconds that a compute
        instance must live before the Monasca libvirt agent plugin will send measurements for it.
        This is so that the Monasca metrics database does not fill with measurements from short
        lived compute instances. However, this means that the Monasca threshold engine will not see
        measurements from a newly created compute instance for at least five minutes on scale up. If
        the newly created compute instance is able to start handling the load in less than five
        minutes, then Heat autoscaling may mistakenly create another compute instance since the
        alarm does not clear.</p>
      <p>If the default <codeph>monasca_libvirt_vm_probation</codeph> turns out to be an issue, it
        can be lowered. However, that will affect all compute instances, not just ones used by Heat
        autoscaling which can increase the number of measurements stored in Monasca if there are
        many short lived compute instances. You should consider how often compute instances are
        created that live less than the new value of <codeph>monasca_libvirt_vm_probation</codeph>.
        If few, if any, compute instances live less than the value of
          <codeph>monasca_libvirt_vm_probation</codeph>, then this value can be decreased without
        causing issues. If many compute instances live less than the
          <codeph>monasca_libvirt_vm_probation</codeph> period, then decreasing
          <codeph>monasca_libvirt_vm_probation</codeph> can cause excessive disk, CPU and memory
        usage by Monasca.</p>
      <p>If you wish to change this value, follow these steps:<ol id="ol_zkg_hgy_kx">
          <li>Log in to the lifecycle manager.</li>
          <li>Edit the <codeph>monasca_libvirt_vm_probation</codeph> value in this configuration
            file:<codeblock>~/helion/my_cloud/config/nova/libvirt-monitoring.yml</codeblock></li>
          <li>Commit your changes to the local
            git:<codeblock>cd ~/helion/hos/ansible
git add --all
git commit -m "changing Monasca Agent configuration option"</codeblock></li>
          <li>Run the configuration
            processor:<codeblock>cd ~/helion/hos/ansible
ansible-playbook -i hosts/localhost config-processor-run.yml</codeblock></li>
          <li>Update your deployment
            directory:<codeblock>cd ~/helion/hos/ansible
ansible-playbook -i hosts/localhost ready-deployment.yml</codeblock></li>
          <li>Run this playbook to reconfigure the Nova service and enact your
            changes:<codeblock>cd ~/scratch/ansible/next/hos/ansible
ansible-playbook -i hosts/verb_hosts nova-reconfigure.yml</codeblock></li>
        </ol></p>
    </section>
  </body>
</topic>
