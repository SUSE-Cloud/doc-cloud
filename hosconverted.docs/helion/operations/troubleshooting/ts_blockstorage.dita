<?xml version="1.0" encoding="UTF-8"?>
<!--Edit status: not edited-->
<!DOCTYPE topic PUBLIC "-//OASIS//DTD DITA Topic//EN" "topic.dtd">
<topic id="troubleshooting_blockstorage">
  <title><ph conkeyref="HOS-conrefs/product-title"/>Block Storage Troubleshooting</title>
  <abstract><shortdesc outputclass="hdphidden">Troubleshooting scenarios with resolutions for the
      Cinder, VSA, Ceph, and 3Par services.</shortdesc></abstract>
  <body>
    <section id="about">
      <p>The block storage service utilizes OpenStack Cinder and can integrate with multiple
        backends including VSA, Ceph, or 3Par. Failures may exist at the Cinder API level, an
        operation may fail, or you may see an alarm trigger in the monitoring service. These may be
        caused by configuration problems, network issues, or issues with your servers or storage
        backends. The purpose of this page and section is to describe how the service works, where
        to find additional information, some of the common problems that come up, and how to address
        them.</p>
    </section>

    <section id="logs">
      <title>Where to find information</title>
      <p>When debugging block storage issues it is helpful to understand the deployment topology and
        know where to locate the logs with additional information.</p>
      <p>The Cinder service consists of:</p>
      <ul>
        <li>An API service, typically deployed and active on the controller nodes.</li>
        <li>A scheduler service, also typically deployed and active on the controller nodes.</li>
        <li>A volume service, which is deployed on all of the controller nodes but only active on
          one of them.</li>
        <li>A backup service, which is deployed on the same controller node as the volume
          service.</li>
      </ul>
      <p><image href="../../../media/hos.docs/troubleshooting/cinder_topology.png"/></p>
      <p>You can refer to your configuration files (usually located in
          <codeph>~/helion/my_cloud/definition/</codeph> on the lifecycle manager) for specifics
        about where your services are located. They will usually be located on the controller
        nodes.</p>
      <p>Cinder uses a MySQL database and communicates between components by consuming messages from
        a RabbitMQ message service.</p>
      <p>The Cinder API service is layered underneath a HAProxy service and accessed using a virtual
        IP address maintained using keepalived.</p>
      <p>If any of the Cinder components is not running on its intended host then an alarm will be
        raised. Details on how to resolve these alarms can be found on our <xref
          href="../alarm_resolutions.dita"/> page. You should check the logs for the service on the
        appropriate nodes. All Cinder logs are stored in <codeph>/var/log/cinder/</codeph> and all
        log entries above <codeph>INFO</codeph> level are also sent to the centralized logging
        service. For details on how to change the logging level of the Cinder service, see <xref
          href="../central_log_configure_services.dita"/>.</p>
      <p>In order to get the full context of an error you may need to examine the full log files on
        individual nodes. Note that if a component runs on more than one node you will need to
        review the logs on each of the nodes that component runs on. Also remember that as logs
        rotate that the time interval you are interested in may be in an older log file.</p>
      <p><b>Log locations:</b></p>
      <p><codeph>/var/log/cinder/cinder-api.log</codeph> - Check this log if you have endpoint or
        connectivity issues</p>
      <p><codeph>/var/log/cinder/cinder-scheduler.log</codeph> - Check this log if the system cannot
        assign your volume to a backend</p>
      <p><codeph>/var/log/cinder/cinder-backup.log</codeph> - Check this log if you have backup or
        restore issues</p>
      <p><codeph>/var/log/cinder-cinder-volume.log</codeph> - Check here for failures during volume
        creation</p>
      <p><codeph>/var/log/nova/nova-compute.log</codeph> - Check here for failures with attaching
        volumes to compute instances</p>
      <p>You can also check the logs for the database and/or the RabbitMQ service if your cloud
        exhibits database or messaging errors.</p>
      <p>If the API servers are up and running but the API is not reachable then checking the
        HAProxy logs on the active keepalived node would be the place to look.</p>
      <p>If you have errors attaching volumes to compute instances using the Nova API then the logs
        would be on the compute node associated with the instance. You can use the following command
        to determine which node is hosting the instance:</p>
      <codeblock>nova show &lt;instance_uuid></codeblock>
      <p>Then you can check the logs located at <codeph>/var/log/nova/nova-compute.log</codeph> on
        that compute node.</p>
    </section>

    <section id="volume_states">
      <title>Understanding the Cinder volume states</title>
      <p>Once the topology is understood, if the issue with the Cinder service relates to a specific
        volume then you should have a good understanding of what the various states a volume can be
        in are. The states are:</p>
      <ul>
        <li>attaching</li>
        <li>available</li>
        <li>backing-up</li>
        <li>creating</li>
        <li>deleting</li>
        <li>downloading</li>
        <li>error</li>
        <li>error attaching</li>
        <li>error deleting</li>
        <li>error detaching</li>
        <li>error extending</li>
        <li>error restoring</li>
        <li>in-use</li>
        <li>extending</li>
        <li>restoring</li>
        <li>restoring backup</li>
        <li>retyping</li>
        <li>uploading</li>
      </ul>
      <p>The common states are <codeph>in-use</codeph> which indicates a volume is currently
        attached to a compute instance and <codeph>available</codeph> means the volume is created on
        a backend and is free to be attached to an instance. All <codeph>-ing</codeph> states are
        transient and represent a transition. If a volume stays in one of those states for too long
        indicating it is stuck, or if it fails and goes into an error state, you should check for
        failures in the logs.</p>
    </section>

    <section id="troubleshooting">
      <title>Initial troubleshooting steps</title>
      <p>These should be the initial troubleshooting steps you go through.</p>
      <ol>
        <li>If you've noticed an issue with the service, you should check your monitoring system for
          any alarms that may have triggered. See <xref href="../alarm_resolutions.dita"/> for
          resolution steps for those alarms.</li>
        <li>Check if the Cinder API service is active by listing the available volumes from the
          lifecycle manager: <codeblock>source ~/service.osrc
openstack volume list</codeblock></li>
        <li>Run a basic diagnostic from the lifecycle manager: <codeblock>cd ~/scratch/ansible/next/hos/ansible
ansible-playbook -i hosts/verb_hosts _cinder_post_check.yml</codeblock>
          <p>This ansible playbook will list all volumes, create a 1 GB volume and then delete it
            using the v1 and v2 APIs, which will exercise basic Cinder capability.</p></li>
      </ol>
    </section>

    <section id="common_issues">
      <title>Common failures</title>
      <p><b>Alerts from the Cinder service</b></p>
      <p>Check for alerts associated with the block storage service, noting that these could include
        alerts related to the server nodes being down, alerts related to the messaging and database
        services, or the HAProxy and keepalived services, as well as alerts directly attributed to
        the block storage service.</p>
      <p>The Operations Console (Ops Console) provides a web UI method for checking alarms. See
          <xref href="../opsconsole_overview.dita"/> for details on how to connect to the Ops
        Console.</p>
      <p><b>Cinder volume service is down</b></p>
      <p>The Cinder volume service could be down if the server hosting the volume service fails. In
        this case you should follow the documented procedure linked below to start the volume
        service on another controller node. See <xref
          href="../blockstorage/managing_cinder_volumebackup_services.dita"/> for details.</p>
      <p><b>Creating a Cinder bootable volume fails</b></p>
      <p>When creating a bootable volume from an image, your Cinder volume must be larger than the
        Virtual Size (raw size) of your image or creation will fail with an error.</p>
      <p>An error like this error would appear in <codeph>cinder-volume.log</codeph> file:</p>
      <codeblock>'2016-06-14 07:44:00.954 25834 ERROR oslo_messaging.rpc.dispatcher ImageCopyFailure: Failed to copy image to volume: qemu-img: /dev/disk/by-path/ip-192.168.92.5:3260-iscsi-iqn.2003-10.com.lefthandnetworks:mg-vsa:146:volume-c0e75c66-a20a-4368-b797-d70afedb45cc-lun-0: error while converting raw: Device is too small
2016-06-14 07:44:00.954 25834 ERROR oslo_messaging.rpc.dispatcher'</codeblock>
      <p>In an example where creating a 1GB bootable volume fails, your image may look like
        this:</p>
      <codeblock>$ qemu-img info /tmp/image.qcow2
image: /tmp/image.qcow2
file format: qcow2
virtual size: 1.5G (1563295744 bytes)
disk size: 354M
cluster_size: 65536
...</codeblock>
      <p>In this case, note that the image format is qcow2 and hte virtual size is 1.5GB, which is
        greater than the size of the bootable volume. Even though the compressed image size is less
        than 1GB, this bootable volume creation will fail.</p>
      <p>When creating your disk model for nodes that will have the cinder volume role make sure
        that there is sufficient disk space allocated for a temporary space for image conversion if
        you will be creating bootable volumes. You should allocate enough space to the filesystem as
        would be needed to cater for the raw size of images to be used for bootable volumes - for
        example Windows images can be quite large in raw format.</p>
      <p>By default, Cinder uses <codeph>/var/lib/cinder</codeph> for image conversion and this will
        be on the root filesystem unless it is explicitly separated. You can ensure there is enough
        space by ensuring that the root file system is sufficiently large, or by creating a logical
        volume mounted at <codeph>/var/lib/cinder</codeph> in the disk model when installing the
        system.</p>
      <p>If your system is already installed, use these steps to update this:</p>
      <ol>
        <li>Edit the configuration item <codeph>image_conversion_dir</codeph> in
            <codeph>cinder.conf.j2</codeph> to point to another location with more disk space. Make
          sure that the new directory location has the same ownership and permissions as
            <codeph>/var/lib/cinder</codeph> (owner:cinder group:cinder. mode 0750).</li>
        <li>Then run this playbook:
          <codeblock>cd ~/scratch/ansible/next/hos/ansible
ansible-playbook -i hosts/verb_hosts cinder-reconfigure.yml</codeblock></li>
      </ol>
      <p><b>API-level failures</b></p>
      <p>If the API is inaccessible, determine if the API service is running on the target node. If
        it isn't, check to see why the API service isn't running in the log files. If it is running
        okay, check if the HAProxy service is functioning properly.</p>
      <note>After a controller node is rebooted, you must make sure to run the
          <codeph>hlm-start.yml</codeph> playbook to ensure all the services are up and running. See
          <xref href="../maintenance/controller/restart_controller.dita"/> for details.</note>
      <p>If the API service is returning an error code, look for the error message in the API logs
        on all API nodes. Successful completions would be logged like this:</p>
      <codeblock>2016-04-25 10:09:51.107 30743 INFO eventlet.wsgi.server [<b>req-a14cd6f3-6c7c-4076-adc3-48f8c91448f6</b> 
dfb484eb00f94fb39b5d8f5a894cd163 7b61149483ba4eeb8a05efa92ef5b197 - - -] 192.168.186.105 - - [25/Apr/2016 
10:09:51] "GET /v2/7b61149483ba4eeb8a05efa92ef5b197/volumes/detail HTTP/1.1" <b>200</b> 13915 0.235921</codeblock>
      <p>where <codeph>200</codeph> represents HTTP status 200 for a successful completion. Look for
        a line with your status code and then examine all entries associated with the request id.
        The request ID in the successful completion is highlighted in bold above.</p>
      <p>The request may have failed at the scheduler or at the volume or backup service and you
        should also check those logs at the time interval of interest, noting that the log file of
        interest may be on a different node.</p>
      <p><b>Operations that do not complete</b></p>
      <p>If you have started an operation, such as creating or deleting a volume, that does not
        complete, the Cinder volume may be stuck in a state. You should follow the procedures for
        detaling with stuck volumes.</p>
      <p>There are six transitory states that a volume can get stuck in:</p>
      <p>
        <table frame="all" rowsep="1" colsep="1" id="table_jwh_vml_gw">
          <tgroup cols="2">
            <colspec colname="c1" colnum="1"/>
            <colspec colname="c2" colnum="2"/>
            <thead>
              <row>
                <entry>State</entry>
                <entry>Description</entry>
              </row>
            </thead>
            <tbody>
              <row>
                <entry>creating</entry>
                <entry>The Cinder volume manager has sent a request to a backend driver to create a
                  volume, but has not received confirmation that the volume is available.</entry>
              </row>
              <row>
                <entry>attaching</entry>
                <entry>Cinder has received a request from Nova to make a volume available for
                  attaching to an instance but has not received confirmation from Nova that the
                  attachment is complete.</entry>
              </row>
              <row>
                <entry>detaching</entry>
                <entry>Cinder has received notification from Nova that it will detach a volume from
                  an instance but has not received notification that the detachment is
                  complete.</entry>
              </row>
              <row>
                <entry>deleting</entry>
                <entry>Cinder has received a request to delete a volume but has not completed the
                  operation.</entry>
              </row>
              <row>
                <entry>backing-up</entry>
                <entry>Cinder backup manager has started to back a volume up to Swift, or some other
                  backup target, but has not completed the operation.</entry>
              </row>
              <row>
                <entry>restoring</entry>
                <entry>Cinder backup manager has started to restore a volume from Swift, or some
                  other backup target, but has not completed the operation.</entry>
              </row>
            </tbody>
          </tgroup>
        </table>
      </p>
      <p>At a high level, the steps that you would take to address any of these states are
        similar:</p>
      <ol>
        <li>Confirm that the volume is actually stuck, and not just temporarily blocked.</li>
        <li>Where possible, remove any resources being held by the volume. For example, if a volume
          is stuck detaching it may be necessary to remove associated iSCSI or DM devices on the
          compute node.</li>
        <li>Reset the state of the volume to an appropriate state, for example to
            <codeph>available</codeph> or <codeph>error</codeph>.</li>
        <li>Do any final cleanup. For example, if you reset the state to <codeph>error</codeph> you
          can then delete the volume.</li>
      </ol>
      <p>The next sections will describe specific steps you can take for volumes stuck in each of
        the transitory states.</p>
      <p><b>Volumes stuck in Creating</b></p>
      <p>Broadly speaking, there are two possible scenarios where a volume would get stuck in
          <codeph>creating</codeph>. The <codeph>cinder-volume</codeph> service could have thrown an
        exception while it was attempting to create the volume, and failed to handle the exception
        correctly. Or the volume backend could have failed, or gone offline, after it received the
        request from Cinder to create the volume.</p>
      <p>These two cases are different in that for the second case you will need to determine the
        reason the backend is offline and restart it. Often, when the backend has been restarted,
        the volume will move from <codeph>creating</codeph> to <codeph>available</codeph> so your
        issue will be resolved.</p>
      <p>If you can create volumes successfully on the same backend as the volume stuck in
          <codeph>creating</codeph> then the backend is not down. So you will need to reset the
        state for the volume and then delete it.</p>
      <p>To reset the state of a volume you can use the <codeph>cinder reset-state</codeph> command.
        You can use either the UUID or the volume name of the stuck volume.</p>
      <p>For example, here is a volume list where we have a stuck volume:</p>
      <codeblock>$ cinder list
+--------------------------------------+-----------+------+------+-------------+------------+
|                  ID                  |   Status  | Name | Size | Volume Type |Attached to |
+--------------------------------------+-----------+------+------+-------------+------------+
| 14b76133-e076-4bd3-b335-fa67e09e51f6 | creating  | vol1 |  1   |      -      |            |
+--------------------------------------+-----------+------+------+-------------+------------+</codeblock>
      <p>You can reset the state by using the <codeph>cinder reset-state</codeph> command, like
        this:</p>
      <codeblock>cinder reset-state --state error 14b76133-e076-4bd3-b335-fa67e09e51f6</codeblock>
      <p>Confirm that with another listing:</p>
      <codeblock>$ cinder list
+--------------------------------------+-----------+------+------+-------------+------------+
|                  ID                  |   Status  | Name | Size | Volume Type |Attached to |
+--------------------------------------+-----------+------+------+-------------+------------+
| 14b76133-e076-4bd3-b335-fa67e09e51f6 | error     | vol1 |  1   |      -      |            |
+--------------------------------------+-----------+------+------+-------------+------------+</codeblock>
      <p>You can then delete the volume:</p>
      <codeblock>$ cinder delete 14b76133-e076-4bd3-b335-fa67e09e51f6
Request to delete volume 14b76133-e076-4bd3-b335-fa67e09e51f6 has been accepted.</codeblock>
      <p><b>Volumes stuck in Deleting</b></p>
      <p>If a volume is stuck in the deleting state then the request to delete the volume may or may
        not have been sent to and actioned by the backend. If you can identify volumes on the
        backend then you can examine the backend to determine whether the volume is still there or
        not. Then you can decide which of the following paths you can take. It may also be useful to
        determine whether the backend is responding, either by checking for recent volume create
        attempts, or creating and deleting a test volume.</p>
      <p>The first option is to reset the state of the volume to <codeph>available</codeph> and then
        attempt to delete the volume again.</p>
      <p>The second option is to reset the state of the volume to <codeph>error</codeph> and then
        delete the volume.</p>
      <p>If you've reset the volume state to <codeph>error</codeph> then the volume may still be
        consuming storage on the backend. If that is the case then you will need to delete it from
        the backend using your backend's specific tool.</p>
      <p><b>Volumes stuck in Attaching</b></p>
      <p>The most complicated situation to deal with is where a volume is stuck either in attaching
        or detaching, because as well as dealing with the state of the volume in Cinder and the
        backend, you have to deal with exports from the backend, imports to the compute node, and
        attachments to the compute instance.</p>
      <p>The two options you have here are to make sure that all exports and imports are deleted and
        to reset the state of the volume to <codeph>available</codeph> or to make sure all of the
        exports and imports are correct and to reset the state of the volume to
          <codeph>in-use</codeph>.</p>
      <p>A volume that is in attaching state should never have been made available to a compute
        instance and therefore should not have any data written to it, or in any buffers between the
        compute instance and the volume backend. In that situation, it is often safe to manually
        tear down the devices exported on the backend and imported on the compute host and then
        reset the volume state to <codeph>available</codeph>.</p>
      <p>You can use the management features of the backend you're using to locate the compute host
        to where the volume is being exported. The following section describes how to do this for a
        VSA backend.</p>
      <p><b>HP StoreVirtual Storage (VSA)</b></p>
      <p>On most backend the volume name will be based on the Cinder UUID, so the volume
          <codeph>7237a7ea-c77f-4edb-9f4f-7928cb605f44</codeph> will have the name
          <codeph>volume-7237a7ea-c77f-4edb-9f4f-7928cb605f44</codeph> on the backend. This is the
        case for VSA. On VSA you can get the IP address of the compute host from the
          <codeph>initiatorAddress</codeph> field in the output from the
          <codeph>getVolumeInfo</codeph> command. For example:</p>
      <codeblock>$ ssh -p16022 192.168.185.3 -lstack 'getVolumeInfo volumeName=volume-7237a7ea-c77f-4edb-9f4f-7928cb605f44' | grep initiatorAddress
Password: 
initiatorAddress 192.168.185.94</codeblock>
      <p>If there is no <codeph>initiatorAddress</codeph> line in the output from
          <codeph>getVolumeInfo</codeph> then the volume has not been exported from the backend and
        it is safe to reset the volume type to <codeph>available</codeph> using the <codeph>cinder
          reset-state</codeph> command as follows:</p>
      <codeblock>$ cinder reset-state --state available --attach-status detached 7237a7ea-c77f-4edb-9f4f-7928cb605f44</codeblock>
      <p>If the volume has been exported to a compute host you can get the name of the iSCSI target
        for the volume, using CLIQ this would be:</p>
      <codeblock>$ ssh -p16022 192.168.185.3 -lstack 'getVolumeInfo volumeName=volume-7237a7ea-c77f-4edb-9f4f-7928cb605f44' | grep iscsiIqn
Password: 
iscsiIqn                           iqn.2003-10.com.lefthandnetworks:vsacluster:423:volume-7237a7ea-c77f-4edb-9f4f-7928cb605f44</codeblock>
      <p>You can then log into the compute host, using the IP address from the
          <codeph>initiatorAddress</codeph> field in the <codeph>getVolumeInfo</codeph> output,
          <codeph>192.168.185.94</codeph> in the example above, and check whether it is importing
        the volume. Search the iSCSI target using <codeph>iscsiadm</codeph>. In the example below
        we've used a substring from the iSCSI target reported by CLIQ. This is to make the example
        easier to read and you can cut and paste the entire string:</p>
      <codeblock>$ sudo iscsiadm -msession |grep volume-7237a7ea-c77f-4edb-9f4f-7928cb605f44
tcp: [3] 192.168.185.3:3260,1 iqn.2003-10.com.lefthandnetworks:vsacluster:423:volume-7237a7ea-c77f-4edb-9f4f-7928cb605f44 (non-flash)</codeblock>
      <p>If the target is not present on the compute host then it is safe to use CLIQ, or CMC, to
        disconnect the volume on the VSA server and then reset the state of the volume to
          <codeph>available</codeph> using the <codeph>cinder reset-state</codeph> command.</p>
      <p>If the target is present on the compute host then you need to check whether or not it has
        been attached to any running instance. You can get a list of the instances running on the
        compute host using <codeph>virsh</codeph>:</p>
      <codeblock>$ sudo virsh list
 Id    Name                           State
----------------------------------------------------
 4     instance-00000018              running</codeblock>
      <p>You can then check each of the instances to see whether the volume has been imported. Using
        the same example, we use a substring from the iSCSI target:</p>
      <codeblock>$ sudo virsh dumpxml 4|grep volume-7237a7ea-c77f-4edb-9f4f-7928cb605f44
        &lt;source dev='/dev/disk/by-path/ip-192.168.185.3:3260-iscsi-iqn.2003-10.com.lefthandnetworks:vsacluster:423:volume-7237a7ea-c77f-4edb-9f4f-7928cb605f44-lun-0'/></codeblock>
      <p>If the iSCSI target does not appear in the XML representation of any of the instances on
        the compute host then you can use <codeph>iscsiadm</codeph> to log out from the scsi
        session, disconnect the volume on VSA as described above, and reset the volume state to
          <codeph>available</codeph>.</p>
      <p>To log out from the scsi session you need the session id, which is the integer in square
        brackets in the output from <codeph>iscsiadm -msession</codeph> which was <codeph>3</codeph>
        in the example above. So the command to log out would be:</p>
      <codeblock>$ sudo iscsiadm -m session -r 3 -u</codeblock>
      <p>However, if the volume has been imported by an instance then you must reset the volume
        state to <codeph>in-use</codeph> and then attempt to detach the volume from the instance. If
        the detach succeeds tehn the problem is resolved. However, if the detach fails there may be
        a more fundamental problem which is outside the scope of this document and you should
        contact the Support team.</p>
      <p><b>Volumes stuck in Detaching</b></p>
      <p>The steps in dealing with a volume stuck in <codeph>detaching</codeph> state are very
        similar to those for a volume stuck in <codeph>attaching</codeph>. However, there is the
        added consideration that the volume was attached to, and probably servicing, I/O from a
        compute instance. So you must take care to ensure that all buffers are properly flushed
        before detaching the volume.</p>
      <p>When a volume is stuck in <codeph>detaching</codeph>, the output from a <codeph>cinder
          list</codeph> command will include the UUID for the instance to which the volume was
        attached. From that you can identify the compute host that is running the instance using the
          <codeph>nova show</codeph> command.</p>
      <p>For example, here are some snippets:</p>
      <codeblock>$ cinder list
+--------------------------------------+-----------+-----------------------+-----------------+
|                  ID                  |   Status  |       Name            |   Attached to   |
+--------------------------------------+-----------+-----------------------+-----------------+
| 85384325-5505-419a-81bb-546c69064ec2 | detaching |        vol1           | 4bedaa76-78ca-… |
+--------------------------------------+-----------+-----------------------+-----------------+</codeblock>
      <codeblock>$ nova show 4bedaa76-78ca-4fe3-806a-3ba57a9af361|grep host
| OS-EXT-SRV-ATTR:host                 | mycloud-cp1-comp0005-mgmt                                                
| OS-EXT-SRV-ATTR:hypervisor_hostname  | mycloud-cp1-comp0005-mgmt                                                                   
| hostId                               | 61369a349bd6e17611a47adba60da317bd575be9a900ea590c1be816</codeblock>
      <p>The first thing to check in this case is whether the instance is still importing the
        volume. Use <codeph>virsh list</codeph> and <codeph>virsh dumpxml</codeph> as described in
        the section above. If the XML for the instance has a reference to the device, then you
        should reset the volume state to <codeph>in-use</codeph> and attempt the <codeph>cinder
          detach</codeph> operation again.</p>
      <codeblock>$ cinder reset-state --state in-use --attach-status attached 85384325-5505-419a-81bb-546c69064ec2</codeblock>
      <p>If the volume gets stuck detaching again, there may be a more fundamental problem, which is
        outside the scope of this document and you should contact the Support team.</p>
      <p>If the volume is not referenced in the XML for the instance then you should remove any
        devices on the compute node and backend and then reset the state of the volume to
          <codeph>available</codeph>.</p>
      <codeblock>$ cinder reset-state --state available --attach-status detached 85384325-5505-419a-81bb-546c69064ec2</codeblock>
      <p>You can use the management features of the backend you're using to locate the compute host
        to where the volume is being exported. The following section describes how to do this for a
        VSA backend.</p>
      <p><b>HP StoreVirtual Storage (VSA)</b></p>
      <p>Use the processes described in the <codeph>attaching</codeph> section above to identify the
        devices on the compute host that should be deleted, and the targets on the VSA node that
        should be removed.</p>
      <p><b>Volumes stuck in backing-up</b></p>
      <p>When a volume is backed up using <codeph>cinder-backup</codeph> the volume is attached to
        the host running the <codeph>cinder-backup</codeph> service, which reads data from the
        volume and stores it elsewhere. Typically, backups are stored in Swift, but Cinder can be
        configured to store them in a number of other repositories, including Ceph.</p>
      <p>Volumes stuck in <codeph>backing-up</codeph> and <codeph>restoring</codeph> are much more
        rare than volumes stuck in <codeph>attaching</codeph> and <codeph>detaching</codeph>. The
        process of attaching a volume to a Nova instance involves many stages of communication
        between Nova and Cinder any of which can break down, but when Cinder is backing up a volume
        the only communication between Cinder and the storage server is simple PUT operations.</p>
      <p>Be aware that the backup process can take a long time, up to several hours for large
        volumes and narrow bandwidth. So you should take care to ensure that the process is actually
        stuck and not just slow.</p>
      <p>One way to do this is to examine the log file for the <codeph>cinder-backup</codeph>
        process. To do that you need to SSH to the controller node running the
          <codeph>cinder-backup</codeph> process. This is usually the first controller node in your
        cluster. Confirm that you are on the correct node by checking for the backup process using
        systemctl:</p>
      <codeblock>systemctl status cinder-backup
 cinder-backup.service - cinder-backup Service
  Loaded: loaded (/usr/lib/systemd/system/cinder-backup.service; disabled)
  Active: active (running) since Thu 2016-04-21 11:44:38 UTC; 1 day 3h ago
 Main PID: 6689 (cinder-backup)
  CGroup: /system.slice/cinder-backup.service
        └─6689 /opt/stack/venv/cinder-20160405T101007Z/bin/pyt ...

Apr 22 15:09:08 cirrushelion-cp1-c1-m1-mgmt sudo[29223]: cinder : ...
Apr 22 15:09:08 cirrushelion-cp1-c1-m1-mgmt sudo[29223]: pam_unix ...
Apr 22 15:09:11 cirrushelion-cp1-c1-m1-mgmt sudo[29300]: cinder : ...
Apr 22 15:09:11 cirrushelion-cp1-c1-m1-mgmt sudo[29300]: pam_unix ...
Apr 22 15:09:11 cirrushelion-cp1-c1-m1-mgmt sudo[29305]: cinder : ...
Apr 22 15:09:11 cirrushelion-cp1-c1-m1-mgmt sudo[29305]: pam_unix ...
Apr 22 15:09:11 cirrushelion-cp1-c1-m1-mgmt sudo[29308]: cinder : ...
Apr 22 15:09:11 cirrushelion-cp1-c1-m1-mgmt sudo[29308]: pam_unix ...
Apr 22 15:09:11 cirrushelion-cp1-c1-m1-mgmt sudo[29311]: cinder : ...
Apr 22 15:09:11 cirrushelion-cp1-c1-m1-mgmt sudo[29311]: pam_unix ...
Hint: Some lines were ellipsized, use -l to show in full.</codeblock>
      <p>Then, check whether the <codeph>cinder-backup</codeph> service is writing data to a backup
        driver by tailing the <codeph>cinder-backup.log</codeph> file. A log message is usually
        written every couple of seconds. Here is an example, which we've truncated:</p>
      <codeblock># tail -f /var/log/cinder/cinder-backup.log |grep 162de6d5-ba92-4e36-aba4-e37cac41081b
2016-04-22 15:21:01.235 6689 DEBUG cinder.backup.drivers.swift [req-41d8f821-7fce-445f-a073-
2016-04-22 15:21:01.235 6689 DEBUG cinder.backup.drivers.swift [req-41d8f821-7fce-445f-a073-
2016-04-22 15:21:01.338 6689 DEBUG cinder.backup.chunkeddriver [req-41d8f821-7fce-445f-a073-
2016-04-22 15:21:04.178 6689 DEBUG requests.packages.urllib3.connectionpool [req-7f7de8dc-3193-
2016-04-22 15:21:04.179 6689 DEBUG swiftclient [req-7f7de8dc-3193-41e1-ac50-352a45c5565a - - 
2016-04-22 15:21:04.180 6689 DEBUG cinder.backup.drivers.swift [req-7f7de8dc-3193-41e1-ac50-
2016-04-22 15:21:04.180 6689 DEBUG cinder.backup.drivers.swift [req-7f7de8dc-3193-41e1-ac50-
2016-04-22 15:21:04.283 6689 DEBUG cinder.backup.chunkeddriver [req-7f7de8dc-3193-41e1-ac50-</codeblock>
      <p>If you determine that the volume is genuinely stuck in <codeph>backing-up</codeph> then you
        will treat the volume similarly to the way you would treat a volume that was stuck in
          <codeph>detaching</codeph>. The difference would be that the volume will be imported on a
        controller node and not on a compute node.</p>
      <p>Identify the volume on the backend and then the devices importing the volume on the
        controller node. Then remove the devices from the controller node as described in the
        sections above and reset the state of the volume to <codeph>available</codeph> and then
          <codeph>volume attach-status</codeph> to <codeph>detached</codeph>. You can now repeat the
        attempt to back up the volume. If the problem reoccurs then you may have a more fundamental
        problem and you should contactt the Support team.</p>
      <p>Note that you may now have a Cinder backup that is stuck in <codeph>creating</codeph>,
        correcting this is outside the scope of this document.</p>
      <p><b>Volumes stuck in restoring</b></p>
      <p>Restoring a Cinder volume from backup will be as slow as backing it up. So you must confirm
        that the volume is actually stuck by examining the <codeph>cinder-backup.log</codeph>. For
        example:</p>
      <codeblock># tail -f cinder-backup.log |grep 162de6d5-ba92-4e36-aba4-e37cac41081b
2016-04-27 12:39:14.612 6689 DEBUG swiftclient [req-0c65ec42-8f9d-430a-b0d5-05446bf17e34 - - 
2016-04-27 12:39:15.533 6689 DEBUG cinder.backup.chunkeddriver [req-0c65ec42-8f9d-430a-b0d5-
2016-04-27 12:39:15.566 6689 DEBUG requests.packages.urllib3.connectionpool [req-0c65ec42-    
2016-04-27 12:39:15.567 6689 DEBUG swiftclient [req-0c65ec42-8f9d-430a-b0d5-05446bf17e34 - - -</codeblock>
      <p>If you determine that the volume is genuinely stuck in <codeph>detaching</codeph> then you
        must follow the procedure described in the detaching section above to remove any volumes
        that remain exported from the backend and imported on the controller node. Remember that in
        this case the volumes will be imported and mounted on the controller node running
          <codeph>cinder-backup</codeph>. So you do not have to search for the correct compute host.
        Also remember that no instances are involved so you do not need to confirm that the volume
        is not imported to any instances.</p>
    </section>

    <section id="debugging_attachment">
      <title>Debugging volume attachment</title>
      <p>In an error case, it is possible for a Cinder volume to fail to complete an operation and
        revert back to its initial state. For example, attaching a Cinder volume to a Nova instance,
        so you would follow the steps above to examine the Nova compute logs for the attach
        request.</p>
    </section>

    <section id="errors_creating">
      <title>Errors creating volumes</title>
      <p>If you are creating a volume and it goes into the <codeph>ERROR</codeph> state, a common
        error to see is <codeph>No valid host was found</codeph>. This means that the scheduler
        could not schedule your volume to a backend. You should check that the volume service is up
        and running. You can use this command:</p>
      <codeblock>$ sudo cinder-manage service list
Binary           Host                                 Zone             Status     State Updated At
cinder-scheduler ha-volume-manager                    nova             enabled    :-)   2016-04-25 11:39:30
cinder-volume    ha-volume-manager@vsa1               nova             enabled    XXX   2016-04-25 11:27:26
cinder-backup    ha-volume-manager                    nova             enabled    :-)   2016-04-25 11:39:28
cinder-volume    ha-volume-manager@ceph1              nova             enabled    :-)   2016-04-25 11:39:36</codeblock>
      <p>In this example, the state of <codeph>XXX</codeph> indicates that the service is down.</p>
      <p>If the service is up, next check that the backend has sufficient space. You can use this
        command to show the available and total space on each backend:</p>
      <codeblock>cinder get-pools –detail</codeblock>
      <p>If your deployment is using volume types, verify that the
          <codeph>volume_backend_name</codeph> in your <codeph>cinder.conf</codeph> file matches the
          <codeph>volume_backend_name</codeph> for the volume type you selected.</p>
      <p>You can verify the backend name on your volume type by using this command:</p>
      <codeblock>openstack volume type list</codeblock>
      <p>Then list the details about your volume type. For example:</p>
      <codeblock>$ openstack volume type show dfa8ecbd-8b95-49eb-bde7-6520aebacde0
+---------------------------------+--------------------------------------+
| Field                           | Value                                |
+---------------------------------+--------------------------------------+
| description                     | None                                 |
| id                              | dfa8ecbd-8b95-49eb-bde7-6520aebacde0 |
| is_public                       | True                                 |
| name                            | my3par                               |
| os-volume-type-access:is_public | True                                 |
| properties                      | volume_backend_name='3par'           |
+---------------------------------+--------------------------------------+</codeblock>
    </section>

    <section id="integrations">
      <title>Diagnosing backend issues</title>
      <p>You can find further troubleshooting steps for specific backend types by vising these
        pages:</p>
    </section>

  </body>
</topic>
