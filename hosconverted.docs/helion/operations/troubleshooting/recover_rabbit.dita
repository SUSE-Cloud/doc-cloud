<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE topic PUBLIC "-//OASIS//DTD DITA Topic//EN" "topic.dtd">
<topic id="recoverrabbit">
  <title><ph conkeyref="HOS-conrefs/product-title"/>Understanding and Recovering RabbitMQ after
    Failure</title>
  <abstract><shortdesc outputclass="hdphidden">RabbitMQ is the message queue service that runs on
      each of your controller nodes and brokers communication between multiple services in your
        <keyword keyref="kw-hos-phrase"/> cloud environment. It is important for cloud operators to
      understand how different troubleshooting scenarios affect RabbitMQ so they can minimize
      downtime in their environments. We are going to discuss multiple scenarios and how it affects
      RabbitMQ. We will also explain how you can recover from them if there are
    issues.</shortdesc></abstract>
  <body>
    <!--not tested-->
    <p conkeyref="HOS-conrefs/applies-to"/>
    <section id="about">
      <p>RabbitMQ is the message queue service that runs on each of your controller nodes and
        brokers communication between multiple services in your <keyword keyref="kw-hos-phrase"/>
        cloud environment. It is important for cloud operators to understand how different
        troubleshooting scenarios affect RabbitMQ so they can minimize downtime in their
        environments. We are going to discuss multiple scenarios and how it affects RabbitMQ. We
        will also explain how you can recover from them if there are issues.</p>
    </section>
    <section id="upgrade"><title>How upgrades affect RabbitMQ</title>
      <p>There are two types of upgrades within <keyword keyref="kw-hos"/> -- major and minor. The
        effect that the upgrade process has on RabbitMQ depends on these types.</p>
      <p>A major upgrade is defined by an erlang change or major version upgrade of RabbitMQ, such
        as the upgrade from <keyword keyref="kw-hos-phrase-20"/> (RabbitMQ version 3.4.3) to 3.0
        (RabbitMQ version 3.6.1). A minor upgrade would be an upgrade where RabbitMQ stays within
        the same version, such as v3.4.3 to v.3.4.6.</p>
      <p>During both types of upgrades there may be minor blips in the authentication process of
        client services as the accounts are recreated.</p>
      <p><b>RabbitMQ during a major upgrade</b></p>
      <p>There will be a RabbitMQ service outage while the upgrade is performed.</p>
      <p>During the upgrade, high availability consistency is compromised -- all but the primary
        node will go down and will be reset, meaning their database copies are deleted. The primary
        node is not taken down until the last step and then it is upgrade. The database of users and
        permissions is maintained during this process. Then the other nodes are brought back into
        the cluster and resynchronized.</p>
      <p><b>RabbitMQ during a minor upgrade</b></p>
      <p>Minor upgrades are performed node by node. This "rolling" process means there should be no
        overall service outage becauuse each node is taken out of its cluster in turn, its database
        is reset, and then it's added back to the cluster and resynchronized.</p>
    </section>
    <section id="operations"><title>How RabbitMQ is affected by other operational processes</title>
      <p>There are operational tasks, such as <xref href="../maintenance/reboot_cloud_down.dita"/>,
        where you use the <codeph>hlm-stop.yml</codeph> and <codeph>hlm-start.yml</codeph> playbooks
        to gracefully restart your cloud. If you use these playbooks, and there are no errors
        associated with them forcing you to troubleshoot further, then RabbitMQ is brought down
        gracefully and brought back up. There is nothing special to note regarding RabbitMQ in these
        normal operational processes.</p>
      <p>However, there are other scenarios where an understanding of RabbitMQ is important when a
        graceful shutdown did not occur.</p>
      <p>These examples that follow assume you are using one of the entry-scale models where
        RabbitMQ is hosted on your controller node cluster. If you are using a mid-scale model or
        have a dedicated cluster that RabbitMQ lives on you may need to alter the steps accordingly.
        To determine which nodes RabbitMQ is on you can use the <codeph>rabbit-status.yml</codeph>
        playbook from your lifecycle
        manager.<codeblock>cd ~/scratch/ansible/next/hos/ansible
ansible-playbook -i hosts/verb_hosts rabbitmq-status.yml</codeblock></p>
      <p><b>Your entire control plane cluster goes down</b></p>
      <p>If you have a scenario where all of your controller nodes went down, either manually or via
        another process such as a power outage, then an understanding of how RabbitMQ should be
        brought back up is important. Follow these steps to recover RabbitMQ on your controller node
        cluster in these cases:</p>
      <ol>
        <li>The order in which the nodes went down is key here. Locate the last node to go down as
          this will be used as the primary node when bringing the RabbitMQ cluster back up. You can
          review the timestamps in the <codeph>/var/log/rabbitmq</codeph> log file to determine what
          the last node was. <note>The "primary" status of a node is transient, it only applies for
            the duration that this process is running. There is no long-term distinction between any
            of the nodes in your cluster. The primary node is simply the one that owns the RabbitMQ
            configuration database that will be synchronized across the cluster.</note></li>
        <li>Run the <codeph>hlm-start.yml</codeph> playbook specifying the primary node (aka the
          last node down determined in the first step):<codeblock>cd ~/scratch/ansible/next/hos/ansible
ansible-playbook -i hosts/verb_hosts hlm-start.yml -e rabbit_primary_hostname=&lt;hostname></codeblock>
          <note>The <codeph>&lt;hostname></codeph> value will be the "shortname" for your node, as
            found in the <codeph>/etc/hosts</codeph> file.</note></li>
      </ol>
      <p><b>If one of your controller nodes goes down</b></p>
      <p>First step here is to determine whether the controller that went down is the primary
        RabbitMQ host or not. The primary host is going to be the first host member in the
          <codeph>FND-RMQ</codeph> group in the file below on your lifecycle manager:</p>
      <codeblock>~/scratch/ansible/next/hos/ansible/hosts/verb_hosts</codeblock>
      <p>In this example below, <codeph>helion-cp1-c1-m1-mgmt</codeph> would be the primary:</p>
      <codeblock>[FND-RMQ-ccp-cluster1:children]
helion-cp1-c1-m1-mgmt
helion-cp1-c1-m2-mgmt
helion-cp1-c1-m3-mgmt</codeblock>
      <p>If your primary RabbitMQ controller node has gone down and you need to bring it back up,
        you can follow these steps. In this playbook you are using the
          <codeph>rabbit_primary_hostname</codeph> parameter to specify the hostname for one of the
        other controller nodes in your environment hosting RabbitMQ, which will service as the
        primary node in the recovery. You will also use the <codeph>--limit</codeph> parameter to
        specify the controller node you are attempting to bring back up.</p>
      <codeblock>cd ~/scratch/ansible/next/hos/ansible
ansible-playbook -i hosts/verb_hosts hlm-start.yml -e rabbit_primary_hostname=&lt;new_primary_hostname> --limit &lt;hostname_of_node_you_are_bringing_up></codeblock>
      <p>If the node you need to bring back is <b>not</b> the primary RabbitMQ node then you can
        just run the <codeph>hlm-start.yml</codeph> playbook with the <codeph>--limit</codeph>
        parameter and your node should recover:</p>
      <codeblock>cd ~/scratch/ansible/next/hos/ansible
ansible-playbook -i hosts/verb_hosts hlm-start.yml --limit &lt;hostname_of_node_you_are_bringing_up></codeblock>
      <p><b>If you are replacing one or more of your controller nodes</b></p>
      <p>The same general process noted above is used if you are removing or replacing one or more
        of your controller nodes.</p>
      <p>If your node needs minor hardware repairs, but doesn't need to be replaced with a new node,
        you should use the <codeph>hlm-stop.yml</codeph> playbook with the <codeph>--limit</codeph>
        parameter to stop services on that node prior to removing it from the cluster.</p>
      <ol>
        <li>Log into the lifecycle manager.</li>
        <li>Run the <codeph>rabbitmq-stop.yml</codeph> playbook, specifying the hostname of the node
          you are removing, which will remove the node from the RabbitMQ cluster:
          <codeblock>cd ~/scratch/ansible/next/hos/ansible
ansible-playbook -i hosts/verb_hosts rabbitmq-stop.yml --limit &lt;hostname_of_node_you_are_removing></codeblock></li>
        <li>Run the <codeph>hlm-stop.yml</codeph> playbook, again specifying the hostname of the
          node you are removing, which will stop the rest of the services and prepare it to be
          removed:
          <codeblock>cd ~/scratch/ansible/next/hos/ansible
ansible-playbook -i hosts/verb_hosts hlm-stop.yml --limit &lt;hostname_of_node_you_are_removing></codeblock></li>
      </ol>
      <p>If your node cannot be repaired and needs to be replaced with another baremetal node, the
        RabbitMQ cluster needs any references to the node removed from it. This is because RabbitMQ
        associates a cookie with each node in the cluster which is derived, in part, by the specific
        hardware. Replacing a hard drive in a node should be fine, but an exchanged motherboard or
        replacing it with another node entirely will potentially trigger a breakage. Under these
        circumstances, the running RabbitMQ cluster must be edited from a running RabbitMQ node with
        these steps:</p>
      <ol>
        <li>SSH to a running RabbitMQ cluster node.</li>
        <li>Run this command to force the cluster to forget the node you are removing, which will
          remove all refernces to it:
          <codeblock>sudo rabbitmqctl forget_cluster_node rabbit@&lt;hostname_of_node_you_are_removing></codeblock></li>
        <li>You can then confirm that the node has been removed with this command:
          <codeblock>sudo rabbitmqctl cluster_status</codeblock></li>
      </ol>
      <p>If the node you are removing/replacing is your primary host then when you are adding it to
        your cluster then you will want to ensure that you specify a new primary host when doing so,
        as follows:</p>
      <codeblock>cd ~/scratch/ansible/next/hos/ansible
ansible-playbook -i hosts/verb_hosts hlm-start.yml -e rabbit_primary_hostname=&lt;new_primary_hostname> --limit &lt;hostname_of_node_you_are_adding></codeblock>
      <p>If the node you are removing/replacing is not your primary host then you can add it as
        follows:</p>
      <codeblock>cd ~/scratch/ansible/next/hos/ansible
ansible-playbook -i hosts/verb_hosts hlm-start.yml --limit &lt;hostname_of_node_you_are_adding></codeblock>
      <p><b>If one of your controller nodes has rebooted or temporarily lost power</b></p>
      <p>After a single reboot, RabbitMQ will not automatically restart. This is by design to
        protect your RabbitMQ cluster. To restart RabbitMQ, you should follow the process below.</p>
      <p>If the rebooted node was your primary RabbitMQ host, you will specify a different primary
        hostname using one of the other nodes in your cluster:</p>
      <codeblock>cd ~/scratch/ansible/next/hos/ansible
ansible-playbook -i hosts/verb_hosts hlm-start.yml -e rabbit_primary_hostname=&lt;new_primary_hostname> --limit &lt;hostname_of_node_that_rebooted></codeblock>
      <p>If the rebooted node was not the primary RabbitMQ host then you can just start it back up
        with this playbook:</p>
      <codeblock>cd ~/scratch/ansible/next/hos/ansible
ansible-playbook -i hosts/verb_hosts hlm-start.yml --limit &lt;hostname_of_node_that_rebooted></codeblock>
    </section>
    <section id="recovery"><title>Recovering RabbitMQ</title></section>
    <p>In this section we will show you how to check the status of RabbitMQ and how to do a variety
      of disaster recovery procedures.</p>
    <p><b>Verifying the status of RabbitMQ</b></p>
    <p>You can verify the status of RabbitMQ on each of your controller nodes by using the following
      steps:</p>
    <ol>
      <li>Log in to the lifecycle manager.</li>
      <li>Run the <codeph>rabbitmq-status.yml</codeph> playbook:
        <codeblock>cd ~/scratch/ansible/next/hos/ansible
ansible-playbook -i hosts/verb_hosts rabbitmq-status.yml</codeblock></li>
      <li>If all is well, you should see an output similar to the following:
        <codeblock>PLAY RECAP ********************************************************************
rabbitmq | status | Check RabbitMQ running hosts in cluster ------------- 2.12s
rabbitmq | status | Check RabbitMQ service running ---------------------- 1.69s
rabbitmq | status | Report status of RabbitMQ --------------------------- 0.32s
-------------------------------------------------------------------------------
Total: ------------------------------------------------------------------ 4.36s
helion-cp1-c1-m1-mgmt  : ok=2    changed=0    unreachable=0    failed=0
helion-cp1-c1-m2-mgmt  : ok=2    changed=0    unreachable=0    failed=0
helion-cp1-c1-m3-mgmt  : ok=2    changed=0    unreachable=0    failed=0</codeblock></li>
    </ol>
    <p>If one or more of your controller nodes are having RabbitMQ issues then continue reading,
      looking for the scenario that best matches yours.</p>
    <p><b>RabbitMQ recovery after a small network outage</b></p>
    <p>In the case of a transient network outage, the version of RabbitMQ included with <keyword
        keyref="kw-hos-phrase"/> is likely to recover automatically without any further action
      needed. However, if yours doesn't and the <codeph>rabbitmq-status.yml</codeph> playbook is
      reporting an issue then use the scenarios below to resolve your issues.</p>
    <p><b>All of your controller nodes have gone down and using other methods have not brought
        RabbitMQ back up</b></p>
    <p>If your RabbitMQ cluster is irrecoverable and you need rapid service recovery because other
      methods either can't resolve the issue or you don't have time to investigate more nuanced
      approaches then we provide a disaster recovery playbook for you to use. This playbook will
      tear down and reset any RabbitMQ services. This does have an extreme effect on your services.
      The process will ensure that the RabbitMQ cluster is recreated.</p>
    <ol>
      <li>Log in to your lifecycle manager.</li>
      <li>Run the RabbitMQ disaster recovery playbook. This generally takes around two minutes.
        <codeblock>cd ~/scratch/ansible/next/hos/ansible
ansible-playbook -i hosts/verb_hosts rabbitmq-disaster-recovery.yml</codeblock></li>
      <li>Run the reconfigure playbooks for both Cinder (Block Storage) and Heat (Orchestration), if
        those services are present in your cloud. These services are affected when the fan-out
        queues are not recovered correctly. The reconfigure generally takes around five minutes.
        <codeblock>cd ~/scratch/ansible/next/hos/ansible
ansible-playbook -i hosts/verb_hosts cinder-reconfigure.yml
ansible-playbook -i hosts/verb_hosts heat-reconfigure.yml
ansible-playbook -i hosts/verb_hosts logging-server-configure.yml</codeblock></li>
      <li>If you need to do a safe recovery of all the services in your environment then you can use
        this playbook. This is a more lengthy process as all services are inspected.
        <codeblock>cd ~/scratch/ansible/next/hos/ansible
ansible-playbook -i hosts/verb_hosts hlm-reconfigure.yml</codeblock></li>
    </ol>
    <p><b>One of your controller nodes has gone down and using other methods have not brought
        RabbitMQ back up</b></p>
    <p>This disaster recovery procedure has the same caveats as the preceding one, but the steps
      differ.</p>
    <p>If your primary RabbitMQ controller node has gone down and you need to perform a disaster
      recovery, use this playbook from your lifecycle manager:</p>
    <codeblock>cd ~/scratch/ansible/next/hos/ansible
ansible-playbook -i hosts/verb_hosts rabbitmq-disaster-recovery.yml -e rabbit_primary_hostname=&lt;new_primary_hostname> --limit &lt;hostname_of_node_that_needs_recovered></codeblock>
    <p>If the controller node is not your primary, you can use this playbook:</p>
    <codeblock>cd ~/scratch/ansible/next/hos/ansible
ansible-playbook -i hosts/verb_hosts rabbitmq-disaster-recovery.yml --limit &lt;hostname_of_node_that_needs_recovered></codeblock>
    <p>No reconfigure playbooks are needed because all of the fan-out exchanges are maintained by
      the running members of your RabbitMQ cluster.</p>
  </body>
</topic>
