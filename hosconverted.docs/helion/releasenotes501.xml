<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE topic PUBLIC "-//OASIS//DTD DITA Topic//EN" "topic.dtd">
<topic xml:lang="en-us" id="release-notes-501">
  <title><keyword keyref="kw-hos-tm"/>
    <keyword keyref="kw-hos-version-501"/>: Release Notes</title>
  <abstract><shortdesc outputclass="hdphidden">An overview of the features contained within <keyword
        keyref="kw-hos-version-501"/>, including known issues and workarounds for this
      release.</shortdesc></abstract>
  <body>
    
    <!--
    <section>
      <title>New in this release</title>
 
    </section>
    -->

    
    <section>
      <title>Fixed in this release</title>
      
      <p id="SCRD-1262"><b><keyword keyref="kw-hos-phrase-501"/> supports SLES12 SP3 for Compute Nodes</b></p>
      <p>SLES12 SP2 is supported for compute nodes in <keyword keyref="kw-hos-phrase-50"/>. <keyword
          keyref="kw-hos-phrase-501"/> includes support for the more recently released SLES12 SP3.
          <note type="important"><keyword keyref="kw-hos-phrase-501"/> does not support SLES12 SP2
          compute nodes. If you intend to upgrade to <keyword keyref="kw-hos-phrase-501"/> you will
          be required to upgrade your compute nodes to SP3. See steps below for upgrading SLES
          compute nodes from SP2 to SP3. </note>
      </p>
      
      <p id="lifecycle_manager_compute_nodes"><b>Using the Lifecycle Manager to Deploy SLES Compute Nodes</b></p>
      <p>The method used for deploying SLES compute nodes using Cobbler on the lifecycle manager
        uses legacy BIOS. 
        <note>UEFI and Secure boot are currently not supported on SLES compute.</note>
      </p>
      <p>Deploying legacy BIOS SLES compute nodes</p>
      <p>The installation process for SLES nodes is almost
        identical to that of HPE Linux nodes as described in the topic for Installation for 
        <xref href="https://docs.hpcloud.com/hos-5.x/helion/installation/installing_kvm.html" scope="external" format="html">
          <keyword keyref="kw-hos"/> Entry-scale KVM Cloud</xref>. 
        The key differences are: 
      </p>
      <p>
        <ul>
          <li>The standard SLES ISO (SLE-12-SP3-Server-DVD-x86_64-GM-DVD1.iso) must be accessible
            via <codeph>/home/stack/sles12sp3.iso</codeph>. Rename the ISO or create a symbolic
            link.
            <codeblock>mv SLE-12-SP3-Server-DVD-x86_64-GM-DVD1.iso /home/stack/sles12sp3.iso</codeblock>
          </li>
          <li>The contents of the SLES SDK ISO (SLE-12-SP3-SDK-DVD-x86_64-GM-DVD1.iso) must be
            mounted or copied to <codeph>/opt/hlm_packager/hlm/sles12/zypper/SDK/</codeph>. If you
            choose to mount the ISO, we recommend creating an <codeph>/etc/fstab</codeph> entry to
            ensure the ISO is mounted after a reboot. 
            <ul>
              <li>If mounting the .iso on loopback
                <codeblock>sudo mount -o loop /home/stack/sles12.iso /opt/hlm_packager/hlm/sles12/zypper/SDK/</codeblock>
              </li>
              <li>Entry for mounting the .iso on loopback in /etc/fstab
                <codeblock>/home/stack/sles12.iso    /opt/hlm_packager/hlm/sles12/zypper/SDK/    iso9660    loop    0    0</codeblock>
              </li>
            </ul>
          </li>
        <li>You must identify the node(s) on which you want to install SLES, by adding the key/value
            pair distro-id: <codeph>sles12sp3-x86_64</codeph> to server details in
              <codeph>servers.yml</codeph>. You will also need to update
              <codeph>net_interfaces.yml</codeph>, <codeph>server_roles.yml</codeph>,
              <codeph>disk_compute.yml</codeph> and <codeph>control_plane.yml</codeph>. For more
            information on configuration of the Input Model for SLES, see SLES Compute Model. </li>
          <li><keyword keyref="kw-hos"/> playbooks currently do not take care of SDK, so it needs to
            be added manually. The following command needs to be run on every SLES compute node:
            <codeblock>deployer_ip=192.168.10.254
zypper addrepo --no-gpg-checks --refresh http://$deployer_ip:79/hlm/sles12/zypper/SDK SLES-SDK</codeblock>
          </li>
        </ul>
      </p>
      
    
      <p id="BUGZILLA-1065762"><b>Upgrading <keyword keyref="kw-hos-phrase"/> with SLES Compute SP2 to SP3</b></p>
      <p>You can upgrade your SLES compute nodes from SP2 to SP3 by following these steps. 
        <ol>
          <li>You must be running <keyword keyref="kw-hos-phrase"/> with SLES12 SP2 Compute</li>
          <li>Upgrade the Compute OS from SP2 to SP3 by following these steps. 
            <ul>
              <li>On the deployer, unmount the sp2 content, example:
                <codeblock>sudo umount /opt/hlm_packager/hlm/sles12/zypper/OS/
sudo umount /opt/hlm_packager/hlm/sles12/zypper/SDK/</codeblock>
              </li>
              <li>Copy or mount the contents of SLES SDK ISO, example:
                <codeblock>sudo mount -o loop SLE-12-SP3-SDK-DVD-x86_64-GM-DVD1.iso /opt/hlm_packager/hlm/sles12/zypper/SDK/
sudo mount -o loop SLE-12-SP3-Server-DVD-x86_64-GM-DVD1.iso /opt/hlm_packager/hlm/sles12/zypper/OS/ </codeblock>
              </li>
              <li>On sles12sp2 compute, remove old repositories if applicable
                <codeblock>sudo zypper removerepo SLES-SDK
sudo zypper removerepo SLES12-SP2-12.2-0</codeblock>
              </li>
              <li>On sles12sp2 compute, clean up old repositories if applicable
                <codeblock>sudo zypper clean</codeblock>
              </li>
              <li>Add updated repositories (Update the value of <codeph>deployer_ip</codeph> as
                necessary)
                <codeblock>deployer_ip=192.168.10.254
zypper addrepo --no-gpg-checks --refresh http://$deployer_ip:79/hlm/sles12/zypper/OS SLES-OS
zypper addrepo --no-gpg-checks --refresh http://$deployer_ip:79/hlm/sles12/zypper/SDK SLES-SDK</codeblock>
              </li>
              <li>Run zypper to refresh
                <codeblock>sudo <codeph>zypper</codeph> refresh</codeblock>
              </li>
              <li>Run <codeph>zypper</codeph> to upgrade: <codeblock>sudo zypper up</codeblock></li>
            </ul>
            <note>After OS upgrade from SP2 to SP3, the administrator will need to run the following
              command.  If the only way to reach the SLES Compute node is from the iLO console, the
              command may be run from there.
            <codeblock>sudo systemctl enable openvswitch</codeblock>If the openvswitch is
              not running after the upgrade, the administrator will need to run the command:
              <codeblock>sudo systemctl start openvswitch</codeblock></note>
          </li>
        
          <li>Mount the 5.0.1 ISO image
            <codeblock>sudo mount -o loop HelionOpenStack-5.0.iso /media/cdrom</codeblock>
          </li>
          <li>Unpack the following tarball:
            <codeblock>cd ~
tar fxv /media/cdrom/hos/hos-5.0.0-20161014T020249Z.tar</codeblock>
          </li>
          <li>Run the included initialization script to update the deployer:
            <codeblock>~/hos-5.0.1/hos-init.bash</codeblock>
          </li>
          <li>Run the configuration processor:
            <codeblock>cd ~/helion/hos/ansible
ansible-playbook -i hosts/localhost config-processor-run.yml</codeblock>
          </li>
          <li>Update the deployment directory:
            <codeblock>cd ~/helion/hos/ansible
ansible-playbook -i hosts/localhost ready-deployment.yml</codeblock>
          </li>
          <li>Run site.yml
            <codeblock>cd ~/scratch/ansible/next/hos/ansible
ansible-playbook -i hosts/verb_hosts site.yml</codeblock>
          </li>
          <li>If Ceph is configured, run hlm-cloud-configure.yml
            <codeblock>cd ~/scratch/ansible/next/hos/ansible
ansible-playbook -i hosts/verb_hosts hlm-cloud-configure.yml</codeblock>
          </li>
          <li>Reboot all</li>
        </ol>
      </p>
      
      
      <p id="DOCS-4201"><b>HPE Linux Changes</b></p>
      <p>HPE Helion OpenStack 5.0.1 includes a new version of the hLinux operating system which has fixes for CVEs known to us and deemed to be issues not mitigated by architecture at the time of ISO creation.</p>
      
    
      <p id="DOCS-4202"><b>MTU Bug for LBaaS</b></p>
      <p>Contains fix for OpenStack bug <xref href="https://review.openstack.org/#/c/407617/" scope="external" format="html">https://review.openstack.org/#/c/407617/</xref></p>
      <p>LBaaSv2 sets up a VIF without specifying its MTU. Therefore, the VIF always gets the
        default MTU of 1500. When attaching the load balancer to a VXLAN-backed project (tenant)
        network, which by default has a MTU of 1450, this leads to packet dropping.</p>
      <p>Removed hardcoded value which prevented LBaaSv2 from using MTU specified by the network object.</p>
      
      <p id="SCRD-1362"><b>Magnum Fedora Atomic for <keyword keyref="kw-hos-phrase"/> must use version fedora-25</b></p>
      <p>You can find more information in the document <xref
          keyref="deploying_kubernetes_fedora_atomic"/>
      </p>
      
      <p id="DOCS-4199"><b>3PAR Fix</b></p>
      <p>Fixed an issue with the 3PAR driver to ensure that CHAP credentials are used correctly after compute host is rebooted.</p>
      
      <p id="SCRD-1642"><b>Adding Compute Nodes with Ceph already configured</b></p>
      <p>An additional step is needed to add compute nodes to a cloud where Ceph has already been configured.
        <ul>
          <li>Complete the compute host deployment using site.yml as noted in <xref
              keyref="adding_compute_nodes"/> in the <keyword
              keyref="kw-hos-phrase"/> documentation.</li>
          <li>Then run the following command.
            <codeblock>cd ~/scratch/ansible/next/hos/ansible/ansible-playbook -i hosts/verb_hosts ceph-client-prepare.yml</codeblock></li>
        </ul>
      This applies to hLinux, SLES, RHEL and RHEV computes.
      </p>
      
    </section>
    
    <!--
    <section>
      <title>Known issues in this release</title>
      
 
    </section>
    -->
  </body>
</topic>
