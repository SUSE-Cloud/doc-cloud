<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE topic PUBLIC "-//OASIS//DTD DITA Topic//EN" "topic.dtd">
<topic id="neutron_troubleshooting">
  <title><ph conkeyref="HOS-conrefs/product-title"/>Network Service Troubleshooting </title>
  <abstract><shortdesc outputclass="hdphidden">Troubleshooting scenarios with resolutions for the
      Networking service.</shortdesc></abstract>
  <body>
    <p conkeyref="HOS-conrefs/applies-to"/>
    <section>
      <title>Troubleshooting Network failures</title>
    </section>

    <section>
      <title>CVR HA - Split-brain result of failover of L3 agent when master comes back up</title>
      <p>This situation is specific to when L3 HA is configured and a network failure occurs to the
        node hosting the currently active l3 agent. L3 HA is intended to provide HA in situations
        where the l3-agent crashes or the node hosting an l3-agent crashes/restarts. In the case of
        a physical networking issue which isolates the active l3 agent, the stand-by l3-agent takes
        over but when the physical networking issue is resolved, traffic to the VMs is disrupted due
        to a "split-brain" situation in which traffic is split over the two L3 agents. The solution
        is to restart the L3-agent that was originally the master. </p>
    </section>

    <section>
      <title>OVSvApp loses connectivity with vCenter</title>
      <p>If the OVSvApp loses connectivity with the vCenter cluster, you will receive the following
        errors: </p>
      <ul id="ul_ykf_vs2_wv">
        <li>The OVSvApp VM will go into ERROR state</li>
        <li>The OVSvApp VM will not get IP address</li>
      </ul>
      <p>When you see these symptoms: <ol id="ol_cs3_lt2_wv">
          <li>Restart the OVSvApp agent on the OVSvApp VM.</li>
          <li> Execute the following command to restart the Network (Neutron) service:
            <codeblock>sudo service neutron-ovsvapp-agent restart</codeblock></li>
        </ol></p>
    </section>

    <!--  docs 3098 -->
    <section><title>Fail over a plain CVR router because the node became unavailable:</title>
      <ol>
        <li> Get a list of l3 agent UUIDs which can be used in the commands that follow
          <codeblock> neutron agent-list | grep l3 </codeblock></li>

        <li>Determine the current host
          <codeblock> neutron l3-agent-list-hosting-router &lt;router uuid> </codeblock></li>

        <li>Remove the router from the current host
          <codeblock>neutron l3-agent-router-remove &lt;current l3 agent uuid> &lt;router uuid></codeblock></li>

        <li>Add the router to a new host
          <codeblock>neutron l3-agent-router-add &lt;new l3 agent uuid> &lt;router uuid> </codeblock></li>
      </ol>
    </section>
    <section><title>Trouble setting maximum transmission units (MTU)</title> Note that if you
      attempt to set MTU on an existing network and the MTU setting is not taking effect, you must
      create a new network. The setting will not take effect on existing networks. To use the
      modified MTU settings, a new network must be created <xref
        href="configure_mtu.dita#configureMTU">after the reconfigure</xref> and VMs booted on that
      network will be able to use the modified MTU setting. </section>
    <section>
      <!--docs 3559-->
      <title>Floating IP on allowed_address_pair port with DVR-routed networks</title>DVR has
      limited support for floating IPs associated with <codeph>allowed_address_pair</codeph> ports
      that are unbound. </section>
    <section>
      <p>You may notice this issue: If you have an <codeph>allowed_address_pair</codeph> associated
        with multiple virtual machine (VM) ports, and if all the VM ports are ACTIVE, then the
          <codeph>allowed_address_pair</codeph> port binding will have the last ACTIVE VM's binding
        host as its bound host.</p>
    </section>
    <section>
      <p>In addition, you may notice that if the floating IP is assigned to the
          <codeph>allowed_address_pair</codeph> that is bound to multiple VMs that are ACTIVE, then
        the floating IP will not work with DVR routers. This is different from the centralized
        router behavior where it can handle unbound <codeph>allowed_address_pair</codeph> ports that
        are associated with floating IPs.</p>
      <p>Currently we support <codeph>allowed_address_pair</codeph> ports with DVR only if they have
        floating IPs enabled, and have just one ACTIVE port. </p>
      <!-- CLI instructions -->
      <p>Using the CLI, you can follow these steps:</p>
      <ol>
        <li>Create a network to add the host
          to:<codeblock>$ neutron net-create vrrp-net</codeblock></li>
        <li>Attach a subnet to that network with a specified allocation-pool
          range:<codeblock>$ neutron subnet-create  --name vrrp-subnet --allocation-pool start=10.0.0.2,end=10.0.0.200 vrrp-net 10.0.0.0/24</codeblock></li>
        <li>Create a router, uplink the vrrp-subnet to it, and attach the router to an upstream
          network called
          public:<codeblock>$ neutron router-create router1
$ neutron router-interface-add router1 vrrp-subnet 
$ neutron router-gateway-set router1 public</codeblock>
          Create a security group called vrrp-sec-group and add ingress rules to allow ICMP and TCP
          port 80 and
          22:<codeblock>$ neutron security-group-create vrrp-sec-group
$ neutron security-group-rule-create  --protocol icmp vrrp-sec-group 
$ neutron security-group-rule-create  --protocol tcp  --port-range-min80 --port-range-max80 vrrp-sec-group 
$ neutron security-group-rule-create  --protocol tcp  --port-range-min22 --port-range-max22 vrrp-sec-group</codeblock></li>
        <li>Next, boot two instances:
          <codeblock>$ nova boot --num-instances 2 --image ubuntu-12.04 --flavor 1 --nic net-id=24e92ee1-8ae4-4c23-90af-accb3919f4d1 vrrp-node --security_groups vrrp-sec-group</codeblock></li>
        <li>When you create two instances, make sure that both the instances are not in ACTIVE state
          before you associate the <codeph>allowed_address_pair</codeph>. The
          instances:<codeblock>$ nova list
+--------------------------------------+-------------------------------------------------+--------+------------+-------------+--------------------------------------------------------+
| ID                                   | Name                                            | Status | Task State | Power State | Networks                                               |
+--------------------------------------+-------------------------------------------------+--------+------------+-------------+--------------------------------------------------------+
| 15b70af7-2628-4906-a877-39753082f84f | vrrp-node-15b70af7-2628-4906-a877-39753082f84f | ACTIVE  | -          | Running     | vrrp-net=10.0.0.3                                      |
| e683e9d1-7eea-48dd-9d3a-a54cf9d9b7d6 | vrrp-node-e683e9d1-7eea-48dd-9d3a-a54cf9d9b7d6 | DOWN    | -          | Running     | vrrp-net=10.0.0.4                                      |
+--------------------------------------+-------------------------------------------------+--------+------------+-------------+--------------------------------------------------------+    </codeblock></li>
        <li>Create a port in the VRRP IP range that was left out of the ip-allocation
          range:<codeblock>$ neutron port-create --fixed-ip ip_address=10.0.0.201 --security-group vrrp-sec-group vrrp-net
Created a new port:
+-----------------------+-----------------------------------------------------------------------------------+
| Field                 | Value                                                                             |
+-----------------------+-----------------------------------------------------------------------------------+
| admin_state_up        | True                                                                              |
| allowed_address_pairs |                                                                                   |
| device_id             |                                                                                   |
| device_owner          |                                                                                   |
| fixed_ips             | {"subnet_id": "94a0c371-d37c-4796-821e-57c2a8ec65ae", "ip_address": "10.0.0.201"} |
| id                    | 6239f501-e902-4b02-8d5c-69062896a2dd                                              |
| mac_address           | fa:16:3e:20:67:9f                                                                 |
| name                  |                                                                                   |
| network_id            | 24e92ee1-8ae4-4c23-90af-accb3919f4d1                                              |
| port_security_enabled | True                                                                              |
| security_groups       | 36c8131f-d504-4bcc-b708-f330c9f6b67a                                              |
| status                | DOWN                                                                              |
| tenant_id             | d4e4332d5f8c4a8eab9fcb1345406cb0                                                  |
+-----------------------+-----------------------------------------------------------------------------------+</codeblock></li>
        <li>Another thing to cross check after you associate the allowed_address_pair port to the VM
          port, is whether the <codeph>allowed_address_pair</codeph> port has inherited the VM's
          host
          binding:<codeblock>$ neutron --os-username admin --os-password ZIy9xitH55 --os-tenant-name admin port-show f5a252b2-701f-40e9-a314-59ef9b5ed7de
+-----------------------+--------------------------------------------------------------------------------------------------------+
| Field                 | Value                                                                                                  |
+-----------------------+--------------------------------------------------------------------------------------------------------+
| admin_state_up        | True                                                                                                   |
| allowed_address_pairs |                                                                                                        |
| {color:red}binding:host_id{color} | hlm201-cp1-comp0001-mgmt                                                                   |
| binding:profile       | {}                                                                                                     |
| binding:vif_details   | {"port_filter": true, "ovs_hybrid_plug": true}                                                         |
| binding:vif_type      | ovs                                                                                                    |
| binding:vnic_type     | normal                                                                                                 |
| device_id             |                                                                                                        |
| device_owner          | compute:None                                                                                           |
| dns_assignment        | {"hostname": "host-10-0-0-201", "ip_address": "10.0.0.201", "fqdn": "host-10-0-0-201.openstacklocal."} |
| dns_name              |                                                                                                        |
| extra_dhcp_opts       |                                                                                                        |
| fixed_ips             | {"subnet_id": "94a0c371-d37c-4796-821e-57c2a8ec65ae", "ip_address": "10.0.0.201"}                      |
| id                    | 6239f501-e902-4b02-8d5c-69062896a2dd                                                                   |
| mac_address           | fa:16:3e:20:67:9f                                                                                      |
| name                  |                                                                                                        |
| network_id            | 24e92ee1-8ae4-4c23-90af-accb3919f4d1                                                                   |
| port_security_enabled | True                                                                                                   |
| security_groups       | 36c8131f-d504-4bcc-b708-f330c9f6b67a                                                                   |
| status                | DOWN                                                                                                   |
| tenant_id             | d4e4332d5f8c4a8eab9fcb1345406cb0                                                                       |
+-----------------------+--------------------------------------------------------------------------------------------------------+</codeblock></li>
        <li>Note that you were allocated a port with the IP address 10.0.0.201 as requested. Next,
          associate a floating IP to this port to be able to access it
          publicly:<codeblock>$ neutron floatingip-create --port-id=6239f501-e902-4b02-8d5c-69062896a2dd public
Created a new floatingip:
+---------------------+--------------------------------------+
| Field               | Value                                |
+---------------------+--------------------------------------+
| fixed_ip_address    | 10.0.0.201                           |
| floating_ip_address | 10.36.12.139                         |
| floating_network_id | 3696c581-9474-4c57-aaa0-b6c70f2529b0 |
| id                  | a26931de-bc94-4fd8-a8b9-c5d4031667e9 |
| port_id             | 6239f501-e902-4b02-8d5c-69062896a2dd |
| router_id           | 178fde65-e9e7-4d84-a218-b1cc7c7b09c7 |
| tenant_id           | d4e4332d5f8c4a8eab9fcb1345406cb0     |
+---------------------+--------------------------------------+</codeblock></li>
        <li>Now update the ports attached to your VRRP instances to include this IP address as an
          allowed-address-pair so they will be able to send traffic out using this address. First
          find the ports attached to these
          instances:<codeblock>$ neutron port-list -- --network_id=24e92ee1-8ae4-4c23-90af-accb3919f4d1
+--------------------------------------+------+-------------------+-----------------------------------------------------------------------------------+
| id                                   | name | mac_address       | fixed_ips                                                                         |
+--------------------------------------+------+-------------------+-----------------------------------------------------------------------------------+
| 12bf9ea4-4845-4e2c-b511-3b8b1ad7291d |      | fa:16:3e:7a:7b:18 | {"subnet_id": "94a0c371-d37c-4796-821e-57c2a8ec65ae", "ip_address": "10.0.0.4"}   |
| 14f57a85-35af-4edb-8bec-6f81beb9db88 |      | fa:16:3e:2f:7e:ee | {"subnet_id": "94a0c371-d37c-4796-821e-57c2a8ec65ae", "ip_address": "10.0.0.2"}   |
| 6239f501-e902-4b02-8d5c-69062896a2dd |      | fa:16:3e:20:67:9f | {"subnet_id": "94a0c371-d37c-4796-821e-57c2a8ec65ae", "ip_address": "10.0.0.201"} |
| 87094048-3832-472e-a100-7f9b45829da5 |      | fa:16:3e:b3:38:30 | {"subnet_id": "94a0c371-d37c-4796-821e-57c2a8ec65ae", "ip_address": "10.0.0.1"}   |
| c080dbeb-491e-46e2-ab7e-192e7627d050 |      | fa:16:3e:88:2e:e2 | {"subnet_id": "94a0c371-d37c-4796-821e-57c2a8ec65ae", "ip_address": "10.0.0.3"}   |
+--------------------------------------+------+-------------------+-----------------------------------------------------------------------------------+</codeblock></li>
        <li>Add this address to the ports c080dbeb-491e-46e2-ab7e-192e7627d050 and
          12bf9ea4-4845-4e2c-b511-3b8b1ad7291d which are 10.0.0.3 and 10.0.0.4 (your vrrp-node
          instances):<codeblock>$ neutron port-update  c080dbeb-491e-46e2-ab7e-192e7627d050 --allowed_address_pairs list=truetype=dict ip_address=10.0.0.201
$ neutron port-update  12bf9ea4-4845-4e2c-b511-3b8b1ad7291d --allowed_address_pairs list=truetype=dict ip_address=10.0.0.201</codeblock></li>
        <li>The allowed-address-pair 10.0.0.201 now shows up on the
          port:<codeblock>$ neutron port-show12bf9ea4-4845-4e2c-b511-3b8b1ad7291d
+-----------------------+---------------------------------------------------------------------------------+
| Field                 | Value                                                                           |
+-----------------------+---------------------------------------------------------------------------------+
| admin_state_up        | True                                                                            |
| allowed_address_pairs | {"ip_address": "10.0.0.201", "mac_address": "fa:16:3e:7a:7b:18"}                |
| device_id             | e683e9d1-7eea-48dd-9d3a-a54cf9d9b7d6                                            |
| device_owner          | compute:None                                                                    |
| fixed_ips             | {"subnet_id": "94a0c371-d37c-4796-821e-57c2a8ec65ae", "ip_address": "10.0.0.4"} |
| id                    | 12bf9ea4-4845-4e2c-b511-3b8b1ad7291d                                            |
| mac_address           | fa:16:3e:7a:7b:18                                                               |
| name                  |                                                                                 |
| network_id            | 24e92ee1-8ae4-4c23-90af-accb3919f4d1                                            |
| port_security_enabled | True                                                                            |
| security_groups       | 36c8131f-d504-4bcc-b708-f330c9f6b67a                                            |
| status                | ACTIVE                                                                          |
| tenant_id             | d4e4332d5f8c4a8eab9fcb1345406cb0                                                | </codeblock></li>
      </ol></section>
    <!--docs-3700-->
    <section>
      <title>OpenStack traffic that must traverse VXLAN tunnel dropped when using HPE 5930
        switch</title>In an OpenStack environment, openstack VXLAN traffic that must traverse a
      layer 3 environment within a manual VXLAN tunnel is being dropped </section>
    <section>Cause: UDP destination port 4789 is conflicting with OpenStack VXLAN traffic. <p>There
        is a configuration setting you can use in the switch to configure the port number the HPN
        kit will use for its own VXLAN tunnels. Setting this to a port number other than the one
        Neutron will use by default (4789) will keep the HPN kit from absconding with Neutron's
        VXLAN traffic. Specifically:</p><b>Parameters: </b><p>port-number: Specifies a UDP port
        number in the range of 1 to 65535. As a best practice, specify a port number in the range of
        1024 to 65535 to avoid conflict with well-known ports. </p><p><b>Usage
        guidelines:</b></p><p>You must configure the same destination UDP port number on all VTEPs
        in a VXLAN. </p> Examples
      <codeblock># Set the destination UDP port number to 6666 for VXLAN packets. 
&lt;Sysname> system-view 
[Sysname] vxlan udp-port 6666</codeblock>Use
      vxlan udp-port to configure the destination UDP port number of VXLAN packets.   Mandatory for
      all VXLAN packets to specify a UDP port Default The destination UDP port number is 4789 for
      VXLAN packets.<p>OVS can be configured to use a different port number
      itself:</p><codeblock># (IntOpt) The port number to utilize if tunnel_types includes 'vxlan'. By
# default, this will make use of the Open vSwitch default value of '4789' if
# not specified.
#
# vxlan_udp_port =
# Example: vxlan_udp_port = 8472
# </codeblock></section>
    <section id="DOCS-3584">
      <title>Issue: PCI-PT virtual machine gets stuck at boot</title>
      <p>If you're using a machine that uses Intel NICs, if the PCI-PT virtual machine gets stuck at
        boot, the boot agent should be disabled.</p>
      <p>When Intel cards are used for PCI-PT, sometimes the tenant virtual machine gets stuck at
        boot. If this happens, you should download Intel bootutils and use it to disable the
        bootagent.</p>
      <p>Use the following steps:</p>
      <ul>
        <li>Download <codeph>preebot.tar.gz</codeph> from the <xref
            href="https://downloadcenter.intel.com/download/19186/Intel-Ethernet-Connections-Boot-Utility-Preboot-Images-and-EFI-Drivers"
            format="html" scope="external">Intel website</xref>.</li>
        <li>Untar the <codeph>preboot.tar.gz</codeph> file on the compute host where the PCI-PT
          virtual machine is to be hosted.</li>
        <li>Go to path <codeph>~/APPS/BootUtil/Linux_x64</codeph> and then run following command:
          <codeblock>./bootutil64e -BOOTENABLE disable -all</codeblock></li>
        <li>Now boot the PCI-PT virtual machine and it should boot without getting stuck.</li>
      </ul>
    </section>
  </body>
</topic>
