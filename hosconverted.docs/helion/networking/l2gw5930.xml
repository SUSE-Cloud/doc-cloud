<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE topic PUBLIC "-//OASIS//DTD DITA Topic//EN" "topic.dtd">
<topic id="l2gw5930">
  <title><ph conkeyref="HOS-conrefs/product-title"/><b>Installing the L2 Gateway Agent for the
      Networking Service</b></title>
  <body>
    <section id="intro">
      <title>Introduction</title>
      
      <p id="l2_summary">The L2 gateway is a service plug-in to the Neutron networking service that
        allows two L2 networks to be seamlessly connected to create a single L2 broadcast domain.
        The initial implementation provides for the ability to connect a virtual Neutron VxLAN
        network to a physical VLAN using a VTEP-capable HPE 5930 switch. The L2 gateway is to be
        enabled only for VxLAN deployments.</p>
      <p>To begin L2 gateway agent setup, you need to configure your switch. These instructions use
        an <xref
          href="http://www8.hp.com/us/en/products/networking-switches/product-detail.html?oid=6604154#!tab=models"
          format="html"  >HPE FlexFabric 5930 Switch Series</xref>
        switch.</p>
    </section>
    <section id="main">
      <p/>
      <ul>
        <li>Use case</li>
        <li><xref href="#l2gw5930/topology" format="dita">Sample network topology</xref></li>
        <li><xref href="#l2gw5930/networks" format="dita">Networks</xref></li>
        <li><xref href="#l2gw5930/switchConfig" format="dita">HPE 5930 Switch
          Configuration</xref></li>
        <li><xref href="#l2gw5930/enabling" format="dita">Enabling and configuring L2 gateway
            agent</xref></li>
        <li><xref href="#l2gw5930/routing" format="dita">Routing between software and hardware VTEP
            networks</xref></li>
        <li><xref href="#l2gw5930/connecting" format="dita">Connecting baremetal server to HPE 5930
            switch</xref></li>
        <li><xref href="#l2gw5930/baremetalConfig" format="dita">Configuration on baremetal server
            to receive tagged packets from the switch</xref></li>
        <li><xref href="#l2gw5930/NICbonding" format="dita">NIC bonding and IRF
          configuration</xref></li>
        <li><xref href="#l2gw5930/scale" format="dita">Scale numbers tested</xref></li>
        <li><xref href="#l2gw5930/commands" format="dita">L2 gateway commands</xref></li>
      </ul>
    </section>
    <section id="topology">
      <title>Sample network topology (for illustration purposes)</title>
    
      <p>When viewing the following network diagram, assume that the
        blue VNET has been created by the tenant and has been assigned a segmentation ID of 1000
        (VNI 1000). The Cloud Admin is now connecting physical servers to this VNET.</p>
      <p>Assume also that the blue L2 Gateway is created and points to
        the HW VTEPS, the physical ports, the VLAN, and if it is an access or trunk port (tagged or
        untagged)</p>
      <!--<image href="../../media/networking/l2gateway5930.png" id="image_fdx_tdb_dx" placement="break"/>-->
      <image id="fig1" href="../../media/networking/l2gateway5930-new.png" placement="break"/>
      Figure 1
    </section>
    <section id="networks">
      <title>Networks</title>
      <p>The following diagram illustrates an example network configuration.</p>
      <!-- <image href="../../media/networking/l2gateway5930-2.png" id="image_zql_12b_dx"
        placement="break"/>-->
      <image id="fig2" href="../../media/networking/l2gateway5930-2-new.png" placement="break"/>
      Figure 2 <p>An L2 gateway is useful in extending virtual networks (VxLAN) in a cloud onto
        physical VLAN networks. The L2 gateway switch converts VxLAN packets into VLAN packets and
        back again, as shown in the following diagram. This topic assumes a VxLAN deployment.</p>
      <!-- <image href="../../media/networking/l2gateway5930-3.png" id="image_awk_f2b_dx"
        placement="break"/>-->
      <image id="fig3" href="../../media/networking/l2gateway5930-3-new.png" placement="break"
      />Figure 3 <ul>
        <li>Management Network: 10.10.85.0/24</li>
        <li>Data Network: 10.1.1.0/24</li>
        <li>Tenant VM Network: 10.10.10.0/24</li>
      </ul> Note: These IP ranges are used in the topology shown in the diagram for illustration
      only. </section>
    <section id="switchConfig">
      <title> HPE 5930 switch configuration</title>
      <ol>
        <li> Telnet to the 5930 switch and provide your username and password. </li>
        <li> Go into system view: <codeblock>system-view</codeblock></li>
        <li> Create the required VLANs and VLAN ranges:
          <codeblock>vlan 103
vlan 83
vlan 183
vlan 1261 to 1270</codeblock></li>
        <li> Assign an IP address to VLAN 103. This is used as a data path network for VxLAN
          traffic. <codeblock>interface vlan 103
ip address 10.1.1.10 255.255.255.0</codeblock></li>
        <li> Assign an IP address to VLAN 83. This is used as a hardware VTEP network.
            <codeblock>interface vlan 83
ip address 10.10.83.3 255.255.255.0</codeblock><p>The 5930
            switch has a fortygigE1/0/5 interface to which a splitter cable is connected that splits
            the network it into four tengigEthernet (tengigEthernet1/0/5:1 to tengigEthernet1/0/5:4)
            interfaces:</p><ul>
            <li>engigEthernet1/0/5:1 and tengigEthernet1/0/5:2 are connected to the compute node.
              This is required just to bring the interface up. In other words, in order to have the
              HPE 5930 switch work as a router, there should be at least one interface of that
              particular VLAN up. Alternatively, the interface can be connected to any host or
              network element. </li>
            <li>tengigEthernet1/0/5:3 is connected to a baremetal server.</li>
            <li> tengigEthernet1/0/5:4 is connected to controller 3, as shown in <xref
                href="#l2gw5930/fig2" format="dita">Figure 2</xref>.</li>
          </ul><p>The switch’s fortygigE1/0/6 interface to which the splitter cable is connected
            splits it into four tengigEthernet (tengigEthernet1/0/6:1 to tengigEthernet1/0/6:4)
            interfaces:</p><ul>
            <li>tengigEthernet1/0/6:1 is connected to controller 2</li>
            <li> tengigEthernet1/0/6:2 is connected to controller 1 <p/>
              Note: 6:3 and 6:4 are not used although they are available.
            </li>
          </ul></li>
        <li>Split the fortygigE 1/0/5 interface into tengig
          interfaces:<codeblock>interface fortygigE 1/0/5
using tengig
The interface FortyGigE1/0/5 will be deleted. Continue? [Y/N]: y</codeblock></li>
        <li> Configure the Ten-GigabitEthernet1/0/5:1
            interface:<codeblock>interface Ten-GigabitEthernet1/0/5:1
port link-type trunk
port trunk permit vlan 83</codeblock>

        
        </li>
      </ol></section>
    
    <section><title>Configuring the provider data path network</title>
        <ol>
        <li>Configure the Ten-GigabitEthernet1/0/5:2
          interface:<codeblock>interface Ten-GigabitEthernet1/0/5:2
port link-type trunk
port trunk permit vlan 103
port trunk permit vlan 1261 to 1270
port trunk pvid vlan 103  </codeblock></li>
        <li>Configure the Ten-GigabitEthernet1/0/5:4
          interface:<codeblock>interface Ten-GigabitEthernet1/0/5:4
port link-type trunk
port trunk permit vlan 103
port trunk permit vlan 1261 to 1270
port trunk pvid vlan 103</codeblock></li>
        <li>Configure the Ten-GigabitEthernet1/0/5:3
          interface:<codeblock>interface Ten-GigabitEthernet1/0/5:3
port link-type trunk
port trunk permit vlan 183
vtep access port</codeblock></li>
        <li>Split the fortygigE 1/0/6 interface into tengig
            interfaces:<codeblock>interface fortygigE 1/0/6
using tengig
The interface FortyGigE1/0/6 will be deleted. Continue? [Y/N]: y</codeblock>
          
     </li>
        <li>Configure the Ten-GigabitEthernet1/0/6:1
          interface:<codeblock>interface Ten-GigabitEthernet1/0/6:1
port link-type trunk
port trunk permit vlan 103
port trunk permit vlan 1261 to 1270
port trunk pvid vlan 103</codeblock></li>
        <li>Configure the Ten-GigabitEthernet1/0/6:2
          interface:<codeblock>interface Ten-GigabitEthernet1/0/6:2
port link-type trunk
port trunk permit vlan 103
port trunk permit vlan 1261 to 1270
port trunk pvid vlan 103</codeblock></li>
        <li>Enable l2vpn: <codeblock>l2vpn enable</codeblock></li>
        <li>Configure a passive TCP connection for OVSDB on port 6632:
          <codeblock>ovsdb server ptcp port 6632</codeblock></li>
        <li>Enable OVSDB server:<codeblock>ovsdb server enable</codeblock></li>
        <li>Enable a VTEP process:<codeblock>vtep enable</codeblock></li>
        <li>Configure 10.10.83.3 as the VTEP source IP. This acts as a hardware VTEP
          IP.<codeblock>tunnel global source-address 10.10.83.3</codeblock></li>
        <li>Configure the VTEP access
          port:<codeblock>interface Ten-GigabitEthernet1/0/5:3
vtep access port</codeblock></li>
        <li>Disable VxLAN tunnel
          mac-learning:<codeblock>vxlan tunnel mac-learning disable</codeblock></li>
        <li>Display the current configuration of the 5930 switch and verify the
          configuration:<codeblock>display current-configuration</codeblock></li>
      </ol>
      <p> After switch configuration is complete, you can dump OVSDB to see the entries.</p>
      <ol>
        <li> Run the ovsdb-client from any Linux machine reachable by the switch:
          <codeblock outputclass="nomaxheight">ovsdb-client dump --pretty tcp:10.10.85.10:6632

sdn@small-hLinux:~$ ovsdb-client dump --pretty tcp:10.10.85.10:6632
Arp_Sources_Local table
_uuid locator src_mac
----- ------- -------

Arp_Sources_Remote table
_uuid locator src_mac
----- ------- -------

Global table
_uuid                                managers switches
------------------------------------ -------- --------------------------------------
2c891edc-439b-4144-84d9-fa4cd88092bf []       [f5f4b43b-40bc-4640-b580-d4b12011f688]

Logical_Binding_Stats table
_uuid bytes_from_local bytes_to_local packets_from_local packets_to_local
----- ---------------- -------------- ------------------ ----------------

Logical_Router table
_uuid description name static_routes switch_binding
----- ----------- ---- ------------- --------------

Logical_Switch table
_uuid description name tunnel_key
----- ----------- ---- ----------

Manager table
_uuid inactivity_probe is_connected max_backoff other_config status target
----- ---------------- ------------ ----------- ------------ ------ ------

Mcast_Macs_Local table
MAC _uuid ipaddr locator_set logical_switch
--- ----- ------ ----------- --------------

Mcast_Macs_Remote table
MAC _uuid ipaddr locator_set logical_switch
--- ----- ------ ----------- --------------

Physical_Locator table
_uuid dst_ip encapsulation_type
----- ------ ------------------

Physical_Locator_Set table
_uuid locators
----- --------

Physical_Port table
_uuid                                description name                         port_fault_status vlan_bindings vlan_stats
------------------------------------ ----------- ---------------------------- ----------------- ------------- ----------
fda90870-656c-479b-91ee-852b70e2007e ""          "Ten-GigabitEthernet1/0/5:3" [UP]              {}            {}

Physical_Switch table
_uuid                                description management_ips name       ports                                  switch_fault_status tunnel_ips     tunnels
------------------------------------ ----------- -------------- ---------- -------------------------------------- ------------------- -------------- -------
f5f4b43b-40bc-4640-b580-d4b12011f688 ""          []             "L2GTWY02" [fda90870-656c-479b-91ee-852b70e2007e] []                  ["10.10.83.3"] []

Tunnel table
_uuid bfd_config_local bfd_config_remote bfd_params bfd_status local remote
----- ---------------- ----------------- ---------- ---------- ----- ------

Ucast_Macs_Local table
MAC _uuid ipaddr locator logical_switch
--- ----- ------ ------- --------------

Ucast_Macs_Remote table
MAC _uuid ipaddr locator logical_switch
--- ----- ------ ------- --------------</codeblock>
        </li>
      </ol>
    </section>
    <section id="enabling">
      <title>Enabling and configuring the L2 gateway agent</title>
      <ol>
        <li>Update the input model (in <codeph>control_plane.yml</codeph>) to specify where you want
          to run the neutron-l2gateway-agent. For example, see the line in bold in the following
          yml:
          <codeblock>---
 product:
 version: 2
 control-planes:
 - name: cp
   region-name: region1
 failure-zones:
 - AZ1
   common-service-components:
     - logging-producer
     - monasca-agent
     - freezer-agent
     - stunnel
     - lifecycle-manager-target
 clusters:
 - name: cluster1
   cluster-prefix: c1
   server-role: ROLE-CONTROLLER
   member-count: 2
   allocation-policy: strict
 service-components:
            
 ... 
 <b>- neutron-l2gateway-agent</b>
 ... )</codeblock></li>
        <li>Update <codeph>l2gateway_agent.ini.j2</codeph>. For example, here the IP address
          (10.10.85.10) must be the management IP address of your 5930 switch. Open the file in vi:
          <codeblock>$vi /home/stack/my_cloud/config/neutron/l2gateway_agent.ini.j2</codeblock></li>
        <li>Then make the
          changes:<codeblock>[ovsdb]
# (StrOpt) OVSDB server tuples in the format
# &lt;ovsdb_name>:&lt;ip address>:&lt;port>[,&lt;ovsdb_name>:&lt;ip address>:&lt;port>]
# - ovsdb_name: a symbolic name that helps identifies keys and certificate files
# - ip address: the address or dns name for the ovsdb server
# - port: the port (ssl is supported)
ovsdb_hosts = hardware_vtep:10.10.85.10:6632</codeblock></li>
        <li>By default, the L2 gateway agent initiates a connection to OVSDB servers running on the
          L2 gateway switches. Set the attribute <codeph>enable_manager</codeph> to <codeph>True
          </codeph>if you want to change this behavior (to make L2 gateway switches initiate a
          connection to the L2 gateway agent). In this case, it is assumed that the Manager table in
          the OVSDB hardware_vtep schema on the L2 gateway switch has been populated with the
          management IP address of the L2 gateway agent and the port.
          <codeblock>#enable_manager = False
#connection can be initiated by the ovsdb server.
#By default 'enable_manager' value is False, turn on the variable to True
#to initiate the connection from ovsdb server to l2gw agent.</codeblock></li>
        <li>If the port that is configured with <codeph>enable_manager = True</codeph> is any port
          other than 6632, update the <codeph>2.0/services/neutron/l2gateway-agent.yml</codeph>
          input model file with that port
          number:<codeblock>endpoints:
    - port: '6632'
      roles:
      - ovsdb-server</codeblock></li>
        <li>Note: The following command can be used to set the Manager table on the switch from a
          remote system:<p>
            <codeblock>sudo vtep-ctl --db=tcp:10.10.85.10:6632 set-manager tcp:10.10.85.130:6632</codeblock>
          </p></li>
        <li>For SSL communication, the command is:
          <codeblock>sudo vtep-ctl --db=tcp:10.10.85.10:6632 set-manager ssl:10.10.85.130:6632</codeblock>where
            <b>10.10.85.10</b> is the management IP address of the L2 gateway switch and
            <b>10.10.85.130</b> is the management IP of the host on which the L2 gateway agent
            runs.<p>Therefore, in the above topology, this command has to be repeated for
              <b>10.10.85.131</b> and <b>10.10.85.132</b>.</p></li>
        <li>If you are not using SSL, comment out the following:
          <codeblock>#l2_gw_agent_priv_key_base_path={{ neutron_l2gateway_agent_creds_dir }}/keys
#l2_gw_agent_cert_base_path={{ neutron_l2gateway_agent_creds_dir }}/certs
#l2_gw_agent_ca_cert_base_path={{ neutron_l2gateway_agent_creds_dir }}/ca_certs</codeblock></li>
        <li>If you are using SSL, then rather than commenting out the attributes, specify the
          directory path of the private key, the certificate, and the CA cert that the agent should
          use to communicate with the L2 gateway switch which has the OVSDB server enabled for SSL
          communication. <p>Make sure that the directory path of the files is given permissions 755,
            and the files’ owner is root and the group is root with 644
              permissions.</p><p><b>Private key:</b> The name should be the same as the symbolic
            name used above in ovsdb_hosts attribute. The extension of the file should be ".key".
            With respect to the above example, the filename will be
              hardware_vtep.key</p><p><b>Certificate</b> The name should be the same as the symbolic
            name used above in ovsdb_hosts attribute. The extension of the file should be “.cert”.
            With respect to the above example, the filename will be hardware_vtep.cert</p><p><b>CA
              certificate</b> The name should be the same as the symbolic name used above in
            ovsdb_hosts attribute. The extension of the file should be “.ca_cert”. With respect to
            the above example, the filename will be hardware_vtep.ca_cert</p></li>
        <li>To enable the HPE 5930 switch for SSL communication, execute the following commands:
          <codeblock>undo ovsdb server ptcp
undo ovsdb server enable
ovsdb server ca-certificate flash:/cacert.pem bootstrap
ovsdb server certificate flash:/sc-cert.pem
ovsdb server private-key flash:/sc-privkey.pem
ovsdb server pssl port 6632
ovsdb server enable</codeblock></li>
        <li>Data from the OVSDB sever with SSL can be viewed using the following command:<p>
            <codeblock>ovsdb-client -C &lt;ca-cert.pem> -p &lt;client-private-key.pem> -c &lt;client-cert.pem> dump ssl:10.10.85.10:6632</codeblock>
          </p>
        </li>
      </ol>
    </section>
    <section id="routing">
      <title>Routing between software and hardware</title>
      <p><b>VTEP networks</b>
      </p>
      <p>In order to allow L2 gateway switches to send VxLAN packets over the correct tunnels
        destined for the compute node and controller node VTEPs, you must ensure that the cloud VTEP
        (compute and controller) IP addresses are in different network/subnet from that of the L2
        gateway switches. You must also create a route between these two networks. This is explained
        below.</p>
      <ol>
        <li>In the following example of the input model file <codeph>networks.yml</codeph>, the
          GUEST-NET represents the cloud data VxLAN network. REMOTE-NET is the network that
          represents the hardware VTEP network.
          <codeblock>#networks.yml
 networks:
    - name: GUEST-NET
      vlanid: 103
      tagged-vlan: false
      cidr: 10.1.1.0/24
      gateway-ip: 10.1.1.10
      network-group: GUEST

    - name: REMOTE-NET
      vlanid: 183
      tagged-vlan: false
      cidr: 10.10.83.0/24
      gateway-ip: 10.10.83.3
      network-group: REMOTE</codeblock></li>
        <li>The route must be configured between the two networks in the
            <codeph>network-groups.yml</codeph> input model file:
              <codeblock>#network_groups.yml
 network-groups:
    - name: REMOTE
      routes:
        - GUEST

    - name: GUEST
      hostname-suffix: guest
      tags:
        - neutron.networks.vxlan:
            tenant-vxlan-id-range: "1:5000"
      routes:
        - REMOTE</codeblock><p><b>Note
              that the IP route is configured on the compute node. Per this route, the HPE 5930 acts
              as a gateway that routes between the two networks.</b></p></li>
        <li>On the compute node, it looks like this:
          <codeblock>stack@padawan-cp1-comp0001-mgmt:~$ sudo ip route
10.10.83.0/24 via 10.1.1.10 dev eth4</codeblock></li>
        <li>Run the following Ansible playbooks to apply the changes. <p>config-processor-run.yml:<codeblock>cd ~/helion/hos/ansible
ansible-playbook -i hosts/localhost config-processor-run.yml</codeblock>
            ready-deployment.yml:<codeblock>ansible-playbook -i hosts/localhost ready-deployment.yml</codeblock>
            hlm-reconfigure.yml:<codeblock>cd ~/scratch/ansible/next/hos/ansible/
ansible-playbook -i hosts/verb_hosts hlm-reconfigure.yml</codeblock>
          </p></li>
      </ol>
      <p><b>Notes: </b></p>
      <ol>
        <li>Make sure that the controller cluster is able to reach the management IP address of the
          L2 gateway switch. Otherwise, the L2 gateway agents running on the controllers will not be
          able to reach the gateway switches. </li>
        <li>Make sure that the interface on the baremetal server connected to the 5930 switch is
          tagged (this is explained shortly). </li>
      </ol>
    </section>
    <section id="connecting"><title>Connecting a baremetal server to the HPE 5930
        switch</title><p>As Ubuntu (baremetal server) is not aware of tagged packets, it is not
        possible for a virtual machine to communicate with a baremetal box.</p><p>You (the
        administrator) must manually perform configuration changes to the interface in the HPE 5930
        switch to which the baremetal server is connected so that the switch can send untagged
        packets. Either one of the following command sets can be used to do
        so:</p><codeblock>Interface &lt;interface number>
service-instance &lt;service-instance id>
encapsulation untagged
xconnect vsi &lt;vsi-name></codeblock>Or:<codeblock>Interface &lt;interface number>
service-instance &lt;service-instance id>
encapsulation s-vid &lt;vlan-id>
xconnect vsi &lt;vsi-name> access-mode ethernet</codeblock><p>There
        are two ways of configuring the baremetal server to communicate with virtual machines. If
        the switch sends tagged traffic, then the baremetal server should be able to receive the
        tagged traffic.</p></section>
    <section id="baremetalConfig">
      <title>Configuration on a baremetal server</title>
      <p><b> To receive tagged packets from the switch</b></p>
      <p>If the configuration changes mentioned previously are not made on the switch to send
        untagged traffic to the baremetal server, then the following changes are required on the
        baremetal server so that it can receive tagged traffic from the switch.</p>
      <p>Bare metal Management ip: 10.10.85.129 on interface em1 Switch 5930 is connected to
        baremetal on eth1 Need to set the IP address into tagged interface of eth1</p>
      <ol>
        <li>create tagged (VLAN 183) interface <codeblock>vconfig add eth1 183</codeblock></li>
        <li> Assign the IP address (10.10.10.129) to eth1.183 tagged interface ( IP from the subnet
          10.10.10.0/24 as VM(10.10.10.4) spawned in Compute node belongs to this subnet).
          <codeblock>ifconfig eth1.183 10.10.10.129/24</codeblock>
        </li>
      </ol>
    </section>
    <section id="NICbonding"><title>NIC bonding and IRF configuration</title> With L2 gateway in
      actiondeployment, NIC bonding can be enabled on compute nodes. For more details on NIC
      bonding, please refer to <xref
        href="../architecture/input_model/configobj/interfacemodels.xml#configobj_interfacemodels"
        >Interface
      Models</xref><!--http://docs.hpcloud.com/#3.x/helion/architecture/input_model/configobj/configurationobjects.html#configobj_interfacemodels-->
      In order to achieve high availability, HPE 5930 switches can be configured to form a cluster
      using Intelligent Resilient Framework (IRF). Please refer to the <xref
        href="http://h20564.www2.hpe.com/hpsc/doc/public/display?docId=c04567141" format="html"
         >HPE FlexFabric 5930 Switch Series configuraton guide</xref> for details. </section>
    <section id="scale">
      <title>Scale numbers tested</title>
      <ul>
        <li> Number of neutron port MACs tested on a single switch: 4000 </li>
        <li>Number of HPE 5930 switches tested: 2 </li>
        <li>Number of baremetal connected to a single HPE 5930 switch: 100 </li>
        <li>Number of L2 gateway connections to different networks: 800 </li>
      </ul>
    </section>
    <section id="commands">
     
      <title>L2 gateway commands</title> These commands are not part of the L2 gateway deployment.
      They are to be executed after L2 gateway is deployed. <ol>
        <li> Create Network <codeblock>neutron net-create net1</codeblock></li>
        <li> Create Subnet <codeblock>neutron subnet-create net1 10.10.10.0/24</codeblock></li>
        <li>Boot a tenant VM ( nova boot –image &lt;Image-id> --flavor 2 –nic net-id=&lt;net_id>
          &lt;VM name>)
          <codeblock>nova boot --image 1f3cd49d-9239-49cf-8736-76bac5360489 --flavor 2 --nic net-id=4f6c58b6-0acc-4e93-bb4c-439b38c27a23 VM</codeblock>
          Assume the VM was assigned the IP address 10.10.10.4 </li>
        <li>Create the L2 gateway filling in your information
          here:<codeblock>neutron l2-gateway-create --device name="&lt;switch name>",interface_names="&lt;interface_name>"  &lt;gateway-name></codeblock>
          For this
          example:<codeblock>neutron l2-gateway-create --device name="L2GTWY02",interface_names="Ten-GigabitEthernet1/0/5:3" gw1</codeblock>Ping
          from the VM (10.10.10.4) to baremetal server and from baremetal server (10.10.10.129) to
          the VM Ping should not work as there is no gateway connection created yet. </li>
        <li>Create l2 gateway Connection
          <codeblock>neutron l2-gateway-connection-create gw1 net1 --segmentation-id 183</codeblock></li>
        <li>Ping from VM (10.10.10.4) to baremetal and from baremetal (10.10.10.129) to VM Ping
          should work. </li>
        <li>Delete l2 gateway Connection
          <codeblock>neutron l2-gateway-connection-delete &lt;gateway id/gateway_name></codeblock>
        </li>
        <li>Ping from the VM (10.10.10.4) to baremetal and from baremetal (10.10.10.129) to the VM.
          Ping should not work as l2 gateway connection was deleted.</li>
      </ol>
    </section>
  </body>
</topic>
