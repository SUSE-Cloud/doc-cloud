<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE topic PUBLIC "-//OASIS//DTD DITA Topic//EN" "topic.dtd">
<topic id="dpdk_troubleshooting">
  <title><ph conkeyref="HOS-conrefs/product-title"/>Troubleshooting DPDK</title>
  <body>
    <section id="main"><title>Troubleshooting DPDK</title>
      <p>
        <ul id="ul_mx2_3qj_bx">
          <li><xref href="#dpdk_troubleshooting/hardware">Hardware configuration</xref></li>
          <li><xref href="#dpdk_troubleshooting/system">System configuration</xref></li>
          <li><xref href="#dpdk_troubleshooting/inputModel">Input model configuration</xref></li>
          <li><xref href="#dpdk_troubleshooting/reboot">Reboot requirements</xref></li>
          <li><xref href="#dpdk_troubleshooting/software">Software configuration</xref>
            <ul id="ul_nx2_3qj_bx">
              <li><xref href="#dpdk_troubleshooting/kernel">Kernel settings</xref></li>
              <li><xref href="#dpdk_troubleshooting/ovs">Open vSwitch settings</xref></li>
              <li><xref href="#dpdk_troubleshooting/settings">DPDK settings</xref></li>
            </ul></li>
          <li><xref href="#dpdk_troubleshooting/runtime">DPDK runtime</xref></li>
          <li><xref href="#dpdk_troubleshooting/errors">Errors</xref>
            <ul id="ul_ox2_3qj_bx">
              <li><xref href="#dpdk_troubleshooting/noIP">VM does not get fixed IP</xref></li>
              <li><xref href="#dpdk_troubleshooting/non">Vestiges of non-existent DPDK
                  devices</xref></li>
              <li><xref href="#dpdk_troubleshooting/startup">Startup issues</xref></li>
            </ul></li>
        </ul>
      </p>
    </section>
    <section id="hardware"><title>Hardware configuration</title>
      <p>Because there are several variations of hardware, it is up to you to verify that the
        hardware is configured properly.</p>
      <ul>
        <!--<li outputclass="highlightThis">Only Intel Niantic (82599) and Mellanox ConnectX-3 Pro NICs
          are supported.</li>-->
        <li>Only Intel based compute nodes are supported. There is no DPDK available for AMD-based
          CPUs.</li>
        <li> PCI-PT must be enabled for the NIC that will be used with DPDK. </li>
        <li>When using Intel Niantic and the igb_uio driver, the VT-d must be enabled in the
          BIOS.</li>
        <li> For DL360 Gen9 systems, the BIOS shared-memory <xref
            href="enabling_pcipt_on_gen9.dita#pcipt-gen9">must be disabled</xref>.</li>
        <li>Adequate memory must be available for <xref href="hugepages.dita#hugepages"
            >hugepage</xref> usage.</li>
        <li>Hyper-threading can be enabled but is not required for base functionality.</li>
        <li>Determine the PCI slot that the DPDK NIC(s) are installed in in order to determine the
          associated NUMA node.</li>
        <li outputclass="highlightThis">Only Intel Haswell and Broadwell microarchitectures are
          supported. They are found in DL360 Gen9 hardware. The Sandy Bridge, which is found in
          DL360 Gen8, is <b>NOT</b> supported. Skylake support, Gen10, is <b>TBD</b>.</li>
      </ul></section>
    <section id="system"><title>System configuration</title>
      <ul>
        <li>There is no DPDK available for RedHat compute nodes. Only HPE Linux compute nodes are
          supported</li>
        <li>If a NIC port is used with PCI-PT, SRIOV-only, or PCI-PT+SRIOV, then it can't be used
          with DPDK. They are mutually exclusive. This is because DPDK depends on an OvS bridge
          which doesn't exist if you use any combination of PCI-PT and SRIOV. You can use DPDK,
          SRIOV-only, and PCI-PT on difference interfaces of the same server.</li>
        <li>There is an association between the PCI slot for the NIC and a NUMA node. Make sure to
          use logical CPU cores that are on the NUMA node associated to the NIC. Use the following
          to determine which CPUs are on which NUMA node.
          <codeblock>$ lscpu
 
Architecture:          x86_64
CPU op-mode(s):        32-bit, 64-bit
Byte Order:            Little Endian
CPU(s):                48
On-line CPU(s) list:   0-47
Thread(s) per core:    2
Core(s) per socket:    12
Socket(s):             2
NUMA node(s):          2
Vendor ID:             GenuineIntel
CPU family:            6
Model:                 63
Model name:            Intel(R) Xeon(R) CPU E5-2650L v3 @ 1.80GHz
Stepping:              2
CPU MHz:               1200.000
CPU max MHz:           1800.0000
CPU min MHz:           1200.0000
BogoMIPS:              3597.06
Virtualization:        VT-x
L1d cache:             32K
L1i cache:             32K
L2 cache:              256K
L3 cache:              30720K
NUMA node0 CPU(s):     0-11,24-35
NUMA node1 CPU(s):     12-23,36-47</codeblock></li>
      </ul>
    </section>
    <section id="inputModel"><title>Input model configuration</title>
      <ul>
        <li>If you don't specify a driver for a DPDK device, the igb_uio will be selected as
          default.</li>
        <li>DPDK devices must be named <codeph>dpdk&lt;port-id></codeph> where the port-id starts at
          0 and increments sequentially.</li>
        <li>Tenant networks supported are untagged VXLAN and VLAN.</li>
        <li>Jumbo Frames MTU does not work with DPDK OvS. There is an upstream patch most likely
          showing up in OvS 2.6 and it can't be back-ported due to changes this patch relies
          upon.</li>
        <li>Sample VXLAN model</li>
        <li>Sample VLAN model</li>
      </ul>
    </section>
    <section id="reboot"><title>Reboot requirements</title>
      <p>A reboot of a compute node must be performed when an input model change causes the
        following:</p>
      <ol id="ol_azn_srj_bx">
        <li>After the initial site.yml play on a new HOS environment</li>
        <li>Changes to an existing HOS environment that modify the
            <codeph>/etc/default/grub</codeph> file, such as<ul id="ul_bzn_srj_bx">
            <li>hugepage allocations</li>
            <li>CPU isolation</li>
            <li>iommu changes</li>
          </ul></li>
        <li>Changes to a NIC port usage type, such as <p>
            <ul id="ul_czn_srj_bx">
              <li>moving from DPDK to any combination of PCI-PT and SRIOV</li>
              <li>moving from DPDK to kernel based eth driver</li>
            </ul>
          </p></li>
      </ol>
    </section>

    <section id="software"><title>Software configuration</title>
      <p>The input model is processed by the Configuration Processor which eventually results in
        changes to the OS. There are several files that should be checked to verify the proper
        settings were applied. In addition, after the inital site.yml play is run all compute nodes
        must be rebooted in order to pickup changes to the <codeph>/etc/default/grub</codeph> file
        for hugepage reservation, CPU isolation and iommu settings.</p>
      <p id="kernel"><b>Kernel settings</b></p><p>Check <codeph>/etc/default/grub</codeph> for the
        following</p><ol id="ol_jdx_xrj_bx">
        <li>hugepages</li>
        <li>CPU isolation</li>
        <li>that iommu is in passthru mode if the igb_uio driver is in use</li>
      </ol>
      <p id="ovs"><b>Open vSwitch settings</b></p><p>Check
          <codeph>/etc/default/openvswitch-switch</codeph> for</p><ol id="ol_kdx_xrj_bx">
        <li>using the <codeph>--dpdk</codeph> option</li>
        <li>core 0 set aside for EAL and kernel to share</li>
        <li>cores assigned to PMD drivers, at least two for each DPDK device</li>
        <li>verify that memory is reserved with socket-mem option</li>
        <li>Once <xref href="https://jira.hpcloud.net/browse/VNETCORE-2509" format="html"
            scope="external">VNETCORE-2509</xref> merges also verify that the umask is 022 and the
          group is libvirt-qemu</li>
      </ol><p id="settings"><b>DPDK settings</b></p><ol id="ol_ldx_xrj_bx">
        <li>check <codeph>/etc/dpdk/interfaces</codeph> for the correct DPDK devices</li>
      </ol>
    </section>

    <section id="runtime">
      <title>DPDK runtime</title>
      <p>All non-bonded DPDK devices will be added to individual OvS bridges. The bridges will be
        named <codeph>br-dpdk0</codeph>, <codeph>br-dpdk1</codeph>, etc. The name of the OvS bridge
        for bonded DPDK devices will be <codeph>br-dpdkbond0</codeph>,
        <codeph>br-dpdkbond1</codeph>, etc.</p>
      <ol id="ol_evt_dsj_bx">
        <li>Since each PMD thread is in a polling loop, it will use 100% of the CPU. Thus for two
          PMDs you would expect to see the ovs-vswitchd process running at 200%. This can be
          verified by running
          <codeblock>$ top
 
top - 16:45:42 up 4 days, 22:24,  1 user,  load average: 2.03, 2.10, 2.14
Tasks: 384 total,   2 running, 382 sleeping,   0 stopped,   0 zombie
%Cpu(s):  9.0 us,  0.2 sy,  0.0 ni, 90.8 id,  0.0 wa,  0.0 hi,  0.0 si,  0.0 st
KiB Mem:  13171580+total, 10356851+used, 28147296 free,   257196 buffers
KiB Swap:        0 total,        0 used,        0 free.  1085868 cached Mem
 
  PID USER      PR  NI    VIRT    RES    SHR S  %CPU %MEM     TIME+ COMMAND                        
 1522 root      10 -10 6475196 287780  10192 S 200.4  0.2  14250:20 ovs-vswitchd</codeblock></li>

        <!--<li>NOTE TO BE REMOVED Until VNETCORE-2509 merges you must manually edit
            <codeph>/etc/libvirt/qemu.conf</codeph> on every compute node and set
            <codeph>user=root</codeph> and <codeph>group=root</codeph>. Either reboot the compute
          node after changing or restart the libvirt service with
          <codeblock>sudo systemctl restart libvirtd.service </codeblock></li>-->

        <li>Verify that <codeph>ovs-vswitchd</codeph> is running with
          <codeblock>--dpdk option. ps -ef | grep ovs-vswitchd </codeblock></li>

        <li>PMD thread(s) are started when a DPDK port is added to an OvS bridge. Verify the port is
          on the bridge. <codeblock>sudo ovs-vsctl show</codeblock></li>

        <li>A DPDK port can't be added to an OvS bridge unless it is bound to a driver. Verify that
          the DPDK port is bound. <codeblock>sudo dpdk_nic_bind -s</codeblock></li>

        <li>Verify that the proper number of hugepages is on the correct NUMA node
          <codeblock>sudo virsh freepages --all</codeblock> or
          <codeblock>grep -R "" /sys/kernel/mm/hugepages/ /proc/sys/vm/*huge*</codeblock></li>

        <li>Verify that the VM and the DPDK PMD threads have both mapped the same hugepage(s)
          <codeblock># this will yield 2 process ids, use the 2nd one
ps -ef | grep ovs-vswitchd  
sudo ls -l /proc/&lt;process id>/fd | grep huge
            
# if running more than 1 VM you will need to figure out which one to use
ps -ef | grep qemu
sudo ls -l /proc/&lt;process id>/fd | grep huge </codeblock></li>
      </ol>
    </section>

    <section id="errors">
      <title>Errors</title>
      <p id="noIP"><b>VM does not get fixed IP</b></p><ol id="ol_fhs_qtj_bx">
        <li>DPDK Poll Mode drivers (PMD) communicates with the VM by direct access of the VM
          hugepage. If a VM is not created using <xref href="hugepages.dita#hugepages"
            >hugepages</xref> there is no way for DPDK to communicate with the VM and the VM will
          never be connected to the network.</li>
        <li>It has been observed that the DPDK communication with VM fails if the shared-memory is
          not disabled in BIOS for DL360 Gen9.</li>
      </ol><p id="non"><b>Vestiges of non-existent DPDK devices</b></p><ol id="ol_ghs_qtj_bx">
        <li>Incorrect input models that don't use the correct DPDK device name or don't use
          sequential port ids starting at 0 may leave non-existent devices in the OvS database.
          While this doesn't affect proper functionality it may be confusing.</li>
      </ol><p id="startup"><b>Startup issues</b></p><ol id="ol_hhs_qtj_bx">
        <li>Running the following will help diagnose startup issues with
          ovs-vswitchd:<codeblock>sudo journalctl -u openvswitch-switch.service --all</codeblock></li>
      </ol>
    </section>

  </body>
</topic>
