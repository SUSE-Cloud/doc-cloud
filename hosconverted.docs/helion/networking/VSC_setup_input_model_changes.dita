<?xml version="1.0" encoding="UTF-8"?>
    <!--Edit status: ready for edit (Nancy)-->
    <!DOCTYPE topic PUBLIC "-//OASIS//DTD DITA Topic//EN" "topic.dtd">
<topic id="VSC-on-VCP">
  <title><ph conkeyref="HOS-conrefs/product-title"/>Input Model Changes for VSC Setup on Virtual
    Control Plane</title>
  <body>
    <section>
      <ul>
        <li>Deploying the VSC via the VM Disk Image</li>
        <li>Deploying the VSC via the playbooks
          <ul>
            <li>Defining the Server-Role for VM hosts</li>
            <li>Add the definition for the HLM_HYPERVISOR_INTERFACES Network Group</li>
            <li>Add the VM Host to the Cloud</li>
            <li>New Service Definition for the DCN-VSC Service</li>
            <li>Control Plane Changes needed to Deploy DCN-VSC on VM Hosts</li>
            <li>Network Groups: Associate the data network to "HLM" network, MANAGEMENT to the
              "MANAGEMENT" network</li>
            <li>DCN Configuration Data Additions</li>
            <li>Setup of the VM-Host and Deployment of services on Controller VMs</li>
          </ul></li>
        <li>Additional Third-Party import needed for VSC deployment</li>
      </ul>
    </section>
    <!-- Deploying VSC via the VM Disk Image -->
    <section id="intro">
      <title>Deploying the VSC via the VM Disk Image</title> 
      The VSC is delivered as a VM definition along
      with the VM disk image. The image already has all the necessary software components in it.
      Furthermore, it is a VXworks based system. To deploy it requires the VM definition xml file be
      customized for the VM host, And then the boot files in the VSC disk image have to be updated
      on the disk image before the VSC VM can be booted up. So the steps to create a disk image,
      installing Hlinux on the VM disk is not needed. The steps taken to deploy the VSC VM is <ol>
        <li>Find a machine to host the VSC VM. If the VSC-VM is going to be running on a node that
          will host other virtual controllers, use the hlm-hypervisor role to bring up the VM Host.
          The plays in the role will be setting up the bridges needed by the VSC. JM: you don't need
          a specific hlm-hypervisor role, you just need to specify passthrough-network-groups in the
          interface model of the servers that will host the VSC VMs. The osconfig network_interface
          role will create the bridges needed by the VSC. </li>
        <li>When (1) is done, use site.yml to bring up and customize the VSC VM on the VM host: <ol>
            <li>Defining the virsh networks needed by the VSC and bring up the virsh networks </li>
            <li>Customize the VSC boot file </li>
            <li>Boot up the VSC </li>
            <li>Additional configuration steps for the VSC by remotely executing a configuration
              script. </li>
          </ol></li>
      </ol>
    </section>
    
    <!-- Deploying the VSC via Playbooks -->
    <section id="deploy_vsc_playbook">
      <title>Deploying the VSC via the playbooks</title>
    </section>
    <section id="expandCollapse">
      <sectiondiv outputclass="expandall">Expand All Sections</sectiondiv>
      <sectiondiv outputclass="collapseall">Collapse All Sections</sectiondiv>
    </section>
    <section id="changes">
      <title outputclass="headerH">Changes needed</title>
      <sectiondiv outputclass="insideSection"> To set up a VSC VM using the hlm-hypervisor role,
        changes are needed in an input model for: <ol>
          <li>Create a Server role for the VM host. </li>
          <li>Assign a machine to be the host of the VSC VM. One VSC VM runs on each VM host. </li>
          <li>In the input model, the VSC has two services: <b>dcn-vsc</b> and <b>dcn-vsc-data</b>. 
            The <b>dcn-vsc</b> service is used for the cloud’s management network and the 
            <b>dcn-vsc-data</b> service is used for the vxlan traffic. Hence it is usually 
            associated with a “guest” network group. The association is realized via “component endpoints.” 
            So in the network group definition (usually defined in a file called 
            <codeph>network_groups.yml</codeph> in the directory 
            <codeph>~/helion/my_cloud/definition/data</codeph>), the management network group needs 
            the <b>dcn-vsc</b> as one of its <b>component endpoints</b>. The guest network group needs to 
            have <b>dcn-vsc-data</b> as one of its <b>component endpoints</b>. Included below is an example 
            of a <codeph>network_group.yml</codeph> file that is configured to use the VSC.  In it the 
            cloud’s management network group is MANAGEMENT and guest network group is GUEST.
          </li>
          <li>New service definition for the DCN-VSC service </li>
          <li>Control Plane changes to add the VM host and to specify the VSC runs on the VM host. </li>
          <li>Network Groups: In my vagrant model, I associate the data network to "MANAGEMENT"
            network, VSC MANAGEMENT to the "HLM" network. In general, the VSC's management network
            should be associated with the network-tag for carrying management traffic. The VSC's
            data network should be associated with the one that carries VXLAN traffic.</li>
        </ol></sectiondiv></section>
    <section id="defining_roles">
      <title outputclass="headerH">Defining the server-role for VM hosts</title>
      <sectiondiv outputclass="insideSection"> In the input model's server_roles.yml file. Add a
        role called 'HYPERVISOR-ROLE' for the VM Hosts. So for a cloud with Controller, compute, and
        VRSG-role, the new server_roles.yml file is now:
        <codeblock outputclass="nomaxheight">#
# (c) Copyright 2015 Hewlett Packard Enterprise Development LP
#
# Licensed under the Apache License, Version 2.0 (the "License"); you may
# not use this file except in compliance with the License. You may obtain
# a copy of the License at
#
# http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
# WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
# License for the specific language governing permissions and limitations
# under the License.
#
---
  product:
    version: 2

  server-roles:

    - name: CONTROLLER-ROLE
      interface-model: INTERFACE_SET_1
      disk-model: DISK_SET_CONTROLLER

    - name: COMPUTE-ROLE
      interface-model: INTERFACE_SET_1
      disk-model: DISK_SET_COMPUTE

    - name: VRSG-ROLE
      interface-model: INTERFACE_SET_1
      disk-model: DISK_SET_COMPUTE

# New role for the VM host
    - name: HYPERVISOR-ROLE
      interface-model: HLM_HYPERVISOR_INTERFACES
      disk-model: DISK_SET_COMPUTE</codeblock>
        The definition of the HLM_HYPERVISOR_INTERFACES will be described below.
      </sectiondiv>
    </section>
    
    <section id="add_definition">
      <title outputclass="headerH">Adding the Definition</title>
      <sectiondiv outputclass="insideSection">
        <p>Adding the VSC-HOST-INTERFACES is declared in a file called
            <codeph>net_interfaces.yml</codeph> or <codeph>interfaces_set_1.yml</codeph>. Under the
          VSC-HOST-INTERFACES’s lan device used for the management traffic, declare the management
          network as a <b>passthrough-network-groups</b>. The same needs to be done for the lan
          device used for the vxlan traffic. </p>
        <p>Example of a VSC-HOST-INTERFACES set:
          <codeblock outputclass="nomaxheight">- name: VSC-HOST-INTERFACES
      network-interfaces:

        - name: hed1
          device:
            name: hed1
          network-groups:
            - MANAGEMENT
          passthrough-network-groups:
            - MANAGEMENT

        - name: hed2
          device:
            name: hed2
          network-groups:
            - HLM

        - name: hed3
          device:
            name: hed3
          network-groups:
            - GUEST
          passthrough-network-groups:
            - GUEST
          </codeblock>
        </p>Example a network_groups.yml file that associates the VSC services with the management
        and guest (vxlan) network groups: <codeblock>#
# (c) Copyright 2015,2016 Hewlett Packard Enterprise Development LP
#
# Licensed under the Apache License, Version 2.0 (the "License"); you may
# not use this file except in compliance with the License. You may obtain
# a copy of the License at
#
# http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
# WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
# License for the specific language governing permissions and limitations
# under the License.
#
---
  product:
    version: 2

  network-groups:
    - name: HLM
      hostname-suffix: hlm
      component-endpoints:
        - lifecycle-manager
        - lifecycle-manager-target


    #
    # GUEST
    #
    # This is the network group that will be used to provide
    # private networks to VMs
    #
    - name: GUEST
      hostname-suffix: guest
      tags:
        - neutron.networks.vxlan
      component-endpoints:
        - dcn-vsc-data
      # Set the mtu to 1550 to allow VMs to use a 1500 MTU without
      # underlying packet fragmentation.
      # Note: this requires setting a 1550 mtu (or higher) on any
      # untagged network group on the same interface as GUEST so
      # the MANAGEMENT network group must also have the mtu set to 1550.
      #
      #mtu: 1550
    - name: MANAGEMENT
      hostname-suffix: mgmt
      hostname: true

      tags:
        - neutron.networks.vlan:
            provider-physical-network: physnet1


      tls-component-endpoints:
        - barbican-api
        - mysql
        - rabbitmq
      component-endpoints:
        - dcn-vsc
        - default

      routes:
        - default

      load-balancers:
        - provider: ip-cluster
          name: lb
          tls-components:
            - default
          components:
            - vertica
            - nova-metadata
          roles:
            - internal
            - admin
          cert-file: helion-internal-cert

        - provider: ip-cluster
          name: extlb
          # external-name: myhelion.test
          tls-components:
            - default
          roles:
            - public
          cert-file: my-public-padawan-vsa-cert


    - name: EXTERNAL-GW</codeblock>
        <p> </p>
      </sectiondiv>
    </section>
    
    <section id="add_VM_host">
      <title outputclass="headerH">Adding the VM host to the cloud</title>
      <sectiondiv outputclass="insideSection"> One machine needs to be used to run each VSC-VM, You
        can either add the machine that will be used for the VSC role (i.e., a node that can be used
        to run the VSC VM) or a machine that will be running the hlm-hypervisor role (that runs
        other controller VMs in addition to the VSC). The example used is for running
        the VSC in a machine in the hlm-hypervisor role. The example adds a VM-Host (name
        vm-host-0001) machine to the server list (usually servers.yml) in addition to the
        controller, two compute nodes, and 1 VRSG node. The servers.yml is:
        <codeblock outputclass="nomaxheight">---
  product:
    version: 2

  baremetal:
    subnet: 192.168.10.0
    netmask: 255.255.255.0
    server-interface: eth2

  servers:

    - id: ccn-0001
      ip-addr: 192.168.10.3
      role: CONTROLLER-ROLE
      server-group: RACK1
      mac-addr: b2:72:8d:ac:7c:6f
      ilo-ip: 192.168.9.3
      ilo-password: password
      ilo-user: admin
      nic-mapping: VAGRANT
    - id: cpn-0001
      ip-addr: 192.168.10.4
      role: COMPUTE-ROLE
      server-group: RACK1
      mac-addr: d6:70:c1:36:43:f7
      ilo-ip: 192.168.9.4
      ilo-password: password
      ilo-user: admin
      nic-mapping: VAGRANT

    - id: cpn-0002
      ip-addr: 192.168.10.5
      role: COMPUTE-ROLE
      server-group: RACK1
      mac-addr: 8e:8e:62:a6:ce:76
      ilo-ip: 192.168.9.5
      ilo-password: password
      ilo-user: admin
      nic-mapping: VAGRANT

    - id: vrsg-0001
      ip-addr: 192.168.10.6
      role: VRSG-ROLE
      server-group: RACK1
      mac-addr: 8e:8e:62:a6:ce:77
      ilo-ip: 192.168.9.6
      ilo-password: password
      ilo-user: admin
      nic-mapping: VAGRANT

#######################################################################
#
# New Machine running as the VM-Host
#######################################################################
    - id: vm-host-0001
      ip-addr: 192.168.10.7
      role: HYPERVISOR-ROLE
      server-group: RACK1
      hlm-hypervisor: true 
      mac-addr: 8e:8e:62:a6:ce:78
      ilo-ip: 192.168.9.7
      ilo-password: password
      ilo-user: admin
      nic-mapping: VAGRANT</codeblock>
        NOTE that the VSC VM should not be hosted on any of the VRS or VRSG nodes. as the VSC VM's
        host requires the use of standard openvswitch whereas the VRS and VRSG nodes uses nuage's
        openvswitch. </sectiondiv></section>
    <section id="new_service_def">
      <title outputclass="headerH">New service definition for the DCN-VSC service</title>
      <sectiondiv outputclass="insideSection">The VSC requires IP addresses to be configured on 2
        LAN interfaces. One is on the management and the other on the data network. In this example,
        the VSC management network is on the "HLM" network. And the VSC's data network is on the
        "MANAGEMENT" network. The association of VSC networks will be different in other input
        models. <!-- <p outputclass="highlightThis"
          ><?oxy_custom_start type="oxy_content_highlight" color="255,255,0"?>(JM: this is
          confusing, you could describe this in terms of the entry-scale-kvm-vsa model which as
          management on MANAGEMENT and data (vxlan) on GUEST). The change is to add a "dcn-vsc-data"
          shadow service component and both sections has a needs-ip attribute. This attribute tells
          the config processor to allocate an IP address for the component. In this case, the CP
          allocates an IP for both the DCN-VSC service and the DCN-VSC-DATA shadow service. The
          "needs-ip" attribute tells config processor to allocate the ip addresses. The changes are
          added to the dcn-vsc.yml file (which is included in the third-party dcn
          example).<?oxy_custom_end?></p> <!-->
        <codeblock outputclass="nomaxheight">---
product:
    version: 2
service-components:
-   name: dcn-vsc
    mnemonic: DCN-VSC
    service: dcn
    needs-ip: true
-   name: dcn-vsc-data
    mnemonic: DCN-VSC-DATA
    service: dcn
    needs-ip: true</codeblock>
        These service components will be added to the list of service components operating on the VM
        Host. </sectiondiv>
    </section>
    <section id="control_place_changes">
      <title outputclass="headerH">Control plane changes needed to seploy DCN-VSC on VM
        hosts</title>
      <sectiondiv outputclass="insideSection">The changes to the control plane changes are: Add the
        VM-Host into control_plane.yml. The common-service-components list should only have
        lifecycle-manager-target. The other service components in common-service-components are
        added to the controllers' and compute nodes' service component list. The need to do this is
        described in HLM-Hypervisor instructions -
        PB3#Hypervisorinstructions-PB3-%22Empty%22hlmhypervisornode. Add dcn-vsc and dcn-vsc-tul to
        the service-component running on the vm-host node.
        <codeblock outputclass="nomaxheight">---
  product:
    version: 2
  control-planes:
    - name: ccp
      region-name: region1
      failure-zones:
        - AZ1
      configuration-data:
        - DCN-CONFIG-CP1
      common-service-components:
        - lifecycle-manager-target
###########################################################################################
#
# The following service components are removed from the common-service-components:
# 
#        - freezer-agent
#        - logging-rotate
#        - logging-producer
#        - monasca-agent
#        - stunnel
###########################################################################################
 clusters:
        - name: cluster0
          cluster-prefix: c0
          server-role: CONTROLLER-ROLE
          member-count: 1
          allocation-policy: strict
          service-components:
            - lifecycle-manager
            # Required for testing in (run-test.sh)
            - openstack-client
            - tempest
            - ntp-server
            - swift-ring-builder
            - mysql
            - ip-cluster
            - keystone-api
            - rabbitmq
            - glance-api:
                ha_mode: false
                glance_stores: 'file'
                glance_default_store: 'file'
            - glance-registry
            - cinder-api
            - cinder-scheduler
            - cinder-volume
            - cinder-backup
            - nova-api
            - nova-scheduler
            - nova-conductor
            - nova-console-auth
            - nova-novncproxy
            - neutron-server
            - neutron-ml2-plugin
            - dcn-ml2
            - horizon
            - swift-proxy
            - memcached
            - swift-account
            - swift-container
            - swift-object
            - heat-api
            - heat-api-cfn
            - heat-api-cloudwatch
            - heat-engine
            - ceilometer-api
            - ceilometer-polling
            - ceilometer-agent-notification
            - ceilometer-common
            - zookeeper
            - kafka
            - spark
            - vertica
            - storm
            - monasca-api
            - monasca-persister
            - monasca-notifier
            - monasca-threshold
            - monasca-transform
            - logging-server
            - ops-console-web
            - ops-console-monitor
            - freezer-api
            - barbican-api
            - barbican-worker
            - designate-api
            - designate-central
            - designate-pool-manager
            - designate-zone-manager
            - designate-mdns
            - powerdns
###########################################################################
#
#  Common service components now added to controllers
###########################################################################
            - freezer-agent
            - logging-rotate
            - logging-producer
            - monasca-agent
            - stunnel

      resources:
        - name: compute
          resource-prefix: compute
          server-role: COMPUTE-ROLE
          service-components:
            - ntp-client
            - nova-compute-kvm
            - nova-compute
            - dcn-vrs
            - dcn-metadata-agent
##################################################################################
#
#  Common service components now added to compute nodes
##################################################################################
            - freezer-agent
            - logging-rotate
            - logging-producer
            - monasca-agent
            - stunnel

###################################################################################
#
#  Service components for a VM-Host. Note that the common service components are
#  not added. Since ntp-servers on controllers are not up when running the
#  hlm-hypervisor-setup playbook, we have to make the VM host a NTP server.
#  
###################################################################################
       - name: vm-host
          resource-prefix: vm-host
          server-role: HYPERVISOR-ROLE
          service-components:
            - ntp-server
            - dcn-vsc
            - dcn-vsc-data</codeblock></sectiondiv></section>
    <section id="network_groups">
      <title outputclass="headerH">Network groups: Associating the data network to the HLM network,
        MANAGEMENT to the MANAGEMENT network</title>
      <sectiondiv outputclass="insideSection"> In the network groups, have the configuration
        processor allocate an IP address for the dcn-vsc service (for the VSC management lan
        interface) and the dcn-vsc-tul service (for the VSC data lan interface). This is done by
        modifying the network_groups.yml file. Add "dcn-vsc" to the HLM network list of component
        endpoints and "dcn-vsc-tul" to the MANAGEMENT network list of component endpoints.
        <codeblock outputclass="nomaxheight">---
  product:
    version: 2
  network-groups:
    - name: HLM
      hostname-suffix: hlm
      component-endpoints:
        - lifecycle-manager
        - lifecycle-manager-target
        - dcn-vsc
#         ^^^^^^^ Added dcn-vsc service to network HLM.  This the VSC's mgmt network
#         in this input model.
#

    - name: MANAGEMENT
      hostname-suffix: mgmt
      hostname: true
      tags:
        - neutron.networks.vxlan
        - neutron.networks.vlan:
            provider-physical-network: physnet1
      tls-component-endpoints:
        - barbican-api
        - mysql
        - rabbitmq
      component-endpoints:
        - dcn-vsc-data
#         ^^^^^^^ Added dcn-vsc-data service to network MANAGEMENT.  This the VSC's data network
#         in this input model.
#
        - default
      routes:
        - default
      load-balancers:
        - provider: ip-cluster
          name: lb
          tls-components:
            - default
          components:
            - vertica
            - nova-metadata
          roles:
            - internal
            - admin
          cert-file: helion-internal-cert

        - provider: ip-cluster
          name: extlb
          external-name: myhelion.test
          tls-components:
            - default
          roles:
            - public
          cert-file: my-public-deployerincloud-cert</codeblock>
      </sectiondiv>
    </section>
    
    <section id="dcn_cnfig">
      <title outputclass="headerH">DCN Configuration Data Additions</title>
      <sectiondiv outputclass="insideSection"> Here are the additional parameters needed to be added
        to the DCN config data (at file &lt;input_model>/data/dcn/dcn_config.yml):
        <codeblock outputclass="nomaxheight">---
  product:
    version: 2
  configuration-data:
    - name:  DCN-CONFIG-CP1
      services:
        - dcn
      data:
        vsd_host_name: vsd.example.com
        vsd_user: OSadmin
        vsd_passwd: OSadmin
        vsd_cms_id: ''
        vsc_active_ip: 192.168.245.253
#
# vsc_passive_ip is commented out because the example here only has 1 VSC.
#       vsc_passive_ip:
#####################################################################################
#
#  Additional Parameters needed for setting up VSC
#
#####################################################################################
        dns_domain_name: example.com
        vsc_mgmt_net: HLM
        vsc_data_net: MANAGEMENT
        vsc_image_name: vsc_singledisk
        vsc_user_name: admin
        vsc_user_pass: admin</codeblock></sectiondiv></section>
    <section id="setup_vm"><title outputclass="headerH">Setup of the VM-Host and deployment of
        services on controller VMs</title>
      <sectiondiv outputclass="insideSection"> Before the site.yml can be run, the VM host can be
        setup so that site.yml's plays to deploy services on the controller VMs hosted on it. To
        setup the VM host, the playbook hlm-hypervisor-setup has to be run right before running
        site.yml:
        <codeblock outputclass="nomaxheight"># At this point, the following has been run:
#  1. third-party import.
#  2. configuration processor
#  3. ready-deployment.yml
stack@deployerincloud-ccp-c0-m1-mgmt:~$ cd scratch/ansible/next/hos/ansible
stack@deployerincloud-ccp-c0-m1-mgmt:~/scratch/ansible/next/hos/ansible$ ansible-playbook -i hosts/verb_hosts hlm-hypervisor-setup.yml

# Now with the hypervisor machine setup, run site.yml:
stack@deployerincloud-ccp-c0-m1-mgmt:~/scratch/ansible/next/hos/ansible$ ansible-playbook -i hosts/verb_hosts site.yml</codeblock>
        REFERENCE: HLM-Hypervisor instructions - PB3 
      </sectiondiv>
    </section>
    
    <section id="additional_third_party">
      <title outputclass="headerH">Additional third-party import needed for VSC deployment</title>
      <sectiondiv outputclass="insideSection"> The VSC deployment is optional. The VSC related
        playbooks are on the lifecycle manager in
          <codeph>~/helion/hos_extensions/dcn/examples</codeph> directory. The directories
        "services" and "ansible" needs to be copied to the third-party area.
        <codeblock outputclass="highlightThis"><?oxy_custom_start type="oxy_content_highlight" color="255,255,0"?>/hos-extensions-dcn/dcn/examples$ ls -altr
total 16
drwxrwxr-x 3 stepenma stepenma 4096 Sep 21 08:01 services
drwxrwxr-x 8 stepenma stepenma 4096 Sep 21 08:01 ..
drwxrwxr-x 4 stepenma stepenma 4096 Sep 21 08:01 .
drwxrwxr-x 5 stepenma stepenma 4096 Sep 23 10:20 ansible<?oxy_custom_end?></codeblock>
        Therefore when you run the command
        <codeblock outputclass="nomaxheight">~/helion/hos_extensions:$ cp -R dcn ~/third-party/</codeblock>
        The playbooks for DCN-VSC deployment is not copied to the third-party area. The contents of
        the dcn/examples/ansible and dcn/examples/services directories also needs to be merged into
        ~/third-party/dcn/ansible and ~/third-party/dcn/services areas. So that the DCN-VSC role
        appears in ~/third-party/dcn/ansible/roles/dcn-vsc/.
        <codeblock outputclass="nomaxheight">cd ~/helion/hos_extensions/dcn/examples
tar cvf /tmp/vsc-ansible.tar ./
cd ~/third-party/dcn
tar xvf /tmp/vsc-ansible.tar</codeblock>
        In addition, the VSC image file "vsc_singledisk.qcow2" needs to be copied to the
        ~/third/party/dcn/ansible/roles/dcn-vsc/files/ directory. The .qcow2 file is found in
        DCN-VSC-4.0R5.zip:
        <codeblock outputclass="nomaxheight">/tmp: $ mkdir -p DCN-VSC-4.0R5
cd DCN-VSC-4.0R5
# Download DCN-VSC-4.0R5.zip   Download it then copy it to /tmp/DCN-VSC-4.0R3.
unzip DCN-VSC-4.0R5.zip
tar xzvf DCN-VSC-05-OEM-2041.tar.gz
cd ~/third-party
mkdir -p dcn/ansible/roles/dcn-vsc/files
cp /tmp/DCN-VSC-4.0R5/single_disk/vsc_singledisk.qcow2 dcn/ansible/roles/dcn-vsc/files/. </codeblock>
        Afterwards, run third-party import. The run the configuration processor on your input model:
        <codeblock outputclass="nomaxheight">cd ~/helion/hos/ansible
ansible-playbook -i hosts/localhost third-party-import.yml
cd
cp -r &lt;your-model> ~/helion/my_cloud/definition/
cd ~/helion
git add -A
git commit -m "My config"
cd ~/helion/hos/ansible/
ansible-playbook -i hosts/localhost config-processor-run.yml -e encrypt="" -e rekey=""</codeblock>
        The configuration processor will assign IP addresses to the VSCs. The addresses can be found
        in the file ~/helion/my_cloud/info/net_info.yml. The example below illustrates one such
        file:
        <codeblock outputclass="nomaxheight">service_ips:
    dcn-vsc:
    -   cluster: vm-host
        cluster_ip: {}
        control_plane: ccp
        hosts:
        -   hostname: deployerincloud-ccp-vm-host0001-DCN-VSC-hlm
            ip_address: 192.168.10.2
            name: deployerincloud-ccp-vm-host0001
        -   hostname: deployerincloud-ccp-vm-host0002-DCN-VSC-hlm
            ip_address: 192.168.10.8
            name: deployerincloud-ccp-vm-host0002
        network: HLM-NET
    dcn-vsc-data:
    -   cluster: vm-host
        cluster_ip: {}
        control_plane: ccp
        hosts:
        -   hostname: deployerincloud-ccp-vm-host0001-DCN-VSC-DATA-mgmt
            ip_address: 192.168.245.4
            name: deployerincloud-ccp-vm-host0001
        -   hostname: deployerincloud-ccp-vm-host0002-DCN-VSC-DATA-mgmt
            ip_address: 192.168.245.6
            name: deployerincloud-ccp-vm-host0002
            network: MANAGEMENT-NET</codeblock>
        The naming of the VSCs: the playbook names the VSC as "vsc-&lt;ip of the machine hosting the
        VSC in hex>" . In the example above the first VSC is running on
        deployerincloud-ccp-vm-host0001 and the second on deployerincloud-ccp-vm-host0002. Suppose
        the IP address of the first VSC's and second VSC's hosts are 10.10.11.21 and 10.10.11.31,
        respectively, and suppose the site's domain name is "example.com", The playbook will name
        the VSC as follows: <table frame="all" rowsep="1" colsep="1" id="table_v1j_2q4_mx">
          <tgroup cols="5">
            <colspec colname="c1" colnum="1"/>
            <colspec colname="c2" colnum="2"/>
            <colspec colname="c3" colnum="3"/>
            <colspec colname="c4" colnum="4"/>
            <colspec colname="c5" colnum="5"/>
            <thead>
              <row>
                <entry>VSC Mgmt IP</entry>
                <entry>VSC Data IP</entry>
                <entry>Hosted on</entry>
                <entry>VSC Hostname/Vursh Name of VSC VM</entry>
                <entry>VSC FQDN</entry>
              </row>
            </thead>
            <tbody>
              <row>
                <entry>192.168.10.2</entry>
                <entry>192.168.245.4</entry>
                <entry> deployerincloud-ccp-vm-host0001 <p>(IP: 10.50.11.21)</p>
                </entry>
                <entry>vsc-0A320B15</entry>
                <entry>vsc-0A320B15.example.com</entry>
              </row>
              <row>
                <entry>192.168.10.8</entry>
                <entry>192.168.245.6</entry>
                <entry>deployerincloud-ccp-vm-host0002(IP: 10.50.11.31)</entry>
                <entry>vsc-0A320B1F</entry>
                <entry>vsc-0A320B1F.example.com</entry>
              </row>
            </tbody>
          </tgroup>
        </table>The DCN-VSC playbooks DO NOT UPDATE the dns-servers with the VSC DNS records, the
        admin have to manually add the VSC FQDNs into the site's DNS servers. The admin has to
        decide which VSC will be the active and the standby.. In the example below, the admin
        decided to use "192.168.245.4" as the "active" VSC and "192.168.245.6" as the "passive" (aka
        "standby") VSC. Next, the admin will have to modify the
        ~/helion/my_cloud/definition/dcn/dcn_config.yml to change the "vsc_active_ip" and the
        "vsc_passive_ip" parameters:
        <codeblock outputclass="nomaxheight">---
  product:
    version: 2
  configuration-data:
    - name:  DCN-CONFIG-CP1
      services:
        - dcn
      data:
        dns_domain_name: example.com
        vsd_host_name: vsd4.example.com
        vsd_user: OSadmin
        vsd_passwd: OSadmin
        vsd_cms_id: ''
        vsc_active_ip:  192.168.245.4   &lt;-- changed by the admin
        # If there is only 1 VSC deployed, do not define vsc_passive_ip.  
        # Comment out the line below:
        vsc_passive_ip: 192.168.245.6   &lt;-- changed by the admin
        vsc_mgmt_net: HLM
        vsc_data_net: MANAGEMENT
        vsc_image_name: vsc_singledisk
        vsc_user_name: admin
        vsc_user_pass: admin</codeblock>
        Now do the following the deploy the cloud with the new VSCs:
        <codeblock outputclass="nomaxheight">$ cd ~/helion
$ git commit -a -m "Updated the VSC active and passive IPs in ~/helion/my_cloud/definition/data/dcn/dcn_config.yml"
$ cd ~/helion/hos/ansible
$ ansible-playbook -i hosts/localhost config-processor-run.yml -e encrypt="" -e rekey=""
$ ansible-playbook -i hosts/localhost ready-deployment.yml
$ cd ~/scratch/ansible/next/hos/ansible
$ ansible-playbook -i hosts/verb_hosts hlm-hypervisor-setup.yml
$ ansible-playbook -i hosts/verb_hosts site.yml</codeblock>
        The VSC-VMs will be deployed before the deployment of VRS and VRS-G. When the VRS and VRS-G
        comes up, they will be using the VSC at "vsc_active_ip". </sectiondiv></section>
  </body>
</topic>
