<?xml version="1.0" encoding="UTF-8"?>
<!--Edit status: not edited-->
<!DOCTYPE topic PUBLIC "-//OASIS//DTD DITA Topic//EN" "topic.dtd">
<topic id="changes_entryscale_kvm_vsa">
    <title><ph conkeyref="HOS-conrefs/product-title"/>Changes to Entry-scale KVM with VSA Model</title>
    
    <body><!--not tested-->
        <p conkeyref="HOS-conrefs/applies-to"/>
        
        <section>
            <title>Introduction</title> 
 

        </section>
 
 
      <table frame="all" rowsep="1" colsep="1" id="cloudConfig">
        <tgroup cols="1">
          <colspec colname="c1" colnum="1"/>
          
          
          <thead>
            <row>
              <entry>cloudConfig.yml changes in V3</entry>
              
              
            </row>
          </thead>
          <tbody>
            <row>
              
              <entry>
<codeblock> 
    # Disc space needs to be allocated to the audit directory before enabling
    # auditing.
    # Default can be either "disabled" or "enabled". Services listed in
    # "enabled-services" and "disabled-services" override the default setting.
    <b>audit-settings:
       audit-dir: /var/audit
       default: disabled
       #enabled-services:
       #  - keystone
       #  - barbican
       disabled-services:
         - nova
         - barbican
         - keystone
         - cinder
         - ceilometer
         - neutron</b>
 </codeblock>                            
                            
                            
                        </entry>
                        
 
                    </row>
                    
                </tbody>
            </tgroup>
        </table>

 
        <table frame="all" rowsep="1" colsep="1" id="control_plane">
            <tgroup cols="1">
                <colspec colname="c1" colnum="1"/>


                <thead>
                    <row>
                      <entry>control_plane.yml  changes in V3</entry>


                    </row>
                </thead>
                <tbody>
                    <row>

                        <entry>
<codeblock>
      <b>configuration-data:
        - OCTAVIA-CONFIG-CP1
        - NEUTRON-CONFIG-CP1</b>
</codeblock>                            
                            
                        </entry>
               
                    </row>    
                    
                    <row>             
         
                        <entry>
<codeblock>
          service-components:
            ...
            <b>- octavia-api
            - octavia-health-manager</b>
            ...
            <b>- barbican-api
            - barbican-client
            - barbican-worker
            - designate-api
            - designate-central
            - designate-pool-manager
            - designate-zone-manager
            - designate-mdns
            - designate-client
            - powerdns</b>
</codeblock>                            
                            
                            
                        </entry>
                        
 
                    </row>
                    
                </tbody>
            </tgroup>
        </table>


        <table frame="all" rowsep="1" colsep="1" id="disks_controller">
            <tgroup cols="2">
                <colspec colname="c1" colnum="1" colwidth="1*"/>
                <colspec colname="c2" colnum="2" colwidth="1*"/>

                <thead>
                    <row>
                        <entry>disks_controller.yml V2</entry>
                        <entry>disks_controller_600GB.yml V3</entry>
                       
                    </row>
                </thead>
                
                <tbody>
                    <row>

                        <entry>
<codeblock>
  disk-models:
  - name: CONTROLLER-DISKS
</codeblock>                                                       
                        </entry>
             
     
                        <entry>
<codeblock>
  disk-models:
  <b>- name: CONTROLLER-600GB-DISKS</b>
</codeblock>                                                       
                        </entry>
                        
                    </row>
                    
                    
                    <row>
                        <entry>
<codeblock>
        logical-volumes:
          ...
          - name: crash
            size: 13%
          ...
          - name: rabbitmq
            size: 13%
          ...
          - name: vertica
            size: 3%
          ...
          - name: kafka
            size: 2%
          ...
          - name: elasticsearch
            size: 30%

</codeblock>                                                      
                        </entry>
                        <entry>
                            <codeblock>
        logical-volumes:
          ...
          - name: crash
            <b>size: 11%</b>
          ...
          - name: rabbitmq
            <b>size: 4%</b>
          ...
          - name: vertica
            <b>size: 13%</b>
          ...
          - name: kafka
            <b>size: 11%</b>
          ...
          - name: elasticsearch
            <b>size: 21%</b>
</codeblock>                                                      
                        </entry>                        
                        
                        
                    </row>
                    <row>
                        <entry>&#160;</entry>

                        <entry>
                            
<codeblock id="glance">

    <b># Glance cache: if a logical volume with consumer usage 'glance-cache'
    # is defined Glance caching will be enabled. The logical volume can be
    # part of an existing volume group or a dedicated volume group.
    #  - name: glance-vg
    #    physical-volumes:
    #      - /dev/sdx
    #    logical-volumes:
    #      - name: glance-cache
    #        size: 95%
    #        mount: /var/lib/glance/cache
    #        fstype: ext4
    #        mkfs-opts: -O large_file
    #        consumer:
    #          name: glance-api
    #          usage: glance-cache</b>
</codeblock>
<codeblock id="audit">
    <b># Audit: Audit logs can consume significant disc space.  If you
    # are enabling audit then it is recommended that you use a dedicated
    # disc.
    #  - name: audit-vg
    #    physical-volumes:
    #      - /dev/sdz
    #    logical-volumes:
    #      - name: audit
    #        size: 95%
    #        mount: /var/audit
    #        fstype: ext4
    #        mkfs-opts: -O large_file</b>
</codeblock>                            
                        </entry>
                        
                    </row>
     
     
     
     
     
                    
                </tbody>
            </tgroup>
        </table>



        <table frame="all" rowsep="1" colsep="1" id="disks_controller_1TB">
            <tgroup cols="2">
                <colspec colname="c1" colnum="1" colwidth="1*"/>
                <colspec colname="c2" colnum="2" colwidth="1*"/> 

                <thead>
                    <row>
                        <entry>disks_controller_1TB.yml V2</entry>
                        <entry>disks_controller_1TB.yml V3</entry>
                     
                    </row>
                </thead>
                <tbody>
                    <row>

                        <entry>
<codeblock>
        logical-volumes:
          ...
          - name: root
            size: 5%
          ...
          - name: crash
            size: 8%
          ...
          - name: log
            size: 10%
          ...
          - name: mysql
            size: 10%
          ...
          - name: rabbitmq
            size: 8%
          ...
          - name: vertica
            size: 5%
          ...
          - name: kafka
            size: 2%
          ...
          - name: elasticsearch
            size: 45%
</codeblock>                            
                        </entry>
                        <entry>
<codeblock>
        logical-volumes:
          ...
          - name: root
            <b>size: 6%</b>
          ...
          - name: crash
            <b>size: 6%</b>
          ...
          - name: log
            <b>size: 16%</b>
          ...
          - name: mysql
            <b>size: 6%</b>
          ...
          - name: rabbitmq
            <b>size: 7%</b>
          ...
          - name: vertica
            <b>size: 19%</b>
          ...
          - name: kafka
            <b>size: 2%</b>
          ...
          - name: elasticsearch
            <b>size: 13%</b>
</codeblock>             
                               
                        </entry>
                    </row>
                    
                    <row>
                        <entry>&#160;</entry>
                        <entry>
                            
<codeblock id="glance2">
    #  Glance cache: if a logical volume with consumer usage 'glance-cache'
    #  is defined Glance caching will be enabled. The logical volume can be
    #  part of an existing volume group or a dedicated volume group.
    #  - name: glance-vg
    #    physical-volumes:
    #      - /dev/sdx
    #    logical-volumes:
    #     - name: glance-cache
    #       size: 95%
    #       mount: /var/lib/glance/cache
    #       fstype: ext4
    #       mkfs-opts: -O large_file
    #       consumer:
    #         name: glance-api
    #         usage: glance-cache
</codeblock>
<codeblock id="audit2">
    # Audit: Audit logs can consume significant disc space.  If you
    # are enabling audit then it is recommended that you use a dedicated
    # disc.
    #  - name: audit-vg
    #    physical-volumes:
    #      - /dev/sdz
    #    logical-volumes:
    #      - name: audit
    #        size: 95%
    #        mount: /var/audit
    #        fstype: ext4
    #        mkfs-opts: -O large_file
</codeblock>                            
                        </entry>
                        
                    </row>                    
                    
                </tbody>
            </tgroup>
        </table>
        


        <table frame="all" rowsep="1" colsep="1" id="network_groups">
            <tgroup cols="2">
                <colspec colname="c1" colnum="1" colwidth="1*"/>
                <colspec colname="c2" colnum="2" colwidth="1*"/>
            
                
                <thead>
                    <row>
                        <entry>network_groups.yml V2</entry>
                        <entry>network_groups.yml V3</entry>                    
                    </row>
                </thead>
                <tbody>
                    <row>
                        <entry>
<codeblock>
          tls-components:
            - default
          roles:
            - public
          cert-file: my-public-cert
          # This is the name of the certificate that will be used on load balancer.
          # Replace this with name of file in "~helion/my_cloud/config/tls/certs/".
          # This is the certificate that matches your setting for external-name
          #
          # Note that it is also possible to have per service certificates:
          #
          # cert-file:
          # default: my-public-cert
          # horizon: my-horizon-cert
          # nova-api: my-nova-cert
          #
          #
</codeblock>                            
                            
                        </entry>
                        <entry>
<codeblock>
          tls-components:
            - default
          roles:
            - public
          cert-file: my-public-cert
          # This is the name of the certificate that will be used on load balancer.
          # HOS will look for a file with this name in the config/tls/certs directory.
          # This is the certificate that matches your setting for external-name
          #
          # Note that it is also possible to have per service certificates:
          #
          # cert-file:
          # default: my-public-cert
          # horizon: my-horizon-cert
          # nova-api: my-nova-cert
          #
          # The configuration-processor will also create a request templates for each
          # named certificates under
          # "info/cert_reqs/"
          #
          # And this will be of the form
          #
          # info/cert_reqs/my-public-cert
          # info/cert_reqs/my-horizon-cert
          # info/cert_reqs/my-nova-cert
          #
          # These request templates contain the subject Alt-names that
          # the certificates need. A customer can add to this template
          # before generating their Certificate Signing Request (CSR).
          # They would then send the CSR to their CA to be signed and
          # receive the certificate, which can then be dropped into
          # "config/tls/certs".
          #
          # When you bring in your own certificate you may want to bring
          # in the trust chains (or CA certificate) for this certificate.
          # This is usually not required if the CA is a public signer that
          # gets bundled by the system. However, we suggest you include it
          # into HOS anyway by copying the file into the directory
          # "config/cacerts/".
          # Note that the file extension should be .crt or it will not
          # be processed by HOS.
          #
</codeblock>                            
                            
                        </entry>
                    </row>
                    
                    <row>
                        <entry>
<codeblock>
   - name: GUEST
      hostname-suffix: guest
      tags:
        - neutron.networks.vxlan
</codeblock>                            
                            
                        </entry>
                        <entry>
<codeblock>                            
    - name: GUEST
      hostname-suffix: guest
      tags:
        - neutron.networks.vxlan
      <b># Set the mtu to 1550 to allow VMs to use a 1500 MTU without
      # underlying packet fragmentation.
      # Note: this requires setting a 1550 mtu (or higher) on any
      # untagged network group on the same interface as GUEST so
      # the MANAGEMENT network group must also have the mtu set to 1550.
      #
      #mtu: 1550    </b>   
</codeblock>
                        </entry>
                        
                        
                    </row>
                    
                    <row>
                        <entry>
<codeblock>
    - name: MANAGEMENT
      hostname-suffix: mgmt
      hostname: true

      component-endpoints:
        - default

      routes:
        - default

      load-balancers:
        - provider: ip-cluster
          name: lb
          components:
            - default
          roles:
            - internal
            - admin
</codeblock>                           
                            
                        </entry>
                        <entry>
<codeblock id="tls-component-endpoints">
    - name: MANAGEMENT
      hostname-suffix: mgmt
      hostname: true

      <b>tls-component-endpoints:
      # The following service endpoint is behind TLS
        - barbican-api</b>
      component-endpoints:
        - default

      routes:
        - default
        <b>- OCTAVIA-MGMT-NET</b>

      load-balancers:
        - provider: ip-cluster
          name: lb
          <b>tls-components:
            - default</b>
          components:
          # These services do not currently support TLS
           <b> - vertica
            - rabbitmq
            - mysql
            - nova-metadata</b>
          roles:
            - internal
            - admin
          <b>cert-file: helion-internal-cert</b>
          # The helion-internal-cert is a reserved name and
          # this certificate will be autogenerated. Customer
          # can bring in their own cert with a different name
          # and follow the process described for the external
          # loadbalancer configuration above. See under
          # my-public-cert. It is important to use the request
          # template generated by the config processor as there
          # are more Subject Alt-name entries for the internal
          # certificate than the external certificate.

</codeblock>                            
                            
                        </entry>                        
                    </row>
                    
                    <row>
                        <entry>&#160;</entry>
                        <entry>
<codeblock>
      <b># Uncomment the following line to accommodate a 1550 MTU for the GUEST network group
      #mtu: 1550</b>
</codeblock>                            
                            
                        </entry>
                        
                    </row>
                    
                </tbody>
            </tgroup>
        </table>
        
                            
        <table frame="all" rowsep="1" colsep="1" id="server_roles">
            <tgroup cols="2">
                <colspec colname="c1" colnum="1" colwidth="1*"/>
                <colspec colname="c2" colnum="2" colwidth="1*"/>
          
                
                <thead>
                    <row>
                        <entry>server_roles.yml V2</entry>
                        <entry>server_roles.yml V3</entry>                     
                    </row>
                </thead>
                <tbody>
                    <row>

                        <entry>
<codeblock>
  server-roles:

    - name: CONTROLLER-ROLE
      interface-model: CONTROLLER-INTERFACES
      disk-model: CONTROLLER-DISKS 
</codeblock>
                        </entry>
                        <entry>
                            <codeblock>
  server-roles:

    - name: CONTROLLER-ROLE
      interface-model: CONTROLLER-INTERFACES
      disk-model: CONTROLLER-1TB-DISKS
</codeblock>
                        </entry>                        
                        
                    </row>
                </tbody>
            </tgroup>
        </table>


        
        
        <table frame="all" rowsep="1" colsep="1" pgwide="1" id="swift_rings">
            <tgroup cols="2">
                <colspec colname="c1" colnum="1" colwidth="1*"/>
                <colspec colname="c2" colnum="2" colwidth="1*"/>
              
                
                <thead>
                    <row>
                        <entry>swift/rings.yml V2</entry>
                        <entry>swift/rings.yml V3</entry>                       
                    </row>
                </thead>
                <tbody>
                    <row>
                        <entry>
<codeblock>
ring-specifications:

  - region-name: region1

</codeblock>
                        </entry>
                        <entry>
<codeblock>
ring-specifications:

  - region-name: region1
    <b>swift-zones:
      - id: 1
        server-groups:
          - AZ1
      - id: 2
        server-groups:
          - AZ2
      - id: 3
        server-groups:
          - AZ3</b>
</codeblock>                            
                            
                        </entry>
                    </row>
                    <row>
                        <entry>
<codeblock>
    rings:
      - name: account
        display-name: Account Ring
        <b>min-part-time: 16</b>
        partition-power: 12
        replication-policy:
          replica-count: 3
</codeblock>                            
                            
                        </entry>
                        <entry>
<codeblock>
    rings:
      - name: account
        display-name: Account Ring
        <b>min-part-hours: 16</b>
        partition-power: 12
        replication-policy:
          replica-count: 3
</codeblock>                            
                            
                        </entry>
                        
                    </row>
                    
                    <row>
                        <entry>
<codeblock>
      - name: container
        display-name: Container Ring
        <b>min-part-time: 16</b>
        partition-power: 12
        replication-policy:
          replica-count: 3
</codeblock>                            
                            
                        </entry>
                        <entry>
<codeblock>
      - name: container
        display-name: Container Ring
        <b>min-part-hours: 16</b>
        partition-power: 12
        replication-policy:
          replica-count: 3
</codeblock>                            
                            
                        </entry>
                        
                    </row>
                    
                    <row>
                        <entry>
<codeblock>
      - name: object-0
        display-name: General
        default: yes
        <b>min-part-time: 16</b>
        partition-power: 12
        replication-policy:
          replica-count: 3
</codeblock>                            
                            
                        </entry>
                        <entry>
<codeblock>
      - name: object-0
        display-name: General
        default: yes
        <b>min-part-hours: 16</b>
        partition-power: 12
        replication-policy:
          replica-count: 3
</codeblock>                                
                        </entry>
                        
                    </row>
                    
                    
                </tbody>
            </tgroup>
        </table>

    
        <table frame="all" rowsep="1" colsep="1" id="octavia">
            <tgroup cols="1">
                <colspec colname="c1" colnum="1"/>
                
                
                <thead>
                    <row>
                        <entry>octavia/octavia_config.yml V3</entry>
                        
                        
                    </row>
                </thead>
                <tbody>
                    <row>
                        
                        <entry>
<codeblock>
<b>  configuration-data:
    - name: OCTAVIA-CONFIG-CP1
      services:
        - octavia
      data:
        amp_network_name: OCTAVIA-MGMT-NET</b>
</codeblock>    
                        </entry>
                    </row>
                </tbody>
            </tgroup>
        </table>
        
        
        <table frame="all" rowsep="1" colsep="1" id="neutron">
            <tgroup cols="1">
                <colspec colname="c1" colnum="1"/>
                
                
                <thead>
                    <row>
                        <entry>neutron/neutron_config.yml V3</entry>
                      
                    </row>
                </thead>
                <tbody>
                    <row>
                        
                        <entry>
<codeblock>
<b>  configuration-data:
    - name:  NEUTRON-CONFIG-CP1
      services:
        - neutron
      data:
        neutron_provider_networks:
        - name: OCTAVIA-MGMT-NET
          provider:
            - network_type: vlan
              physical_network: physnet1
              segmentation_id: 106
          cidr: 172.30.1.0/24
          no_gateway:  True
          enable_dhcp: True
          allocation_pools:
            - start: 172.30.1.10
              end: 172.30.1.250
          host_routes:
            # route to MANAGEMENT-NET-1
            - destination: 192.168.245.0/24
              nexthop:  172.30.1.1</b>
</codeblock>
                        </entry>
                    </row>
                </tbody>
            </tgroup>
        </table>
        

 
 
 
    </body>
</topic>
