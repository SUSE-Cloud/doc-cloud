<?xml version="1.0" encoding="UTF-8"?>
<!--Edit status: not edited-->
<!DOCTYPE topic PUBLIC "-//OASIS//DTD DITA Topic//EN" "topic.dtd">
<topic id="entryscale_kvm_ceph">
  <title><ph conkeyref="HOS-conrefs/product-title"/>Entry-scale KVM with Ceph Model</title>
  <body>
    <!--not tested-->
    <p conkeyref="HOS-conrefs/applies-to"/>
    <section id="about">
      <p>This example provides a KVM-based cloud using Ceph for both block and object storage.</p>
    </section>
    <section id="vlan">
      <p>The network traffic is segregated into the following VLANs:</p>
      <ul>
        <li><b>Cloud Management</b> - This is the network that will be used for all internal traffic
          between the cloud services.</li>
        <li><b>OSD Internal</b> - This is the network that will be used for internal traffic of
          cluster among Ceph OSD servers. Only Ceph OSD servers will need connectivity to this
          network.</li>
        <li><b>OSD Client</b> - This is the network that Ceph clients will use to talk to Ceph
          Monitor and OSDs. Cloud controllers, Nova Compute, Ceph Monitor, OSD and Rados Gateway
          servers will need connectivity to this network.</li>
      </ul>
      <p>This diagram below illustrates the physical networking used in this configuration. Click
        any network name in the diagram to see that network isolated.</p>
    </section>
    <p><image href="../../../media/entryScaleCeph/Entry-Scale-Ceph-AllNetworks.png"/></p>
    <p><xref href="../../../media/entryScaleCeph/Entry-Scale-Ceph-AllNetworks.png"
          scope="external" format="html">Download full image</xref></p>
      <p><xref href="./../../../media/templates/HOS_Network_Diagram_Template.zip" scope="external"
          format="html">Download Editable Visio Network Diagram Template</xref></p>
      <p>This configuration is based on the <codeph>entry-scale-kvm-ceph</codeph> cloud input model
        which is included with the <keyword keyref="kw-hos"/> distro. You will need to make the
        changes outlined below prior to the deployment of your Ceph cluster.</p>
    <section
      conref="../../planning/rec_min_entryscale_kvm_ceph.xml#rec_min_entryscale_kvm_ceph/rec_min"/>
    <section id="ceph3_nicmappings">
      <title>nic_mappings.yml</title>
      <p>Ensure that your baremetal server NIC interfaces are correctly specified in the
          <codeph>~/helion/my_cloud/definition/data/nic_mappings.yml</codeph> file and that they
        meet the server requirements.</p>
      <p>Here is an example with notes in-line:</p>
      <codeblock>nic-mappings:

## NIC specification for controller nodes.  A bonded interface is used for the management 
## network while a separate interface is used to connect to the Ceph nodes.
  - name: CONTROLLER-NIC-MAPPING
    physical-ports:
       - logical-name: hed1
         type: simple-port
         bus-address: "0000:07:00.0"
        
       - logical-name: hed2
         type: simple-port
         bus-address: "0000:08:00.0"
         
       - logical-name: hed3
         type: simple-port
         bus-address: "0000:09:00.0"
       
       - logical-name: hed4
         type: simple-port
         bus-address: "0000:0a:00.0"
         
## NIC specification for compute nodes. One interface is used for the management 
## network while the second interface is used to connect to the Ceph nodes.
  - name: COMPUTE-NIC-MAPPING
    physical-ports:
       - logical-name: hed3
         type: simple-port
         bus-address: "0000:04:00.0"

       - logical-name: hed4
         type: simple-port
         bus-address: "0000:04:00.1"

## NIC specification for OSD nodes. The first interface is used for management network
## traffic. The second interface is used for client or public traffic. The third
## interface is used for internal OSD traffic.
  - name: OSD-NIC-MAPPING
    physical-ports:
       - logical-name: hed1
         type: simple-port
         bus-address: "0000:06:00.0"

       - logical-name: hed2
         type: simple-port
         bus-address: "0000:06:00.1"

       - logical-name: hed3
         type: simple-port
         bus-address: "0000:06:00.2"

## NIC specification for RADOS Gateway nodes. The first interface is used for management network
## traffic. The second interface is used for client or public traffic.
  - name: RGW-NIC-MAPPING
    physical-ports:
       - logical-name: hed1
         type: simple-port
         bus-address: "0000:07:00.0"

       - logical-name: hed2
         type: simple-port
         bus-address: "0000:07:00.1"</codeblock>
    </section>
    <section id="ceph3_servers">
      <title>servers.yml</title>
      <p>Ensure that your servers in the
          <codeph>~/helion/my_cloud/definition/data/servers.yml</codeph> file are mapped to the
        correct NIC interface.</p>
      <p>An example with the bolded line for <codeph>nic-mapping</codeph> illustrating this:</p>
      <codeblock># Controller Nodes
  - id: controller1
    ip-addr: 10.13.111.138
    server-group: RACK1
    role: CONTROLLER-ROLE
    <b>nic-mapping: CONTROLLER-NIC-MAPPING</b>
    mac-addr: "f0:92:1c:05:69:10"
    ilo-ip: 10.12.8.214
    ilo-password: password
    ilo-user: admin
    
# Compute Nodes
  - id: compute1
    ip-addr: 10.13.111.139
    server-group: RACK1
    role: COMPUTE-ROLE
    <b>nic-mapping: COMPUTE-NIC-MAPPING</b>
    mac-addr: "83:92:1c:55:69:b0"
    ilo-ip: 10.12.8.215
    ilo-password: password
    ilo-user: admin
        
# OSD Nodes
  - id: osd1
    ip-addr: 10.13.111.140
    server-group: RACK1
    role: OSD-ROLE
    <b>nic-mapping: OSD-NIC-MAPPING</b>
    mac-addr: "d9:92:1c:25:69:e0"
    ilo-ip: 10.12.8.216
    ilo-password: password
    ilo-user: admin

# Ceph RGW Nodes
  - id: rgw1
    ip-addr: 192.168.10.12
    role: RGW-ROLE
    server-group: RACK1
    <b>nic-mapping: RGW-NIC-MAPPING</b>
    mac-addr: "8b:f6:9e:ca:3b:62"
    ilo-ip: 192.168.9.12
    ilo-password: password
    ilo-user: admin

  - id: rgw2
    ip-addr: 192.168.10.13
    role: RGW-ROLE
    server-group: RACK2
    <b>nic-mapping: RGW-NIC-MAPPING</b>
    mac-addr: "8b:f6:9e:ca:3b:63"
    ilo-ip: 192.168.9.13
    ilo-password: password
    ilo-user: admin </codeblock>
    </section>
    <section id="ceph3_netinterfaces">
      <title>net_interfaces.yml</title>
      <p>Define a new interface set for your OSD interfaces in the
          <codeph>~/helion/my_cloud/definition/data/net_interfaces.yml</codeph> file.</p>
      <p>Here is an example with notes in-line:</p>
      <codeblock>
- name: CONTROLLER-INTERFACES
  network-interfaces:
## This bonded interface is used by the controller 
## nodes for cloud management traffic.
    - name: BOND0
      device:
         name: bond0
      bond-data:
         options:
            mode: active-backup
            miimon: 200
            primary: hed1
         provider: linux
         devices:
            - name: hed1
            - name: hed2
        network-groups:
        - EXTERNAL-API
        - EXTERNAL-VM
        - GUEST
        - MANAGEMENT
## This interface is used to connect the controller
## node to the Ceph nodes so that any Ceph client
## like cinder-volume can route data directly to
## Ceph over this interface.
    - name: ETH2
      device:
        name: hed3
      network-groups:
        - OSD-CLIENT
        
- name: COMPUTE-INTERFACES
  network-interfaces:
    - name: HETH3
      device:
         name: hed3
      forced-network-groups:
         - EXTERNAL-VM
         - GUEST
         - MANAGEMENT
## This interface is used to connect the compute node
## to the Ceph cluster so that a workload VM can route
## data traffic to the Ceph cluster over this interface.
    - name: HETH4
       device:
          name: hed4
       forced-network-groups:
          - OSD-CLIENT
        
- name: OSD-INTERFACES
  network-interfaces:
## This defines the interface used for management 
## traffic like logging, monitoring, etc.
    - name: HETH1
      device:
          name: hed1
      network-groups:
        - MANAGEMENT
## This defines the interface used for client
## or data traffic.
    - name: HETH2
      device:
          name: hed2
      network-groups:
        - OSD-CLIENT
## This defines the interface used for internal
## cluster communication among OSD nodes.
    - name: HETH3
      device:
          name: hed3
      network-groups:
        - OSD-INTERNAL

   - name: RGW-INTERFACES
     network-interfaces:
       - name: BOND0
         device:
            name: bond0
         bond-data:
            options:
                mode: active-backup
                miimon: 200
                primary: hed3
            provider: linux
            devices:
              - name: hed3
              - name: hed4
         network-groups:
           - MANAGEMENT
           - OSD-CLIENT</codeblock>
    </section>
    <section id="ceph3_networkgroups">
      <title>network_groups.yml</title>
      <p>Define the OSD network group in the
          <codeph>~/helion/my_cloud/definition/data/network_groups.yml</codeph> file:</p>
      <codeblock>#
# OSD client
#
# This is the network group that will be used for
# internal traffic of cluster among OSDs.
#
- name: OSD-CLIENT
  hostname-suffix: osdc
  
  component-endpoints
    - ceph-monitor
    - ceph-osd
    - ceph-radosgw
    
#
# OSD internal
#
# This is the network group that will be used for
# internal traffic of cluster among OSDs.
#
- name: OSD-INTERNAL
  hostname-suffix: osdi
  
  component-endpoints:
    - ceph-osd-internal</codeblock>
    </section>
    <section id="ceph3_networks">
      <title>networks.yml</title>
      <p>Define the OSD VLAN in the <codeph>~/helion/my_cloud/definition/data/networks.yml</codeph>
        file.</p>
      <p>The example below defines two separate network VLANs:</p>
      <codeblock>
- name: OSD-CLIENT-NET
  vlanid: 112
  tagged-vlan: true
  cidr: 192.168.187.0/24
  gateway-ip: 192.168.187.1
  network-group: OSD-CLIENT

- name: OSD-INTERNAL-NET
  vlanid: 116
  tagged-vlan: true
  cidr: 192.168.200.0/24
  gateway-ip: 192.168.200.1
  network-group: OSD-INTERNAL</codeblock>
    </section>
    <section id="ceph3_servergroups">
      <title>server_groups.yml</title>
      <p>Add the OSD network to the server groups in the
          <codeph>~/helion/my_cloud/definition/data/server_groups.yml</codeph> file, indicated by
        the bold portion below:</p>
      <codeblock>
- name: CLOUD
  server-groups:
   - AZ1
   - AZ2
   - AZ3
  networks:
   - EXTERNAL-API-NET
   - EXTERNAL-VM-NET
   - GUEST-NET
   - MANAGEMENT-NET
   <b>- OSD-CLIENT-NET</b>
   <b>- OSD-INTERNAL-NET</b></codeblock>
    </section>
    <section id="ceph3_firewallrules">
      <title>firewall_rules.yml</title>
      <p>Modify the firewall rules in the
          <codeph>~/helion/my_cloud/definition/data/firewall_rules.yml</codeph> file to allow OSD
        nodes to be pingable via the OSD network, indicated by the bold portion below:</p>
      <p>
        <note>Enabling ping for <codeph>OSD-CLIENT</codeph> and <codeph>OSD-INTERNAL</codeph> is
          optional. Enabling ping on these networks might make debugging connectivity issues on
          these networks easier.</note>
      </p>
      <codeblock>
- name: PING
  network-groups:
  - MANAGEMENT
  - GUEST
  - EXTERNAL-API
  <b>- OSD-CLIENT
  - OSD-INTERNAL</b>
  rules:
  # open ICMP echo request (ping)
  - type: allow
    remote-ip-prefix:  0.0.0.0/0
    # icmp type
    port-range-min: 8
    # icmp code
    port-range-max: 0
    protocol: icmp</codeblock>
    </section>
    <section id="ceph3_readme">
      <title>Edit the README.html and README.md Files</title>
      <p>You can edit the <codeph>~/helion/my_cloud/definition/README.html</codeph> and
          <codeph>~/helion/my_cloud/definition/README.md</codeph> files to reflect the OSD network
        group information if you wish. This change does not have any semantic implication and only
        assists with the readability of your model.</p>
    </section>
  </body>
</topic>
