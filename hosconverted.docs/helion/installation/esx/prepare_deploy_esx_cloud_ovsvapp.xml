<?xml version="1.0" encoding="UTF-8"?>
<!--Edit status: not edited-->
<!DOCTYPE topic PUBLIC "-//OASIS//DTD DITA Topic//EN" "topic.dtd">
<topic id="install_esx">
  <title><ph conkeyref="HOS-conrefs/product-title"/>Prepare and Deploy ESX Computes and OVSvAPPs </title>
  <body>
    <!--not tested-->
    <p>The following sections describe the procedure to install and configure ESX compute and
      OVSvAPPs on vCenter.</p>
    <section id="prepare_esx_cloud_deployment"><b>Preparation for ESX Cloud Deployment</b><p>This
        section describes the procedures to prepare and deploy the ESX computes and OVSvAPPs for
        deployment. </p><p>
        <ol id="ol_xpc_zqs_ft">
          <li>Login to the lifecycle manager.</li>
          <li>Source <codeph>service.osrc</codeph>.</li>
          <li><xref href="#install_esx/register-vcenter" format="dita">Register a vCenter
              Server</xref></li>
          <li><xref href="#install_esx/activate-cluster" format="dita">Activate Clusters</xref><ol
              id="ol_isb_wcc_vt">
              <li><xref href="#install_esx/modify-volume-config" format="dita">Modify the Volume
                  Configuration File</xref></li>
              <li><xref href="#install_esx/commit-your-cloud" format="dita">Commit your Cloud
                  Definition</xref></li>
              <li><xref href="#install_esx/deploy-compute-proxy-ovsvapps" format="dita">Deploy ESX
                  Compute Proxy and OVSvApps</xref></li>
            </ol></li>
        </ol>
      </p></section>
    <p><b>Manage vCenters and Clusters</b></p>
    <p>The following section describes the detailed procedure on managing the vCenters and
      clusters.</p>
    <p id="register-vcenter"><b>Register a vCenter Server</b><note>
        <ul id="ul_cdm_vqm_1y">
          <li>If ESX related roles are added in the input model after the controller cluster is up,
            you must run <codeph>hlm-reconfigure.yml</codeph> before adding the eon
            resource-manager.</li>
          <li>Make sure to provide single quotes (' ') or backslash (\) for the
            special characters (&amp; ! ; " ' () | \ >) that are used when setting a password for
            vCenter registration . For example: '2the!Moon' or 2the\!Moon. You can use either of the
            format to set the vCenter password.</li>
        </ul>
      </note></p>
    <p>vCenter provides centralized management of virtual host and virtual machines from a single
        console.<ol id="ol_xdj_5js_ft">
        <li>Add a vCenter using EON python
            client.<codeblock># eon resource-manager-add [--name &lt;RESOURCE_MANAGER_NAME>] --ip-address &lt;RESOURCE_MANAGER_IP_ADDR> --username &lt;RESOURCE_MANAGER_USERNAME> --password &lt;RESOURCE_MANAGER_PASSWORD> [--port &lt;RESOURCE_MANAGER_PORT>] --type &lt;RESOURCE_MANAGER_TYPE></codeblock><p>where:
              <ul id="ul_bp3_yjs_ft">
              <li>Resource Manager Name - the identical name of the vCenter server.</li>
              <li>Resource Manager IP address - the IP address of the vCenter server.</li>
              <li>Resource Manager Username - the admin privilege username for the vCenter.</li>
              <li>Resource Manager Password - the password for the above username.</li>
              <li>Resource Manager Port - the vCenter server port. By default it is 443.</li>
              <li> Resource Manager Type - specify <codeph>vcenter</codeph>.<note type="important"
                  >Please do not change the vCenter Port unless you are certain it is required to do
                  so.</note></li>
            </ul></p><p><b>Sample
            Output:</b><codeblock># eon resource-manager-add --name vc01 --ip-address 10.1.200.38 --username administrator@vsphere.local --password init123# --port 443 --type vcenter
+-------------+--------------------------------------+
| Property    | Value                                |
+-------------+--------------------------------------+
| ID          | BC9DED4E-1639-481D-B190-2B54A2BF5674 |
| IPv4Address | 10.1.200.38                          |
| Name        | vc01                                 |
| Password    | &lt;SANITIZED>                          |
| Port        | 443                                  |
| State       | registered                           |
| Type        | vcenter                              |
| Username    | administrator@vsphere.local          |
+-------------+--------------------------------------+</codeblock></p></li>
      </ol></p>
    <section><b>Show vCenter</b><ol id="ol_wh5_rsb_lt">
        <li>Show vCenter using EON python client.
              <codeblock># eon resource-manager-show &lt;RESOURCE_MANAGER_ID></codeblock><p><b>Sample
              Output:</b><codeblock>eon resource-manager-show BC9DED4E-1639-481D-B190-2B54A2BF5674
+-------------+--------------------------------------+
| Property    | Value                                |
+-------------+--------------------------------------+
| Clusters    | Cluster1, virtClust, Cluster2        |
| Datacenters | DC1, DC2                             |
| ID          | BC9DED4E-1639-481D-B190-2B54A2BF5674 |
| IPv4Address | 10.1.200.38                          |
| Name        | vc01                                 |
| Password    | &lt;SANITIZED>                          |
| Port        | 443                                  |
| State       | registered                           |
| Type        | vcenter                              |
| Username    | administrator@vsphere.local          |
+-------------+--------------------------------------+</codeblock></p></li>
      </ol></section>
    <section id="register-network"><b>Create and edit an activation template</b>
      <p>This involves getting a sample network information template and modifying the details of
        the template. You will use the template to register the cloud network configuration for the
        vCenter.</p><p>
        <ol>
          <li>Execute the following command to generate a sample activation
                template:<codeblock>eon get-activation-template [--filename &lt;ACTIVATION_JSON_NAME>] --type &lt;RESOURCE_TYPE></codeblock><p><b>Sample
                Output:</b></p><p>
              <codeblock>eon get-activation-template --filename activationtemplate.json --type esxcluster
---------------------------------------------------------------
Saved the sample network file in /home/user/activationtemplate.json
---------------------------------------------------------------</codeblock>
            </p></li>
          <li>Change to the <codeph>/home/user/</codeph>
            directory:<codeblock>cd /home/user/</codeblock></li>
          <li>Modify the template (json file) as per your environment. See <xref
              href="../install_entryscale_esx_kvm_vsa_template_json.dita#topic4797cgikdjm">Sample
              activationtemplate.json File for Helion Entry Scale ESX, KVM with VSA
            Model</xref>.</li>
        </ol>
      </p></section>
    <section id="activate-cluster"><b>Activate Clusters</b><p>This involves using the activation
        template to register the cloud network configuration for the vCenter.</p><p>This process
        spawns one compute proxy VM per cluster and one OVSvApp VM per host and configures the
        networking as defined in the JSON template. This process also updates the input model with
        the service VM details, executes the ansible playbooks on the new nodes, and performs a post
        activation check on the service VMs. </p><ol id="ol_tbr_4l4_vv">
        <li>Activate the cluster for the selected
            vCenter.<codeblock># eon resource-activate &lt;RESOURCE_ID> --config-json /home/user/&lt;ACTIVATION_JSON_NAME></codeblock><note>To
            retrieve the resource ID, execute <codeph>eon
            resource-list</codeph>.</note><note>Activating the first cluster in a datacenter
            requires you to provide an activation template JSON. You can provide the same activation
            template or a new activation template to activate the subsequent
            clusters.</note><note>Minimum disk space required for a cluster activation is (number of
            hosts in that cluster +1 )* 45 GB.</note></li>
        <li>Execute the following command to view the status of the
            cluster:<codeblock>eon resource-list
+--------------------------------------+-----------+-------------+--------------------------------------+------------+-------+------------+------------+
| ID                                   | Name      | Moid        | Resource Manager ID                  | IP Address | Port  | Type       | State      |
+--------------------------------------+-----------+-------------+--------------------------------------+------------+-------+------------+------------+
| 1228fce5-df5d-445c-834e-ae633ac7e426 | Cluster2  | domain-c184 | BC9DED4E-1639-481D-B190-2B54A2BF5674 | UNSET      | UNSET | esxcluster | imported   |
| 469710f6-e9f2-48a4-aace-1f00cbd60487 | virtClust | domain-c943 | BC9DED4E-1639-481D-B190-2B54A2BF5674 | UNSET      | UNSET | esxcluster | activated  |
| a3003a32-6e3a-4d89-a072-ec64a4247fb0 | Cluster1  | domain-c21  | BC9DED4E-1639-481D-B190-2B54A2BF5674 | UNSET      | UNSET | esxcluster | imported   |
+--------------------------------------+-----------+-------------+--------------------------------------+------------+-------+------------+------------+</codeblock><p>When
            the state is <codeph>activated</codeph>, the input model is updated with the service VM
            details, a git commit has been performed, the required playbooks and the post activation
            checks are completed successfully.</p><p>
            <note audience="INTERNAL">In multi-region setups, monitoring services will be
              deployed/running in a shared control-plane, unlike the normal deployment, where all
              services will be in one single control-plane. During ESX cluster activation in the
              multi-region scenarios, <codeph>generate_hosts_file</codeph> monitoring playbooks will
              be skipped due to absence of monitoring services in the ESX control-plane. To overcome
              this, you must run the playbook manually without
              limit.<codeblock audience="INTERNAL">cd ~/scratch/ansible/next/hos/ansible/ 
ansible-playbook -i hosts/verb_hosts site.yml --tag "generate_hosts_file"</codeblock></note>
            <note>Whenever an ESX compute is activated or deactivated, you must run the following
              playbook to reflect the changes in
              opsconsole.<codeblock>cd scratch/ansible/next/hos/ansible/
ansible-playbook -i hosts/verb_hosts ops-console-reconfigure.yml </codeblock></note>
          </p></li>
      </ol></section>
    <section id="modify-volume-config"><b>Modify the Volume Configuration File</b>
      <p>Once the cluster is activated you must configure the volume.</p><p>Perform the following
        steps to modify the volume configuration files:</p><ol id="ol_bhx_n5p_st">
        <li>Change the directory. The <codeph>cinder.conf.j2</codeph> is present in following
          directories
            :<codeblock>cd /home/stack/helion/hos/ansible/roles/_CND-CMN/templates</codeblock><p>OR<codeblock>cd /home/stack/helion/my_cloud/config/cinder</codeblock></p><p>It
            is recommended to modify the <codeph>cinder.conf.j2</codeph> available in
              <codeph>/home/stack/helion/my_cloud/config/cinder</codeph></p></li>
        <li>Modify the <codeph>cinder.conf.j2</codeph> as follows:
          <codeblock># Configure the enabled backends
enabled_backends=&lt;unique-section-name>

# Start of section for VMDK block storage
#
# If you have configured VMDK Block storage for cinder you must
# uncomment this section, and replace all strings in angle brackets
# with the correct values for vCenter you have configured. You
# must also add the section name to the list of values in the
# 'enabled_backends' variable above. You must provide unique section
# each time you configure a new backend.

#[&lt;unique-section-name>]
#vmware_api_retry_count = 10
#vmware_tmp_dir = /tmp
#vmware_image_transfer_timeout_secs = 7200
#vmware_task_poll_interval = 0.5
#vmware_max_objects_retrieval = 100
#vmware_volume_folder = cinder-volumes
#volume_driver = cinder.volume.drivers.vmware.vmdk.VMwareVcVmdkDriver
#vmware_host_ip = &lt;ip_address_of_vcenter>
#vmware_host_username = &lt;vcenter_username>
#vmware_host_password = &lt;password>
#
#volume_backend_name = &lt;vmdk-backend-name>
#
# End of section for VMDK block storage</codeblock></li>
      </ol></section>
    <section id="commit-your-cloud"><b>Commit your Cloud Definition</b>
      <p>
        <ol>
          <li> Add the cloud deployment definition to git
            :<codeblock>cd /home/stack/helion/hos/ansible;
git add -A;
git commit -m 'Adding ESX Configurations or other commit message';</codeblock></li>
          <li>Prepare your environment for deployment:
            <codeblock>ansible-playbook -i hosts/localhost config-processor-run.yml;
ansible-playbook -i hosts/localhost ready-deployment.yml;
cd /home/stack/scratch/ansible/next/hos/ansible;</codeblock></li>
        </ol>
      </p></section>
    <section id="deploy-compute-proxy-ovsvapps"><b>Configuring VMDK block storage</b><p>Execute the
        following command to configure VMDK block
        storage:<codeblock>ansible-playbook -i hosts/verb_hosts cinder-reconfigure.yml</codeblock></p><!--The variable <b>esx-ovsvapp</b> and <b>esx-compute</b> must be taken from the <b>name</b> key in the <codeph>resource-nodes</codeph> section in the <codeph>data/control_plane.yml</codeph> file (<codeph>/home/stack/helion/my_cloud/definition/data/control_plane.yml</codeph>) removed as per Satya's comment. --><p>
        If there are more backends (like VSA) defined, you must create and use specific volume type
        to ensure that volume is created in ESX, as shown
        below:<codeblock># cinder type-create "ESX VMDK Storage"
...
# cinder type-key "ESX VMDK Storage" set volume_backend_name=&lt;name of VMDK backend selected during installation>
...
# cinder create --volume-type "ESX VMDK Storage" 1
...</codeblock></p></section>
    <section>
      <title>Validate the block storage</title>
      <p>You can validate that the VMDK block storage is added to the cloud successfully using the
        following
        command:<codeblock># cinder service-list</codeblock><codeblock>+------------------+-------------------------+------+---------+-------+----------------------------+-----------------+
|      Binary      |           Host          | Zone |  Status | State |         Updated_at         | Disabled Reason |
+------------------+-------------------------+------+---------+-------+----------------------------+-----------------+
|  cinder-backup   |    ha-volume-manager    | nova | enabled |   up  | 2016-06-14T08:44:51.000000 |        -        |
| cinder-scheduler |    ha-volume-manager    | nova | enabled |  down | 2016-06-13T08:47:07.000000 |        -        |
| cinder-scheduler |    ha-volume-manager    | nova | enabled |   up  | 2016-06-14T08:44:57.000000 |        -        |
|  cinder-volume   |    ha-volume-manager    | nova | enabled |  down | 2016-06-13T07:36:41.000000 |        -        |
|  cinder-volume   | ha-volume-manager@vmdk1 | nova | enabled |   up  | 2016-06-14T08:44:55.000000 |        -        |
|  cinder-volume   |  ha-volume-manager@vsa1 | nova | enabled |   up  | 2016-06-14T08:44:50.000000 |        -        |
+------------------+-------------------------+------+---------+-------+----------------------------+-----------------+</codeblock></p>
    </section>
    <section id="verify">
      <title>Validate the compute</title>
      <p>You can validate that the ESX compute cluster is added to the cloud successfully using the
        following command:</p>
      <p>
        <codeblock>#  nova service-list</codeblock>
        <codeblock>+-----+------------------+------------------------------+----------+---------+-------+----------------------------+-----------------+
| Id  | Binary           | Host                         | Zone     | Status  | State | Updated_at                 | Disabled Reason |
+-----+------------------+------------------------------+----------+---------+-------+----------------------------+-----------------+
| 3   | nova-conductor   | esxhos-joh-core-m1-mgmt      | internal | enabled | up    | 2016-03-30T12:57:47.000000 | -               |
| 63  | nova-scheduler   | esxhos-joh-core-m1-mgmt      | internal | enabled | up    | 2016-03-30T12:57:43.000000 | -               |
| 66  | nova-conductor   | esxhos-joh-core-m2-mgmt      | internal | enabled | up    | 2016-03-30T12:57:48.000000 | -               |
| 111 | nova-conductor   | esxhos-joh-core-m3-mgmt      | internal | enabled | up    | 2016-03-30T12:57:41.000000 | -               |
| 129 | nova-scheduler   | esxhos-joh-core-m3-mgmt      | internal | enabled | up    | 2016-03-30T12:57:41.000000 | -               |
| 132 | nova-consoleauth | esxhos-joh-core-m1-mgmt      | internal | enabled | up    | 2016-03-30T12:57:44.000000 | -               |
| 135 | nova-scheduler   | esxhos-joh-core-m2-mgmt      | internal | enabled | up    | 2016-03-30T12:57:47.000000 | -               |
| 138 | nova-compute     | esxhos-joh-esx-comp0001-mgmt | nova     | enabled | up    | 2016-03-30T12:57:41.000000 | -               |
+-----+------------------+------------------------------+----------+---------+-------+----------------------------+-----------------+</codeblock>
      </p>
    </section>
    <p>You can validate the hypervisor list using the following
      command:<codeblock># nova hypervisor-list</codeblock><codeblock>+----+-------------------------------------------------+-------+----------+
| ID | Hypervisor hostname                             | State | Status   |
+----+-------------------------------------------------+-------+----------+
| 9  | domain-c40.9FDCFA66-6677-42A1-83FF-16DC32448021 | up    | enabled  |
+----+-------------------------------------------------+-------+----------+</codeblock></p>
    <section>
      <title>Validate the neutron</title>
      <p>You can validate that the networking of the ESX cluster using the following
        command:<codeblock># neutron agent-list</codeblock><codeblock>+--------------------------------------+--------------------+---------------------------------+-------+----------------+---------------------------+
| id                                   | agent_type         | host                            | alive | admin_state_up | binary                    |
+--------------------------------------+--------------------+---------------------------------+-------+----------------+---------------------------+
| 097bdbc3-108c-41ca-8b52-9d249f65077f | Metadata agent     | esxhos-joh-core-m3-mgmt         | :-)   | True           | neutron-metadata-agent    |
| 2b255256-9505-489e-93bf-0d37f7ff83e4 | DHCP agent         | esxhos-joh-core-m3-mgmt         | :-)   | True           | neutron-dhcp-agent        |
| 43843bcb-f929-4a42-a1e1-207ff97dc09e | OVSvApp Agent      | esxhos-joh-esx-ovsvapp0002-mgmt | :-)   | True           | ovsvapp-agent             |
| 4f0be657-fe9e-4bab-bef8-d8e688b6573e | L3 agent           | esxhos-joh-core-m3-mgmt         | :-)   | True           | neutron-vpn-agent         |
| 6db7d585-604e-4205-b29c-b9684bfb0cb2 | OVSvApp Agent      | esxhos-joh-esx-ovsvapp0001-mgmt | :-)   | True           | ovsvapp-agent             |
| 700f1a18-d237-4605-bf54-145951bd12db | DHCP agent         | esxhos-joh-core-m1-mgmt         | :-)   | True           | neutron-dhcp-agent        |
| a723fb23-2c87-4f69-b272-643df870f0b6 | DHCP agent         | esxhos-joh-core-m2-mgmt         | :-)   | True           | neutron-dhcp-agent        |
| bd512ed5-9f39-4ad6-a505-148f58cf2e64 | L3 agent           | esxhos-joh-core-m1-mgmt         | :-)   | True           | neutron-vpn-agent         |
| e3d4b8be-077e-4503-bed2-031871f3829e | Open vSwitch agent | esxhos-joh-core-m2-mgmt         | :-)   | True           | neutron-openvswitch-agent |
| e5d75917-1d26-4443-99f4-45d8c574e3f0 | Open vSwitch agent | esxhos-joh-core-m1-mgmt         | :-)   | True           | neutron-openvswitch-agent |
| e6a40540-adad-4ee7-b194-db18ac7287bd | Metadata agent     | esxhos-joh-core-m1-mgmt         | :-)   | True           | neutron-metadata-agent    |
| e734a20e-159c-450b-9fb8-dc46824ef12e | Metadata agent     | esxhos-joh-core-m2-mgmt         | :-)   | True           | neutron-metadata-agent    |
| f019c268-a6bd-4f66-8a46-6762c720a5bd | L3 agent           | esxhos-joh-core-m2-mgmt         | :-)   | True           | neutron-vpn-agent         |
| fc83c8dd-ead4-4ad5-aed3-95ab41e567bc | Open vSwitch agent | esxhos-joh-core-m3-mgmt         | :-)   | True           | neutron-openvswitch-agent |
+--------------------------------------+--------------------+---------------------------------+-------+----------------+---------------------------+</codeblock></p>
    </section>
  </body>
</topic>

