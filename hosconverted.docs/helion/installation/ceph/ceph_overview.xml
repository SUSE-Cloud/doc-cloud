<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE topic PUBLIC "-//OASIS//DTD DITA Topic//EN" "topic.dtd">
<topic id="ceph_overview">
    <title><ph conkeyref="HOS-conrefs/product-title"/>Ceph Overview</title>
    <abstract>
        <shortdesc outputclass="hdphidden"><keyword keyref="kw-hos-tm"/>
            <keyword keyref="kw-hos-version"/> supports the Hammer version of Ceph cluster. Ceph
            cluster is a distributed object storage solution that can scale horizontally up to
            multiple petabytes and is based on the Reliable Autonomic Distributed Object Store
            (RADOS) object-based storage system. </shortdesc>
    </abstract>
    <body>
        <p conkeyref="HOS-conrefs/applies-to"/>
        <section>
            <title>Overview</title>
            <p><keyword keyref="kw-hos-tm"/>
                <keyword keyref="kw-hos-version"/> supports the Hammer version of Ceph cluster. Ceph
                cluster is a distributed object storage solution that can scale horizontally up to
                multiple petabytes and is based on the Reliable Autonomic Distributed Object Store
                (RADOS) object-based storage system. Ceph cluster stores all data as objects. It
                also provides components that support other storage protocols. For example: RBD (
                RADOS Block Devices) supports block storage protocol, RADOS Gateway support S3/Swift
                API protocol for object storage access and so on. For example, RBD (RADOS Block
                Devices) supports the block storage protocol, RADOS Gateway supports the S3/Swift
                API protocol for object storage access, and so on. The following are Cephâ€™s key
                features :</p>
            <p>
                <ol id="ol_gvj_3fn_fw">
                    <li>Uses of any commodity hardware.
                        <!--(any brand, even assembled pc also)--></li>
                    <li> Scales horizontally upto a petabytes (there is no theoretical limit). </li>
                    <li>Self healing, self managing.</li>
                    <li> No single point of failure. </li>
                    <li>Supports various client protocol for block device access, object storage
                        access using the S3/Swift API and more.</li>
                </ol>
            </p>
        </section>
        <p>Ceph deployment is flexible. Although it supports a variety of storage protocols based on
            deployed components, you can add extra components (except mandatory ones) <i>only</i> if
            you need to support specific storage protocols. For example, if you are not going to
            support the S3/Swift protocol, then you might choose not to deploy the RADOS Gateway.
            For easy readability, the various aspects of cluster deployment, such as hardware
            configuration, service configuration parameters, and deployment architecture, are
            categorized into separate sections. Each major section is further categorized into
            following segments:</p>
        <p>
            <ul id="ul_enh_4sp_kw">
                <li>Core Ceph</li>
                <li>RADOS Gateway</li>
            </ul>
        </p>
        <p>Each sub-section provides recommendations vs. supported configurations. We recommend the
            production configuration, although all supported configurations are tested and
            validated. Use alternative supported configurations only if you have a strong need for
            them after evaluating their pros and cons.</p>
        <p><b>Core Ceph</b></p>
        <p>Core Ceph has two primary components, Object Storage Daemon (OSD) and Monitor, which are
            mandatory for cluster function. The following table briefly describes these
            components.</p>
        <p>
            <table frame="all" rowsep="1" colsep="1" id="ceph1">
                <tgroup cols="2">
                    <colspec colname="c1" colnum="1" colwidth="1*"/>
                    <colspec colname="c2" colnum="2" colwidth="3.4*"/>
                    <thead>
                        <row>
                            <entry>Components</entry>
                            <entry>Description</entry>
                        </row>
                    </thead>
                    <tbody>
                        <row>
                            <entry><b>OSD</b></entry>
                            <entry>A Ceph OSD Daemon (OSD) stores data, handles data replication,
                                recovery, backfilling, and rebalancing, and provides monitoring
                                information to Ceph Monitors by checking other Ceph daemons for a
                                <!--heartbeat-->
                                    activity.<p><!--The default <keyword keyref="kw-hos-tm"/>configuration makes three copies of your data (but it can be adjusted).--></p></entry>
                        </row>
                        <row>
                            <entry><b>Monitor</b></entry>
                            <entry>Ceph Monitor maintains maps of the cluster state including the
                                monitor map, the OSD map, the placement group (PG) map, and the
                                CRUSH map. It also maintains a history (called an "epoch") of each
                                state change in the Ceph Monitors, Ceph OSD Daemons, and
                                PGs.</entry>
                        </row>
                    </tbody>
                </tgroup>
            </table>
        </p>

        <p><b>RADOS Gateway</b></p>
        <p>The RADOS Gateway service is an object storage interface that enables user to perform
            HTTP-based CRUD operations on an object. It supports both the OpenStack Swift and Amazon
            S3 REST APIs. It has the following two types of users, unlike the rest of OpenStack
            services, which rely completely on Keystone for user management (see more details at
                <xref href="usage_ceph_storage_helion.dita#ceph_storage_usage/rados-gw-object-storage">Use
                RADOS Gateway to access objects using S3/Swift API</xref> ).<ol id="ol_kyr_y41_nw">
                <li>Keystone</li>
                <li>RADOS Gateway user (managed by Ceph itself and does not require Keystone)</li>
            </ol></p>
        <p>RADOS Gateway is an optional component. Deploy it only if you need to access objects
            storage functionality using Swift or S3 API. Features include the following:<ol
                id="ol_adn_ysp_kw">
                <li>RADOS Gateway is configured to run in a simple (non-federated or single region)
                    mode. </li>
                <li>The HAProxy on the <keyword keyref="kw-hos-tm"/> controller node acts as a load
                    balancer for RADOS Gateway servers (in least connection mode, the load balancer
                    selects the server with the least number of connections) for RADOS Gateway
                    servers. </li>
                <li>Provides OpenStack Keystone integration (users having a configured set of roles
                    can access Swift APIs served by <codeph>radosgw</codeph>). </li>
                <li>The default <keyword keyref="kw-hos-tm"/> configuration installs RADOS Gateway
                    on standalone nodes. </li>
                <li>Access to Amazon S3 APIs is limited to RADOS Gateway users. </li>
                <li>The RADOS Gateway external and internal endpoints are SSL/TLS- enabled,
                    including the public end points represented by HAProxy.</li>
            </ol></p>
        <section>
            <title>Deployment Architecture</title>
            <p>Consider the following issues when deploying Ceph:<ul id="ul_e4v_2tp_kw">
                    <li>Ceph networking </li>
                    <li>Placement of service components (such as OSD, monitor, RADOS Gateway) across
                        nodes. For example, RADOS Gateway and monitor can be deployed on standalone
                        nodes or together.</li>
                </ul></p>
        </section>
        <section id="ceph-networking"><b>Ceph Networking</b><p>Ceph clients transmits traffic
                directly to OSD daemons for storage operations instead of the client routing
                requests to a specific gateways. OSD daemons perform data replication and
                participate in recovery activities. In general, a storage pool is configured with a
                replica count of 3, causing daemons to transact three sets of client data over the
                cluster network. As a result, 4 MB of write traffic results in a total of 12 MB of
                data movement in the Ceph clusters (4 MB * 3 replicas = 12MB). Also, Ceph clusters
                routinely share data among themselves. It is important to segregate Ceph data
                traffic into three segments, as follows:</p><p>
                <ul id="ul_iqq_shn_fw">
                    <li>Management traffic, which includes monitoring and logging. </li>
                    <li>Client traffic (often termed as data traffic) includes client requests sent
                        to OSD daemons. </li>
                    <li>Cluster traffic (often termed as replication traffic), which includes
                        replication and recovery data traffic among OSD daemons.</li>
                </ul>
            </p><p>For a high-performance clusters, a proper network configuration is very
                important. Use multiple networks for different data traffics. For a cluster of a
                reasonable size (a few terabytes), we recommend having a cluster with at least two
                networks, such as a single network for management and client data traffic
                (front-side) and a cluster (back-side) network. For large Ceph clusters, we
                recommend segregating all three traffic configurations. Segregation enables helps
                make for a secure connection because a cluster network is not required to be
                connected to the Internet directly. It allows OSD daemons to keep communicating
                without intervention so that placement groups can be brought to active and clean
                states relatively easily, whenever required. Apart from network sepration(using
                VLANs), be sure to consider NICs used for Ceph servers too. We strongly recommend
                using bonded NICs to prevent single points of failure. Note that the network (and
                hence VLAN) separation is different from NIC separation even though they are linked
                to each other. In a multi-network model, emphasize VLAN separation, which ideally
                should be complemented by NIC separation, using the following priority
                    order:</p><p><ol id="ol_dwc_rbl_jw">
                    <li>Separation of VLAN</li>
                    <li>Separation of NIC for VLANs.</li>
                </ol> The number of NICs you have will depend on your number of bonded interfaces,
                as follows:</p><p>
                <ol id="ol_ssd_tbl_jw">
                    <li>Three bonded interfaces with six NICs: first for management, second for OSD
                        client, and third for OSD internal (preferred setup).</li>
                    <li> Two bonded interfaces with four NICs: management VLANs hooked to the first
                        bonded interface, and OSD networks hooked to the second bonded interface
                        (most-used setup).</li>
                    <li>Only one bonded interface: two NICs for all VLANs (for a few NICs
                        only).</li>
                </ol>
            </p><p><keyword keyref="kw-hos-tm"/> Ceph software offers significant flexibility when
                defining and deploying OpenStack-based clouds, letting you implement a wide variety
                of different Ceph configurations. This allows you to design, model, and deploy cloud
                based on your requirements. You have the following VLAN choices based on traffic
                types:</p><p>
                <ol id="ol_m3x_vhn_fw">
                    <li>Single: A single VLAN is used for all traffic, meant primarily for a small
                        cluster. </li>
                    <li>Double deployment: One VLAN is used for cloud management and client traffic
                        and another VLAN is used for Ceph internal traffic.</li>
                    <li>Triple deployment: Separate VLANs are used for management, client, and
                        internal traffic.<p>As mentioned previously, you can link all VLANs to the
                            same bonded interface, to a separate bonded interface, or a combination
                            of interfaces. For a large cluster, you can use a separate bonded
                            interface which in turn necessiates having at least six NICs for OSD
                            nodes and four NICs for monitor nodes.</p></li>
                </ol>
            </p></section>
        <section>
            <p><b>Placement of service component</b></p>
            <p>Although you can place service components in multiple ways, this section focuses on
                recommended deployments only, covering Core Ceph (OSD and monitors) and the RADOS
                Gateway. For details on alternative supported deployment architecture, please refer
                to <xref href="alternative_supported_choice.dita#config_ceph"
                    >Alternative supported choices.</xref>
                <ul id="ul_qll_xbd_jw">
                    <li>Core Ceph (i.e. OSD and monitors)</li>
                    <li>Rados Gateway </li>
                </ul></p>
            <p>
                <ul id="ul_tyr_tnq_kw">
                    <li><b>Core Ceph</b><p>We recommend the following deployment composition is
                            recommended to avoid single points of failure for the Ceph cluster
                                deployment.<ol id="ol_ecm_qtq_kw">
                                <li>Three monitor nodes, to retain the odd number criteria of the
                                    monitor quorum. </li>
                                <li>At least three OSD nodes, to ensure that the object is
                                    replicated on three separate physical nodes, if the replica
                                    count of the pool is set to three (the recommended storage pool
                                    configuration is to set the replica count to three). </li>
                            </ol></p><p>As mentioned in <xref href="#ceph_overview/ceph-networking"
                                format="dita">Ceph networking</xref>, we recommend using separate
                            VLANS to separate Ceph traffic. The cloud management network is used for
                            logging and/or monitoring, the OSD client network is used for Ceph
                            client traffic, and the OSD internal network is used for internal Ceph
                            traffic, such as replication.</p></li>
                    <li><p><b>RADOS Gateway</b></p><p>We recommend deploying at least two instances
                            of RADOS Gateway on a standalone node front- ended by HAProxy. </p> The
                        following diagram illustrates the physical architecture of the RADOS Gateway
                        and using the <codeph>entry-scale-kvm-ceph</codeph> configuration. <p><image
                                href="../../../media/ceph/ceph/physical-architecture_ceph.png"
                                id="image_m25_12x_lw"/></p><p>The default example model
                                (<codeph>entry-scale-kvm-ceph</codeph>) is the recommended mechanism
                            to deploy the Ceph cluster with the RADOS Gateway. </p></li>
                </ul>
            </p>
        </section>
        <section><b>Ceph Deployment Architecture </b><p>The following diagram illustrates the Ceph
                deployment architecture.</p><p>
                <image href="../../../media/ceph/ceph/ceph_rgw_architecture.png"
                    id="image_t5j_lzp_kw"/></p><p>The preceding diagram illustrates the Ceph
                deployment scenario of <codeph>entry-scale-kvm-ceph input model</codeph>. Monitors
                are deployed on three controller nodes. Three standalone nodes are used for OSD and
                two standalone nodes are used for the RADOS Gateway, front-ended by HAProxy running
                on controller nodes. Management, client, and internal data traffic is separated
                using independent VLANs.</p></section>
        <section><b>Alternative supported architecture</b><p><keyword keyref="kw-hos-tm"/>
                <keyword keyref="kw-hos-version"/> also supports an alternate deployment
                architecture, for which the following points are relevant.<ul id="ul_asz_bqq_kw">
                    <li>Core networking</li>
                    <li>Placement of service components<ul id="ul_hst_dqq_kw">
                            <li>Core Ceph<ul id="ul_p13_2qq_kw">
                                    <li>Monitor on standalone node</li>
                                </ul></li>
                        </ul><ul id="ul_dym_lst_fw">
                            <li>RadosGateway <ul id="ul_tjm_nst_fw">
                                    <li>Co-hosted on cluster nodes hosting monitor service
                                        components </li>
                                    <li>Co-hosted on controller nodes</li>
                                </ul></li>
                        </ul></li>
                </ul></p></section>
        <section><b>Core networking</b><p>Ceph clients transmit traffic directly to OSD daemons for
                storage operations instead of the client routing requests to a specific gateway. For
                more information, refer to <xref href="#ceph_overview/ceph-networking" format="dita"
                    >Ceph networking</xref>.</p></section>
        <section><b>Placement of service components</b><p>
                <ul id="ul_edw_xql_hw">
                    <li><b>Core Ceph</b><p>The architecture choices which is supported in <keyword
                                keyref="kw-hos-tm"/>
                            <keyword keyref="kw-hos-version"/> is as follows.<ul id="ul_fnt_wtq_kw">
                                <li>Monitor on standalone node<p>You can deploy the Ceph monitor
                                        service on a one or more dedicated clusters or resource
                                        nodes. Ensure that you modify your environment after
                                        installing the lifecycle manager. For more details, refer to
                                            <xref
                                            href="alternative_supported_choice.dita#config_ceph/deploying-monitor-on-standalone-node"
                                            >Install a monitor service on a dedicated resource
                                            node</xref>. </p></li>
                            </ul></p></li>
                </ul>
            </p><p>
                <ul id="ul_r3n_nrq_kw">
                    <li><b>RADOS Gateway</b><p>RADOS Gateway service can be co-hosted with other
                                <keyword keyref="kw-hos-tm"/> services as follows.</p><p>
                            <ul id="ul_e2l_4xl_fw">
                                <li>Alternate RADOS Gateway deployment architecture choice <ul
                                        id="ul_jjw_bsq_kw">
                                        <li>
                                            <p>Co-hosted on cluster nodes hosting monitor service
                                                components </p>
                                        </li>
                                        <li>
                                            <p>Co-hosted on controller nodes</p>
                                        </li>
                                    </ul><p>The default <codeph>entry-scale-kvm-ceph</codeph> input
                                        model deploys the <codeph>radosgw</codeph> services on two
                                        dedicated cluster nodes. However, you can also install the
                                        RADOS Gateway on a cluster node hosting the Ceph monitor
                                        service or on controller nodes. Refer to <xref
                                            href="alternative_supported_choice.dita#config_ceph/install-rados-gateway-on-cluster-node-that-host-ceph-monitor"
                                            >Installing RADOS Gateway on (dedicated) cluster node(s)
                                            that host Ceph Monitor service</xref> or <xref
                                            href="alternative_supported_choice.dita#config_ceph/install-rados-gateway-on-controller-nodes"
                                            >Installing RADOS Gateway on controller nodes</xref> for
                                        more details.</p><p>
                                        <note>Because the RADOS Gateway service shares server
                                            resources with multiple services, these alternate
                                            configurations will result in sub-optimal performance,
                                            as compared to the default configuration.</note>
                                    </p></li>
                            </ul>
                        </p></li>
                </ul>
            </p></section>
        <section>
            <title>Hardware recommendations</title>
            <p>For hardware recommendations, refer to <xref
                    href="../../planning/rec_min_entryscale_kvm_ceph.dita#rec_min_entryscale_kvm_ceph"
                />.</p>
        </section>
    </body>
</topic>
