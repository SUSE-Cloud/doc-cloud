<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE topic PUBLIC "-//OASIS//DTD DITA Topic//EN" "topic.dtd">
<topic id="config_ceph">
    <title><ph conkeyref="HOS-conrefs/product-title"/>Alternative Supported Choices</title>
    <abstract>
        <shortdesc outputclass="hdphidden">This section provides insight on how to alter the
                <codeph>entry-scale-kvm-ceph</codeph> input model to deploy Ceph with various
            supported options. We recommend that you deploye your supported choice only after
            evaluating all pros and cons.</shortdesc>
    </abstract>
    <body>
        <!--not tested-->
        <p conkeyref="HOS-conrefs/applies-to"/>
        <p>This section provides insight on how to alter the <codeph>entry-scale-kvm-ceph</codeph>
            input model to deploy Ceph with various supported options. We recommend that you deploye
            your supported choice only after evaluating all pros and cons. For technical details,
            please consult with the technical support team. The choices available can impact the
            performance and scaling of clusters. Choices are illustrated for reference purposes and
            you can combine one or more of them as needed. The content is categorized as
            follows:</p>
        <p> </p>
        <p>
            <ol id="ol_sqb_mrg_kw">
                <li>Core Ceph<ul id="ul_ejq_yrg_kw">
                        <li><xref href="#config_ceph/deploying-monitor-on-standalone-node"
                                format="dita">Installing the Monitor Service on Standalone
                                Nodes</xref>
                        </li>
                        <li><xref href="#config_ceph/single-vlan-for-all-ceph-traffic" format="dita"
                                >Using a Single VLAN for All Ceph Traffic (Management, Client, and
                                Internal OSD)</xref></li>
                        <li><xref href="#config_ceph/using-two-vlan" format="dita">Using Two VLANs:
                                For Management and Client Traffic and for Internal OSD`
                                Traffic</xref>
                        </li>
                    </ul>
                </li>
                <li>RADOS Gateway<ul id="ul_ptr_frt_mx">
                        <li><xref
                                href="#config_ceph/install-rados-gateway-on-cluster-node-that-host-ceph-monitor"
                                format="dita">Installing RADOS Gateway on Dedicated Cluster Nodes
                                that Host the Ceph Monitor Service</xref>
                        </li>
                        <li><xref href="#config_ceph/install-rados-gateway-on-controller-nodes"
                                format="dita">Installing RADOS Gateway on Controller
                            Nodes</xref></li>
                        <li><xref href="#config_ceph/install-more-two-rados-gateway-servers"
                                format="dita">Installing More than Two RADOS Gateway
                            Servers</xref></li>
                    </ul></li>
                <li audience="INTERNAL"><xref href="#config_ceph/ceph-deployment-vcp" format="dita">Ceph Deployment with
                        Virtual Control Plane </xref></li>
            </ol>
        </p>
        <section>
            <title>Core Ceph</title>
        </section>
        <section id="deploying-monitor-on-standalone-node"><b>Installing the Monitor Service on
                Standalone Nodes</b>
            <p>The following section provides the procedure for installing the monitor service on
                standalone nodes instead of installing on controller nodes, as mentioned in
                    <codeph>entry-scale-kvm-ceph</codeph>.</p><p>
                <note type="attention">If you want to install the monitor service as a dedicated
                    resource node, you must decide before deploying Ceph. <keyword
                        keyref="kw-hos-phrase"/> does not support deployment transition. After Ceph
                    is deployed, you cannot migrate the monitor service from controller nodes to
                    dedicated resource nodes. </note>
            </p><p><b>Prerequisite</b></p><p>Perform the following steps to install the Ceph monitor
                on a dedicated node. Note that the Ceph requires at least three monitoring servers
                to form a cluster in case of a node failure.<ol id="ol_sys_n3l_lw">
                    <li>Log in to the lifecycle manager.</li>
                    <li>Copy the <codeph>entry-scale-kvm-ceph</codeph> input model to the
                            <codeph>~/helion/my_cloud/definition</codeph> directory before you begin
                        the editing
                        process:<codeblock>cp -r ~/helion/examples/entry-scale-kvm-ceph/* ~/helion/my_cloud/definition/</codeblock></li>
                    <li>Edit the <codeph>control_plane.yml</codeph> to create a new cluster, such as
                        with <codeph>ceph-mon</codeph>, as shown
                        here.<codeblock>clusters:
  - name: cluster1
    cluster-prefix: c1
    server-role: CONTROLLER-ROLE
    member-count: 3
    allocation-policy: strict
    service-components:
      - lifecycle-manager
      - ntp-server
      ...
                                    
  <b>- name: ceph-mon
    cluster-prefix: ceph-mon
    server-role: CEP-MON-ROLE
    min-count: 3
    allocation-policy: strict
    service-components:
      - ntp-client
      - ceph-monitor</b>
    
  - name: rgw
    cluster-prefix: rgw
    server-role: RGW-ROLE
    ...</codeblock></li>
                    <li>Edit the <codeph>~/helion/my_cloud/definition/data/servers.yml</codeph> file
                        to define the Ceph monitor node (monitor services). The following example
                        shows three nodes for monitor services. We recommend using an odd number of
                        monitor
                        nodes.<codeblock># Ceph Monitor Nodes
- id: ceph-mon1
  ip-addr: 10.13.111.141
  server-group: RACK1
  role: CEP-MON-ROLE
  nic-mapping: MY-4PORT-SERVER
  mac-addr: "f0:92:1c:05:69:10"
  ilo-ip: 10.12.8.217
  ilo-password: password
  ilo-user: admin

- id: ceph-mon2
  ip-addr: 10.13.111.142
  server-group: RACK2
  role: CEP-MON-ROLE
  nic-mapping: MY-4PORT-SERVER
  mac-addr: "83:92:1c:55:69:b0"
  ilo-ip: 10.12.8.218
  ilo-password: password
  ilo-user: admin

- id: ceph-mon3
  ip-addr: 10.13.111.143
  server-group: RACK3
  role: CEP-MON-ROLE
  nic-mapping: MY-4PORT-SERVER
  mac-addr: "d9:92:1c:25:69:e0"
  ilo-ip: 10.12.8.219
  ilo-password: password
  ilo-user: admin

# Ceph RGW Nodes
- id: rgw1
  ...</codeblock></li>
                    <li>Edit the
                            <codeph>~/helion/my_cloud/definition/data/net_interfaces.yml</codeph>
                        file to define a new network interface set for your Ceph monitors, as shown here.<p>
                            <codeblock>## This defines the interface used for management
## traffic such as logging, monitoring, etc.
- name: CEP-MON-INTERFACES
  network-interfaces:
    - name: BOND0
      device:
          name: bond0
      bond-data:
          options:
              mode: active-backup
              miimon: 200
              primary: hed1
          provider: linux
          devices:
            - name: hed1
            - name: hed2
      network-groups:
        - MANAGEMENT

- name: RGW-INTERFACES
  network-interfaces:
  ...</codeblock>
                        </p></li>
                    <li>Edit
                            <codeph>~/helion/my_cloud/definition/data/disks_ceph_monitor.yml</codeph>
                        to define the disk model for monitor nodes.
                        <codeblock>disk-models:
- name: CEP-MON-DISKS
  # Disk model to be used for Ceph monitor nodes
  # /dev/sda_root is used as a volume group for /, /var/log and /var/crash
  # sda_root is a templated value to align with whatever partition is really used
  # This value is checked in os config and replaced by the partition actually used
  # on sda e.g. sda1 or sda5
                            
  volume-groups:
    - name: hlm-vg
      physical-volumes:
        - /dev/sda_root
                            
      logical-volumes:
      # The policy is not to consume 100% of the space of each volume group.
      # 5% should be left free for snapshots and to allow for some flexibility.
        - name: root
          size: 30%
          fstype: ext4
          mount: /
        - name: log
          size: 45%
          mount: /var/log
          fstype: ext4
          mkfs-opts: -O large_file
        - name: crash
          size: 20%
          mount: /var/crash
          fstype: ext4
          mkfs-opts: -O large_file
      consumer:
         name: os</codeblock></li>
                    <li>Edit the <codeph>~/helion/my_cloud/definition/data/server_roles.yml</codeph>
                        file to define a new server role for your Ceph monitors:
                        <codeblock>- name: CEP-MON-ROLE
  interface-model: CEP-MON-INTERFACES
  disk-model: CEP-MON-DISKS</codeblock></li>
                    <li>Commit your
                        configuration:<codeblock>cd ~/helion/hos/ansible
git add -A
git commit -m "&lt;commit message>"</codeblock></li>
                    <li>Run the following playbook to add your nodes into Cobbler:
                        <codeblock>cd ~/helion/hos/ansible/
ansible-playbook -i hosts/localhost cobbler-deploy.yml</codeblock></li>
                    <li>To reimage all the nodes using PXE, run the following playbook:
                        <codeblock>cd ~/helion/hos/ansible/
ansible-playbook -i hosts/localhost bm-reimage.yml</codeblock></li>
                    <li>Run the configuration processor:
                        <codeblock>cd ~/helion/hos/ansible/
ansible-playbook -i hosts/localhost config-processor-run.yml</codeblock></li>
                    <li>Update your deployment directory with this playbook:
                        <codeblock>cd ~/helion/hos/ansible/
ansible-playbook -i hosts/localhost ready-deployment.yml</codeblock></li>
                    <li>Deploy these changes:
                        <codeblock>cd ~/scratch/ansible/next/hos/ansible
ansible-playbook -i hosts/verb_hosts site.yml</codeblock></li>
                </ol></p></section>
        <section id="single-vlan-for-all-ceph-traffic">
            <b>Using a Single VLAN for All Ceph Traffic (Management, Client, and Internal OSD)</b>
            <p>You can use a single VLAN to transmit all Ceph traffic. This configuration is
                recommended for a small cluster deployment.</p>
            <!--<p><b>Architecture diagram</b></p> <p>&lt;need diagram></p>-->
            <p><!--<b>Procedure to configure Entry-scale-kvm-ceph-single-network</b>--></p><p>Perform
                the following steps to to configure
                    <codeph>Entry-scale-kvm-ceph-single-network</codeph>. </p><p>
                <ol id="ol_d5k_4ss_lw">
                    <li> Log in to the lifecycle manager.</li>
                    <li>Copy the <codeph>entry-scale-kvm-ceph</codeph> input model to the
                            <codeph>~/helion/my_cloud/definition</codeph> directory before you begin
                        the editing
                        process:<codeblock>cp -r ~/helion/examples/entry-scale-kvm-ceph/* ~/helion/my_cloud/definition/</codeblock></li>
                    <li>Validate that NIC interfaces are correctly specified in
                            <codeph>nic_mapping.yml</codeph> for servers that are used in the
                        cloud.</li>
                    <li>Ensure that you have at least two NICs for Ceph nodes to create a bonded
                        interface for it.</li>
                    <li>Validate that your servers are mapped to a correct NIC interface
                        specification in <codeph>servers.yml</codeph>.
                        <!--A server meeting the criteria of at least two NICs is good enough for this input model.-->
                        The following is an example of a server node used for OSD
                        deployment:<codeblock># Ceph OSD Nodes
    - id: osd1
      ip-addr: 192.168.10.9
      role: OSD-ROLE
      server-group: RACK1
      nic-mapping: MY-2PORT-SERVER
      mac-addr: "8b:f6:9e:ca:3b:78"
      ilo-ip: 192.168.9.9
      ilo-password: password
      ilo-user: admin</codeblock></li>
                    <li>Delete the OSD-INTERNAL and OSD-CLIENT network groups from
                            <codeph>network_groups.yml</codeph>. This is necessary because only the
                        management network is used for Ceph traffic, thus OSD-INTERNAL and
                        OSD-CLIENT network groups are not required. </li>
                    <li>Define <codeph>net_interfaces.yml</codeph> to use only management network
                        groups, as shown
                        here.<codeblock>  - name: CONTROLLER-INTERFACES
      network-interfaces:
        - name: BOND0
          device:
              name: bond0
          bond-data:
              options:
                  mode: active-backup
                  miimon: 200
                  primary: hed3
              provider: linux
              devices:
                - name: hed3
                - name: hed4
          network-groups:
            - EXTERNAL-API
            - EXTERNAL-VM
            - GUEST
            - MANAGEMENT

    - name: COMPUTE-INTERFACES
      network-interfaces:
        - name: BOND0
          device:
              name: bond0
          bond-data:
              options:
                  mode: active-backup
                  miimon: 200
                  primary: hed3
              provider: linux
              devices:
                - name: hed3
                - name: hed4
          network-groups:
            - EXTERNAL-VM
            - GUEST
            - MANAGEMENT 
                                    
    - name: OSD-INTERFACES
      network-interfaces:
        - name: BOND0
          device:
              name: bond0
          bond-data:
              options:
                  mode: active-backup
                  miimon: 200
                  primary: hed3
              provider: linux
              devices:
                - name: hed3
                - name: hed4
          network-groups:
            - MANAGEMENT

    - name: RGW-INTERFACES
      network-interfaces:
        - name: BOND0
          device:
              name: bond0
          bond-data:
              options:
                  mode: active-backup
                  miimon: 200
                  primary: hed3
              provider: linux
              devices:
                - name: hed3
                - name: hed4
          network-groups:
            - MANAGEMENT</codeblock></li>
                    <li>Delete VLAN information for OSD-INTERNAL-NET and OSD-CLIENT-NET from
                            <codeph>networks.yml</codeph>. Only Management VLANs are used.</li>
                    <li>After you set up your configuration files, perform steps <b>8 to 13</b> in
                            <xref href="#config_ceph/deploying-monitor-on-standalone-node"
                            format="dita">Deploying the Monitor Service on Standalone
                        Nodes</xref>.</li>
                </ol>
            </p>
            <p id="using-two-vlan"><b>Using Two VLANs: For Management and Client Traffic and for
                    Internal OSD Traffic</b></p>
            <p>You can use dual VLANs to transmit Ceph traffic. In this configuration one VLAN
                transmits management and client traffic and the other VLAN transmits internal OSD
                traffic. A separate bonded interface for two VLANs is used with four NICs. This
                configuration provides two aspects:<ul id="ul_ypz_qgy_lw">
                    <li>Use of two networks, such as VLANs. </li>
                    <li>Use of separate bonded interfaces for each VLAN (different from what is
                        provided in <codeph>entry-scale-kvm-ceph</codeph>). </li>
                </ul></p>
        </section>
        <p>The use of separate NICs segregates traffic at the interface level and requires your
            server to have at least four NICs. But using a separate bonded interface for each VLAN
            is not mandatory, and thus you can use a single bonded interface (or server with only
            two NICs) for Ceph deployment. </p>
        <!--<p><b>Architecture diagram </b></p><p>&lt;need diagram></p>-->
        <p><!--<b>Procedure to configure Entry-scale-kvm-ceph-dual-network</b>--></p>
        <p>Perform the following steps to to configure
                <codeph>Entry-scale-kvm-ceph-dual-network</codeph>. </p>
        <p>
            <ol id="ol_cxs_5ts_lw">
                <li>Log in to the lifecycle manager.</li>
                <li>Copy the <codeph>entry-scale-kvm-ceph</codeph> input model to the
                        <codeph>~/helion/my_cloud/definition</codeph> directory before you begin the
                    editing
                    process:<codeblock>cp -r ~/helion/examples/entry-scale-kvm-ceph/* ~/helion/my_cloud/definition/</codeblock></li>
                <li>Validate that NIC interfaces are correctly specified in
                        <codeph>nic_mapping.yml</codeph> for servers that are used in the cloud. For
                    Ceph OSD nodes, four port servers are required. You can use
                        <b>HP-DL360-4PORT</b> as it is defined in <codeph>nic_mapping.yml</codeph>
                    of <codeph>entry-scale-kvm-ceph</codeph> or define a new NIC mapping (as shown
                    here) for new sets of servers having four port
                    servers.<codeblock> - name: HP-4PORT-SERVER
      physical-ports:
        - logical-name: hed1
          type: simple-port
          bus-address: "0000:07:00.0"

        - logical-name: hed2
          type: simple-port
          bus-address: "0000:08:00.0"

        - logical-name: hed3
          type: simple-port
          bus-address: "0000:09:00.0"

        - logical-name: hed4
          type: simple-port
          bus-address: "0000:0a:00.0"</codeblock></li>
                <li>Modify OSD nodes to use four port servers, as shown here. Change the NIC mapping
                    attribute from <b>HP-DL360-4PORT</b> to use any other name defined in
                        <codeph>nic_mapping.yml</codeph>.<codeblock> # Ceph OSD Nodes
    - id: osd1
      ip-addr: 192.168.10.9
      role: OSD-ROLE
      server-group: RACK1
      nic-mapping: HP-DL360-4PORT
      mac-addr: "8b:f6:9e:ca:3b:78"
      ilo-ip: 192.168.9.9
      ilo-password: password
      ilo-user: admin</codeblock></li>
                <li>Delete OSD-CLIENT from <codeph>network_groups.yml</codeph>. Note that no
                    dedicated network group exists for client traffic. Only the management network
                    group is used for client traffic.</li>
                <li>Edit <codeph>net_interfaces.yml</codeph> with a bonded NIC as shown
                    here.<codeblock> - name: CONTROLLER-INTERFACES
      network-interfaces:
        - name: BOND0
          device:
              name: bond0
          bond-data:
              options:
                  mode: active-backup
                  miimon: 200
                  primary: hed1
              provider: linux
              devices:
                - name: hed1
                - name: hed2
          network-groups:
            - EXTERNAL-API
            - EXTERNAL-VM
            - GUEST
            - MANAGEMENT

    - name: COMPUTE-INTERFACES
      network-interfaces:
        - name: BOND0
          device:
              name: bond0
          bond-data:
              options:
                  mode: active-backup
                  miimon: 200
                  primary: hed1
              provider: linux
              devices:
                - name: hed1
                - name: hed2
          network-groups:
            - EXTERNAL-VM
            - GUEST
            - MANAGEMENT

    - name: OSD-INTERFACES
      network-interfaces:
        - name: BOND0
          device:
              name: bond0
          bond-data:
              options:
                  mode: active-backup
                  miimon: 200
                  primary: hed1
              provider: linux
              devices:
                - name: hed1
                - name: hed2
          network-groups:
            - MANAGEMENT
        - name: BOND1
          device:
              name: bond1
          bond-data:
              options:
                  mode: active-backup
                  miimon: 200
                  primary: hed3
              provider: linux
              devices:
                - name: hed3
                - name: hed4
          network-groups:
            - OSD-INTERNAL

    - name: RGW-INTERFACES
     network-interfaces:
        - name: BOND0
          device:
              name: bond0
          bond-data:
              options:
                  mode: active-backup
                  miimon: 200
                  primary: hed1
              provider: linux
              devices:
                - name: hed1
                - name: hed2
          network-groups:
            - MANAGEMENT</codeblock></li>
                <li>Delete OSD-CLIENT from <codeph>server_groups.yml</codeph>. </li>
                <li>Delete VLAN information for OSD-CLIENT-NET from <codeph>networks.yml</codeph>.
                    Only management VLANs are used for client traffic. </li>
                <li>After you set up your configuration files, perform steps <b>8 to 13</b> in <xref
                        href="#config_ceph/deploying-monitor-on-standalone-node" format="dita"
                        >Installing Monitor on Standalone Node</xref>.</li>
            </ol>
        </p>
        <section>
            <title>RADOS Gateway</title>
        </section>
        <section id="install-rados-gateway-on-cluster-node-that-host-ceph-monitor">
            <b>Installing RADOS Gateway on Dedicated Cluster Nodes that Host the Ceph Monitor
                Service</b>
            <p>You can configure RADOS Gateway to install on one or more dedicated cluster nodes
                hosting the Ceph monitor service as follows:</p><ol id="ol_hfp_q22_sv">
                <li>Remove the sections for servers in the
                        <codeph>~/helion/my_cloud/definition/data/servers.yml</codeph> file that
                    have the <codeph>role: RGW-ROLE</codeph> attribute.</li>
                <li>Edit the <codeph>~/helion/my_cloud/definition/data/control_plane.yml</codeph>
                    file and add the following lines to the <codeph>service-components</codeph>
                    section for the cluster nodes that have the <codeph>server-role:
                        MON-ROLE</codeph>
                    attribute.<codeblock>- ceph-radosgw
- apache2</codeblock></li>
                <li>Edit the <codeph>~/helion/my_cloud/definition/data/net_interfaces.yml</codeph>
                    file to remove the RGW-INTERFACES section. This section defines RADOS Gateway
                    network interfaces, which are not required in this configuration:
                    <codeblock> - name: RGW-INTERFACES 
   network-interfaces: 
     - name: BOND0 
       device: 
          name: bond0 
       bond-data: 
          options: 
             mode: active-backup 
             miimon: 200 
             primary: hed3 
          provider: linux 
          devices: 
             - name: hed3 
             - name: hed4 
       network-groups: 
         - MANAGEMENT 
         - OSD-CLIENT</codeblock></li>
                <li>After you set up your configuration files, perform steps <b>8 to 13</b> in <xref
                        href="#config_ceph/deploying-monitor-on-standalone-node" format="dita"
                        >Deploying the Monitor on Standalone Nodes</xref>. </li>
            </ol></section>
        <section id="install-rados-gateway-on-controller-nodes">
            <b>Installing RADOS Gateway on Controller Nodes</b>
            <p>You can configure RADOS Gateway to install on controller nodes. To do this, perform
                the following steps:</p><ol id="ol_kfp_q22_sv">
                <li>Remove the sections for servers in the
                        <codeph>~/helion/my_cloud/definition/data/servers.yml</codeph> file that
                    have the <codeph>role: RGW-ROLE</codeph> attribute.</li>
                <li>Edit the <codeph>~/helion/my_cloud/definition/data/net_interfaces.yml</codeph>
                    file to remove the RGW-INTERFACES section. This section defines RADOS Gateway
                    network interfaces, which are not required in this configuration:
                    <codeblock> - name: RGW-INTERFACES 
   network-interfaces: 
     - name: BOND0 
       device: 
          name: bond0 
       bond-data: 
          options: 
             mode: active-backup 
             miimon: 200 
             primary: hed3 
          provider: linux 
          devices: 
             - name: hed3 
             - name: hed4 
       network-groups: 
         - MANAGEMENT 
         - OSD-CLIENT</codeblock></li>
                <li>Edit the <codeph>~/helion/my_cloud/definition/data/control_plane.yml</codeph>
                    file and add the following line to <codeph>service-components</codeph> for the
                    cluster with the <codeph>server-role: CONTROLLER-ROLE</codeph>
                    attribute.<codeblock>- ceph-radosgw</codeblock></li>
                <li>After you set up your configuration files, perform steps <b>8 to 13</b> in <xref
                        href="#config_ceph/deploying-monitor-on-standalone-node" format="dita"
                        >Deploying the Monitor Service on Standalone Nodes</xref>.</li>
            </ol>
        </section>
        <section id="install-more-two-rados-gateway-servers">
            <b>Installing More than Two RADOS Gateway Servers</b>
            <p>To deploy more than two RADOS Gateway servers, you need to add a section to the
                    <codeph>~/helion/my_cloud/definition/data/servers.yml</codeph> file for each
                additional RADOS Gateway node.</p>
            <note>Installing additional RADOS Gateway servers is possible only if RADOS Gateway is
                installed on dedicated cluster nodes or on dedicated cluster nodes that host the
                Ceph monitor service. Additional RADOS Gateway servers cannot be added if RADOS
                Gateway is installed on a controller node.</note>
        </section>
        <section id="ceph-deployment-vcp" audience="INTERNAL"><title>Ceph Deployment with Virtual Control Plane
                </title><keyword keyref="kw-hos-tm"/>
            <keyword keyref="kw-hos-version"/> supports the deployment of control plane elements on
            virtual machines which can be co-located on one baremetal machine or spread across three
            baremetal machines. The baremetal machine in this context is termed as VM factory
            host(s). The following aspects must be considered while deploying Ceph with the virtual
            control plane.<ol id="ol_wg4_s25_mx">
                <li>Deploy OSD and monitor node on a single VM factor host - It is applicable to X1
                    cloud model. The number of VM factory host is one. Therefore, Ceph cluster will
                    not have HA support as there will be only one instance of the monitor component
                    of Ceph.</li>
                <li>Deploy OSD and monitor on set of three VM factor hosts - It is applicable to S1
                    cloud model.</li>
                <li>Deploy monitor on set of three VM factor hosts but OSD nodes are deployed
                    independently as a resource nodes - It is applicable to M1 cloud model.</li>
            </ol><p>The following aspects must be considered while deploying Ceph with virtual
                control plane:</p><p>
                <ol id="ol_zgr_djn_kx">
                    <li>Scale-out of cluster - Adding a new set of monitor or OSD nodes is not
                        validated by the engineering team. Although, technically it is feasible but
                        not recommended because it can have a significant performance impact.
                        However, one can increase a cluster capacity by adding more disks to the VM
                        factory host and configuring them as OSD (a scale-in path to increase
                        capacity) nodes.</li>
                    <li>Deployment of RADOS Gateway on VM factory host is not formally
                        supported.</li>
                    <li>Performance of Ceph components (OSD and monitor nodes) are sensitive to
                        compute resources i.e. memory, core CPU, and so on. It is strongly
                        recommended to plan and allocate minimum amount of resources for Ceph
                        components to avoid resource contention because same set of machines will be
                        running the control plane elements and the Ceph components. Starvation of
                        resources might causes impact on the performance and the stability of the
                        Ceph clusters health. Consider the following resource aspect for planning:
                            <ul id="ul_a3t_c3n_kx">
                            <li>CPU</li>
                            <li>RAM</li>
                            <li>Disk space for monitoring logs</li>
                        </ul><p>The following section focus on the change of the input model for X1
                            and S1 model ONLY. The change in the input model for M1 model is similar
                            except that OSD node is deployed as the resource nodes. For other
                            aspects of cluster management like upgrade, adding new set of disks,
                            stopping and starting services and so on, you can follow the similar
                            approach that is used for the deployment of Ceph cluster using <keyword
                                keyref="kw-hos-tm"/> .</p></li>
                </ol>
            </p><p><b>Steps to deploy Ceph on VM factory host(s)</b></p><p>
                <ol id="ol_hts_y3n_kx">
                    <li>Log in to the lifecycle manager.</li>
                    <li>Go to <codeph>~/helion/my_cloud/definition/</codeph>.<note>We assume that
                            you have already copied the cloud model representing control element
                            deployment on VM factory host(s).</note></li>
                    <li>Edit your <codeph>control_plane.yml</codeph> file to add
                            <codeph>ceph-osd</codeph> and <codeph>ceph-monitor</codeph> components
                        to the vmfactory nodes. For
                        example:<codeblock> - name: vmfactory	
          resource-prefix: vmf	
          server-role: HLM-HYPERVISOR-ROLE	
          min-count:
          allocation-policy: strict	
          service-components: 	
            - ntp-server	
            - ceph-osd	
            - ceph-monitor	
            - lifecycle-manager	
            - tempest	
            - openstack-client</codeblock></li>
                    <li>Edit <codeph>disks_vmfactory.yml</codeph> file of VM factory hosts to define
                        data and journal disks for OSD. For example, the following disk model
                        illustrates the usage of <codeph>/dev/sdd</codeph>,
                            <codeph>/dev/sde</codeph>, and <codeph>/dev/sdf</codeph> as data disks
                        and <codeph>/dev/sdg</codeph> as journal disks for OSD. Disks allocated to
                        OSD must not be used for any other
                        purpose.<codeblock>---	
   product: 	
     version: 2	
     
   disk-models: 	
     - name: HLM-HYPERVISOR-DISKS	
       volume-groups: 	
	- name: hlm-vg	
        physical-volumes: 	
         - /dev/sda_root	
        logical-volumes: 	
        # The policy is not to consume 100% of the space of each volume group.
        # 5% should be left free for snapshots and to allow for some flexibility. 	
          - name: root	
            size: 35%
            fstype: ext4	
            mount: /	
          - name: log	
            size: 50%	
            mount: /var/log	
            fstype: ext4	
            mkfs-opts: -O large_file	
          - name: crash	
            size: 10%	
            mount: /var/crash	
            fstype: ext4	
            mkfs-opts: -O large_file	
      - name: vg-images	
        # this VG is dedicated to libvirt images to keep VM IOPS off the OS disk 	
        physical-volumes: 	
          - /dev/sdb	
          - /dev/sdc	
        logical-volumes: 	
          - name: images	
            size: 95%	
            mount: /var/lib/libvirt/images	
            fstype: ext4	
            mkfs-opts: -O large_file	
            
   device-groups: 	
      - name: ceph-osd-disks
        devices: 	
       - name: /dev/sdd	
       - name: /dev/sde	
       - name: /dev/sdf
        consumer: 	
           name: ceph	
           attrs: 	
             usage: data	
             journal_disk: /dev/sdg</codeblock></li>
                    <li>Commit your configuration to the local git
                        repo.<codeblock>cd ~/helion/hos/ansible
git add -A
git commit -m "My config or other commit message"</codeblock></li>
                    <li>After setting up the configuration files, continue with the installation
                        procedure mentioned at <xref
                            href="../installing_kvm.dita#install_kvm/config_processor"/>. <note>
                            <ol>
                                <li>Running <codeph>ceph-stop.yml</codeph> and
                                        <codeph>ceph-start.yml</codeph> playbooks on the VM factory
                                    host stops all the services (OSD and monitor) on the node. There
                                    is no playbook that can stop only one service.</li>
                                <li>If you are deploying Ceph on a single VM factory host (i.e. X1
                                    model), make the following changes before deployment.<ul
                                        id="ul_dzr_vgr_lx">
                                        <li>Edit
                                                <codeph>~/helion/my_cloud/config/ceph/settings.yml</codeph>
                                            file to add the following
                                            content.<codeblock>extra:
     global:
           osd_crush_chooseleaf_type: 0</codeblock></li>
                                        <li>Commit your configuration to the local git repo and
                                            continue with the installation procedure mentioned at
                                                <xref
                                                href="../installing_kvm.dita#install_kvm/config_processor"
                                            />.</li>
                                    </ul></li>
                            </ol>
                        </note></li>
                </ol>
            </p></section>
    </body>
</topic>

