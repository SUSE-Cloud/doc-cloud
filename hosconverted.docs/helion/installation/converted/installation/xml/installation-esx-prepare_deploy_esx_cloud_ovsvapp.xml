<?xml version="1.0"?>
<!DOCTYPE section PUBLIC "-//OASIS//DTD DocBook XML V4.5//EN" "http://www.oasis-open.org/docbook/xml/4.5/docbookx.dtd" [ <!ENTITY % entities SYSTEM "entities.xml"> %entities; ]><!--Edit status: not edited-->
<section id="idg-installation-esx-prepare_deploy_esx_cloud_ovsvapp-dita-1">
   <title>
      <phrase/>Prepare and Deploy ESX Computes and OVSvAPPs </title>
    
    <para>The following sections describe the procedure to install and configure ESX compute and
      OVSvAPPs on vCenter.</para>


    <para><emphasis role="bold">Preparation for ESX Cloud Deployment</emphasis></para>


   <para>This
        section describes the procedures to prepare and deploy the ESX computes and OVSvAPPs for
        deployment. </para>


   <orderedlist id="ol_xpc_zqs_ft">
          <listitem>
            <para>Login to the lifecycle manager.</para>

         </listitem>
          <listitem>
            <para>Source <literal>service.osrc</literal>.</para>

         </listitem>
          <listitem>
            <para><ulink url="#install_esx/register-vcenter">Register a vCenter
              Server</ulink></para>

         </listitem>
          <listitem>
            <para><ulink url="#install_esx/activate-cluster">Activate Clusters</ulink></para>

            <orderedlist id="ol_isb_wcc_vt">
              <listitem>
                  <para><ulink url="#install_esx/modify-volume-config">Modify the Volume
                  Configuration File</ulink></para>

               </listitem>
              <listitem>
                  <para><ulink url="#install_esx/commit-your-cloud">Commit your Cloud
                  Definition</ulink></para>

               </listitem>
              <listitem>
                  <para><ulink url="#install_esx/deploy-compute-proxy-ovsvapps">Deploy ESX
                  Compute Proxy and OVSvApps</ulink></para>

               </listitem>
            </orderedlist>
         </listitem>
        </orderedlist>

    <para><emphasis role="bold">Manage vCenters and Clusters</emphasis></para>


    <para>The following section describes the detailed procedure on managing the vCenters and
      clusters.</para>


    <para><emphasis role="bold">Register a vCenter Server</emphasis></para>

<note>
         <itemizedlist id="ul_cdm_vqm_1y">
            <listitem>
               <para>If ESX related roles are added in the input model after the controller cluster is up,
            you must run <literal>hlm-reconfigure.yml</literal> before adding the eon
            resource-manager.</para>

            </listitem>
            <listitem>
               <para>Make sure to provide single quotes (' ') or backslash (\) for the
            special characters (&amp; ! ; " ' () | \ &gt;) that are used when setting a password for
            vCenter registration . For example: '2the!Moon' or 2the\!Moon. You can use either of the
            format to set the vCenter password.</para>

            </listitem>
        </itemizedlist>
      </note>

    <para>vCenter provides centralized management of virtual host and virtual machines from a single
        console.</para>

<orderedlist id="ol_xdj_5js_ft">
        <listitem>
            <para>Add a vCenter using EON python
            client.</para>

            <screen># eon resource-manager-add [--name &lt;RESOURCE_MANAGER_NAME&gt;] --ip-address &lt;RESOURCE_MANAGER_IP_ADDR&gt; --username &lt;RESOURCE_MANAGER_USERNAME&gt; --password &lt;RESOURCE_MANAGER_PASSWORD&gt; [--port &lt;RESOURCE_MANAGER_PORT&gt;] --type &lt;RESOURCE_MANAGER_TYPE&gt;</screen>
            <para>where:
              </para>
<itemizedlist id="ul_bp3_yjs_ft">
                  <listitem>
                     <para>Resource Manager Name - the identical name of the vCenter server.</para>
                  </listitem>
                  <listitem>
                     <para>Resource Manager IP address - the IP address of the vCenter server.</para>
                  </listitem>
                  <listitem>
                     <para>Resource Manager Username - the admin privilege username for the vCenter.</para>
                  </listitem>
                  <listitem>
                     <para>Resource Manager Password - the password for the above username.</para>
                  </listitem>
                  <listitem>
                     <para>Resource Manager Port - the vCenter server port. By default it is 443.</para>
                  </listitem>
                  <listitem>
                     <para> Resource Manager Type - specify <literal>vcenter</literal>.</para>
                     <important>
                        <para>Please do not change the vCenter Port unless you are certain it is required to do
                  so.</para>
                     </important>
                  </listitem>
               </itemizedlist>

            <para><emphasis role="bold">Sample
            Output:</emphasis></para>
<screen># eon resource-manager-add --name vc01 --ip-address 10.1.200.38 --username administrator@vsphere.local --password init123# --port 443 --type vcenter
+-------------+--------------------------------------+
| Property    | Value                                |
+-------------+--------------------------------------+
| ID          | BC9DED4E-1639-481D-B190-2B54A2BF5674 |
| IPv4Address | 10.1.200.38                          |
| Name        | vc01                                 |
| Password    | &lt;SANITIZED&gt;                          |
| Port        | 443                                  |
| State       | registered                           |
| Type        | vcenter                              |
| Username    | administrator@vsphere.local          |
+-------------+--------------------------------------+</screen>

         </listitem>
      </orderedlist>

    <para><emphasis role="bold">Show vCenter</emphasis></para>


   <orderedlist id="ol_wh5_rsb_lt">
        <listitem>
         <para>Show vCenter using EON python client.
              </para>


         <screen># eon resource-manager-show &lt;RESOURCE_MANAGER_ID&gt;</screen>
         <para><emphasis role="bold">Sample
              Output:</emphasis></para>

<screen>eon resource-manager-show BC9DED4E-1639-481D-B190-2B54A2BF5674
+-------------+--------------------------------------+
| Property    | Value                                |
+-------------+--------------------------------------+
| Clusters    | Cluster1, virtClust, Cluster2        |
| Datacenters | DC1, DC2                             |
| ID          | BC9DED4E-1639-481D-B190-2B54A2BF5674 |
| IPv4Address | 10.1.200.38                          |
| Name        | vc01                                 |
| Password    | &lt;SANITIZED&gt;                          |
| Port        | 443                                  |
| State       | registered                           |
| Type        | vcenter                              |
| Username    | administrator@vsphere.local          |
+-------------+--------------------------------------+</screen>

      </listitem>
      </orderedlist>
    <para><emphasis role="bold">Create and edit an activation template</emphasis></para>


   <para>This involves getting a sample network information template and modifying the details of
        the template. You will use the template to register the cloud network configuration for the
        vCenter.</para>


   <orderedlist>
          <listitem>
            <para>Execute the following command to generate a sample activation
                template:</para>

            <screen>eon get-activation-template [--filename &lt;ACTIVATION_JSON_NAME&gt;] --type &lt;RESOURCE_TYPE&gt;</screen>
            <para><emphasis role="bold">Sample
                Output:</emphasis></para>

            <screen>eon get-activation-template --filename activationtemplate.json --type esxcluster
---------------------------------------------------------------
Saved the sample network file in /home/user/activationtemplate.json
---------------------------------------------------------------</screen>

         </listitem>
          <listitem>
            <para>Change to the <literal>/home/user/</literal>
            directory:</para>

            <screen>cd /home/user/</screen>
         </listitem>
          <listitem>
            <para>Modify the template (json file) as per your environment. See <xref linkend="topic4797cgikdjm"/>.</para>

         </listitem>
        </orderedlist>

    <para><emphasis role="bold">Activate Clusters</emphasis></para>


   <para>This involves using the activation
        template to register the cloud network configuration for the vCenter.</para>


   <para>This process
        spawns one compute proxy VM per cluster and one OVSvApp VM per host and configures the
        networking as defined in the JSON template. This process also updates the input model with
        the service VM details, executes the ansible playbooks on the new nodes, and performs a post
        activation check on the service VMs. </para>


   <orderedlist id="ol_tbr_4l4_vv">
        <listitem>
         <para>Activate the cluster for the selected
            vCenter.</para>


         <screen># eon resource-activate &lt;RESOURCE_ID&gt; --config-json /home/user/&lt;ACTIVATION_JSON_NAME&gt;</screen>
         <note>
            <para>To
            retrieve the resource ID, execute <literal>eon
            resource-list</literal>.</para>


         </note>
         <note>
            <para>Activating the first cluster in a datacenter
            requires you to provide an activation template JSON. You can provide the same activation
            template or a new activation template to activate the subsequent
            clusters.</para>


         </note>
         <note>
            <para>Minimum disk space required for a cluster activation is (number of
            hosts in that cluster +1 )* 45 GB.</para>


         </note>
      </listitem>
        <listitem>
         <para>Execute the following command to view the status of the
            cluster:</para>


         <screen>eon resource-list
+--------------------------------------+-----------+-------------+--------------------------------------+------------+-------+------------+------------+
| ID                                   | Name      | Moid        | Resource Manager ID                  | IP Address | Port  | Type       | State      |
+--------------------------------------+-----------+-------------+--------------------------------------+------------+-------+------------+------------+
| 1228fce5-df5d-445c-834e-ae633ac7e426 | Cluster2  | domain-c184 | BC9DED4E-1639-481D-B190-2B54A2BF5674 | UNSET      | UNSET | esxcluster | imported   |
| 469710f6-e9f2-48a4-aace-1f00cbd60487 | virtClust | domain-c943 | BC9DED4E-1639-481D-B190-2B54A2BF5674 | UNSET      | UNSET | esxcluster | activated  |
| a3003a32-6e3a-4d89-a072-ec64a4247fb0 | Cluster1  | domain-c21  | BC9DED4E-1639-481D-B190-2B54A2BF5674 | UNSET      | UNSET | esxcluster | imported   |
+--------------------------------------+-----------+-------------+--------------------------------------+------------+-------+------------+------------+</screen>
         <para>When
            the state is <literal>activated</literal>, the input model is updated with the service VM
            details, a git commit has been performed, the required playbooks and the post activation
            checks are completed successfully.</para>


         <note userlevel="INTERNAL">
               <para>In multi-region setups, monitoring services will be
              deployed/running in a shared control-plane, unlike the normal deployment, where all
              services will be in one single control-plane. During ESX cluster activation in the
              multi-region scenarios, <literal>generate_hosts_file</literal> monitoring playbooks will
              be skipped due to absence of monitoring services in the ESX control-plane. To overcome
              this, you must run the playbook manually without
              limit.</para>

               <screen>cd ~/scratch/ansible/next/hos/ansible/ 
ansible-playbook -i hosts/verb_hosts site.yml --tag "generate_hosts_file"</screen>
            </note>
<note>
               <para>Whenever an ESX compute is activated or deactivated, you must run the following
              playbook to reflect the changes in
              opsconsole.</para>

               <screen>cd scratch/ansible/next/hos/ansible/
ansible-playbook -i hosts/verb_hosts ops-console-reconfigure.yml </screen>
            </note>

      </listitem>
      </orderedlist>
    <para><emphasis role="bold">Modify the Volume Configuration File</emphasis></para>


   <para>Once the cluster is activated you must configure the volume.</para>


   <para>Perform the following
        steps to modify the volume configuration files:</para>


   <orderedlist id="ol_bhx_n5p_st">
        <listitem>
         <para>Change the directory. The <literal>cinder.conf.j2</literal> is present in following
          directories
            :</para>


         <screen>cd /home/stack/helion/hos/ansible/roles/_CND-CMN/templates</screen>
         <para>OR</para>

<screen>cd /home/stack/helion/my_cloud/config/cinder</screen>

         <para>It
            is recommended to modify the <literal>cinder.conf.j2</literal> available in
              <literal>/home/stack/helion/my_cloud/config/cinder</literal></para>


      </listitem>
        <listitem>
         <para>Modify the <literal>cinder.conf.j2</literal> as follows:
          </para>


         <screen># Configure the enabled backends
enabled_backends=&lt;unique-section-name&gt;

# Start of section for VMDK block storage
#
# If you have configured VMDK Block storage for cinder you must
# uncomment this section, and replace all strings in angle brackets
# with the correct values for vCenter you have configured. You
# must also add the section name to the list of values in the
# 'enabled_backends' variable above. You must provide unique section
# each time you configure a new backend.

#[&lt;unique-section-name&gt;]
#vmware_api_retry_count = 10
#vmware_tmp_dir = /tmp
#vmware_image_transfer_timeout_secs = 7200
#vmware_task_poll_interval = 0.5
#vmware_max_objects_retrieval = 100
#vmware_volume_folder = cinder-volumes
#volume_driver = cinder.volume.drivers.vmware.vmdk.VMwareVcVmdkDriver
#vmware_host_ip = &lt;ip_address_of_vcenter&gt;
#vmware_host_username = &lt;vcenter_username&gt;
#vmware_host_password = &lt;password&gt;
#
#volume_backend_name = &lt;vmdk-backend-name&gt;
#
# End of section for VMDK block storage</screen>
      </listitem>
      </orderedlist>
    <para><emphasis role="bold">Commit your Cloud Definition</emphasis></para>


   <orderedlist>
          <listitem>
            <para> Add the cloud deployment definition to git
            :</para>

            <screen>cd /home/stack/helion/hos/ansible;
git add -A;
git commit -m 'Adding ESX Configurations or other commit message';</screen>
         </listitem>
          <listitem>
            <para>Prepare your environment for deployment:
            </para>

            <screen>ansible-playbook -i hosts/localhost config-processor-run.yml;
ansible-playbook -i hosts/localhost ready-deployment.yml;
cd /home/stack/scratch/ansible/next/hos/ansible;</screen>
         </listitem>
        </orderedlist>

    <para><emphasis role="bold">Configuring VMDK block storage</emphasis></para>


   <para>Execute the
        following command to configure VMDK block
        storage:</para>

<screen>ansible-playbook -i hosts/verb_hosts cinder-reconfigure.yml</screen>

   <para>
        If there are more backends (like VSA) defined, you must create and use specific volume type
        to ensure that volume is created in ESX, as shown
        below:</para>

<screen># cinder type-create "ESX VMDK Storage"
...
# cinder type-key "ESX VMDK Storage" set volume_backend_name=&lt;name of VMDK backend selected during installation&gt;
...
# cinder create --volume-type "ESX VMDK Storage" 1
...</screen>

    
      <bridgehead renderas="sect4">Validate the block storage</bridgehead>
      <para>You can validate that the VMDK block storage is added to the cloud successfully using the
        following
        command:</para>

<screen># cinder service-list</screen>
<screen>+------------------+-------------------------+------+---------+-------+----------------------------+-----------------+
|      Binary      |           Host          | Zone |  Status | State |         Updated_at         | Disabled Reason |
+------------------+-------------------------+------+---------+-------+----------------------------+-----------------+
|  cinder-backup   |    ha-volume-manager    | nova | enabled |   up  | 2016-06-14T08:44:51.000000 |        -        |
| cinder-scheduler |    ha-volume-manager    | nova | enabled |  down | 2016-06-13T08:47:07.000000 |        -        |
| cinder-scheduler |    ha-volume-manager    | nova | enabled |   up  | 2016-06-14T08:44:57.000000 |        -        |
|  cinder-volume   |    ha-volume-manager    | nova | enabled |  down | 2016-06-13T07:36:41.000000 |        -        |
|  cinder-volume   | ha-volume-manager@vmdk1 | nova | enabled |   up  | 2016-06-14T08:44:55.000000 |        -        |
|  cinder-volume   |  ha-volume-manager@vsa1 | nova | enabled |   up  | 2016-06-14T08:44:50.000000 |        -        |
+------------------+-------------------------+------+---------+-------+----------------------------+-----------------+</screen>

   
    
      <bridgehead renderas="sect4">Validate the compute</bridgehead>
      <para>You can validate that the ESX compute cluster is added to the cloud successfully using the
        following command:</para>


      <screen>#  nova service-list</screen>
<screen>+-----+------------------+------------------------------+----------+---------+-------+----------------------------+-----------------+
| Id  | Binary           | Host                         | Zone     | Status  | State | Updated_at                 | Disabled Reason |
+-----+------------------+------------------------------+----------+---------+-------+----------------------------+-----------------+
| 3   | nova-conductor   | esxhos-joh-core-m1-mgmt      | internal | enabled | up    | 2016-03-30T12:57:47.000000 | -               |
| 63  | nova-scheduler   | esxhos-joh-core-m1-mgmt      | internal | enabled | up    | 2016-03-30T12:57:43.000000 | -               |
| 66  | nova-conductor   | esxhos-joh-core-m2-mgmt      | internal | enabled | up    | 2016-03-30T12:57:48.000000 | -               |
| 111 | nova-conductor   | esxhos-joh-core-m3-mgmt      | internal | enabled | up    | 2016-03-30T12:57:41.000000 | -               |
| 129 | nova-scheduler   | esxhos-joh-core-m3-mgmt      | internal | enabled | up    | 2016-03-30T12:57:41.000000 | -               |
| 132 | nova-consoleauth | esxhos-joh-core-m1-mgmt      | internal | enabled | up    | 2016-03-30T12:57:44.000000 | -               |
| 135 | nova-scheduler   | esxhos-joh-core-m2-mgmt      | internal | enabled | up    | 2016-03-30T12:57:47.000000 | -               |
| 138 | nova-compute     | esxhos-joh-esx-comp0001-mgmt | nova     | enabled | up    | 2016-03-30T12:57:41.000000 | -               |
+-----+------------------+------------------------------+----------+---------+-------+----------------------------+-----------------+</screen>

   
    <para>You can validate the hypervisor list using the following
      command:</para>

<screen># nova hypervisor-list</screen>
<screen>+----+-------------------------------------------------+-------+----------+
| ID | Hypervisor hostname                             | State | Status   |
+----+-------------------------------------------------+-------+----------+
| 9  | domain-c40.9FDCFA66-6677-42A1-83FF-16DC32448021 | up    | enabled  |
+----+-------------------------------------------------+-------+----------+</screen>

    
      <bridgehead renderas="sect4">Validate the neutron</bridgehead>
      <para>You can validate that the networking of the ESX cluster using the following
        command:</para>

<screen># neutron agent-list</screen>
<screen>+--------------------------------------+--------------------+---------------------------------+-------+----------------+---------------------------+
| id                                   | agent_type         | host                            | alive | admin_state_up | binary                    |
+--------------------------------------+--------------------+---------------------------------+-------+----------------+---------------------------+
| 097bdbc3-108c-41ca-8b52-9d249f65077f | Metadata agent     | esxhos-joh-core-m3-mgmt         | :-)   | True           | neutron-metadata-agent    |
| 2b255256-9505-489e-93bf-0d37f7ff83e4 | DHCP agent         | esxhos-joh-core-m3-mgmt         | :-)   | True           | neutron-dhcp-agent        |
| 43843bcb-f929-4a42-a1e1-207ff97dc09e | OVSvApp Agent      | esxhos-joh-esx-ovsvapp0002-mgmt | :-)   | True           | ovsvapp-agent             |
| 4f0be657-fe9e-4bab-bef8-d8e688b6573e | L3 agent           | esxhos-joh-core-m3-mgmt         | :-)   | True           | neutron-vpn-agent         |
| 6db7d585-604e-4205-b29c-b9684bfb0cb2 | OVSvApp Agent      | esxhos-joh-esx-ovsvapp0001-mgmt | :-)   | True           | ovsvapp-agent             |
| 700f1a18-d237-4605-bf54-145951bd12db | DHCP agent         | esxhos-joh-core-m1-mgmt         | :-)   | True           | neutron-dhcp-agent        |
| a723fb23-2c87-4f69-b272-643df870f0b6 | DHCP agent         | esxhos-joh-core-m2-mgmt         | :-)   | True           | neutron-dhcp-agent        |
| bd512ed5-9f39-4ad6-a505-148f58cf2e64 | L3 agent           | esxhos-joh-core-m1-mgmt         | :-)   | True           | neutron-vpn-agent         |
| e3d4b8be-077e-4503-bed2-031871f3829e | Open vSwitch agent | esxhos-joh-core-m2-mgmt         | :-)   | True           | neutron-openvswitch-agent |
| e5d75917-1d26-4443-99f4-45d8c574e3f0 | Open vSwitch agent | esxhos-joh-core-m1-mgmt         | :-)   | True           | neutron-openvswitch-agent |
| e6a40540-adad-4ee7-b194-db18ac7287bd | Metadata agent     | esxhos-joh-core-m1-mgmt         | :-)   | True           | neutron-metadata-agent    |
| e734a20e-159c-450b-9fb8-dc46824ef12e | Metadata agent     | esxhos-joh-core-m2-mgmt         | :-)   | True           | neutron-metadata-agent    |
| f019c268-a6bd-4f66-8a46-6762c720a5bd | L3 agent           | esxhos-joh-core-m2-mgmt         | :-)   | True           | neutron-vpn-agent         |
| fc83c8dd-ead4-4ad5-aed3-95ab41e567bc | Open vSwitch agent | esxhos-joh-core-m3-mgmt         | :-)   | True           | neutron-openvswitch-agent |
+--------------------------------------+--------------------+---------------------------------+-------+----------------+---------------------------+</screen>

   
  </section>
