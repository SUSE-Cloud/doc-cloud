<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE topic PUBLIC "-//OASIS//DTD DITA Topic//EN" "topic.dtd">
<topic id="install_vcp">
  <title><ph conkeyref="HOS-conrefs/product-title"/>HLM-Hypervisor instructions</title>

  <body>

  </body>
  <topic id="d1e43">
    <title>Introduction</title>
    <body>
      <p>You can read an overview of the design on the 
        <xref href="https://wiki.hpcloud.net/display/core/HLM+creation+of+VMs" scope="external" format="html">Helion lifecycle manager creation of VMs</xref> page. 
        This page describes the steps needed
        to set up and use  a HLM-Hypervisor (previously known as a VM-factory) and
        related functionality of <keyword keyref="kw-hos-phrase"/>. This comprises model changes
        and running a couple of new playbooks to bring up the VMs.</p>
    </body>
  </topic>
  
  <topic id="d1e51">
    <title>Model changes</title>
    <body> </body>
    <topic id="d1e55">
      <title>passthrough-network-groups</title>
      <body>
        <p>
          <ph>With PB3 the specification of </ph>
          <codeph>passthrough-network-groups</codeph>
          <ph> is now supported within the Interfaces section of the input model.</ph>
        </p>
        <p>A <codeph>passthrough-network</codeph> group is a network group that can be accessed by a VM hosted by the
          hypervisor system regardless of whether the network group is required on the hypervisor
          system itself.</p>
        
        
          
          
        <dl>
          <dlentry>
            <dt>network-groups:</dt>
            <dd>List of network groups that this server may require access to.
              <p>Access to the network is
                only configured if the network-group is required by at least one service hosted on this
                server.</p>
            </dd>
          </dlentry>
          
          <dlentry>
            <dt>forced-network-groups:</dt>
            <dd>List of network-groups that this server requires access to. <p>Access to the network
                is configured regardless of whether the network-group is required by any service
                hosted on this server. </p>
              
              <p>Network groups should not appear in
                both <codeph>network-groups</codeph> and <codeph>forced-network-groups</codeph></p>
            </dd>
          </dlentry>
          
          
          <dlentry>
            <dt>passthrough-network-groups:</dt>
            <dd>List of network-groups that need to be passed through this server to hosted VMs.
              <p>No access
                is configured for this server if the network group is
                listed <codeph>only</codeph> in <codeph>passthrough-network-groups</codeph>. </p>
              The <codeph>passthrough-network-groups</codeph> may include networks from
              either <codeph>network-groups</codeph> or <codeph>forced-network-groups</codeph> but may
              also include network groups not used directly by this server.If
              a <codeph>passthrough-network-groups</codeph> is listed in either
              of <codeph>network-groups</codeph> or <codeph>forced-network-groups</codeph>the access
              to the network group will be such that both the server and hosted VMs are network
              peers.
            </dd>
          </dlentry>
        </dl>
   


      </body>
      <topic id="d1e146">
        <title>Example:</title>
        <body>
          <p>The following hypervisor interface-model example specifies :</p>
          <ul>
            <li>
              <codeph>bond0</codeph>
              <ul>
                <li>
                  <codeph>PXE</codeph> available to hypervisor and Hosted-VMs</li>
                <li>
                  <codeph>CLM</codeph> available to hypervisor and Hosted-VMs</li>
                <li>
                  <codeph>CAN</codeph> available to Hosted-VMs only</li>
              </ul>
            </li>
            <li>
              <codeph>bond1</codeph>
              <ul>
                <li>
                  <codeph>VxLAN-VLAN1-TUL</codeph>, <ph>
                    <codeph>EXT</codeph> and </ph>
                  <ph>
                    <codeph>VLAN2-TUL</codeph> available to Hosted-VMs only</ph>
                </li>
              </ul>
            </li>
          </ul>
          <codeblock>  interface-models:
    - name: HLM-HYPERVISOR-INTERFACES
      network-interfaces:
        - name: BOND0
          device:
            name: bond0
          bond-data:
            options:
              mode: active-backup
              miimon: 200
              primary: hed5
            provider: linux
            devices:
              - name: hed5
          network-groups:
            - PXE
            - CLM
          passthrough-network-groups:
            - PXE
            - CLM
            - CAN
        - name: BOND1
          device:
            name: bond1
          bond-data:
            options:
              mode: active-backup
              miimon: 200
              primary: hed1
            provider: linux
            devices:
              - name: hed1
          passthrough-network-groups:
            - VxLAN-VLAN1-TUL
            - EXT
            - VLAN2-TUL</codeblock>
        </body>
      </topic>
    </topic>
    <topic id="d1e225">
      <title>hlm-hypervisor</title>
      <body>
        <p>
          <ph>With PB3 we now support the specification of </ph>
          <codeph>hlm-hypervisor</codeph>
          <ph> boolean within the Servers section of the input model. This setting is used to
            identify nodes that can host VMs (non-NOVA). </ph>
        </p>
        <p>This is related to the specification of <codeph>hypervisor-id</codeph> that is required
          to identify servers to be <codeph>instantiated</codeph> as hosted-VMs.</p>
        <ul>
          <li>
            Any server referenced as a target of a 
            <codeph>hypervisor-id</codeph>
             must have 
            <codeph>hlm-hypervisor</codeph>
             set to True
          </li>
          <li>
            A server with <codeph>hlm-hypervisor</codeph> of True setting may or may not have
              Helion lifecycle manager Hosted-VMs associated with it.
           
          </li>
        </ul>
      </body>
      <topic id="d1e283">
        <title>Example: </title>
        <body>
          <p>In the following example from <codeph>servers.yml</codeph>
          </p>
          <ul>
            <li>
              a single server (
              <codeph>hypervisor1)</codeph>
               is declared as supporting Hosted-VMs using 
              <codeph>hlm-hypervisor: True</codeph>
            </li>
            <li>three nodes
                (<codeph>controller1</codeph>, <codeph>controller2</codeph> and <codeph>controller3</codeph>)
              are declared as Hosted-VMs on<codeph>hypervisor1</codeph> (
                <codeph>hypervisor-id: hypervisor1</codeph>)
            </li>
          </ul>
          <codeblock>    - id: hypervisor1
      ip-addr: 10.225.107.74
      role: HLM-HYPERVISOR-ROLE
      server-group: RACK2
      hlm-hypervisor: True
      nic-mapping: HP-BL460-6PORT
      mac-addr: 9c:7e:96:22:9e:d8
      ilo-ip: 10.1.18.42
      ilo-password: whatever
      ilo-user: whatever

    - id: controller1
      ip-addr: 10.225.107.75
      role: CONTROLLER-ROLE
      hypervisor-id: hypervisor1
      nic-mapping: VIRTUAL-1-PORT

    - id: controller2
      ip-addr: 10.225.107.76
      role: CONTROLLER-ROLE
      hypervisor-id: hypervisor1
      nic-mapping: VIRTUAL-1-PORT

    - id: controller3
      ip-addr: 10.225.107.77
      role: CONTROLLER-ROLE
      hypervisor-id: hypervisor1
      nic-mapping: VIRTUAL-1-PORT

    - id: compute1
      ip-addr: 10.225.107.78
      role: COMPUTE-ROLE
      server-group: RACK1
      nic-mapping: HP-BL460-6PORT
      mac-addr: 6c:9e:96:22:7e:d8
      ilo-ip: 10.1.18.44
      ilo-password: whatever
      ilo-user: whatever</codeblock>
   


          <p>The basic steps are as follows, with examples below. In all of these examples, it is assumed
            that  hed1 is the physical interface to which the VMs need to be connected.</p>
          <ol>
            <li> Create a new network interfaces group for the Hypervisor nodes, appropriate to the
              hardware in use. It is called HLM-HYPERVISOR-INTERFACES in this example. You may also want to create nic mappings for these
              nodes. 
              <codeblock>@@ -94,3 +94,12 @@
           network-groups:
             - MANAGEMENT
 
+    - name: HLM-HYPERVISOR-INTERFACES
+      network-interfaces:
+        - name: hed1
+          device:
+              name: hed1
+          network-groups:
+            - EXTERNAL-VM
+            - GUEST
+            - MANAGEMENT</codeblock>
            </li>
            <li> Create a new disk model for the Hypervisor nodes. Here's an example for a
              Hypervisor node with 4 disks.  In this specific example, we've included storage to be
              set-aside in the <keyword keyref="kw-hos"/> input model for gluster.
              
                        <p>You will need to tailor this disk model to the hardware that you are using. For
                          example, if your Hypervisor nodes only have one disk, you would delete the
                          line referencing <codeph>/dev/sdb</codeph> from the <codeph>hlm-vg</codeph> volume group definition, and
                          remove the <codeph>vg-images</codeph> volume group altogether.</p>
              
              <codeblock>---
  product:
    version: 2
  disk-models:
  - name: HLM-HYPERVISOR-DISKS
    volume-groups:
      - name: hlm-vg
        physical-volumes:
         - /dev/sda_root
         - /dev/sdb
        logical-volumes:
          - name: root
            size: 35%
            fstype: ext4
            mount: /
          - name: log
            size: 50%
            mount: /var/log
            fstype: ext4
            mkfs-opts: -O large_file
          - name: crash
            size: 10%
            mount: /var/crash
            fstype: ext4
            mkfs-opts: -O large_file
      # optional VG dedicated to VM images to keep VM IOPS off the OS disk
      # The consumer stanza allows user defined location of where to store the qcows 
      - name: hlm-images
        physical-volumes:
          - /dev/sdc
          - /dev/sdd
        logical-volumes:
          - name: hlm-images
            size: 95%
            mount: /var/lib/hlm-images
            fstype: ext4
            mkfs-opts: -O large_file
            consumer:
				name: hlm-hypervisor
				usage: hlm-hypervisor-images
 
    # Optionally define some storage in the input model to be used
    # by gluster
    device-groups:
      - name: gluster
        devices:
          - name: /dev/sde
          - name: /dev/sdf
        consumer:
          name: gluster
          suppress-warnings: True</codeblock>
              
              Note the <codeph>suppress-warnings: True</codeph> flag for "gluster".  This indicates to the
              configuration processor that is should not issue warnings when it cannot find a
              "gluster" service definition.   </li>
            
            
            <li> Create a new role that uses this interface and disk model.
              <codeblock>@@ -34,3 +34,7 @@
     - name: DEPLOYER-ROLE
       interface-model: DEPLOYER-INTERFACES
       disk-model: DEPLOYER-DISKS
+
+    - name: HLM-HYPERVISOR-ROLE
+      interface-model: HLM-HYPERVISOR-INTERFACES
+      disk-model: HLM-HYPERVISOR-DISKS</codeblock>
            </li>
            <li> Define a new type of node in the "clusters" section of control_plane.yml so that
              nodes with this role will get their own CP group.
              <codeblock>@@ -135,3 +135,10 @@
             - ntp-client
             - vsa
 
+        - name: hypervisor
+          resource-prefix: vmf
+          server-role: HLM-HYPERVISOR-ROLE
+          allocation-policy: strict
+          min-count: 0
+          service-components:
+            - lifecycle-manager-target
+            - ntp-server</codeblock>
              
              
              If the lifecycle manager is also a hypervisor then you should add the component
              <codeph>lifecycle-manager</codeph> instead of <codeph>lifecycle-manager-target</codeph>. It is required that the
              hypervisor nodes have the <codeph>ntp-server</codeph> component.   </li>
            
            
            <li>Assign the HLM-HYPERVISOR-ROLE to the appropriate nodes in servers.yml
              
              <ol>
                <li>If the lifecycle manager node  is also going to be a Hypervisor, then
                  give it the HLM-HYPERVISOR-ROLE in servers.yml</li>
                <li>Having a baremetal node that is both a Hypervisor and a Controller is not
                  supported.</li>
                <li>Also add definitions for all of the control plane VMs that you intend to create
                  later. Note that you don't need iLO information or mac-addresses for VMs. </li>
                <li>You will need to modify the IP addresses etc in this example to match
                  your machine.
                  <codeblock>@@ -80,7 +80,7 @@
 
+    - id: hypervisor1
+      ip-addr: 10.225.107.74
+      role: HLM-HYPERVISOR-ROLE
+      hlm-hypervisor: True
+      server-group: RACK2
+      nic-mapping: HP-BL460-6PORT
+      mac-addr: 9c:7e:96:22:9e:d8
+      ilo-ip: 10.1.18.42
+      ilo-password: whatever
+      ilo-user: whatever
+
+    - id: controller1
+      ip-addr: 10.225.107.75
+      role: CONTROLLER-ROLE
+      hypervisor-id: hypervisor1
+      nic-mapping: VIRTUAL-1-PORT
+
+    - id: controller2
+      ip-addr: 10.225.107.76
+      role: CONTROLLER-ROLE
+      hypervisor-id: hypervisor1
+      nic-mapping: VIRTUAL-1-PORT
+
+    - id: controller3
+      ip-addr: 10.225.107.77
+      role: CONTROLLER-ROLE
+      hypervisor-id: hypervisor1
+      nic-mapping: VIRTUAL-1-PORT
+
+    - id: compute1
+      ip-addr: 10.225.107.78
+      role: COMPUTE-ROLE
+      server-group: RACK1
+      nic-mapping: HP-BL460-6PORT
+      mac-addr: 6c:9e:96:22:7e:d8
+      ilo-ip: 10.1.18.44
+      ilo-password: whatever
+      ilo-user: whatever</codeblock>
                </li>
              </ol>
            </li>
            <li> Add a definition of this new nic-mapping to <codeph>nic_mappings.yml</codeph>
              <codeblock>    - name: VIRTUAL-1-PORT
      physical-ports:
      - bus-address: '0000:01:01.0'
        logical-name: hed1
        type: simple-port</codeblock>
              
              See the section <xref href="#nic_mapping_for_hlm_virtual_controllers">NIC Mapping for Helion lifecycle manager Virtual-Controllers</xref> below for more information and
              examples. </li>
            
            <li> You will also need to make model changes for the vms. The vm role above is
              CONTROLLER-ROLE.  You need to add information to the model that specifies: <ol>
                <li> The number of vcpus - this is done by adding a <codeph>cpu-model</codeph> to the vm role with a
                  <codeph>vm-size</codeph> stanza </li>
                <li> The amount of RAM to be used - this is done by adding a <codeph>memory-model</codeph> to the vm
                  role with a <codeph>vm-size</codeph> stanza </li>
                <li> The sizes of the virtual disks - this is done by adding a <codeph>vm-size</codeph> stanza to
                  the disk-model used with the vm-role. </li>
              </ol> The CONTROLLER-ROLE definition will look like:
              <codeblock>    - name: CONTROLLER-ROLE
      interface-model: CONTROLLER-INTERFACES
      disk-model: CONTROLLER-DISKS
      cpu-model: CONTROLLER-CPU
      memory-model: CONTROLLER-MEMORY</codeblock>
              
              The cpu-model will look like the following - place it in a yaml file and set <codeph>vcpus</codeph>
              to the value that you want to use:
              <codeblock>---
  product:
    version: 2

  cpu-models:
   - name: CONTROLLER-CPU
     vm-size:
        vcpus: 4</codeblock>
              
              The memory-model will look like the following - place it in a yaml file and set <codeph>ram</codeph>
              to the value that you want to use (valid values are in "KMG"):
              
              <codeblock>---
  product:
    version: 2
  memory-models:
   - name: CONTROLLER-MEMORY
     vm-size:
       ram: 16G</codeblock>
              
              The <codeph>disk-model</codeph> will have a stanza like the following added to it:
              <codeblock>  disk-models:
  - name: CONTROLLER-DISKS
    vm-size:
      disks:
      - name: /dev/vda_root
        size: 1T
      - name: /dev/vdb
        size: 1T
      - name: /dev/vdc
        size: 1T
      - name: /dev/vdd
        size: 1T</codeblock>
              
              <codeph>disks</codeph> is a list of all the physical-volumes in volume-groups and devices in
              device-groups used in the disk-model, one entry for each with the name in the <codeph>name:</codeph>
              field and the size in the <codeph>size:</codeph> field.  Valid values for size are <codeph>KMGT</codeph> </li>
            <li>Note hat the network that is associated with ansible traffic needs be on the network
              interface in the vm which is on the "lowest" pci address. In the nic-mapping
              description below this would be hed1. </li>
            <li>You do need to have a <codeph>vda_root</codeph> described in your input model,</li>
          </ol>
          <p> </p>
        </body>
      </topic>
    </topic>
  </topic>
  <topic id="d1e603">
    <title>Bootstrap Instructions</title>
    <body>
      <p>Start with one baremetal node and install it directly from the <keyword keyref="kw-hos-phrase"/> iso e.g. through
        virtual media on its iLO. Follow the usual install instructions e.g. <xref keyref="install_entryscale_kvm"/>
          
         but make sure
        you use a model with the changes described above. Follow all the usual steps, including
        cobbler-deploy, bm-reimage and config-processor-run. Stop after
          running <codeph>ready-deployment.yml</codeph> though, and move to the instructions in the
        next section, to bring up the virtual control plane nodes.</p>
      
      <note type="note">
        The disks on the VCP VMs are presented as <codeph>virtio</codeph> devices, i.e.
            named <codeph>vda</codeph>, <codeph>vdb</codeph> etc, so you will need to use these
          names in your disk input models.
      </note>

    </body>
  </topic>
  
  <topic id="d1e637">
    <title>Customizing for VCP</title>
    <body>
      <p>The <codeph>ready-deployment.yml</codeph> playbook will have
          created <codeph>~/scratch/ansible/next/hos/ansible</codeph> so change to that directory
        and perform the rest of your work from that directory.</p>
      
      <codeblock>cd ~/scratch/ansible/next/hos/ansible</codeblock>
      

      <p>There are a number of installation choices at this point:</p>
      <ol>
        <li>You are installing a VCP system which is fully described and managed by input
          model.</li>
        <li>You are installing a VCP system where additional steps need to be performed after
          provisoning the VCP VMs before running <codeph>site.yml</codeph>
        </li>
        <li>You are installing a VCP system where additional steps need to be performed between
          configuring the hypervisor nodes, deploying the hypervisors, provisioning the VCP VMs, or
          running site.yml</li>
      </ol>

    </body>
    <topic id="d1e683">
      <title>Installing a fully input model managed Virtual Control Plane based <keyword keyref="kw-hos"/> Cloud</title>
      <body>
        <p>The plays to setup the hlm-hypervisor have been encapsulated into a single play that can
          be run before running site.yml to bring up the full cloud.</p>
        <codeblock>cd ~/scratch/ansible/next/hos/ansible
ansible-playbook -i hosts/verb_hosts hlm-hypervisor-setup.yml
ansible-playbook -i hosts/verb_hosts site.yml</codeblock>
        <p>This will configure and deploy the Helion lifecycle manager Hypervisor nodes, provision the VCP VMs, and fully
          deploy the <keyword keyref="kw-hos"/> Cloud.</p>
        <!--<note type="note">
          You can confirm that your <keyword keyref="kw-hos"/> build has the necessary patches by grepping
              for <codeph>hlm-hypervisor-setup.yml</codeph>in
            the <codeph>site.yml</codeph> playbook; if it isn't found you will need to choose the
            next installation option.
        </note>-->
        <p> </p>
      </body>
    </topic>
    
    
    <topic id="d1e717">
      <title>Provisioning just the Virtual Control Plane VMs</title>
      <body>
        <p>If your <keyword keyref="kw-hos"/> Cloud installation requires that you perform additional actions between the
          provisoning of the VCP VMs and the running of the site.yml playbook then you can run the
          hlm-hypervisor-setup.yml playbook to provision the VCP VMs only, as follows:</p>
        
        <codeblock>cd ~/scratch/ansible/next/hos/ansible
ansible-playbook -i hosts/verb_hosts hlm-hypervisor-setup.yml</codeblock>
        
        <p>This will configure and deploy the Helion lifecycle manager Hypervisor nodes, and then provision the VCP VMs.
          Once you have performed the additional actions you can complete the installation by
          running the <codeph>site.yml</codeph> playbook.</p>
        
        
        
        <!--<note type="note">
          You can confirm that your <keyword keyref="kw-hos"/> build has the necessary patches by check for the
            existence of the hlm-hypervisor-setup.yml play; if it doesn't exist please proceed to
            the next installation option.
        </note>-->

      </body>
    </topic>
    <topic id="d1e748">
      <title>Manually running each phase of the Virtual Control Plane provisioning</title>
      <body>
        <p>If you need explicit control over when the different phases of the configuration and
          deployment of the HLM Hypervisor nodes and the provisioning of the VCP VMs happen then you
          can run through the following steps, performing any required additional actions before
          moving on to the next step.</p>
      </body>
      
      <topic id="d1e755">
        <title>Configure OS and Networking on HLM Hypervisor nodes</title>
        <body>
          <codeblock>cd ~/scratch/ansible/next/hos/ansible
ansible-playbook -i hosts/verb_hosts osconfig-run.yml --limit=hlm-hypervisors</codeblock>
         <!-- <note type="note">
            If you get ansible error complaining that hlm-hypervisors is unknown, try using
              vm-factories instead; your <keyword keyref="kw-hos"/> build may have been cut during the interim period
              before all the renaming patches landed.
          </note>-->
          <note type="warning">
            The <codeph>--limit</codeph> argument here is very important; without it ansible will
              attempt to run the playbook against all nodes defined in the input model, including
              the VMs that haven't yet been provisioned, and will fail.
          </note>

          <p>This will configure the OS environment and any relevant networking infrastructure on
            the HLM Hypervisor nodes that will be needed to support the associated VCP VMs.</p>
        </body>
      </topic>
      
      <topic id="d1e792">
        <title>Deploy and Configure the HLM Hypervisor nodes</title>
        <body>
          <codeblock>cd ~/scratch/ansible/next/hos/ansible
ansible-playbook -i hosts/verb_hosts hlm-hypervisor-deploy.yml</codeblock>
          <p>This will install and configure any additional software that needs to be in place
            before you can provision the VCP VMs.</p>
          
          <p>Provision the VCP VMs</p>
          
          <codeblock>cd ~/scratch/ansible/next/hos/ansible
ansible-playbook -i hosts/verb_hosts hlm-hypervisor-vms-deploy.yml</codeblock>
          <p>This will provision and start the VCP VMs, but will not perform any additional setup
            operations on them.</p>
          
          <p>When you have completed any additional actuons you can then
              run <codeph>site.yml</codeph> to complete the installation of the <keyword keyref="kw-hos"/> Cloud.</p>
          <!--<note type="warning">
            <p>If you <keyword keyref="kw-hos"/> build is too old, i.e. the <codeph>hlm-hypervisor-*.yml</codeph> playbooks
              don't exist, you may need to use the following commands instead.</p>
          </note>-->
        </body>
      </topic>
    </topic>
    
<!--    <topic id="d1e835">
      <title>Deploying the Hypervisor nodes and VCP VMs using older <keyword keyref="kw-hos"/> build</title>
      <body>
        <ol>
          <li> Run osconfig on the Hypervisor nodes <b>only</b> (use -\-limit).
            <codeblock>cd ~/scratch/ansible/next/hos/ansible
ansible-playbook -i hosts/verb_hosts osconfig-run.yml -\-limit=vm-factories</codeblock>
            Your HLM Hypervisor nodes OS and networking infrastructure will now be configured. </li>
          <li>  Run the new vmfactory-deploy playbook against the same set of nodes. 
            <codeblock>cd ~/scratch/ansible/next/hos/ansible
ansible-playbook -i hosts/verb_hosts vmfactory-deploy.yml</codeblock>
            When this play completes successfully the Hypervisor nodes will be ready to start
            spinning up VMs, with all the required OVS bridges and virsh portgroups in place and
            ready to go. </li>
          <li> Run provisioner.yml to spin up the instances defined in the model:
            <codeblock>cd ~/scratch/ansible/next/hos/ansible
ansible-playbook -i hosts/verb_hosts provisioner.yml</codeblock>
          </li>
        </ol>
        <p>Your VMs should now be up and ready for a Cloud deploy. Ssh to them to make sure. Ping
          won't work - the default <keyword keyref="kw-hos"/> firewall rules disallow it.</p>
        <p>At this point all the nodes in your model should be up and running an operating system
          - the VMs from the qcow2 image described in the previous section, and the baremetal ones
          from Cobbler or some similar means. You're now ready to run site.yml against all nodes in
          the model, in the usual way, to complete the install of your cloud. Simples!</p>
      </body>
    </topic>-->
  </topic>
  
  
  <topic id="nic_mapping_for_hlm_virtual_controllers">
    <title>NIC Mapping for HLM Virtual-Controllers</title>
    <body>
<!--      <p>
        <codeph>The following text, should be used as the basis for a <keyword keyref="kw-hos"/> V4.0 documentation update
          relating to Virtual Control Plane.</codeph>
      </p>-->
      <p>For a bare-metal system the set of <codeph>nic-mappings</codeph> specified
          within <codeph>nic_mappings.yml</codeph> file define the mapping of PCI bus-addresses to
        device name.</p>
      <p>For a HLM Virtual-Controller the set of <codeph>nic-mappings</codeph> specified
          within <codeph>nic_mappings.yml</codeph> file defines the mapping of PCI bus-addresses to
        device name, but is also used to construct the HLM Virtual-Controller VM with network
        interfaces at those PCI bus-addresses.</p>
      <p>
        <ph>When defining the network-interface model for a HLM Virtual-Controller it will become
          apparent how many network interfaces are required for each controller and therefore how
          many NICs need to be defined in the </ph>
        <codeph>nic-mappings</codeph>
        <ph> for this VM. </ph>
      </p>
      <p>It is recommended that you define a separate <codeph>nic-mappings</codeph> for each number
        of NICs that you reference.  <ph>For example the following defines three mappings, one with
          a single NIC, a second with two NICs and a third with eight NICs:</ph>
      </p>
      <codeblock>...
  nic-mappings:
...
 
    - name: HOS-VIRTUAL-ONE-PORT
      physical-ports:
        - logical-name: hed1
          bus-address: '0000:01:01.0'
          type: simple-port
    - name: HOS-VIRTUAL-TWO-PORT
      physical-ports:
        - logical-name: hed1
          type: simple-port
          bus-address: "0000:01:01.0"
        - logical-name: hed2
          type: simple-port
          bus-address: "0000:01:02.0"
 
    - name: HOS-VIRTUAL-EIGHT-PORT
      physical-ports:
        - bus-address: '0000:01:01.0'
          logical-name: hed1
          type: simple-port
        - bus-address: '0000:01:02.0'
          logical-name: hed2
          type: simple-port
        - bus-address: '0000:01:03.0'
          logical-name: hed3
          type: simple-port
        - bus-address: '0000:01:04.0'
          logical-name: hed4
          type: simple-port
        - bus-address: '0000:01:05.0'
          logical-name: hed5
          type: simple-port
        - bus-address: '0000:01:06.0'
          logical-name: hed6
          type: simple-port
        - bus-address: '0000:01:07.0'
          logical-name: hed7
          type: simple-port
        - bus-address: '0000:01:08.0'
          logical-name: hed8
          type: simple-port</codeblock>
    </body>
    <topic id="d1e956">
      <title>Recommended Mappings</title>
      <body>
        <p>It is recommended that the nic-mappings for HLM Virtual-Controllers use the PCI-bus
          addresses of the form <codeph>"0000:01:<codeph>xx</codeph>.0"</codeph>,
            where <codeph>xx</codeph> is the value <codeph>01..1f</codeph> (1..31 in
          hexadecimal).</p>
        <p>This style isolates the virtual NIC interfaces onto PCI bus <codeph>01</codeph> and
          should prevent bus-address clashes with other PCI devices on the system.</p>
        <p> </p>
      </body>
    </topic>
  </topic>
  <topic id="d1e983">
    <title>Notification of actions to be taken post upgrade</title>
    <body>
      <p>As part of an upgrade play actions that need to be taken - that cannot be automatically
        done since sharing the host with other vms will be placed in <b>/var/run/hos</b>.
      </p>
      <p>The actions that are currently notified are</p>
      <pre>/var/run/hos/reload_apparmor</pre>
      <pre>/var/run/hos/reload_libvirtd</pre>
      <pre>/var/run/hos/restart_libvirtd
</pre>
    </body>
  </topic>
  <topic id="d1e1009">
    <title>Reboot of a HLM-Hypervisor node</title>
    <body>
      <p>Rebooting  a HLM-Hypervisor node follows the standard procedure in the <keyword keyref="kw-hos"/> documentation <xref keyref="reboot_cloud_rolling"/>.
      </p>
      
      <p>The sequence here is to target the <keyword keyref="kw-hos"/> vms first then the host itself - then shutdown the
        vms before reboot. Note that there is an ansible host group in verb hosts called &lt;hlm-hypervisor node name&gt;-vms which targets 
        the vms on the node with name of
        hlm-hypervisor-node-name. In this case <b>liam-vcp-cp1-vmf-m1-mgmt-vms</b>
      </p>

      <codeblock>ansible-playbook -i hosts/verb_hosts --limit liam-vcp-cp1-vmf-m1-mgmt-vms hlm-stop.yml
ansible-playbook -i hosts/verb_hosts --limit liam-vcp-cp1-vmf-m1-mgmt hlm-stop.yml
ansible-playbook -i hosts/verb_hosts --limit liam-vcp-cp1-vmf-m1-mgmt hlm-hypervisor-vms-stop.yml # ie. shutdown the vms. </codeblock>
      <p>
        <b>reboot node</b>


      <codeblock>ansible-playbook -i hosts/verb_hosts --limit liam-vcp-cp1-vmf-m1-mgmt hlm-hypervisor-vms-start.yml (the vms should now be on autoboot)
ansible-playbook -i hosts/verb_hosts --limit liam-vcp-cp1-vmf-m1-mgmt-vms hlm-start.yml
<!--
[Gerry Fahy tested and]
I think 1 more step is necessary here i.e. ..-->.
ansible-playbook -i hosts/verb_hosts --limit liam-vcp-cp1-vmf-m1-mgmt hlm-start.yml</codeblock>
      </p>
    </body>
  </topic>
  
  
  <topic id="d1e1053">
    <title>Staged install on HLM-Hypervisor individual play books.</title>
    <body>
      <p>This is a call out of the set of various playbooks to allow for a staged deployment of the
        hlm-hypervisor to allow for NFV vms to be installed as well at an appropriate time. </p>
      <p>Once the node has been installed and the input model created from the NFV seed VM, the next
        stage is to configure this node and deploy the other nodes in the control plane.</p>
      <p>The install of the other BM nodes can follow standard practise of running the cobbler
        deploy and the bm-reimage playbooks - then you need to run the osconfig playbook and
        hlm-hypervisor on these nodes.</p>
      
      <p>This brings up the networking on the nodes and deploys a minimal set of packages to allow
        for virtual machine deployment. This is done by
      
      <codeblock>ansible-playbook -i hosts/verb_hosts --limit hlm-hypervisors osconfig-run.yml
ansible-playbook -i hosts/verb_hosts --limit hlm-hypervisors hlm-hypervisors-deploy.yml</codeblock>
      </p>
      
      <p>At this stage you now have the nodes up with the networking configured and the basic set of
        packages installed to allow for subsequent deploy of the NFV vms.</p>
      <p>Once this is done we then would need to deploy the <keyword keyref="kw-hos"/> vms - this can be done with
      <codeblock>ansible-playbook - i hosts/verb_hosts --limit hlm-hypervisors hlm-hypervisor-vms-deploy.yml</codeblock>
    </p>
      <p>Note that these 3 plays have been encapsulated into a single play that can be run with
        <codeblock>ansible-playbook -i hosts/verb_hosts hlm-hypervisor-setup.yml</codeblock></p>

      <p>At this point you now have all the nodes to be able to deploy the cloud - this can now be
        achieved by running
      <codeblock>ansible-playbook -i hosts/verb_hosts site.yml</codeblock>
       </p>
    </body>
  </topic>
  
  
  <topic id="d1e1125">
    <title>Virtual Controller Replacement</title>
    <body>
      <p>There are several use cases here, all related to repair or replacement of VCP
        infrastructure. Essentially in each case you start by recovering the hypervisor (if
        necessary) and then create new VMs to replace the ones that were lost. Once they complete
        <codeph>hlm-hypervisor-vms-deploy.yml</codeph>, it is the equivalent of having just completed
        <codeph>bm-reimage.yml</codeph> for a physical node replacement, and can proceed with the existing
        instructions for controller recovery.</p>
      <ol>
        <li>Repair or replacement of a single bad VM. One that has been damaged or lost, not just a
          simple reboot.</li>
        <li>Of a single hypervisor, not causing e.g. RabbitMQ to lose quorum or similar effects,
          because all the VMs on this host are part of 3-node clusters and the other nodes are still
          up.</li>
        <li>Disaster recovery - loss of all hypervisors (and therefore control plane VMs)</li>
      </ol>
      <p>
        <b>Use case 1 - single bad VM.</b>
      </p>
      <codeblock>ansible-playbook -i hosts/verb_hosts --limit hlm-hypervisors osconfig-run.yml
ansible-playbook -i hosts/verb_hosts --limit hlm-hypervisors hlm-hypervisors-deploy.yml
ansible-playbook - i hosts/verb_hosts --limit hlm-hypervisors hlm-hypervisor-vms-deploy.yml</codeblock>
      <p>
        <b>Use case 2 - single lost hypervisor but services are still running on the rest of the
          control plane</b>
      </p>
      <p>This starts with recovering the hypervisor, which is very similar to recovering a dead
        physical controller.</p>
      <ol>
        <li>You'll need to update servers.yml and cobbler if the node's mac address or ilo details
          have changed (because of physical repairs/replacements).</li>
        <li> Reimage just this node and then run the hypervisor plays on it
          <codeblock>ansible-playbook -i hosts/localhost bm-reimage.yml --limit=hypervisor1
ansible-playbook -i hosts/verb_hosts --limit hypervisor1 osconfig-run.yml
ansible-playbook -i hosts/verb_hosts --limit hypervisor1 hlm-hypervisors-deploy.yml
ansible-playbook - i hosts/verb_hosts --limit hypervisor1 hlm-hypervisor-vms-deploy.yml</codeblock>
          At this stage, the VMs are up with a bare operating system, and you should follow the physical
          controller recovery instructions from here. </li>
      </ol>
      <p>
        <b>Use case 3 - lost all hypervisors and thus the complete control plane</b>
      </p>
      <p>Currently this use case is not correctly documented, even for a completely physical system
        - see <xref scope="external" format="html" href="https://jira.hpcloud.net/browse/DOCS-2921">DOCS-2921</xref>. 
        However once there is a working procedure for physical it could be adapted using similar techniques to the
        above.</p>
    </body>
  </topic>
  
  
  <topic id="d1e1196">
    <title>Ansible host-specific variables for network-group access</title>
    <body>
      <!--<p>This support is currently in review as: </p>
      <ul>
        <li>
          <xref format="html" scope="external" href="https://review.hpcloud.net/120162"
            >https://review.hpcloud.net/120162</xref> - <xref format="html" scope="external"
            href="http://jira.hpcloud.net/browse/HLM-4664"/>
        </li>
      </ul>-->
      <p>From a 3rd-party view-point there are two important scenarios for accessing the
        network:</p>
      <ul>
        <li>3rd-Party service on a HLM managed Server.</li>
        <li>3rd-Party VM on a HLM-Hypervisor Server.</li>
      </ul>
      <p>Each requires different sub-sets of data to establish the network configuration and
        potentially the end-point to which they need to attach. The following table indicates the set of data that is made available, and the scenarios that
        may find this data useful. The network-group specifications are provided under an
          attribute: <codeph>host.my_network_groups:</codeph> containing a list of dicts, one per
        network-group.</p>
     
      <table>
        <tgroup cols="7">
          <colspec colnum="1" colname="c1"/>
          <colspec colnum="2" colname="c2"/>
          <colspec colnum="3" colname="c3"/>
          <colspec colnum="4" colname="c4"/>
          <colspec colnum="5" colname="c5"/>
          <colspec colnum="6" colname="c6"/>
          <colspec colnum="7" colname="c7"/>
          <thead>
            <row>
              <entry> </entry>
              <entry> </entry>
              <entry> </entry>
              <entry> </entry>
              <entry namest="c5" nameend="c6">Consumed by 3rd-Party scenario</entry>
              <entry> </entry>
            </row>
            <row>
              <entry> </entry>
              <entry>Index</entry>
              <entry>Attribute</entry>
              <entry>Value</entry>
              <entry>3rd-Party VM</entry>
              <entry>3rd-Party Service</entry>
              <entry>Comment</entry>
            </row>
          </thead>
          <tbody>

            <row>
              <entry> </entry>
              <entry>
                <codeph>&lt;network-group-name&gt;</codeph>
              </entry>
              <entry> </entry>
              <entry> </entry>
              <entry>
                <image keyref="check"/>
              </entry>
              <entry>
                <image keyref="check"/>
              </entry>
              <entry>name of the network-group</entry>
            </row>
            <row>
              <entry> </entry>
              <entry> </entry>
              <entry>
                <codeph>network-name</codeph>
              </entry>
              <entry>
                <codeph>&lt;network-name&gt;</codeph>
              </entry>
              <entry>
                <image keyref="check"/>
              </entry>
              <entry>
                <image keyref="check"/>
              </entry>
              <entry>name of the network</entry>
            </row>
            <row>
              <entry> </entry>
              <entry> </entry>
              <entry>
                <codeph>passthrough-device</codeph>
              </entry>
              <entry>
                <codeph>&lt;bridge-name&gt;</codeph>
              </entry>
              <entry>
                <image keyref="check"/>
              </entry>
              <entry> </entry>
              <entry>openvswitch-bridge to attach to for unfiltered access to the specified
                network</entry>
            </row>
            <row>
              <entry> </entry>
              <entry> </entry>
              <entry>
                <codeph>tagged-vlan</codeph>
              </entry>
              <entry>
                <codeph>true/false</codeph>
              </entry>
              <entry>
                <image keyref="check"/>
              </entry>
              <entry> </entry>
              <entry>is this network running tagged</entry>
            </row>
            <row>
              <entry> </entry>
              <entry> </entry>
              <entry>
                <codeph>vlanid</codeph>
              </entry>
              <entry>
                <codeph>&lt;value&gt;</codeph>
              </entry>
              <entry>
                <image keyref="check"/>
              </entry>
              <entry> </entry>
              <entry>which vlanid: is it tagged with</entry>
            </row>
            <row>
              <entry> </entry>
              <entry> </entry>
              <entry>
                <codeph>device</codeph>
              </entry>
              <entry>
                <codeph>&lt;dev-name&gt;</codeph>
              </entry>
              <entry> </entry>
              <entry>
                <image keyref="check"/>
              </entry>
              <entry>Device used for local-services</entry>
            </row>
            <row>
              <entry> </entry>
              <entry> </entry>
              <entry>
                <codeph>address</codeph>
              </entry>
              <entry>
                <codeph>&lt;ip-address&gt;</codeph>
              </entry>
              <entry> </entry>
              <entry>
                <image keyref="check"/>
              </entry>
              <entry>Address only used for local-services</entry>
            </row>
            <row>
              <entry> </entry>
              <entry> </entry>
              <entry>
                <codeph>cidr</codeph>
              </entry>
              <entry>
                <p>
                  <codeph>&lt;cidr&gt;</codeph>
                </p>
              </entry>
              <entry>
                <image keyref="check"/>
              </entry>
              <entry>
                <image keyref="check"/>
              </entry>
              <entry>CIDR in use by this network</entry>
            </row>
            <row>
              <entry> </entry>
              <entry> </entry>
              <entry>
                <codeph>gateway-ip</codeph>
              </entry>
              <entry>
                <codeph>&lt;ip-address&gt;</codeph>
              </entry>
              <entry>
                <image keyref="check"/>
              </entry>
              <entry>
                <image keyref="check"/>
              </entry>
              <entry>gateway-ip in use on this network &lt;optional&gt;</entry>
            </row>
            <row>
              <entry> </entry>
              <entry> </entry>
              <entry>
                <codeph>mtu</codeph>
              </entry>
              <entry>
                <codeph>&lt;mtu&gt;</codeph>
              </entry>
              <entry>
                <image keyref="check"/>
              </entry>
              <entry>
                <image keyref="check"/>
              </entry>
              <entry>the MTU specified or inherited on this network</entry>
            </row>
          </tbody>
        </tgroup>
      </table>
      <p>For Example:</p>
      <codeblock>host:
    ...
    my_network_groups:
        BLS:
        -   address: 10.244.47.102
            cidr: 10.244.47.0/24
            device: br-vlan3537
            gateway-ip: 10.244.47.1
            network-name: BLS-NET
            passthrough-device: br-bond0
            tagged-vlan: true
            vlanid: 3537
        CAN:
        -   cidr: 10.244.45.0/24
            device: br-bond0
            gateway-ip: 10.244.45.1
            network-name: CAN-NET
            passthrough-device: br-bond0
            tagged-vlan: true
            vlanid: 3535
        CLM:
        -   address: 10.244.44.102
            cidr: 10.244.44.0/24
            device: vlan3534
            gateway-ip: 10.244.44.1
            network-name: CLM-NET
            passthrough-device: br-bond0
            tagged-vlan: true
            vlanid: 3534
        PXE:
        -   address: 10.244.43.102
            cidr: 10.244.43.0/24
            device: br-bond0
            gateway-ip: 10.244.43.1
            network-name: PXE-NET
            passthrough-device: br-bond0
            tagged-vlan: false
            vlanid: 3533
        VLAN1-TUL:
        -   device: br-bond1
            network-name: VLAN1-TUL-NET
            passthrough-device: br-bond1
            tagged-vlan: false
        VLAN2-TUL:
        -   device: br-bond1
            network-name: VLAN2-TUL-NET
            passthrough-device: br-bond1
            tagged-vlan: false
        VxLAN-TUL:
        -   cidr: 10.244.48.0/24
            device: br-bond1
            gateway-ip: 10.244.48.1
            network-name: VxLAN-TUL-NET
            passthrough-device: br-bond1
            tagged-vlan: false
            vlanid: 3538</codeblock>
      <p>Which decodes to mean:</p>
      <ul>
        <li>BLS: available locally (<codeph>address: 10.244.47.102</codeph>) and passthrough to
          hosted-VMs (<codeph>
            <codeph>
              <ph>passthrough-device: </ph>
              <codeph>br-bond0,</codeph>
            </codeph> vlanid: 3537, cidr:, gateway: </codeph>
          <ph>)</ph>
        </li>
        <li>CAN: not-available locally, but passthrough to Hosted-VMs (passthrough-device: <codeph>br-bond0<codeph>
              <codeph>,</codeph>
            </codeph> vlanid: 3535, cidr:, gateway: </codeph>)</li>
        <li>CLM: available locally (<codeph>address: 10.244.44.102</codeph>) and passthrough to
          hosted-VMs (passthrough-device: <codeph>br-bond0<codeph>
              <codeph>,</codeph>
            </codeph> vlanid: 3534, cidr:, gateway: </codeph>)</li>
        <li>PXE: <ph>available locally (</ph>
          <codeph>address: 10.244.43.1102</codeph>
          <ph>) and passthrough to hosted-VMs (passthrough-device: </ph>
          <codeph>br-bond0<codeph>
              <codeph>,</codeph>
            </codeph> vlanid: 3533, cidr:, gateway: </codeph>
          <ph>)</ph>
        </li>
        <li>
                 VLAN1-TUL: not available locally, but passthrough to Hosted-VMs (<codeph>passthrough-device: br-bond1,   
          tagged-vlan: false</codeph>)
        </li>
        <li>VLAN2-TUL: not available locally, but passthrough to Hosted-VMs (<codeph>passthrough-device: br-bond1, 
          tagged-vlan: false</codeph>)</li>
        <li>VxLAN-TUL: not available locally, but passthrough to Hosted-VMs
            (<codeph>passthrough-device: 
          br-bond1,
          tagged-vlan: false, cidr:, gateway:</codeph>)
           
        </li>
      </ul>
      <p>
        <ph>A 3rd-Party application can provide an ansible-playbook that processes the specified set
          of ansible-vars and extracts the information required to support the configuration of the
          3rd-party application.</ph>
      </p>
      <p>
        <ph> </ph>
      </p>
    </body>
    <topic id="d1e1813">
      <title>Example 3rd-Party <codeph>Ansible</codeph> for Network Configuration Access</title>
      <body>
        <p>
          <ph>The following is an example ansible playbook to find the name of the
              <codeph>device</codeph> associated with the <codeph>MANAGEMENT</codeph>
            network-group. It can be easily customised to select any known network and any attribute
            of that network shown in the table above.</ph>
        </p>
        <codeblock>#
# (c) Copyright 2016 Hewlett Packard Enterprise Development LP
#
# Licensed under the Apache License, Version 2.0 (the "License"); you may
# not use this file except in compliance with the License. You may obtain
# a copy of the License at
#
# http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
# WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
# License for the specific language governing permissions and limitations
# under the License.
#
# Example playbook to interrogate and recover network specific
# configuration data for Third-Party applications
- name: third-party | network-details | display all available network groups
  debug:
    var: host.my_network_groups
- name: third-party | network-details | select a network group
  set_fact:
    tp_network_group_name: 'MANAGEMENT'
- name: third-party | network-details | display which network group we're interrogating
  debug:
    var: tp_network_group_name
- name: third-party | network-details | get the details of the selected network group
  set_fact:
    tp_network_group_data: "{{ host.my_network_groups | item(tp_network_group_name) }}"
- name: third-party | network-details | display the details of the selected network group
  debug:
    var: tp_network_group_data
- name: third-party | network-details | get the 'device' for the selected network group
  set_fact:
    tp_network_group_device: "{{ tp_network_group_data[0] | item('device') }}"
- name: third-party | network-details | display the 'device' for the selected network group
  debug:
    var: tp_network_group_device</codeblock>

      </body>
    </topic>
  </topic>
  <topic id="d1e1847">
    <title>Ansible host-specific variables for gluster</title>
    <body>
      <p>If disks have been set aside for gluster in the hypervisor input model as outlined earlier,
        then a list of these disks can be found
          in <codeph>host.my_device_groups.gluster.devices</codeph>, for example:</p>
      <codeblock>host:
    my_device_groups:
        gluster:
        -   consumer:
                name: gluster
                suppress_warnings: true
            devices:
            -   name: /dev/sde
            -   name: /dev/sdf
            name: gluster

</codeblock>
    </body>
  </topic>
  <topic id="d1e1863">
    <title>"Empty" HLM Hypervisor Node</title>
    <body>
      <p>In order to create an empty hlm-hypervisor node then specifying  <codeph>hlm-hypervisor: True</codeph> is
        necessary.</p>
      <codeblock>- id: hypervisor1
      ip-addr: 10.225.107.74
      role: HLM-HYPERVISOR-ROLE
      server-group: RACK2
      <b>hlm-hypervisor: True</b>
      nic-mapping: HP-BL460-6PORT
      mac-addr: 9c:7e:96:22:9e:d8
      ilo-ip: 10.1.18.42
      ilo-password: whatever
      ilo-user: whatever</codeblock>
      <p>In addition, if these nodes are being used in scenarios where there are no <keyword keyref="kw-hos"/> servers to
        consume log data etc., you will need to ensure that in the input model the only
        <codeph>common-service-components</codeph> would be the <codeph>lifecycle-manager-target</codeph>.</p>
    </body>
  </topic>
  <topic id="d1e1878">
    <title>Monasca Monitoring of HLM Hypervisors</title>
    <body>
      <p>If your build includes a top-level
          play <codeph>hlm-hyperviros-monitoring-deploy.yml</codeph> then you have the option of
        enabling Monasca monitoring for the HLM Hypervisor nodes by running that playbook as
        follows:</p>
      <codeblock>cd ~/scratch/ansible/next/hos/ansible
ansible-playbook -i hosts/verb_hosts hlm-hypervisor-monitoring-deploy.yml</codeblock>
    </body>
    <topic id="d1e1894">
      <title>Monitored Metrics</title>
      <body>
        <p>Currently two types of metrics are gathered:</p>
        <ol>
          <li>Overall summary state for a HLM Hypervisor node
              -  <codeph>hlm-hypervisor.vcp_vms</codeph>
            <ol>
              <li>Reports 0 indicating healthy so long as all monitored VMs, and associated
                networks, are running/active.</li>
              <li>If any of the VMs or associated networks are not running/active, reports a value
                of 1.</li>
              <li>Associated additional dimensions:<ol>
                  <li>service: hlm-hypervisor</li>
                  <li>component: vcp</li>
                </ol>
              </li>
            </ol>
          </li>
          <li>Summary state for each VCP VM running on a HLM Hypervisor node
              - <codeph>hlm-hypervisor.vcp_vm.&lt;vm_name&gt;</codeph>
            <ol>
              <li>Reports 0 indicating health so long as a VM, and its associated networks, are
                running/active.</li>
              <li>If the VM or any of its networks are not running/active, reports a value of
                1.</li>
              <li>Associated additional dimensions:<ol>
                  <li>service: hlm-hypervisor</li>
                  <li>component: vcp_vm</li>
                  <li>domain: &lt;vm_name&gt;</li>
                </ol>
              </li>
            </ol>
          </li>
        </ol>
      </body>
    </topic>
    <topic id="d1e1964">
      <title>Configured Alarms</title>
      <body>
        <p>Alarms are configured to track these metrics; an alarm will be triggered if the metric
          starts reporting a state of 1.</p>
        <p>If an alarm is triggered the metric's measurement value_meta will include a detail
          section outlining the detected issues.</p>
        <p> </p>
      </body>
    </topic>
    <topic id="d1e1977">
      <title>HLM Hypervisor Monitoring Configuration</title>
      <body>
        <p>The monitoring mechanism is driven by per-VM JSON configuration files in
          <codeph>/etc/hlm-hypervisor/vms</codeph> with the following format:</p>
        <codeblock>{
    "hhv_vm_config": {
        "name": "&lt;hostname&gt;",
        "domain": "&lt;vm_name&gt;",
        "networks": [
            "&lt;vnet_1&gt;",
            "&lt;vnet_2&gt;"
        ],
        "dimensions": {
            "service": "hlm-hypervisor",
            "component": "vcp_vm",
        	"domain": "&lt;vm_name&gt;"
        },
        "check_type": "vm"
    }
}</codeblock>
        <p>The HLM Hypervisor Monasca detection plugin <codeph>hhv.py</codeph>, exposes two
            classes, <codeph>HLMHypervisorSummary</codeph> and<codeph>HLMHypervisorVMs</codeph>,
          which respectively generate the appropriate Monasca configuration to support both types of
          metric.</p>
        <note type="note">
          You can disable either type of metric by removing the associated class from
              the <codeph>hhv_monitor.plugins</codeph> list in
              the <codeph>roles/hlm-hypervisor-monitoring/defaults/main.yml</codeph> file.
        </note>
        <p>The HLM Hypervisor Monasca check plugin, <codeph>hhv.py</codeph>, processes the
          configuration generated by the detection plugin and updates the relevant metrics.</p>

        <!--<p>Extending monitoring to additional VMs</p>-->
       

      </body>
    </topic>
  </topic>
</topic>
