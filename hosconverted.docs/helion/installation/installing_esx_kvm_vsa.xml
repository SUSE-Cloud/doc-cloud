<?xml version="1.0" encoding="UTF-8"?>
<!--Edit status: not edited-->
<!DOCTYPE topic PUBLIC "-//OASIS//DTD DITA Topic//EN" "topic.dtd">
<topic id="install_esx">
  <title><ph conkeyref="HOS-conrefs/product-title"/>Installation for Helion Entry Scale ESX, KVM
    with VSA Model</title>
  <body>
    <!--not tested-->
    <p conkeyref="HOS-conrefs/applies-to"/>
    <p>This document describes the procedures for the deployment of an ESX cloud using the input
      model and adding more ESX hosts to an already activated cluster.</p>
    <p> It contains the following topics:<ul id="ul_wrs_dwq_lt">
        <li>Prerequisite</li>
        <li>Deploy ESX Cloud</li>
        <li>Prepare and deploy ESX</li>
        <li>Prepare and Deploy ESX Computes and OVSvAPPs</li>
      </ul></p>
    <section>
      <note type="important">Before you start your ESX cloud deployment make sure to you read the
        following instructions carefully.</note>
    </section>
    <section conref="installing_kvm.xml#install_kvm/important_notes"/>
    <section conref="installing_kvm.xml#install_kvm/prereqs"/>
    <section id="prereqs">
      <title>Prerequisite</title>
      <p>ESX/vCenter integration is not fully automatic, vCenter administrators are advised of the
        following responsibilities to ensure secure operation:</p>
      <p>
        <ul id="ul_shv_s4b_2t">
          <li>The VMware administrator is responsible for administration of the vCenter servers and
            the ESX nodes using the VMware administration tools. These responsibilities include: <ul
              id="ul_py3_j5l_ft">
              <li>Installing and configuring vCenter server</li>
              <li>Installing and configuring ESX server and ESX cluster</li>
              <li>Installing and configuring shared datastores</li>
              <li>Establishing network connectivity between the ESX network and the HPE Helion
                management network</li>
            </ul></li>
          <li>The VMware administration staff is responsible for the review of vCenter logs. These
            logs are not automatically included in Helion centralized logging.</li>
          <li>Logging levels for vCenter should be set appropriately to prevent logging of the
            password for the Helion message queue.</li>
          <li>The vCenter cluster and ESX Compute nodes must be appropriately backed up.</li>
          <li>Backup procedures for vCenter should ensure that the file containing the Helion
            configuration as part of Nova and Cinder volume services is backed up and the backups
            are protected appropriately.</li>
          <li>Since the file containing the Helion message queue password could appear in the swap
            area of a vCenter server, appropriate controls should be applied to the vCenter cluster
            to prevent discovery of the password via snooping of the swap area or memory dumps.</li>
          <li>It is recommended to have a common shared storage for all the ESXi hosts in a
            particular cluster. </li>
          <li>Ensure that you have enabled HA (High Availability) and DRS (Distributed Resource
            Scheduler) settings in a cluster configuration before running the installation. DRS/HA
            is disabled only for OVSvApp. This is done so that it does not move to a different host.
            If you do not enable DRS/HA prior to installation then you will not be able to disable
            it only for OVSvApp. As a result DRS/HA can migrate OVSvApp to different host, which
            will create a network loop.<p>
              <note>No two clusters should have the same name across datacenters in a given
                vCenter.</note>
            </p></li>
          <li>L3 HA VRRP is enabled by default.</li>
          <li>Ensure that the cluster is not inside a folder in vCenter.</li>
        </ul>
      </p>
    </section>
    <section id="deployCloud"><b>Deploy ESX Cloud</b><p>At a high level, here are the steps to
        configure and deploy ESX cloud:</p><p><image href="../../media/esx/esx_deploy.jpg"
          id="image_kjt_zlm_ft"/></p></section>
    <section>
      <title>Procedure to Deploy ESX cloud</title>
    </section>
    <p>The following topics in this section explain how to deploy ESX cloud.</p>
    <section conref="installing_kvm.xml#install_kvm/setup_deployer"/>
    <section id="Configure">
      <title>Prepare and Deploy Cloud Controllers</title>
      <ol id="ol_c1w_pfl_ft">
        <li>Setup your configuration files, as follows: <ol id="ol_sgn_tqy_2x">
            <li>Copy the example configuration files into the required setup directory and edit them
              as required:
                <codeblock>cp -r ~/helion/examples/entry-scale-esx-kvm-vsa/* ~/helion/my_cloud/definition/</codeblock><p>See
                a sample set of configuration files in the
                  <codeph>~/helion/examples/entry-scale-esx-kvm-vsa</codeph> directory. The
                accompanying README.md file explains the contents of each of the configuration
                files. </p>You can deploy ESX with OVS vApps in EON or NoOp driver in EON. <ul
                id="ul_twm_qhn_fx">
                <li>For ESX with OvsvApps in EON, modify the input model as per your
                  environment.</li>
                <li>For NoOp with EON, modify the input model as mentioned in <xref
                    href="esx/noops_driver.xml#install_esx"/>.</li>
              </ul><p>
                <note>If you want to use a dedicated deployer node in your ESX deployment, add
                    <b>eon-client</b> service-component, to manage vCenter via EON operation from
                  the deployer node, in the <codeph>control_plane.yml</codeph> file as shown in the
                  following example. </note>
                <codeblock> clusters:
        - name: cluster0
          cluster-prefix: c0
          server-role: HLM-ROLE
          service-components:
            - lifecycle-manager
            - <b>eon-client</b> 
            ...</codeblock>
              </p></li>
          </ol></li>
        <li>Commit your cloud deploy configuration to the <xref href="using_git.xml"> local git
            repo</xref>, as follows: <codeblock>cd ~/helion/hos/ansible
git add -A
git commit -m "My config or other commit message"</codeblock>
          <note>This step needs to be repeated any time you make changes to your configuration files
            before you move onto the following steps. See <xref href="using_git.xml">Using Git for
              Configuration Management</xref> for more information.</note></li>
      </ol>
    </section>
    <p>Then you need to run the following commands to complete your configuration. These commands
      also verify your configuration.</p>
    <ol id="ol_og4_jy4_st">
      <li>Run the following playbook which confirms that there is iLo connectivity for each of your
        nodes so that they are accessible to be re-imaged in a later step:
        <codeblock>cd ~/helion/hos/ansible
ansible-playbook -i hosts/localhost bm-power-status.yml</codeblock></li>
      <li>Run the configuration processor, as follows:
        <codeblock>cd ~/helion/hos/ansible
ansible-playbook -i hosts/localhost config-processor-run.yml</codeblock></li>
    </ol>
    <p>If you receive an error during either of these steps then there is an issue with one or more
      of your configuration files. We recommend that you verify that all of the information in each
      of your configuration files is correct for your environment and then commit those changes to
      git using the instructions above.</p>

    <section conref="installing_kvm.xml#install_kvm/provision"/>
    <section conref="installing_kvm.xml#install_kvm/config_processor"/>
    <section conref="installing_kvm.xml#install_kvm/deploy"/>
    <section id="prepAndDeploy">
      <title>Prepare and Deploy ESX Computes and OVSvAPPs </title>
      <p>The following sections describe the procedure to install and configure ESX compute and
        OVSvAPPs on vCenter.</p>
      <ul id="ul_lns_fjl_ft">
        <li><xref href="#install_esx/prepare_esx_cloud_deployment" format="dita">Preparation for ESX
            Cloud Deployment</xref></li>
      </ul>
    </section>
    <section id="prepare_esx_cloud_deployment"><b>Preparation for ESX Cloud Deployment</b><p>This
        section describes the procedures to prepare and deploy the ESX computes and OVSvAPPs for
        deployment. </p><p>
        <ol id="ol_xpc_zqs_ft">
          <li>Login to the lifecycle manager.</li>
          <li>Source <codeph>service.osrc</codeph>.</li>
          <li><xref href="#install_esx/register-vcenter" format="dita">Register a vCenter
              Server</xref></li>
          <li><xref href="#install_esx/activate-cluster" format="dita">Activate Clusters</xref><ol
              id="ol_isb_wcc_vt">
              <li><xref href="#install_esx/modify-volume-config" format="dita">Modify the Volume
                  Configuration File</xref></li>
              <li><xref href="#install_esx/commit-your-cloud" format="dita">Commit your Cloud
                  Definition</xref></li>
              <li><xref href="#install_esx/deploy-compute-proxy-ovsvapps" format="dita">Deploy ESX
                  Compute Proxy and OVSvApps</xref></li>
            </ol></li>
        </ol>
      </p></section>
    <p><b>Manage vCenters and Clusters</b></p>
    <p>The following section describes the detailed procedure on managing the vCenters and
      clusters.</p>
    <p id="register-vcenter"><b>Register a vCenter Server</b><note>
        <ul id="ul_u11_bpb_zx">
          <li>If ESX related roles are added in the input model after the controller cluster is up,
            you must run <codeph>hlm-reconfigure.yml</codeph> before adding the eon
            resource-manager.</li>
          <li>If you choose to use special charaters (&amp; ! ; " ' () | \ &lt;> ) during the
            vCenter registration, you must provide those special characters within quotes to read as
            a string. For example: "2the!Moon". </li>
        </ul>
      </note></p>
    <p>vCenter provides centralized management of virtual host and virtual machines from a single
        console.<ol id="ol_xdj_5js_ft">
        <li>Add a vCenter using EON python
            client.<codeblock># eon resource-manager-add [--name &lt;RESOURCE_MANAGER_NAME>] --ip-address &lt;RESOURCE_MANAGER_IP_ADDR> --username &lt;RESOURCE_MANAGER_USERNAME> --password &lt;RESOURCE_MANAGER_PASSWORD> [--port &lt;RESOURCE_MANAGER_PORT>] --type &lt;RESOURCE_MANAGER_TYPE></codeblock><p>where:
              <ul id="ul_bp3_yjs_ft">
              <li>Resource Manager Name - the identical name of the vCenter server.</li>
              <li>Resource Manager IP address - the IP address of the vCenter server.</li>
              <li>Resource Manager Username - the admin privilege username for the vCenter.</li>
              <li>Resource Manager Password - the password for the above username.</li>
              <li>Resource Manager Port - the vCenter server port. By default it is 443.</li>
              <li> Resource Manager Type - specify <codeph>vcenter</codeph>.<note type="important"
                  >Please do not change the vCenter Port unless you are certain it is required to do
                  so.</note></li>
            </ul></p><p><b>Sample
            Output:</b><codeblock><codeph># eon resource-manager-add --name vc01 --ip-address 10.1.200.38 --username administrator@vsphere.local --password init123# --port 443 --type vcenter</codeph>
+-------------+--------------------------------------+
| Property    | Value                                |
+-------------+--------------------------------------+
| ID          | BC9DED4E-1639-481D-B190-2B54A2BF5674 |
| IPv4Address | 10.1.200.38                          |
| Name        | vc01                                 |
| Password    | &lt;SANITIZED>                          |
| Port        | 443                                  |
| Type        | vcenter                              |
| Username    | administrator@vsphere.local          |
+-------------+--------------------------------------+</codeblock></p></li>
      </ol></p>
    <section><b>Show vCenter</b><ol id="ol_wh5_rsb_lt">
        <li>Show vCenter using EON python client.
              <codeblock># eon resource-manager-show &lt;RESOURCE_MANAGER_ID></codeblock><p><b>Sample
              Output:</b><codeblock><codeph># eon resource-manager-show BC9DED4E-1639-481D-B190-2B54A2BF5674</codeph>
+-------------+--------------------------------------+
| Property    | Value                                |
+-------------+--------------------------------------+
| Clusters    | Cluster1, virtClust, Cluster2        |
| Datacenters | DC1, DC2                             |
| ID          | BC9DED4E-1639-481D-B190-2B54A2BF5674 |
| IPv4Address | 10.1.200.38                          |
| Name        | vc01                                 |
| Password    | &lt;SANITIZED>                          |
| Port        | 443                                  |
| Type        | vcenter                              |
| Username    | administrator@vsphere.local          |
+-------------+--------------------------------------+</codeblock></p></li>
      </ol></section>
    <section id="register-network"><b>Create and edit an activation template</b>
      <p>This involves getting a sample network information template and modifying the details of
        the template. You will use the template to register the cloud network configuration for the
        vCenter.</p><p>
        <ol>
          <li>Execute the following command to generate a sample activation
                template:<codeblock>eon get-activation-template [--filename &lt;ACTIVATION_JSON_NAME>] --type &lt;RESOURCE_TYPE></codeblock><p><b>Sample
                Output:</b></p><p>
              <codeblock>eon get-activation-template --filename activationtemplate.json --type esxcluster
---------------------------------------------------------------
Saved the sample network file in /home/user/activationtemplate.json
---------------------------------------------------------------</codeblock>
            </p></li>
          <li>Change to the <codeph>/home/user/</codeph>
            directory:<codeblock>cd /home/user/</codeblock></li>
          <li>Modify the template (json file) as per your environment. See <xref
              href="install_entryscale_esx_kvm_vsa_template_json.xml#topic4797cgikdjm">Sample
              activationtemplate.json File for Helion Entry Scale ESX, KVM with VSA
            Model</xref>.</li>
        </ol>
      </p></section>
    <!--<section id="CG" product="CG">
      <note>Modify the following <xref
          href="../carrier_grade/Sample-activationtemplate-json-File-for-ESX-Compute.xml#topic_swf_rvv_1y"
          > template (json file)</xref> to deploy esx for Helion OpenStack Carrier Grade S1 single
        and multi-region input models.</note>
    </section>-->
    <section id="activate-cluster"><b>Activate Clusters</b><p>This involves using the activation
        template to register the cloud network configuration for the vCenter.<note>The minimum disk
          space required for a cluster activation is (number of hosts in that cluster +1 )* 45
          GB.</note></p><p>This process spawns one compute proxy VM per cluster and one OVSvApp VM
        per host and configures the networking as defined in the JSON template. This process also
        updates the input model with the service VM details and triggers the ansible playbooks on
        the new nodes.</p><ol id="ol_tbr_4l4_vv">
        <li>Activate the cluster for the selected
            vCenter.<codeblock># eon resource-activate &lt;RESOURCE_ID> --config-json /home/user/&lt;ACTIVATION_JSON_NAME></codeblock><note>Activating
            the cluster in a datacenter requires you to provide an activation template JSON. You can
            provide the same activation template or a new activation template to activate the
            subsequent clusters.</note></li>
        <li>Execute the following command to view the status of the
            cluster:<codeblock>eon resource-list
+--------------------------------------+-----------+-------------+--------------------------------------+------------+-------+------------+------------+
| ID                                   | Name      | Moid        | Resource Manager ID                  | IP Address | Port  | Type       | State      |
+--------------------------------------+-----------+-------------+--------------------------------------+------------+-------+------------+------------+
| 1228fce5-df5d-445c-834e-ae633ac7e426 | Cluster2  | domain-c184 | BC9DED4E-1639-481D-B190-2B54A2BF5674 | UNSET      | UNSET | esxcluster | imported   |
| 469710f6-e9f2-48a4-aace-1f00cbd60487 | virtClust | domain-c943 | BC9DED4E-1639-481D-B190-2B54A2BF5674 | UNSET      | UNSET | esxcluster | activated  |
| a3003a32-6e3a-4d89-a072-ec64a4247fb0 | Cluster1  | domain-c21  | BC9DED4E-1639-481D-B190-2B54A2BF5674 | UNSET      | UNSET | esxcluster | imported   |
+--------------------------------------+-----------+-------------+--------------------------------------+------------+-------+------------+------------+</codeblock><p>When
            the state is <codeph>activated</codeph>, the input model is updated with the service VM
            details, a git commit has been performed, the required playbooks and the post activation
            checks are completed successfully.</p></li>
      </ol></section>
    <section id="modify-volume-config"><b>Modify the Volume Configuration File</b>
      <p>Once the cluster is activated you must configure the volume.</p><p>Perform the following
        steps to modify the volume configuration files:</p><ol id="ol_bhx_n5p_st">
        <li>Change the directory. The <codeph>cinder.conf.j2</codeph> is present in following
          directories
            :<codeblock>cd /home/stack/helion/hos/ansible/roles/_CND-CMN/templates</codeblock><p>OR<codeblock>cd /home/stack/helion/my_cloud/config/cinder</codeblock></p><p>It
            is recommended to modify the <codeph>cinder.conf.j2</codeph> available in
              <codeph>/home/stack/helion/my_cloud/config/cinder</codeph></p></li>
        <li>Modify the <codeph>cinder.conf.j2</codeph> as follows:
          <codeblock># Configure the enabled backends
enabled_backends=&lt;unique-section-name>

# Start of section for VMDK block storage
#
# If you have configured VMDK Block storage for cinder you must
# uncomment this section, and replace all strings in angle brackets
# with the correct values for vCenter you have configured. You
# must also add the section name to the list of values in the
# 'enabled_backends' variable above. You must provide unique section
# each time you configure a new backend.

#[&lt;unique-section-name>]
#vmware_api_retry_count = 10
#vmware_tmp_dir = /tmp
#vmware_image_transfer_timeout_secs = 7200
#vmware_task_poll_interval = 0.5
#vmware_max_objects_retrieval = 100
#vmware_volume_folder = cinder-volumes
#volume_driver = cinder.volume.drivers.vmware.vmdk.VMwareVcVmdkDriver
#vmware_host_ip = &lt;ip_address_of_vcenter>
#vmware_host_username = &lt;vcenter_username>
#vmware_host_password = &lt;password>
#
#volume_backend_name = &lt;vmdk-backend-name>
#
# End of section for VMDK block storage</codeblock></li>
      </ol></section>
    <section id="commit-your-cloud"><b>Commit your Cloud Definition</b>
      <p>
        <ol>
          <li> Add the cloud deployment definition to git
            :<codeblock>cd /home/stack/helion/hos/ansible;
git add -A;
git commit -m 'Adding ESX Configurations or other commit message';</codeblock></li>
          <li>Prepare your environment for deployment:
            <codeblock>ansible-playbook -i hosts/localhost config-processor-run.yml;
ansible-playbook -i hosts/localhost ready-deployment.yml;
cd /home/stack/scratch/ansible/next/hos/ansible;</codeblock></li>
        </ol>
      </p></section>
    <section id="deploy-compute-proxy-ovsvapps"><b>Configuring VMDK block storage</b><p>Execute the
        following command to configure VMDK block
        storage:<codeblock>ansible-playbook -i hosts/verb_hosts cinder-reconfigure.yml</codeblock></p><!--The variable <b>esx-ovsvapp</b> and <b>esx-compute</b> must be taken from the <b>name</b> key in the <codeph>resource-nodes</codeph> section in the <codeph>data/control_plane.yml</codeph> file (<codeph>/home/stack/helion/my_cloud/definition/data/control_plane.yml</codeph>) removed as per Satya's comment. --><p>
        If there are more backends (like VSA) defined, you must create and use specific volume type
        to ensure that volume is created in ESX, as shown
        below:<codeblock># cinder type-create "ESX VMDK Storage"
...
# cinder type-key "ESX VMDK Storage" set volume_backend_name=&lt;name of VMDK backend selected during installation>
...
# cinder create --volume-type "ESX VMDK Storage" 1
...</codeblock></p></section>
    <section>
      <title>Validate the block storage</title>
      <p>You can validate that the VMDK block storage is added to the cloud successfully using the
        following
        command:<codeblock># cinder service-list</codeblock><codeblock>+------------------+-------------------------+------+---------+-------+----------------------------+-----------------+
|      Binary      |           Host          | Zone |  Status | State |         Updated_at         | Disabled Reason |
+------------------+-------------------------+------+---------+-------+----------------------------+-----------------+
|  cinder-backup   |    ha-volume-manager    | nova | enabled |   up  | 2016-06-14T08:44:51.000000 |        -        |
| cinder-scheduler |    ha-volume-manager    | nova | enabled |  down | 2016-06-13T08:47:07.000000 |        -        |
| cinder-scheduler |    ha-volume-manager    | nova | enabled |   up  | 2016-06-14T08:44:57.000000 |        -        |
|  cinder-volume   |    ha-volume-manager    | nova | enabled |  down | 2016-06-13T07:36:41.000000 |        -        |
|  cinder-volume   | ha-volume-manager@vmdk1 | nova | enabled |   up  | 2016-06-14T08:44:55.000000 |        -        |
|  cinder-volume   |  ha-volume-manager@vsa1 | nova | enabled |   up  | 2016-06-14T08:44:50.000000 |        -        |
+------------------+-------------------------+------+---------+-------+----------------------------+-----------------+</codeblock></p>
    </section>
    <section id="verify">
      <title>Validate the compute</title>
      <p>You can validate that the ESX compute cluster is added to the cloud successfully using the
        following command:</p>
      <p>
        <codeblock>#  nova service-list</codeblock>
        <codeblock>+-----+------------------+------------------------------+----------+---------+-------+----------------------------+-----------------+
| Id  | Binary           | Host                         | Zone     | Status  | State | Updated_at                 | Disabled Reason |
+-----+------------------+------------------------------+----------+---------+-------+----------------------------+-----------------+
| 3   | nova-conductor   | esxhos-joh-core-m1-mgmt      | internal | enabled | up    | 2016-03-30T12:57:47.000000 | -               |
| 63  | nova-scheduler   | esxhos-joh-core-m1-mgmt      | internal | enabled | up    | 2016-03-30T12:57:43.000000 | -               |
| 66  | nova-conductor   | esxhos-joh-core-m2-mgmt      | internal | enabled | up    | 2016-03-30T12:57:48.000000 | -               |
| 111 | nova-conductor   | esxhos-joh-core-m3-mgmt      | internal | enabled | up    | 2016-03-30T12:57:41.000000 | -               |
| 129 | nova-scheduler   | esxhos-joh-core-m3-mgmt      | internal | enabled | up    | 2016-03-30T12:57:41.000000 | -               |
| 132 | nova-consoleauth | esxhos-joh-core-m1-mgmt      | internal | enabled | up    | 2016-03-30T12:57:44.000000 | -               |
| 135 | nova-scheduler   | esxhos-joh-core-m2-mgmt      | internal | enabled | up    | 2016-03-30T12:57:47.000000 | -               |
| 138 | nova-compute     | esxhos-joh-esx-comp0001-mgmt | nova     | enabled | up    | 2016-03-30T12:57:41.000000 | -               |
+-----+------------------+------------------------------+----------+---------+-------+----------------------------+-----------------+</codeblock>
      </p>
      <p>Verify the Hypervisor hostname and its status.</p>
      <codeblock># nova hypervisor-list</codeblock>
      <codeblock>+----+-------------------------------------------------+-------+----------+
| ID | Hypervisor hostname                             | State | Status   |
+----+-------------------------------------------------+-------+----------+
| 9  | domain-c40.9FDCFA66-6677-42A1-83FF-16DC32448021 | up    | enabled  |
+----+-------------------------------------------------+-------+----------+</codeblock>
    </section>
    <section id="verify-neutron">
      <title>Validate the neutron</title>
      <p>You can validate that the ESX compute cluster is added to the cloud successfully using the
        following command:</p>
      <p>
        <codeblock># neutron agent-list</codeblock>
        <codeblock>+--------------------------------------+--------------------+---------------------------------+-------+----------------+---------------------------+
| id                                   | agent_type         | host                            | alive | admin_state_up | binary                    |
+--------------------------------------+--------------------+---------------------------------+-------+----------------+---------------------------+
| 097bdbc3-108c-41ca-8b52-9d249f65077f | Metadata agent     | esxhos-joh-core-m3-mgmt         | :-)   | True           | neutron-metadata-agent    |
| 2b255256-9505-489e-93bf-0d37f7ff83e4 | DHCP agent         | esxhos-joh-core-m3-mgmt         | :-)   | True           | neutron-dhcp-agent        |
| 43843bcb-f929-4a42-a1e1-207ff97dc09e | OVSvApp Agent      | esxhos-joh-esx-ovsvapp0002-mgmt | :-)   | True           | ovsvapp-agent             |
| 4f0be657-fe9e-4bab-bef8-d8e688b6573e | L3 agent           | esxhos-joh-core-m3-mgmt         | :-)   | True           | neutron-vpn-agent         |
| 6db7d585-604e-4205-b29c-b9684bfb0cb2 | OVSvApp Agent      | esxhos-joh-esx-ovsvapp0001-mgmt | :-)   | True           | ovsvapp-agent             |
| 700f1a18-d237-4605-bf54-145951bd12db | DHCP agent         | esxhos-joh-core-m1-mgmt         | :-)   | True           | neutron-dhcp-agent        |
| a723fb23-2c87-4f69-b272-643df870f0b6 | DHCP agent         | esxhos-joh-core-m2-mgmt         | :-)   | True           | neutron-dhcp-agent        |
| bd512ed5-9f39-4ad6-a505-148f58cf2e64 | L3 agent           | esxhos-joh-core-m1-mgmt         | :-)   | True           | neutron-vpn-agent         |
| e3d4b8be-077e-4503-bed2-031871f3829e | Open vSwitch agent | esxhos-joh-core-m2-mgmt         | :-)   | True           | neutron-openvswitch-agent |
| e5d75917-1d26-4443-99f4-45d8c574e3f0 | Open vSwitch agent | esxhos-joh-core-m1-mgmt         | :-)   | True           | neutron-openvswitch-agent |
| e6a40540-adad-4ee7-b194-db18ac7287bd | Metadata agent     | esxhos-joh-core-m1-mgmt         | :-)   | True           | neutron-metadata-agent    |
| e734a20e-159c-450b-9fb8-dc46824ef12e | Metadata agent     | esxhos-joh-core-m2-mgmt         | :-)   | True           | neutron-metadata-agent    |
| f019c268-a6bd-4f66-8a46-6762c720a5bd | L3 agent           | esxhos-joh-core-m2-mgmt         | :-)   | True           | neutron-vpn-agent         |
| fc83c8dd-ead4-4ad5-aed3-95ab41e567bc | Open vSwitch agent | esxhos-joh-core-m3-mgmt         | :-)   | True           | neutron-openvswitch-agent |
+--------------------------------------+--------------------+---------------------------------+-------+----------------+---------------------------+</codeblock>
      </p>
    </section>
  </body>
</topic>
