<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE topic PUBLIC "-//OASIS//DTD DITA Topic//EN" "concept.dtd" >
<topic xml:lang="en-us" id="knownissues50">
  <title>Known Issues in this Release</title>
  <body>
    
    <section id="BUGZILLA-1068107">
      <title>Upgrading from <keyword keyref="kw-hos-phrase-404"/></title>
      <note type="warning">If your system is currently on version <keyword keyref="kw-hos-phrase-404"/> the correct upgrade path is to <keyword keyref="kw-hos-phrase-501"/></note>
    </section>
    
    <section id="DOCS-4101">
      <title>OpenStack Security Advisory OSSA-2017-001</title>
      <p>The OpenStack Security Advisory OSSA-2017-001 which describes CVE-2017-2592 was released on
        January 26, 2017. It discloses a security defect which causes Keystone tokens to be logged
        when a Neutron API call encounters unexpected error conditions. When this happens, the log
        entry that contains the token is propagated to the centralized logging facility. The result
        is that anyone with access to the centralized logging facility or the local log file
        containing the token could use the token to impersonate the owner of the token until the
        token expires. Tokens expire in 4 hours by default. Neutron is the only <keyword
          keyref="kw-hos"/> service affected by this issue. The error conditions which cause this
        token to leak into the logs are not expected to happen frequently. However, customers should
        be aware of this risk. Customers may wish to further limit the access to the centralized
        logging facility and watch for suspicious activity that could indicate usage of a stolen
        Keystone token to impersonate another user. The Keystone token timeout value could also be
        changed but it is not recommended; testing would be required to ensure things still work in
        the customer environment if the token timeout value is reduced.
      </p>
    </section>
    
    <section id="BUGZILLA-1061611">
      <title><keyword keyref="kw-hos"/> Directory Umask</title>
      <p><keyword keyref="kw-hos"/> directory umask is 0022. Changing this value may cause unstable
        internal communication in the <keyword keyref="kw-hos"/> product and will negatively affect upgrade.</p>
    </section>
    
    
    <section id="BUGZILLA-1060915">
      <title>Adding Top-Level Domains with Designate</title>
      <p>In <keyword keyref="kw-hos-phrase"/> and earlier ( with designate V2.0.1 ) when using
        Designate to add Top-Level Domains (that controls who can create zones), there is a caching
        defect. When an entry is made to the tld list (<codeph>openstack tld create --name abc.net</codeph>), in
        order to take effect the designate-central on all the controllers needs to be restarted. 
        <codeblock>$ sudo systemctl restart designate-central</codeblock>
      </p>
    </section>
    
    <section id="BUGZILLA-1057463">
      <title>Adding New Flavors After Upgrade</title>
      <p>
        After upgrading to <keyword keyref="kw-hos-phrase"/> from <keyword keyref="kw-hos-phrase-40"/>
        you may experience an error when attempting to add new flavors. </p>
      <p> Example:
        <codeblock>$ openstack flavor create --ram 512 --vcpus 1 --disk 0 --ephemeral 0 --swap 0 --public m1.tinystu
        
Not all flavors have been migrated to the API database (HTTP 409) (Request-ID: req-bdae0a10-e6c3-42d4-9a58-b4438fcc4309)</codeblock>
      </p>
      <p>This is caused by the time fields associated with the flavor not being updated correctly.
        You can view the time fields by selecting all from the instance_types table in the nova
        mysql database.
        <codeblock>$ sudo mysql
$ use nova
$ select * from instance_types;</codeblock> 
      </p>
       <p>The time fields <b>created_at</b>, <b>updated_at</b> or <b>deleted_at</b> may have
         invalid dates or not all of the fields are NULL.
       </p>
      <p>To resolve the new flavors issues the time fields must all be set to NULL.
        <codeblock>$ sudo mysql
$ use nova
$ update instance_types set created_at=NULL, updated_at=NULL, deleted_at=NULL;</codeblock>
      </p>
      <p>Once the tables are updated, exit mysql and run the following command.
        <codeblock>$ sudo /opt/stack/service/nova-api/venv/bin/nova-manage --config-dir /opt/stack/service/nova-api/etc/nova db online_data_migrations</codeblock>
      </p>
      <p>You can now create a new flavors in <keyword keyref="kw-hos-phrase"/>. </p>
    </section>
    
    <section id="DOCS-4188">
      <title>Using VSA as a Cinder Backend</title>
      <p>When using VSA as a Cinder backend it is necessary to make some changes to the kvm-hypervisor.conf.j2 file with the following steps:
      <ol>
        <li>Log into the lifecycle manager</li>
        <li>Edit the <codeph>~/helion/my_cloud/config/nova/kvm-hypervisor.conf.j2</codeph> file.</li>
        <li>Remove the following lines under the libvert section: 
          <codeblock>volume_use_multipath = True</codeblock></li>
        <li>Commit the configuration to the local git repo
          <codeblock>cd ~/helion/hos/ansible
git add -A
git commit -m "My config or other commit message"</codeblock>
        </li>
        <li>Run the configuration processor
          <codeblock>cd ~/helion/hos/ansible
ansible-playbook -i hosts/localhost config-processor-run.yml</codeblock>
        </li>
        <li>Use the playbook below to create a deployment directory
          <codeblock>cd ~/helion/hos/ansible
ansible-playbook i hosts/localhost ready-deployment.yml</codeblock>
        </li>
        <li>Run the Nova reconfigure playbook
            <codeblock>cd ~/scratch/ansible/next/hos/ansible
ansible-playbook -i hosts/verb_hosts nova-reconfigure.yml</codeblock>
          </li>
        </ol>
      This step is not needed if Cinder backend is HP 3PAR.</p>
      <p>When using storage solutions other than 3PAR or VSA, please check with the storage provider
        whether the <b>volume_use_multipath = True</b> line should be removed.</p>
    </section>
    
    <section id="DOCS-4187">
      <title>Vertica Log Rotation</title>
      <p>Vertica database logs are not rotated automatically for non-primary cluster nodes. As a
        result, Vertica logs on those nodes may grow continuously, eventually filling the filesystem
        to its capacity. This issue can be patched by logging into each node with Vertica installed
        and run the following command. <note>Vertica nodes are all listed as members of the FND-VDB
          group in the deployer's
            <codeph>~/scratch/ansible/next/hos/ansible/hosts/verb_hosts</codeph> file. </note>
        <codeblock>echo -e '#!/bin/sh\nlogrotate /opt/vertica/config/logrotate_base.conf' | sudo tee /etc/cron.daily/vertica-logrotate; sudo chmod 755 /etc/cron.daily/vertica-logrotate</codeblock>
        While this step is not necessary on the Vertica cluster's primary node, it is also not
        harmful. This command is idempotent and future-proof with regard to upgrading to later
          <keyword keyref="kw-hos"/> versions. </p>
    </section>
    
    <section id="DOCS-4124">
      <title>VPNaaS does not support SHA-384 and SHA-512</title>
      <p>The VPNaaS plugin limits the ipsec and ike auth algorithm to <b>SHA-1</b> and
          <b>SHA-256</b>. If you attempt to  add a new driver (for example,  a hardware VPN
        gateway), and the new driver supports additional auth algorithm, such as <b>SHA2-384</b>,
          <b>SHA2-512</b>, it cannot be integrated with the current VPNaaS plugin.</p>
    </section>
    
    <section id="unknown">
      <title>SLES - No kdump initial ramdisk found</title>
      <p>If you are deploying SLES 12 nodes and you see the following exception during the initial
        boot of those nodes: <codeblock>No kdump initial ramdisk found....</codeblock> No
        further action is required. Please see: <xref
          href="https://www.novell.com/support/kb/doc.php?id=7018428" format="html" scope="external"
        /> for additional information:
        <note>This exception may also occur by running <codeph>journalctl -u kdump.service</codeph></note>
      </p>
    </section>
    
    <section id="DOCS-3871">
      <title>Manual Cleaning</title>
      <p>Manual Cleaning (Raid Configuration) is not supported if the target disk type is SATA SSD.</p>
    </section>
    
    <section id="DOCS-4145">
      <title>Transparent VLAN</title>
      <p>In <keyword keyref="kw-hos-phrase"/> Transparent VLAN is not supported.</p>
    </section>
    
    <section id="DOCS-4155">
      <title>Disaster Recovery Panel</title>
      <p>Although the Disaster Recovery panel can be viewed when logged in as an <b>admin</b> user,
        only the <b>hlm_backup</b> user is the supported user of this feature. The <b>admin</b> user
        is unable to view the default backup jobs or create new jobs. Therefore, all backup and recovery actions should
        be performed only when logged in as the <b>hlm_backup</b> user.</p>
    </section>
    
    <section id="DOCS-4148">
      <title>Update ironic-conductor.conf.j2 file to be able to connect to Swift</title>
      <p>Ironic conductor uses credentials from the <codeph>[swift]</codeph> section of the config
        file, and falls back to <codeph>[keystone_authtoken]</codeph> section if no credentials are
        specified in the <codeph>[swift]</codeph> section.</p>
      <p>As a work-around, add the following to the <codeph>/ironic-conductor.conf.j2</codeph> file.
        <codeblock>auth_type = password
auth_url = {{ ironic_cnd_admin.identity_uri }}/v3
username = {{ ironic_swift.glance_user }}
password = {{ ironic_swift.glance_password}}
project_name = {{ ironic_swift.glance_tenant }}
default_domain_name = default</codeblock>
      </p>
    </section>
    
    <section id="BUGZILLA-1049731">
      <title>Attaching a volume with Device Name in Horizon</title>
      <p>In Horizon the screen for attaching a volume to an instance requires a <b>Device Name</b>.
        This field is editable but cannot be empty or it will revert to the default value.</p>
      <p>
        To attach a volume to an instance without specifying a Device Name, use the command line (CLI). </p>
    </section>
    
    <section id="DOCS-4157">
      <title>Ironic: ILO Firmware</title>
      <p>In <keyword keyref="kw-hos-phrase"/> the ILO Firmware version needs to be minimum 2.30 for
        agent_ilo driver.</p>
    </section>
    
    <section id="DOCS-4149">
      <title>Ironic 'shreading' (cleaning) may take a long time depending on the disk size/performance.</title>
      <p>During deployment, with <codeph>automated_cleaning</codeph> set to True and when the Nova instance is
        deleted, it may take an unusually long time to finish or may even seem as though it's hung
        depending on the disk performance/size.</p>
      <p>Example Log msg in the ironic-conductor.log:</p>
      <codeblock>2017-05-25 18:08:37.648 40436 DEBUG ironic.drivers.modules.agent_base_vendor
[req-89ff4f08-c13f-49f9-b761-f0e0a5611cd1 - - - - -] Cleaning command status for node
84f57c65-caf5-4a4a-941f-82d27afc93e6 on step {u'priority': 10, u'interface': u'deploy',
u'reboot_requested': False, u'abortable': True, u'step': u'erase_devices'}: None
continue_cleaning /opt/stack/venv/ironic-20170522T163729Z/lib/python2.7/site-packages/ironic/drivers/modules/agent_base_vendor.py:347</codeblock>
      <p>For more information on OpenStack Ironic Node cleaning, see: <xref
          href="https://docs.openstack.org/developer/ironic/deploy/cleaning.html" format="html"
          scope="external"
        >https://docs.openstack.org/developer/ironic/deploy/cleaning.html</xref>
      </p>
    </section>
    
    <section id="DOCS-4177">
      <title>Ironic Multi-Tenancy Conversion</title>
      <p>Ironic Multi-tenancy is only supported as a fresh install of <keyword
          keyref="kw-hos-phrase"/>. An existing Ironic flat environment to multi-tenancy environment
        conversion is not supported. </p>
    </section>
    
    <section id="DOCS-4146">
      <title>Do not enable HEAT Auditing</title>
      <p>Audit log is disabled by default and cannot be enabled due to a Keystone middleware defect in Newton.</p>
    </section>
    
    <section id="DOCS-4163">
      <title>Ops Console Notification Panel</title>
      <p>If there are no Alarm notification methods configured, the Ops Console notifications tab
        will not allow creation of new notification methods. It is recommended to not delete the
        default notification method if no other methods have been created.</p>
      <p>You can work around this issue by creating a new notification method using the <xref
        href="https://docs.openstack.org/cli-reference/monasca.html" format="html"
        scope="external">Monasca command-line</xref>, after which the UI will function again.
      </p>
    </section>
    
    <section id="DOCS-4143">
      <title>LBaaS</title>
      <p>LBaaS Hot-Plugin is not supported with DPDK.</p>
      <p>LBaaS <codeph>load_balancer_expiry_age</codeph> value in the housekeeping section of the
        configuration for octavia-housekeeping service needs to be configured appropriately to the
        default <codeph>value=604800</codeph>.</p>
    </section>
    
    <section id="DOCS-4185">
      <title>F5 Loadbalancer Support</title>
      <p>If you are planning to upgrade from <keyword keyref="kw-hos-phrase-30"/> or <keyword keyref="kw-hos-phrase-40"/>,
        please contact F5 and HPE PointNext to determine which F5 drivers have been certified for
        use with Helion OpenStack. Loading drivers not certified by HPE may result in catastrophic
        failure of your cloud deployment. The last tested versions are 8.0.8 for <keyword keyref="kw-hos"/> 3.x and 9.0.3
        for <keyword keyref="kw-hos"/> 4.x . More information is expected in 4th quarter 2017, including the correct
        drivers for <keyword keyref="kw-hos"/> 5.x.</p>
    </section>
    
    <section id="VNETCORE-2909">
      <title>VMs fail to get an IP after upgrade</title>
      <p>When a VMs failed to get an IP after an upgrade, apply the following work-around:</p>
      <p>After running the rabbitmq-disaster-recovery on a controller node, check the
        ownership of <codeph>/var/run/neutron/lock/neutron-iptables</codeph>. If it is not owned by
          <b>neutron:neutron</b>, use the chown command to change the ownership to
          <b>neutron:neutron</b>: 
          <codeblock>sudo chown neutron:neutron /var/run/neutron/lock/neutron-iptables </codeblock>
      </p>
    </section>
    
    <section id="IRONIC-634">
      <title>Persistent failures during the early phase of boot</title>
      <p>If you are experiencing intermittent or persistent failures during the early phase of boot
        process for certain classes of machines (e.g. DL360 Gen8 and DL160 Gen9), the likely issue
        is DHCP timeouts due to slow convergence of STP. 
      </p>
      <p>You can work around this issue by enabling rapid spanning tree
        protocol on the switch. If this is not an option due to conservative loop detection
        strategies, rebuild ipxe images with higher DHCP timeouts, or alternatively maintain link
        status to the server NICs during power cycles. </p>
    </section>
    
    <section id="MAG-69">
      <title>Swarm cluster behind proxy fails</title>
      <p>Swarm cluster is not support behind a proxy even with -http(s)-proxy parameters
        specified.</p>
    </section>
    
    <section id="VNETCORE-2274">
      <title>Neutron Quota Failures</title>
      <p>The options <codeph>--security_group</codeph> and <codeph>--security_group_rule</codeph>
        are not valid command options for the <codeph>neutron quota-update</codeph> command. Please
        use following options: <ul>
          <li>-security-group</li>
          <li>security-group-rule</li>
        </ul>
      </p>
    </section>
    
    <section id="DOCS-4122">
      <title>Starting/Stopping Swift</title>
      <p>The command line (CLI) cmd, <codeph>swift-init</codeph> is not supported. To start/stop
        Swift use the Ansible playbooks, <codeph>swift-start.yml</codeph> and
          <codeph>swift-stop.yml</codeph>.</p>
    </section>
    
    <section id="HNOV-999">
      <title>Live Migrations</title>
      <p>Live Migration from  non-HLinux nodes including SLES nodes to HLinux nodes are not supported.</p>
    </section>
    
  </body>
</topic>
