<?xml version="1.0" encoding="UTF-8"?>
<!--Edit status: ready for edit (Nancy)-->
<!DOCTYPE topic PUBLIC "-//OASIS//DTD DITA Topic//EN" "topic.dtd">
<topic id="DCN_ml2">
  <title><ph conkeyref="HOS-conrefs/product-title"/>Bringing up a Cloud with DCN ml2 Mechanism
    Driver</title>

  <body>
    <section id="DCN_downloads"><title>Getting the required packages</title><p/>
      <b>Summary of files needed</b>
      <table>
        <tgroup cols="4">
          <colspec colnum="1" colname="c1"/>
          <colspec colnum="2" colname="c2"/>
          <colspec colnum="3" colname="c3"/>
          <colspec colnum="4" colname="c4"/>
          <thead>
            <row>
              <entry>Source</entry>
              <entry>Package name</entry>
              <entry>Required files</entry>
              <entry>Notes</entry>

            </row>
          </thead>
          <tbody>

            <row>
              <entry>ISWS</entry>
              <entry>DCN-VRS-KVM-4.0R3</entry>
              <entry>
                <p>nuage-openvswitch-common_4.0.3-25_all.deb</p>
                <p>nuage-openvswitch-switch_4.0.3-25_amd64.devb</p>
                <p>nuage-openvswitch-datapath-dkms_4.0.3-25_all.deb</p>
                <p>nuage-python-openvswitch_4.0.3-25_all.deb</p>
              </entry>
              <entry> </entry>
            </row>
            <row>
              <entry>ISWS</entry>
              <entry>DVN-VSC-4.0R3</entry>
              <entry>vsc_singledisk.qcow2</entry>
              <entry>VSC disk image</entry>
            </row>
            <row>
              <entry>ISWS</entry>
              <entry>DCN-VSD-QCOW-4.0R3</entry>
              <entry>VSD-4.0.3_26-hp.qcow2</entry>
              <entry>VSD disk image</entry>
            </row>
            <row>
              <entry>ISWS</entry>
              <entry>Nuage-PreRelease-DOC-MIBs-OSTACK-VSG-Cloudmgt-4.0R3</entry>
              <entry>
                <p>nuage-openstack-upgrade-4.0.3-21.tar.gz</p>
                <p>nuage-openstack-neutron_8.0.0-4.0.3-21_all.deb</p>
                <p>nuagenetlib_8.0.0-4.0.3-21_all.deb</p>
                <p>nuage-metadata-agent_4.0.3-25_all.deb</p>
              </entry>
              <entry>rename to nuage-openstack-upgrade.tar.gz</entry>
            </row>
            <row>
              <entry>ISWS</entry>
              <entry>Nuage-PreRelease-DOC-MIBs-OSTACK-VSG-Cloudmgt-4.0R1</entry>
              <entry>nuage-metadata-agent_4.0.1-4_all.deb</entry>
              <entry outputclass="highlightThis">VNETCORE-2587 workaround</entry>
            </row>
            <row>
              <entry>PyPi</entry>
              <entry><xref format="html"  
                  href="https://pypi.python.org/packages/64/61/079eb60459c44929e684fa7d9e2fdca403f67d64dd9dbac27296be2e0fab/configobj-5.0.6.tar.gz"
                  >configobj-5.0.6.tar.gz</xref>
              </entry>
              <entry>*</entry>
              <entry>(Do not unpack)</entry>
            </row>
            <row>
              <entry>PyPi</entry>
              <entry><xref  format="html"  
                  href="https://pypi.python.org/packages/ce/63/f42f5aa951ebf2c8dac81f77a8edcc1c218640a2a35a03b9ff2d4aa64c3d/uuid-1.30.tar.gz"
                  >uuid-1.30.tar.gz</xref></entry>
              <entry>*</entry>
              <entry>(Do not unpack)</entry>
            </row>
            <row>
              <entry>Debian</entry>
              <entry><xref format="html"  
                  href="http://ftp.fr.debian.org/debian/pool/main/p/protobuf-c/libprotobuf-c0_0.14-1+b1_amd64.deb"
                  >libprotobuf-c0</xref></entry>
              <entry>*</entry>
              <entry> </entry>
            </row>
          </tbody>
        </tgroup>
      </table>
      <p><b>NOTE</b>: To download the Nuage-PreRelease-DOC-MIBs-OSTACK-VSG-Cloudmgt-4.0* packages,
        you will need to be allowed to view and dowload "Tightly Controlled Internal Access" code.
        If you are not permitted to view such code, this mean you don't have the permissions. To get
        the permission to access "Tightly Controlled Internal Access" code, contact the ISWS
        webmaster. The URL is found on the "Contact Web Master" link of the page when you open<xref
          format="html"  
          href="http://hpntceqtlsprod.rose.rdlabs.hpecorp.net/Index.aspx"
          >http://hpntceqtlsprod.rose.rdlabs.hpecorp.net/Index.aspx</xref>.</p>
    </section>
    <section id="download_instructions"><title>Detailed Instructions</title>
      <ol>
        <li>Go to the ISWS Page (<xref outputclass="highlightThis"  format="html"  
            href="http://hpntceqtlsprod.rose.rdlabs.hpecorp.net/Index.aspx"
            >http://hpntceqtlsprod.rose.rdlabs.hpecorp.net/Index.aspx</xref>). There are no direct
          download links, so this step needs to be done manually. <ol>
            <li>Search for and download <b>DCN-VRS-KVM-4.0R3</b></li>
            <li>Get the following nuage*.deb files from
                DCN-VRS-KVM-4.0R3.zip/<b>DCN-VRS-4.0.3-25-ubuntu.14.04.tar.gz</b>
              <ol>
                <li> nuage-openvswitch-common_4.0.3-25_all.deb</li>
                <li> nuage-openvswitch-switch_4.0.3-25_amd64.devb</li>
                <li> nuage-openvswitch-datapath-dkms_4.0.3-25_all.deb</li>
                <li> nuage-python-openvswitch_4.0.3-25_all.deb</li>
              </ol></li>
            <li>Search for and download <b>DCN-VSC-4.0R3</b>
              <ol>
                <li>Get vsc_singledisk.qcow2 from the package</li>
              </ol></li>
            <li>Also on the ISWS page, download
                <b>Nuage-PreRelease-DOC-MIBs-OSTACK-VSG-Cloudmgt-4.0R3</b></li>
            <li>Get the following files from
                Nuage-PreRelease-DOC-MIBs-OSTACK-VSG-Cloudmgt-4.0R3.zip<b>/DCN-openstack-4.0.3.tar.gz</b>
              <ol>
                <li> nuage-openstack-upgrade-4.0.3-21.tar.gz <ol>
                    <li> Rename <b>nuage-openstack-upgrade-4.0.3-21.tar.gz</b> to
                        <b>nuage-openstack-upgrade.tar.gz</b><codeblock outputclass="nomaxheight">mv nuage-openstack-upgrade-4.0.3-21.tar.gz nuage-openstack-upgrade.tar.gz</codeblock></li>
                  </ol></li>
                <li> nuage-openstack-neutron_8.0.0-4.0.3-21_all.deb</li>
                <li> nuagenetlib_8.0.0-4.0.3-21_all.deb</li>
                <li> nuage-metadata-agent_4.0.3-25_all.deb</li>
              </ol></li>
            <li><xref outputclass="highlightThis"  format="html"   href="jira.hpcloud.net/browse/VNETCORE-2587"/>
              workaround: Get the following file from
              Nuage-PreRelease-DOC-MIBs-OSTACK-VSG-Cloudmgt-4.0R1 <ol>
                <li> nuage-metadata-agent_4.0.1-4_all.deb <pre/></li>
              </ol></li>
          </ol></li>
        <li>Download the following files from PyPi:</li>
      </ol>
      <ul>
        <li><xref  format="html"  
            href="https://pypi.python.org/packages/64/61/079eb60459c44929e684fa7d9e2fdca403f67d64dd9dbac27296be2e0fab/configobj-5.0.6.tar.gz"
            >configobj-5.0.6.tar.gz</xref>
        </li>
        <li><xref  format="html"  
            href="https://pypi.python.org/packages/ce/63/f42f5aa951ebf2c8dac81f77a8edcc1c218640a2a35a03b9ff2d4aa64c3d/uuid-1.30.tar.gz"
            >uuid-1.30.tar.gz</xref></li>
        <li>
          <xref format="html"  
            href="https://pypi.python.org/packages/ff/a9/5751cdf17a70ea89f6dde23ceb1705bfb638fd8cee00f845308bf8d26397/httplib2-0.9.2.tar.gz#md5=bd1b1445b3b2dfa7276b09b1a07b7f0e"
            >httplib2-0.9.2.tar.gz</xref></li>
      </ul><codeblock outputclass="nomaxheight">wget https://pypi.python.org/packages/64/61/079eb60459c44929e684fa7d9e2fdca403f67d64dd9dbac27296be2e0fab/configobj-5.0.6.tar.gz
wget https://pypi.python.org/packages/ce/63/f42f5aa951ebf2c8dac81f77a8edcc1c218640a2a35a03b9ff2d4aa64c3d/uuid-1.30.tar.gz
wget https://pypi.python.org/packages/ff/a9/5751cdf17a70ea89f6dde23ceb1705bfb638fd8cee00f845308bf8d26397/httplib2-0.9.2.tar.gz</codeblock>
      <p>3. Currently we need to have libprotobuf-c0 debian package (until 4.0.R5 version come out).
        The package can be downloaded at the following URL: </p>
      <ul>
        <li>
          <xref outputclass="highlightThis"  format="html"  
            href="http://ftp.fr.debian.org/debian/pool/main/p/protobuf-c/libprotobuf-c0_0.14-1+b1_amd64.deb"
            >http://ftp.fr.debian.org/debian/pool/main/p/protobuf-c/libprotobuf-c0_0.14-1+b1_amd64.deb</xref><codeblock outputclass="nomaxheight">wget http://ftp.fr.debian.org/debian/pool/main/p/protobuf-c/libprotobuf-c0_0.14-1+b1_amd64.deb</codeblock></li>
      </ul>
    </section>
    <section id="d1e361"><title>VSD Setup</title>

      <p><!--Reference: Nuagenetworks VSP Install Guide Release 4.0.R2: <xref outputclass="highlightThis"  format="html"
           
          href="https://wiki.hpcloud.net/download/attachments/58956116/Nuage-VSP-4.0.R3-Install-Guide.pdf?version=1&amp;modificationDate=1470764034000&amp;api=v2"
          >Nuage-VSP-4.0.R3-Install-Guide.pdf</xref>.--></p>
      <p>Assumption: The network needed for deploying the cloud has been created. In this example,
        this means the networks needed for padawan (padawan-vagrant0 to padawan-vagrant7) has been
        created. The DNS server for padawan-vagrant0 is running. It doesn't matter if the padawan
        cloud is already running. </p>
      <p>For padawan, the VSD's and VSC's MGMT network is libvirt-vagrant (IP 192.168.121.0/24) ,
        the VSC's control network is padawan-vagrant0 (IP 192.168.245.0/24)</p>
      <ol>
        <li>In the design, the VSD's MGMT IP address is 192.168.121.252, VSC's MGMT IP address is
          192.168.121.253. VSD's control network is 192.168.245.252).</li>
        <li>Downloaded the Package_&lt;ID&gt;_DCN-VSD-QCOW-4.0R3.zip from the ISWS website.</li>
        <li>Extracted the file DCN-VSD-QCOW-4.0R3.zip.</li>
        <li>From DCN-VSD-QCOW-4.0R3.zip, extract DCN-VSD-4.0.3_HP_r4.3_26-QCOW.tar.gz.</li>
        <li>Then untarball DCN-VSD-4.0.3_HP_r4.3_26-QCOW.tar.gz. Copy VSD-4.0.3_26-hp.qcow2 to
          /home/DCN-VSD</li>
        <li> Run the following command: to bring up the VSD VM as root: <ol>
            <li> The VSD communicates with the neutron-api server by issuing curl requests over the
              management network. For padawan, the VNIC is created over the "padawan-vagrant0"
              network
              (192.168.245.0/24).<codeblock outputclass="nomaxheight"># cd /home/DCN-VSD
# sudo cp VSD-4.0.3_26-hp.qcow2 /var/lib/libvirt/images
# sudo chown libvirt-qemu:kvm /var/lib/libvirt/images/VSD-4.0.3_26-hp.qcow2
# sudo virt-install --connect qemu:///system -n my-vsd -r 12000 --os-type=linux \
  --os-variant=rhel6 --disk path=/var/lib/libvirt/images/VSD-4.0.3_26-hp.qcow2,device=disk,bus=virtio,format=qcow2 \
  --network network='padawan-vagrant0',mac=RANDOM,model='virtio' \
  --vcpus=2 --graphics vnc,listen=0.0.0.0 --noautoconsole --import</codeblock>
            </li>
          </ol></li>
        <li> (Optional) If you want to change other characteristics of the VSD VM, shut it down now.
          Otherwise, go to step 9:<codeblock outputclass="nomaxheight">virsh destroy my-vsd</codeblock></li>
        <li> (Optional) From virt-manager, change the characteristics you want.</li>
        <li> Reboot the VM, after doing steps 6 and 7:<codeblock outputclass="nomaxheight">virsh start my-vsd</codeblock></li>
        <li> Login to the VSD VM via the serial
          console:<codeblock outputclass="nomaxheight">virsh console my-vsd</codeblock></li>
        <li>Log in to the vsd as root, password Alcateldc</li>
        <li> Edit the /etc/sysconfig/network-scripts/ifcfg-eth0 file. Make sure the VSD's IP address
          is one that won't be assigned to a VM created by vagrant. I used an address in the range
          of 192.168.121.250 -
          254.<codeblock outputclass="nomaxheight">DEVICE="eth0"
NM_CONTROLLED="no"
ONBOOT="yes"
TYPE="Ethernet"
BOOTPROTO="static"
IPV6_AUTOCONF=no
IPV6_FAILURE_FATAL=no
IPADDR=192.168.121.252
GATEWAY=192.168.121.1
NETMASK=255.255.255.0</codeblock></li>
        <li> Restart the network:<codeblock outputclass="nomaxheight">/etc/init.d/network restart</codeblock></li>
        <li> Verify the networking by trying to ping to DNS server for the padawan
          cloud<codeblock outputclass="nomaxheight">ping 192.168.121.1</codeblock></li>
        <li> Edit /etc/sysconfig/network to set the
          hostname:<codeblock outputclass="nomaxheight">NETWORKING=yes
HOSTNAME=myvsd.example.com</codeblock></li>
        <li> Edit the /etc/hosts file: add an entry for myvsd.example.com also add the VSC MGMT IP
          address as
          well:<codeblock outputclass="nomaxheight">127.0.0.1 localhost localhost.localdomain localhost4 localhost4.localdomain4
::1 localhost localhost.localdomain localhost6 localhost6.localdomain6
### The two lines above are the defaults from OS.
### Add one line for each vsd host below with FQDN as the first entry.
### You may add aliases but they must come after the FQDN.
192.168.121.252 myvsd.example.com   xmpp_myvsd.example.com
192.168.121.253 vsc.example.com</codeblock></li>
        <li> Reboot the VSD. After the reboot verify the hostname is setup correctly</li>
        <li> Verify that ssh is working by ssh into the VSD from the padawan
          host.<codeblock outputclass="nomaxheight">$HOME/DCN-VSD$ ssh root@192.168.121.252
root@192.168.245.252's password:
Last login: Tue Jul 19 17:20:02 2016 from 192.168.121.1
Welcome to VSD. (4.0.3_26)
[root@myvsd ~]#</codeblock></li>
        <li> Edit /etc/ntp.conf - add ntp servers: 192.168.121.3, and ntp
          hp.net.<codeblock outputclass="nomaxheight"> ...
# Use public servers from the pool.ntp.org project.
# Please consider joining the pool (http://www.pool.ntp.org/join.html).
server 192.168.121.3 iburst
server ntp.hp.net iburst
 ...</codeblock></li>
        <li> On the padawan host, edit /var/lib/libvirt/dnsmasq/vagrant-libvirt.addnhosts, add the
          lines "192.168.121.252 myvsd.example.com" and "192.168.121.252 xmpp_myvsd.example.com, and
          192.168.121.253 vsc.example.com".
          <codeblock outputclass="nomaxheight">192.168.121.252 myvsd.example.com
192.168.121.252 xmpp_myvsd.example.com
192.168.121.253 vsc.example.com
</codeblock></li>
        <li> Need to put a SRV record into the dnsmasq config file
          (/var/lib/libvirt/dnsmasq/vagrant-libvirt.conf) for the dnsmasq managing the 192.168.121
          network. Add the following lines to the
          file:<codeblock outputclass="nomaxheight">address=/myvsd.example.com/192.168.121.252
srv-host=_xmpp-client._tcp.example.com,myvsd.example.com,5222</codeblock></li>
        <li> Remember the PID of the dnsmasq process for the 192.168.121 network. Also note down the
          command line for the dnsmasq process for the 192.168.121 network. It is needed to rerun
          the dnsmasq for the
          network.<codeblock outputclass="nomaxheight">$ ps -ef | grep dnsmasq
 
libvirt+  1810     1  0 Jun21 ?        00:00:12 /usr/sbin/dnsmasq --conf-file=/var/lib/libvirt/dnsmasq/default.conf
stack    33685 33669  0 10:49 pts/9    00:00:00 grep --color=auto dnsmasq
libvirt+ 36632     1  0 Aug17 ?        00:00:35 /usr/sbin/dnsmasq --conf-file=/var/lib/libvirt/dnsmasq/vagrant-libvirt.conf
libvirt+ 51408     1  0 Aug17 ?        00:00:00 /usr/sbin/dnsmasq --conf-file=/var/lib/libvirt/dnsmasq/deployerincloud-vagrant0.conf
libvirt+ 51517     1  0 Aug17 ?        00:00:00 /usr/sbin/dnsmasq --conf-file=/var/lib/libvirt/dnsmasq/deployerincloud-vagrant1.conf
libvirt+ 51624     1  0 Aug17 ?        00:00:00 /usr/sbin/dnsmasq --conf-file=/var/lib/libvirt/dnsmasq/deployerincloud-vagrant2.conf
libvirt+ 51732     1  0 Aug17 ?        00:00:00 /usr/sbin/dnsmasq --conf-file=/var/lib/libvirt/dnsmasq/deployerincloud-vagrant3.conf
libvirt+ 51839     1  0 Aug17 ?        00:00:00 /usr/sbin/dnsmasq --conf-file=/var/lib/libvirt/dnsmasq/deployerincloud-vagrant4.conf
libvirt+ 51946     1  0 Aug17 ?        00:00:00 /usr/sbin/dnsmasq --conf-file=/var/lib/libvirt/dnsmasq/deployerincloud-vagrant5.conf
libvirt+ 52052     1  0 Aug17 ?        00:00:00 /usr/sbin/dnsmasq --conf-file=/var/lib/libvirt/dnsmasq/deployerincloud-vagrant6.conf
libvirt+ 52161     1  0 Aug17 ?        00:00:00 /usr/sbin/dnsmasq --conf-file=/var/lib/libvirt/dnsmasq/deployerincloud-vagrant7.conf</codeblock></li>
        <li> Kill the dnsmasq process for the network.<codeblock outputclass="nomaxheight">kill 36632</codeblock></li>
        <li> Run dnsmasq for the network For instance, if the vagrant-libvirt is the one for the
          192.168.121 network:<codeblock outputclass="nomaxheight">$ /usr/sbin/dnsmasq --conf-file=/var/lib/libvirt/dnsmasq/vagrant-libvirt.conf</codeblock>
          <ph/></li>
        <li> On the vsd, validate the XMPP service is recognized. For
          example:<codeblock outputclass="nomaxheight">[root@vsd ~]# dig @192.168.121.1 SRV _xmpp-client._tcp.example.com
 
; &lt;&lt;&gt;&gt; DiG 9.8.2rc1-RedHat-9.8.2-0.37.rc1.el6_7.7 &lt;&lt;&gt;&gt; @192.168.121.1 SRV _xmpp-client._tcp.example.com
; (1 server found)
;; global options: +cmd
;; Got answer:
;; -&gt;&gt;HEADER&lt;&lt;- opcode: QUERY, status: NOERROR, id: 3116
;; flags: qr aa rd ra; QUERY: 1, ANSWER: 1, AUTHORITY: 0, ADDITIONAL: 1
 
;; QUESTION SECTION:
;_xmpp-client._tcp.example.com.    IN    SRV
 
;; ANSWER SECTION:
_xmpp-client._tcp.example.com. 0 IN    SRV    0 0 5222 myvsd.example.com.
 
;; ADDITIONAL SECTION:
myvsd.example.com.    0    IN    A    192.168.121.252
 
;; Query time: 0 msec
;; SERVER: 192.168.121.1#53(192.168.121.1)
;; WHEN: Fri Aug 19 14:22:22 2016
;; MSG SIZE  rcvd: 98
 
[root@vsd ~]# dig +noall +an @192.168.121.1 SRV _xmpp-client._tcp.example.com
_xmpp-client._tcp.example.com. 0 IN    SRV    0 0 5222 myvsd.example.com.</codeblock>
        </li>
        <li> On the padawan, controller nodes, find the DNS server IP
          address:<codeblock outputclass="nomaxheight">stack@padawan-ccp-c1-m1-mgmt:~$ cat /etc/resolv.conf
nameserver 192.168.121.1</codeblock></li>
        <li>Edit /etc/resolv.conf - the dns server is 192.168.121.1</li>
        <li>reboot the VSD</li>
        <li>login to the VSD.</li>
        <li> Verify the nslookup is
          working:<codeblock outputclass="nomaxheight">[root@myvsd ~]#  /opt/vsd/install/checkDNS.sh -a myvsd.example.com -1 -v 4
PASS: myvsd.example.com resolves to IP address  192.168.121.252</codeblock></li>
        <li> Install VSD standalone: run
          /opt/vsd/vsd-install.sh<codeblock outputclass="nomaxheight">[root@myvsd ~]# /opt/vsd/vsd-install.sh
-------------------------------------------------------------
  V I R T U A L I Z E D  S E R V I C E S  D I R E C T O R Y
  version 4.0.3_26
-------------------------------------------------------------
VSD supports two configurations:
1)  HA, consisting of 3 redundant installs of VSD.
2)  Standalone, where all services are installed on a single machine.
Is this a redundant (r) or standalone (s) installation [r|s]? (default=s): s
WARN: Memory is at 11626 MB; 16GB is preferred
Deploy VSD on single host myvsd.example.com  ...
VSD node:      myvsd.example.com
Continue [y|n]? (default=y): y
Starting VSD deployment. This may take as long as 20 minutes in some situations ...
VSD package deployment and configuration DONE. Please initialize VSD.
DONE: VSD deployed.
Starting VSD initialization. This may take as long as 20 minutes in some situations ...
A self-signed certificate has been generated to get you started using VSD.
VSD installed and the services have started.
[root@myvsd ~]#</codeblock></li>
        <li>Install the VSD license. From the padawan host, open up a web browser and open up the
          URL <xref outputclass="highlightThis"  format="html"   href="https://192.168.245.252:8443/"
            >https://192.168.245.252:8443</xref>. The VSD screen should come up. Login to the VSD
          using: User Name=csproot, Password=csproot, Organization=csp</li>
        <li>Ask Muhammad Haseeb (<xref outputclass="highlightThis"  format="html"  
            href="mailto:muhammad.haseeb@hpe.com">muhammad.haseeb@hpe.com</xref>) to get you a VSD
          license for doing development work. </li>
        <li>To install the license, go to the Platform Configuration Screen. On the left hand side
          of the screen, click on the icon for licenses. The click on the '+' on the lower left
          corner of the screen.to add the license. A "New License" window pops up. Paste the license
          code into the License text box. Then press "Create". Then the VSD should be ready for
          use.</li>
        <li>Add the OSadmin user to the VSD. The "OSadmin" user needs to be a member of the CMS
          group.</li>
      </ol>
    </section>
    <section id="d1e636"><title>Input model changes</title>
      <p/></section>
    <section id="d1e645"><title>control_plane.yml</title>
      <p>Path: ~/helion/my_cloud/definition/data/<b>control_plane.yml</b></p>
      <p>Summary of changes:</p>
      <ul>
        <li> Remove (all nodes): neutron-vpn-agent, neutron-metadata-agent,
          neutron-openvswitch-agent, neutron-l3-agent, neutron-dhcp-agent </li>
        <li>Under configuration-data, add DCN-CONFIG-CP1</li>
        <li>Add (controller node): dcn-ml2</li>
        <li>Add (compute nodes): dcn-vrs, dcn-metadata-agent</li>
        <li>Add (VRS-G node): dcn-vrsg</li>
        <li>Add (VM host node(s) to host VSC(s)): hypervisor role to host VSC VM(s) running dcn-vsc
          and dcn-vsc-data services.
          <p outputclass="expandcode"><b>Open | Close control_plane.yml</b></p>
          <p outputclass="hiddencode">
          <codeblock outputclass="nomaxheight">#
# (c) Copyright 2015-2016 Hewlett Packard Enterprise Development LP
#
# Licensed under the Apache License, Version 2.0 (the "License"); you may
# not use this file except in compliance with the License. You may obtain
# a copy of the License at
#
# http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
# WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
# License for the specific language governing permissions and limitations
# under the License.
#
---
  product:
    version: 2
  control-planes:
    - name: ccp
      control-plane-prefix: ccp
      region-name: region1
      failure-zones:
        - AZ1
        - AZ2
        - AZ3
      configuration-data:
        - OCTAVIA-CONFIG-CP1
        - NEUTRON-CONFIG-CP1
        # DCN: Add DCN-CONFIG-CP1 to be consumed
        - DCN-CONFIG-CP1
      common-service-components:
        - lifecycle-manager-target
        - freezer-agent
        - stunnel
        - monasca-agent
        - logging-rotate
        - logging-producer
      load-balancers:
        - provider: ip-cluster
          name: internal-lb
          tls-components:
            - default
          components:
            - vertica
            - rabbitmq
            - nova-metadata
          roles:
            - internal
            - admin
          cert-file: helion-internal-cert
        - provider: ip-cluster
          name: external-lb
          external-name: myhelion.test
          tls-components:
            - default
          roles:
            - public
          cert-file: my-public-padawan-cert
      clusters:
        - name: cluster0
          cluster-prefix: c0
          server-role: HLM-ROLE
          member-count: 1
          allocation-policy: strict
          service-components:
            - lifecycle-manager
            - tempest
            # Required for testing in (run-test.sh)
            - openstack-client
            - ceilometer-client
            - cinder-client
            - designate-client
            - glance-client
            - heat-client
            - ironic-client
            - keystone-client
            - neutron-client
            - nova-client
            - swift-client
            - monasca-client
            - barbican-client
            - ntp-client
        - name: cluster1
          cluster-prefix: c1
          server-role: CONTROLLER-ROLE
          member-count: 3
          allocation-policy: strict
          service-components:
            - ntp-server
            - swift-ring-builder
            - mysql
            - ip-cluster
            - keystone-api
            - rabbitmq
            - glance-api:
                ha_mode: false
                glance_stores: 'file'
                glance_default_store: 'file'
            - glance-registry
            - cinder-api
            - cinder-scheduler
            - cinder-volume
            - cinder-backup
            - nova-api
            - nova-scheduler
            - nova-conductor
            - nova-console-auth
            - nova-novncproxy
            - neutron-server
            - neutron-ml2-plugin
            # DCN: Remove incompatible services
            #- neutron-vpn-agent
            #- neutron-dhcp-agent
            #- neutron-metadata-agent
            #- neutron-openvswitch-agent
            # DCN: Add dcn-ml2
            - dcn-ml2
            - octavia-api
            - octavia-health-manager
            - horizon
            - swift-proxy
            - memcached
            - swift-account
            - swift-container
            - swift-object
            - heat-api
            - heat-api-cfn
            - heat-api-cloudwatch
            - heat-engine
            - ceilometer-api
            - ceilometer-polling
            - ceilometer-agent-notification
            - ceilometer-common
            - zookeeper
            - kafka
            - spark
            - vertica
            - storm
            - monasca-api
            - monasca-persister
            - monasca-notifier
            - monasca-threshold
            - monasca-transform
            - logging-server
            - ops-console-web
            - ops-console-monitor
            - freezer-api
            - barbican-api
            - barbican-worker
            - designate-api
            - designate-central
            - designate-pool-manager
            - designate-zone-manager
            - designate-mdns
            - powerdns
      resources:
        - name: compute
          resource-prefix: comp
          server-role: COMPUTE-ROLE
          allocation-policy: any
          service-components:
            - ntp-client
            - nova-compute-kvm
            - nova-compute
            # DCN: Add dcn-metadata-agent and dcn-vrs.
            - dcn-metadata-agent
            - dcn-vrs
            # DCN: Remove incompatible services
            #- neutron-l3-agent
            #- neutron-metadata-agent
            #- neutron-openvswitch-agent
            #- neutron-lbaasv2-agent
            
        - name: node-vrsg
          resource-prefix: vrsg
          server-role: VRSG-ROLE
          service-components:
            - ntp-client
            # DCN: Add dcn-vrsg
            - dcn-vrsg

        - name: vm-host
          resource-prefix: vm-host
          server-role: HYPERVISOR-ROLE
          service-components:
            - ntp-server
            - dcn-vsc
            - dcn-vsc-data</codeblock>
        </p></li>
      </ul>
    </section>
    <section id="d1e695"><title>dcn_config.yml</title>
      <p>Path: ~/helion/my_cloud/definition/data/dcn/<b>dcn_config.yml</b></p>
      <p>Summary of changes:</p>
      <ul>
        <li> Add new file. Sample provided below.
          <p outputclass="expandcode"><b>Open | Close dcn_config.yml</b></p>
          <p outputclass="hiddencode">
            <codeblock outputclass="nomaxheight">#
# (c) Copyright 2016 Hewlett Packard Enterprise Development LP
#
# Licensed under the Apache License, Version 2.0 (the "License"); you may
# not use this file except in compliance with the License. You may obtain
# a copy of the License at
#
# http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
# WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
# License for the specific language governing permissions and limitations
# under the License.
#
---
  product:
    version: 2
  configuration-data:
    - name:  DCN-CONFIG-CP1
      services:
        - dcn
      data:
          vsd_host_name: myvsd.example.com
          vsd_user: OSadmin
          vsd_passwd: OSadmin
          vsc_active_ip: 192.168.245.253
          # Uncomment the line below if using more than one VSC
          #vsc_passive_ip: 192.168.245.249
          dns_domain_name: example.com
          vsc_mgmt_net: HLM
          vsc_data_net: MANAGEMENT
          vsc_image_name: vsc_singledisk
          vsc_user_name: admin
          vsc_user_pass: admin
          vsc_start_delay: 100
          vsc_start_timeout: 200

</codeblock></p><note
            type="note"><p>VSC HA</p> <p>For VSC HA, we can have two VSCs. IP for one VSC on management
              network is to be supplied as vsc_active_ip and IP for the other VSC is to be provided
              as vsc_passive_ip.</p></note></li>
      </ul>
    </section>
    <section id="d1e737"><title>interfaces_set_1.yml</title>
      <p>Path: ~/helion/my_cloud/definition/data/<b>interfaces_set_1.yml</b></p>
      <p>Summary of changes:</p>
      <ul>
        <li> Add a new "HLM_HYPERVISOR_INTERFACES".
          <p outputclass="expandcode"><b>Open | Close interfaces_set_1.yml</b></p>
          <p outputclass="hiddencode"><codeblock outputclass="nomaxheight">#
# (c) Copyright 2015-2016 Hewlett Packard Enterprise Development LP
#
# Licensed under the Apache License, Version 2.0 (the "License"); you may
# not use this file except in compliance with the License. You may obtain
# a copy of the License at
#
# http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
# WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
# License for the specific language governing permissions and limitations
# under the License.
#
---
  product:
    version: 2
  interface-models:
    - name: INTERFACE_SET_1
      network-interfaces:
        - name: hed1
          device:
            name: hed1
          network-groups:
            - MANAGEMENT
        - name: hed2
          device:
            name: hed2
          network-groups:
            - HLM
    - name: HLM_HYPERVISOR_INTERFACES
      network-interfaces:
        - name: hed1
          device:
            name: hed1
          network-groups:
            - MANAGEMENT
          passthrough-network-groups:
            - MANAGEMENT
        - name: hed2
          device:
            name: hed2
          network-groups:
            - HLM
          passthrough-network-groups:
            - HLM</codeblock></p></li>
      </ul>
    </section>
    <section id="d1e768"><title>network_groups.yml</title>
      <p>Path: ~/helion/my_cloud/definition/data/<b>network_groups.yml</b></p>
      <p>Summary of changes:</p>
      <ul>
        <li> Add a "dcn-vsc" and "dcn-vsc-data" components to the appropriate network groups.
          <p outputclass="expandcode"><b>Open | Close network_groups.yml</b></p>
          <p outputclass="hiddencode"><codeblock outputclass="nomaxheight">#
# (c) Copyright 2015-2016 Hewlett Packard Enterprise Development LP
#
# Licensed under the Apache License, Version 2.0 (the "License"); you may
# not use this file except in compliance with the License. You may obtain
# a copy of the License at
#
# http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
# WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
# License for the specific language governing permissions and limitations
# under the License.
#
---
  product:
    version: 2
  network-groups:
    - name: HLM
      hostname-suffix: hlm
      component-endpoints:
        - lifecycle-manager
        - lifecycle-manager-target
        - dcn-vsc
    - name: MANAGEMENT
      hostname-suffix: mgmt
      hostname: true
      tags:
        - neutron.networks.vxlan
        - neutron.networks.vlan:
            provider-physical-network: physnet1
      tls-component-endpoints:
        - barbican-api
        - mysql
        - rabbitmq
      component-endpoints:
        - dcn-vsc-data
        - default
      routes:
        - default
      load-balancers:
        - provider: ip-cluster
          name: lb
          tls-components:
            - default
          components:
            - vertica
            - nova-metadata
          roles:
            - internal
            - admin
          cert-file: helion-internal-cert
        - provider: ip-cluster
          name: extlb
          external-name: myhelion.test
          tls-components:
            - default
          roles:
            - public
          cert-file: my-public-deployerincloud-cert

</codeblock></p></li>
      </ul>
      <p> </p>
    </section>
    <section id="d1e802"><title>server_roles.yml</title>
      <p>Path: ~/helion/my_cloud/definition/data/<b>server_roles.yml</b></p>
      <p>Summary of changes:</p>
      <ul>
        <li> Add a new "VRSG" role.</li>
        <li> Add a new "HYPERVISOR"
          role.
          <p outputclass="expandcode"><b>Open | Close server_roles.yml</b></p>
          <p outputclass="hiddencode"><codeblock outputclass="nomaxheight">#
# (c) Copyright 2015 Hewlett Packard Enterprise Development LP
#
# Licensed under the Apache License, Version 2.0 (the "License"); you may
# not use this file except in compliance with the License. You may obtain
# a copy of the License at
#
# http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
# WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
# License for the specific language governing permissions and limitations
# under the License.
#
---
  product:
    version: 2
  server-roles:
    - name: CONTROLLER-ROLE
      interface-model: INTERFACE_SET_1
      disk-model: DISK_SET_CONTROLLER
    - name: COMPUTE-ROLE
      interface-model: INTERFACE_SET_1
      disk-model: DISK_SET_COMPUTE
    - name: VRSG-ROLE
      interface-model: INTERFACE_SET_1
      disk-model: DISK_SET_COMPUTE

    - name: HYPERVISOR-ROLE
      interface-model: HLM_HYPERVISOR_INTERFACES
      disk-model: DISK_SET_COMPUTE</codeblock></p></li>
      </ul>
    </section>
    <section id="d1e836"><title>servers.yml</title>
      <p>Path: ~/helion/my_cloud/definition/data/<b>servers.yml</b></p>
      <p>Summary of changes:</p>
      <ul>
        <li> Use one of the existing compute nodes as a VRSG node(shown below), or add another one
          for this purpose.</li>
        <li> For each VSC, add a vm-host server
            entry.
          <p outputclass="expandcode"><b>Open | Close servers.yml</b></p>
          <p outputclass="hiddencode"><codeblock outputclass="nomaxheight">#
# (c) Copyright 2015-2016 Hewlett Packard Enterprise Development LP
#
# Licensed under the Apache License, Version 2.0 (the "License"); you may
# not use this file except in compliance with the License. You may obtain
# a copy of the License at
#
# http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
# WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
# License for the specific language governing permissions and limitations
# under the License.
#
---
  product:
    version: 2
  baremetal:
    subnet: 192.168.10.0
    netmask: 255.255.255.0
    server-interface: eth2
  servers:
    - id: ccn-0001
      ip-addr: 192.168.10.3
      role: CONTROLLER-ROLE
      server-group: RACK1
      mac-addr: b2:72:8d:ac:7c:6f
      ilo-ip: 192.168.9.3
      ilo-password: password
      ilo-user: admin
      nic-mapping: VAGRANT
    - id: cpn-0001
      ip-addr: 192.168.10.4
      role: COMPUTE-ROLE
      server-group: RACK1
      mac-addr: d6:70:c1:36:43:f7
      ilo-ip: 192.168.9.4
      ilo-password: password
      ilo-user: admin
      nic-mapping: VAGRANT
    - id: vrsg-0001
      ip-addr: 192.168.10.5
      role: VRSG-ROLE
      server-group: RACK1
      mac-addr: 8e:8e:62:a6:ce:76
      ilo-ip: 192.168.9.5
      ilo-password: password
      ilo-user: admin
      nic-mapping: VAGRANT
 - id: vm-host-0001
      ip-addr: 192.168.10.6
      role: HYPERVISOR-ROLE
      server-group: RACK1
      hlm-hypervisor: true
      mac-addr: 8c:1e:62:a6:ce:f2
      ilo-ip: 192.168.9.6
      ilo-password: password
      ilo-user: admin
      nic-mapping: VAGRANT
</codeblock></p><note
            type="note"><p>VSC HA</p> <p> Upto two &lt;vm-host-nnnn&gt; nodes can be added here for VSC HA
              deployment.</p></note></li>
      </ul>
    </section>
    <section id="d1e882"><title>Bringing up the Cloud</title>
      <p/>
      <p> </p><note type="note"><p>Vagrant environment with VRS - Additional steps required</p> <p> For a
          Vagrant environment (not baremetal), additional steps are required prior to the steps
          below.</p>
        <p>One needs to change the hlm-input-model prior to building the VENVs, so that the deployer
          can apply the correct settings to the VRS-G node.</p>
        <p>Follow these steps:</p>
        <ol>
          <li> Clone hlm-input-model at the same level of hlm-dev-tools
            <codeblock outputclass="nomaxheight">cd ~/hlm
git clone <xref outputclass="highlightThis"  format="html"   href="git://git.gozer.hpcloud.net/hp/hlm-input-model">git://git.gozer.hpcloud.net/hp/hlm-input-model</xref></codeblock></li>
          <li>Change perform the input model changes described in the previous section</li>
          <li><ph>Rebuild the venvs</ph></li>
        </ol>
        <p>If using Padawan, there is a patch that can be used to make a padawan cloud as described
          above. See </p></note>
      <p>After the deployer is up:</p>
      <ol>
        <li> Copy helion/hos_extensions/dcn to
          ~/third-party<codeblock outputclass="nomaxheight">cp -r ~/helion/hos_extensions/dcn ~/third-party/</codeblock></li>
        <li>Copy the files obtained in to the following destinations: <ul>
            <li>
              <table>
                <tgroup cols="2">
                  <colspec colnum="1" colname="c1"/>
                  <colspec colnum="2" colname="c2"/>
                  <tbody>
                    <row>
                      <entry><b>Package Needed</b></entry>
                      <entry><b>Destination on deployer</b></entry>
                    </row>
                    <row>
                      <entry>
                        <p>&lt;vsd_host_name&gt;-ca.crt (<b>TEMPORARY - to workaround <xref outputclass="highlightThis" 
                              format="html"  
                              href="jira.hpcloud.net/browse/VNETCORE-2547"/></b>)</p>
                        <p>VSD has to be already installed (cf: )</p>
                        <ol>
                          <li>Login to the VSD (hostname is myvsd.example.com) through the console
                            or ssh</li>
                          <li>cd to /opt/vsd/ejbca/p12</li>
                          <li>Copy the the contents of rootca.pem. Paste the contents to a file
                            named myvsd.example.com-ca.crt. </li>
                          <li>On the deployer create "files" folder under
                            ~/third-party/dcn/ansible/roles/dcn-server-cms/files/ and copy the
                            ca-certificate.</li>
                        </ol>
                        <p>If the VSD hostname is "myvsd.example.com", then the name of the
                          ca-certificate file is myvsd.example.com-ca.crt</p></entry>
                      <entry>
                        ~/third-party/dcn/ansible/roles/dcn-server-cms/files/
                        <p>(directory may have to be created)</p></entry>
                    </row>
                    <row>
                      <entry>
                        configobj-5.0.6.tar.gz</entry>
                      <entry>
                        ~/third-party/dcn/venvs/plugin/src/</entry>
                    </row>
                    <row>
                      <entry>
                        uuid-1.30.tar.gz</entry>
                      <entry>
                        ~/third-party/dcn/venvs/plugin/src/</entry>
                    </row>
                    <row>
                      <entry>
                        nuage-openstack-neutron_8.0.0-4.0.3-21_all.deb</entry>
                      <entry>
                        ~/third-party/dcn/venvs/plugin/debs/</entry>
                    </row>
                    <row>
                      <entry>
                        nuagenetlib_8.0.0-4.0.3-21_all.deb</entry>
                      <entry>
                        ~/third-party/dcn/venvs/plugin/debs/</entry>
                    </row>
                    <row>
                      <entry>
                        httplib2-0.9.2.tar.gz</entry>
                      <entry>
                        ~/third-party/dcn/venvs/dcn-metadata-agent/src</entry>
                    </row>
                    <row>
                      <entry>
                        nuage-openstack-upgrade.tar.gz</entry>
                      <entry>
                        ~/third-party/dcn/ansible/roles/dcn-server-cms/files
                        <p>(directory may have to be created)</p></entry>
                    </row>
                    <row>
                      <entry>
                        nuage-openvswitch-common_4.0.3-25_all.deb</entry>
                      <entry>
                        ~/third-party/dcn/<b>pkgs/debian/</b></entry>
                    </row>
                    <row>
                      <entry>nuage-openvswitch-datapath-dkms_4.0.3-25_all.deb</entry>
                      <entry>
                        ~/third-party/dcn/pkgs/debian/</entry>
                    </row>
                    <row>
                      <entry>
                        nuage-openvswitch-switch_4.0.3-25_amd64.deb</entry>
                      <entry>
                        ~/third-party/dcn/pkgs/debian/</entry>
                    </row>
                    <row>
                      <entry>
                        nuage-python-openvswitch_4.0.3-25_all.deb</entry>
                      <entry>
                        ~/third-party/dcn/pkgs/debian/</entry>
                    </row>
                    <row>
                      <entry>
                        nuage-metadata-agent_4.0.1-4_all.deb</entry>
                      <entry>
                        ~/third-party/dcn/pkgs/debian/</entry>
                    </row>
                    <row>
                      <entry>
                        libprotobuf-c0_0.14-1+b1_amd64.deb</entry>
                      <entry>
                        ~/third-party/dcn/pkgs/debian</entry>
                    </row>
                    <row>
                      <entry>
                        If deploying VSC using the ansible
                          playbooks:vsc_singledisk.qcow2</entry>
                      <entry>
                        ~/third-party/dcn/ansible/roles/dcn-vsc/files</entry>
                    </row>
                  </tbody>
                </tgroup>
              </table><codeblock outputclass="nomaxheight">cp configobj-5.0.6.tar.gz ~/third-party/dcn/venvs/plugin/src/
cp uuid-1.30.tar.gz ~/third-party/dcn/venvs/plugin/src/
cp nuage-openstack-neutron_8.0.0-4.0.3-21_all.deb ~/third-party/dcn/venvs/plugin/debs/
cp nuagenetlib_8.0.0-4.0.3-21_all.deb ~/third-party/dcn/venvs/plugin/debs/
cp httplib2-0.9.2.tar.gz ~/third-party/dcn/venvs/dcn-metadata-agent/src/
mkdir ~/third-party/dcn/ansible/roles/dcn-server-cms/files
cp nuage-openstack-upgrade.tar.gz ~/third-party/dcn/ansible/roles/dcn-server-cms/files
cp nuage-openvswitch-common_4.0.3-25_all.deb ~/third-party/dcn/pkgs/debian/
cp nuage-openvswitch-datapath-dkms_4.0.3-25_all.deb ~/third-party/dcn/pkgs/debian/
cp nuage-openvswitch-switch_4.0.3-25_amd64.deb ~/third-party/dcn/pkgs/debian/
cp nuage-python-openvswitch_4.0.3-25_all.deb ~/third-party/dcn/pkgs/debian/
cp nuage-metadata-agent_4.0.1-4_all.deb ~/third-party/dcn/pkgs/debian/
cp libprotobuf-c0_0.14-1+b1_amd64.deb ~/third-party/dcn/pkgs/debian/</codeblock></li>
          </ul></li>
        <li> To enable VSC deployment using the playbooks: <ol>
            <li> copy ~/third-party/dcn/examples to
              ~/third-party<codeblock outputclass="nomaxheight">cp -r ~/third-party/dcn/examples/* ~/third-party/dcn/</codeblock></li>
            <li> copy the VSC disk
                image:<codeblock outputclass="nomaxheight">cp vsc_singledisk.qcow2 ~/third-party/dcn/ansible/roles/dcn-vsc/files/</codeblock><note
                type="note"><p>VSA HA</p> <p>Note, this image will be used for each of the VSC deployments
                  in HA case.</p></note></li>
          </ol></li>
        <li><ph>Apply workaround for </ph><xref outputclass="highlightThis"  format="html"  
            href="jira.hpcloud.net/browse/VNETCORE-2587"/>The nuage-metadata-agent 4.0.1-4 doesn't
          work with the nuage-python-openvswitch_4.0.3-25 bits. The observation is that the
          nuage-metadata-agent is not running after installation. To make it work, we will have to
            <b>install nuage-metadata-agent 4.0.3-25 OVER nuage-metadata-agent 4.0.1-4</b>.
            <b>Installing nuage-metadata-agent 4.0.3-25 without first installing
            nuage-metadata-agent 4.0.1-4 won't work.</b>
          <ol>
            <li>Get the nuage-metadata-agent 4.0.3-25 debian package
              (nuage-metadata-agent_4.0.3-25_all.deb) from a Nuage-PreRelease zip file.</li>
            <li>Copy the nuage-metadata-agent_4.0.3-25_all.deb to the compute nodes'
              /tmp/directory.</li>
            <li>On each compute node, do <ol>
                <li>run "sudo service nuage-metadata-agent stop"</li>
                <li>cd to /tmp </li>
                <li>sudo dpkg --install nuage-metadata-agent_4.0.3-25_all.deb</li>
                <li>edit /usr/sbin/nuage-metadata-agent as the root user:</li>
                <li>Change the 1st line: from "#! /usr/bin/python" to "#!
                  /opt/stack/service/dcn-metadata-agent/venv/bin/python2"</li>
                <li>Save the file</li>
                <li>run "sudo service nuage-metadata-agent start"</li>
                <li>Verify that the nuage metadata agent is running by doing "sudo service
                  nuage-metadata-agent status"</li>
              </ol></li>
          </ol></li>
        <li>Apply workaround for VNETCORE-2547 (Described at )<xref outputclass="highlightThis"  format="html"  
            href="jira.hpcloud.net/browse/VNETCORE-2547"/>
          <ol>
            <li>Login to the VSD (hostname is vsd.example.com) through the console or via ssh</li>
            <li>cd to /opt/vsd/ejbca/p12</li>
            <li>Open the file rootca.pem. Copy its contents. Then paste the contents to a new file
              called &lt;vsd_fqdn&gt;-ca.crt in the
              helion/hos_extensions/dcn/ansible/roles/dcn-server-cms/files/ directory. In this case,
              the vsd hostname is "vsd.example.com" so the certificate file name is
              vsd.example.com-ca.crt</li>
          </ol></li>
        <li>(If RHEL) Apply workaround for <xref outputclass="highlightThis"  format="html"  
            href="jira.hpcloud.net/browse/HLM-4996"/></li>
        <li> On VRS-G after HOS install and prior to create FIP Subnet on VSD, apply workaround of
            <xref outputclass="highlightThis"  format="html"   href="jira.hpcloud.net/browse/VNETCORE-2756"/>
          <ph> </ph></li>
        <li>
          <ph>Run ansible-playbook -i hosts/localhost third-party-import.yml on
            helion/hos/ansible</ph><ph> </ph><codeblock outputclass="nomaxheight">cd ~/helion/hos/ansible
ansible-playbook -i hosts/localhost third-party-import.yml
</codeblock></li>
        <li> Copy desired hlm-input-models. Example, for
          Padawan:<codeblock outputclass="nomaxheight">cp -r ~/hp-ci/padawan/* ~/helion/my_cloud/definition/</codeblock></li>
        <li>Make the changes described in </li>
        <li> Commit the changes<codeblock outputclass="nomaxheight">git add -A
git commit -m "My config"</codeblock></li>
        <li> Deploy the cloud as
            usual:<codeblock outputclass="nomaxheight">cd ~/helion
cd ~/helion/hos/ansible/
ansible-playbook -i hosts/localhost config-processor-run.yml -e encrypt="" -e rekey=""
ansible-playbook -i hosts/localhost ready-deployment.yml
cd ~/scratch/ansible/next/hos/ansible
*ansible-playbook -i hosts/verb_hosts hlm-hypervisor-setup.yml
ansible-playbook -i hosts/verb_hosts site.yml</codeblock><note
            type="note"><p>VSC deployment</p> <p>*Step "<ph>ansible-playbook -i hosts/verb_hosts
                hlm-hypervisor-setup.yml</ph><ph>" is done to setup one or more &lt;vm-host-nnnn&gt;
                nodes, which hosts one or more VSCs.</ph></p>
            <p><ph><ph>This step can be </ph>avoided<ph> if VSC is not being deployed via playbooks
                  or hypervisor role is not used in the process.</ph></ph></p></note></li>
      </ol>
    </section>
    <section id="d1e1411"><title>Upgrading DCN</title>
      <p>For upgrading DCN without upgrade HOS: see </p>
      <p>For upgrading HOS with or without upgrading DCN, see </p>
    </section>
    <section id="d1e1422"><title>Manual VSC Setup (deprecated)</title><note type="note">
        <p>Manual VSC setup is rendered obsolete due to the VSC deployment playbooks. It is kept to
          assist in troubleshooting VSC issues</p></note>
      <p> </p>
      <ol>
        <li>Prerequisites: The VSC's MGMT interface is the first nic. The VSC's control interface is
          the second nic. The VSC MGMT network identical to the VSD's. At this point, the following
          FQDN should be resolved by resolvconf: <ol>
            <li>myvsd.example.com</li>
            <li>xmpp_myvsd.example.com</li>
            <li>vsc.example.com</li>
          </ol></li>
        <li> Get the DCN-VSC-4.0R3.zip. From the zip file extract DCN-VSC-03-OEM-26-2.tar.gz,
          Unarchive the tarball.<codeblock outputclass="nomaxheight">tar xzvf DCN-VSC-02-OEM-26-2.tar.gz .</codeblock> These
          are the files copied to the current
          directory:<codeblock outputclass="nomaxheight">drwxr-xr-x@ 7 root  staff       238 Aug 19 15:21 .
-rw-r--r--@ 1 root  staff      8196 Aug 19 15:33 ./.DS_Store
drwxr-xr-x@ 4 root  staff       136 Jul  1 16:51 ./license
drwxr-xr-x@ 6 root  staff       204 Jul  1 16:51 ./license/XMPP
-rw-r--r--@ 1 root  staff      1843 Jul  1 16:51 ./license/XMPP/COPYING
-rw-r--r--@ 1 root  staff       201 Jul  1 16:51 ./license/XMPP/LICENSE.txt
-rw-r--r--@ 1 root  staff      1063 Jul  1 16:51 ./license/XMPP/MIT-LICENSE.txt
-rw-r--r--@ 1 root  staff        38 Jul  1 16:51 ./license/XMPP/SOURCE.txt
-rw-r--r--@ 1 root  staff       558 Jul  1 16:51 ./md5sum.txt
drwxr-xr-x@ 6 root  staff       204 Jul  1 16:51 ./single_disk
-rw-r--r--@ 1 root  staff      2306 Jul  1 16:51 ./single_disk/vsc.xml
-rw-r--r--@ 1 root  staff  65628160 Jul  1 16:51 ./single_disk/vsc_singledisk.ova
-rw-r--r--@ 1 root  staff  45809664 Jul  1 16:51 ./single_disk/vsc_singledisk.qcow2
-rw-r--r--@ 1 root  staff  45678592 Jul  1 16:51 ./single_disk/vsc_singledisk.vmdk
drwxr-xr-x@ 7 root  staff       238 Jul  1 16:51 ./two_disks
-rw-r--r--@ 1 root  staff  45547520 Jul  1 16:51 ./two_disks/i386-cpm_image_disk.vmdk
-rw-r--r--@ 1 root  staff  45809664 Jul  1 16:51 ./two_disks/vsc.imgdisk.qcow2
-rw-r--r--@ 1 root  staff    524288 Jul  1 16:51 ./two_disks/vsc.usrdisk.qcow2
-rw-r--r--@ 1 root  staff    393216 Jul  1 16:51 ./two_disks/vsc.usrdisk.vmdk
-rw-r--r--@ 1 root  root      2716 Jul  1 16:51 ./two_disks/vsc.xml</codeblock></li>
        <li> cd to the single_disk directory.<codeblock outputclass="nomaxheight">cd single_disk</codeblock></li>
        <li> Copy vsc_singledisk.qcow2 to /var/lib/libvirt/qemu/images amd change the owner to <xref
            format="html"   href="http://libvirt-qemukvm/"
          >libvirt-qemu:kvm</xref>:<codeblock outputclass="nomaxheight">sudo cp vsc_singledisk.qcow2 /var/lib/libvirt/images
sudo chown libvirt-qemu:kvm /var/lib/libvirt/images/vsc_singledisk.qcow2</codeblock></li>
        <li>Backup the file vsc.xml as vsc.xml.BAK</li>
        <li>Edit vsc.xml: <ol>
            <li> replace the line<codeblock outputclass="nomaxheight">&lt;type arch='x86_64' machine='rhel6.0.0'&gt;hvm&lt;/type&gt;</codeblock>
              <ph>with</ph><ph> </ph><codeblock outputclass="nomaxheight">&lt;type arch='x86_64' machine='pc-i440fx-trusty'&gt;hvm&lt;/type&gt;</codeblock></li>
            <li> Replace the line:<codeblock outputclass="nomaxheight">&lt;emulator&gt;/usr/libexec/qemu-kvm&lt;/emulator&gt;</codeblock>
              <ph>with</ph><ph> </ph><codeblock outputclass="nomaxheight">&lt;emulator&gt;/usr/bin/kvm-spice&lt;/emulator&gt;</codeblock></li>
            <li> The vsc.xml defines 2 networks. The first NIC, on bridge br0, is dedicated for
              management traffic. Control traffic goes on the VNIC over bridge
              br1.<codeblock outputclass="nomaxheight"> &lt;interface type='bridge'&gt;
      &lt;source bridge='br0'/&gt;
      &lt;model type='virtio'/&gt;
      
&lt;address type='pci' domain='0x0000' bus='0x00' slot='0x03' function='0x0'/&gt;
    &lt;/interface&gt;
    &lt;interface type='bridge'&gt;
      &lt;source bridge='br1'/&gt;
      &lt;model type='virtio'/&gt;
      
&lt;address type='pci' domain='0x0000' bus='0x00' slot='0x04' function='0x0'/&gt;
 &lt;/interface&gt;</codeblock>
              For padawan, both management and data are on the same network. So replace their
              definitions
              with:<codeblock outputclass="nomaxheight">&lt;interface type='network'&gt;
      &lt;mac address='52:54:00:8c:7c:57'/&gt;
      &lt;source network='libvirt-vagrant'/&gt;
      &lt;model type='virtio'/&gt;
      
&lt;address type='pci' domain='0x0000' bus='0x00' slot='0x03' function='0x0'/&gt;
    &lt;/interface&gt;
    &lt;interface type='network'&gt;
      &lt;mac address='52:54:00:4a:7f:77'/&gt;
      &lt;source network='padawan-vagrant0'/&gt;
      &lt;model type='virtio'/&gt;
      
&lt;address type='pci' domain='0x0000' bus='0x00' slot='0x04' function='0x0'/&gt;
&lt;/interface&gt;</codeblock>
            </li>
            <li> The vsc.xml defines a serial port
              console:<codeblock outputclass="nomaxheight">    &lt;serial type='pty'&gt;
      &lt;source path='/dev/pts/1'/&gt;
      &lt;target port='0'/&gt;
      &lt;alias name='serial0'/&gt;
    &lt;/serial&gt;
    &lt;console type='pty' tty='/dev/pts/1'&gt;
      &lt;source path='/dev/pts/1'/&gt;
      &lt;target type='serial' port='0'/&gt;
      &lt;alias name='serial0'/&gt;
    &lt;/console&gt;</codeblock>
              Replace their definitions
              with:<codeblock outputclass="nomaxheight">&lt;serial type='pty'&gt;
  &lt;target port='0'/&gt;
&lt;/serial&gt;
&lt;console type='pty'&gt;
  &lt;target type='serial' port='0'/&gt;
&lt;/console&gt;
&lt;memballoon model='virtio'&gt;
  
&lt;address type='pci' domain='0x0000' bus='0x00' slot='0x02' function='0x0'/&gt;
&lt;/memballoon&gt;</codeblock></li>
          </ol></li>
        <li> Define the VSC VM in libvirt and start it
          up:<codeblock outputclass="nomaxheight">sudo virsh define vsc.xml
sudo virsh start vsc</codeblock></li>
        <li> Login to the VSC console:<codeblock outputclass="nomaxheight">virsh console vsc</codeblock></li>
        <li>Wait for the VSC to boot up to the login: prompt. It should take about 2 minutes.</li>
        <li> Login to the VSC as admin/admin. At the *A:vm# prompt, enter
          "bof"<codeblock outputclass="nomaxheight">A:vm1# bof</codeblock></li>
        <li> The run the following commands in the bof# prompt. The
          ipaddress-cidr=192.168.121.253/24 The dns-ip is 192.168.121.1. Also a static route needs
          to be provided. That routes the data traffic to the VSC MGMT interface gateway of
            (192.168.121.1):<codeblock outputclass="nomaxheight">A:vm1&gt;bof# address &lt;ipaddress-cidr&gt;
A:vm1&gt;bof# primary-dns &lt;dns-ip&gt;
A:vm1&gt;bof# static-route 192.168.245.0/24 next-hop 192.168.121.1
A:vm1&gt;bof# save cf1:
A:vm1&gt;bof# exit</codeblock><note
            type="note"><p>VSC HA setup</p> <p>Note, for setting up VSC with high availability, we need to
              bring up two or more of these VSC nodes, by following these steps, but provide a
              distinct IP address, from the same subnet, here in "<ph>A:vm1&gt;bof# address
                &lt;ipaddress-cidr&gt;</ph><ph>".</ph></p></note></li>
        <li> Reboot the
          VSC:<codeblock outputclass="nomaxheight">A:vm1# admin reboot
Are you sure you want to reboot (y/n)? y</codeblock></li>
        <li> After the VSC comes back up, login to the VSC again as admin/admin. Then run the
          command "ping router "management" &lt;dns-ip&gt; to verify connectivity with other systems
          in the padawan-cloud. For example ping to
          ccn-0001:<codeblock outputclass="nomaxheight">A:vm1# ping router "management" 192.168.245.3
PING 192.168.245.3 56 data bytes
64 bytes from 192.168.245.3: icmp_seq=1 ttl=64 time=0.535ms.
64 bytes from 192.168.245.3: icmp_seq=2 ttl=64 time=0.253ms.
64 bytes from 192.168.245.3: icmp_seq=3 ttl=64 time=0.222ms.
64 bytes from 192.168.245.3: icmp_seq=4 ttl=64 time=0.244ms.
64 bytes from 192.168.245.3: icmp_seq=5 ttl=64 time=0.245ms.

---- 192.168.245.3 PING Statistics ----
5 packets transmitted, 5 packets received, 0.00% packet loss
round-trip min = 0.222ms, avg = 0.299ms, max = 0.535ms, stddev = 0.118ms</codeblock></li>
        <li> Set the IP address on the Control
          interface:<codeblock outputclass="nomaxheight">exit all
admin save
configure router interface "control" address 192.168.245.253/24
exit all
admin
safe cf1:
exit
admin save</codeblock></li>
        <li> Setup the ntp server
          addresses<codeblock outputclass="nomaxheight">exit all
admin save
configure system time ntp server 16.110.135.123
configure system time ntp server 192.168.121.1
exit
admin save</codeblock></li>
        <li> Configure the xmpp
          connection<codeblock outputclass="nomaxheight">exit all
configure vswitch-controller xmpp-server "vsd_xmpp:password@myvsd.example.com"
admin save</codeblock></li>
        <li> Configure the system
          name<codeblock outputclass="nomaxheight">exit all
configure system name "vsc.example.com"
admin save</codeblock></li>
        <li>
          <ph>Validation: Verification of VSC requires VSD to be up. To verify that the VSC
            recognizes the VRSs in the cloud, The VRS/VRS-G needs to be
          running.</ph><codeblock outputclass="nomaxheight">A:vm1#
*A:vsc.example.com# show 
*A:vsc.example.com&gt;show# vswitch-controller 
*A:vsc.example.com&gt;show&gt;vswitch-controller# xmpp-server 
===============================================================================
XMPP Server Table
===============================================================================
XMPP FQDN                       Last changed since State
 User Name                                         
-------------------------------------------------------------------------------
myvsd.example.com                 0d 00:49:50        Functional
 OSadmin                                            
-------------------------------------------------------------------------------
No. of XMPP server's: 1
===============================================================================
*A:vsc.example.com&gt;show&gt;vswitch-controller# 
*A:vsc.example.com&gt;show&gt;vswitch-controller# vsd 

===============================================================================
Virtual Services Directory Table
===============================================================================
User Name                       Uptime             Status
-------------------------------------------------------------------------------
cna@myvsd.example.com/nuage       0d 00:50:14        available
-------------------------------------------------------------------------------
No. of VSD's: 1
===============================================================================
*A:vsc.example.com&gt;show&gt;vswitch-controller# 
#
# Verifies that the VSC sees the VRSs.  In this setup there are 2 VRS and 1 VRS-G.
*A:vsc.example.com&gt;show&gt;vswitch-controller# vswitches 
===============================================================================
VSwitch Table
===============================================================================
vswitch-instance               Personality Uptime       Num VM/Host/Bridge/Cont
-------------------------------------------------------------------------------
va-192.168.121.71/1            None        0d 02:03:45  0/0/0/0
va-192.168.121.184/1           None        0d 02:05:07  0/0/0/0
va-192.168.121.223/1           None        0d 02:21:15  0/0/0/0
-------------------------------------------------------------------------------
No. of virtual switches: 3
===============================================================================
*A:vsc.example.com&gt;show&gt;vswitch-controller# 



</codeblock></li>
        <li> Additional Verification. When compute nodes comes up, check that the VSC has
          connectivity with those nodes. For example, if a compute node is running VRS and it has
          the IP of 192.168.245.7, ping to the
          IP:<codeblock outputclass="nomaxheight">ping router "management" 192.168.245.7</codeblock></li>
      </ol>
    </section>
    <section id="d1e1691"><title>DCN VSC HA setup</title>
      <p>The DCN VSC should generally be setup in HA as VSC is the controller.</p>
      <p>Notes for setting up VSC in HA were embedded in the above.</p>
      <p>Summary of the steps are as follows:</p>
      <ol>
        <li>In the Input model provision a node for hosting primary VSC and another for hosting
          secondary VSC.</li>
        <li>Once the nodes have come up, gather IPs (service_ips: dcn-vsc-data) for both the VSCs.
          This can be obtained from helion/my_cloud/info/net_info.yml on the deployer.</li>
        <li>Edit dcn_config.yml file to have the correct vsc_active_ip and vsc_passive_ip. Also
          ensure other paramenters for VSC in dcn_config are correctly populated. <ol>
            <li>vsc_mgmt_net: HLM (example network mapping)</li>
            <li> vsc_data_net: MANAGEMENT (example network mapping)</li>
            <li> vsc_image_name: vsc_singledisk (example name of the qcow2 image provided by the 3rd
              party)</li>
            <li> vsc_user_name: admin (example user name)</li>
            <li> vsc_user_pass: admin (example password)</li>
            <li> vsc_start_delay: 100 (lower bound on seconds we expect VSC VM to take to boot and
              stabilize, after this we start polling)</li>
            <li> vsc_start_timeout: 300 (upper bound on seconds we expect VSC VM to become
              available, after this we fail the deployment)</li>
          </ol></li>
        <li>Make sure the the FQDN for both VSCs are resolvable on all nodes.</li>
        <li>Copy example vsc deployment ansible and services to third-party/dcn area.</li>
        <li>Copy vsc qcow2 image provided &lt;vsc_image_name&gt;.qcow2 to
          third-party/dcn/ansible/roles/dcn-vsc/files/ on deployer.</li>
        <li>Deploy the cloud, including the third-party deployment steps as described above.</li>
        <li>User shall now have a Helion DCN cloud with VSC deployed in HA (primary and
          secondary).</li>
      </ol>
      <p>Input model changes and third party import steps are described in more detail here: </p>
    </section>
    <section id="d1e1774"><title>Logging</title>
      <p>The DCN VRS, VRS-G and nuage-metadata processes generate additional log files in
        /var/log/openvswitch. These are:</p>
      <ul>
        <li>VRSG: <ul>
            <li>nuage-housekeeper.log</li>
            <li>nuage-rpc.log</li>
            <li>nuage-SysMon.log</li>
            <li>nuage-service-extension-config.log</li>
          </ul></li>
        <li>VRS - all of the above, plus: <ul>
            <li>vm-monitor.log</li>
            <li>nuage-metadata-agent.log</li>
          </ul></li>
      </ul>
      <p>These files are under centralized logging control and log rotated.</p>
    </section>
    <section id="d1e1812"><title>Monitoring</title>
      <p> The VRS and VRS-G processes added by DCN are:</p>
      <ul>
        <li>nuage-housekeeper</li>
        <li>nuage-rpc</li>
        <li>nuage-SysMon</li>
        <li>nuage-metadata-agent (VRS-only)</li>
        <li>vm-monitor (VRS-only)</li>
      </ul>
      <p>Process checks (pid_count) were implemented in Monasca for the above processes.</p>
      <p>Ops Console and the Monasca CLI can be used to verify that the DCN processes are up and
        running. They can be filtered by dimension "SERVICE: dcn". </p>
      <p><image keyref=""/></p>
      <p><ph>on the deployer</ph></p>
      <p>/var/lib/libvirt/dnsmasq/vagrant-libvirt.conf</p>
      <p> </p>
    </section>



  </body>
</topic>