<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE topic PUBLIC "-//OASIS//DTD DITA Topic//EN" "topic.dtd">
<topic id="hw_support_diskcalc">
  <title><ph conkeyref="HOS-conrefs/product-title"/>Disk Calculator</title>
  <body>
    <section id="disk_calc">
      <title>Disk Calculator for Compute-Centric Deployments</title>
      <p>This topic provides guidance on how to estimate the amount of disk space required for a
        compute-centric <keyword keyref="kw-hos"/> deployment. To accurately estimate the disk space
        needed, it is important to understand how Helion utilizes resources. Although there are a
        variety of factors, including the number of compute nodes, a large portion of the
        utilization is driven by operational tools, such as monitoring, metering, and logging.</p>
      <!--   applies to upgrade to 30 only? -->
      <!--<p>These instructions are intended for new installations. The process is slightly different
        for an upgrade where you are starting with an existing deployment. For information on how to
        estimate disk size for an upgrade, see the <xref href="../upgrade/update_disk_models.xml"/>
        topic.</p>-->
      <note type="attention">The disk calculator does not accurately estimate a Swift-centric
        deployment at this time. For more information on Swift, see the <xref
          href="rec_min_swift.xml#rec_min_swift">Recommended minimum hardware requirements for an
          entry-scale Swift model</xref> topic.</note>
      <p>The usage of disk space by operational tools can be estimated from the following
        parameters: <ul>
          <li><b>Number of compute nodes</b> + <b>Number of VM's running on each compute
            node</b></li>
          <li><b>Number of services being monitored or metered</b> + <b>Amount of logs
            created</b></li>
          <li><b>Retention periods for operational data</b> (for Elastic Search, Vertica/InfluxDB,
            and Kafka)</li>
        </ul></p>
      <note type="attention">If you also enable auditing, follow the steps in the <xref
          href="#hw_support_diskcalc/disk_calc_audit_adj">Audit Logging
          Adjustment</xref> section to enter additional input parameters.</note>
      <p><b>Disk Estimation Process</b>
      </p>
      <p><keyword keyref="kw-hos"/> provides entry scale and scale-out models for deployment. This
        disk estimation tool, currently in a spreadsheet form, helps you decide which disk model to
        start from as well as what customizations you need to meet your deployment requirements. The
        disk estimation process also provides default settings and minimum values for the parameters
        that drive disk size. <note type="attention">Kafka is the queuing system used to process
          metering monitoring and logging (MML) data. Kakfa stores the queued data on disk, so the
          disk space available will have a large impact on the amount of data the MML systems can
          process. Providing less than the minimum disk space for Kakfa will result in loss of MML
          data and can affect other components on the control plane. The default for Kafka is 1 hour
          which is 17 GB.</note></p>
      <p><b>To estimate the disk sizes required for your deployment:</b></p>
      <ol>
        <li><xref href="#hw_support_diskcalc/disk_calc_input">Enter input
            parameters.</xref></li>
        <li>If you also enable auditing, follow the steps in the <xref
            href="#hw_support_diskcalc/disk_calc_audit_adj">Audit Logging
            Adjustment</xref> section to enter additional input parameters.</li>
        <li><xref href="#hw_support_diskcalc/disk_calc_select">Select the deployment
            model you want to support based on the calculations.</xref></li>
        <li><xref href="#hw_support_diskcalc/disk_calc_match">Match the selected
            deployment to a disk model example.</xref></li>
      </ol>
    </section>
    <section id="disk_calc_input"><title>Enter Input Parameters</title><p>The Disk Calculator
        spreadsheet automatically displays the minimum requirements for the components that define
        disk size. You can replace the default values with either the number you have to work with
        or the number that you want to support. <note type="attention">If you want to enable audit
          logging, follow the steps in the <xref
            href="#hw_support_diskcalc/disk_calc_audit_adj">Audit Logging
            Adjustment</xref> section to enter additional input parameters.</note></p><table
        frame="all" rowsep="1" colsep="1" id="table_disk_calc_input">
        <tgroup cols="3">
          <colspec colname="c1" colnum="1"/>
          <colspec colname="c2" colnum="2"/>
          <colspec colname="c3" colnum="3"/>
          <thead>
            <row>
              <entry>Input Parameter</entry>
              <entry>Default</entry>
              <entry>Minimum</entry>
            </row>
          </thead>
          <tbody>
            <row>
              <entry>System Memory</entry>
              <entry>64 GB</entry>
              <entry>64 GB</entry>
            </row>
            <row>
              <entry>Compute Nodes</entry>
              <entry>100</entry>
              <entry>100</entry>
            </row>
            <row>
              <entry>VMs per Compute Node</entry>
              <entry>40</entry>
              <entry>40</entry>
            </row>
            <row>
              <entry>Component: Vertica</entry>
              <entry>45 days retention period</entry>
              <entry>30 days</entry>
            </row>
            <row>
              <entry>Component: Logging</entry>
              <entry>22 services covered <p>7 days retention period</p></entry>
              <entry><p> </p>7 days retention period</entry>
            </row>
            <row>
              <entry>Component: Kafka (message queue)</entry>
              <entry>0.17 of an hour retention period</entry>
              <entry>0.042 of an hour retention period</entry>
            </row>
            <row>
              <entry>Component: Elastic Search (log storage)</entry>
              <entry>7 days retention period</entry>
              <entry>7 days retention period</entry>
            </row>
            <row>
              <entry>Component: Audit</entry>
              <entry>0 days retention period</entry>
              <entry>0 days retention period</entry>
            </row>
          </tbody>
        </tgroup>
      </table> The following diagram shows the input parameters in the spreadsheet. <p>
        <image href="../../media/DiskCalc1.png" placement="break"/>
      </p><p>To provide the paramters required to estimate disk size:</p><ol>
        <li><xref href="../../media/hos.docs/diskcalc40.zip" format="zip">Open the disk calculator
            spreadsheet</xref>.</li>
        <li>At the bottom of the spreadsheet, click on the <b>Draft Sizing Tool4</b> tab.</li>
        <li> To set the server RAM size, replace the default value in the <b>System Memory</b>
          field.</li>
        <li>To set the number of compute nodes, replace the default in the <b>Compute Nodes</b>
          field.</li>
        <li>To set the average number of virtual machines per compute node, replace the default in
          the <b>VM's per Compute Node</b> field.</li>
        <li>To set the number of days you want the metering and logging files retained, replace the
          default in the <b>Vertica Retention Period</b> field.</li>
        <li>To set logging values, replace the default in <b>Number of Services Covered</b> and
            <b>Retention Period</b>. <note type="attention">If you enable additional logging of
            services than those set by default, then you must increase the number in the <b>Logging
              Number of Services</b> Field.</note></li>
        <li>To set a value for Kafka messages to be retained, replace the default in the <b>Kafka
            Retention Period</b> field.</li>
        <li>To set a value for Elastic Search log file retention, replace the default in the
            <b>Elastic Search Retention Period</b> field.</li>
        <li>To set a value for Audit logging file retention, replace the default in the <b>Audit
            Retention Period</b> field.</li>
      </ol></section>
    <section id="disk_calc_audit_adj">
      <title>Audit Logging Adjustment</title>
      <p>If you want to enable audit logging, you must enter additional input parameters to ensure
        there is enough room to retain the audit logs. The following diagram shows the parameters
        you need to specify in the Disk Calculator spreadsheet.</p>
      <p><image href="../../media/DiskCalc2.png" placement="break"/></p>
      <p>To add audit logging to disk size calculations:</p>
      <ol>
        <li>Determine which services you have enabled to collect audit logging information. This is
          part of HLM configuration.</li>
        <li>Enter the number of Audit Enabled services on cluster.  Auditing is disabled by default, 
          so these values will initially be 0.  If audit logging is enabled, initial suggested 
          values would be 9 for API/Core Services, 1 for Networking, 1 for Swift, and 2 for MMLB.
        
            <note type="attention">If you enable logging for services beyond the defaults, you must
            change the <b>Number of Services on a Cluster</b> field in the spreadsheet. It is
            recommended that you increase the total services covered as well as increment the number
            on the appropriate cluster. For example, if you enable Apache logs on the core services,
            then the total would increase to 23 and the api/core services entry would change from 13
            to 14.</note></li>
        <li>To include Glance image space in your estimation, determine the size of the images that
          will be cached.</li>
        <li>Enter the total size needed to store Glance images in the
            <b>/var/lib/glance/work_dir</b> field.</li>
      </ol>
    </section>
    <section id="disk_calc_select">
      <title>Select the Deployment Model</title>
      <p>To decide which architecture will meet all of your requirements, use the values given in
        the Disk Calculator spreadsheet. Keeping in mind the rough scale you expect to target as
        well as any need to separate services, choose an Entry Scale, Entry Scale MML, or Mid Scale
        deployment. Once you have chosen a deployment you can match it to the sample disk models in
        the <xref href="#hw_support_diskcalc/disk_calc_match">Match to a Disk
          Model</xref> section. The following diagram shows the deployment options that are
        recommended if you use the default values in the Disk Calculator spreadsheet.</p>
      <p><image href="../../media/DiskCalc3.png" placement="break" id="image_cmk_lxc_wv"/></p>
      <p>For example, in the above diagram, if you wanted to choose an Entry Scale MML deployment,
        the calculator recommends the following disk sizes:</p>
      <ul id="ul_uwg_fs2_wv">
        <li>216GB for API/Core Service</li>
        <li>216GB for Neutron (networking)</li>
        <li>216GB for Swift (storage)</li>
        <li>573GB for MMLB</li>
        <li>252GB for MySQL/RabbitMQ</li>
      </ul>
    </section>
    <section id="disk_calc_match">
      <title>Match to a Disk Model</title>
      <p>For each of the entry-scale and scale-out cloud models, there is a set of associated disk
        models that can be used as the basis for your deployment. These models provide examples of
        pontetial parameters for operational tools and are expected to be used as the starting point
        for actual deployments. Since each deployment can vary greatly, the disk calculator
        spreadsheet provides a way to create the basic disk model and customize it to fit the
        specific parameters your deployment. Once you have estimated disk sizes and chosen a
        deployment architecture, you can choose which example disk partitioning file to use from the
        tables below. Keep in mind if you are enabling more options than are listed in the Disk
        Calculator, or if you want to plan for growth, you will need to manually adjust paramters as
        necessary.</p>
      <p>Disk models are provided for each deployment option based on the expected size of the disk
        available to the control plane nodes. The available space is then partitioned by percentage
        to be allocated to each of the required volumes on the control plane. Each of the disk
        models is targeted at a specific set of parameters which can be found in the following
        tables:</p>
      <ul>
        <li><xref href="#hw_support_diskcalc/disk_calc_entry">Entry Scale:</xref> 600
          GB, 1 TB</li>
        <li><xref href="#hw_support_diskcalc/disk_calc_Mid">Mid Scale/ Entry Scale
            MML Servers:</xref> 600 GB, 2 TB, 4.5 TB</li>
      </ul>
    </section>
    <section id="disk_calc_entry">
      <title>Entry Scale Disk Models</title>
      <p>These models include a single cluster of control plane nodes and all services.</p>
      <p><b>600GB Entry Scale</b></p>
      <table frame="all" rowsep="1" colsep="1" id="table_disk_matchE6">
        <tgroup cols="2">
          <colspec colname="c1" colnum="1" colwidth="1*"/>
          <colspec colname="c2" colnum="2" colwidth="2*"/>
          <thead>
            <row>
              <entry>Component</entry>
              <entry>Parameters</entry>
            </row>
          </thead>
          <tbody>
            <row>
              <entry>compute nodes</entry>
              <entry>100 <p>This model provides lower than recommended retention and should only be
                  used for POC deployments.</p></entry>
            </row>
          </tbody>
        </tgroup>
      </table>
      <p><b>1TB Entry Scale</b></p>
      <table frame="all" rowsep="1" colsep="1" id="table_disk_matchE1">
        <tgroup cols="2">
          <colspec colname="c1" colnum="1" colwidth="1*"/>
          <colspec colname="c2" colnum="2" colwidth="2*"/>
          <thead>
            <row>
              <entry>Component</entry>
              <entry>Parameters</entry>
            </row>
          </thead>
          <tbody>
            <row>
              <entry>compute nodes</entry>
              <entry>100</entry>
            </row>
            <row>
              <entry>local logging<p>var/log</p></entry>
              <entry>7 day retention</entry>
            </row>
            <row>
              <entry>metering/monitoring<p>/var/vertica</p></entry>
              <entry>45 day retention</entry>
            </row>
            <row>
              <entry>centralized logging<p>/var/lib/elasticsearch</p></entry>
              <entry>7 day retention</entry>
            </row>
            <row>
              <entry>Kafka Message Queue<p>/var/kafka</p></entry>
              <entry>4 hour retention</entry>
            </row>
          </tbody>
        </tgroup>
      </table>
    </section>
    <section id="disk_calc_Mid">
      <title>MML Disk Models</title>
      <p>These mid-scale and entry-scale MML models include seperate control plane nodes for core
        services, metering/monitoring/logging, and MySQL/RabbitMQ. Optionally you can also seperate
        out Swift (storage) and Neutron (networking). MML servers are the ones that will need
        modification based on the scale and operational parameters.</p>
      <p><b>600GB MML Server</b></p>
      <table frame="all" rowsep="1" colsep="1" id="table_disk_matchM6">
        <tgroup cols="2">
          <colspec colname="c1" colnum="1" colwidth="1*"/>
          <colspec colname="c2" colnum="2" colwidth="2*"/>
          <thead>
            <row>
              <entry>Component</entry>
              <entry>Parameters</entry>
            </row>
          </thead>
          <tbody>
            <row>
              <entry>compute nodes</entry>
              <entry>100</entry>
            </row>
            <row>
              <entry>local logging<p>var/log</p></entry>
              <entry>7 day retention</entry>
            </row>
            <row>
              <entry>metering/monitoring<p>/var/vertica</p></entry>
              <entry>30 day retention<p>
                  <note type="caution">45 days is the default minimum.</note>
                </p></entry>
            </row>
            <row>
              <entry>centralized logging<p>/var/lib/elasticsearch</p></entry>
              <entry>7 day retention</entry>
            </row>
            <row>
              <entry>Kafka Message Queue<p>/var/kafka</p></entry>
              <entry>4 hour retention</entry>
            </row>
          </tbody>
        </tgroup>
      </table>
      <p><b>2TB MML Server</b></p>
      <table frame="all" rowsep="1" colsep="1" id="table_disk_matchM2T">
        <tgroup cols="2">
          <colspec colname="c1" colnum="1" colwidth="1*"/>
          <colspec colname="c2" colnum="2" colwidth="2*"/>
          <thead>
            <row>
              <entry>Component</entry>
              <entry>Parameters</entry>
            </row>
          </thead>
          <tbody>
            <row>
              <entry>compute nodes</entry>
              <entry>200</entry>
            </row>
            <row>
              <entry>local logging<p>var/log</p></entry>
              <entry>7 day retention</entry>
            </row>
            <row>
              <entry>metering/monitoring<p>/var/vertica</p></entry>
              <entry>45 day retention</entry>
            </row>
            <row>
              <entry>centralized logging<p>/var/lib/elasticsearch</p></entry>
              <entry>7 day retention</entry>
            </row>
            <row>
              <entry>Kafka Message Queue<p>/var/kafka</p></entry>
              <entry>12 hour retention</entry>
            </row>
          </tbody>
        </tgroup>
      </table>
      <p><b>4.5TB MML Server</b></p>
      <table frame="all" rowsep="1" colsep="1" id="table_disk_matchM4T">
        <tgroup cols="2">
          <colspec colname="c1" colnum="1" colwidth="1*"/>
          <colspec colname="c2" colnum="2" colwidth="2*"/>
          <thead>
            <row>
              <entry>Component</entry>
              <entry>Parameters</entry>
            </row>
          </thead>
          <tbody>
            <row>
              <entry>compute nodes</entry>
              <entry>200</entry>
            </row>
            <row>
              <entry>local logging<p>var/log</p></entry>
              <entry>7 day retention</entry>
            </row>
            <row>
              <entry>metering/monitoring<p>/var/vertica</p></entry>
              <entry>45 day retention</entry>
            </row>
            <row>
              <entry>centralized logging<p>/var/lib/elasticsearch</p></entry>
              <entry>45 day retention</entry>
            </row>
            <row>
              <entry>Kafka Message Queue<p>/var/kafka</p></entry>
              <entry>12 hour retention</entry>
            </row>
          </tbody>
        </tgroup>
      </table>
    </section>

    <section id="disk_cinder">
      <title>Notes about disk sizing for Cinder bootable volumes</title>
      <p>When creating your disk model for nodes that will have the cinder volume role make sure
        that there is sufficient disk space allocated for a temporary space for image conversion if
        you will be creating bootable volumes.</p>
      <p>By default, Cinder uses <codeph>/var/lib/cinder</codeph> for image conversion and this will
        be on the root filesystem unless it is explicitly separated. You can ensure there is enough
        space by ensuring that the root file system is sufficiently large, or by creating a logical
        volume mounted at <codeph>/var/lib/cinder</codeph> in the disk model when installing the
        system.</p>
      <p>If you have post-installation issues with creating bootable volumes, see the <xref
          href="../operations/troubleshooting/ts_blockstorage.xml">Block Storage
          Troubleshooting</xref> documentation for steps to resolve these issues.</p>
    </section>
  </body>
</topic>
