<?xml version="1.0"?>
<!DOCTYPE section PUBLIC "-//OASIS//DTD DocBook XML V4.5//EN" "http://www.oasis-open.org/docbook/xml/4.5/docbookx.dtd" [ <!ENTITY % entities SYSTEM "entities.xml"> %entities; ]><!--Edit status: not edited-->
<section id="HP3.0HA">
   <title>
      <phrase/>High Availability</title>
    
    
    <para>This page covers the following topics:</para>

    <itemizedlist>
      <listitem>
         <para><ulink url="#HP3.0HA/concepts_overview">High Availability Concepts Overview</ulink></para>

         <itemizedlist>
            <listitem>
               <para><ulink url="#HP3.0HA/cloud_infrastructure">Highly Available Cloud
              Infrastructure</ulink></para>

            </listitem>
            <listitem>
               <para><ulink url="#HP3.0HA/tenant_workloads">Highly Available Cloud-Aware Tenant
              Workloads</ulink></para>

            </listitem>
        </itemizedlist>
      </listitem>
      <listitem>
         <para><ulink url="#HP3.0HA/highly_available_cloud_infrastructure">Highly Available Cloud
          Infrastructure</ulink></para>

      </listitem>
      <listitem>
         <para><ulink url="#HP3.0HA/high_availablity_controllers">High Availability of Controllers</ulink></para>

         <itemizedlist>
            <listitem>
               <para><ulink url="#HP3.0HA/api_request">API Request Message Flow</ulink></para>

            </listitem>
            <listitem>
               <para><ulink url="#HP3.0HA/node_failure">Handling Node Failure</ulink></para>

            </listitem>
            <listitem>
               <para><ulink url="#HP3.0HA/network_partitions">Handling Network Partitions</ulink></para>

            </listitem>
            <listitem>
               <para><ulink url="#HP3.0HA/galera_cluster">MySQL Galera Cluster</ulink></para>

            </listitem>
            <listitem>
               <para><ulink url="#HP3.0HA/singleton_services">Singleton Services</ulink></para>

               <itemizedlist>
                  <listitem>
                     <para><ulink url="#HP3.0HA/cinder_volume">Cinder-Volume</ulink></para>

                  </listitem>
                  <listitem>
                     <para><ulink url="#HP3.0HA/nova_consoleauth">Nova-consoleauth</ulink></para>

                  </listitem>
               </itemizedlist>
            </listitem>
            <listitem>
               <para><ulink url="#HP3.0HA/failed_controller_nodes">Rebuilding or Replacing failed
              Controller Nodes</ulink></para>

            </listitem>
        </itemizedlist>
      </listitem>
      <listitem>
         <para><ulink url="#HP3.0HA/CVR">High Availability Routing - Centralized</ulink></para>

      </listitem>
      <listitem>
         <para><ulink url="#HP3.0HA/DVR">High Availability Routing - Distributed</ulink></para>

      </listitem>
      <listitem>
         <para><ulink url="#HP3.0HA/availability_zones">Availability Zones</ulink></para>

      </listitem>
      <listitem>
         <para><ulink url="#HP3.0HA/compute_kvm">Compute with KVM</ulink></para>

      </listitem>
      <listitem>
         <para><ulink url="#HP3.0HA/nova_availability_zones">Nova Availability Zones</ulink></para>

      </listitem>
      <listitem>
         <para><ulink url="#HP3.0HA/compute_esx">Compute with ESX Hypervisor</ulink></para>

      </listitem>
      <listitem>
         <para><ulink url="#HP3.0HA/block_storage_vsa">Block Storage with StoreVirtual VSA</ulink></para>

      </listitem>
      <listitem>
         <para><ulink url="#HP3.0HA/deploy_vsa_cluster">Deploy VSA cluster across Availability
          Zones/Racks</ulink></para>

      </listitem>
      <listitem>
         <para><ulink url="#HP3.0HA/cinder_availability_zones">Cinder Availability Zones</ulink></para>

      </listitem>
      <listitem>
         <para><ulink url="#HP3.0HA/object_storage_swift">Object Storage with Swift</ulink></para>

      </listitem>
      <listitem>
         <para><ulink url="#HP3.0HA/highly_available_app_workloads">Highly Available Cloud Applications
          and Workloads</ulink></para>

      </listitem>
      <listitem>
         <para><ulink url="#HP3.0HA/what_not_ha">What is not Highly Available?</ulink></para>

         <itemizedlist>
            <listitem>
               <para><ulink url="#HP3.0HA/deployer">Deployer</ulink></para>

            </listitem>
            <listitem>
               <para><ulink url="#HP3.0HA/control_plane">Control Plane</ulink></para>

            </listitem>
            <listitem>
               <para><ulink url="#HP3.0HA/keystone_cron">Keystone Cron Jobs</ulink></para>

            </listitem>
        </itemizedlist>
      </listitem>
      <listitem>
         <para><ulink url="#HP3.0HA/more_information">More Information</ulink></para>

      </listitem>
    </itemizedlist>


    
      <bridgehead renderas="sect4">High Availability Concepts Overview</bridgehead>
      <para>A highly available (HA) cloud ensures that a minimum level of cloud resources are always
        available on request, which results in uninterrupted operations for users.</para>

      <para>In order to achieve this high availability of infrastructure and workloads, we define the
        scope of HA to be limited to protecting these only against single points of failure (SPOF).
        Single points of failure include:</para>

      <itemizedlist>
        <listitem>
            <para><emphasis role="bold">Hardware SPOFs</emphasis>: Hardware failures can take the form of server failures, memory
          going bad, power failures, hypervisors crashing, hard disks dying, NIC cards breaking,
          switch ports failing, network cables loosening, and so forth.</para>

         </listitem>
        <listitem>
            <para><emphasis role="bold">Software SPOFs</emphasis>: Server processes can crash due to software defects, out-of-memory
          conditions, operating system kernel panic, and so forth.</para>

         </listitem>
      </itemizedlist>
      <para>By design, <phrase/> strives to create a system architecture resilient to
        SPOFs, and does not attempt to automatically protect the system against multiple cascading
        levels of failures; such cascading failures will result in an unpredictable state. Hence,
        the cloud operator is encouraged to recover and restore any failed component, as soon as the
        first level of failure occurs.</para>

   

    
      <bridgehead renderas="sect4">Highly Available Cloud Infrastructure</bridgehead>
      <para>The highly available cloud infrastructure consists of the following:</para>

      <itemizedlist>
        <listitem>
            <para>High Availability of Controllers</para>

         </listitem>
        <listitem>
            <para>Availability Zones</para>

         </listitem>
        <listitem>
            <para>Compute with KVM</para>

         </listitem>
        <listitem>
            <para>Nova Availability Zones</para>

         </listitem>
        <listitem>
            <para>Compute with ESX</para>

         </listitem>
        <listitem>
            <para>Block Storage with StoreVirtual VSA</para>

         </listitem>
        <listitem>
            <para>Object Storage with Swift</para>

         </listitem>
      </itemizedlist>
   

    
      <bridgehead renderas="sect4">High Availability of Controllers</bridgehead>
      <para>The <phrase/> installer deploys highly available configurations of
        OpenStack cloud services, resilient against single points of failure.</para>

      <para>The high availability of the controller components comes in two main forms.</para>

      <itemizedlist>
        <listitem>
            <para>Many services are stateless and multiple instances are run across the control plane in
          active-active mode. The API services (nova-api, cinder-api, etc.) are accessed through the
          HA proxy load balancer whereas the internal services (nova-scheduler, cinder-scheduler,
          etc.), are accessed through the message broker. These services use the database cluster to
          persist any data. </para>

            <note>
               <para>The HA proxy load balancer is also run in active-active mode and
            keepalived (used for Virtual IP (VIP) Management) is run in active-active mode, with
            only one keepalived instance holding the VIP at any one point in time.</para>

            </note>
         </listitem>
        <listitem>
            <para>The high availability of the message queue service and the database service is achieved
          by running these in a clustered mode across the three nodes of the control plane: RabbitMQ
          cluster with Mirrored Queues and Percona MySQL Galera cluster.</para>

         </listitem>
      </itemizedlist>
      <mediaobject id="ControlPlane1">
            <imageobject role="fo"><imagedata fileref="..-media-ha30-HPE_HA_Flow.png" width="75%" format="PNG"/></imageobject><imageobject role="html"><imagedata fileref="..-media-ha30-HPE_HA_Flow.png"/></imageobject>
         </mediaobject>

      <para>The above diagram illustrates the HA architecture with the focus on VIP management and load
        balancing. It only shows a subset of active-active API instances and does not show examples
        of other services such as nova-scheduler, cinder-scheduler, etc.</para>

      <para>In the above diagram, requests from an OpenStack client to the API services are sent to VIP
        and port combination; for example, 192.0.2.26:8774 for a Nova request. The load balancer
        listens for requests on that VIP and port. When it receives a request, it selects one of the
        controller nodes configured for handling Nova requests, in this particular case, and then
        forwards the request to the IP of the selected controller node on the same port.</para>

      <para>The nova-api service, which is listening for requests on the IP of its host machine, then
        receives the request and deals with it accordingly. The database service is also accessed
        through the load balancer. RabbitMQ, on the other hand, is not currently accessed through
        VIP/HA proxy as the clients are configured with the set of nodes in the RabbitMQ cluster and
        failover between cluster nodes is automatically handled by the clients.</para>

      <para>The sections below cover the following topics in detail:</para>

      <itemizedlist>
        <listitem>
            <para><ulink url="#HP3.0HA/api_request">API Request Message Flow</ulink></para>

         </listitem>
        <listitem>
            <para><ulink url="#HP3.0HA/node_failure">Handling Node Failure</ulink></para>

         </listitem>
        <listitem>
            <para><ulink url="#HP3.0HA/network_partitions">Handling Network Partitions</ulink></para>

         </listitem>
        <listitem>
            <para><ulink url="#HP3.0HA/galera_cluster">MySQL Galera Cluster</ulink></para>

         </listitem>
      </itemizedlist>
   

    
      <bridgehead renderas="sect4">High Availability Routing - Centralized</bridgehead>
      <para>Incorporating High Availability into a system involves implementing redundancies in the
        component that is being made highly available. In Centralized Virtual Router (CVR), that
        element is the Layer 3 agent a.k.a L3 agent. By making L3 agent highly available, upon
        failure all HA routers are migrated from the primary L3 agent to a secondary L3 agent. The
        implementation efficiency of an HA subsystem is measured by the number of packets that are
        lost when the secondary L3 agent is made the master. </para>

      <para>In <phrase/>, the primary and secondary L3 agents run continuously, and
        failover involves a rapid switchover of mastership to the secondary agent (IEFT RFC 5798).
        The failover essentially involves a switchover from a already running master to already
        running slave. This substantially reduces the latency of the HA. The mechanism used by the
        master and the slave to implement a failover is implemented using Linux’s pacemaker HA
        resource manager. This CRM (Cluster resource manager) uses VRRP (Virtual Router Redundancy
        Protocol) to implement the HA mechanism. VRRP is a industry standard protocol and defined in
        RFC 5798.</para>

      <mediaobject id="Layer3HA">
            <imageobject role="fo"><imagedata fileref="..-media-ha30-HPE_HA_Layer-3HA.png" width="75%" format="PNG"/></imageobject><imageobject role="html"><imagedata fileref="..-media-ha30-HPE_HA_Layer-3HA.png"/></imageobject>
         </mediaobject>

      <para>L3 HA uses of VRRP comes with several benefits.</para>

      <para>The primary benefit is the failover mechanism does not involve interprocess communication
        overhead (order of 10s of seconds). By not using an RPC mechanism to invoke the secondary
        agent to assume the primary agents role enables VRRP to achieve failover within 1-2 seconds. </para>

      <para>In VRRP, the primary and secondary routers are all active. As the routers are running, it
        is a matter of making the router aware of its primary/master status. This switchover takes
        less than 2 seconds instead of 60+ seconds it would have taken to start a backup router and
        failover. </para>

      <para>The failover depends upon a heartbeat link between the primary and secondary. That link in
          <phrase/> uses keepalived package of the pacemaker resource
        manager. The heartbeats are sent at a 2 second intervals between the primary and secondary.
        As per the VRRP protocol, if the secondary does not hear from the master after 3 intervals,
        it assumes the function of the primary. </para>

      <para>Further, all the routable IP addresses i.e. the VIPs (virtual IPs) are assigned to the
        primary agent. </para>

      <para>For information on more creating HA routers, see: <xref linkend="CreateHARouter"/></para>

   

    
      <bridgehead renderas="sect4">High Availability Routing - Distributed</bridgehead>
      <para>The OpenStack Distributed Virtual Router (DVR) function delivers HA through its distributed
        architecture. The one centralized function remaining is source network address translation
        (SNAT), where high availability is provided by DVR SNAT HA. </para>

      <para>DVR SNAT HA is enabled on a per router basis and requires that two or more L3 agents
        capable of providing SNAT services be running on the system. If a minimum number of L3
        agents is configured to 1 or lower, the neutron server will fail to start and a log message
        will be created. The L3 Agents must be running on a control-plane node, L3 agents running on
        a compute node do not provide SNAT services. </para>

      <para>For more information on creating HA routers, see: <xref linkend="CreateHARouter"/></para>

      
   

    
      <bridgehead renderas="sect4">Availability Zones</bridgehead>
      <mediaobject id="DeploymentZones">
            <imageobject role="fo"><imagedata fileref="..-media-ha30-HA_AvailabilityZones_3.png" width="75%" format="PNG"/></imageobject><imageobject role="html"><imagedata fileref="..-media-ha30-HA_AvailabilityZones_3.png"/></imageobject>
         </mediaobject>

      <para>While planning your OpenStack deployment, you should decide on how to zone various types of
        nodes - such as compute, block storage, and object storage. For example, you may decide to
        place all servers in the same rack in the same zone. For larger deployments, you may plan
        more elaborate redundancy schemes for redundant power, network ISP connection, and even
        physical firewalling between zones (<emphasis>this aspect is outside the scope of this
        document</emphasis>).</para>

      <para><phrase/> offers APIs, CLIs and Horizon UIs for the administrator to
        define and user to consume, availability zones for Nova, Cinder and Swift services. This
        section outlines the process to deploy specific types of nodes to specific physical servers,
        and makes a statement of available support for these types of availability zones in the
        current release.</para>

      <note>
         <para>By default, <phrase/> is deployed in a single availability zone upon
        installation. Multiple availability zones can be configured by an administrator
        post-install, if required. Refer to the <ulink url="http://docs.openstack.org/openstack-ops/content/scaling.html">Chapter 5: Scaling</ulink> (in the OpenStack Operations Guide for more
        information).</para>

      </note>
   


    
      <bridgehead renderas="sect4">Compute with KVM</bridgehead>
      <para>You can deploy your KVM nova-compute nodes either during initial installation, or by adding
        compute nodes post initial installation.</para>

      <para>While adding compute nodes post initial installation, you can specify the target physical
        servers for deploying the compute nodes.</para>

      <para>Learn more about <xref linkend="add_compute_node"/></para>

   

    
      <bridgehead renderas="sect4">Nova Availability Zones</bridgehead>
      <para>Nova host aggregates and Nova availability zones can be used to segregate Nova compute
        nodes across different failure zones.</para>

   

    
      <bridgehead renderas="sect4">Compute with ESX Hypervisor</bridgehead>
      <para>Compute nodes deployed on ESX Hypervisor can be made highly available using the HA feature
        of VMware ESX Clusters. For more information on VMware HA, please refer to your VMware ESX
        documentation.</para>

   

    
      <bridgehead renderas="sect4">Block Storage with StoreVirtual VSA</bridgehead>
      <para>Highly available Cinder block storage volumes are provided by the network RAID 10
        implementation in the HPE StoreVirtual VSA software. You can deploy the VSA nodes in three
        node cluster and specify Network RAID 10 protection for Cinder volumes.</para>

      <para>The underlying SAN/iQ operating system of the StoreVirtual VSA ensures that the two-way
        replication maintains two mirrored copies of data for each volume.</para>

      <para>This Network RAID 10 capability ensures that failure of any single server does not cause
        data loss, and maintains data access to the clients.</para>

      <para>Furthermore, each of the VSA nodes of the cluster can be strategically deployed in
        different zones of your data center for maximum redundancy and resiliency. For more
        information on how to deploy VSA nodes on desired target servers, refer to the <xref linkend="config_vsa"/> document.</para>

      <mediaobject id="BlockStorage">
            <imageobject role="fo"><imagedata fileref="..-media-ha30-ha-block-storage.png" width="75%" format="PNG"/></imageobject><imageobject role="html"><imagedata fileref="..-media-ha30-ha-block-storage.png"/></imageobject>
         </mediaobject>

   

    
      <bridgehead renderas="sect4">Deploy VSA cluster across Availability Zones/Racks</bridgehead>
      <para>In the Availablity Zone image above, the input model example has 3 VSA servers in three
        different server-groups (Racks) (server-groups are are logical separations). You can
        configure these server-groups in different physical Racks to provide the required hardware
        isolation. See input model examples for <xref linkend="entryscale_kvm_vsa"/>, <xref linkend="example_entryscale_esx_mml"/>, and <xref linkend="midscale_kvm_vsa"/></para>

      <para>The recommended configuration for a VSA Cluster is to use RAID 10 with 3 mirrors to
        guarantee that data is replicated across 3 VSA nodes spreading across AZs/Racks. Using one
        of the two options below, you can expand storage capacity by adding 3 more nodes.</para>
<orderedlist>
            <listitem>
               <para>Add new three VSA nodes to the existing cluster and ensure that each new VSA node is
            on different AZ/Rack. </para>
               <note>
                  <para>There is a 1500 volumes limit per VSA cluster.</para>
               </note>
            </listitem>
            <listitem>
               <para>Create a new VSA cluster with the 3 new nodes.</para>
            </listitem>
        </orderedlist>

   

    
      <bridgehead renderas="sect4">Cinder Availability Zones</bridgehead>
      <para>Cinder availability zones are not supported for general consumption in the current
        release.</para>

   

    
      <bridgehead renderas="sect4">Object Storage with Swift</bridgehead>
      <para>High availability in Swift is achieved at two levels.</para>

      <para><emphasis role="bold">Control Plane</emphasis></para>

      <para>The Swift API is served by multiple Swift proxy nodes. Client requests are directed to all
        Swift proxy nodes by the HA Proxy load balancer in round-robin fashion. The HA Proxy load
        balancer regularly checks the node is responding, so that if it fails, traffic is directed
        to the remaining nodes. The Swift service will continue to operate and respond to client
        requests as long as at least one Swift proxy server is running.</para>

      <para>If a Swift proxy node fails in the middle of a transaction, the transaction fails. However
        it is standard practice for Swift clients to retry operations. This is transparent to
        applications that use the python-swiftclient library.</para>

      <para>The entry-scale example cloud models contain three Swift proxy nodes. However, it is
        possible to add additional clusters with additional Swift proxy nodes to handle a larger
        workload or to provide additional resiliency.</para>

      <para><emphasis role="bold">Data</emphasis></para>

      <para>Multiple replicas of all data is stored. This happens for account, container and object
        data. The example cloud models recommend a replica count of three. However, you may change
        this to a higher value if needed.</para>

      <para>When Swift stores different replicas of the same item on disk, it ensures that as far as
        possible, each replica is stored in a different zone, server or drive. This means that if a
        single server of disk drives fails, there should be two copies of the item on other servers
        or disk drives.</para>

      <para>If a disk drive is failed, Swift will continue to store three replicas. The replicas that
        would normally be stored on the failed drive are “handed off” to another drive on the
        system. When the failed drive is replaced, the data on that drive is reconstructed by the
        replication process. The replication process re-creates the “missing” replicas by copying
        them to the drive using one of the other remaining replicas. While this is happening, Swift
        can continue to store and retrieve data.</para>

   

    
      <bridgehead renderas="sect4">Highly Available Cloud Applications and Workloads</bridgehead>
      <para>Projects writing applications to be deployed in the cloud must be aware of the cloud
        architecture and potential points of failure and architect their applications accordingly
        for high availability.</para>

      <para>Some guidelines for consideration:</para>

      <orderedlist>
        <listitem>
            <para>Assume intermittent failures and plan for retries </para>

            <itemizedlist>
               <listitem>
                  <para><emphasis role="bold">OpenStack Service APIs</emphasis>: invocations can fail - you should carefully evaluate
              the response of each invocation, and retry in case of failures.</para>

               </listitem>
               <listitem>
                  <para><emphasis role="bold">Compute</emphasis>: VMs can die - monitor and restart them</para>

               </listitem>
               <listitem>
                  <para><emphasis role="bold">Network</emphasis>: Network calls can fail - retry should be successful</para>

               </listitem>
               <listitem>
                  <para><emphasis role="bold">Storage</emphasis>: Storage connection can hiccup - retry should be successful</para>

               </listitem>
            </itemizedlist>
         </listitem>
        <listitem>
            <para>Build redundancy into your application tiers </para>

            <itemizedlist>
               <listitem>
                  <itemizedlist>
                     <listitem>
                        <para>Replicate VMs containing stateless services such as Web application tier or Web
                  service API tier and put them behind load balancers (you must implement your own
                  HA Proxy type load balancer in your application VMs until <phrase/> delivers the LBaaS service). </para>

                     </listitem>
                     <listitem>
                        <para>Boot the replicated VMs into different Nova availability zones.</para>

                     </listitem>
                     <listitem>
                        <para>If your VM stores state information on its local disk (Ephemeral Storage), and
                  you cannot afford to lose it, then boot the VM off a Cinder volume.</para>

                     </listitem>
                     <listitem>
                        <para>Take periodic snapshots of the VM which will back it up to Swift through
                  Glance.</para>

                     </listitem>
                     <listitem>
                        <para>Your data on ephemeral may get corrupted (but not your backup data in Swift and
                  not your data on Cinder volumes).</para>

                     </listitem>
                     <listitem>
                        <para>Take regular snapshots of Cinder volumes and also back up Cinder volumes or your
                  data exports into Swift.</para>

                     </listitem>
                  </itemizedlist>
               </listitem>
            </itemizedlist>
         </listitem>
        <listitem>
            <para>Instead of rolling your own highly available stateful services, use readily available
            <phrase/> platform services such as Designate, the DNS service. </para>

         </listitem>
      </orderedlist>
   

    <formalpara id="what_not_ha">
      <title>What is not Highly Available?</title>
      
   </formalpara>

    
      <bridgehead renderas="sect4">More Information</bridgehead>
      <itemizedlist>
        <listitem>
            <para><ulink url="http://docs.openstack.org/high-availability-guide/content/ch-intro.html">OpenStack
            High-availability Guide</ulink></para>

         </listitem>
        <listitem>
            <para><ulink url="http://12factor.net/">12-Factor
          Apps</ulink></para>

         </listitem>
      </itemizedlist>
   
  </section>
