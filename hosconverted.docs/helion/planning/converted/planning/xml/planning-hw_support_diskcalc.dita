<?xml version="1.0"?>
<!DOCTYPE section PUBLIC "-//OASIS//DTD DocBook XML V4.5//EN" "http://www.oasis-open.org/docbook/xml/4.5/docbookx.dtd" [ <!ENTITY % entities SYSTEM "entities.xml"> %entities; ]><section id="hw_support_diskcalc">
   <title>
      <phrase/>Disk Calculator</title>
    
      <bridgehead renderas="sect4">Disk Calculator for Compute-Centric Deployments</bridgehead>
      <para>This topic provides guidance on how to estimate the amount of disk space required for a
        compute-centric <phrase/> deployment. To accurately estimate the disk space
        needed, it is important to understand how Helion utilizes resources. Although there are a
        variety of factors, including the number of compute nodes, a large portion of the
        utilization is driven by operational tools, such as monitoring, metering, and logging.</para>

      <important>
         <para>The disk calculator does not accurately estimate a Swift-centric
        deployment at this time. For more information on Swift, see the <xref linkend="rec_min_swift"/> topic.</para>

      </important>
      <para>The usage of disk space by operational tools can be estimated from the following
        parameters: </para>
<itemizedlist>
            <listitem>
               <para><emphasis role="bold">Number of compute nodes</emphasis> + <emphasis role="bold">Number of VM's running on each compute
            node</emphasis></para>
            </listitem>
            <listitem>
               <para><emphasis role="bold">Number of services being monitored or metered</emphasis> + <emphasis role="bold">Amount of logs
            created</emphasis></para>
            </listitem>
            <listitem>
               <para><emphasis role="bold">Retention periods for operational data</emphasis> (for Elastic Search, Vertica/InfluxDB,
            and Kafka)</para>
            </listitem>
        </itemizedlist>

      <important>
         <para>If you also enable auditing, follow the steps in the <ulink url="#hw_support_diskcalc/disk_calc_audit_adj">Audit Logging
          Adjustment</ulink> section to enter additional input parameters.</para>

      </important>
      <para><emphasis role="bold">Disk Estimation Process</emphasis></para>

      <para><phrase/> provides entry scale and scale-out models for deployment. This
        disk estimation tool, currently in a spreadsheet form, helps you decide which disk model to
        start from as well as what customizations you need to meet your deployment requirements. The
        disk estimation process also provides default settings and minimum values for the parameters
        that drive disk size. </para>
<important>
            <para>Kafka is the queuing system used to process
          metering monitoring and logging (MML) data. Kakfa stores the queued data on disk, so the
          disk space available will have a large impact on the amount of data the MML systems can
          process. Providing less than the minimum disk space for Kakfa will result in loss of MML
          data and can affect other components on the control plane. The default for Kafka is 1 hour
          which is 17 GB.</para>
         </important>

      <para><emphasis role="bold">To estimate the disk sizes required for your deployment:</emphasis></para>

      <orderedlist>
        <listitem>
            <para><ulink url="#hw_support_diskcalc/disk_calc_input">Enter input
            parameters.</ulink></para>

         </listitem>
        <listitem>
            <para>If you also enable auditing, follow the steps in the <ulink url="#hw_support_diskcalc/disk_calc_audit_adj">Audit Logging
            Adjustment</ulink> section to enter additional input parameters.</para>

         </listitem>
        <listitem>
            <para><ulink url="#hw_support_diskcalc/disk_calc_select">Select the deployment
            model you want to support based on the calculations.</ulink></para>

         </listitem>
        <listitem>
            <para><ulink url="#hw_support_diskcalc/disk_calc_match">Match the selected
            deployment to a disk model example.</ulink></para>

         </listitem>
      </orderedlist>
   
    
      <bridgehead renderas="sect4">Enter Input Parameters</bridgehead>
      <para>The Disk Calculator
        spreadsheet automatically displays the minimum requirements for the components that define
        disk size. You can replace the default values with either the number you have to work with
        or the number that you want to support. </para>
<important>
            <para>If you want to enable audit
          logging, follow the steps in the <ulink url="#hw_support_diskcalc/disk_calc_audit_adj">Audit Logging
            Adjustment</ulink> section to enter additional input parameters.</para>
         </important>

      <informaltable id="table_disk_calc_input" colsep="1" rowsep="1">
         <tgroup cols="3">
            <colspec colname="c1" colnum="1"/>
            <colspec colname="c2" colnum="2"/>
            <colspec colname="c3" colnum="3"/>
            <thead>
               <row>
                  <entry>Input Parameter</entry>
                  <entry>Default</entry>
                  <entry>Minimum</entry>
               </row>
            </thead>
            <tbody>
               <row>
                  <entry>System Memory</entry>
                  <entry>64 GB</entry>
                  <entry>64 GB</entry>
               </row>
               <row>
                  <entry>Compute Nodes</entry>
                  <entry>100</entry>
                  <entry>100</entry>
               </row>
               <row>
                  <entry>VMs per Compute Node</entry>
                  <entry>40</entry>
                  <entry>40</entry>
               </row>
               <row>
                  <entry>Component: Vertica</entry>
                  <entry>45 days retention period</entry>
                  <entry>30 days</entry>
               </row>
               <row>
                  <entry>Component: Logging</entry>
                  <entry>22 services covered <para>7 days retention period</para>

                  </entry>
                  <entry>
                     7 days retention period</entry>
               </row>
               <row>
                  <entry>Component: Kafka (message queue)</entry>
                  <entry>0.17 of an hour retention period</entry>
                  <entry>0.042 of an hour retention period</entry>
               </row>
               <row>
                  <entry>Component: Elastic Search (log storage)</entry>
                  <entry>7 days retention period</entry>
                  <entry>7 days retention period</entry>
               </row>
               <row>
                  <entry>Component: Audit</entry>
                  <entry>0 days retention period</entry>
                  <entry>0 days retention period</entry>
               </row>
            </tbody>
        </tgroup>
      </informaltable>
      <para> The following diagram shows the input parameters in the spreadsheet. </para>

      <mediaobject>
            <imageobject role="fo"><imagedata fileref="..-media-DiskCalc1.png" width="75%" format="PNG"/></imageobject><imageobject role="html"><imagedata fileref="..-media-DiskCalc1.png"/></imageobject>
         </mediaobject>

      <para>To provide the paramters required to estimate disk size:</para>

      <orderedlist>
        <listitem>
            <para><ulink url="../../media/hos.docs/diskcalc40.zip">Open the disk calculator
            spreadsheet</ulink>.</para>

         </listitem>
        <listitem>
            <para>At the bottom of the spreadsheet, click on the <emphasis role="bold">Draft Sizing Tool4</emphasis> tab.</para>

         </listitem>
        <listitem>
            <para> To set the server RAM size, replace the default value in the <emphasis role="bold">System Memory</emphasis>
          field.</para>

         </listitem>
        <listitem>
            <para>To set the number of compute nodes, replace the default in the <emphasis role="bold">Compute Nodes</emphasis>
          field.</para>

         </listitem>
        <listitem>
            <para>To set the average number of virtual machines per compute node, replace the default in
          the <emphasis role="bold">VM's per Compute Node</emphasis> field.</para>

         </listitem>
        <listitem>
            <para>To set the number of days you want the metering and logging files retained, replace the
          default in the <emphasis role="bold">Vertica Retention Period</emphasis> field.</para>

         </listitem>
        <listitem>
            <para>To set logging values, replace the default in <emphasis role="bold">Number of Services Covered</emphasis> and
            <emphasis role="bold">Retention Period</emphasis>. </para>

            <important>
               <para>If you enable additional logging of
            services than those set by default, then you must increase the number in the <emphasis role="bold">Logging
              Number of Services</emphasis> Field.</para>

            </important>
         </listitem>
        <listitem>
            <para>To set a value for Kafka messages to be retained, replace the default in the <emphasis role="bold">Kafka
            Retention Period</emphasis> field.</para>

         </listitem>
        <listitem>
            <para>To set a value for Elastic Search log file retention, replace the default in the
            <emphasis role="bold">Elastic Search Retention Period</emphasis> field.</para>

         </listitem>
        <listitem>
            <para>To set a value for Audit logging file retention, replace the default in the <emphasis role="bold">Audit
            Retention Period</emphasis> field.</para>

         </listitem>
      </orderedlist>
   
    
      <bridgehead renderas="sect4">Audit Logging Adjustment</bridgehead>
      <para>If you want to enable audit logging, you must enter additional input parameters to ensure
        there is enough room to retain the audit logs. The following diagram shows the parameters
        you need to specify in the Disk Calculator spreadsheet.</para>

      <mediaobject>
            <imageobject role="fo"><imagedata fileref="..-media-DiskCalc2.png" width="75%" format="PNG"/></imageobject><imageobject role="html"><imagedata fileref="..-media-DiskCalc2.png"/></imageobject>
         </mediaobject>

      <para>To add audit logging to disk size calculations:</para>

      <orderedlist>
        <listitem>
            <para>Determine which services you have enabled to collect audit logging information. This is
          part of HLM configuration.</para>

         </listitem>
        <listitem>
            <para>Enter the number of Audit Enabled services on cluster.  Auditing is disabled by default, 
          so these values will initially be 0.  If audit logging is enabled, initial suggested 
          values would be 9 for API/Core Services, 1 for Networking, 1 for Swift, and 2 for MMLB.
        
            </para>

            <important>
               <para>If you enable logging for services beyond the defaults, you must
            change the <emphasis role="bold">Number of Services on a Cluster</emphasis> field in the spreadsheet. It is
            recommended that you increase the total services covered as well as increment the number
            on the appropriate cluster. For example, if you enable Apache logs on the core services,
            then the total would increase to 23 and the api/core services entry would change from 13
            to 14.</para>

            </important>
         </listitem>
        <listitem>
            <para>To include Glance image space in your estimation, determine the size of the images that
          will be cached.</para>

         </listitem>
        <listitem>
            <para>Enter the total size needed to store Glance images in the
            <emphasis role="bold">/var/lib/glance/work_dir</emphasis> field.</para>

         </listitem>
      </orderedlist>
   
    
      <bridgehead renderas="sect4">Select the Deployment Model</bridgehead>
      <para>To decide which architecture will meet all of your requirements, use the values given in
        the Disk Calculator spreadsheet. Keeping in mind the rough scale you expect to target as
        well as any need to separate services, choose an Entry Scale, Entry Scale MML, or Mid Scale
        deployment. Once you have chosen a deployment you can match it to the sample disk models in
        the <ulink url="#hw_support_diskcalc/disk_calc_match">Match to a Disk
          Model</ulink> section. The following diagram shows the deployment options that are
        recommended if you use the default values in the Disk Calculator spreadsheet.</para>

      <mediaobject id="image_cmk_lxc_wv">
            <imageobject role="fo"><imagedata fileref="..-media-DiskCalc3.png" width="75%" format="PNG"/></imageobject><imageobject role="html"><imagedata fileref="..-media-DiskCalc3.png"/></imageobject>
         </mediaobject>

      <para>For example, in the above diagram, if you wanted to choose an Entry Scale MML deployment,
        the calculator recommends the following disk sizes:</para>

      <itemizedlist id="ul_uwg_fs2_wv">
        <listitem>
            <para>216GB for API/Core Service</para>

         </listitem>
        <listitem>
            <para>216GB for Neutron (networking)</para>

         </listitem>
        <listitem>
            <para>216GB for Swift (storage)</para>

         </listitem>
        <listitem>
            <para>573GB for MMLB</para>

         </listitem>
        <listitem>
            <para>252GB for MySQL/RabbitMQ</para>

         </listitem>
      </itemizedlist>
   
    
      <bridgehead renderas="sect4">Match to a Disk Model</bridgehead>
      <para>For each of the entry-scale and scale-out cloud models, there is a set of associated disk
        models that can be used as the basis for your deployment. These models provide examples of
        pontetial parameters for operational tools and are expected to be used as the starting point
        for actual deployments. Since each deployment can vary greatly, the disk calculator
        spreadsheet provides a way to create the basic disk model and customize it to fit the
        specific parameters your deployment. Once you have estimated disk sizes and chosen a
        deployment architecture, you can choose which example disk partitioning file to use from the
        tables below. Keep in mind if you are enabling more options than are listed in the Disk
        Calculator, or if you want to plan for growth, you will need to manually adjust paramters as
        necessary.</para>

      <para>Disk models are provided for each deployment option based on the expected size of the disk
        available to the control plane nodes. The available space is then partitioned by percentage
        to be allocated to each of the required volumes on the control plane. Each of the disk
        models is targeted at a specific set of parameters which can be found in the following
        tables:</para>

      <itemizedlist>
        <listitem>
            <para><ulink url="#hw_support_diskcalc/disk_calc_entry">Entry Scale:</ulink> 600
          GB, 1 TB</para>

         </listitem>
        <listitem>
            <para><ulink url="#hw_support_diskcalc/disk_calc_Mid">Mid Scale/ Entry Scale
            MML Servers:</ulink> 600 GB, 2 TB, 4.5 TB</para>

         </listitem>
      </itemizedlist>
   
    
      <bridgehead renderas="sect4">Entry Scale Disk Models</bridgehead>
      <para>These models include a single cluster of control plane nodes and all services.</para>

      <para><emphasis role="bold">600GB Entry Scale</emphasis></para>

      <informaltable id="table_disk_matchE6" colsep="1" rowsep="1">
         <tgroup cols="2">
            <colspec colname="c1" colnum="1" colwidth="1*"/>
            <colspec colname="c2" colnum="2" colwidth="2*"/>
            <thead>
               <row>
                  <entry>Component</entry>
                  <entry>Parameters</entry>
               </row>
            </thead>
            <tbody>
               <row>
                  <entry>compute nodes</entry>
                  <entry>100 <para>This model provides lower than recommended retention and should only be
                  used for POC deployments.</para>

                  </entry>
               </row>
            </tbody>
        </tgroup>
      </informaltable>
      <para><emphasis role="bold">1TB Entry Scale</emphasis></para>

      <informaltable id="table_disk_matchE1" colsep="1" rowsep="1">
         <tgroup cols="2">
            <colspec colname="c1" colnum="1" colwidth="1*"/>
            <colspec colname="c2" colnum="2" colwidth="2*"/>
            <thead>
               <row>
                  <entry>Component</entry>
                  <entry>Parameters</entry>
               </row>
            </thead>
            <tbody>
               <row>
                  <entry>compute nodes</entry>
                  <entry>100</entry>
               </row>
               <row>
                  <entry>local logging<para>var/log</para>

                  </entry>
                  <entry>7 day retention</entry>
               </row>
               <row>
                  <entry>metering/monitoring<para>/var/vertica</para>

                  </entry>
                  <entry>45 day retention</entry>
               </row>
               <row>
                  <entry>centralized logging<para>/var/lib/elasticsearch</para>

                  </entry>
                  <entry>7 day retention</entry>
               </row>
               <row>
                  <entry>Kafka Message Queue<para>/var/kafka</para>

                  </entry>
                  <entry>4 hour retention</entry>
               </row>
            </tbody>
        </tgroup>
      </informaltable>
   
    
      <bridgehead renderas="sect4">MML Disk Models</bridgehead>
      <para>These mid-scale and entry-scale MML models include seperate control plane nodes for core
        services, metering/monitoring/logging, and MySQL/RabbitMQ. Optionally you can also seperate
        out Swift (storage) and Neutron (networking). MML servers are the ones that will need
        modification based on the scale and operational parameters.</para>

      <para><emphasis role="bold">600GB MML Server</emphasis></para>

      <informaltable id="table_disk_matchM6" colsep="1" rowsep="1">
         <tgroup cols="2">
            <colspec colname="c1" colnum="1" colwidth="1*"/>
            <colspec colname="c2" colnum="2" colwidth="2*"/>
            <thead>
               <row>
                  <entry>Component</entry>
                  <entry>Parameters</entry>
               </row>
            </thead>
            <tbody>
               <row>
                  <entry>compute nodes</entry>
                  <entry>100</entry>
               </row>
               <row>
                  <entry>local logging<para>var/log</para>

                  </entry>
                  <entry>7 day retention</entry>
               </row>
               <row>
                  <entry>metering/monitoring<para>/var/vertica</para>

                  </entry>
                  <entry>30 day retention<caution>
                           <para>45 days is the default minimum.</para>
                        </caution>

                  </entry>
               </row>
               <row>
                  <entry>centralized logging<para>/var/lib/elasticsearch</para>

                  </entry>
                  <entry>7 day retention</entry>
               </row>
               <row>
                  <entry>Kafka Message Queue<para>/var/kafka</para>

                  </entry>
                  <entry>4 hour retention</entry>
               </row>
            </tbody>
        </tgroup>
      </informaltable>
      <para><emphasis role="bold">2TB MML Server</emphasis></para>

      <informaltable id="table_disk_matchM2T" colsep="1" rowsep="1">
         <tgroup cols="2">
            <colspec colname="c1" colnum="1" colwidth="1*"/>
            <colspec colname="c2" colnum="2" colwidth="2*"/>
            <thead>
               <row>
                  <entry>Component</entry>
                  <entry>Parameters</entry>
               </row>
            </thead>
            <tbody>
               <row>
                  <entry>compute nodes</entry>
                  <entry>200</entry>
               </row>
               <row>
                  <entry>local logging<para>var/log</para>

                  </entry>
                  <entry>7 day retention</entry>
               </row>
               <row>
                  <entry>metering/monitoring<para>/var/vertica</para>

                  </entry>
                  <entry>45 day retention</entry>
               </row>
               <row>
                  <entry>centralized logging<para>/var/lib/elasticsearch</para>

                  </entry>
                  <entry>7 day retention</entry>
               </row>
               <row>
                  <entry>Kafka Message Queue<para>/var/kafka</para>

                  </entry>
                  <entry>12 hour retention</entry>
               </row>
            </tbody>
        </tgroup>
      </informaltable>
      <para><emphasis role="bold">4.5TB MML Server</emphasis></para>

      <informaltable id="table_disk_matchM4T" colsep="1" rowsep="1">
         <tgroup cols="2">
            <colspec colname="c1" colnum="1" colwidth="1*"/>
            <colspec colname="c2" colnum="2" colwidth="2*"/>
            <thead>
               <row>
                  <entry>Component</entry>
                  <entry>Parameters</entry>
               </row>
            </thead>
            <tbody>
               <row>
                  <entry>compute nodes</entry>
                  <entry>200</entry>
               </row>
               <row>
                  <entry>local logging<para>var/log</para>

                  </entry>
                  <entry>7 day retention</entry>
               </row>
               <row>
                  <entry>metering/monitoring<para>/var/vertica</para>

                  </entry>
                  <entry>45 day retention</entry>
               </row>
               <row>
                  <entry>centralized logging<para>/var/lib/elasticsearch</para>

                  </entry>
                  <entry>45 day retention</entry>
               </row>
               <row>
                  <entry>Kafka Message Queue<para>/var/kafka</para>

                  </entry>
                  <entry>12 hour retention</entry>
               </row>
            </tbody>
        </tgroup>
      </informaltable>
   

    
      <bridgehead renderas="sect4">Notes about disk sizing for Cinder bootable volumes</bridgehead>
      <para>When creating your disk model for nodes that will have the cinder volume role make sure
        that there is sufficient disk space allocated for a temporary space for image conversion if
        you will be creating bootable volumes.</para>

      <para>By default, Cinder uses <literal>/var/lib/cinder</literal> for image conversion and this will
        be on the root filesystem unless it is explicitly separated. You can ensure there is enough
        space by ensuring that the root file system is sufficiently large, or by creating a logical
        volume mounted at <literal>/var/lib/cinder</literal> in the disk model when installing the
        system.</para>

      <para>If you have post-installation issues with creating bootable volumes, see the <citetitle>FIXME: broken external xref</citetitle> documentation for steps to resolve these issues.</para>

   
  </section>
