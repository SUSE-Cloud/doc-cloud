<?xml version="1.0" encoding="UTF-8"?>
<!--Edit status: not edited-->
<!DOCTYPE topic
  PUBLIC "-//OASIS//DTD DITA Topic//EN" "http://docs.oasis-open.org/dita/v1.1/OS/dtd/topic.dtd" ><topic xml:lang="en-us" id="topic6490">
<title>HPE Helion <tm tmtype="reg">OpenStack</tm> Icinga Service</title>
<prolog>
<metadata>
<othermeta name="layout" content="default"/>
</metadata>
</prolog>
<body>
<p>
<!--PUBLISHED-->
 <!--./community/community.services-icinga.md-->
 <!--permalink: /helion/community/services/icinga/--></p>
<p>The <b>
<i>Icinga</i>
</b> service, which runs in the undercloud, helps cloud admins monitor the disk usage of Swift storage node(s).</p>
<p>HPE Helion OpenStack allows you to monitor Swift cluster in any of the followings ways:</p>
<ul>
<li>
<xref type="section" href="#topic6490/diskuse">Monitor Disk Usage</xref>
</li>
<li>
<xref type="section" href="#topic6490/replication">Replication Status using Icinga</xref>
</li>
<li>
<xref type="section" href="#topic6490/health">Service Health Check</xref>
</li>
</ul>
<section id="prerequisites"> <title>Prerequisites</title>
<ul>
<li>HPE Helion OpenStack cloud is successfully deployed. </li>
<li>All nodes are functional by default as they are part of cloud deployment</li>
<li>The Icinga service is active and running in the undercloud</li>
</ul>
</section>
<section id="diskuse"> <title>Monitoring Disk Usage</title>
<p>Icinga checks the following:</p>
<ul>
<li>That the file systems used by the servers are mounted. </li>
<li>The disk usage (by percentage) for the devices in use by Swift (/srv/node). </li>
</ul>
<p>Note that other file systems such as <codeph>/usr</codeph> or <codeph>/var</codeph> are not monitored.</p>
<p>Perform the following steps to monitor the usage of Swift disk:</p>
<ol>
<li>
<p>In the undercloud Horizon dashboard, log in to the <b>Icinga Dashboard</b> (http://&lt;undercloud_IP&gt;/icinga/). The default login credentials are as follows:</p>

<ul>
<li>Username: <i>icingaadmin</i>
</li>
<li>Password: <i>icingaadmin</i> </li>
</ul>
</li>
<li>
<p>Click <b>Status</b> on the left panel and then click <b>Host Details</b>.
<!--A BR tag was used here in the original source.--><image href="../media/icinga_host-details.png" placement="break"/>
</p>
</li>
<li>
<p>In the <b>Host</b> column, click the icon next to the host IP (with tooltip that shows View Service Details For This Host) of the Swift storage node that you want to monitor.</p>
</li>
<li>
<p>The <b>Service Status Details For Host All Hosts</b> page opens.
<!--A BR tag was used here in the original source.--><image href="../media/swift_icinga_view-details.png" placement="break"/>
</p>
</li>
<li>
<p>Click the target Swift node IP address to open the <b>Service Status Details For Host &lt;Swift node IP address &gt;</b> to view the disk usage of the selected Swift node.
<!--A BR tag was used here in the original source.--><image href="../media/swift_icinga-disk-usage.png" placement="break"/>
</p>
</li>
</ol>
</section>
<section id="status-messages-for-disk-usage-monitoring"> <title>Status Messages for disk usage monitoring</title>
<p>The Icinga service presents the following messages to indicate the disk usage status of the cloud:</p>
<table>
<tgroup cols="3">
<colspec colname="col1"/>
<colspec colname="col2"/>
<colspec colname="col3"/>
<thead>
<row>
<entry>Status</entry>
    <entry>
<required-cleanup remap="center">
<ph>Message</ph>
</required-cleanup>
</entry>
    <entry>
<required-cleanup remap="center">
<ph>Cause/Resolution</ph>
</required-cleanup>
</entry>
</row>
</thead>
<tbody>
<row>
<entry>OK</entry>
    <entry>No devices to report</entry>
    <entry> This message appears on Proxy servers where there are no account, container or object servers configured. This is a normal status.</entry>
</row>
<row>
<entry>OK </entry>
    <entry>Percent used</entry>
    <entry> Percent disk usage for devices used by Swift (/srv/node)</entry>
</row>
<row>
<entry>WARNING </entry>
    <entry>Disk space low</entry>
    <entry>The percentage used space of one of the disk drives exceeds the user defined threshold(Default set to 85% for HPE Helion OpenStack 1.0). It is important to prevent Swift devices becoming full because it is difficult to recover if this happens. To resolve, add more devices to the rings or ask your users to delete objects.</entry>
</row>
<row>
<entry>FAIL </entry>
    <entry>Disk space critically low</entry>
    <entry>The available space on one of the disk drives has dropped below the "fallocate_reserve" given in &lt;object-server-configuration&gt;. If no value is given in the object server configuration file, this value defaults to zero. Swift cannot store more data on the drive if the available space drops below this defined limit.</entry>
</row>
<row>
<entry>FAIL </entry>
    <entry>Not mounted</entry>
    <entry> The named device is not mounted. The device may have failed to mount or was unmounted due to an error. To resolve, stop all Swift processes, mount all devices and restart Swift.</entry>
</row>
<row>
<entry>UNKNOWN</entry>
    <entry>No devices to report</entry>
    <entry/>
</row>
</tbody>
</tgroup>
</table>
<p>
  <xref href="#topic6490"> Return to Top </xref>
</p>
</section>
<section id="repication"> <title>Monitor Node Replication Status using Icinga</title>
<p>Cloud admins can use the <b>
<i>icinga</i>
</b> service running in the undercloud to monitor the replication status of nodes within the Swift deployment.</p>
<p>Perform the following steps to monitor the replication status:</p>
<ol>
<li>
<p>In the undercloud, log in to the <b>Icinga Dashboard</b> (http://&lt;undercloud_IP&gt;/icinga/) The default login credentials are as follows:</p>

<ul>
<li>Username: <i>icingaadmin</i>
</li>
<li>Password: <i>icingaadmin</i> </li>
</ul>
</li>
<li>
<p>Click <b>Status</b> on the left panel and then click <b>Host Details</b>.
<!--A BR tag was used here in the original source.--><image href="../media/icinga_host-details.png" placement="break"/>
</p>
</li>
<li>
<p>In the <b>Host</b> column, click the icon next to the host IP to see <b>Host Status Details For All Hosts</b>.
<!--A BR tag was used here in the original source.--><image href="../media/swift_icinga_view-details.png" placement="break"/>
</p>
</li>
<li>Click the target Swift node IP address to open the <b>Service Status Details For Host <varname>IP Address</varname>
</b> to see the replication status of the selected Swift node.
<!--A BR tag was used here in the original source.--><image href="../media/swift_icinga-replication-status.png" placement="break"/>
</li>
</ol>
</section>
<section id="status-messages-for-replication-monitoring"> <title>Status Messages for replication monitoring</title>
<p>The Icinga service presents the following messages to indicate the replication status of the cloud:</p>
<table>
<tgroup cols="3">
<colspec colname="col1"/>
<colspec colname="col2"/>
<colspec colname="col3"/>
<thead>
<row>
<entry>Status</entry>
    <entry>
<required-cleanup remap="center">
<ph>Message</ph>
</required-cleanup>
</entry>
    <entry>
<required-cleanup remap="center">
<ph>Cause/Resolution</ph>
</required-cleanup>
</entry>
</row>
</thead>
<tbody>
<row>
<entry>OK</entry>
    <entry>Ok</entry>
    <entry>Everything is fine.</entry>
</row>
<row>
<entry>WARNING </entry>
    <entry>Replication not progressing</entry>
    <entry>An account, container or object replicator process has not finished a complete scan recently(elapsed time exceeded the threshold defined) . Examine the account-replicator, container-replicator and object-replicator logs to see which process is "stuck". Restarting the relevant process usually resolves the issue.</entry>
</row>
<row>
<entry>FAIL </entry>
    <entry>Replication never completed</entry>
    <entry> The named replication process has never run on this node. Check that the replicator has been configured and started. Examine the log files to see if the replicator process has reported problems.</entry>
</row>
</tbody>
</tgroup>
</table>
<p>
  <xref href="#topic6490"> Return to Top </xref>
</p>
</section>
<section id="health"> <title>Monitoring the Health of Services using Icinga</title>
<p>Cloud admins can use the <b>
<i>Icinga</i>
</b> service running in the undercloud to monitor the health status of Account, Container, Proxy, and Object services.</p>
<p>You can monitor the health status of the swift-proxy, swift-account, swift-container and swift-object services by following these steps:</p>
<ol>
<li>
<p>In the undercloud Horizon dashboard, log in to the <b>Icinga Dashboard</b> (http://&lt;Undercloud_IP&gt;/icinga/). The default login credentials are as follows::</p>

<ul>
<li>Username: <i>icingaadmin</i>
</li>
<li>Password: <i>icingaadmin</i>  </li>
</ul>
</li>
<li>
<p>Click <b>Status</b> on the left panel and then click <b>Host Details </b>.
<!--A BR tag was used here in the original source.--><image href="../media/icinga_host-details.png" placement="break"/>
</p>
</li>
<li>
<p>In the Host column, click the icon next to the host IP of the Swift storage node to open <b>Service Status Details For Host <varname>All Hosts</varname>
</b>.
<!--A BR tag was used here in the original source.--><image href="../media/swift_icinga_view-details.png" placement="break"/>
</p>
</li>
<li>
<p>Click the Swift node IP address to open the <b>Service Status Details For Host <varname>Swift node IP address </varname>
</b> to view the health status of the selected Swift node.
<!--A BR tag was used here in the original source.--><image href="../media/swift_icinga-health-status.png" placement="break"/>
</p>
</li>
</ol>
</section>
<section id="status-messages-for-health-monitoring"> <title>Status Messages for health monitoring</title>
<p>The Icinga service presents the following messages to indicate the health status of the cloud:</p>
<table>
<tgroup cols="3">
<colspec colname="col1"/>
<colspec colname="col2"/>
<colspec colname="col3"/>
<thead>
<row>
<entry>Status</entry>
    <entry>
<required-cleanup remap="center">
<ph>Message</ph>
</required-cleanup>
</entry>
    <entry>
<required-cleanup remap="center">
<ph>Cause/Resolution</ph>
</required-cleanup>
</entry>
</row>
</thead>
<tbody>
<row>
<entry>OK</entry>
    <entry>OK</entry>
    <entry/>
</row>
<row>
<entry>WARNING </entry>
    <entry>Disabled : proxy-server</entry>
    <entry>Service is temporarily disabled.<!--A BR tag was used here in the original source.--><!--A BR tag was used here in the original source.-->
    The proxy-server has been disabled by the creation of a file in the "disable_path" config parameter. This is often used to temporarily disable a proxy server by removing it from the
    load balancer or high availability system. To resolve this issue, delete the file indicated in the "disable_path" configuration parameter.</entry>
</row>
<row>
<entry>FAIL </entry>
    <entry>No response</entry>
    <entry> The named server is not responding. The process may not be running or may be stuck in some way. To resolve, start or restart the server.</entry>
</row>
</tbody>
</tgroup>
</table>
<p>
  <xref href="#topic6490"> Return to Top </xref>
</p>
<!-- ===================== horizontal rule ===================== -->
</section>
</body>
</topic>
